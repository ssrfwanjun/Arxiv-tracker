<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-30 04:49</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260130_0449</div>
    <div class="row"><div class="card">
<div class="title">BlindSight: Harnessing Sparsity for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Tharun Adithya Srikrishnan, Deval Shah, Timothy Hein, Ahmed Hasssan, Stephen Youn, Steven K. Reinhardt</div>
<div class="meta-line">First: 2025-07-11T23:15:30+00:00 · Latest: 2026-01-28T18:45:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.09071v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.09071v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) enable joint processing of text and images. However, incorporating vision data significantly increases the prompt length, resulting in a longer time to first token (TTFT). This bottleneck can be alleviated by leveraging the inherent sparsity in the attention computation. Analyzing these attention patterns in VLMs when processing a series of images, we observe the absence of inter-image attention in a substantial portion of layers. Based on this, we propose BlindSight: an approach to optimize multi-image VLM inference using an input-template-aware attention sparsity mask with no runtime overhead. We utilize a dataset to derive a prompt-agnostic categorization for attention heads: Dense, Sink, Intra-Image, and Intra-Image+Sink. We develop a Triton-based GPU kernel to leverage this sparsity. BlindSight achieves a 1.8-3.2x speedup in the attention computation (prompt length 36K-300K). BlindSight generalizes across VLMs (Qwen2-VL, Qwen2.5-VL, Gemma 3), with only a 0.78% absolute accuracy degradation on average on multi-image comprehension benchmarks. Finally, we advocate for the design of efficient VLMs that combine BlindSight-inspired sparse and dense layers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BlindSight：利用稀疏性实现高效视觉语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLM）支持文本与图像的联合处理。然而，视觉数据的引入显著增加了提示长度，导致首词生成时间（TTFT）延长。通过利用注意力计算固有的稀疏性可缓解此瓶颈。分析VLM处理系列图像时的注意力模式，我们发现在大部分层中不存在图像间注意力。基于此，我们提出BlindSight：一种利用输入模板感知的注意力稀疏掩码优化多图像VLM推理的方法，且无需运行时开销。我们通过数据集推导出注意力头的提示无关分类：密集型、汇聚型、图像内型、图像内+汇聚型。开发了基于Triton的GPU内核以利用此稀疏性。BlindSight在注意力计算上实现1.8-3.2倍加速（提示长度36K-300K）。该方法可泛化至多种VLM（Qwen2-VL、Qwen2.5-VL、Gemma 3），在多图像理解基准测试中平均仅产生0.78%的绝对准确率下降。最后，我们倡导设计融合BlindSight启发的稀疏层与密集层的高效VLM架构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inference bottleneck in large vision-language models (VLMs) where incorporating multiple images drastically increases prompt length and time to first token (TTFT). The method, BlindSight, analyzes attention patterns across layers to identify sparse inter-image interactions, categorizes attention heads into types like Dense and Intra-Image, and implements a prompt-agnostic sparsity mask with a custom Triton GPU kernel to eliminate runtime overhead. Experiments show BlindSight accelerates attention computation by 1.8-3.2x for prompts of 36K to 300K tokens, generalizes across models including Qwen2-VL and Gemma 3, and maintains multi-image comprehension with only a 0.78% average accuracy drop.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉语言模型（VLM）处理多图像时因提示过长导致首词元时间（TTFT）增加的问题，通过利用注意力计算中的稀疏性进行优化。提出的BlindSight方法分析各层注意力模式，将注意力头分类为密集、汇点、图像内或图像内+汇点类型，并采用一种与提示无关、基于输入模板的稀疏注意力掩码，配合定制的Triton GPU内核实现零运行时开销。实验结果表明，在36K至300K令牌的提示长度下，BlindSight将注意力计算速度提升了1.8-3.2倍，且在Qwen2-VL等模型的多图像理解基准测试中平均准确率仅下降0.78%，为未来设计结合稀疏与密集层的高效VLM提供了方向。</div>
</details>
</div>
<div class="card">
<div class="title">Open-Vocabulary Functional 3D Human-Scene Interaction Generation</div>
<div class="meta-line">Authors: Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron, Michael J. Black, Yan Zhang</div>
<div class="meta-line">First: 2026-01-28T18:34:25+00:00 · Latest: 2026-01-28T18:34:25+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20835v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &quot;sitting on a sofa&#x27;&#x27;, while supporting fine-grained functional human-scene interactions, e.g., &quot;increasing the room temperature&#x27;&#x27;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放词汇功能性三维人-场景交互生成</div>
<div class="mono" style="margin-top:8px">生成与三维场景进行功能性交互的三维人体，仍然是具身人工智能、机器人和交互式内容创作领域的一个开放性问题。核心挑战在于同时理解三维场景中功能元素的语义，以及实现功能感知交互所需的三维人体姿态。现有方法通常缺乏对物体功能性和相应人-场景接触的显式推理，导致生成不自然或功能错误的交互。本研究提出FunHSI，一个无需训练、功能驱动的框架，能够根据开放词汇任务提示生成功能正确的人-场景交互。给定任务提示后，FunHSI通过功能感知接触推理识别场景功能元素，重建其三维几何结构，并通过接触图建模高层级交互。随后利用视觉-语言模型合成执行任务的图像化人体，并估计提议的三维身体与手部姿态。最后通过分阶段优化细化提议的三维身体构型，确保物理合理性与功能正确性。与现有方法相比，FunHSI不仅能合成更合理的通用三维交互（如“坐在沙发上”），同时支持细粒度功能性人-场景交互（如“调高室温”）。大量实验表明，FunHSI能在多样室内外场景中持续生成功能正确且物理合理的人-场景交互。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to generate 3D humans that functionally interact with 3D scenes, addressing the lack of explicit reasoning over object functionality and human-scene contact in existing methods. The proposed FunHSI framework is training-free and functionality-driven: given an open-vocabulary task prompt, it performs functionality-aware contact reasoning to identify functional scene elements, reconstructs their 3D geometry, models interactions via a contact graph, leverages vision-language models to synthesize a human image and estimate 3D poses, and refines the configuration through stage-wise optimization for physical plausibility. Experimental results show that FunHSI consistently generates functionally correct and physically plausible interactions, such as &quot;sitting on a sofa&quot; and &quot;increasing the room temperature&quot;, across diverse indoor and outdoor scenes.</div>
<div class="mono" style="margin-top:8px">该研究旨在生成与3D场景进行功能交互的3D人体，以解决现有方法缺乏对物体功能性和人-场景接触的显式推理的问题。所提出的FunHSI框架无需训练且以功能驱动：给定开放词汇任务提示，它执行功能感知的接触推理以识别场景中的功能元素，重建其3D几何结构，通过接触图建模高级交互，利用视觉语言模型合成执行任务的人体图像并估计3D姿态，最后通过分阶段优化细化3D身体配置以确保物理合理性和功能正确性。大量实验表明，FunHSI能在多样化的室内外场景中一致地生成功能正确且物理合理的人-场景交互，例如“坐在沙发上”和“调高室温”。</div>
</details>
</div>
<div class="card">
<div class="title">FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</div>
<div class="meta-line">Authors: Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu</div>
<div class="meta-line">First: 2026-01-01T09:19:39+00:00 · Latest: 2026-01-28T16:05:20+00:00</div>
<div class="meta-line">Comments: 21 pages, 13 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00269v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00269v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FaithSCAN：面向可信视觉问答的模型驱动单轮幻觉检测</div>
<div class="mono" style="margin-top:8px">视觉问答中的忠实性幻觉指视觉语言模型生成流畅但缺乏视觉依据的答案，严重削弱其在安全关键应用中的可靠性。现有检测方法主要分为两类：依赖辅助模型或知识库的外部验证方法，以及采用重复采样或不确定性估计的不确定性驱动方法。前者计算开销大且受限于外部资源质量，后者仅能捕捉模型不确定性的有限维度，未能充分挖掘与多样化失效模式相关的丰富内部信号。这两类范式在效率、鲁棒性和检测性能上均存在固有局限。为应对这些挑战，我们提出FaithSCAN：一种通过挖掘视觉语言模型丰富内部信号（包括词元级解码不确定性、中间视觉表征和跨模态对齐特征）实现幻觉检测的轻量网络。这些信号通过分支证据编码和不确定性感知注意力进行融合。我们还将LLM-as-a-Judge范式扩展至视觉问答幻觉检测，提出一种低成本策略自动生成模型相关的监督信号，在无需昂贵人工标注的情况下实现监督训练，同时保持高检测精度。在多个视觉问答基准上的实验表明，FaithSCAN在效果和效率上均显著优于现有方法。深入分析揭示幻觉源于视觉感知、跨模态推理和语言解码等系统内部状态变化，不同内部信号提供互补的诊断线索，且幻觉模式随视觉语言模型架构变化，为理解多模态幻觉的成因提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the problem of faithfulness hallucinations in visual question answering (VQA), where models generate plausible but visually ungrounded answers, compromising reliability. The proposed method, FaithSCAN, is a lightweight network that detects hallucinations by fusing rich internal signals from vision-language models, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features, using branch-wise evidence encoding and uncertainty-aware attention. It also introduces a low-cost, model-dependent supervision strategy based on an LLM-as-a-Judge paradigm to generate training signals without human annotation. Experimental results on multiple VQA benchmarks demonstrate that FaithSCAN significantly outperforms existing methods in both detection effectiveness and efficiency, with analysis revealing that hallucinations stem from systematic variations in internal states related to visual perception, cross-modal reasoning, and language decoding, and that different internal signals provide complementary diagnostic cues.</div>
<div class="mono" style="margin-top:8px">针对视觉问答中现有幻觉检测方法依赖低效的外部验证或仅捕捉有限模型不确定性的局限，本文提出了FaithSCAN，一种模型驱动的单次检测网络。该方法通过分支证据编码和不确定性感知注意力，融合了视觉语言模型的丰富内部信号，包括令牌级解码不确定性、中间视觉表示和跨模态对齐特征。它还引入了一种低成本策略，无需人工标注即可自动生成监督信号进行训练。在多个VQA基准上的实验表明，FaithSCAN在检测效果和效率上均显著优于现有方法，同时分析揭示幻觉源于视觉感知、跨模态推理和语言解码的系统性变化，且其模式因模型架构而异。</div>
</details>
</div>
<div class="card">
<div class="title">bi-modal textual prompt learning for vision-language models in remote sensing</div>
<div class="meta-line">Authors: Pankhi Kashyap, Mainak Singha, Biplab Banerjee</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-28T14:58:14+00:00 · Latest: 2026-01-28T14:58:14+00:00</div>
<div class="meta-line">Comments: Accepted in ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20675v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20675v1">PDF</a> · <a href="https://github.com/ipankhi/BiMoRS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向遥感视觉语言模型的双模态文本提示学习</div>
<div class="mono" style="margin-top:8px">提示学习已成为在有限监督下适配视觉语言模型（如CLIP）至下游任务的有效策略。尽管提示学习在自然图像数据集上展现出强大的泛化能力，但其在遥感影像中的可迁移性仍待深入探索。遥感数据具有多标签场景、高类内变异性和多样空间分辨率等独特挑战，阻碍了现有提示学习方法的直接应用。现有基于提示的方法常难以识别主导语义线索，且在遥感新类别场景中泛化能力不足。为此，我们提出BiMoRS——一个专为遥感任务设计的轻量级双模态提示学习框架。BiMoRS采用冻结的图像描述模型（如BLIP-2）从遥感图像中提取文本语义摘要，通过BERT分词器进行标记化处理，并与CLIP编码器的高级视觉特征融合。轻量级交叉注意力模块基于融合的文本-视觉表征对可学习查询提示进行条件化处理，在不改变CLIP主干网络的情况下生成上下文感知提示。我们在三个领域泛化任务的四个遥感数据集上评估BiMoRS，其性能持续优于基线方法，平均提升达2%。代码已开源：https://github.com/ipankhi/BiMoRS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limited transferability of prompt learning from natural images to remote sensing (RS) data, which is challenged by multi-label scenes, high intra-class variability, and diverse spatial resolutions. The proposed method, BiMoRS, is a lightweight bi-modal prompt learning framework that uses a frozen image captioning model to extract textual semantic summaries from RS images, tokenizes them with BERT, and fuses them with visual features from CLIP; a cross-attention module then conditions a learnable query prompt on this fused representation without modifying the CLIP backbone. Experimental results on four RS datasets across three domain generalization tasks show consistent performance improvements, outperforming strong baselines by up to 2% on average.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有提示学习方法从自然图像到遥感图像的迁移性有限，遥感数据存在多标签场景、类内高变异性等独特挑战。所提出的BiMoRS是一种轻量级双模态提示学习框架，它使用冻结的图像描述模型从遥感图像中提取文本语义摘要，通过BERT进行分词，并与CLIP的视觉特征通过交叉注意力模块融合，从而在不修改CLIP主干的情况下生成上下文感知的提示。在四个遥感数据集上的三个领域泛化任务实验表明，BiMoRS consistently outperforms strong baselines by up to 2% on average。</div>
</details>
</div>
<div class="card">
<div class="title">DeepSeek-OCR 2: Visual Causal Flow</div>
<div class="meta-line">Authors: Haoran Wei, Yaofeng Sun, Yukun Li</div>
<div class="meta-line">First: 2026-01-28T12:46:07+00:00 · Latest: 2026-01-28T12:46:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20552v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20552v1">PDF</a> · <a href="http://github.com/deepseek-ai/DeepSeek-OCR-2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepSeek-OCR 2：视觉因果流</div>
<div class="mono" style="margin-top:8px">本文提出DeepSeek-OCR 2，旨在探索一种新型编码器DeepEncoder V2的可行性——该编码器能够依据图像语义动态重排视觉标记。传统视觉语言模型在处理图像时，始终以固定的光栅扫描顺序（左上至右下）和静态位置编码将视觉标记输入大语言模型。然而，这与人类视觉感知机制相悖：人类视觉遵循由内在逻辑结构驱动的、灵活而语义连贯的扫描模式。尤其在处理复杂版式图像时，人类视觉展现出基于因果关系的序列化处理能力。受此认知机制启发，DeepEncoder V2被设计为具备因果推理能力的编码器，使其在基于大语言模型的内容解析前，能智能地重排视觉标记。本研究探索了一种新颖范式：是否可通过两级级联的一维因果推理结构有效实现二维图像理解，从而提供一种有望实现真正二维推理的全新架构方案。代码与模型权重已公开于http://github.com/deepseek-ai/DeepSeek-OCR-2。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of conventional vision-language models that process visual tokens in a fixed raster-scan order, which contradicts the flexible, semantically-driven scanning patterns of human visual perception. The proposed method, DeepSeek-OCR 2, introduces a novel encoder called DeepEncoder V2, which is designed to dynamically reorder visual tokens based on image semantics using causal reasoning capabilities before feeding them to a large language model for interpretation. Experimental findings demonstrate the feasibility of this approach, showing that effective 2D image understanding can be achieved through two cascaded 1D causal reasoning structures, offering a new architectural paradigm for genuine 2D reasoning.</div>
<div class="mono" style="margin-top:8px">本研究的动机是观察到传统的视觉语言模型以固定的光栅扫描顺序处理视觉标记，这与人类视觉感知中灵活、语义驱动的扫描模式相矛盾。为此，作者提出了DeepSeek-OCR 2，其核心是一个名为DeepEncoder V2的新型编码器，该编码器能够在将视觉标记输入大语言模型之前，根据图像语义对其进行动态重排序。关键的实验结果表明，这种串联两个一维因果推理结构的方法，为实现更真实的二维图像理解提供了一种可行的新架构范式。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing Open-source World Models</div>
<div class="meta-line">Authors: Robbyant Team, Zelin Gao, Qiuyu Wang, Yanhong Zeng, Jiapeng Zhu, Ka Leong Cheng, Yixuan Li, Hanlin Wang, Yinghao Xu, Shuailei Ma, Yihang Chen, Jie Liu, Yansong Cheng, Yao Yao, Jiayi Zhu, Yihao Meng, Kecheng Zheng, Qingyan Bai, Jingye Chen, Zehong Shen, Yue Yu, Xing Zhu, Yujun Shen, Hao Ouyang</div>
<div class="meta-line">First: 2026-01-28T12:37:01+00:00 · Latest: 2026-01-28T12:37:01+00:00</div>
<div class="meta-line">Comments: Project page: https://technology.robbyant.com/lingbot-world; Code: https://github.com/robbyant/lingbot-world</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20540v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20540v1">PDF</a> · <a href="https://github.com/robbyant/lingbot-world">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as &quot;long-term memory&quot;. (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推进开源世界模型发展</div>
<div class="mono" style="margin-top:8px">我们推出LingBot-World，一个源于视频生成的开源世界模拟器。作为顶级世界模型，LingBot-World具备以下特性：(1) 在广泛环境中保持高保真度与强健动态，涵盖写实场景、科学情境、卡通风格等。(2) 实现分钟级时间跨度的同时保持时序上下文一致性，即“长期记忆”能力。(3) 支持实时交互，在以每秒16帧生成时延迟低于1秒。我们公开代码与模型，旨在缩小开源与闭源技术间的差距。相信本次发布将赋能社区在内容创作、游戏开发、机器人学习等领域的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance open-source world models by developing LingBot-World, a high-fidelity video-based world simulator that supports diverse environments including realistic, scientific, and cartoon styles. The method focuses on achieving long-term contextual consistency over minute-level horizons and real-time interactivity with latency under one second for 16 frames per second generation. Experimental results demonstrate robust dynamics and fidelity across varied settings, enabling practical applications in content creation, gaming, and robot learning while bridging the gap between open-source and closed-source technologies.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过开发高质量、可访问的世界模型来缩小开源与闭源技术之间的差距。方法提出了LingBot-World，这是一个基于视频生成的开源世界模拟器，具有跨多样化环境的高保真度和鲁棒动态特性，支持分钟级生成时长并保持长期上下文一致性，同时实现了低延迟的实时交互。关键实验结果表明，该模型能够以每秒16帧、低于1秒的延迟生成一致的长序列，适用于内容创作、游戏和机器人学习等领域。</div>
</details>
</div>
<div class="card">
<div class="title">AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors</div>
<div class="meta-line">Authors: Matic Fučka, Vitjan Zavrtanik, Danijel Skočaj</div>
<div class="meta-line">First: 2026-01-28T12:02:58+00:00 · Latest: 2026-01-28T12:02:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20524v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20524v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://maticfuc.github.io/anomaly_vfm/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnomalyVFM——将视觉基础模型转化为零样本异常检测器</div>
<div class="mono" style="margin-top:8px">零样本异常检测旨在无需任何域内训练图像的情况下，检测并定位图像中的异常区域。尽管现有方法利用视觉语言模型（如CLIP）迁移高层概念知识，但基于纯视觉基础模型（如DINOv2）的方法在性能上仍显滞后。我们认为这一差距源于两个实际问题：（一）现有辅助异常检测数据集多样性有限；（二）视觉基础模型的适应策略过于浅层。为应对这两项挑战，我们提出AnomalyVFM——一个通用且高效的框架，可将任何预训练的视觉基础模型转化为强大的零样本异常检测器。该方法结合了鲁棒的三阶段合成数据集生成方案与参数高效的适应机制，利用低秩特征适配器和置信度加权的像素损失。这些组件共同使现代视觉基础模型显著超越当前最优方法。具体而言，以RADIO为骨干网络时，AnomalyVFM在9个多样化数据集上实现了94.1%的平均图像级AUROC，较先前方法提升3.3个百分点。项目页面：https://maticfuc.github.io/anomaly_vfm/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the performance gap in zero-shot anomaly detection between vision-language models and vision foundation models (VFMs), attributing it to limited dataset diversity and shallow adaptation strategies. The proposed AnomalyVFM framework introduces a three-stage synthetic dataset generation scheme combined with parameter-efficient adaptation using low-rank feature adapters and a confidence-weighted pixel loss to transform pretrained VFMs into anomaly detectors. Experimental results demonstrate that AnomalyVFM with a RADIO backbone achieves an average image-level AUROC of 94.1% across nine datasets, outperforming previous state-of-the-art methods by 3.3 percentage points.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型与视觉基础模型在零样本异常检测中的性能差距，将其归因于数据集多样性不足和适应策略过于浅层。提出的AnomalyVFM框架通过三阶段合成数据集生成方案，结合使用低秩特征适配器和置信度加权像素损失的参数高效适应机制，将预训练的视觉基础模型转化为有效的异常检测器。实验结果表明，以RADIO为骨干的AnomalyVFM在九个数据集上实现了94.1%的平均图像级AUROC，比先前最优方法提升了3.3个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models</div>
<div class="meta-line">Authors: Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-23T16:17:47+00:00 · Latest: 2026-01-28T10:49:58+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026. Our code is available at https://github.com/xuyang-liu16/MixKV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20707v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.20707v2">PDF</a> · <a href="https://github.com/xuyang-liu16/MixKV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose MixKV, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. MixKV adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that MixKV consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), MixKV improves baseline methods by an average of 5.1% across five multi-modal understanding benchmarks and achieves remarkable gains of 8.0% and 9.0% for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, MixKV extends seamlessly to LLMs with comparable performance gains. Our code is available at https://github.com/xuyang-liu16/MixKV.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合重要性与多样性：大型视觉语言模型中KV缓存压缩的联合优化</div>
<div class="mono" style="margin-top:8px">近期的大型视觉语言模型在处理长序列多模态数据时展现出卓越能力，但其产生的键值缓存扩展形成了关键的内存瓶颈，从根本上限制了部署可扩展性。现有KV缓存压缩方法主要关注保留高重要性KV对以最小化存储，却常忽视多模态KV缓存中特有的模态语义冗余模式。本研究首先分析了LVLMs中KV缓存如何在注意力头间呈现不同程度的冗余，并证明仅依赖重要性只能覆盖部分KV缓存信息分布，可能导致语义覆盖损失。为此，我们提出MixKV——一种融合重要性与多样性的新型KV缓存压缩方法。MixKV能自适应头级语义冗余，在压缩KV对时选择性平衡多样性与重要性。大量实验表明，MixKV能持续提升多种LVLMs现有方法的性能。在极端压缩条件下，MixKV在五项多模态理解基准上将基线方法平均提升5.1%，在GUI定位任务中为SnapKV和AdaKV分别带来8.0%和9.0%的显著增益，同时保持相当的推理效率。此外，MixKV可无缝扩展至LLMs并获得可比性能提升。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The expansion of key-value (KV) cache memory in large vision-language models (LVLMs) during long multi-modal sequence processing creates a deployment bottleneck. Existing compression methods focus on retaining high-importance KV pairs but overlook modality-specific semantic redundancy patterns across attention heads. To address this, the authors propose MixKV, a method that jointly optimizes importance and diversity by adapting to head-wise semantic redundancy when selecting KV pairs for compression. Experiments show that under extreme compression (budget=64), MixKV improves baseline methods by an average of 5.1% across five multi-modal benchmarks and achieves gains up to 9.0% on GUI grounding tasks while maintaining inference efficiency, with comparable benefits extending to large language models.</div>
<div class="mono" style="margin-top:8px">本研究针对大规模视觉语言模型在处理长序列多模态数据时，键值缓存扩展导致的关键内存瓶颈问题。现有压缩方法仅关注保留高重要性键值对，忽视了不同注意力头之间模态特定的语义冗余模式。提出的MixKV方法通过适应注意力头级别的语义冗余，在压缩键值对时联合优化重要性和多样性选择。实验结果表明，在极端压缩条件下（预算=64），MixKV在五个多模态理解基准上平均提升基线方法5.1%，在GUI定位任务上对SnapKV和AdaKV分别实现8.0%和9.0%的显著提升，同时保持推理效率，该方法也能有效扩展到语言模型。</div>
</details>
</div>
<div class="card">
<div class="title">MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</div>
<div class="meta-line">Authors: Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou</div>
<div class="meta-line">First: 2026-01-28T09:44:31+00:00 · Latest: 2026-01-28T09:44:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20433v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20433v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARE：基于视觉语言模型的多模态对齐与强化可解释深度伪造检测</div>
<div class="mono" style="margin-top:8px">深度伪造检测是广泛研究的关键课题，对遏制恶意内容传播至关重要，现有方法主要将问题建模为分类或空间定位任务。生成模型的快速发展对深度伪造检测提出了新要求。本文提出基于视觉语言模型的多模态对齐与强化可解释深度伪造检测方法MARE，旨在提升视觉语言模型在深度伪造检测与推理中的准确性与可靠性。具体而言，MARE设计了融合人类反馈强化学习的综合奖励函数，激励生成符合人类偏好且文本-空间对齐的推理内容。此外，MARE引入伪造解耦模块，从高层面部语义中捕捉本质伪造痕迹，从而提升真实性检测能力。我们对MARE生成的推理内容进行了全面评估，定量与定性实验结果均表明，MARE在准确性与可靠性方面达到了最先进的性能水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more accurate and reliable deepfake detection as generative models advance, this paper proposes MARE, a method that enhances vision-language models (VLMs) for explainable detection. The approach integrates reinforcement learning from human feedback (RLHF) with comprehensive reward functions to align textual reasoning with spatial visual evidence and introduces a forgery disentanglement module to separate intrinsic forgery traces from high-level facial semantics. Experimental evaluations show that MARE achieves state-of-the-art performance in both detection accuracy and the reliability of its generated reasoning content.</div>
<div class="mono" style="margin-top:8px">随着生成模型的快速发展，对深度伪造检测的准确性和可靠性提出了新需求，本文提出了MARE方法，旨在通过视觉语言模型实现可解释的检测。该方法利用基于人类反馈的强化学习设计综合奖励函数，以生成符合人类偏好且与空间视觉对齐的推理内容，并引入伪造解缠模块从高层面部语义中捕获内在伪造痕迹。全面的实验评估表明，MARE在检测准确性和可靠性方面均达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Let&#x27;s Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models</div>
<div class="meta-line">Authors: Yuhao Sun, Chengyi Cai, Jiacheng Zhang, Zesheng Ye, Xingliang Yuan, Feng Liu</div>
<div class="meta-line">First: 2026-01-28T09:24:14+00:00 · Latest: 2026-01-28T09:24:14+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20419v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiFTA：视觉语言模型中细粒度文本-视觉对齐的双向精炼方法</div>
<div class="mono" style="margin-top:8px">近期研究表明，将细粒度文本描述与局部图像块对齐能显著提升预训练视觉语言模型（如CLIP）的零样本性能。然而，我们发现细粒度文本描述和局部图像块常包含冗余信息，降低了文本-视觉对齐效果。本文从两个角度解决该问题：\emph{视图精炼}与\emph{描述精炼}，统称为\textit{\textbf{细粒度文本-视觉对齐双向精炼}}（BiFTA）。\emph{视图精炼}通过移除具有高\emph{交并比}（IoU）的冗余图像块，获得更具区分度的视觉样本；\emph{描述精炼}通过剔除具有高余弦相似度的冗余文本描述，确保剩余描述的多样性。BiFTA在6个基准数据集上为基于ViT和ResNet的CLIP模型均实现了更优的零样本性能，证实了在视觉-文本对齐中消除冗余信息的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue that fine-grained text descriptions and localized image patches often contain redundant information, which reduces the effectiveness of text-visual alignment in vision-language models like CLIP. The proposed method, BiFTA, tackles this through two complementary refinements: view refinement removes redundant image patches with high Intersection over Union ratios to create more distinctive visual samples, and description refinement eliminates redundant text descriptions with high pairwise cosine similarity to ensure greater diversity. Experimental results demonstrate that BiFTA achieves superior zero-shot performance on six benchmark datasets for both ViT-based and ResNet-based CLIP models, justifying the necessity of removing redundant information in fine-grained alignment.</div>
<div class="mono" style="margin-top:8px">该研究针对细粒度文本描述和局部图像块常包含冗余信息，从而阻碍视觉语言模型（如CLIP）中文本-视觉对齐有效性的问题。提出的BiFTA方法通过两种互补的细化策略解决此问题：视图细化基于高交并比移除冗余图像块以生成更具区分性的视觉样本，描述细化则利用高余弦相似度剔除冗余文本描述以确保更高的多样性。实验结果表明，BiFTA在六个基准数据集上为基于ViT和ResNet的CLIP模型均实现了优异的零样本性能，证实了在细粒度对齐中去除冗余信息的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</div>
<div class="meta-line">Authors: Bikang Pan, Qun Li, Xiaoying Tang, Wei Huang, Zhen Fang, Feng Liu, Jingya Wang, Jingyi Yu, Ye Shi</div>
<div class="meta-line">First: 2024-12-02T08:25:09+00:00 · Latest: 2026-01-28T08:33:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.01256v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.01256v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text features in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representations and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NLPrompt：面向视觉语言模型的噪声标签提示学习</div>
<div class="mono" style="margin-top:8px">视觉语言基础模型（如CLIP）的出现革新了图文表示，通过提示学习实现了广泛应用。然而，现实数据集常含噪声标签，可能损害提示学习性能。本文提出在提示学习中使用平均绝对误差（MAE）损失（称为PromptMAE），能在保持高精度的同时显著增强对噪声标签的鲁棒性。尽管MAE因其鲁棒性被认可，但由于收敛慢且在非提示学习场景中表现不佳，鲜少用于噪声标签学习。为阐明PromptMAE的鲁棒性，我们借助特征学习理论证明MAE能抑制噪声样本的影响，从而提高信噪比并增强整体鲁棒性。此外，我们提出PromptOT——一种基于提示的最优传输数据净化方法，以进一步提升鲁棒性。PromptOT利用视觉语言模型中的文本特征作为原型构建最优传输矩阵，有效将数据集划分为干净与噪声子集，从而对干净子集应用交叉熵损失，对噪声子集应用MAE损失。我们提出的噪声标签提示学习方法NLPrompt，提供了一种简洁高效的方案，利用视觉语言模型的强表征能力和精准对齐特性实现鲁棒的提示学习。通过多种噪声设置下的广泛实验，我们验证了NLPrompt能带来显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the degradation of prompt learning performance in vision-language models like CLIP due to noisy labels in real-world datasets. The method introduces PromptMAE, which uses mean absolute error loss to suppress noisy sample influence and improve robustness, and PromptOT, a prompt-based optimal transport technique that partitions data into clean and noisy subsets for tailored loss application. Experimental results across various noise settings show that this combined approach, termed NLPrompt, significantly enhances performance while maintaining high accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对现实数据集中噪声标签导致视觉语言模型（如CLIP）提示学习性能下降的问题，提出了NLPrompt方法。该方法结合了PromptMAE（使用平均绝对误差损失抑制噪声样本影响以提升鲁棒性）和PromptOT（一种基于提示的最优传输数据净化技术，通过划分干净和噪声子集并应用相应损失函数）。在不同噪声设置下的实验表明，NLPrompt能显著提高鲁棒性并保持高准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Instruction Pretraining for Domain-Specific Foundation Models</div>
<div class="meta-line">Authors: Yuxuan Li, Yicheng Zhang, Wenhao Tang, Yimian Dai, Ming-Ming Cheng, Xiang Li, Jian Yang</div>
<div class="meta-line">First: 2025-09-22T10:57:42+00:00 · Latest: 2026-01-28T07:15:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17562v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.17562v3">PDF</a> · <a href="https://github.com/zcablii/ViTP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern computer vision is converging on a closed loop in which perception, reasoning and generation mutually reinforce each other. However, this loop remains incomplete: the top-down influence of high-level reasoning on the foundational learning of low-level perceptual features is not yet underexplored. This paper addresses this gap by proposing a new paradigm for pretraining foundation models in downstream domains. We introduce Visual insTruction Pretraining (ViTP), a novel approach that directly leverages reasoning to enhance perception. ViTP embeds a Vision Transformer (ViT) backbone within a Vision-Language Model and pretrains it end-to-end using a rich corpus of visual instruction data curated from target downstream domains. ViTP is powered by our proposed Visual Robustness Learning (VRL), which compels the ViT to learn robust and domain-relevant features from a sparse set of visual tokens. Extensive experiments on 16 challenging remote sensing and medical imaging benchmarks demonstrate that ViTP establishes new state-of-the-art performance across a diverse range of downstream tasks. The code is available at https://github.com/zcablii/ViTP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向领域专用基础模型的视觉指令预训练</div>
<div class="mono" style="margin-top:8px">现代计算机视觉正形成一个感知、推理与生成相互强化的闭环系统，但该闭环仍不完整：高层推理对底层感知特征基础学习的自上而下影响尚未被充分探索。本文提出一种面向下游领域的基础模型预训练新范式，引入视觉指令预训练（ViTP），该方法直接利用推理增强感知能力。ViTP将视觉Transformer骨干网络嵌入视觉-语言模型中，使用从目标下游领域构建的丰富视觉指令数据进行端到端预训练。该方法通过我们提出的视觉鲁棒性学习（VRL）机制，迫使ViT从稀疏的视觉标记中学习鲁棒且与领域相关的特征。在16个具有挑战性的遥感与医学影像基准测试上的大量实验表明，ViTP在多种下游任务中均实现了最先进的性能。代码已开源：https://github.com/zcablii/ViTP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the incomplete integration of high-level reasoning into foundational perceptual learning within the vision-language loop. The method introduces Visual Instruction Pretraining (ViTP), which embeds a Vision Transformer backbone within a vision-language model and pretrains it end-to-end using domain-specific visual instruction data, enhanced by a Visual Robustness Learning mechanism to learn robust features from sparse visual tokens. Experimental results on 16 remote sensing and medical imaging benchmarks show that ViTP achieves new state-of-the-art performance across diverse downstream tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对高级推理如何影响低级感知特征学习这一视觉基础模型中的未充分探索问题，提出了视觉指令预训练（ViTP）方法，将推理直接融入感知过程。该方法将视觉Transformer主干嵌入视觉-语言模型中，利用从下游目标领域收集的视觉指令数据进行端到端预训练，并通过提出的视觉鲁棒性学习从稀疏视觉标记中提取鲁棒且与领域相关的特征。在16个遥感和医学成像基准测试上的广泛实验表明，ViTP在多种下游任务中均取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Physically Guided Visual Mass Estimation from a Single RGB Image</div>
<div class="meta-line">Authors: Sungjae Lee, Junhan Jeong, Yeonjoo Hong, Kwang In Kim</div>
<div class="meta-line">First: 2026-01-28T06:53:36+00:00 · Latest: 2026-01-28T06:53:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20303v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20303v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating object mass from visual input is challenging because mass depends jointly on geometric volume and material-dependent density, neither of which is directly observable from RGB appearance. Consequently, mass prediction from pixels is ill-posed and therefore benefits from physically meaningful representations to constrain the space of plausible solutions. We propose a physically structured framework for single-image mass estimation that addresses this ambiguity by aligning visual cues with the physical factors governing mass. From a single RGB image, we recover object-centric three-dimensional geometry via monocular depth estimation to inform volume and extract coarse material semantics using a vision-language model to guide density-related reasoning. These geometry, semantic, and appearance representations are fused through an instance-adaptive gating mechanism, and two physically guided latent factors (volume- and density-related) are predicted through separate regression heads under mass-only supervision. Experiments on image2mass and ABO-500 show that the proposed method consistently outperforms state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物理引导的单幅RGB图像视觉质量估计</div>
<div class="mono" style="margin-top:8px">从视觉输入估计物体质量具有挑战性，因为质量同时取决于几何体积和材料相关密度，而这两者均无法直接从RGB外观观测。因此，基于像素的质量预测是不适定问题，需要借助具有物理意义的表征来约束合理解空间。我们提出一种物理结构化的单图像质量估计框架，通过将视觉线索与决定质量的物理因素对齐来解决这一模糊性。从单幅RGB图像中，我们通过单目深度估计恢复以物体为中心的三维几何以获取体积信息，并利用视觉语言模型提取粗略材料语义以指导密度相关推理。这些几何、语义和外观表征通过实例自适应门控机制进行融合，两个物理引导的潜在因子（体积相关和密度相关）在仅质量监督下通过独立回归头进行预测。在image2mass和ABO-500数据集上的实验表明，该方法持续优于现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the ill-posed challenge of estimating an object&#x27;s mass from a single RGB image, where mass is a product of unobservable volume and material density. To resolve this ambiguity, the method introduces a physically structured framework that aligns visual data with governing physical factors: it estimates 3D geometry via monocular depth for volume and extracts coarse material semantics using a vision-language model to inform density. These geometry, semantic, and appearance features are fused via an instance-adaptive gating mechanism, and separate regression heads predict volume- and density-related latent factors supervised solely by mass. Experimental results on the image2mass and ABO-500 datasets demonstrate that this approach consistently outperforms existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决从单张RGB图像估计物体质量的病态挑战，因为质量取决于不可直接观测的几何体积和材料密度。所提出的方法通过构建一个物理结构化的框架来解决这一模糊性：首先通过单目深度估计恢复三维几何以推断体积，并利用视觉语言模型提取粗略的材料语义以指导密度推理；这些表征通过一个实例自适应门控机制进行融合，并由独立的回归头在仅使用质量监督的情况下预测体积和密度相关的潜在因子。在image2mass和ABO-500数据集上的实验结果表明，该方法 consistently outperforms existing state-of-the-art methods。</div>
</details>
</div>
<div class="card">
<div class="title">Hallucination Begins Where Saliency Drops</div>
<div class="meta-line">Authors: Xiaofeng Zhang, Yuanchao Zhu, Chaochen Gu, Xiaosong Yuan, Qiyan Zhao, Jiawei Cao, Feilong Tang, Sinan Fan, Yaomin Shen, Chen Shen, Hao Tang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T05:50:52+00:00 · Latest: 2026-01-28T05:50:52+00:00</div>
<div class="meta-line">Comments: Accepted in ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20279v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20279v1">PDF</a> · <a href="https://github.com/zhangbaijin/LVLMs-Saliency">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies have examined attention dynamics in large vision-language models (LVLMs) to detect hallucinations. However, existing approaches remain limited in reliably distinguishing hallucinated from factually grounded outputs, as they rely solely on forward-pass attention patterns and neglect gradient-based signals that reveal how token influence propagates through the network. To bridge this gap, we introduce LVLMs-Saliency, a gradient-aware diagnostic framework that quantifies the visual grounding strength of each output token by fusing attention weights with their input gradients. Our analysis uncovers a decisive pattern: hallucinations frequently arise when preceding output tokens exhibit low saliency toward the prediction of the next token, signaling a breakdown in contextual memory retention. Leveraging this insight, we propose a dual-mechanism inference-time framework to mitigate hallucinations: (1) Saliency-Guided Rejection Sampling (SGRS), which dynamically filters candidate tokens during autoregressive decoding by rejecting those whose saliency falls below a context-adaptive threshold, thereby preventing coherence-breaking tokens from entering the output sequence; and (2) Local Coherence Reinforcement (LocoRE), a lightweight, plug-and-play module that strengthens attention from the current token to its most recent predecessors, actively counteracting the contextual forgetting behavior identified by LVLMs-Saliency. Extensive experiments across multiple LVLMs demonstrate that our method significantly reduces hallucination rates while preserving fluency and task performance, offering a robust and interpretable solution for enhancing model reliability. Code is available at: https://github.com/zhangbaijin/LVLMs-Saliency</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>幻觉始于显著性下降之处</div>
<div class="mono" style="margin-top:8px">近期研究通过分析大型视觉语言模型（LVLMs）的注意力动态来检测幻觉现象。然而，现有方法仅依赖前向传播的注意力模式，忽略了能揭示词元影响力在网络中传播的梯度信号，导致难以可靠区分幻觉输出与事实性输出。为弥补这一缺陷，我们提出LVLMs-Saliency——一种梯度感知诊断框架，通过融合注意力权重与输入梯度来量化每个输出词元的视觉基础强度。分析发现决定性规律：当前序输出词元对下一词元预测的显著性较低时，常引发幻觉，这标志着上下文记忆保持机制的失效。基于此发现，我们提出双机制推理时框架以缓解幻觉：（1）显著性引导拒绝采样（SGRS）：在自回归解码过程中动态过滤候选词元，拒绝显著性低于上下文自适应阈值的词元，从而防止破坏连贯性的词元进入输出序列；（2）局部连贯性增强（LocoRE）：轻量级即插即用模块，通过强化当前词元对最近前序词元的注意力，主动对抗LVLMs-Saliency识别的上下文遗忘行为。跨多个LVLMs的大量实验表明，该方法在保持流畅性与任务性能的同时显著降低幻觉率，为提升模型可靠性提供了鲁棒且可解释的解决方案。代码发布于：https://github.com/zhangbaijin/LVLMs-Saliency</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To improve the detection and mitigation of hallucinations in large vision-language models, this research introduces LVLMs-Saliency, a gradient-aware diagnostic framework that fuses attention weights with input gradients to quantify visual grounding strength. The analysis reveals that hallucinations often occur when preceding tokens exhibit low saliency, indicating a breakdown in contextual memory. Based on this insight, the authors propose a dual-mechanism inference-time framework combining Saliency-Guided Rejection Sampling to filter low-saliency tokens and Local Coherence Reinforcement to strengthen attention to recent context, which experiments show significantly reduces hallucination rates while maintaining fluency and task performance across multiple models.</div>
<div class="mono" style="margin-top:8px">为改进大型视觉语言模型中的幻觉检测与缓解，现有基于注意力的方法难以可靠区分幻觉与事实性输出，本研究引入了LVLMs-Saliency，这是一个梯度感知的诊断框架，通过融合注意力权重与输入梯度来量化视觉基础强度。分析发现，幻觉常在先前输出词元对预测下一个词元表现出低显著性时发生，这表明上下文记忆保持出现断裂。基于此，作者提出了一种双机制推理时框架，结合了显著性引导拒绝采样以过滤低显著性候选词元，以及局部连贯性增强以加强对最近前驱词的注意力，实验表明该方法在保持流畅性和任务性能的同时显著降低了幻觉率。</div>
</details>
</div>
<div class="card">
<div class="title">Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</div>
<div class="meta-line">Authors: Matthew Lisondra, Beno Benhabib, Goldie Nejat</div>
<div class="meta-line">First: 2025-05-26T20:08:09+00:00 · Latest: 2026-01-28T05:01:06+00:00</div>
<div class="meta-line">Comments: v2: Expanded systematic review; resubmitted to Robotics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20503v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20503v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models, have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interaction, mobile service robots can achieve more flexible understanding, adaptive behavior, and robust task execution in dynamic real-world environments. Despite this progress, embodied AI for mobile service robots continues to face fundamental challenges related to the translation of natural language instructions into executable robot actions, multimodal perception in human-centered environments, uncertainty estimation for safe decision-making, and computational constraints for real-time onboard deployment. In this paper, we present the first systematic review focused specifically on the integration of foundation models in mobile service robotics. We analyze how recent advances in foundation models address these core challenges through language-conditioned control, multimodal sensor fusion, uncertainty-aware reasoning, and efficient model scaling. We further examine real-world applications in domestic assistance, healthcare, and service automation, highlighting how foundation models enable context-aware, socially responsive, and generalizable robot behaviors. Beyond technical considerations, we discuss ethical, societal, and human-interaction implications associated with deploying foundation model-enabled service robots in human environments. Finally, we outline future research directions emphasizing reliability and lifelong adaptation, privacy-aware and resource-constrained deployment, and governance and human-in-the-loop frameworks required for safe, scalable, and trustworthy mobile service robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于基础模型的移动服务机器人具身人工智能：系统性综述</div>
<div class="mono" style="margin-top:8px">基础模型（包括大语言模型、视觉语言模型、多模态大语言模型和视觉语言动作模型）的快速发展为移动服务机器人领域的具身人工智能开辟了新途径。通过将基础模型与具身人工智能原理（智能系统通过物理交互进行感知、推理和行动）相结合，移动服务机器人能够在动态现实环境中实现更灵活的理解、自适应行为和鲁棒的任务执行。尽管取得进展，移动服务机器人的具身人工智能仍面临核心挑战：自然语言指令到可执行机器人动作的转化、以人为中心环境的多模态感知、安全决策的不确定性估计，以及实时机载部署的计算约束。本文首次针对基础模型在移动服务机器人领域的集成进行系统性综述，分析基础模型如何通过语言条件控制、多模态传感器融合、不确定性感知推理和高效模型缩放应对这些挑战。进一步探讨家庭辅助、医疗保健和服务自动化等实际应用，阐明基础模型如何实现情境感知、社会响应和可泛化的机器人行为。除技术考量外，还讨论了在人类环境中部署基于基础模型的服务机器人所涉及的伦理、社会和人机交互影响。最后，展望未来研究方向，强调可靠性及终身适应、隐私保护与资源受限部署，以及构建安全、可扩展、可信赖的移动服务机器人所需的治理与人机协同框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid progress of foundation models such as LLMs and VLMs motivates their integration into embodied AI for mobile service robots to achieve more flexible understanding and robust task execution in dynamic environments. This systematic review analyzes how recent advances address core challenges like instruction translation and multimodal perception through methods including language-conditioned control, sensor fusion, and uncertainty-aware reasoning. Key findings highlight that these models enable context-aware and generalizable behaviors in applications like domestic assistance, while also revealing critical needs for future work in reliability, privacy-aware deployment, and human-in-the-loop governance.</div>
<div class="mono" style="margin-top:8px">以大型语言模型和视觉语言模型为代表的基础模型快速发展，推动了其在移动服务机器人具身智能中的应用，旨在使机器人在动态现实环境中实现更灵活的理解与鲁棒的任务执行。本文通过系统性综述，分析了如何通过语言条件控制、多模态传感器融合及高效模型缩放等方法，应对将自然语言指令转化为机器人动作、多模态感知、不确定性估计及计算约束等核心挑战。主要实验结果表明，基础模型在家庭辅助、医疗保健等应用中能实现更具情境感知和可泛化的行为，同时指出了未来在可靠性、隐私保护部署及治理框架等方面的关键研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Classification Accuracy: Neural-MedBench and the Need for Deeper Reasoning Benchmarks</div>
<div class="meta-line">Authors: Miao Jing, Mengting Jia, Junling Lin, Zhongxia Shen, Huan Gao, Mingkun Xu, Shangyang Li</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-09-26T12:20:01+00:00 · Latest: 2026-01-28T02:56:44+00:00</div>
<div class="meta-line">Comments: 23 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22258v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.22258v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://neuromedbench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in vision-language models (VLMs) have achieved remarkable performance on standard medical benchmarks, yet their true clinical reasoning ability remains unclear. Existing datasets predominantly emphasize classification accuracy, creating an evaluation illusion in which models appear proficient while still failing at high-stakes diagnostic reasoning. We introduce Neural-MedBench, a compact yet reasoning-intensive benchmark specifically designed to probe the limits of multimodal clinical reasoning in neurology. Neural-MedBench integrates multi-sequence MRI scans, structured electronic health records, and clinical notes, and encompasses three core task families: differential diagnosis, lesion recognition, and rationale generation. To ensure reliable evaluation, we develop a hybrid scoring pipeline that combines LLM-based graders, clinician validation, and semantic similarity metrics. Through systematic evaluation of state-of-the-art VLMs, including GPT-4o, Claude-4, and MedGemma, we observe a sharp performance drop compared to conventional datasets. Error analysis shows that reasoning failures, rather than perceptual errors, dominate model shortcomings. Our findings highlight the necessity of a Two-Axis Evaluation Framework: breadth-oriented large datasets for statistical generalization, and depth-oriented, compact benchmarks such as Neural-MedBench for reasoning fidelity. We release Neural-MedBench at https://neuromedbench.github.io/ as an open and extensible diagnostic testbed, which guides the expansion of future benchmarks and enables rigorous yet cost-effective assessment of clinically trustworthy AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越分类准确率：Neural-MedBench与深度推理基准的必要性</div>
<div class="mono" style="margin-top:8px">视觉语言模型在标准医学基准测试中虽取得显著进展，但其真实临床推理能力仍不明确。现有数据集过度侧重分类准确率，形成评估假象——模型看似熟练却仍无法胜任高风险诊断推理。我们推出Neural-MedBench，这是一个紧凑但注重推理强度的基准测试，专门用于探索神经学多模态临床推理的边界。该基准整合多序列MRI扫描、结构化电子健康记录与临床笔记，涵盖三大核心任务体系：鉴别诊断、病灶识别与推理依据生成。为确保可靠评估，我们开发了混合评分流程，融合基于大语言模型的评分器、临床医师验证与语义相似度指标。通过对GPT-4o、Claude-4及MedGemma等前沿模型的系统评估，发现其性能较传统数据集出现显著下滑。错误分析表明，推理缺陷（而非感知错误）是模型的主要短板。本研究提出双轴评估框架的必要性：面向广度的大型数据集用于统计泛化，以及面向深度的紧凑型基准（如Neural-MedBench）用于保障推理可靠性。我们在https://neuromedbench.github.io/开源发布Neural-MedBench作为可扩展的诊断测试平台，旨在引导未来基准的拓展，并为临床可信人工智能提供严谨且高效的评估方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the limitation of current medical vision-language models (VLMs) that excel in classification accuracy on standard benchmarks but may lack deeper clinical reasoning abilities, creating an evaluation illusion. To probe multimodal clinical reasoning in neurology, the authors introduce Neural-MedBench, a compact benchmark integrating multi-sequence MRI scans, electronic health records, and clinical notes for tasks like differential diagnosis, lesion recognition, and rationale generation, evaluated via a hybrid pipeline combining LLM-based graders, clinician validation, and semantic metrics. Experimental results on VLMs such as GPT-4o, Claude-4, and MedGemma reveal a significant performance drop compared to conventional datasets, with error analysis indicating reasoning failures as the primary shortcoming, underscoring the need for a Two-Axis Evaluation Framework that balances breadth-oriented datasets with depth-oriented benchmarks like Neural-MedBench for rigorous assessment of clinical AI.</div>
<div class="mono" style="margin-top:8px">本研究针对当前医学视觉语言模型（VLMs）在标准基准测试中分类准确率高但可能缺乏深度临床推理能力的问题，指出这造成了评估幻觉。为探究神经学中的多模态临床推理，作者提出了Neural-MedBench，一个紧凑的基准测试，整合了多序列MRI扫描、结构化电子健康记录和临床笔记，用于鉴别诊断、病灶识别和原理生成等任务，并通过结合基于大语言模型的评分器、临床医生验证和语义度量的混合流程进行评估。在GPT-4o、Claude-4和MedGemma等模型上的实验结果显示，与传统数据集相比性能显著下降，错误分析表明推理失败是主要缺陷，这强调了需要采用双轴评估框架，平衡广度导向的数据集和深度导向的基准测试（如Neural-MedBench），以严格评估临床可信赖的人工智能。</div>
</details>
</div>
<div class="card">
<div class="title">PromptVFX: Text-Driven Fields for Open-World 3D Gaussian Animation</div>
<div class="meta-line">Authors: Mert Kiray, Paul Uhlenbruck, Nassir Navab, Benjamin Busam</div>
<div class="meta-line">First: 2025-06-01T17:22:59+00:00 · Latest: 2026-01-28T00:03:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.01091v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.01091v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://obsphera.github.io/promptvfx/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual effects (VFX) are key to immersion in modern films, games, and AR/VR. Creating 3D effects requires specialized expertise and training in 3D animation software and can be time consuming. Generative solutions typically rely on computationally intense methods such as diffusion models which can be slow at 4D inference. We reformulate 3D animation as a field prediction task and introduce a text-driven framework that infers a time-varying 4D flow field acting on 3D Gaussians. By leveraging large language models (LLMs) and vision-language models (VLMs) for function generation, our approach interprets arbitrary prompts (e.g., &quot;make the vase glow orange, then explode&quot;) and instantly updates color, opacity, and positions of 3D Gaussians in real time. This design avoids overheads such as mesh extraction, manual or physics-based simulations and allows both novice and expert users to animate volumetric scenes with minimal effort on a consumer device even in a web browser. Experimental results show that simple textual instructions suffice to generate compelling time-varying VFX, reducing the manual effort typically required for rigging or advanced modeling. We thus present a fast and accessible pathway to language-driven 3D content creation that can pave the way to democratize VFX further. Code available at https://obsphera.github.io/promptvfx/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PromptVFX：面向开放世界3D高斯动画的文本驱动场</div>
<div class="mono" style="margin-top:8px">视觉特效（VFX）是现代影视、游戏及AR/VR沉浸体验的关键。传统3D特效制作需掌握专业3D动画软件技能且耗时漫长。现有生成方案多依赖计算密集的扩散模型等方法，在4D推理时效率较低。本研究将3D动画重构为场预测任务，提出一种文本驱动框架，通过推断作用于3D高斯体的时变4D流场，结合大语言模型（LLM）与视觉语言模型（VLM）进行函数生成，可实时解析任意指令（如“让花瓶泛橙光后爆炸”）并即刻更新3D高斯体的颜色、透明度与空间位置。该设计避免了网格提取、人工或物理模拟等开销，使新手与专家用户都能在消费级设备（甚至网页浏览器）上轻松制作体素场景动画。实验表明，简单文本指令即可生成引人入胜的时变视觉特效，大幅减少了传统绑定与高级建模所需的人工投入。本研究由此开辟了一条快速、易用的语言驱动3D内容创作路径，为视觉特效的进一步普及化奠定基础。代码发布于 https://obsphera.github.io/promptvfx/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high expertise and time costs of creating 3D visual effects (VFX) in films and games, and the computational intensity of existing generative 4D methods. The method reformulates 3D animation as a field prediction task, introducing a text-driven framework that uses large language and vision-language models to interpret prompts and infer a time-varying 4D flow field to animate 3D Gaussians in real-time, avoiding mesh extraction or physics simulation. Experiments demonstrate that simple text instructions can generate compelling dynamic VFX, significantly reducing the manual effort for rigging and modeling, and enabling real-time operation on consumer devices.</div>
<div class="mono" style="margin-top:8px">该研究旨在简化和加速3D视觉特效（VFX）的创作，传统方法需要专业知识且耗时，而现有生成方法通常依赖计算密集的4D推理。方法提出了PromptVFX，一个文本驱动的框架，将3D动画重新定义为场预测任务，利用大语言模型（LLMs）和视觉语言模型（VLMs）解析任意文本提示，推断出时变4D流场，直接实时更新3D高斯模型的颜色、透明度和位置，避免了网格提取或手动模拟。实验结果表明，简单的文本指令即可生成引人注目的时变VFX，极大减少了手动操作，支持在消费级设备上实时动画，为语言驱动的3D内容创作提供了一条快速、易用的途径。</div>
</details>
</div>
<div class="card">
<div class="title">Look in the Middle: Structural Anchor Pruning for Scalable Visual RAG Indexing</div>
<div class="meta-line">Authors: Zhuchenyang Liu, Ziyu Hu, Yao Zhang, Yu Xiao</div>
<div class="meta-line">First: 2026-01-27T22:50:11+00:00 · Latest: 2026-01-27T22:50:11+00:00</div>
<div class="meta-line">Comments: 18 pages, 6 figures, 11 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent Vision-Language Models (e.g., ColPali) enable fine-grained Visual Document Retrieval (VDR) but incur prohibitive index vector size overheads. Training-free pruning solutions (e.g., EOS-attention based methods) can reduce index vector size by approximately 60% without model adaptation, but often underperform random selection in high-compression scenarios (&gt; 80%). Prior research (e.g., Light-ColPali) attributes this to the conclusion that visual token importance is inherently query-dependent, thereby questioning the feasibility of training-free pruning. In this work, we propose Structural Anchor Pruning (SAP), a training-free pruning method that identifies key visual patches from middle layers to achieve high performance compression. We also introduce Oracle Score Retention (OSR) protocol to evaluate how layer-wise information affects compression efficiency. Evaluations on the ViDoRe benchmark demonstrate that SAP reduces index vectors by over 90% while maintaining robust retrieval fidelity, providing a highly scalable solution for Visual RAG. Furthermore, our OSR-based analysis reveals that semantic structural anchor patches persist in the middle layers, unlike traditional pruning solutions that focus on the final layer where structural signals dissipate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>聚焦中间层：面向可扩展视觉RAG索引的结构锚点剪枝</div>
<div class="mono" style="margin-top:8px">近期视觉语言模型（如ColPali）虽能实现细粒度视觉文档检索，但索引向量规模过大。无需训练的剪枝方案（如基于EOS注意力的方法）可在不调整模型的情况下将索引向量压缩约60%，但在高压缩场景（&gt;80%）中常逊于随机选择。先前研究（如Light-ColPali）将此归因于视觉令牌重要性本质上的查询依赖性，从而质疑无需训练剪枝的可行性。本研究提出结构锚点剪枝——一种通过识别中间层关键视觉块实现高性能压缩的无训练剪枝方法，并引入Oracle评分保留协议评估分层信息对压缩效率的影响。ViDoRe基准测试表明，SAP能在保持检索鲁棒性的同时将索引向量压缩超90%，为视觉RAG提供高度可扩展的解决方案。基于OSR的分析进一步揭示：语义结构锚点块持续存在于中间层，与传统聚焦于结构信号已消散的最终层的剪枝方案形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the prohibitive index vector size overhead in fine-grained Visual Document Retrieval using Vision-Language Models, where existing training-free pruning methods underperform random selection at high compression rates. The proposed Structural Anchor Pruning method identifies key visual patches from middle model layers to achieve efficient compression, and an Oracle Score Retention protocol is introduced to evaluate layer-wise information impact. Experimental results on the ViDoRe benchmark show that the method reduces index vectors by over 90% while maintaining robust retrieval fidelity, and analysis reveals that semantic structural anchor patches persist in middle layers, unlike final-layer approaches where such signals dissipate.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型进行细粒度视觉文档检索时索引向量规模过大的问题，本研究提出了结构锚点剪枝（SAP），这是一种无需训练的方法，其从模型中间层而非最终层识别关键视觉图像块。在ViDoRe基准上的评估表明，SAP能将索引向量减少90%以上，同时保持稳健的检索性能，实现了高性能压缩。通过引入的Oracle Score Retention协议进行分析发现，语义结构锚点图像块持续存在于中间层，而在最终层此类信号会消散，这解释了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse CLIP: Co-Optimizing Interpretability and Performance in Contrastive Learning</div>
<div class="meta-line">Authors: Chuan Qin, Constantin Venhoff, Sonia Joseph, Fanyi Xiao, Stefan Scherer</div>
<div class="meta-line">First: 2026-01-27T21:39:00+00:00 · Latest: 2026-01-27T21:39:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20075v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20075v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contrastive Language-Image Pre-training (CLIP) has become a cornerstone in vision-language representation learning, powering diverse downstream tasks and serving as the default vision backbone in multimodal large language models (MLLMs). Despite its success, CLIP&#x27;s dense and opaque latent representations pose significant interpretability challenges. A common assumption is that interpretability and performance are in tension: enforcing sparsity during training degrades accuracy, motivating recent post-hoc approaches such as Sparse Autoencoders (SAEs). However, these post-hoc approaches often suffer from degraded downstream performance and loss of CLIP&#x27;s inherent multimodal capabilities, with most learned features remaining unimodal.
  We propose a simple yet effective approach that integrates sparsity directly into CLIP training, yielding representations that are both interpretable and performant. Compared to SAEs, our Sparse CLIP representations preserve strong downstream task performance, achieve superior interpretability, and retain multimodal capabilities. We show that multimodal sparse features enable straightforward semantic concept alignment and reveal training dynamics of how cross-modal knowledge emerges. Finally, as a proof of concept, we train a vision-language model on sparse CLIP representations that enables interpretable, vision-based steering capabilities. Our findings challenge conventional wisdom that interpretability requires sacrificing accuracy and demonstrate that interpretability and performance can be co-optimized, offering a promising design principle for future models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏CLIP：在对比学习中协同优化可解释性与性能</div>
<div class="mono" style="margin-top:8px">对比语言-图像预训练（CLIP）已成为视觉-语言表征学习的基石，支撑着多样化的下游任务，并作为多模态大语言模型（MLLMs）的默认视觉骨干。尽管CLIP取得了成功，但其稠密且不透明的潜在表征带来了显著的可解释性挑战。一个常见假设是，可解释性与性能之间存在权衡：在训练中强制稀疏性会降低准确性，这推动了如稀疏自编码器（SAEs）等后处理方法的出现。然而，这些后处理方法通常会导致下游性能下降和CLIP固有跨模态能力的损失，且大多数学习到的特征仍保持单模态。
我们提出了一种简单而有效的方法，将稀疏性直接集成到CLIP训练中，从而产生既具可解释性又高性能的表征。与SAEs相比，我们的稀疏CLIP表征保持了强大的下游任务性能，实现了更优的可解释性，并保留了跨模态能力。我们展示了多模态稀疏特征能够实现直接的语义概念对齐，并揭示了跨模态知识涌现的训练动态。最后，作为概念验证，我们在稀疏CLIP表征上训练了一个视觉-语言模型，该模型具备可解释的、基于视觉的引导能力。我们的发现挑战了可解释性需要牺牲准确性的传统观念，证明了可解释性与性能可以协同优化，为未来模型提供了一个有前景的设计原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the interpretability challenges of CLIP&#x27;s dense latent representations, which are widely used in vision-language tasks but are opaque, creating a perceived trade-off between interpretability and performance. The method introduces sparsity directly into CLIP training, rather than relying on post-hoc techniques like Sparse Autoencoders, to co-optimize for both interpretability and performance. Experimental results show that Sparse CLIP preserves strong downstream task accuracy, achieves superior interpretability with multimodal sparse features that enable semantic concept alignment, and allows for interpretable vision-based steering in vision-language models, challenging the notion that interpretability necessitates performance degradation.</div>
<div class="mono" style="margin-top:8px">本研究针对CLIP密集潜在表示的可解释性挑战展开，CLIP广泛应用于视觉-语言任务但其表示不透明，导致可解释性与性能之间存在权衡的普遍认知。方法提出Sparse CLIP，将稀疏性直接整合到对比训练过程中，无需依赖稀疏自编码器等后处理技术即可生成可解释表示。关键实验结果表明，Sparse CLIP在保持强大下游任务性能的同时，实现了更优的可解释性，保留了多模态能力，支持语义概念对齐，并在概念验证模型中实现了可解释的视觉引导功能，从而挑战了可解释性必须牺牲准确性的传统观点。</div>
</details>
</div>
<div class="card">
<div class="title">Diagnosing Vision Language Models&#x27; Perception by Leveraging Human Methods for Color Vision Deficiencies</div>
<div class="meta-line">Authors: Kazuki Hayashi, Shintaro Ozaki, Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe</div>
<div class="meta-line">First: 2025-05-23T04:43:55+00:00 · Latest: 2026-01-27T21:19:29+00:00</div>
<div class="meta-line">Comments: Accepted to appear in the main conference of EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17461v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17461v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale Vision-Language Models (LVLMs) are being deployed in real-world settings that require visual inference. As capabilities improve, applications in navigation, education, and accessibility are becoming practical. These settings require accommodation of perceptual variation rather than assuming a uniform visual experience. Color perception illustrates this requirement: it is central to visual understanding yet varies across individuals due to Color Vision Deficiencies, an aspect largely ignored in multimodal AI. In this work, we examine whether LVLMs can account for variation in color perception using the Ishihara Test. We evaluate model behavior through generation, confidence, and internal representation, using Ishihara plates as controlled stimuli that expose perceptual differences. Although models possess factual knowledge about color vision deficiencies and can describe the test, they fail to reproduce the perceptual outcomes experienced by affected individuals and instead default to normative color perception. These results indicate that current systems lack mechanisms for representing alternative perceptual experiences, raising concerns for accessibility and inclusive deployment in multimodal settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>借鉴人类色觉缺陷检测方法诊断视觉语言模型的感知能力</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型正被部署于需要视觉推理的现实场景中。随着能力提升，其在导航、教育和无障碍领域的应用日趋实用。这些场景需适应感知差异而非假设统一的视觉体验。色彩感知即体现此需求：它虽是视觉理解的核心，却因色觉缺陷存在个体差异——这一维度在多模态人工智能研究中长期被忽视。本研究通过石原氏色盲检测图检验LVLM能否处理色彩感知差异。我们以石原图板作为揭示感知差异的受控刺激，通过生成结果、置信度与内部表征评估模型行为。尽管模型掌握色觉缺陷的事实知识并能描述检测原理，却无法复现受影响个体的感知结果，反而默认采用标准色彩感知模式。结果表明当前系统缺乏表征替代性感知体验的机制，这对多模态场景下的无障碍设计与包容性部署提出了警示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for Vision-Language Models (VLMs) to accommodate perceptual variation, such as Color Vision Deficiencies (CVDs), for inclusive deployment in applications like navigation and accessibility, rather than assuming uniform visual experience. The method evaluates VLMs using the Ishihara Test, analyzing model behavior through generation, confidence, and internal representations when presented with controlled Ishihara plate stimuli. Key experimental findings reveal that while VLMs possess factual knowledge about CVDs and can describe the test, they fail to reproduce the perceptual outcomes of affected individuals and default to normative color perception, indicating a lack of mechanisms for representing alternative perceptual experiences.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型（VLM）在导航和辅助功能等应用中需要适应感知差异（如色觉缺陷）以实现包容性部署的问题。方法上，利用石原氏色盲测试作为受控刺激，通过分析模型在生成、置信度和内部表征方面的行为来评估其对感知差异的响应。主要实验结果表明，尽管模型具备关于色觉缺陷的事实性知识并能描述测试，但无法复现色觉缺陷个体的感知结果，而是默认采用标准色觉感知，这表明当前系统缺乏表征替代性感知体验的机制。</div>
</details>
</div>
<div class="card">
<div class="title">DiSa: Saliency-Aware Foreground-Background Disentangled Framework for Open-Vocabulary Semantic Segmentation</div>
<div class="meta-line">Authors: Zhen Yao, Xin Li, Taotao Jing, Shuai Zhang, Mooi Choo Chuah</div>
<div class="meta-line">First: 2026-01-27T21:15:10+00:00 · Latest: 2026-01-27T21:15:10+00:00</div>
<div class="meta-line">Comments: 19 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20064v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20064v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary semantic segmentation aims to assign labels to every pixel in an image based on text labels. Existing approaches typically utilize vision-language models (VLMs), such as CLIP, for dense prediction. However, VLMs, pre-trained on image-text pairs, are biased toward salient, object-centric regions and exhibit two critical limitations when adapted to segmentation: (i) Foreground Bias, which tends to ignore background regions, and (ii) Limited Spatial Localization, resulting in blurred object boundaries. To address these limitations, we introduce DiSa, a novel saliency-aware foreground-background disentangled framework. By explicitly incorporating saliency cues in our designed Saliency-aware Disentanglement Module (SDM), DiSa separately models foreground and background ensemble features in a divide-and-conquer manner. Additionally, we propose a Hierarchical Refinement Module (HRM) that leverages pixel-wise spatial contexts and enables channel-wise feature refinement through multi-level updates. Extensive experiments on six benchmarks demonstrate that DiSa consistently outperforms state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiSa：面向开放词汇语义分割的显著性感知前景-背景解耦框架</div>
<div class="mono" style="margin-top:8px">开放词汇语义分割旨在根据文本标签为图像中的每个像素分配类别。现有方法通常利用视觉-语言模型（如CLIP）进行密集预测。然而，基于图像-文本对预训练的VLM偏向于显著的物体中心区域，在适应分割任务时存在两个关键局限：（i）前景偏差，易忽略背景区域；（ii）空间定位局限，导致物体边界模糊。为应对这些问题，本文提出DiSa——一种新颖的显著性感知前景-背景解耦框架。通过在设计好的显著性感知解耦模块中显式引入显著性线索，DiSa以分治策略分别建模前景与背景的集成特征。此外，我们提出层级优化模块，该模块利用像素级空间上下文，通过多级更新实现通道级特征优化。在六个基准数据集上的大量实验表明，DiSa持续优于现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the foreground bias and limited spatial localization of vision-language models like CLIP when adapted for open-vocabulary semantic segmentation, where they often ignore backgrounds and produce blurred object boundaries. The proposed DiSa framework introduces a Saliency-aware Disentanglement Module to separately model foreground and background features using saliency cues, and a Hierarchical Refinement Module for multi-level spatial and channel-wise feature enhancement. Experimental results on six benchmarks show that DiSa consistently outperforms existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（如CLIP）在适应开放词汇语义分割任务时存在的前景偏见和空间定位能力有限的问题，即模型倾向于忽略背景区域并产生模糊的对象边界。提出的DiSa框架引入了一个显著性感知解耦模块，利用显著性线索分别建模前景和背景特征，并配合一个分层细化模块进行多级别的空间和通道特征增强。在六个基准测试上的实验结果表明，DiSa consistently outperforms existing state-of-the-art methods。</div>
</details>
</div>
<div class="card">
<div class="title">Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</div>
<div class="meta-line">Authors: Kevin Robbins, Xiaotong Liu, Yu Wu, Le Sun, Grady McPeak, Abby Stylianou, Robert Pless</div>
<div class="meta-line">First: 2026-01-24T17:30:23+00:00 · Latest: 2026-01-27T18:04:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17535v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17535v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>它会零样本吗？：预测任意查询的零样本分类性能</div>
<div class="mono" style="margin-top:8px">像CLIP这样的视觉-语言模型为文本和图像创建了对齐的嵌入空间，使得任何人都能通过简单命名想要区分的类别来构建视觉分类器。然而，在一个领域表现良好的模型可能在另一个领域失效，非专业用户没有直接的方法来评估所选VLM是否适用于其问题。我们基于先前仅使用文本比较来评估模型在给定自然语言任务中表现的工作，探索了同时生成与该任务相关的合成图像以评估和优化零样本准确率预测的方法。研究表明，与仅使用文本的基线评分相比，生成的图像显著提升了预测质量。此外，该方法还为用户提供了用于评估的图像类型反馈。在标准CLIP基准数据集上的实验证明，这种基于图像的方法能帮助用户在没有标注样本的情况下，预测VLM是否适用于其应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge for non-expert users in predicting whether a Vision-Language Model (VLM) like CLIP will perform well for their specific zero-shot classification task, as model effectiveness varies across domains. The method improves upon prior text-only evaluation by generating synthetic images relevant to the user&#x27;s query and using these images alongside text comparisons to assess and refine the prediction of zero-shot accuracy. Experimental results on standard CLIP benchmarks show that incorporating generated imagery substantially improves prediction quality over text-only baselines and provides users with interpretable feedback on the image types used for assessment.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决非专家用户难以评估视觉语言模型（如CLIP）在其特定零样本分类任务上性能的挑战。该方法改进了纯文本评估，通过生成与用户查询相关的合成图像，并结合文本比较来预测模型性能。在标准CLIP基准数据集上的实验结果表明，引入生成图像显著提高了预测准确性，优于纯文本基线，并为用户提供了评估依据的视觉反馈。</div>
</details>
</div>
<div class="card">
<div class="title">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</div>
<div class="meta-line">Authors: Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T17:58:12+00:00 · Latest: 2026-01-27T17:58:12+00:00</div>
<div class="meta-line">Comments: Accepted in ICLR 2026, Codebase: https://github.com/Nicous20/EgoHandICL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19850v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19850v1">PDF</a> · <a href="https://github.com/Nicous20/EgoHandICL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoHandICL：基于上下文学习的以自我为中心视角三维手部重建</div>
<div class="mono" style="margin-top:8px">在以自我为中心的视觉中，由于深度模糊性、自遮挡及复杂的手-物交互作用，实现鲁棒的三维手部重建具有挑战性。现有方法通过扩充训练数据或添加辅助线索来缓解这些问题，但在未见场景中往往表现不佳。我们提出EgoHandICL，这是首个用于三维手部重建的上下文学习框架，能在挑战性的以自我为中心条件下提升语义对齐性、视觉一致性和鲁棒性。EgoHandICL引入了基于视觉-语言模型的互补范例检索机制、针对多模态上下文定制的ICL分词器，以及采用手部引导几何与感知目标训练的掩码自编码器架构。在ARCTIC和EgoExo4D数据集上的实验表明，本方法持续优于现有最优方法。我们还展示了其在真实场景中的泛化能力，并通过将重建手部作为视觉提示，提升了EgoVLM的手-物交互推理性能。代码与数据：https://github.com/Nicous20/EgoHandICL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of robust 3D hand reconstruction in egocentric vision, where depth ambiguity, self-occlusion, and complex hand-object interactions hinder performance. The proposed EgoHandICL framework introduces an in-context learning approach featuring complementary exemplar retrieval guided by vision-language models, a specialized tokenizer for multimodal context, and a masked autoencoder architecture trained with hand-specific geometric and perceptual objectives. Experimental results on ARCTIC and EgoExo4D datasets demonstrate consistent improvements over state-of-the-art methods, with additional validation showing enhanced real-world generalization and improved hand-object interaction reasoning in EgoVLM when using reconstructed hands as visual prompts.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决以自我为中心视角下进行鲁棒3D手部重建的难题，该任务面临深度模糊、自遮挡和复杂手物交互等挑战。提出的EgoHandICL框架采用上下文学习方法，其核心包括一个由视觉语言模型引导的互补范例检索机制、一个为多模态上下文定制的分词器，以及一个基于掩码自编码器并辅以手部几何与感知目标训练的架构。在ARCTIC和EgoExo4D数据集上的实验表明，该方法相比现有先进技术取得了持续的性能提升，并且重建的手部模型被证明能有效增强视觉语言模型在手物交互推理中的能力。</div>
</details>
</div>
<div class="card">
<div class="title">ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</div>
<div class="meta-line">Authors: Abhijit Mishra, Mingda Li, Hsiang Fu, Richard Noh, Minji Kim</div>
<div class="meta-line">First: 2025-02-20T18:01:41+00:00 · Latest: 2026-01-27T17:16:10+00:00</div>
<div class="meta-line">Comments: In Proceedings of the IJCNLP-AACL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14780v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.14780v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (&lt;500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReVision：面向隐私保护任务型视觉指令重写的数据集与基线视觉语言模型</div>
<div class="mono" style="margin-top:8px">随着AR、VR及配备高性能摄像头的现代智能手机成为人机交互的主要界面，高效且保护隐私的多模态交互变得至关重要。现有支持多模态交互的大型视觉语言模型通常依赖云端处理，引发两大核心问题：（1）向服务器传输敏感视觉数据带来的隐私风险；（2）模型难以在设备端实现实时运行。本文提出视觉指令重写这一创新方法，将多模态指令转化为纯文本命令，使轻量级设备端指令重写模型（2.5亿参数）能够与现有对话式AI系统无缝集成，从而增强视觉数据隐私保护。为此，我们构建了涵盖14个领域、超过3.9万条样本的数据集，并开发了紧凑型视觉语言模型——该模型基于图像描述数据集预训练，并针对指令重写任务进行微调。通过BLEU、METEOR、ROUGE等自然语言生成指标及语义解析分析，实验结果表明：即使量化后存储占用低于500MB的模型版本，仍能实现高效的指令重写，为注重隐私的多模态AI应用提供了可行方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses privacy and efficiency concerns in multimodal AI systems, where cloud-based large vision-language models (VLMs) risk exposing sensitive visual data and lack real-time on-device usability. The authors propose Visual Instruction Rewriting, a method that converts multimodal instructions into text-only commands using a compact, on-device VLM with 250M parameters, pretrained on image captioning data and fine-tuned for rewriting. Experiments across 14 domains with over 39,000 examples show that even a quantized model (&lt;500MB) achieves effective rewriting as measured by BLEU, METEOR, ROUGE, and semantic parsing, enabling privacy-preserving multimodal applications.</div>
<div class="mono" style="margin-top:8px">该研究针对多模态交互中的隐私和效率问题，其中基于云的大型视觉语言模型（VLM）存在暴露敏感视觉数据且缺乏实时设备端可用性的风险。作者提出视觉指令重写方法，通过一个紧凑的设备端VLM（2.5亿参数）将多模态指令转换为纯文本命令，该模型在图像描述数据上预训练并针对此任务微调。在涵盖14个领域、超过39,000个样本的数据集上的实验表明，即使量化模型（&lt;500MB）也能通过NLG指标（BLEU、METEOR、ROUGE）和语义解析评估实现有效的指令重写，从而支持隐私保护的多模态AI应用。</div>
</details>
</div>
<div class="card">
<div class="title">Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision</div>
<div class="meta-line">Authors: Zhixiang Wei, Yi Li, Zhehan Kan, Xinghua Jiang, Zuwei Long, Shifeng Liu, Hongze Shen, Wei Liu, Xiaoyu Tan, Haojia Lin, Yubo Zhu, Qianyu Li, Di Yin, Haoyu Cao, Weibo Gu, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, Yunsheng Wu, Mingkong Tang, Shuangyin Liu, Lexiang Tang, Haodong Lin, Junru Lu, Jiarui Qin, Lingfeng Qiao, Ruizhi Qiao, Bo Ke, Jianfeng He, Ke Li, Yangning Li, Yunhang Shen, Mengdan Zhang, Peixian Chen, Kun Yin, Bing Liu, Yunfei Wu, Huang Chen, Zhongpeng Cai, Xiaotian Li</div>
<div class="meta-line">First: 2026-01-27T17:01:16+00:00 · Latest: 2026-01-27T17:01:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19798v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19798v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input&#x27;&#x27; to ``vision-as-target.&#x27;&#x27; By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Youtu-VL：通过统一视觉-语言监督释放视觉潜能</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言模型（VLMs）已取得显著进展，但当前架构在保留细粒度视觉信息方面常显不足，导致多模态理解停留在粗粒度层面。我们将此缺陷归因于主流VLM训练范式的固有局限——其存在文本主导的优化偏差，仅将视觉信号视为被动条件输入而非监督目标。为此，我们提出Youtu-VL框架，采用视觉-语言统一自回归监督（VLUAS）范式，将优化目标从“视觉作为输入”根本性转向“视觉作为目标”。通过将视觉标记直接整合至预测流，Youtu-VL对视觉细节与语言内容实施统一自回归监督。此外，我们将该范式扩展至视觉中心任务，使标准VLM无需任务特定适配即可执行此类任务。大量实证评估表明，Youtu-VL在通用多模态任务与视觉中心任务上均取得优异性能，为开发全面通用视觉智能体奠定了坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of current Vision-Language Models (VLMs) in retaining fine-grained visual information, which stems from a text-dominant training paradigm that treats visual signals as passive inputs rather than supervisory targets. To overcome this, the authors propose Youtu-VL, a framework based on a Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm that shifts the optimization objective to treat vision as a target, integrating visual tokens directly into the prediction stream for unified autoregressive supervision over both visual and linguistic content. Experimental results show that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, providing a robust foundation for comprehensive generalist visual agents.</div>
<div class="mono" style="margin-top:8px">当前的视觉语言模型常因文本主导的训练范式而丢失细粒度视觉信息，该范式将视觉信号视为被动输入而非学习目标。为此，研究者提出了Youtu-VL框架，采用视觉语言统一自回归监督范式，将优化目标从“视觉作为输入”转变为“视觉作为目标”，对视觉细节和语言内容进行统一的自回归监督。广泛的实验评估表明，Youtu-VL在通用多模态任务和以视觉为中心的任务上均取得了有竞争力的性能，为开发全面的通用视觉智能体奠定了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models</div>
<div class="meta-line">Authors: Gyubeum Lim, Yemo Koo, Vijay Krishna Madisetti</div>
<div class="meta-line">First: 2025-10-22T17:47:12+00:00 · Latest: 2026-01-27T16:39:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21850v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to bridge the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCoPE VLM：基于选择性上下文处理的高效文档导航视觉语言模型</div>
<div class="mono" style="margin-top:8px">理解长上下文视觉信息仍是视觉语言模型面临的核心挑战，尤其在GUI控制和网页导航等代理任务中。虽然网页和GUI环境本质上是结构化文档，但现有VLMs的训练目标通常忽略了面向决策的文档理解。当前方法主要通过扩展视觉嵌入来处理长序列高分辨率输入，但这些方案内存消耗大，难以在本地部署。为此，我们提出SCoPE VLM——一种采用新型滚动链机制的选择性递归文档导航专家模型，能精准聚焦相关段落。我们构建了专门的数据生成流程来构建信息化的滚动链轨迹，并提出定制强化学习方法“情景组相对策略优化”以弥合训练与推理的差距。该方法显著降低了内存占用，并有效模拟了类人阅读行为。据我们所知，SCoPE VLM是首个在多页文档问答中显式建模代理阅读模式的框架，推动了多模态智能体的能力边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling vision-language models to efficiently process long-context visual documents for agentic tasks like web navigation, where current methods are memory-intensive and lack training for decision-oriented understanding. The proposed SCoPE VLM introduces a Chain of Scroll mechanism for selective, recursive navigation of document segments, supported by a data generation pipeline for training trajectories and a tailored reinforcement learning method called Episodic Group Relative Policy Optimization to align training with inference. Experimental results show the method substantially reduces memory usage, effectively models human-like reading behaviors, and advances performance in multi-page document question answering as the first framework to explicitly model such agentic reading patterns.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决视觉语言模型在处理长上下文视觉文档（如网页导航等代理任务）时效率低下的挑战，现有方法通常内存消耗大且缺乏面向决策的理解训练。提出的SCoPE VLM采用了一种新颖的链式滚动机制，选择性地递归导航文档片段，并通过专门的数据生成管道构建训练轨迹，以及定制化的强化学习方法——情景组相对策略优化，以弥合训练与推理之间的差距。实验结果表明，该方法显著降低了内存使用，有效模拟了类人的阅读行为，并在多页面文档问答任务中提升了性能，成为首个明确建模此类代理阅读模式的框架。</div>
</details>
</div>
<div class="card">
<div class="title">KeepLoRA: Continual Learning with Residual Gradient Adaptation</div>
<div class="meta-line">Authors: Mao-Lin Luo, Zi-Hao Zhou, Yi-Lin Zhang, Yuanyu Wan, Tong Wei, Min-Ling Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T14:38:57+00:00 · Latest: 2026-01-27T14:38:57+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19659v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19659v1">PDF</a> · <a href="https://github.com/MaolinLuo/KeepLoRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KeepLoRA：基于残差梯度适应的持续学习方法</div>
<div class="mono" style="margin-top:8px">预训练视觉语言模型的持续学习需要平衡三个相互竞争的目标：保留预训练知识、维护已学习任务序列的知识、保持获取新知识的可塑性。本文提出一种简洁高效的KeepLoRA方法以有效平衡这些目标。我们首先分析模型参数空间中的知识保留机制，发现通用知识主要编码在主成分子空间，而任务特定知识编码在残差子空间。基于此发现，KeepLoRA通过将LoRA参数更新限制在残差子空间来学习新任务，避免干扰已习得能力。具体而言，我们将新任务的梯度投影到与预训练模型主成分子空间及先前任务特征主导方向均正交的子空间来实现知识注入。理论与实验分析证实，KeepLoRA能平衡三个目标并取得最先进的性能。实现代码发布于https://github.com/MaolinLuo/KeepLoRA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge in continual learning for pre-trained vision-language models of balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from previously learned tasks, and maintaining plasticity for new tasks. The proposed method, KeepLoRA, is motivated by an analysis showing that general knowledge resides in the principal parameter subspace while task-specific knowledge is encoded in the residual subspace. To learn new tasks without interference, KeepLoRA restricts LoRA parameter updates by projecting the task&#x27;s gradient onto a subspace orthogonal to both the pre-trained model&#x27;s principal subspace and the dominant directions of previous task features. Experimental results confirm that this approach effectively balances the three objectives and achieves state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型持续学习中的核心挑战，即平衡三个相互竞争的目标：保留预训练知识、维持已学任务的知识以及保持学习新任务的可塑性。该方法名为KeepLoRA，其动机源于一项分析发现：通用知识主要编码在参数的主子空间中，而任务特定知识则编码在残差子空间中。该方法通过将LoRA参数更新限制在残差子空间中来学习新任务，具体做法是将新任务的梯度投影到与预训练模型主子空间及先前任务特征主方向均正交的子空间上。实验结果表明，该方法有效平衡了三个目标，并取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Detect Unseen Jailbreak Attacks in Large Vision-Language Models</div>
<div class="meta-line">Authors: Shuang Liang, Zhihao Xu, Jiaqi Weng, Jialing Tao, Hui Xue, Xiting Wang</div>
<div class="meta-line">First: 2025-08-08T16:13:28+00:00 · Latest: 2026-01-27T13:58:13+00:00</div>
<div class="meta-line">Comments: 12 pages; Previously this version appeared as arXiv:2510.15430 which was submitted as a new work by accident</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09201v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.09201v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks. To mitigate these risks, existing detection methods are essential, yet they face two major challenges: generalization and accuracy. While learning-based methods trained on specific attacks fail to generalize to unseen attacks, learning-free methods based on hand-crafted heuristics suffer from limited accuracy and reduced efficiency. To address these limitations, we propose Learning to Detect (LoD), a learnable framework that eliminates the need for any attack data or hand-crafted heuristics. LoD operates by first extracting layer-wise safety representations directly from the model&#x27;s internal activations using Multi-modal Safety Concept Activation Vectors classifiers, and then converting the high-dimensional representations into a one-dimensional anomaly score for detection via a Safety Pattern Auto-Encoder. Extensive experiments demonstrate that LoD consistently achieves state-of-the-art detection performance (AUROC) across diverse unseen jailbreak attacks on multiple LVLMs, while also significantly improving efficiency. Code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习检测大型视觉语言模型中的未见越狱攻击</div>
<div class="mono" style="margin-top:8px">尽管进行了广泛的模型对齐，大型视觉语言模型（LVLMs）仍易受越狱攻击。现有检测方法虽对缓解风险至关重要，但面临泛化性与准确性两大挑战：基于特定攻击训练的检测方法难以泛化至未见攻击，而基于人工启发式的无学习方法则存在准确性不足与效率低下的问题。为此，我们提出“学习检测”（LoD）框架，该可学习框架无需任何攻击数据或人工启发式设计。LoD首先通过多模态安全概念激活向量分类器直接从模型内部激活中提取层级安全表征，再通过安全模式自编码器将高维表征转换为一维异常分数以进行检测。大量实验表明，LoD在多种LVLMs上针对不同未见越狱攻击均能持续取得最先进的检测性能（AUROC），同时显著提升效率。代码发布于 https://anonymous.4open.science/r/Learning-to-Detect-51CB。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing jailbreak detection methods for Large Vision-Language Models (LVLMs), which struggle with generalization to unseen attacks and accuracy-efficiency trade-offs, this work introduces Learning to Detect (LoD). The method extracts layer-wise safety representations from the model&#x27;s internal activations using Multi-modal Safety Concept Activation Vectors and then compresses them into a one-dimensional anomaly score via a Safety Pattern Auto-Encoder for detection, requiring no prior attack data or hand-crafted heuristics. Experiments show that LoD achieves state-of-the-art detection performance (AUROC) across diverse unseen jailbreak attacks on multiple LVLMs while also significantly improving efficiency.</div>
<div class="mono" style="margin-top:8px">针对现有大型视觉语言模型（LVLM）越狱攻击检测方法在泛化性和准确性上的不足，本研究提出了一个名为“学习检测”（LoD）的可学习框架。该方法通过多模态安全概念激活向量从模型内部激活中提取分层安全表征，然后利用安全模式自编码器将这些高维表征压缩为一维异常分数进行检测，无需先验攻击数据或人工设计的启发式规则。大量实验表明，LoD在多个LVLM上针对各种未见越狱攻击，持续实现了最先进的检测性能（AUROC），同时显著提升了效率。</div>
</details>
</div>
<div class="card">
<div class="title">ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving</div>
<div class="meta-line">Authors: Yujin Wang, Yutong Zheng, Wenxian Fan, Tianyi Wang, Hongqing Chu, Daxin Tian, Bingzhao Gao, Jianqiang Wang, Hong Chen</div>
<div class="meta-line">First: 2026-01-27T13:17:50+00:00 · Latest: 2026-01-27T13:17:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19582v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19582v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScenePilot-Bench：面向自动驾驶的大规模视觉语言模型评估数据集与基准</div>
<div class="mono" style="margin-top:8px">本文提出ScenePilot-Bench，这是一个用于评估自动驾驶场景中视觉语言模型的大规模第一人称驾驶基准。该基准基于ScenePilot-4K数据集构建，该数据集包含3,847小时多样化驾驶视频，并标注了多粒度信息，包括场景描述、风险评估、关键参与者识别、自车轨迹和相机参数。基准采用四维评估体系，从场景理解、空间感知、运动规划和GPT-Score四个维度评估视觉语言模型能力，同时引入安全感知指标和跨区域泛化设置。我们在ScenePilot-Bench上对代表性视觉语言模型进行基准测试，通过实证分析明确了当前性能边界，并揭示了面向驾驶推理的不足。ScenePilot-Bench为安全关键型自动驾驶场景中的视觉语言模型评估与推进提供了完整框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for rigorous evaluation of vision-language models in safety-critical autonomous driving applications, this work introduces ScenePilot-Bench, a large-scale benchmark built upon the ScenePilot-4K dataset containing 3,847 hours of driving videos annotated with multi-granular information like scene descriptions and risk assessments. The method employs a four-axis evaluation suite assessing scene understanding, spatial perception, motion planning, and GPT-Score, incorporating safety-aware metrics and cross-region generalization tests. Experimental benchmarking of representative VLMs reveals current performance boundaries and identifies specific gaps in driving-oriented reasoning, providing a comprehensive framework for advancing VLMs in this domain.</div>
<div class="mono" style="margin-top:8px">为在安全关键的自动驾驶场景中严格评估视觉语言模型，本研究提出了ScenePilot-Bench，一个大规模的第一人称驾驶基准。该方法基于ScenePilot-4K数据集构建基准，该数据集包含3,847小时的驾驶视频，并标注了多粒度信息；评估采用一个四轴测试套件，涵盖场景理解、空间感知、运动规划和GPT-Score，并包含安全感知指标。对代表性模型进行基准测试的主要实验结果表明了当前性能边界，并揭示了面向驾驶的推理能力存在的具体差距，为模型发展提供了一个全面的评估框架。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking</div>
<div class="meta-line">Authors: Wen Wen, Tianwu Zhi, Kanglong Fan, Yang Li, Xinge Peng, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-30T04:57:26+00:00 · Latest: 2026-01-27T11:48:21+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25787v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.25787v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improving vision-language models (VLMs) in the post-training stage typically relies on supervised fine-tuning or reinforcement learning, methods that necessitate costly, human-annotated data. While self-supervised techniques have proven effective for enhancing reasoning capabilities, their application to perceptual domains such as image quality assessment (IQA) remains largely unexplored. In this work, we introduce EvoQuality, a novel framework that enables a VLM to autonomously refine its quality perception capabilities without any ground-truth labels. EvoQuality adapts the principle of self-consistency to the ranking-based nature of IQA. It generates pseudo-labels by performing pairwise majority voting on the VLM&#x27;s own outputs to establish a consensus on relative quality. These pseudo-rankings are then formulated into a fidelity reward that guides the model&#x27;s iterative evolution through group relative policy optimization (GRPO). By iteratively leveraging its own predictions, EvoQuality progressively refines the VLM&#x27;s perceptual capability. Extensive experiments show that EvoQuality boosts the base VLM&#x27;s zero-shot performance by 31.8% on PLCC across diverse IQA benchmarks. Remarkably, despite being entirely self-supervised, EvoQuality achieves performance that is competitive with, or even surpasses, state-of-the-art supervised VLM-based IQA models, outperforming these models on 5 out of 7 IQA benchmarks. Furthermore, the framework demonstrates significant flexibility, allowing it to be stacked with pre-trained IQA models to bolster generalization on unseen datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于投票与排序的自演进视觉语言模型在图像质量评估中的应用</div>
<div class="mono" style="margin-top:8px">后训练阶段提升视觉语言模型（VLM）通常依赖监督微调或强化学习，这些方法需要昂贵的人工标注数据。虽然自监督技术在增强推理能力方面已证明有效，但其在图像质量评估（IQA）等感知领域的应用仍鲜有探索。本研究提出EvoQuality框架，使VLM能在无真实标签条件下自主优化其质量感知能力。该框架将自一致性原理适配于IQA的排序特性，通过对VLM自身输出进行成对多数投票生成伪标签以建立相对质量共识，进而将伪排序转化为保真度奖励，通过组相对策略优化（GRPO）指导模型迭代演进。实验表明：EvoQuality将基础VLM在多个IQA基准上的零样本PLCC性能提升31.8%；尽管完全自监督，其性能仍可媲美甚至超越基于VLM的监督式SOTA模型，在7个基准中的5个表现更优；该框架还展现出与预训练IQA模型堆叠以增强泛化能力的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance vision-language models (VLMs) for image quality assessment (IQA) without costly human annotations, this research introduces EvoQuality, a self-supervised framework. The method adapts self-consistency to IQA by generating pseudo-labels through pairwise majority voting on the VLM&#x27;s own outputs to establish relative quality rankings, which are then used as a fidelity reward to iteratively optimize the model via group relative policy optimization (GRPO). Experimental results demonstrate that EvoQuality improves the base VLM&#x27;s zero-shot performance by 31.8% in PLCC across benchmarks, matches or surpasses supervised state-of-the-art VLM-based IQA models on 5 out of 7 benchmarks, and shows flexibility by stacking with pre-trained IQA models to improve generalization.</div>
<div class="mono" style="margin-top:8px">为了在无需昂贵人工标注的情况下提升视觉语言模型（VLM）的图像质量评估（IQA）能力，本研究提出了自监督框架EvoQuality。该方法将自一致性原理应用于IQA，通过对VLM自身输出进行成对多数投票来生成伪标签以建立相对质量排序，随后将这些排序作为保真度奖励，通过组相对策略优化（GRPO）迭代优化模型。实验表明，EvoQuality将基础VLM在多个IQA基准上的零样本性能提升了31.8%（PLCC），在7个基准中的5个上达到甚至超越了监督式VLM基IQA模型的性能，并展现出与预训练模型堆叠以提升泛化能力的灵活性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
