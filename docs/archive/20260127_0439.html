<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-27 04:39</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260127_0439</div>
    <div class="row"><div class="card">
<div class="title">VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents</div>
<div class="meta-line">Authors: Zirui Wang, Junyi Zhang, Jiaxin Ge, Long Lian, Letian Fu, Lisa Dunlap, Ken Goldberg, XuDong Wang, Ion Stoica, David M. Chan, Sewon Min, Joseph E. Gonzalez</div>
<div class="meta-line">First: 2026-01-23T18:43:34+00:00 · Latest: 2026-01-23T18:43:34+00:00</div>
<div class="meta-line">Comments: Project page: https://visgym.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16973v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://visgym.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisGym：面向多模态智能体的多样化、可定制、可扩展环境套件</div>
<div class="mono" style="margin-top:8px">当前视觉语言模型在多步骤视觉交互任务中的表现仍缺乏系统评估，尤其在感知、记忆与动作的长时程整合机制方面。本研究推出VisGym——包含17个环境的评估训练平台，涵盖符号推理、实景图像理解、导航与操作四大类任务，支持对任务难度、输入表征、规划时域及反馈机制的灵活调控。平台同时提供可生成结构化示范的多步骤求解器，支持监督微调。评估显示：前沿模型在交互场景中普遍表现欠佳，简单配置与困难配置下的成功率分别仅为46.6%与26.0%。实验揭示关键局限：模型难以有效利用长时上下文，无界历史窗口的表现反逊于截断窗口；多项文本符号任务在视觉化呈现后难度显著提升。研究发现，在部分可观测或动态未知场景中，显式目标观察、文本反馈及探索性示范能通过监督微调带来稳定增益，这为改进多步骤视觉决策提供了明确的失效模式分析与优化路径。代码、数据与模型详见：https://visgym.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of comprehensive evaluation for Vision-Language Models (VLMs) in multi-step visual interaction tasks that require integrating perception, memory, and action over time, this work introduces VisGym, a suite of 17 diverse and customizable environments. The method provides a gymnasium for assessment and training, spanning symbolic puzzles to real-image tasks, with controls for difficulty and input representation, and includes multi-step solvers to generate structured demonstrations for supervised fine-tuning. Experimental results reveal that current frontier models struggle significantly in interactive settings, achieving only 46.6% and 26.0% success rates on easy and hard configurations, respectively, and exhibit specific limitations such as ineffective use of long context and increased difficulty when tasks are rendered visually. However, improvements were observed with explicit goal observations, textual feedback, and exploratory demonstrations, identifying concrete failure modes and potential pathways for enhancing multi-step visual decision-making.</div>
<div class="mono" style="margin-top:8px">针对当前缺乏评估视觉语言模型在多步视觉决策中综合能力的基准，本研究提出了VisGym，这是一个包含17个多样化、可定制环境的测试平台，涵盖符号谜题、真实图像理解、导航和操作任务。该方法提供了对任务参数的灵活控制，并包含多步求解器以生成用于监督微调的结构化演示。实验结果表明，前沿视觉语言模型在交互式环境中表现显著不足，在简单和困难配置下的成功率分别仅为46.6%和26.0%，主要局限包括对长上下文利用不佳，以及任务从文本形式转换为视觉呈现后难度大幅增加。然而，通过提供明确的目标观察、文本反馈和在部分可观测或未知动态设置中的探索性演示，模型性能得到了稳定提升，从而揭示了具体的失败模式和潜在的改进路径。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Large Vision-language Models for Surgical Tool Detection</div>
<div class="meta-line">Authors: Nakul Poudel, Richard Simon, Cristian A. Linte</div>
<div class="meta-line">First: 2026-01-23T17:00:46+00:00 · Latest: 2026-01-23T17:00:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the interrelated components of surgical scenes. Recent advances in large vision-language models that integrate multimodal data processing offer strong potential for modeling surgical tasks and providing human-like scene reasoning and understanding. Despite their promise, systematic investigations of VLMs in surgical applications remain limited. In this study, we evaluate the effectiveness of large VLMs for the fundamental surgical vision task of detecting surgical tools. Specifically, we investigate three state-of-the-art VLMs, Qwen2.5, LLaVA1.5, and InternVL3.5, on the GraSP robotic surgery dataset under both zero-shot and parameter-efficient LoRA fine-tuning settings. Our results demonstrate that Qwen2.5 consistently achieves superior detection performance in both configurations among the evaluated VLMs. Furthermore, compared with the open-set detection baseline Grounding DINO, Qwen2.5 exhibits stronger zero-shot generalization and comparable fine-tuned performance. Notably, Qwen2.5 shows superior instrument recognition, while Grounding DINO demonstrates stronger localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估大型视觉语言模型在外科手术工具检测中的应用</div>
<div class="mono" style="margin-top:8px">外科手术是高度复杂的过程，人工智能已成为支持手术引导与决策的关键变革力量。然而，当前多数AI系统的单模态特性限制了其对手术流程的整体理解能力，这凸显了对能够全面建模手术场景关联组件的通用手术AI系统的需求。近期整合多模态数据处理的大型视觉语言模型进展，为建模手术任务及提供类人场景推理与理解展现出巨大潜力。尽管前景广阔，针对VLM在手术应用中的系统性研究仍显不足。本研究评估了大型VLM在基础手术视觉任务——手术工具检测中的有效性。具体而言，我们在GraSP机器人手术数据集上，以零样本和参数高效LoRA微调两种设置，测试了Qwen2.5、LLaVA1.5和InternVL3.5三种前沿VLM。结果表明，在评估的VLM中，Qwen2.5在两种配置下均表现出最优检测性能。与开放集检测基线Grounding DINO相比，Qwen2.5展现出更强的零样本泛化能力和相当的微调性能。值得注意的是，Qwen2.5在器械识别方面更优，而Grounding DINO在定位能力上更强。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for general-purpose surgical AI systems that can holistically model surgical workflows beyond unimodal approaches, this study evaluates the effectiveness of large vision-language models (VLMs) for surgical tool detection. The method involves systematically testing three state-of-the-art VLMs—Qwen2.5, LLaVA1.5, and InternVL3.5—on the GraSP robotic surgery dataset under zero-shot and parameter-efficient LoRA fine-tuning settings. Key experimental findings show that Qwen2.5 consistently achieves superior detection performance in both configurations, exhibits stronger zero-shot generalization than the open-set detection baseline Grounding DINO, and demonstrates comparable fine-tuned performance, with a noted strength in instrument recognition while Grounding DINO excels in localization.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决当前大多数外科人工智能系统单模态性质的局限性，推动能够全面建模手术场景的通用外科AI系统的发展。方法上，系统评估了三种先进的大规模视觉语言模型（Qwen2.5、LLaVA1.5和InternVL3.5）在GraSP机器人手术数据集上的表现，包括零样本学习和参数高效的LoRA微调设置。主要实验结果表明，Qwen2.5在两种配置下均表现出最优的检测性能，相比开放集检测基线Grounding DINO具有更强的零样本泛化能力，且微调后性能相当，其中Qwen2.5在器械识别方面更优，而Grounding DINO在定位方面更强。</div>
</details>
</div>
<div class="card">
<div class="title">A Multi-Stage Hybrid Framework for Automated Interpretation of Multi-View Engineering Drawings Using Vision Language Model</div>
<div class="meta-line">Authors: Muhammad Tayyab Khan, Zane Yong, Lequn Chen, Wenhe Feng, Nicholas Yew Jin Tan, Seung Ki Moon</div>
<div class="meta-line">First: 2025-10-23T09:07:31+00:00 · Latest: 2026-01-23T11:39:05+00:00</div>
<div class="meta-line">Comments: This draft has been accepted in the 13th International Conference on Industrial Engineering and Applications (ICIEA 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21862v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21862v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Engineering drawings are fundamental to manufacturing communication, serving as the primary medium for conveying design intent, tolerances, and production details. However, interpreting complex multi-view drawings with dense annotations remains challenging using manual methods, generic optical character recognition (OCR) systems, or traditional deep learning approaches, due to varied layouts, orientations, and mixed symbolic-textual content. To address these challenges, this paper proposes a three-stage hybrid framework for the automated interpretation of 2D multi-view engineering drawings using modern detection and vision language models (VLMs). In the first stage, YOLOv11-det performs layout segmentation to localize key regions such as views, title blocks, and notes. The second stage uses YOLOv11-obb for orientation-aware, fine-grained detection of annotations, including measures, GD&amp;T symbols, and surface roughness indicators. The third stage employs two Donut-based, OCR-free VLMs for semantic content parsing: the Alphabetical VLM extracts textual and categorical information from title blocks and notes, while the Numerical VLM interprets quantitative data such as measures, GD&amp;T frames, and surface roughness. Two specialized datasets were developed to ensure robustness and generalization: 1,000 drawings for layout detection and 1,406 for annotation-level training. The Alphabetical VLM achieved an overall F1 score of 0.672, while the Numerical VLM reached 0.963, demonstrating strong performance in textual and quantitative interpretation, respectively. The unified JSON output enables seamless integration with CAD and manufacturing databases, providing a scalable solution for intelligent engineering drawing analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的多视图工程图自动解析多阶段混合框架</div>
<div class="mono" style="margin-top:8px">工程图纸是制造沟通的基础载体，用于传达设计意图、公差与生产细节。然而，面对布局多变、方向各异且符号-文本混合的复杂多视图图纸，传统人工方法、通用光学字符识别系统或经典深度学习方案均存在解析瓶颈。为此，本文提出一种融合现代检测技术与视觉语言模型的三阶段混合框架，用于二维多视图工程图的自动解析。第一阶段采用YOLOv11-det进行布局分割，定位视图区、标题栏及注释区等关键区域。第二阶段使用YOLOv11-obb实现面向标注的细粒度方向感知检测，涵盖尺寸标注、几何公差符号及表面粗糙度标识。第三阶段部署两个基于Donut架构的无OCR视觉语言模型进行语义解析：字母型VLM从标题栏和注释中提取文本与分类信息，数值型VLM则解析尺寸、几何公差框及表面粗糙度等量化数据。为保障模型鲁棒性与泛化能力，构建了两个专项数据集：包含1,000张图纸的布局检测数据集和1,406张标注级训练数据集。字母型VLM综合F1分数达0.672，数值型VLM达0.963，分别在文本与量化解析任务中表现优异。该框架输出的标准化JSON数据可实现与CAD及制造数据库的无缝对接，为智能工程图分析提供可扩展解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the challenges of manually interpreting complex multi-view engineering drawings with dense, mixed symbolic-textual content, this paper proposes a three-stage hybrid framework using vision language models (VLMs). The method first segments the drawing layout with YOLOv11-det, then performs fine-grained, orientation-aware detection of annotations with YOLOv11-obb, and finally parses semantic content using two specialized, OCR-free Donut-based VLMs for textual and numerical data. Experimental results on two custom datasets show the Alphabetical VLM achieved an F1 score of 0.672 for textual information, while the Numerical VLM reached 0.963 for quantitative data, demonstrating effective automated interpretation and generating unified JSON output for CAD integration.</div>
<div class="mono" style="margin-top:8px">为克服手动方法和通用OCR在解释布局多变、图文混合的复杂多视图工程图方面的局限，本文提出了一种利用现代检测和视觉语言模型的三阶段混合框架。该方法依次使用YOLOv11-det进行布局分割，YOLOv11-obb进行细粒度标注检测，以及两个基于Donut的VLM分别解析文本和数值语义内容。在专用数据集上的实验结果表明，字母VLM在文本提取上的整体F1分数达到0.672，而数字VLM在定量数据解释上达到0.963，证明了其有效的自动化分析能力，并生成结构化JSON输出以便与制造系统集成。</div>
</details>
</div>
<div class="card">
<div class="title">X-Aligner: Composed Visual Retrieval without the Bells and Whistles</div>
<div class="meta-line">Authors: Yuqian Zheng, Mariana-Iuliana Georgescu</div>
<div class="meta-line">First: 2026-01-23T09:33:38+00:00 · Latest: 2026-01-23T09:33:38+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16582v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16582v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Composed Video Retrieval (CoVR) facilitates video retrieval by combining visual and textual queries. However, existing CoVR frameworks typically fuse multimodal inputs in a single stage, achieving only marginal gains over initial baseline. To address this, we propose a novel CoVR framework that leverages the representational power of Vision Language Models (VLMs). Our framework incorporates a novel cross-attention module X-Aligner, composed of cross-attention layers that progressively fuse visual and textual inputs and align their multimodal representation with that of the target video. To further enhance the representation of the multimodal query, we incorporate the caption of the visual query as an additional input. The framework is trained in two stages to preserve the pretrained VLM representation. In the first stage, only the newly introduced module is trained, while in the second stage, the textual query encoder is also fine-tuned. We implement our framework on top of BLIP-family architecture, namely BLIP and BLIP-2, and train it on the Webvid-CoVR data set. In addition to in-domain evaluation on Webvid-CoVR-Test, we perform zero-shot evaluations on the Composed Image Retrieval (CIR) data sets CIRCO and Fashion-IQ. Our framework achieves state-of-the-art performance on CoVR obtaining a Recall@1 of 63.93% on Webvid-CoVR-Test, and demonstrates strong zero-shot generalization on CIR tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>X-Aligner：无需复杂设计的组合式视觉检索方法</div>
<div class="mono" style="margin-top:8px">组合式视频检索（CoVR）通过结合视觉与文本查询实现视频检索。现有CoVR框架通常采用单阶段多模态融合，仅能获得有限性能提升。为此，我们提出一种基于视觉语言模型（VLM）表征能力的新型CoVR框架。该框架引入创新的交叉注意力模块X-Aligner，通过多层交叉注意力逐步融合视觉与文本输入，并将其多模态表征与目标视频对齐。为增强多模态查询表征，我们额外引入视觉查询的标题作为输入。框架采用两阶段训练以保持预训练VLM表征：第一阶段仅训练新增模块，第二阶段同时微调文本查询编码器。我们在BLIP系列架构（BLIP与BLIP-2）上实现该框架，并使用Webvid-CoVR数据集进行训练。除在Webvid-CoVR-Test进行领域内评估外，还在组合式图像检索（CIR）数据集CIRCO与Fashion-IQ进行零样本评估。本框架在CoVR任务中取得最先进性能，在Webvid-CoVR-Test获得63.93%的Recall@1，并在CIR任务中展现出强大的零样本泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limited performance gains of existing Composed Video Retrieval (CoVR) frameworks, which typically fuse multimodal inputs in a single stage. The proposed method introduces a novel framework built on Vision Language Models (VLMs) like BLIP, featuring a cross-attention module called X-Aligner that progressively fuses visual and textual queries and aligns them with target video representations; it also incorporates visual query captions and employs a two-stage training strategy to preserve pretrained VLM knowledge. Experimental results show state-of-the-art performance, achieving 63.93% Recall@1 on the Webvid-CoVR-Test dataset and demonstrating strong zero-shot generalization on Composed Image Retrieval benchmarks CIRCO and Fashion-IQ.</div>
<div class="mono" style="margin-top:8px">本研究针对现有组合视频检索框架通常单阶段融合多模态输入、性能提升有限的问题，提出了一种基于视觉语言模型的新框架。该方法引入了名为X-Aligner的交叉注意力模块，逐步融合视觉与文本查询并使其与目标视频表示对齐，同时结合视觉查询的标题描述，并采用两阶段训练策略以保留预训练模型知识。实验结果表明，该方法在Webvid-CoVR-Test数据集上取得了63.93%的Recall@1最优性能，并在CIRCO和Fashion-IQ等组合图像检索任务上展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">AlphaFace: High Fidelity and Real-time Face Swapper Robust to Facial Pose</div>
<div class="meta-line">Authors: Jongmin Yu, Hyeontaek Oh, Zhongtian Sun, Angelica I Aviles-Rivero, Moongu Jeon, Jinhong Yang</div>
<div class="meta-line">First: 2026-01-23T04:01:49+00:00 · Latest: 2026-01-23T04:01:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16429v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16429v1">PDF</a> · <a href="https://github.com/andrewyu90/Alphaface_Official.git">Code1</a> · <a href="https://github.com/andrewyu90/Alphaface_Official.git&amp;#39">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing face-swapping methods often deliver competitive results in constrained settings but exhibit substantial quality degradation when handling extreme facial poses. To improve facial pose robustness, explicit geometric features are applied, but this approach remains problematic since it introduces additional dependencies and increases computational cost. Diffusion-based methods have achieved remarkable results; however, they are impractical for real-time processing. We introduce AlphaFace, which leverages an open-source vision-language model and CLIP image and text embeddings to apply novel visual and textual semantic contrastive losses. AlphaFace enables stronger identity representation and more precise attribute preservation, all while maintaining real-time performance. Comprehensive experiments across FF++, MPIE, and LPFF demonstrate that AlphaFace surpasses state-of-the-art methods in pose-challenging cases. The project is publicly available on `https://github.com/andrewyu90/Alphaface_Official.git&#x27;.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlphaFace：面向面部姿态的高保真实时人脸交换器</div>
<div class="mono" style="margin-top:8px">现有的人脸交换方法在受限场景下常能取得竞争性结果，但在处理极端面部姿态时会出现显著的质量下降。为提升面部姿态鲁棒性，现有方法采用显式几何特征，但这一方案仍存在问题，因其引入了额外依赖并增加了计算成本。基于扩散的方法已取得显著成果，但难以实现实时处理。本文提出AlphaFace，其利用开源视觉语言模型及CLIP图像与文本嵌入，应用新颖的视觉与文本语义对比损失。AlphaFace实现了更强的身份表征与更精确的属性保留，同时保持实时性能。在FF++、MPIE和LPFF数据集上的综合实验表明，AlphaFace在姿态挑战性场景中超越了现有最优方法。项目已公开于`https://github.com/andrewyu90/Alphaface_Official.git`。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing face-swapping methods struggle with extreme facial poses, often degrading quality or relying on computationally heavy geometric features, while diffusion models are too slow for real-time use. To address this, AlphaFace introduces a method that utilizes an open-source vision-language model and CLIP embeddings, applying novel visual and textual semantic contrastive losses to enhance identity representation and attribute preservation while maintaining real-time performance. Experiments on FF++, MPIE, and LPFF datasets show that AlphaFace outperforms state-of-the-art methods in pose-challenging scenarios.</div>
<div class="mono" style="margin-top:8px">现有的人脸交换方法在处理极端面部姿态时效果不佳，常导致质量下降或依赖计算量大的几何特征，而扩散模型则难以实时运行。为此，AlphaFace提出了一种方法，利用开源视觉语言模型和CLIP嵌入，通过新颖的视觉与文本语义对比损失来增强身份表征和属性保持，同时维持实时性能。在FF++、MPIE和LPFF数据集上的综合实验表明，AlphaFace在姿态挑战性场景中超越了现有最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-22T19:05:41+00:00</div>
<div class="meta-line">Comments: Work done as part of the EleutherAI SOAR Program</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型的空间盲区</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）发展迅速，但其捕捉空间关系的能力仍存在盲区。当前VLMs通常采用对比语言-图像预训练（CLIP）风格的图像编码器构建。其训练方案常将图像展平为一维补丁序列，丢弃了空间推理所需的二维结构。我们认为这种空间感知能力的缺失是VLM设计中的一个空白维度，也是机器人学与具身AI等需要空间基础的应用瓶颈。为此，我们研究了（i）采用替代目标训练的图像编码器与（ii）二维位置编码。实验表明，这些架构选择能在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models (VLMs) often struggle with spatial reasoning because their typical CLIP-style image encoders flatten images into 1D patch sequences, discarding crucial 2D structural information. To address this limitation, the study investigates alternative image encoder training objectives and the use of 2D positional encodings to better preserve spatial relationships. Experimental results demonstrate that these architectural modifications lead to measurable improvements in spatial reasoning performance across several benchmarks.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在空间关系理解上存在明显缺陷，因为它们通常采用CLIP风格的图像编码器，将图像展平为一维补丁序列，从而丢失了必要的二维结构信息。为解决这一问题，本研究探索了替代的图像编码器训练目标以及二维位置编码的集成，以更好地保留空间关系。实验结果表明，这些架构上的改进在多个基准测试中显著提升了模型的空间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：面向文档的具身视觉语言前端系统</div>
<div class="mono" style="margin-top:8px">GutenOCR是通过微调Qwen2.5-VL-3B与Qwen2.5-VL-7B构建的具身OCR前端系列模型。该单检查点视觉语言模型通过统一的提示词接口实现文本识别、检测与定位功能。基于商业文档、科学文献及合成定位数据训练，模型支持整页与局部阅读，可输出行级/段落级边界框，并响应“X位于何处？”的条件查询。我们提出了具身OCR评估框架，实验表明GutenOCR-7B在1.05万份保留的商业与科学文档上的综合具身OCR分数较其骨干模型Qwen2.5-VL-7B提升超一倍（0.40→0.82）。在Fox与OmniDocBench v1.5基准测试中，该方法显著提升了区域/行级OCR性能与文本检测召回率，但在页面级线性化、色彩引导OCR及公式密集版式处理方面存在权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for unified document understanding models that combine reading, detection, and grounding capabilities. The authors introduce GutenOCR, a family of vision-language models created by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on business documents, scientific articles, and synthetic grounding data, enabling a prompt-based interface for full-page and localized reading with bounding box outputs. Experimental results show that GutenOCR-7B more than doubles the composite grounded OCR score of its backbone model (from 0.40 to 0.82) on 10.5K held-out pages, substantially improves region- and line-level OCR and text-detection recall on Fox and OmniDocBench v1.5 benchmarks, while revealing trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发统一结合文本识别、检测与定位能力的文档理解模型。作者通过对Qwen2.5-VL视觉语言模型（30亿和70亿参数）在商业文档、科学论文及合成定位数据上进行微调，构建了GutenOCR系列模型，该单检查点模型通过提示接口支持整页/局部阅读、边界框输出和条件查询。实验结果表明，在10.5K份保留的商业与科学文档上，GutenOCR-7B将基础模型的综合定位OCR分数从0.40提升至0.82（提升一倍以上），在Fox和OmniDocBench基准测试中显著改善了区域/行级OCR性能与文本检测召回率，但也在页面级线性化、颜色引导OCR和公式密集布局方面显示出性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models</div>
<div class="meta-line">Authors: Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan, Yangfan He, Yuchen Li, Jingqun Tang</div>
<div class="meta-line">First: 2026-01-22T16:02:56+00:00 · Latest: 2026-01-22T16:02:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16065v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16065v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as &#x27;distracting tokens&#x27;. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model&#x27;s visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DTP：一种面向视觉语言动作模型的高效干扰令牌剪枝框架</div>
<div class="mono" style="margin-top:8px">视觉语言动作（VLA）模型通过利用视觉语言模型（VLM）强大的感知能力理解环境并直接输出动作，在机器人操作领域取得了显著进展。然而，VLA模型默认可能过度关注任务无关区域的图像令牌（即“干扰令牌”），这种注意力偏差会干扰模型在每一步生成预期动作令牌的过程，从而影响任务成功率。本文提出了一种即插即用的干扰令牌剪枝（DTP）框架，能够动态检测并剪除这些干扰性图像令牌。通过修正模型的视觉注意力模式，我们在不改变原始架构或增加额外输入的前提下，旨在提升任务成功率，并探索模型的性能上限。在SIMPLER基准测试（Li等人，2024）上的实验表明，该方法在不同类型的新型VLA模型中均能持续提升任务成功率，证明了其对基于Transformer的VLA模型的普适性。进一步分析揭示了所有测试模型的任务成功率与任务无关区域注意力强度呈负相关，这一普遍现象可为未来研究提供指引。代码已开源：https://anonymous.4open.science/r/CBD3。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Action (VLA) models for robotic manipulation can be misled by attending excessively to irrelevant image regions, termed &#x27;distracting tokens&#x27;, which degrades action generation and task success. To address this, the authors propose a plug-and-play Distracting Token Pruning (DTP) framework that dynamically identifies and removes these distracting tokens, thereby refining visual attention without modifying the base model architecture. Experiments on the SIMPLER benchmark demonstrate consistent relative improvements in task success rates across various transformer-based VLA models, with further analysis revealing a negative correlation between success rate and attention on task-irrelevant areas, highlighting a common vulnerability in VLAs.</div>
<div class="mono" style="margin-top:8px">用于机器人操作的视觉-语言-动作（VLA）模型可能会因关注图像中不相关的区域（即“干扰令牌”）而分心，从而降低任务成功率。为解决此问题，我们提出了干扰令牌剪枝（DTP）框架，这是一种即插即用的方法，能动态识别并移除这些干扰图像令牌，以修正模型的视觉注意力，而无需改变其架构。在SIMPLER基准测试上的实验表明，DTP能持续提升多种基于Transformer的VLA模型的任务成功率，进一步分析揭示了成功率与对任务无关区域的注意力呈负相关，这为未来研究指出了一个普遍问题。</div>
</details>
</div>
<div class="card">
<div class="title">DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning</div>
<div class="meta-line">Authors: Junha Lee, Eunha Park, Minsu Cho</div>
<div class="meta-line">First: 2026-01-22T15:23:35+00:00 · Latest: 2026-01-22T15:23:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16046v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16046v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DextER：基于语言驱动的灵巧抓取生成与具身推理</div>
<div class="mono" style="margin-top:8px">语言驱动的灵巧抓取生成要求模型理解任务语义、三维几何及复杂的手-物交互。尽管已有视觉-语言模型应用于此问题，现有方法直接将观测映射至抓取参数，缺乏对物理交互的中间推理。本文提出DextER（具身推理的灵巧抓取生成），引入基于接触的具身推理以实现多指操控。核心洞见在于：预测手部哪些连杆接触物体表面何处，可形成一种具身感知的中间表示，从而桥接任务语义与物理约束。DextER通过自回归方式生成具身接触令牌（指定手指连杆与物体表面的接触位置），随后生成编码手部构型的抓取令牌。在DexGYS数据集上，DextER实现67.14%的成功率，较最优方法提升3.83个百分点，意图对齐度提升96.4%。我们还通过部分接触指定实现了可引导的生成，为抓取合成提供细粒度控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating dexterous grasps from language instructions, which requires understanding task semantics, 3D geometry, and complex hand-object interactions. The proposed method, DextER, introduces an embodied reasoning approach that first autoregressively generates contact tokens specifying which finger links contact specific object surfaces, and then produces grasp tokens encoding the full hand configuration. Experimental results on the DexGYS benchmark show that DextER achieves a 67.14% success rate, outperforming the state-of-the-art by 3.83 percentage points and improving intention alignment by 96.4%, while also enabling steerable generation through partial contact specification.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决根据语言指令生成灵巧抓取姿态的挑战，这需要理解任务语义、三维几何以及复杂的手-物体交互。提出的方法DextER引入了一种具身推理方法，首先自回归地生成接触令牌，指定哪些手指链接接触物体表面的特定位置，然后生成编码完整手部配置的抓取令牌。在DexGYS基准测试上的实验结果表明，DextER实现了67.14%的成功率，以3.83个百分点的优势超越了现有最佳方法，并将意图对齐度提高了96.4%，同时还能通过部分接触指定实现可引导的生成控制。</div>
</details>
</div>
<div class="card">
<div class="title">RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture</div>
<div class="meta-line">Authors: Anas Anwarul Haq Khan, Mariam Husain, Kshitij Jadhav</div>
<div class="meta-line">First: 2026-01-22T12:11:53+00:00 · Latest: 2026-01-22T12:11:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15891v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RadJEPA：基于联合嵌入预测架构的胸部X光放射学编码器</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型的最新进展指导了视觉表征的学习；然而，这种监督形式受限于配对图像文本数据的可用性，引发了是否能在不依赖语言监督的情况下学习稳健放射学编码器的问题。本研究提出RadJEPA，一种基于联合嵌入预测架构的自监督框架，无需语言监督即可学习。该模型仅通过未标注的胸部X光图像进行预训练，学习预测掩码图像区域的潜在表征。这种预测目标与图像文本预训练和DINO式自蒸馏有本质区别：RadJEPA不追求跨视图或跨模态的全局表征对齐，而是显式建模潜在空间预测。我们在疾病分类、语义分割和报告生成任务上评估所学编码器。在多项基准测试中，RadJEPA的表现均超越包括Rad-DINO在内的前沿方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitation of medical vision-language models that depend on paired image-text data by proposing RadJEPA, a self-supervised framework based on a Joint Embedding Predictive Architecture that learns visual representations from unlabeled chest X-ray images alone. The method trains an encoder to predict the latent representations of masked image regions, explicitly modeling latent-space prediction instead of aligning global representations across views or modalities. Experimental evaluation on disease classification, semantic segmentation, and report generation tasks demonstrates that RadJEPA outperforms state-of-the-art approaches, including Rad-DINO.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于依赖配对图像-文本数据的医学视觉语言模型存在局限性，因此探讨是否能在无需语言监督的情况下学习稳健的放射学编码器。方法上提出了RadJEPA，这是一个基于联合嵌入预测架构的自监督框架，仅使用未标记的胸部X光图像进行预训练，以预测掩码图像区域的潜在表示，明确建模潜在空间预测而非对齐全局表示。主要实验结果表明，在疾病分类、语义分割和报告生成等基准测试中，RadJEPA的性能超过了包括Rad-DINO在内的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning</div>
<div class="meta-line">Authors: Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang, Zheng Wei</div>
<div class="meta-line">First: 2026-01-21T08:09:25+00:00 · Latest: 2026-01-22T12:09:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14750v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14750v2">PDF</a> · <a href="https://github.com/TencentBAC/RoT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维渲染：将文本思维链呈现为图像以实现视觉潜在推理</div>
<div class="mono" style="margin-top:8px">思维链提示在释放大语言模型的推理能力方面取得了显著成功。尽管思维链提示增强了推理能力，但其冗长性带来了巨大的计算开销。近期研究往往仅关注结果对齐，缺乏对中间推理过程的监督，这些缺陷使得潜在推理链的可分析性变得模糊。为解决这些问题，我们提出了思维渲染框架——首个通过将文本步骤渲染为图像来具象化推理链的框架，使潜在逻辑变得显式且可追溯。具体而言，我们利用现有视觉语言模型的视觉编码器作为语义锚点，将视觉嵌入与文本空间对齐。该设计确保了即插即用的实现方式，无需额外预训练开销。在数学与逻辑推理基准上的大量实验表明，相较于显式思维链，我们的方法实现了3-4倍的标记压缩和显著的推理加速，同时保持与其他方法相当的竞争力，验证了该范式的可行性。代码已开源：https://github.com/TencentBAC/RoT</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the computational inefficiency and lack of intermediate supervision in traditional Chain-of-Thought (CoT) prompting for Large Language Models (LLMs). The proposed Render-of-Thought (RoT) framework converts verbose textual reasoning steps into images, making the latent reasoning chain explicit and traceable by aligning visual embeddings with the textual space using existing Vision Language Model (VLM) encoders. Experimental results on mathematical and logical reasoning benchmarks show that RoT achieves 3-4x token compression and significant inference acceleration compared to explicit CoT while maintaining competitive performance.</div>
<div class="mono" style="margin-top:8px">思维链提示提升了大型语言模型的推理能力，但其冗长性导致计算开销大，且缺乏对中间推理步骤的监督，使潜在推理链难以分析。为解决这些问题，Render-of-Thought框架首次通过将文本推理步骤渲染为图像来具体化推理链，利用现有视觉语言模型的视觉编码器对齐视觉与文本嵌入，无需额外预训练。在数学和逻辑推理基准上的大量实验表明，该方法实现了3-4倍的令牌压缩和显著的推理加速，同时保持竞争力，验证了该范式的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</div>
<div class="meta-line">Authors: Pascal Benschop, Justin Dauwels, Jan van Gemert</div>
<div class="meta-line">First: 2026-01-22T09:14:11+00:00 · Latest: 2026-01-22T09:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用合成生成视频评估视觉语言模型的情境与空间感知能力</div>
<div class="mono" style="margin-top:8px">当语义依赖于细微的时间或几何线索时，视觉语言模型的空间推理能力仍显脆弱。我们引入一个合成基准测试，探究两项互补技能：情境感知（判断互动行为是否具有危害性）和空间感知（追踪行为主体与对象的关系，并推理相对位置与运动）。通过极简视频对，我们测试三个挑战：区分暴力与良性活动、跨视角绑定施暴者角色、判断细粒度轨迹对齐。我们在免训练设置下评估近期VLMs，但该基准适用于任何视频分类模型。结果显示各任务表现仅略高于随机水平。简单的稳定颜色线索辅助能部分减少施暴者角色混淆，但未解决根本缺陷。通过开源数据与代码，我们旨在提供可复现的诊断工具，并推动轻量级空间先验的探索，以补充大规模预训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision language models (VLMs) exhibit fragile spatial reasoning when semantics depend on subtle temporal or geometric cues, motivating the creation of a synthetic video benchmark to assess situational awareness (distinguishing harmful from benign interactions) and spatial awareness (tracking roles and reasoning about positions and motion). The method employs minimal video pairs to test three challenges: violence versus benign activity classification, assailant role binding across viewpoints, and fine-grained trajectory alignment judgment, evaluating recent VLMs in a training-free setting. Experimental results reveal performance only slightly above chance across tasks; while stable color cues partly reduce role confusions, they do not resolve the underlying weakness, highlighting the need for lightweight spatial priors to complement large-scale pretraining.</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在语义依赖于细微时间或几何线索时的空间推理能力仍然薄弱。为此，研究者构建了一个合成视频基准测试，用于探究两种互补技能：情境感知（识别互动是有害还是良性的）和空间感知（追踪施动者角色、相对位置和运动）。该方法在无需训练的设置下，通过极简视频对评估VLMs，涵盖三个挑战：区分暴力与良性活动、跨视角绑定攻击者角色以及判断细粒度轨迹对齐。实验结果表明，当前VLMs在这些任务上的表现仅略高于随机水平；虽然添加稳定的颜色线索部分减少了角色混淆，但未能解决根本弱点。发布的基准测试旨在提供可复现的诊断工具，并促进探索轻量级空间先验，以补充大规模预训练。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-22T09:09:47+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但这些模型仍易受对抗性越狱攻击的影响——攻击者通过精心设计的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为克服这些局限，我们提出通过采用同步扰动随机逼近的零阶优化方法（ZO-SPSA）对LVLMs实施黑盒越狱攻击。ZO-SPSA具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低资源需求，减少GPU内存消耗。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，同时保持与白盒方法相当的不可感知扰动。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱在现实场景中的可行性，并暴露了当前LVLMs安全机制的关键弱点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods in real-world black-box settings, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free, model-agnostic approach approximates gradients through input-output interactions without requiring internal model knowledge, thereby reducing computational resource demands. Experimental evaluations on InstructBLIP, LLaVA, and MiniGPT-4 demonstrate a high jailbreak success rate of 83.0% on InstructBLIP with imperceptible perturbations, and adversarial examples from MiniGPT-4 show strong transferability, achieving an attack success rate of 64.18% on other models, highlighting significant safety weaknesses in current LVLMs.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型（LVLMs）易受对抗性越狱攻击的漏洞，以及现有白盒方法在现实黑盒场景中不切实际的问题，提出了一种基于同时扰动随机逼近（ZO-SPSA）的零阶优化黑盒攻击方法。这种无需梯度、与模型无关的方法不依赖模型内部知识，且降低了资源需求。在InstructBLIP、LLaVA和MiniGPT-4等模型上的实验评估表明，该方法在InstructBLIP上实现了83.0%的高越狱成功率，且扰动难以察觉；同时，生成的对抗样本表现出强迁移性，从MiniGPT-4迁移到其他LVLMs时攻击成功率可达64.18%，揭示了当前模型安全机制的关键弱点。</div>
</details>
</div>
<div class="card">
<div class="title">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</div>
<div class="meta-line">Authors: Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T17:59:44+00:00 · Latest: 2026-01-22T08:52:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project page: https://faceong.github.io/VIKI-R/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09049v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09049v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://faceong.github.io/VIKI-R/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIKI-R：基于强化学习的具身多智能体协同协调框架</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体仍是人工智能的核心挑战，需要感知驱动的推理与可扩展的协作策略。尽管近期研究已利用大语言模型进行多智能体规划，但基于视觉语言模型的视觉推理探索仍处于起步阶段，且现有方法对异构具身形态的支持有限。本研究提出首个面向具身多智能体协作的层次化基准VIKI-Bench，包含智能体激活、任务规划与轨迹感知三层结构，集成多类机器人形态、多视角视觉观测及结构化监督信号，以评估基于视觉输入的推理能力。为验证该基准的实用性，我们提出两阶段框架VIKI-R：首先通过思维链标注样本微调预训练视觉语言模型，随后在多级奖励信号下进行强化学习。大量实验表明，VIKI-R在所有任务层级上均显著优于基线方法。此外，强化学习能促使异构智能体间涌现组合式协作模式。VIKI-Bench与VIKI-R共同为推进具身AI系统中多智能体视觉驱动协作提供了统一测试平台与方法体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments requires perception-driven reasoning and scalable cooperation strategies, but existing vision-language model (VLM) approaches often lack support for diverse robot embodiments. To address this, the authors introduce VIKI-Bench, a hierarchical benchmark for embodied multi-agent cooperation with structured levels for agent activation, task planning, and trajectory perception, and propose VIKI-R, a two-stage framework that fine-tunes a pretrained VLM using Chain-of-Thought demonstrations and then applies reinforcement learning with multi-level rewards. Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels and enables the emergence of compositional cooperation patterns among heterogeneous agents.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决动态环境中协调多个具身智能体的核心挑战，这需要视觉推理和可扩展的合作策略。作者提出了VIKI-Bench，这是一个为具身多智能体协作设计的层次化基准测试，包含三个结构化层级，并提出了VIKI-R，一个两阶段框架：首先使用思维链标注的演示数据微调预训练的视觉语言模型，然后通过多级奖励信号进行强化学习。实验结果表明，VIKI-R在所有任务层级上均显著优于基线方法，并促进了异构智能体之间组合式合作模式的出现。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework</div>
<div class="meta-line">Authors: Shubham Shukla, Kunal Sonalkar</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-22T07:33:41+00:00 · Latest: 2026-01-22T07:33:41+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026 Workshop on Physical Retail AI (PRAW)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, &quot;outer fabric&quot; is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn&#x27;t exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的零样本产品属性标注：三层评估框架</div>
<div class="mono" style="margin-top:8px">细粒度属性预测对于时尚零售应用（包括商品目录丰富化、视觉搜索和推荐系统）至关重要。视觉语言模型（VLM）无需任务特定训练即可实现零样本预测，但其在多属性时尚任务上的系统性评估仍待深入探索。关键挑战在于时尚属性常具有条件性，例如当外衣不可见时“外层面料”属性便无定义，这要求模型在尝试分类前先检测属性适用性。我们提出三层评估框架以分解该挑战：（1）所有属性在全类别（含“不适用”类）上的整体任务性能；（2）属性适用性检测；（3）属性可确定时的细粒度分类。基于明确在属性标签空间中定义“不适用”（指属性不存在或不可见）的DeepFashion-MultiModal数据集，我们在18个属性的5000张图像上，对涵盖旗舰级（GPT-5、Gemini 2.5 Pro）、高效级（GPT-5 Mini、Gemini 2.5 Flash）和超高效级（GPT-5 Nano、Gemini 2.5 Flash-Lite）的九种VLM进行基准测试，并与基于预训练Fashion-CLIP嵌入训练的分类器对比。研究发现：（1）零样本VLM宏平均F1达64.0%，较基于预训练Fashion-CLIP嵌入的逻辑回归提升三倍；（2）VLM擅长细粒度分类（第三层：70.8% F1），但在适用性检测上表现欠佳（第二层：34.1% NA-F1），这揭示了关键瓶颈；（3）高效模型能以更低成本实现旗舰模型90%以上的性能，为实际部署提供可行路径。该诊断框架可帮助从业者定位错误源于可见性检测还是分类过程，从而指导生产系统的针对性改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for fine-grained attribute prediction in fashion retail applications like catalog enrichment and visual search, where existing zero-shot Vision-Language Models (VLMs) lack systematic evaluation, especially for conditional attributes that may not be applicable. The authors propose a three-tier evaluation framework to decompose the task: assessing overall performance, attribute applicability detection, and fine-grained classification when attributes are determinable. They benchmark nine VLMs against classifiers trained on Fashion-CLIP embeddings using the DeepFashion-MultiModal dataset. Key findings show that zero-shot VLMs achieve a 64.0% macro-F1, a threefold improvement over logistic regression baselines, excel in fine-grained classification (70.8% F1) but struggle with applicability detection (34.1% NA-F1), and that efficient models deliver over 90% of flagship performance at lower cost, highlighting a practical deployment path.</div>
<div class="mono" style="margin-top:8px">本研究针对时尚零售应用中细粒度属性预测的需求，现有零样本视觉语言模型缺乏系统评估，尤其对于可能不适用条件属性的情况。作者提出了一个三层评估框架，分别评估整体性能、属性适用性检测和细粒度分类，使用DeepFashion-MultiModal数据集，对九种视觉语言模型与基于Fashion-CLIP嵌入的分类器进行了基准测试。主要结果表明，零样本视觉语言模型实现了64.0%的宏观F1分数，比逻辑回归基线提高了三倍，在细粒度分类上表现出色，达到70.8%的F1分数，但在适用性检测上显著不足，仅为34.1%的NA-F1分数，而高效模型能保持旗舰模型90%以上的性能，提供了成本效益高的部署方案。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-event Video-Text Retrieval</div>
<div class="meta-line">Authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</div>
<div class="meta-line">First: 2023-08-22T16:32:46+00:00 · Latest: 2026-01-22T06:58:13+00:00</div>
<div class="meta-line">Comments: [fixed typos in equations] accepted to ICCV2023 Poster; some figures are not supported when viewed online, please download the file and view locally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2308.11551v3">Abs</a> · <a href="https://arxiv.org/pdf/2308.11551v3">PDF</a> · <a href="https://github.com/gengyuanmax/MeVTR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多事件视频-文本检索</div>
<div class="mono" style="margin-top:8px">视频-文本检索（VTR）是互联网海量视频-文本数据时代的关键多模态任务。主流方法普遍采用双流视觉-语言模型架构学习视频-文本对的联合表征。然而，这些模型基于视频-文本双射对应的假设，忽略了更实际的场景：视频内容通常包含多个事件，而用户查询或网页元数据等文本往往针对单一事件。这导致传统训练目标与实际应用存在差距，可能造成早期模型在推理时性能下降。本研究提出多事件视频-文本检索（MeVTR）任务，针对视频包含多个不同事件的场景，作为传统VTR任务的细分方向。我们提出了简洁的Me-Retriever模型，结合关键事件视频表征与新型MeVTR损失函数。综合实验表明，该框架在视频到文本和文本到视频任务中均优于现有模型，为MeVTR任务建立了稳健基线。本工作为后续研究奠定了坚实基础。代码已开源：https://github.com/gengyuanmax/MeVTR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the practical gap in conventional Video-Text Retrieval (VTR), where models typically assume one-to-one video-text correspondences, while real-world videos often contain multiple events and texts describe single events, leading to performance degradation. To address this, the authors introduce the Multi-event Video-Text Retrieval (MeVTR) task and propose a simple model, Me-Retriever, which uses key event video representation and a novel MeVTR loss. Experimental results demonstrate that this framework outperforms other models in both Video-to-Text and Text-to-Video retrieval, establishing a robust baseline for the MeVTR task.</div>
<div class="mono" style="margin-top:8px">本研究针对传统视频-文本检索任务的一个实际局限：现有模型通常假设视频与文本一一对应，但实际视频常包含多个事件，而文本往往描述单一事件。为弥补这一差距，作者提出了多事件视频-文本检索任务，并设计了Me-Retriever模型，该模型采用关键事件视频表示和一种新的MeVTR损失函数。实验结果表明，该框架在视频到文本和文本到视频检索任务上均优于现有模型，为多事件视频-文本检索任务建立了坚实的基准。</div>
</details>
</div>
<div class="card">
<div class="title">DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection</div>
<div class="meta-line">Authors: Morteza Poudineh, Marc Lalonde</div>
<div class="meta-line">First: 2026-01-21T20:35:51+00:00 · Latest: 2026-01-21T20:35:51+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15453v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DevPrompt：基于偏差的提示学习用于单正常样本图像异常检测</div>
<div class="mono" style="margin-top:8px">少样本正常图像异常检测（FNSAD）旨在仅用少量正常训练样本检测图像中的异常区域，由于监督有限和潜在缺陷的多样性，该任务极具挑战性。现有方法利用视觉-语言模型（如CLIP）通过基于提示的学习对齐图像与文本特征，但常存在正常与异常提示间区分度弱、缺乏针对局部异常的原则性评分机制等问题。本文提出一种偏差引导的提示学习框架，将视觉-语言模型的语义能力与基于偏差评分的统计可靠性相结合。具体而言，我们使用可学习的上下文向量替代固定提示前缀（在正常/异常提示间共享），并通过异常专属后缀令牌实现类别感知对齐。为增强可分性，引入结合Top-K多示例学习（MIL）的偏差损失，将局部特征建模为相对正态分布的高斯偏差，使网络能为统计显著偏离的局部区域分配更高异常分数，从而提升定位能力与可解释性。在MVTecAD和VISA基准上的实验表明，本方法在像素级检测性能上优于PromptAD等基线模型。消融实验进一步验证了可学习提示、基于偏差的评分及Top-K MIL策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of few-normal shot anomaly detection (FNSAD), where limited normal samples and diverse defects hinder effective anomaly localization. The method, DevPrompt, introduces a deviation-guided prompt learning framework that combines vision-language models with statistical deviation scoring, using learnable context vectors and anomaly-specific suffix tokens for class-aware feature alignment, and employs a deviation loss with Top-K Multiple Instance Learning to model patch features as Gaussian deviations for improved discriminability. Experimental results on MVTecAD and VISA benchmarks show superior pixel-level detection performance over PromptAD and other baselines, with ablations confirming the contributions of learnable prompts, deviation scoring, and the Top-K MIL strategy.</div>
<div class="mono" style="margin-top:8px">本研究针对少样本正常图像异常检测（FNSAD）的挑战，即有限的正常样本和多样的缺陷使得异常定位困难。所提出的DevPrompt方法采用了一种基于偏差的提示学习框架，将视觉-语言模型的语义能力与统计偏差评分相结合，通过可学习的上下文向量和异常特定后缀令牌实现类别感知对齐，并引入带有Top-K多示例学习的偏差损失，将图像块特征建模为高斯偏差以增强区分度。在MVTecAD和VISA基准测试上的实验结果表明，该方法在像素级检测性能上优于PromptAD及其他基线模型，消融研究进一步验证了可学习提示、偏差评分和Top-K MIL策略的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
