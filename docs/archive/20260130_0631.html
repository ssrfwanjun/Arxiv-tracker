<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-30 06:31</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260130_0631</div>
    <div class="row"><div class="card">
<div class="title">End-to-end example-based sim-to-real RL policy transfer based on neural stylisation with application to robotic cutting</div>
<div class="meta-line">Authors: Jamie Hathaway, Alireza Rastegarpanah, Rustam Stolkin</div>
<div class="meta-line">First: 2026-01-28T18:45:55+00:00 · Latest: 2026-01-28T18:45:55+00:00</div>
<div class="meta-line">Comments: 14 pages, 9 figures. Submitted to Nature Scientific Reports</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20846v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20846v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whereas reinforcement learning has been applied with success to a range of robotic control problems in complex, uncertain environments, reliance on extensive data - typically sourced from simulation environments - limits real-world deployment due to the domain gap between simulated and physical systems, coupled with limited real-world sample availability. We propose a novel method for sim-to-real transfer of reinforcement learning policies, based on a reinterpretation of neural style transfer from image processing to synthesise novel training data from unpaired unlabelled real world datasets. We employ a variational autoencoder to jointly learn self-supervised feature representations for style transfer and generate weakly paired source-target trajectories to improve physical realism of synthesised trajectories. We demonstrate the application of our approach based on the case study of robot cutting of unknown materials. Compared to baseline methods, including our previous work, CycleGAN, and conditional variational autoencoder-based time series translation, our approach achieves improved task completion time and behavioural stability with minimal real-world data. Our framework demonstrates robustness to geometric and material variation, and highlights the feasibility of policy adaptation in challenging contact-rich tasks where real-world reward information is unavailable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经风格化的端到端示例型仿真到现实强化学习策略迁移及其在机器人切割中的应用</div>
<div class="mono" style="margin-top:8px">尽管强化学习已成功应用于复杂不确定环境中的一系列机器人控制问题，但其对大量数据（通常来源于仿真环境）的依赖，由于仿真与物理系统间的领域差异以及现实世界样本有限，限制了实际部署。我们提出一种新颖的仿真到现实强化学习策略迁移方法，该方法通过将图像处理中的神经风格迁移重新解释，从未配对未标记的现实世界数据集中合成新的训练数据。我们采用变分自编码器联合学习风格迁移的自监督特征表示，并生成弱配对的源-目标轨迹以提升合成轨迹的物理真实性。我们以机器人切割未知材料的案例研究展示了该方法的应用。与基线方法（包括我们先前的工作、CycleGAN及基于条件变分自编码器的时间序列翻译）相比，我们的方法以极少的现实世界数据实现了更优的任务完成时间和行为稳定性。该框架展现出对几何与材料变化的鲁棒性，并突显了在缺乏现实世界奖励信息的密集接触挑战性任务中进行策略适应的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the sim-to-real gap in reinforcement learning for robotics, where reliance on simulation data limits real-world deployment due to domain differences and scarce physical samples. The method reinterprets neural style transfer to synthesize training data from unpaired, unlabeled real-world datasets, using a variational autoencoder to learn self-supervised feature representations and generate weakly paired source-target trajectories for enhanced physical realism. Experimental results on robotic cutting of unknown materials show that this approach outperforms baselines like CycleGAN and conditional variational autoencoders, achieving faster task completion, greater behavioral stability with minimal real data, and robustness to geometric and material variations.</div>
<div class="mono" style="margin-top:8px">该研究针对机器人强化学习中仿真到现实的差距问题，由于领域差异和物理样本稀缺，依赖仿真数据限制了实际部署。方法将神经风格迁移重新解释，利用未配对、未标记的真实世界数据集合成训练数据，采用变分自编码器学习特征表示并生成弱配对轨迹以增强真实性。在机器人切割未知材料的实验中，相比CycleGAN等基线方法，该方法在任务完成时间和行为稳定性上表现更优，对几何和材料变化具有鲁棒性，实现了在无真实奖励的接触密集任务中的策略适应。</div>
</details>
</div>
<div class="card">
<div class="title">Discrete Variational Autoencoding via Policy Search</div>
<div class="meta-line">Authors: Michael Drolet, Firas Al-Hafez, Aditya Bhatt, Jan Peters, Oleg Arenz</div>
<div class="meta-line">First: 2025-09-29T12:44:05+00:00 · Latest: 2026-01-28T18:33:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24716v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.24716v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略搜索的离散变分自编码</div>
<div class="mono" style="margin-top:8px">变分自编码器（VAE）中的离散潜在瓶颈具有高比特效率，可通过自回归离散分布建模，实现基于Transformer的参数高效多模态搜索。然而，离散随机变量无法实现精确可微参数化，因此离散VAE通常依赖近似方法（如Gumbel-Softmax重参数化或直通梯度估计），或采用高方差的无梯度方法（如REINFORCE），这些方法在图像重建等高维任务中效果有限。受策略搜索常用技术启发，我们提出一种离散VAE训练框架，利用非参数编码器的自然梯度更新参数编码器，无需重参数化。该方法结合自动步长适应和基于Transformer的编码器，可扩展至ImageNet等挑战性数据集，在从紧凑潜在空间重建高维数据方面，优于近似重参数化方法和基于量化的离散自编码器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Discrete latent bottlenecks in variational autoencoders offer bit efficiency and enable multimodal search, but training them is challenging due to the non-differentiability of discrete variables, with existing methods like Gumbel-Softmax or REINFORCE facing limitations in high-dimensional tasks. To address this, the authors propose a training framework inspired by policy search that uses the natural gradient of a non-parametric encoder to update a parametric encoder, avoiding reparameterization, and integrate it with automatic step size adaptation and a transformer-based encoder. Experiments on datasets like ImageNet show that this method outperforms both approximate reparameterization techniques and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.</div>
<div class="mono" style="margin-top:8px">离散变分自编码器（VAE）能提供高效的潜在表示，但由于离散变量的不可微分性，其训练面临挑战，现有方法如Gumbel-Softmax或REINFORCE在高维任务中往往效果有限。为此，作者受策略搜索技术启发，提出一种训练框架，利用非参数编码器的自然梯度来更新参数编码器，无需重参数化，并结合自动步长适应和基于Transformer的编码器。在ImageNet等数据集上的实验表明，该方法在从紧凑潜在空间重建高维数据方面，优于近似重参数化技术和基于量化的离散自编码器。</div>
</details>
</div>
<div class="card">
<div class="title">FLOL: Fast Baselines for Real-World Low-Light Enhancement</div>
<div class="meta-line">Authors: Juan C. Benito, Daniel Feijoo, Alvaro Garcia, Marcos V. Conde</div>
<div class="meta-line">First: 2025-01-16T18:06:09+00:00 · Latest: 2026-01-28T18:31:35+00:00</div>
<div class="meta-line">Comments: Journal Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.09718v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.09718v2">PDF</a> · <a href="https://github.com/cidautai/FLOL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Light Image Enhancement (LLIE) is a key task in computational photography and imaging. The problem of enhancing images captured during night or in dark environments has been well-studied in the computer vision literature. However, current deep learning-based solutions struggle with efficiency and robustness for real-world scenarios (e.g., scenes with noise, saturated pixels). We propose a lightweight neural network that combines image processing in the frequency and spatial domains. Our baseline method, FLOL, is one of the fastest models for this task, achieving results comparable to the state-of-the-art on popular real-world benchmarks such as LOLv2, LSRW, MIT-5K and UHD-LL. Moreover, we are able to process 1080p images in real-time under 12ms. Code and models at https://github.com/cidautai/FLOL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLOL：面向真实世界低光照增强的快速基线方法</div>
<div class="mono" style="margin-top:8px">低光照图像增强是计算摄影与成像领域的关键任务。针对夜间或暗光环境下拍摄图像的增强问题，计算机视觉领域已有深入研究。然而，当前基于深度学习的解决方案在真实场景（如存在噪声、饱和像素的场景）中的效率与鲁棒性仍面临挑战。本文提出一种结合频域与空域图像处理的轻量级神经网络。我们的基线方法FLOL是该任务中最快的模型之一，在LOLv2、LSRW、MIT-5K和UHD-LL等主流真实场景基准测试中取得了与前沿方法相当的结果，且能在12毫秒内实时处理1080p图像。代码与模型发布于https://github.com/cidautai/FLOL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for efficient and robust low-light image enhancement (LLIE) in real-world scenarios, where existing deep learning methods often struggle with computational speed and handling noise or saturated pixels. The proposed method, FLOL, introduces a lightweight neural network that processes images in both frequency and spatial domains to achieve fast and effective enhancement. Experimental results demonstrate that FLOL achieves performance comparable to state-of-the-art models on benchmarks like LOLv2 and LSRW while being one of the fastest, capable of real-time processing of 1080p images in under 12 milliseconds.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决现实场景中低光照图像增强对效率和鲁棒性的需求，现有深度学习方法常在处理噪声和饱和像素时难以兼顾速度。提出的方法FLOL是一种轻量级神经网络，通过在频域和空间域结合处理图像。实验结果表明，FLOL在LOLv2和LSRW等基准测试上取得了与最先进模型相当的性能，同时速度极快，能在12毫秒内实时处理1080p图像。</div>
</details>
</div>
<div class="card">
<div class="title">MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents</div>
<div class="meta-line">Authors: Vishnu Sashank Dorbala, Dinesh Manocha</div>
<div class="meta-line">First: 2026-01-28T18:31:17+00:00 · Latest: 2026-01-28T18:31:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20831v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20831v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemCtrl：将多模态大语言模型用作具身智能体的主动记忆控制器</div>
<div class="mono" style="margin-top:8px">基础模型依赖上下文学习进行个性化决策，但其有限的上下文窗口需要记忆压缩与检索系统（如RAG）。现有系统常将记忆视为大型离线存储空间，这不适用于需在严格内存与计算约束下在线运行的具身智能体。本研究提出MemCtrl框架，利用多模态大语言模型在线修剪记忆。MemCtrl通过可训练的记忆头μ增强MLLMs，该记忆头作为门控机制，在探索过程中决定保留、更新或丢弃哪些观测与反思。我们通过两种方式训练μ进行评估：1）通过离线专家训练，2）通过在线强化学习训练，发现增强后的MLLMs在具身任务完成能力上显著提升。在EmbodiedBench基准的多个子集上对两个低性能MLLMs进行MemCtrl增强后，μ增强的MLLMs平均提升约16%，特定指令子集提升超20%。最后，我们对μ收集的记忆片段进行定性分析，发现增强模型在长复杂指令类型上表现尤为优异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of enabling embodied agents to perform personalized decision-making under strict memory and computational constraints, this work introduces MemCtrl, a framework that uses Multimodal Large Language Models (MLLMs) as active online memory controllers. The method augments an MLLM with a trainable memory head (μ) that gates which observations or reflections to retain, update, or discard during exploration, with μ trained either via an offline expert or through online reinforcement learning. Experimental evaluation on the EmbodiedBench benchmark shows that augmenting low-performing MLLMs with MemCtrl yields an average improvement of around 16% in task completion, with over 20% gains on specific instruction subsets, and qualitative analysis indicates superior handling of long and complex instructions.</div>
<div class="mono" style="margin-top:8px">为使具身智能体能在严格的内存与计算限制下在线运行，本研究提出了MemCtrl框架，利用多模态大语言模型作为主动内存控制器。该方法通过为MLLM增加一个可训练的内存头（μ），在探索过程中动态地门控、修剪和更新观察或反思，μ可通过离线专家或在线强化学习进行训练。在EmbodiedBench基准测试上的实验表明，用MemCtrl增强性能较低的MLLM能使任务完成率平均提升约16%，在特定指令子集上提升超过20%，定性分析还显示其能更好地处理长而复杂的指令。</div>
</details>
</div>
<div class="card">
<div class="title">A Methodology for Designing Knowledge-Driven Missions for Robots</div>
<div class="meta-line">Authors: Guillermo GP-Lenza, Carmen DR. Pita-Romero, Miguel Fernandez-Cortizas, Pascual Campoy</div>
<div class="meta-line">Venue: 2024 7th Iberian Robotics Conference (ROBOT)</div>
<div class="meta-line">First: 2026-01-28T17:39:03+00:00 · Latest: 2026-01-28T17:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20797v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20797v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a comprehensive methodology for implementing knowledge graphs in ROS 2 systems, aiming to enhance the efficiency and intelligence of autonomous robotic missions. The methodology encompasses several key steps: defining initial and target conditions, structuring tasks and subtasks, planning their sequence, representing task-related data in a knowledge graph, and designing the mission using a high-level language. Each step builds on the previous one to ensure a cohesive process from initial setup to final execution. A practical implementation within the Aerostack2 framework is demonstrated through a simulated search and rescue mission in a Gazebo environment, where drones autonomously locate a target. This implementation highlights the effectiveness of the methodology in improving decision-making and mission performance by leveraging knowledge graphs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向机器人的知识驱动任务设计方法</div>
<div class="mono" style="margin-top:8px">本文提出了一种在ROS 2系统中实现知识图谱的完整方法，旨在提升自主机器人任务的效率与智能水平。该方法涵盖多个关键步骤：定义初始与目标条件、构建任务与子任务结构、规划执行序列、在知识图谱中表示任务相关数据，以及使用高级语言设计任务流程。各步骤环环相扣，确保从初始配置到最终执行的连贯性。通过在Aerostack2框架中实施的模拟搜救任务（基于Gazebo环境，无人机自主定位目标）验证了该方法的实用性。该实例表明，利用知识图谱能有效提升决策质量与任务执行效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance the efficiency and intelligence of autonomous robotic missions, this paper introduces a methodology for integrating knowledge graphs into ROS 2 systems. The method involves defining mission conditions, structuring and sequencing tasks, representing task data in a knowledge graph, and designing the mission with a high-level language, creating a cohesive process from setup to execution. In a simulated search and rescue mission using the Aerostack2 framework and Gazebo, drones successfully autonomously located a target, demonstrating that the methodology effectively improves decision-making and mission performance through knowledge graph utilization.</div>
<div class="mono" style="margin-top:8px">为提高自主机器人任务的效率和智能性，本文提出了一种在ROS 2系统中集成知识图谱的方法论。该方法包括定义任务初始与目标条件、构建任务与子任务结构、规划执行序列、在知识图谱中表示任务相关数据，并使用高级语言设计任务，从而形成从初始设置到最终执行的连贯流程。通过在Aerostack2框架中使用Gazebo模拟无人机搜救任务进行实验验证，结果表明该方法通过利用结构化知识，有效提升了自主决策能力和任务执行性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning From a Steady Hand: A Weakly Supervised Agent for Robot Assistance under Microscopy</div>
<div class="meta-line">Authors: Huanyu Tian, Martin Huber, Lingyun Zeng, Zhe Han, Wayne Bennett, Giuseppe Silvestri, Gerardo Mendizabal-Ruiz, Tom Vercauteren, Alejandro Chavez-Badiola, Christos Bergeles</div>
<div class="meta-line">First: 2026-01-28T17:03:43+00:00 · Latest: 2026-01-28T17:03:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20776v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper rethinks steady-hand robotic manipulation by using a weakly supervised framework that fuses calibration-aware perception with admittance control. Unlike conventional automation that relies on labor-intensive 2D labeling, our framework leverages reusable warm-up trajectories to extract implicit spatial information, thereby achieving calibration-aware, depth-resolved perception without the need for external fiducials or manual depth annotation. By explicitly characterizing residuals from observation and calibration models, the system establishes a task-space error budget from recorded warm-ups. The uncertainty budget yields a lateral closed-loop accuracy of approx. 49 micrometers at 95% confidence (worst-case testing subset) and a depth accuracy of &lt;= 291 micrometers at 95% confidence bound during large in-plane moves. In a within-subject user study (N=8), the learned agent reduces overall NASA-TLX workload by 77.1% relative to the simple steady-hand assistance baseline. These results demonstrate that the weakly supervised agent improves the reliability of microscope-guided biomedical micromanipulation without introducing complex setup requirements, offering a practical framework for microscope-guided intervention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从稳健操作中学习：显微环境下机器人辅助的弱监督智能体</div>
<div class="mono" style="margin-top:8px">本文通过融合标定感知与导纳控制的弱监督框架，重新思考稳健操作机器人技术。与传统依赖费时二维标注的自动化方法不同，本框架利用可复用的预热轨迹提取隐式空间信息，无需外部标记或人工深度标注即可实现标定感知的深度解析感知。系统通过显式表征观测与标定模型的残差，从记录的预热数据建立任务空间误差预算。该不确定度预算在95%置信度下实现约49微米的横向闭环精度（最差测试子集），大幅面内移动时深度精度≤291微米（95%置信区间）。在受试者内用户研究（N=8）中，学习型智能体较基础稳健操作辅助将NASA-TLX综合工作负荷降低77.1%。结果表明，该弱监督智能体在无需复杂设置的前提下提升了显微镜引导生物医学微操作的可靠性，为显微介入提供了实用框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance the reliability of microscope-guided biomedical micromanipulation by improving upon conventional steady-hand robotic assistance, which often requires labor-intensive labeling. The method introduces a weakly supervised framework that fuses calibration-aware perception with admittance control, using reusable warm-up trajectories to extract implicit spatial information for depth-resolved perception without external markers or manual depth annotation. Experimental results show the system achieves a lateral closed-loop accuracy of approximately 49 micrometers and a depth accuracy within 291 micrometers at 95% confidence, and in a user study with eight participants, it reduces overall NASA-TLX workload by 77.1% compared to a simple steady-hand assistance baseline.</div>
<div class="mono" style="margin-top:8px">本研究旨在提高显微镜引导下生物医学微操作中稳态手机器人辅助的可靠性，以克服依赖劳动密集型二维标注的传统自动化方法的局限。该方法引入了一个弱监督框架，将校准感知与导纳控制相融合，利用可重复使用的预热轨迹提取隐含空间信息，从而实现无需外部基准点或手动深度标注的深度感知。关键实验结果表明，该系统在95%置信水平下实现了约49微米的横向闭环精度和不超过291微米的深度精度；在一项涉及八名参与者的用户研究中，学习到的智能体将NASA-TLX总体工作负荷较简单的稳态手辅助基线降低了77.1%。</div>
</details>
</div>
<div class="card">
<div class="title">Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction</div>
<div class="meta-line">Authors: Matej Halinkovic, Nina Masarykova, Alexey Vinel, Marek Galinski</div>
<div class="meta-line">First: 2026-01-28T15:53:32+00:00 · Latest: 2026-01-28T15:53:32+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20720v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20720v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explored. Models often rely on fusion schemes that introduce heuristic alignment and discrete selection steps which prevent full utilization of available information and can introduce unwanted bias. We propose Li-ViP3D++, a query-based multimodal PnP framework that introduces Query-Gated Deformable Fusion (QGDF) to integrate multi-view RGB and LiDAR in query space. QGDF (i) aggregates image evidence via masked attention across cameras and feature levels, (ii) extracts LiDAR context through fully differentiable BEV sampling with learned per-query offsets, and (iii) applies query-conditioned gating to adaptively weight visual and geometric cues per agent. The resulting architecture jointly optimizes detection, tracking, and multi-hypothesis trajectory forecasting in a single end-to-end model. On nuScenes, Li-ViP3D++ improves end-to-end behavior and detection quality, achieving higher EPA (0.335) and mAP (0.502) while substantially reducing false positives (FP ratio 0.147), and it is faster than the prior Li-ViP3D variant (139.82 ms vs. 145.91 ms). These results indicate that query-space, fully differentiable camera-LiDAR fusion can increase robustness of end-to-end PnP without sacrificing deployability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Li-ViP3D++：面向端到端感知与轨迹预测的查询门控可变形相机-激光雷达融合</div>
<div class="mono" style="margin-top:8px">从原始传感器数据实现端到端感知与轨迹预测是自动驾驶的核心能力之一。模块化流水线会限制信息流动并放大上游误差。近期基于查询的完全可微分感知-预测模型缓解了这些问题，但相机与激光雷达在查询空间的互补性尚未得到充分探索。现有模型常依赖引入启发式对齐和离散选择步骤的融合方案，这阻碍了信息的充分利用并可能引入偏差。本文提出Li-ViP3D++，一种基于查询的多模态感知-预测框架，通过查询门控可变形融合在查询空间集成多视角RGB与激光雷达数据。该融合机制：（i）通过跨相机与特征层的掩码注意力聚合图像证据；（ii）通过具有逐查询可学习偏移量的完全可微分鸟瞰图采样提取激光雷达上下文；（iii）应用查询条件门控为每个智能体自适应加权视觉与几何线索。该架构在单一端到端模型中联合优化检测、跟踪与多假设轨迹预测。在nuScenes数据集上，Li-ViP3D++提升了端到端行为与检测质量，获得更高的EPA（0.335）和mAP（0.502），同时显著降低误报率（FP比率0.147），且较前代Li-ViP3D变体速度更快（139.82毫秒 vs 145.91毫秒）。结果表明，查询空间中完全可微分的相机-激光雷达融合能在不牺牲部署性的前提下增强端到端感知-预测的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of modular pipelines and suboptimal fusion in end-to-end autonomous driving perception and prediction, this paper introduces Li-ViP3D++, a query-based multimodal framework. The core method is Query-Gated Deformable Fusion (QGDF), which integrates multi-view camera and LiDAR data in query space through masked attention for images, differentiable BEV sampling with learned offsets for LiDAR, and query-conditioned gating to adaptively weight modalities per agent. Experimental results on nuScenes demonstrate that the model improves end-to-end behavior and detection, achieving higher EPA (0.335) and mAP (0.502), substantially reduces false positives (FP ratio 0.147), and runs faster (139.82 ms) than its predecessor.</div>
<div class="mono" style="margin-top:8px">为解决模块化流程和次优融合在端到端感知与轨迹预测中的局限，本文提出了基于查询的多模态框架Li-ViP3D++。该方法采用新颖的查询门控可变形融合模块，通过掩码注意力整合多视角图像数据，利用带学习偏移量的可微分BEV采样处理激光雷达数据，并基于查询条件门控自适应加权多模态信息。在nuScenes数据集上的实验表明，该模型提升了端到端行为与检测质量，获得了更高的EPA（0.335）和mAP（0.502），显著降低了误报率（FP比率0.147），且运行速度（139.82毫秒）优于前代模型，证明了其鲁棒性与可部署性。</div>
</details>
</div>
<div class="card">
<div class="title">One Step Is Enough: Dispersive MeanFlow Policy Optimization</div>
<div class="meta-line">Authors: Guowei Zou, Haitao Wang, Hejun Wu, Yukun Qian, Yuhang Wang, Weibing Li</div>
<div class="meta-line">First: 2026-01-28T15:34:29+00:00 · Latest: 2026-01-28T15:34:29+00:00</div>
<div class="meta-line">Comments: Code and project page: https://guowei-zou.github.io/dmpo-page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20701v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://guowei-zou.github.io/dmpo-page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time robotic control demands fast action generation. However, existing generative policies based on diffusion and flow matching require multi-step
  sampling, fundamentally limiting deployment in time-critical scenarios. We propose Dispersive MeanFlow Policy Optimization (DMPO), a unified framework that
  enables true one-step generation through three key components: MeanFlow for mathematically-derived single-step inference without knowledge distillation,
  dispersive regularization to prevent representation collapse, and reinforcement learning (RL) fine-tuning to surpass expert demonstrations. Experiments
  across RoboMimic manipulation and OpenAI Gym locomotion benchmarks demonstrate competitive or superior performance compared to multi-step baselines. With
  our lightweight model architecture and the three key algorithmic components working in synergy, DMPO exceeds real-time control requirements (&gt;120Hz) with
  5-20x inference speedup, reaching hundreds of Hertz on high-performance GPUs. Physical deployment on a Franka-Emika-Panda robot validates real-world
  applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步足矣：弥散均值流策略优化</div>
<div class="mono" style="margin-top:8px">实时机器人控制需要快速生成动作。然而，现有基于扩散和流匹配的生成策略需要多步采样，从根本上限制了在时间敏感场景中的部署。我们提出弥散均值流策略优化（DMPO），这是一个通过三个关键组件实现真正一步生成的统一框架：无需知识蒸馏即可通过数学推导实现单步推理的均值流、防止表征坍塌的弥散正则化，以及超越专家示范的强化学习微调。在RoboMimic操作任务和OpenAI Gym运动基准测试中的实验表明，其性能与多步基线方法相当或更优。凭借轻量级模型架构与三个关键算法组件的协同作用，DMPO以5-20倍的推理加速比（在高性能GPU上可达数百赫兹）满足实时控制需求（&gt;120Hz）。在Franka-Emika-Panda机器人上的物理部署验证了其实际应用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the inefficiency of multi-step sampling in diffusion and flow matching policies for real-time robotic control, this paper introduces Dispersive MeanFlow Policy Optimization (DMPO), a framework enabling true one-step action generation. The method integrates MeanFlow for mathematically-derived single-step inference, dispersive regularization to prevent representation collapse, and reinforcement learning fine-tuning to improve upon demonstrations. Experimental results on RoboMimic and OpenAI Gym benchmarks show DMPO achieves competitive or superior performance to multi-step baselines while providing a 5-20x inference speedup, exceeding 120Hz for real-time control and validating deployment on a physical robot.</div>
<div class="mono" style="margin-top:8px">该研究针对实时机器人控制中快速动作生成的需求，现有基于扩散和流匹配的生成策略因多步采样而受限。提出的弥散均值流策略优化（DMPO）通过一个统一框架实现真正的一步生成，其关键包括均值流用于无需知识蒸馏的数学推导单步推理、弥散正则化防止表示崩溃，以及强化学习微调以超越专家演示。在RoboMimic操作和OpenAI Gym运动基准测试中，实验表明DMPO相比多步基线具有竞争力或更优性能，推理速度提升5-20倍，超过120Hz的实时控制要求，在高性能GPU上达到数百赫兹，并在Franka-Emika-Panda机器人上通过物理部署验证了实际适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Tendon-based modelling, estimation and control for a simulated high-DoF anthropomorphic hand model</div>
<div class="meta-line">Authors: Péter Polcz, Katalin Schäffer, Miklós Koller</div>
<div class="meta-line">First: 2026-01-28T15:10:02+00:00 · Latest: 2026-01-28T15:10:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20682v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20682v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tendon-driven anthropomorphic robotic hands often lack direct joint angle sensing, as the integration of joint encoders can compromise mechanical compactness and dexterity. This paper presents a computational method for estimating joint positions from measured tendon displacements and tensions. An efficient kinematic modeling framework for anthropomorphic hands is first introduced based on the Denavit-Hartenberg convention. Using a simplified tendon model, a system of nonlinear equations relating tendon states to joint positions is derived and solved via a nonlinear optimization approach. The estimated joint angles are then employed for closed-loop control through a Jacobian-based proportional-integral (PI) controller augmented with a feedforward term, enabling gesture tracking without direct joint sensing. The effectiveness and limitations of the proposed estimation and control framework are demonstrated in the MuJoCo simulation environment using the Anatomically Correct Biomechatronic Hand, featuring five degrees of freedom for each long finger and six degrees of freedom for the thumb.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于肌腱的高自由度仿人手机器人模型建模、估计与控制仿真研究</div>
<div class="mono" style="margin-top:8px">肌腱驱动仿人手机器人常缺乏直接关节角度传感，因集成关节编码器可能影响机械紧凑性与灵巧性。本文提出一种通过测量肌腱位移与张力估计关节位置的计算方法。首先基于Denavit-Hartenberg约定建立高效的仿人手运动学建模框架，采用简化肌腱模型推导出肌腱状态与关节位置的非线性方程组，并通过非线性优化方法求解。随后将估计的关节角度用于闭环控制，采用基于雅可比矩阵的比例-积分控制器并结合前馈项，实现无需直接关节传感的姿态跟踪。通过在MuJoCo仿真环境中使用解剖学精确生物机电手模型（每个长指五自由度、拇指六自由度），验证了所提估计与控制框架的有效性与局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of direct joint sensing in tendon-driven robotic hands, which often compromises mechanical compactness, this paper develops a computational method for estimating joint positions from measured tendon displacements and tensions. The approach introduces an efficient kinematic modeling framework based on the Denavit-Hartenberg convention, derives a system of nonlinear equations relating tendon states to joint positions via a simplified tendon model, and solves it using nonlinear optimization. The estimated angles are then used in a Jacobian-based PI controller with feedforward for closed-loop gesture tracking. Simulation results on the Anatomically Correct Biomechatronic Hand model in MuJoCo demonstrate the framework&#x27;s effectiveness and limitations for a high-degree-of-freedom hand.</div>
<div class="mono" style="margin-top:8px">针对腱驱动拟人机械手因追求机械紧凑性而缺乏直接关节角度传感的问题，本文提出了一种通过测量腱位移和张力来估计关节位置的计算方法。该方法首先基于Denavit-Hartenberg约定引入了一个高效的拟人手运动学建模框架，利用简化的腱模型推导了腱状态与关节位置的非线性方程组，并通过非线性优化进行求解。随后，将估计的关节角度用于一个带有前馈项的基于雅可比矩阵的比例积分闭环控制器，以实现无需直接关节传感的姿态跟踪。在MuJoCo仿真环境中使用具有高自由度手指的解剖学正确生物机电手模型进行的实验，验证了该估计与控制框架的有效性并揭示了其局限性。</div>
</details>
</div>
<div class="card">
<div class="title">GPO: Growing Policy Optimization for Legged Robot Locomotion and Whole-Body Control</div>
<div class="meta-line">Authors: Shuhao Liao, Peizhuo Li, Xinrong Yang, Linnan Chang, Zhaoxin Fan, Qing Wang, Lei Shi, Yuhong Cao, Wenjun Wu, Guillaume Sartoretti</div>
<div class="meta-line">First: 2026-01-28T14:49:52+00:00 · Latest: 2026-01-28T14:49:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20668v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training reinforcement learning (RL) policies for legged robots remains challenging due to high-dimensional continuous actions, hardware constraints, and limited exploration. Existing methods for locomotion and whole-body control work well for position-based control with environment-specific heuristics (e.g., reward shaping, curriculum design, and manual initialization), but are less effective for torque-based control, where sufficiently exploring the action space and obtaining informative gradient signals for training is significantly more difficult. We introduce Growing Policy Optimization (GPO), a training framework that applies a time-varying action transformation to restrict the effective action space in the early stage, thereby encouraging more effective data collection and policy learning, and then progressively expands it to enhance exploration and achieve higher expected return. We prove that this transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion, thereby ensuring stable training. We evaluate GPO on both quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware. Policies trained with GPO consistently achieve better performance. These results suggest that GPO provides a general, environment-agnostic optimization framework for learning legged locomotion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GPO：面向足式机器人运动与全身控制的渐进式策略优化</div>
<div class="mono" style="margin-top:8px">由于高维连续动作空间、硬件约束与有限探索能力，足式机器人的强化学习策略训练仍具挑战。现有运动与全身控制方法在基于位置的控制中（借助环境特定启发式策略，如奖励塑形、课程设计与手动初始化）表现良好，但在基于力矩的控制中效果有限——后者因动作空间探索难度显著增加且训练梯度信号难以获取而更具挑战。本文提出渐进式策略优化框架，通过时变动作变换在训练初期限制有效动作空间，以促进更高效的数据收集与策略学习，随后逐步扩展该空间以增强探索并提升预期回报。我们证明该变换保持了PPO更新规则，仅引入有界且渐近消失的梯度失真，从而确保训练稳定性。在四足与六足机器人上的实验表明，GPO训练的策略均取得更优性能，包括仿真训练策略在硬件上的零样本部署。这些结果证明GPO为足式运动学习提供了通用且环境无关的优化框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training reinforcement learning policies for legged robots is challenging due to high-dimensional continuous actions and limited exploration, especially for torque-based control where existing methods often rely on environment-specific heuristics. To address this, the authors propose Growing Policy Optimization (GPO), a framework that applies a time-varying action transformation to initially restrict the effective action space for more efficient data collection and policy learning, then progressively expands it to enhance exploration and maximize returns, with theoretical guarantees that the transformation preserves the PPO update rule and introduces only bounded, vanishing gradient distortion. Experimental evaluations on quadruped and hexapod robots, including zero-shot deployment of simulation-trained policies on hardware, demonstrate that policies trained with GPO consistently achieve superior performance, indicating its effectiveness as a general, environment-agnostic optimization framework for legged locomotion.</div>
<div class="mono" style="margin-top:8px">由于高维连续动作空间、硬件限制和探索困难，训练足式机器人的强化学习策略具有挑战性，特别是在基于扭矩的控制中，依赖环境特定启发式方法的现有方法效果不佳。为此，研究者提出了成长策略优化（GPO），该框架通过时变动作变换在早期限制有效动作空间以促进更有效的数据收集和策略学习，随后逐步扩展以增强探索并实现更高预期回报，同时证明该变换保持了PPO更新规则且梯度失真有限有界。在四足和六足机器人上的实验评估，包括仿真训练策略的零样本硬件部署，表明GPO训练的策略始终获得更优性能，这提示GPO为足式运动学习提供了一个通用、环境无关的优化框架。</div>
</details>
</div>
<div class="card">
<div class="title">Fusion of Visual-Inertial Odometry with LiDAR Relative Localization for Cooperative Guidance of a Micro-Scale Aerial Vehicle</div>
<div class="meta-line">Authors: Václav Pritzl, Matouš Vrba, Petr Štěpán, Martin Saska</div>
<div class="meta-line">First: 2023-06-30T11:00:37+00:00 · Latest: 2026-01-28T13:57:04+00:00</div>
<div class="meta-line">Comments: Preprint version. This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2306.17544v3">Abs</a> · <a href="https://arxiv.org/pdf/2306.17544v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A novel relative localization approach for guidance of a micro-scale Unmanned Aerial Vehicle (UAV) by a well-equipped aerial robot fusing Visual-Inertial Odometry (VIO) with Light Detection and Ranging (LiDAR) is proposed in this paper. LiDAR-based localization is accurate and robust to challenging environmental conditions, but 3D LiDARs are relatively heavy and require large UAV platforms, in contrast to lightweight cameras. However, visual-based self-localization methods exhibit lower accuracy and can suffer from significant drift with respect to the global reference frame. To benefit from both sensory modalities, we focus on cooperative navigation in a heterogeneous team of a primary LiDAR-equipped UAV and a secondary micro-scale camera-equipped UAV. We propose a novel cooperative approach combining LiDAR relative localization data with VIO output on board the primary UAV to obtain an accurate pose of the secondary UAV. The pose estimate is used to precisely and reliably guide the secondary UAV along trajectories defined in the primary UAV reference frame. The experimental evaluation has shown the superior accuracy of our method to the raw VIO output, reaching the average 3D Absolute Trajectory Error (ATE) of 0.28 m, and demonstrated its capability to guide the secondary UAV along desired trajectories while mitigating VIO drift. Thus, such a heterogeneous system can explore large areas with LiDAR precision, as well as visit locations inaccessible to the large LiDAR-carrying UAV platforms, as was showcased in a real-world cooperative mapping scenario.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合视觉惯性里程计与激光雷达相对定位的微小型飞行器协同引导方法</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的相对定位方法，用于通过装备精良的空中机器人引导微小型无人机，该方法融合了视觉惯性里程计与激光雷达技术。基于激光雷达的定位在恶劣环境条件下具有高精度和鲁棒性，但三维激光雷达相对较重，需要较大的无人机平台，而轻量级相机则更为灵活。然而，基于视觉的自定位方法精度较低，且易在全局参考系中产生显著漂移。为综合利用两种传感模式的优势，本研究聚焦于异构无人机团队的协同导航，其中主无人机配备激光雷达，从无人机为微小型相机平台。我们提出了一种创新的协同方法，将激光雷达相对定位数据与主无人机上的VIO输出相结合，以获取从无人机的精确位姿。该位姿估计用于在主无人机参考系中精确可靠地引导从无人机沿预定轨迹飞行。实验评估表明，本方法相较于原始VIO输出具有更优精度，平均三维绝对轨迹误差达到0.28米，并证明了其在抑制VIO漂移的同时引导从无人机沿期望轨迹飞行的能力。因此，此类异构系统既能以激光雷达精度探索广阔区域，又能访问大型激光雷达无人机平台无法抵达的位置，这在实际协同建图场景中得到了验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of visual-inertial odometry (VIO) for micro-scale UAVs, which is lightweight but prone to drift, and LiDAR-based localization, which is accurate but requires larger platforms. The proposed method enables cooperative navigation by fusing LiDAR-based relative localization data from a primary, well-equipped UAV with the VIO output of a secondary micro-scale UAV to compute the secondary agent&#x27;s accurate pose. Experimental results demonstrate that this fusion significantly reduces drift, achieving an average 3D Absolute Trajectory Error of 0.28 meters and enabling precise trajectory following for the micro-UAV, as validated in a real-world cooperative mapping scenario.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决基于视觉惯性里程计（VIO）的微型无人机定位存在的漂移和精度不足问题，以及在其小型平台上安装重型3D激光雷达的不现实性。所提出的方法通过融合来自大型主无人机的激光雷达相对定位数据与微型从无人机的VIO输出，以实现协同导航，从而计算出从机的高精度位姿。实验结果表明，该方法相比原始VIO具有更高的精度，达到了0.28米的平均三维绝对轨迹误差，并能有效引导微型无人机沿期望轨迹飞行、抑制漂移，这在一个真实世界的协同建图场景中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">MeCo: Enhancing LLM-Empowered Multi-Robot Collaboration via Similar Task Memoization</div>
<div class="meta-line">Authors: Baiqing Wang, Helei Cui, Bo Zhang, Xiaolong Zheng, Bin Guo, Zhiwen Yu</div>
<div class="meta-line">First: 2026-01-28T13:15:58+00:00 · Latest: 2026-01-28T13:15:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20577v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20577v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-robot systems have been widely deployed in real-world applications, providing significant improvements in efficiency and reductions in labor costs. However, most existing multi-robot collaboration methods rely on extensive task-specific training, which limits their adaptability to new or diverse scenarios. Recent research leverages the language understanding and reasoning capabilities of large language models (LLMs) to enable more flexible collaboration without specialized training. Yet, current LLM-empowered approaches remain inefficient: when confronted with identical or similar tasks, they must replan from scratch because they omit task-level similarities. To address this limitation, we propose MeCo, a similarity-aware multi-robot collaboration framework that applies the principle of ``cache and reuse&#x27;&#x27; (a.k.a., memoization) to reduce redundant computation. Unlike simple task repetition, identifying and reusing solutions for similar but not identical tasks is far more challenging, particularly in multi-robot settings. To this end, MeCo introduces a new similarity testing method that retrieves previously solved tasks with high relevance, enabling effective plan reuse without re-invoking LLMs. Furthermore, we present MeCoBench, the first benchmark designed to evaluate performance on similar-task collaboration scenarios. Experimental results show that MeCo substantially reduces planning costs and improves success rates compared with state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MeCo：通过相似任务记忆化增强大语言模型赋能的机器人协作</div>
<div class="mono" style="margin-top:8px">多机器人系统已在现实应用中广泛部署，显著提升了效率并降低了人力成本。然而，现有协作方法多依赖大量任务特定训练，限制了其对新场景的适应性。近期研究利用大语言模型的语言理解与推理能力，实现了无需专门训练的灵活协作。但现有方法效率仍不足：面对相同或相似任务时，因忽略任务级相似性而需从头重新规划。为此，我们提出MeCo——一个基于相似性感知的多机器人协作框架，应用“缓存与复用”（即记忆化）原则以减少冗余计算。与简单任务重复不同，识别并复用相似但不相同任务的解决方案更具挑战性，尤其在多机器人场景中。为此，MeCo提出新的相似性检测方法，可检索高相关度的已解决任务，实现无需重新调用大语言模型的有效规划复用。此外，我们推出首个评估相似任务协作性能的基准测试集MeCoBench。实验表明，相较于前沿方法，MeCo显著降低了规划成本并提高了任务成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the inefficiency of current LLM-empowered multi-robot systems, which must replan from scratch for similar tasks, ignoring task-level similarities and incurring redundant computation. To address this, the proposed MeCo framework introduces a similarity-aware collaboration method that applies memoization, featuring a novel similarity testing technique to retrieve and reuse plans from previously solved, highly relevant tasks without re-invoking the LLM. Experimental results on the newly introduced MeCoBench benchmark demonstrate that MeCo significantly reduces planning costs and improves success rates compared to state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">本研究针对当前基于大语言模型的多机器人系统在处理相似任务时需从头重新规划的低效问题，提出了MeCo框架，该框架通过记忆化机制重用相似历史任务的解决方案。该方法引入了一种新颖的相似性测试技术来检索和适配相关的过往计划，从而避免冗余的大模型调用，并使用新基准MeCoBench进行评估。实验结果表明，与现有先进方法相比，MeCo能显著降低规划成本并提高任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Progressive-Resolution Policy Distillation: Leveraging Coarse-Resolution Simulations for Time-Efficient Fine-Resolution Policy Learning</div>
<div class="meta-line">Authors: Yuki Kadokawa, Hirotaka Tahara, Takamitsu Matsubara</div>
<div class="meta-line">Venue: IEEE Transactions on Automation Science and Engineering 2025</div>
<div class="meta-line">First: 2024-12-10T12:50:25+00:00 · Latest: 2026-01-28T13:14:30+00:00</div>
<div class="meta-line">Comments: accepted for IEEE Transactions on Automation Science and Engineering (T-ASE)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.07477v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.07477v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yuki-kadokawa.github.io/prpd/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In earthwork and construction, excavators often encounter large rocks mixed with various soil conditions, requiring skilled operators. This paper presents a framework for achieving autonomous excavation using reinforcement learning (RL) through a rock excavation simulator. In the simulation, resolution can be defined by the particle size/number in the whole soil space. Fine-resolution simulations closely mimic real-world behavior but demand significant calculation time and challenging sample collection, while coarse-resolution simulations enable faster sample collection but deviate from real-world behavior. To combine the advantages of both resolutions, we explore using policies developed in coarse-resolution simulations for pre-training in fine-resolution simulations. To this end, we propose a novel policy learning framework called Progressive-Resolution Policy Distillation (PRPD), which progressively transfers policies through some middle-resolution simulations with conservative policy transfer to avoid domain gaps that could lead to policy transfer failure. Validation in a rock excavation simulator and nine real-world rock environments demonstrated that PRPD reduced sampling time to less than 1/7 while maintaining task success rates comparable to those achieved through policy learning in a fine-resolution simulation. Additional videos and supplementary results are available on our project page: https://yuki-kadokawa.github.io/prpd/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>渐进分辨率策略蒸馏：利用粗分辨率仿真实现高效细分辨率策略学习</div>
<div class="mono" style="margin-top:8px">在土方工程与建筑施工中，挖掘机常需处理混杂于复杂土质条件中的大型岩石，这对操作人员技能提出较高要求。本文提出一种基于强化学习的自主挖掘框架，通过岩石挖掘仿真器实现。仿真中的分辨率可由整个土壤空间中颗粒尺寸/数量定义：细分辨率仿真能高度还原真实场景行为，但计算耗时显著且样本采集困难；粗分辨率仿真虽能快速采集样本，却与真实行为存在偏差。为融合两种分辨率的优势，本研究探索将粗分辨率仿真中训练的策略用于细分辨率仿真的预训练。为此，我们提出名为“渐进分辨率策略蒸馏”的新型策略学习框架，通过若干中间分辨率仿真逐步迁移策略，并采用保守策略迁移机制以避免因领域差异导致的策略迁移失效。在岩石挖掘仿真器及九种真实岩石环境中的验证表明，该框架在保持与细分辨率仿真策略学习相当任务成功率的同时，将采样时间缩短至原时间的1/7以下。更多视频与补充结果详见项目页面：https://yuki-kadokawa.github.io/prpd/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of autonomous excavation in complex soil conditions by leveraging reinforcement learning in a simulation environment where fine-resolution simulations are accurate but computationally expensive, while coarse-resolution ones are faster but less realistic. The authors propose Progressive-Resolution Policy Distillation (PRPD), a framework that progressively transfers policies from coarse to fine resolution through intermediate steps with conservative policy updates to mitigate domain gaps. Experimental validation in a rock excavation simulator and nine real-world environments showed that PRPD reduced sampling time to less than one-seventh while achieving task success rates comparable to training solely in fine-resolution simulation.</div>
<div class="mono" style="margin-top:8px">本研究针对复杂土壤条件下自主挖掘的挑战，利用强化学习，其中高分辨率模拟准确但计算成本高，而低分辨率模拟更快但真实性较差。作者提出了渐进式分辨率策略蒸馏（PRPD）框架，通过中间分辨率模拟以保守转移方式逐步将策略从低分辨率迁移到高分辨率，以减轻领域差距。在岩石挖掘模拟器和九个真实环境中的实验验证表明，PRPD将采样时间减少到七分之一以下，同时保持了与仅在高分辨率模拟中学习相当的任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Vibro-Sense: Robust Vibration-based Impulse Response Localization and Trajectory Tracking for Robotic Hands</div>
<div class="meta-line">Authors: Wadhah Zai El Amri, Nicolás Navarro-Guerrero</div>
<div class="meta-line">First: 2026-01-28T12:49:39+00:00 · Latest: 2026-01-28T12:49:39+00:00</div>
<div class="meta-line">Comments: Under Review: Springer Autonomous Robots Journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20555v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20555v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rich contact perception is crucial for robotic manipulation, yet traditional tactile skins remain expensive and complex to integrate. This paper presents a scalable alternative: high-accuracy whole-body touch localization via vibro-acoustic sensing. By equipping a robotic hand with seven low-cost piezoelectric microphones and leveraging an Audio Spectrogram Transformer, we decode the vibrational signatures generated during physical interaction. Extensive evaluation across stationary and dynamic tasks reveals a localization error of under 5 mm in static conditions. Furthermore, our analysis highlights the distinct influence of material properties: stiff materials (e.g., metal) excel in impulse response localization due to sharp, high-bandwidth responses, whereas textured materials (e.g., wood) provide superior friction-based features for trajectory tracking. The system demonstrates robustness to the robot&#x27;s own motion, maintaining effective tracking even during active operation. Our primary contribution is demonstrating that complex physical contact dynamics can be effectively decoded from simple vibrational signals, offering a viable pathway to widespread, affordable contact perception in robotics. To accelerate research, we provide our full datasets, models, and experimental setups as open-source resources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vibro-Sense：基于振动的鲁棒脉冲响应定位与机械手轨迹跟踪技术</div>
<div class="mono" style="margin-top:8px">丰富的接触感知对机器人操控至关重要，但传统触觉皮肤仍存在成本高昂、集成复杂的问题。本文提出一种可扩展的替代方案：通过振动声学传感实现高精度全身触觉定位。通过为机械手配备七个低成本压电麦克风并利用音频频谱变换器，我们解码物理交互过程中产生的振动特征。在静态与动态任务中的广泛评估表明，静态条件下的定位误差低于5毫米。进一步分析揭示了材料特性的显著影响：刚性材料（如金属）因具有尖锐的高带宽响应而在脉冲响应定位中表现优异，而纹理材料（如木材）则为轨迹跟踪提供更优的摩擦特征。该系统对机器人自身运动具有鲁棒性，即使在主动操作期间仍能保持有效跟踪。我们的核心贡献在于证明：复杂物理接触动力学可通过简单振动信号有效解码，为机器人领域实现广泛、经济的接触感知提供了可行路径。为加速研究，我们将完整数据集、模型及实验设置作为开源资源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high cost and integration complexity of traditional tactile skins for robotic manipulation, this work proposes a scalable vibro-acoustic sensing approach for whole-body touch localization. The method equips a robotic hand with seven low-cost piezoelectric microphones and employs an Audio Spectrogram Transformer to decode vibrational signatures from physical interactions. Experimental results show a static localization error under 5 mm, with stiff materials like metal providing sharp impulse responses for localization and textured materials like wood offering superior features for trajectory tracking, all while maintaining robustness to the robot&#x27;s own motion.</div>
<div class="mono" style="margin-top:8px">为解决传统触觉皮肤在机器人操作中成本高、集成复杂的问题，本文提出了Vibro-Sense，一种利用振动声学传感实现全身触觉定位的可扩展系统。该方法在机器人手上安装七个低成本压电麦克风，并采用音频谱图变换器解码物理交互产生的振动特征。实验结果表明，静态定位误差小于5毫米，其中金属等硬质材料因其尖锐、高带宽的响应在脉冲响应定位中表现最佳，而木材等纹理材料则为轨迹跟踪提供了更优的特征；该系统在机器人主动运动时也能保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Listen, Look, Drive: Coupling Audio Instructions for User-aware VLA-based Autonomous Driving</div>
<div class="meta-line">Authors: Ziang Guo, Feng Yang, Xuefeng Zhang, Jiaqi Guo, Kun Zhao, Yixiao Zhou, Peng Lu, Zufeng Zhang, Sifa Zheng</div>
<div class="meta-line">First: 2026-01-17T19:12:34+00:00 · Latest: 2026-01-28T12:10:59+00:00</div>
<div class="meta-line">Comments: Accepted by IV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12142v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12142v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Action (VLA) models promise an open-vocabulary interface that can translate perceptual ambiguity into semantically grounded driving decisions, yet they still treat language as a static prior fixed at inference time. As a result, the model must infer continuously shifting objectives from pixels alone, yielding delayed or overly conservative maneuvers. We argue that effective VLAs for autonomous driving need an online channel in which users can influence driving with specific intentions. To this end, we present EchoVLA, a user-aware VLA that couples camera streams with in situ audio instructions. We augment the nuScenes dataset with temporally aligned, intent-specific speech commands generated by converting ego-motion descriptions into synthetic audios. Further, we compose emotional speech-trajectory pairs into a multimodal Chain-of-Thought (CoT) for fine-tuning a Multimodal Large Model (MLM) based on Qwen2.5-Omni. Specifically, we synthesize the audio-augmented dataset with different emotion types paired with corresponding driving behaviors, leveraging the emotional cues embedded in tone, pitch, and speech tempo to reflect varying user states, such as urgent or hesitant intentions, thus enabling our EchoVLA to interpret not only the semantic content but also the emotional context of audio commands for more nuanced and emotionally adaptive driving behavior. In open-loop benchmarks, our approach reduces the average L2 error by $59.4\%$ and the collision rate by $74.4\%$ compared to the baseline of vision-only perception. More experiments on nuScenes dataset validate that EchoVLA not only steers the trajectory through audio instructions, but also modulates driving behavior in response to the emotions detected in the user&#x27;s speech.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>听、看、驾驶：基于用户感知的视觉语言动作模型耦合音频指令的自动驾驶</div>
<div class="mono" style="margin-top:8px">视觉语言动作（VLA）模型有望通过开放词汇接口将感知模糊性转化为语义明确的驾驶决策，但其仍将语言视为推理时固定的静态先验。这导致模型仅能从像素推断持续变化的目标，产生延迟或过度保守的驾驶行为。我们认为，有效的自动驾驶VLA需要用户能通过在线通道以具体意图影响驾驶。为此，我们提出EchoVLA——一种耦合摄像头流与实时音频指令的用户感知VLA。我们通过将自车运动描述转换为合成音频，为nuScenes数据集增加了时间对齐的意图特定语音指令。进一步，我们将情感语音-轨迹对组合成多模态思维链（CoT），用于基于Qwen2.5-Omni的多模态大模型（MLM）微调。具体而言，我们合成音频增强数据集，将不同情感类型与对应驾驶行为配对，利用音调、音高和语速中嵌入的情感线索反映用户状态（如紧急或犹豫意图），使EchoVLA不仅能解析音频指令的语义内容，还能理解其情感语境，实现更细腻且情感自适应的驾驶行为。在开环基准测试中，相比纯视觉感知基线，我们的方法将平均L2误差降低59.4%，碰撞率降低74.4%。在nuScenes数据集上的更多实验验证，EchoVLA不仅能通过音频指令引导轨迹，还能根据检测到的用户语音情感调节驾驶行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Vision Language Action (VLA) models in autonomous driving, which treat language as a static prior and struggle to infer real-time user intentions from visual data alone, leading to delayed or overly conservative maneuvers. The proposed method, EchoVLA, couples camera streams with in-situ audio instructions; it augments the nuScenes dataset with synthetic, intent-specific speech commands and uses a multimodal Chain-of-Thought (CoT) approach to fine-tune a Qwen2.5-Omni-based Multimodal Large Model (MLM), enabling interpretation of both semantic content and emotional cues like tone and pitch for adaptive driving. Experimental results on nuScenes show that EchoVLA reduces the average L2 error by 59.4% and the collision rate by 74.4% compared to a vision-only baseline, effectively steering trajectories and modulating behavior based on detected speech emotions.</div>
<div class="mono" style="margin-top:8px">本研究针对自动驾驶中视觉语言动作（VLA）模型将语言视为静态先验、导致延迟或保守操作的问题。提出的EchoVLA方法将摄像头流与实时音频指令耦合，通过合成带有情感的语音命令增强nuScenes数据集，并利用多模态思维链微调基于Qwen2.5-Omni的模型，以同时解析语义和情感上下文。实验结果表明，与仅视觉基线相比，EchoVLA将平均L2误差降低了59.4%，碰撞率降低了74.4%，验证了其有效轨迹引导和情感自适应驾驶行为。</div>
</details>
</div>
<div class="card">
<div class="title">A Practical Framework of Key Performance Indicators for Multi-Robot Lunar and Planetary Field Tests</div>
<div class="meta-line">Authors: Julia Richter, David Oberacker, Gabriela Ligeza, Valentin T. Bickel, Philip Arm, William Talbot, Marvin Grosse Besselmann, Florian Kehl, Tristan Schnell, Hendrik Kolvenbach, Rüdiger Dillmann, Arne Roennau, Marco Hutter</div>
<div class="meta-line">First: 2026-01-28T12:10:56+00:00 · Latest: 2026-01-28T12:10:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20529v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20529v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic prospecting for critical resources on the Moon, such as ilmenite, rare earth elements, and water ice, requires robust exploration methods given the diverse terrain and harsh environmental conditions. Although numerous analog field trials address these goals, comparing their results remains challenging because of differences in robot platforms and experimental setups. These missions typically assess performance using selected, scenario-specific engineering metrics that fail to establish a clear link between field performance and science-driven objectives. In this paper, we address this gap by deriving a structured framework of KPI from three realistic multi-robot lunar scenarios reflecting scientific objectives and operational constraints. Our framework emphasizes scenario-dependent priorities in efficiency, robustness, and precision, and is explicitly designed for practical applicability in field deployments. We validated the framework in a multi-robot field test and found it practical and easy to apply for efficiency- and robustness-related KPI, whereas precision-oriented KPI require reliable ground-truth data that is not always feasible to obtain in outdoor analog environments. Overall, we propose this framework as a common evaluation standard enabling consistent, goal-oriented comparison of multi-robot field trials and supporting systematic development of robotic systems for future planetary exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多机器人月球与行星实地测试的关键性能指标实用框架</div>
<div class="mono" style="margin-top:8px">月球关键资源（如钛铁矿、稀土元素、水冰）的机器人勘探需适应复杂地形与恶劣环境，因此对探测方法提出更高要求。尽管已有大量模拟实地试验，但因机器人平台与实验设置的差异，结果对比仍具挑战性。现有任务多采用特定场景的工程指标评估性能，未能建立实地表现与科学目标间的明确关联。本文通过三个反映科学目标与操作约束的多机器人月球场景，构建结构化KPI框架以填补此空白。该框架强调效率、鲁棒性与精度在场景中的优先级，专为实地部署的实用性设计。在多机器人实地测试中验证表明：框架对效率和鲁棒性相关KPI实用易行，而精度导向的KPI需依赖可靠地面真值数据——这在户外模拟环境中常难以获取。总体而言，本框架可作为通用评估标准，实现多机器人实地试验的一致化、目标导向对比，并为未来行星探测机器人系统的系统化开发提供支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable consistent and goal-oriented comparison of multi-robot field trials for lunar and planetary exploration, this research addresses the challenge of evaluating diverse analog missions with different platforms and metrics. The authors derive a structured framework of Key Performance Indicators (KPIs) from three realistic multi-robot lunar scenarios, explicitly prioritizing efficiency, robustness, and precision based on scenario-dependent scientific and operational goals. Validation through a multi-robot field test found the framework practical for efficiency and robustness KPIs, though precision KPIs were limited by the difficulty of obtaining reliable ground-truth data in outdoor analog environments.</div>
<div class="mono" style="margin-top:8px">本研究旨在为多机器人月球和行星现场测试建立一个标准化的评估框架，以解决现有各类模拟试验因使用不同平台和特定场景指标而难以比较的问题，这些指标通常与科学目标缺乏明确关联。该方法通过从三个现实的多机器人月球场景中推导出一套结构化的关键绩效指标，强调效率、鲁棒性和精度方面的场景依赖性优先级，以适用于实际现场部署。在多机器人现场测试中的实验验证表明，该框架对于效率和鲁棒性相关的关键绩效指标实用且易于应用，但精度导向的指标则受限于在户外模拟环境中获取可靠地面真实数据的困难。</div>
</details>
</div>
<div class="card">
<div class="title">STORM: Slot-based Task-aware Object-centric Representation for robotic Manipulation</div>
<div class="meta-line">Authors: Alexandre Chapin, Emmanuel Dellandréa, Liming Chen</div>
<div class="meta-line">First: 2026-01-28T08:46:04+00:00 · Latest: 2026-01-28T08:46:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20381v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20381v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual foundation models provide strong perceptual features for robotics, but their dense representations lack explicit object-level structure, limiting robustness and contractility in manipulation tasks. We propose STORM (Slot-based Task-aware Object-centric Representation for robotic Manipulation), a lightweight object-centric adaptation module that augments frozen visual foundation models with a small set of semantic-aware slots for robotic manipulation. Rather than retraining large backbones, STORM employs a multi-phase training strategy: object-centric slots are first stabilized through visual--semantic pretraining using language embeddings, then jointly adapted with a downstream manipulation policy. This staged learning prevents degenerate slot formation and preserves semantic consistency while aligning perception with task objectives. Experiments on object discovery benchmarks and simulated manipulation tasks show that STORM improves generalization to visual distractors, and control performance compared to directly using frozen foundation model features or training object-centric representations end-to-end. Our results highlight multi-phase adaptation as an efficient mechanism for transforming generic foundation model features into task-aware object-centric representations for robotic control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STORM：面向机器人操作的基于槽位的任务感知物体中心表征</div>
<div class="mono" style="margin-top:8px">视觉基础模型为机器人学提供了强大的感知特征，但其稠密表征缺乏显式的物体级结构，限制了操作任务的鲁棒性与可组合性。我们提出STORM（面向机器人操作的基于槽位的任务感知物体中心表征），这是一种轻量级的物体中心适配模块，通过为机器人操作引入少量语义感知槽位来增强冻结的视觉基础模型。STORM采用多阶段训练策略而非重训练大型骨干网络：首先通过语言嵌入的视觉-语义预训练稳定物体中心槽位，随后与下游操作策略联合适配。这种分阶段学习避免了槽位退化形成，在保持语义一致性的同时将感知与任务目标对齐。在物体发现基准和仿真操作任务上的实验表明，相较于直接使用冻结基础模型特征或端到端训练物体中心表征，STORM能提升对视觉干扰的泛化能力与控制性能。我们的研究结果凸显了多阶段适配作为一种高效机制，可将通用基础模型特征转化为面向机器人控制的任务感知物体中心表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual foundation models offer rich perceptual features for robotics but lack explicit object-level structure, which hampers robustness and interpretability in manipulation tasks. To address this, STORM introduces a lightweight object-centric adaptation module that enhances frozen visual foundation models with a small set of semantic-aware slots through a multi-phase training strategy: first stabilizing slots via visual-semantic pretraining with language embeddings, then jointly adapting them with a downstream manipulation policy. Experiments on object discovery and simulated manipulation tasks demonstrate that STORM improves generalization to visual distractors and achieves better control performance compared to using frozen foundation model features directly or training object-centric representations end-to-end.</div>
<div class="mono" style="margin-top:8px">视觉基础模型为机器人学提供了丰富的感知特征，但缺乏显式的物体级结构，这限制了操作任务的鲁棒性和可控性。为此，STORM提出了一种轻量级的以物体为中心的适配模块，通过多阶段训练策略，利用语义感知的槽来增强冻结的基础模型：首先通过语言嵌入的视觉-语义预训练稳定槽，然后与下游操作策略联合适配。在物体发现基准和模拟操作任务上的实验表明，与直接使用冻结特征或端到端训练物体中心表示相比，STORM提升了对视觉干扰的泛化能力，并改善了控制性能。</div>
</details>
</div>
<div class="card">
<div class="title">RF-MatID: Dataset and Benchmark for Radio Frequency Material Identification</div>
<div class="meta-line">Authors: Xinyan Chen, Qinchun Li, Ruiqin Ma, Jiaqi Bai, Li Yi, Jianfei Yang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-28T08:43:48+00:00 · Latest: 2026-01-28T08:43:48+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20377v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20377v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate material identification plays a crucial role in embodied AI systems, enabling a wide range of applications. However, current vision-based solutions are limited by the inherent constraints of optical sensors, while radio-frequency (RF) approaches, which can reveal intrinsic material properties, have received growing attention. Despite this progress, RF-based material identification remains hindered by the lack of large-scale public datasets and the limited benchmarking of learning-based approaches. In this work, we present RF-MatID, the first open-source, large-scale, wide-band, and geometry-diverse RF dataset for fine-grained material identification. RF-MatID includes 16 fine-grained categories grouped into 5 superclasses, spanning a broad frequency range from 4 to 43.5 GHz, and comprises 142k samples in both frequency- and time-domain representations. The dataset systematically incorporates controlled geometry perturbations, including variations in incidence angle and stand-off distance. We further establish a multi-setting, multi-protocol benchmark by evaluating state-of-the-art deep learning models, assessing both in-distribution performance and out-of-distribution robustness under cross-angle and cross-distance shifts. The 5 frequency-allocation protocols enable systematic frequency- and region-level analysis, thereby facilitating real-world deployment. RF-MatID aims to enable reproducible research, accelerate algorithmic advancement, foster cross-domain robustness, and support the development of real-world application in RF-based material identification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RF-MatID：面向射频材料识别的数据集与基准</div>
<div class="mono" style="margin-top:8px">精确的材料识别在具身人工智能系统中具有关键作用，可支持广泛的应用场景。然而，当前基于视觉的解决方案受限于光学传感器的固有约束，而能够揭示材料本征特性的射频方法正受到日益关注。尽管取得进展，射频材料识别仍因缺乏大规模公开数据集和基于学习方法的有限基准评估而受阻。本研究提出RF-MatID——首个面向细粒度材料识别的开源、大规模、宽频带、几何多样化的射频数据集。该数据集涵盖5个超类下的16个细粒度类别，频率范围覆盖4至43.5 GHz，包含频域与时域表征的14.2万个样本，并系统性地纳入入射角与探测距离等受控几何扰动。通过评估前沿深度学习模型，我们建立了多场景、多协议的基准测试框架，评估模型在分布内性能及跨角度/跨距离偏移下的分布外鲁棒性。5种频率分配协议支持系统性的频率级与区域级分析，有助于实际部署。RF-MatID旨在推动可复现研究、加速算法发展、增强跨领域鲁棒性，并支持射频材料识别的实际应用开发。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitations of vision-based material identification and the lack of large-scale public datasets for radio-frequency (RF) approaches, this work introduces RF-MatID, a comprehensive dataset for fine-grained RF material identification. The method involves constructing a large-scale, wide-band dataset with 142k samples across 16 material categories, incorporating systematic geometry perturbations like angle and distance variations, and establishing a benchmark using state-of-the-art deep learning models evaluated under multiple protocols. Key experimental findings demonstrate the benchmark&#x27;s assessment of in-distribution performance and out-of-distribution robustness to cross-angle and cross-distance shifts, facilitating systematic frequency analysis and supporting real-world deployment.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于视觉材料识别方法的局限性，以及射频传感在揭示材料内在特性方面的潜力，但当前缺乏大规模公开数据集和全面的基准测试阻碍了其发展。方法上，研究构建了RF-MatID开源数据集，包含142k个样本，涵盖16个细粒度材料类别，频率范围4至43.5 GHz，并系统引入了入射角度和距离等几何扰动。主要实验结果通过对最先进深度学习模型进行基准测试得出，评估了模型在分布内性能以及跨角度和跨距离偏移下的分布外鲁棒性，并通过五种频率分配协议支持系统性分析。</div>
</details>
</div>
<div class="card">
<div class="title">Demonstration-Free Robotic Control via LLM Agents</div>
<div class="meta-line">Authors: Brian Y. Tsui, Alan Y. Fang, Tiffany J. Hwu</div>
<div class="meta-line">First: 2026-01-28T07:49:35+00:00 · Latest: 2026-01-28T07:49:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20334v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20334v1">PDF</a> · <a href="https://github.com/robiemusketeer/faea-sim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation has increasingly adopted vision-language-action (VLA) models, which achieve strong performance but typically require task-specific demonstrations and fine-tuning, and often generalize poorly under domain shift. We investigate whether general-purpose large language model (LLM) agent frameworks, originally developed for software engineering, can serve as an alternative control paradigm for embodied manipulation. We introduce FAEA (Frontier Agent as Embodied Agent), which applies an LLM agent framework directly to embodied manipulation without modification. Using the same iterative reasoning that enables software agents to debug code, FAEA enables embodied agents to reason through manipulation strategies. We evaluate an unmodified frontier agent, Claude Agent SDK, across the LIBERO, ManiSkill3, and MetaWorld benchmarks. With privileged environment state access, FAEA achieves success rates of 84.9%, 85.7%, and 96%, respectively. This level of task success approaches that of VLA models trained with less than 100 demonstrations per task, without requiring demonstrations or fine-tuning. With one round of human feedback as an optional optimization, performance increases to 88.2% on LIBERO. This demonstration-free capability has immediate practical value: FAEA can autonomously explore novel scenarios in simulation and generate successful trajectories for training data augmentation in embodied learning. Our results indicate that general-purpose agents are sufficient for a class of manipulation tasks dominated by deliberative, task-level planning. This opens a path for robotics systems to leverage actively maintained agent infrastructure and benefit directly from ongoing advances in frontier models. Code is available at https://github.com/robiemusketeer/faea-sim</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型智能体的免演示机器人控制</div>
<div class="mono" style="margin-top:8px">机器人操作领域日益采用视觉-语言-动作模型，虽性能优异但通常需要任务特定演示与微调，且在领域迁移时泛化能力不足。本研究探讨了原为软件工程开发的通用大语言模型智能体框架，能否作为具身操作的控制范式替代方案。我们提出FAEA（前沿智能体作为具身智能体），将LLM智能体框架直接应用于具身操作而无需修改。借助软件智能体调试代码的迭代推理机制，FAEA使具身智能体能够推演操作策略。我们在LIBERO、ManiSkill3和MetaWorld基准测试中评估了未经修改的前沿智能体Claude Agent SDK。在获取环境特权状态条件下，FAEA分别取得84.9%、85.7%和96%的成功率。该任务成功率接近每任务需少于100次演示训练的VLA模型水平，且无需演示或微调。通过单轮人工反馈作为可选优化，LIBERO任务性能提升至88.2%。这种免演示能力具有直接实用价值：FAEA可在仿真中自主探索新场景，并为具身学习训练数据增强生成成功轨迹。结果表明通用智能体足以应对以审慎任务级规划为主导的操作任务类别，为机器人系统利用持续维护的智能体基础设施、直接受益于前沿模型进展开辟了新路径。代码发布于https://github.com/robiemusketeer/faea-sim</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of vision-language-action models in robotic manipulation, which often require task-specific demonstrations and fine-tuning and struggle with domain generalization. The study proposes FAEA (Frontier Agent as Embodied Agent), which directly applies an unmodified general-purpose large language model agent framework, originally designed for software engineering, to embodied control, enabling iterative reasoning for manipulation strategies. Experimental evaluation on LIBERO, ManiSkill3, and MetaWorld benchmarks shows success rates of 84.9%, 85.7%, and 96% with privileged state access, approaching the performance of demonstration-trained models without needing demonstrations or fine-tuning, and performance improves to 88.2% on LIBERO with optional human feedback, demonstrating practical value for autonomous exploration and data augmentation.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人领域视觉-语言-动作（VLA）模型通常需要任务特定演示、微调且在领域变化下泛化能力不足的局限性展开。研究探讨了原本为软件工程设计的、未经修改的通用大语言模型（LLM）智能体框架，能否作为具身操作的一种替代控制范式。所提出的方法，即前沿智能体作为具身智能体（FAEA），直接应用此类LLM智能体框架，使其能够像软件智能体调试代码一样，通过迭代推理来制定操作策略。在拥有环境状态特权访问的条件下，于LIBERO、ManiSkill3和MetaWorld基准测试上进行评估，FAEA分别取得了84.9%、85.7%和96%的成功率，其性能接近那些每个任务使用少于100次演示进行训练的VLA模型，但本身无需任何演示或微调。在引入一轮可选的人类反馈优化后，在LIBERO上的性能进一步提升至88.2%，这证明了该系统在仿真中进行自主探索以及生成训练数据以增强具身学习的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Tactile-Force Alignment in Vision-Language-Action Models for Force-aware Manipulation</div>
<div class="meta-line">Authors: Yuzhe Huang, Pei Lin, Wanlin Li, Daohan Li, Jiajun Li, Jiaming Jiang, Chenxi Xiao, Ziyuan Jiao</div>
<div class="meta-line">First: 2026-01-28T07:34:41+00:00 · Latest: 2026-01-28T07:34:41+00:00</div>
<div class="meta-line">Comments: 17pages,9fig</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20321v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20321v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently emerged as powerful generalists for robotic manipulation. However, due to their predominant reliance on visual modalities, they fundamentally lack the physical intuition required for contact-rich tasks that require precise force regulation and physical reasoning. Existing attempts to incorporate vision-based tactile sensing into VLA models typically treat tactile inputs as auxiliary visual textures, thereby overlooking the underlying correlation between surface deformation and interaction dynamics. To bridge this gap, we propose a paradigm shift from tactile-vision alignment to tactile-force alignment. Here, we introduce TaF-VLA, a framework that explicitly grounds high-dimensional tactile observations in physical interaction forces. To facilitate this, we develop an automated tactile-force data acquisition device and curate the TaF-Dataset, comprising over 10 million synchronized tactile observations, 6-axis force/torque, and matrix force map. To align sequential tactile observations with interaction forces, the central component of our approach is the Tactile-Force Adapter (TaF-Adapter), a tactile sensor encoder that extracts discretized latent information for encoding tactile observations. This mechanism ensures that the learned representations capture history-dependent, noise-insensitive physical dynamics rather than static visual textures. Finally, we integrate this force-aligned encoder into a VLA backbone. Extensive real-world experiments demonstrate that TaF-VLA policy significantly outperforms state-of-the-art tactile-vision-aligned and vision-only baselines on contact-rich tasks, verifying its ability to achieve robust, force-aware manipulation through cross-modal physical reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言-动作模型中的触觉-力对齐实现力感知操控</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型近期已成为机器人操控领域强大的通用框架。然而，由于其主要依赖视觉模态，这类模型本质上缺乏对需要精确力调节与物理推理的密集接触任务所需的物理直觉。现有将基于视觉的触觉传感融入VLA模型的尝试通常将触觉输入视为辅助视觉纹理，从而忽视了表面形变与交互动力学之间的内在关联。为弥补这一不足，我们提出从触觉-视觉对齐转向触觉-力对齐的范式转变。本文介绍TaF-VLA框架，该框架将高维触觉观测显式关联至物理交互力。为此，我们开发了自动化触觉-力数据采集装置，并构建了包含超1000万组同步触觉观测、六维力/力矩及矩阵力图的TaF数据集。为实现序列化触觉观测与交互力的对齐，我们提出核心组件触觉-力适配器（TaF-Adapter），该触觉传感器编码器通过提取离散化潜在信息来编码触觉观测。该机制确保学习到的表征能捕获具有历史依赖性、对噪声不敏感的物理动力学特征，而非静态视觉纹理。最终，我们将此力对齐编码器集成至VLA主干网络。大量真实世界实验表明，TaF-VLA策略在密集接触任务上显著优于当前最先进的触觉-视觉对齐及纯视觉基线模型，验证了其通过跨模态物理推理实现鲁棒力感知操控的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models lack physical intuition for contact-rich tasks requiring precise force control. To address this, the authors propose a tactile-force alignment paradigm, introducing the TaF-VLA framework which grounds tactile observations in physical forces using a novel Tactile-Force Adapter encoder trained on a large synchronized dataset of tactile and force data. Experimental results show that TaF-VLA significantly outperforms tactile-vision-aligned and vision-only baselines on contact-rich manipulation tasks, demonstrating robust, force-aware physical reasoning.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型缺乏对需要精确力控的接触式操作任务的物理直觉。为此，研究者提出了一种触觉-力对齐范式，引入了TaF-VLA框架，该框架通过一个新颖的触觉-力适配器编码器，将触觉观测与物理力信息对齐，该编码器基于自动采集的大规模同步触觉与力数据集进行训练。实验结果表明，所得到的策略在接触式任务上显著优于触觉-视觉对齐和纯视觉基线，实现了鲁棒的、具有力感知的操作能力。</div>
</details>
</div>
<div class="card">
<div class="title">Legged Robot State Estimation Using Invariant Neural-Augmented Kalman Filter with a Neural Compensator</div>
<div class="meta-line">Authors: Seokju Lee, Hyun-Bin Kim, Kyung-Soo Kim</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2025-03-01T04:23:57+00:00 · Latest: 2026-01-28T06:25:30+00:00</div>
<div class="meta-line">Comments: 8 pages, 10 figures, Accepted to IROS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.00344v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.00344v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://seokju-lee.github.io/innkf_webpage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents an algorithm to improve state estimation for legged robots. Among existing model-based state estimation methods for legged robots, the contact-aided invariant extended Kalman filter defines the state on a Lie group to preserve invariance, thereby significantly accelerating convergence. It achieves more accurate state estimation by leveraging contact information as measurements for the update step. However, when the model exhibits strong nonlinearity, the estimation accuracy decreases. Such nonlinearities can cause initial errors to accumulate and lead to large drifts over time. To address this issue, we propose compensating for errors by augmenting the Kalman filter with an artificial neural network serving as a nonlinear function approximator. Furthermore, we design this neural network to respect the Lie group structure to ensure invariance, resulting in our proposed Invariant Neural-Augmented Kalman Filter (InNKF). The proposed algorithm offers improved state estimation performance by combining the strengths of model-based and learning-based approaches. Project webpage: https://seokju-lee.github.io/innkf_webpage</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于不变神经增强卡尔曼滤波与神经补偿器的足式机器人状态估计</div>
<div class="mono" style="margin-top:8px">本文提出一种改进足式机器人状态估计算法。现有基于模型的状态估计方法中，接触辅助不变扩展卡尔曼滤波器将状态定义在李群上以保持不变性，从而显著加速收敛，并通过接触信息作为更新步骤的测量值实现更精确的状态估计。然而，当模型呈现强非线性时，估计精度会下降，此类非线性可能导致初始误差累积并随时间产生大幅漂移。为解决该问题，我们提出通过人工神经网络作为非线性函数逼近器增强卡尔曼滤波器以补偿误差，并设计该神经网络遵循李群结构以确保不变性，由此构建出所提出的不变神经增强卡尔曼滤波器。该算法结合基于模型与基于学习方法的优势，提升了状态估计性能。项目网页：https://seokju-lee.github.io/innkf_webpage</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the accuracy degradation in legged robot state estimation when model nonlinearities cause error accumulation and drift over time. The method proposes an Invariant Neural-Augmented Kalman Filter (InNKF) that compensates for errors by augmenting a contact-aided invariant extended Kalman filter with an artificial neural network designed as a nonlinear function approximator that respects the underlying Lie group structure to preserve invariance. Experimental results demonstrate that the proposed algorithm, combining model-based and learning-based approaches, achieves improved state estimation performance compared to the baseline method.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升足式机器人的状态估计精度，针对接触辅助不变扩展卡尔曼滤波器在模型强非线性下精度下降、导致误差累积和漂移的问题。所提出的方法，即不变神经增强卡尔曼滤波器，通过引入一个作为非线性函数逼近器的人工神经网络来补偿误差，该网络设计为尊重李群结构以保持不变性。实验结果表明，这种结合模型驱动与数据驱动的方法，相比基线方法实现了更优的状态估计性能。</div>
</details>
</div>
<div class="card">
<div class="title">SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation</div>
<div class="meta-line">Authors: Hongyi Zhao, Shuo Wang, Qijie He, Ziyuan Pu</div>
<div class="meta-line">First: 2026-01-26T12:53:12+00:00 · Latest: 2026-01-28T06:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18442v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18442v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SG-CADVLM：一种基于上下文感知解码的视觉语言模型，用于安全关键场景生成</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆安全验证需在安全关键场景下进行测试，但此类事件在真实驾驶中罕见且因碰撞风险导致测试成本高昂。事故报告提供了安全关键事件的真实规格，为稀缺的真实碰撞轨迹数据提供了重要替代来源，使其成为通过仿真生成真实高风险场景的宝贵资源。现有方法存在显著局限：数据驱动方法因依赖现有潜在分布而缺乏多样性，对抗性方法则常生成缺乏物理真实性的非现实场景。基于大语言模型（LLM）和视觉语言模型（VLM）的方法展现出巨大潜力，但受上下文抑制问题困扰——内部参数知识覆盖事故规格，导致生成场景偏离实际事故特征。本文提出SG-CADVLM（一种基于上下文感知解码的视觉语言模型，用于安全关键场景生成），该框架集成上下文感知解码与多模态输入处理，能够根据事故报告和路网图生成安全关键场景。该框架缓解了VLM的幻觉问题，同时实现道路几何与车辆轨迹的同步生成。实验结果表明：SG-CADVLM生成关键风险场景的比例达84.4%，而基线方法仅为12.5%，提升了469%，且能生成可用于自动驾驶测试的可执行仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity and high cost of real-world safety-critical scenarios for autonomous vehicle validation, this research leverages crash reports as specifications for generating realistic high-risk simulations. The proposed SG-CADVLM framework integrates Context-Aware Decoding with multi-modal input processing of crash reports and road network diagrams to mitigate vision-language model hallucination and context suppression, enabling the simultaneous generation of road geometry and vehicle trajectories. Experiments show the method generates critical risk scenarios at a rate of 84.4%, a 469% improvement over a baseline rate of 12.5%, while producing executable simulations.</div>
<div class="mono" style="margin-top:8px">为解决自动驾驶安全验证中真实安全关键场景稀缺且测试成本高的问题，本研究利用事故报告作为生成高风险仿真场景的规范。提出的SG-CADVLM框架集成了上下文感知解码与多模态输入处理（事故报告和路网图），以缓解视觉语言模型的幻觉和上下文抑制问题，能够同时生成道路几何和车辆轨迹。实验结果表明，该方法生成关键风险场景的比例达到84.4%，相比基线方法的12.5%提升了469%，并能生成可执行的仿真测试场景。</div>
</details>
</div>
<div class="card">
<div class="title">OmniEVA: Embodied Versatile Planner via Task-Adaptive 3D-Grounded and Embodiment-aware Reasoning</div>
<div class="meta-line">Authors: Yuecheng Liu, Dafeng Chi, Shiguang Wu, Zhanguang Zhang, Yuzheng Zhuang, Bowen Yang, He Zhu, Lingfeng Zhang, Pengwei Xie, David Gamaliel Arcos Bravo, Yingxue Zhang, Jianye Hao, Xingyue Quan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-11T10:32:22+00:00 · Latest: 2026-01-28T05:17:46+00:00</div>
<div class="meta-line">Comments: Published as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09332v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.09332v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://omnieva.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal large language models (MLLMs) have opened new opportunities for embodied intelligence, enabling multimodal understanding, reasoning, and interaction, as well as continuous spatial decision-making. Nevertheless, current MLLM-based embodied systems face two critical limitations. First, Geometric Adaptability Gap: models trained solely on 2D inputs or with hard-coded 3D geometry injection suffer from either insufficient spatial information or restricted 2D generalization, leading to poor adaptability across tasks with diverse spatial demands. Second, Embodiment Constraint Gap: prior work often neglects the physical constraints and capacities of real robots, resulting in task plans that are theoretically valid but practically infeasible. To address these gaps, we introduce OmniEVA -- an embodied versatile planner that enables advanced embodied reasoning and task planning through two pivotal innovations: (1) a Task-Adaptive 3D Grounding mechanism, which introduces a gated router to perform explicit selective regulation of 3D fusion based on contextual requirements, enabling context-aware 3D grounding for diverse embodied tasks. (2) an Embodiment-Aware Reasoning framework that jointly incorporates task goals and embodiment constraints into the reasoning loop, resulting in planning decisions that are both goal-directed and executable. Extensive experimental results demonstrate that OmniEVA not only achieves state-of-the-art general embodied reasoning performance, but also exhibits a strong ability across a wide range of downstream scenarios. Evaluations of a suite of proposed embodied benchmarks, including both primitive and composite tasks, confirm its robust and versatile planning capabilities. Project page: https://omnieva.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniEVA：基于任务自适应三维感知与具身约束推理的通用具身规划器</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）的最新进展为具身智能开辟了新机遇，实现了多模态理解、推理与交互以及连续空间决策。然而，当前基于MLLM的具身系统面临两大关键局限：其一，几何适应性鸿沟——仅基于二维输入训练或采用硬编码三维几何注入的模型，因空间信息不足或二维泛化受限，难以适应不同空间需求的任务；其二，具身约束鸿沟——现有研究常忽略真实机器人的物理约束与能力，导致任务规划理论可行而实践不可行。为弥合这些鸿沟，我们提出OmniEVA——一种通过两项关键创新实现高级具身推理与任务规划的通用具身规划器：（1）任务自适应三维感知机制，引入门控路由器根据上下文需求对三维融合进行显式选择性调控，实现面向多样化具身任务的上下文感知三维感知；（2）具身约束推理框架，将任务目标与具身约束共同纳入推理循环，生成既目标导向又可执行的规划决策。大量实验结果表明，OmniEVA不仅实现了最先进的通用具身推理性能，还在广泛下游场景中展现出强大能力。对包括基础任务与复合任务在内的系列具身基准评估，证实了其稳健且通用的规划能力。项目页面：https://omnieva.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the geometric adaptability gap and embodiment constraint gap in current multimodal large language model (MLLM)-based embodied systems, this paper introduces OmniEVA, an embodied versatile planner. The method employs a task-adaptive 3D grounding mechanism with a gated router for selective 3D fusion and an embodiment-aware reasoning framework that integrates task goals and physical robot constraints. Experimental results on a suite of embodied benchmarks, including primitive and composite tasks, demonstrate state-of-the-art general reasoning performance and robust, versatile planning capabilities across diverse downstream scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决当前基于多模态大语言模型（MLLM）的具身智能系统存在的两个关键局限：几何适应性差距（模型缺乏足够或适当泛化的空间信息）和具身约束差距（规划忽略真实机器人的物理限制）。为此，作者提出了OmniEVA，一个具身通用规划器，其核心创新包括一个带有门控路由器的任务自适应3D接地机制，用于实现上下文感知的3D信息融合，以及一个将任务目标与物理约束联合考虑的具身感知推理框架。在一系列具身基准测试（包括基础任务和复合任务）上的实验结果表明，OmniEVA在通用具身推理性能上达到了最先进水平，并在广泛的下游场景中展现出强大且通用的规划能力。</div>
</details>
</div>
<div class="card">
<div class="title">Shallow-π: Knowledge Distillation for Flow-based VLAs</div>
<div class="meta-line">Authors: Boseong Jeon, Yunho Choi, Taehan Kim</div>
<div class="meta-line">First: 2026-01-28T05:16:26+00:00 · Latest: 2026-01-28T05:16:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20262v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20262v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing demand for real-time robotic deployment necessitates fast and on-device inference for vision-language-action (VLA) models. Within the VLA literature, efficiency has been extensively studied at the token level, such as visual token pruning. In contrast, systematic transformer layer reduction has received limited attention and, to the best of our knowledge, has not been explored for flow-based VLA models under knowledge distillation. In this work, we propose Shallow-pi, a principled knowledge distillation framework that aggressively reduces the transformer depth of both the VLM backbone and the flow-based action head, compressing the model from 18 to 6 layers. Shallow-pi achieves over two times faster inference with less than one percent absolute drop in success rate on standard manipulation benchmarks, establishing state-of-the-art performance among reduced VLA models. Crucially, we validate our approach through industrial-scale real-world experiments on Jetson Orin and Jetson Thor across multiple robot platforms, including humanoid systems, in complex and dynamic manipulation scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Shallow-π：面向流式视觉语言动作模型的知识蒸馏方法</div>
<div class="mono" style="margin-top:8px">实时机器人部署需求的增长，要求视觉语言动作模型具备快速、端侧推理能力。现有研究多聚焦于令牌级效率优化（如视觉令牌剪枝），而系统性减少Transformer层的研究相对有限，尤其在基于流式架构的VLA模型知识蒸馏领域尚未探索。本文提出Shallow-π——一种通过知识蒸馏大幅压缩VLA模型深度的框架，将VLM主干网络与流式动作头的总层数从18层压缩至6层。该方法在标准操作基准测试中实现推理速度提升两倍以上，成功率绝对降幅小于1%，确立了精简VLA模型的性能新标杆。我们通过在Jetson Orin与Jetson Thor平台上开展工业级真实场景验证，涵盖人形机器人等多平台在复杂动态操作任务中的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for efficient real-time robotic deployment, which requires fast on-device inference for vision-language-action (VLA) models, yet systematic transformer layer reduction has been underexplored for flow-based VLAs under knowledge distillation. The method introduces Shallow-π, a knowledge distillation framework that aggressively reduces transformer depth from 18 to 6 layers in both the VLM backbone and flow-based action head. Experimental results show over two times faster inference with less than a 1% absolute drop in success rate on standard benchmarks, achieving state-of-the-art performance among reduced VLAs, and validation on industrial-scale real-world robots confirms effectiveness in complex manipulation scenarios.</div>
<div class="mono" style="margin-top:8px">该研究旨在满足机器人实时部署对高效、快速设备端推理的需求，而现有工作对基于流的视觉-语言-动作模型中系统性的Transformer层压缩在知识蒸馏下的探索不足。方法上提出了Shallow-π，一个知识蒸馏框架，将VLM主干和基于流的动作头的Transformer层数从18层大幅压缩至6层。实验结果表明，在标准操作基准上，推理速度提升超过两倍，成功率绝对下降小于1%，在压缩后的VLA模型中达到领先性能，并在包括人形机器人在内的多个平台上，于复杂动态操作场景中进行了工业级现实世界验证。</div>
</details>
</div>
<div class="card">
<div class="title">Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</div>
<div class="meta-line">Authors: Matthew Lisondra, Beno Benhabib, Goldie Nejat</div>
<div class="meta-line">First: 2025-05-26T20:08:09+00:00 · Latest: 2026-01-28T05:01:06+00:00</div>
<div class="meta-line">Comments: v2: Expanded systematic review; resubmitted to Robotics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20503v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20503v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models, have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interaction, mobile service robots can achieve more flexible understanding, adaptive behavior, and robust task execution in dynamic real-world environments. Despite this progress, embodied AI for mobile service robots continues to face fundamental challenges related to the translation of natural language instructions into executable robot actions, multimodal perception in human-centered environments, uncertainty estimation for safe decision-making, and computational constraints for real-time onboard deployment. In this paper, we present the first systematic review focused specifically on the integration of foundation models in mobile service robotics. We analyze how recent advances in foundation models address these core challenges through language-conditioned control, multimodal sensor fusion, uncertainty-aware reasoning, and efficient model scaling. We further examine real-world applications in domestic assistance, healthcare, and service automation, highlighting how foundation models enable context-aware, socially responsive, and generalizable robot behaviors. Beyond technical considerations, we discuss ethical, societal, and human-interaction implications associated with deploying foundation model-enabled service robots in human environments. Finally, we outline future research directions emphasizing reliability and lifelong adaptation, privacy-aware and resource-constrained deployment, and governance and human-in-the-loop frameworks required for safe, scalable, and trustworthy mobile service robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于基础模型的移动服务机器人具身人工智能：系统性综述</div>
<div class="mono" style="margin-top:8px">基础模型（包括大语言模型、视觉语言模型、多模态大语言模型和视觉语言动作模型）的快速发展为移动服务机器人领域的具身人工智能开辟了新途径。通过将基础模型与具身人工智能原理（智能系统通过物理交互进行感知、推理和行动）相结合，移动服务机器人能够在动态现实环境中实现更灵活的理解、自适应行为和鲁棒的任务执行。尽管取得进展，移动服务机器人的具身人工智能仍面临核心挑战：自然语言指令到可执行机器人动作的转化、以人为中心环境的多模态感知、安全决策的不确定性估计，以及实时机载部署的计算约束。本文首次针对基础模型在移动服务机器人领域的集成进行系统性综述，分析基础模型如何通过语言条件控制、多模态传感器融合、不确定性感知推理和高效模型缩放应对这些挑战。进一步探讨家庭辅助、医疗保健和服务自动化等实际应用，阐明基础模型如何实现情境感知、社会响应和可泛化的机器人行为。除技术考量外，还讨论了在人类环境中部署基于基础模型的服务机器人所涉及的伦理、社会和人机交互影响。最后，展望未来研究方向，强调可靠性及终身适应、隐私保护与资源受限部署，以及构建安全、可扩展、可信赖的移动服务机器人所需的治理与人机协同框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid progress of foundation models like LLMs and VLMs motivates their integration into embodied AI for mobile service robots to achieve more flexible understanding and robust task execution in dynamic environments. This systematic review analyzes how recent advances address core challenges such as translating language to actions and multimodal perception through methods like language-conditioned control and uncertainty-aware reasoning. Key findings highlight that foundation models enable context-aware and socially responsive behaviors in applications like domestic assistance and healthcare, while also identifying future research needs in reliability, privacy-aware deployment, and human-in-the-loop frameworks.</div>
<div class="mono" style="margin-top:8px">基础模型（如大语言模型和视觉语言模型）的快速发展推动了其在移动服务机器人具身AI中的应用，旨在提升动态环境中灵活理解和鲁棒任务执行的能力。本文通过系统性综述，分析了近期研究如何通过语言条件控制、不确定性感知推理等方法应对语言指令到动作转换和多模态感知等核心挑战。主要实验结果表明，基础模型在家庭辅助和医疗等应用中实现了情境感知和社交响应行为，同时指出了未来在可靠性、隐私感知部署和人机协同框架方面的研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance</div>
<div class="meta-line">Authors: Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang, Boyan Li, Yiran Qin, Jin Liu, Can Zhao, Li Kang, Haoqin Hong, Zhenfei Yin, Philip Torr, Hao Su, Ruimao Zhang, Daolin Ma</div>
<div class="meta-line">First: 2026-01-28T04:22:47+00:00 · Latest: 2026-01-28T04:22:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20239v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20239v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TouchGuide：通过触觉引导在推理时操控视觉运动策略</div>
<div class="mono" style="margin-top:8px">精细且接触密集的操作对机器人而言仍具挑战性，主要因触觉反馈未充分利用。为此，我们提出TouchGuide，一种新颖的跨策略视觉-触觉融合范式，在低维动作空间内融合多模态信息。具体而言，TouchGuide在推理时通过两阶段引导预训练的扩散或流匹配视觉运动策略：首先，策略在早期采样阶段仅使用视觉输入生成粗略但视觉合理的动作；其次，任务特定的接触物理模型（CPM）提供触觉引导以调整和优化动作，确保其符合真实物理接触条件。CPM通过有限专家演示的对比学习训练，提供触觉感知的可行性评分，引导采样过程生成满足物理接触约束的精细化动作。此外，为促进高质量、低成本的数据训练TouchGuide，我们开发了TacUMI数据采集系统。TacUMI在精度与成本间实现良好平衡：利用刚性指尖获取直接触觉反馈，从而收集可靠的触觉数据。在系鞋带、芯片交接等五项高接触难度任务上的大量实验表明，TouchGuide持续显著优于当前最先进的视觉-触觉策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of fine-grained and contact-rich robotic manipulation by better utilizing tactile feedback, this paper introduces TouchGuide, a cross-policy visuo-tactile fusion method that refines actions in a low-dimensional space. The method operates in two stages at inference time: a pre-trained diffusion or flow-matching visuomotor policy first generates a coarse action from visual input, which is then steered and refined by a task-specific Contact Physical Model (CPM) that uses tactile guidance to ensure actions satisfy realistic physical contact constraints; the CPM is trained via contrastive learning on limited expert data, and a cost-effective data collection system named TacUMI is introduced to gather reliable tactile feedback. Experimental results on five challenging contact-rich tasks, including shoe lacing and chip handover, demonstrate that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.</div>
<div class="mono" style="margin-top:8px">为解决精细化和接触丰富的操作任务中触觉反馈利用不足的挑战，本文提出了TouchGuide，一种在推理时进行跨策略视觉-触觉融合的方法。该方法分为两个阶段：首先，预训练的扩散或流匹配视觉运动策略仅使用视觉输入生成粗略动作；随后，一个通过对比学习在有限专家演示上训练的任务特定接触物理模型（CPM）提供触觉引导，以调整和细化动作，确保其符合真实的物理接触条件。在鞋带系扣和芯片交接等五个具有挑战性的接触丰富任务上的广泛实验表明，TouchGuide持续且显著地优于最先进的视觉-触觉策略。</div>
</details>
</div>
<div class="card">
<div class="title">MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption</div>
<div class="meta-line">Authors: Chen Li, Zhantao Yang, Han Zhang, Fangyi Chen, Chenchen Zhu, Anudeepsekhar Bolimera, Marios Savvides</div>
<div class="meta-line">First: 2025-10-07T04:54:39+00:00 · Latest: 2026-01-28T03:51:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.05580v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.05580v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, incur high compute costs, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetaVLA：面向高效具身适应的统一元协同训练框架</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型在具身推理中展现出潜力，但距离真正的通用系统仍有差距——它们常需任务特定微调、计算成本高昂，且对未见任务泛化能力弱。本文提出MetaVLA：一种统一、主干网络无关的高效可扩展对齐后训练框架。MetaVLA引入上下文感知元协同训练机制，将多样目标任务整合至单一微调阶段，同时利用结构多样的辅助任务提升领域内泛化能力。与朴素多任务监督微调不同，MetaVLA集成基于注意力神经过程的轻量元学习机制，能以最小架构变更或推理开销实现多上下文快速适应。在LIBERO基准测试中，配备六项辅助任务的MetaVLA在长时程任务上以最高8.0%优势超越OpenVLA，训练步数从24万降至7.5万，GPU时间减少约76%。结果表明可扩展、低资源的后训练具备可行性，为通用具身智能体发展开辟道路。代码即将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models for embodied AI often require costly, task-specific fine-tuning and struggle with generalization. To address this, MetaVLA proposes a unified post-training framework using Context-Aware Meta Co-Training, which consolidates multiple target tasks into a single fine-tuning stage and leverages diverse auxiliary tasks to improve generalization; it incorporates a lightweight meta-learning mechanism from Attentive Neural Processes for rapid adaptation without significant architectural changes. Experiments on the LIBERO benchmark show MetaVLA outperforms OpenVLA by up to 8.0% on long-horizon tasks while reducing training steps from 240K to 75K and cutting GPU time by approximately 76%, demonstrating efficient and scalable adaptation.</div>
<div class="mono" style="margin-top:8px">用于具身AI的视觉-语言-动作模型通常需要昂贵且任务特定的微调，并存在泛化能力不足的问题。为此，MetaVLA提出了一个统一的后期训练框架，采用上下文感知元协同训练，将多个目标任务整合到单一微调阶段，并利用多样化的辅助任务，同时通过源自注意力神经过程的轻量级元学习机制实现快速适应。在LIBERO基准测试中，MetaVLA在长视野任务上比OpenVLA性能提升高达8.0%，同时将训练步数从24万减少到7.5万，GPU时间降低约76%，证明了高效且可扩展的适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">Judgelight: Trajectory-Level Post-Optimization for Multi-Agent Path Finding via Closed-Subwalk Collapsing</div>
<div class="meta-line">Authors: Yimin Tang, Sven Koenig, Erdem Bıyık</div>
<div class="meta-line">First: 2026-01-27T09:20:14+00:00 · Latest: 2026-01-28T03:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19388v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Agent Path Finding (MAPF) is an NP-hard problem with applications in warehouse automation and multi-robot coordination. Learning-based MAPF solvers offer fast and scalable planning but often produce feasible trajectories that contain unnecessary or oscillatory movements. We propose Judgelight, a post-optimization layer that improves trajectory quality after a MAPF solver generates a feasible schedule. Judgelight collapses closed subwalks in agents&#x27; trajectories to remove redundant movements while preserving all feasibility constraints. We formalize this process as MAPF-Collapse, prove that it is NP-hard, and present an exact optimization approach by formulating it as integer linear programming (ILP) problem. Experimental results show Judgelight consistently reduces solution cost by around 20%, particularly for learning-based solvers, producing trajectories that are better suited for real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Judgelight：基于闭合子游走折叠的多智能体路径规划轨迹级后优化方法</div>
<div class="mono" style="margin-top:8px">多智能体路径规划（MAPF）是仓库自动化和多机器人协调等领域中的NP难问题。基于学习的MAPF求解器能够实现快速可扩展的规划，但生成的可行轨迹常包含不必要或振荡性移动。本文提出Judgelight后优化层，在MAPF求解器生成可行调度后提升轨迹质量。该方法通过折叠智能体轨迹中的闭合子游走来消除冗余移动，同时保持所有可行性约束。我们将此过程形式化为MAPF-Collapse问题，证明其NP难特性，并通过整数线性规划（ILP）建模提出精确优化方法。实验结果表明，Judgelight能持续降低约20%的求解成本，尤其对基于学习的求解器效果显著，生成的轨迹更适用于实际部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-Agent Path Finding (MAPF) solvers, especially learning-based ones, often generate feasible but suboptimal trajectories with redundant or oscillatory movements, limiting their practicality for real-world deployment. To address this, the authors propose Judgelight, a post-optimization layer that improves trajectory quality by collapsing closed subwalks within the agents&#x27; paths while maintaining feasibility constraints; this MAPF-Collapse problem is formalized, proven NP-hard, and solved exactly via an integer linear programming (ILP) formulation. Experiments demonstrate that Judgelight consistently reduces solution costs by approximately 20%, with particularly significant improvements for learning-based solvers, yielding trajectories more suitable for real-world applications.</div>
<div class="mono" style="margin-top:8px">多智能体路径规划（MAPF）求解器，特别是基于学习的方法，常生成可行但包含冗余或振荡运动的次优轨迹，限制了实际部署的实用性。为此，研究者提出了Judgelight，一种后优化层，通过折叠智能体路径中的闭合子行走来消除不必要的运动，同时保持可行性，该问题被形式化为NP难的MAPF-Collapse，并采用整数线性规划（ILP）进行精确求解。实验结果表明，Judgelight能持续降低约20%的求解成本，尤其对基于学习的求解器提升显著，从而产生更适合实际应用的轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">TRACER: Texture-Robust Affordance Chain-of-Thought for Deformable-Object Refinement</div>
<div class="meta-line">Authors: Wanjun Jia, Kang Li, Fan Yang, Mengfei Duan, Wenrui Chen, Yiming Jiang, Hui Zhang, Kailun Yang, Zhiyong Li, Yaonan Wang</div>
<div class="meta-line">First: 2026-01-28T03:12:18+00:00 · Latest: 2026-01-28T03:12:18+00:00</div>
<div class="meta-line">Comments: The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20208v1">PDF</a> · <a href="https://github.com/Dikay1/TRACER">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The central challenge in robotic manipulation of deformable objects lies in aligning high-level semantic instructions with physical interaction points under complex appearance and texture variations. Due to near-infinite degrees of freedom, complex dynamics, and heterogeneous patterns, existing vision-based affordance prediction methods often suffer from boundary overflow and fragmented functional regions. To address these issues, we propose TRACER, a Texture-Robust Affordance Chain-of-thought with dEformable-object Refinement framework, which establishes a cross-hierarchical mapping from hierarchical semantic reasoning to appearance-robust and physically consistent functional region refinement. Specifically, a Tree-structured Affordance Chain-of-Thought (TA-CoT) is formulated to decompose high-level task intentions into hierarchical sub-task semantics, providing consistent guidance across various execution stages. To ensure spatial integrity, a Spatial-Constrained Boundary Refinement (SCBR) mechanism is introduced to suppress prediction spillover, guiding the perceptual response to converge toward authentic interaction manifolds. Furthermore, an Interactive Convergence Refinement Flow (ICRF) is developed to aggregate discrete pixels corrupted by appearance noise, significantly enhancing the spatial continuity and physical plausibility of the identified functional regions. Extensive experiments conducted on the Fine-AGDDO15 dataset and a real-world robotic platform demonstrate that TRACER significantly improves affordance grounding precision across diverse textures and patterns inherent to deformable objects. More importantly, it enhances the success rate of long-horizon tasks, effectively bridging the gap between high-level semantic reasoning and low-level physical execution. The source code and dataset will be made publicly available at https://github.com/Dikay1/TRACER.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRACER：面向可变形物体细化的纹理鲁棒性可供性思维链框架</div>
<div class="mono" style="margin-top:8px">机器人操作可变形物体的核心挑战在于复杂外观与纹理变化下高层语义指令与物理交互点的对齐。由于近乎无限的自由度、复杂动力学特性及异质纹理模式，现有基于视觉的可供性预测方法常出现边界溢出与功能区域碎片化问题。为此，我们提出TRACER框架——一种融合纹理鲁棒性可供性思维链与可变形物体细化的方法，建立了从层次化语义推理到外观鲁棒且物理一致的功能区域细化的跨层次映射。具体而言，我们构建了树状结构可供性思维链（TA-CoT），将高层任务意图分解为层次化子任务语义，为各执行阶段提供一致性指导。为保障空间完整性，引入空间约束边界细化（SCBR）机制以抑制预测溢出，引导感知响应收敛至真实交互流形。进一步，开发了交互式收敛细化流（ICRF）以聚合受外观噪声干扰的离散像素，显著提升识别功能区域的空间连续性与物理合理性。在Fine-AGDDO15数据集及真实机器人平台上的大量实验表明，TRACER能显著提升针对可变形物体固有纹理与模式的可供性定位精度，更重要的是提高了长时序任务的成功率，有效弥合了高层语义推理与底层物理执行间的鸿沟。源代码与数据集将通过https://github.com/Dikay1/TRACER公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of aligning high-level semantic instructions with precise physical interaction points in robotic manipulation of deformable objects, which suffer from complex appearance variations and fragmented functional region predictions. The proposed TRACER framework establishes a cross-hierarchical mapping through a Tree-structured Affordance Chain-of-Thought (TA-CoT) to decompose task intentions, a Spatial-Constrained Boundary Refinement (SCBR) mechanism to suppress prediction spillover, and an Interactive Convergence Refinement Flow (ICRF) to aggregate discrete pixels and enhance spatial continuity. Experiments on the Fine-AGDDO15 dataset and a real-world robotic platform show that TRACER significantly improves affordance grounding precision across diverse textures and enhances the success rate of long-horizon manipulation tasks.</div>
<div class="mono" style="margin-top:8px">机器人对可变形物体的操作面临核心挑战：在复杂的表观和纹理变化下，如何将高层语义指令与物理交互点对齐，现有基于视觉的功能可供性预测方法常出现边界溢出和功能区域碎片化问题。为此，研究者提出TRACER框架，通过树状结构可供性思维链（TA-CoT）分解任务意图，空间约束边界细化（SCBR）机制抑制预测溢出，以及交互收敛细化流（ICRF）聚合受噪声干扰的离散像素，建立了从分层语义推理到外观鲁棒、物理一致的功能区域细化的跨层次映射。在Fine-AGDDO15数据集和真实机器人平台上的大量实验表明，TRACER显著提升了针对可变形物体多样纹理的功能可供性定位精度，并提高了长时序任务的执行成功率，有效弥合了高层语义推理与低层物理执行之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">A Taylor Series Approach to Correct Localization Errors in Robotic Field Mapping using Gaussian Processes</div>
<div class="meta-line">Authors: Muzaffar Qureshi, Tochukwu Elijah Ogri, Kyle Volle, Rushikesh Kamalapurkar</div>
<div class="meta-line">First: 2026-01-28T00:53:14+00:00 · Latest: 2026-01-28T00:53:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gaussian Processes (GPs) are powerful non-parametric Bayesian models for regression of scalar fields, formulated under the assumption that measurement locations are perfectly known and the corresponding field measurements have Gaussian noise. However, many real-world scalar field mapping applications rely on sensor-equipped mobile robots to collect field measurements, where imperfect localization introduces state uncertainty. Such discrepancies between the estimated and true measurement locations degrade GP mean and covariance estimates. To address this challenge, we propose a method for updating the GP models when improved estimates become available. Leveraging the differentiability of the kernel function, a second-order correction algorithm is developed using the precomputed Jacobians and Hessians of the GP mean and covariance functions for real-time refinement based on measurement location discrepancy data. Simulation results demonstrate improved prediction accuracy and computational efficiency compared to full model retraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于泰勒级数的高斯过程机器人场映射定位误差校正方法</div>
<div class="mono" style="margin-top:8px">高斯过程作为强大的非参数贝叶斯回归模型，其标量场回归建立在测量位置精确已知且测量噪声服从高斯分布的假设之上。然而，实际标量场映射常依赖搭载传感器的移动机器人采集数据，定位误差导致的状态不确定性会破坏这一假设。测量位置估计值与真实值之间的偏差会劣化高斯过程的均值与协方差估计。针对此问题，本文提出一种在获得更优位置估计时更新高斯过程模型的方法。利用核函数的可微特性，基于预计算的高斯过程均值与协方差函数的雅可比矩阵和海森矩阵，开发了二阶校正算法，可根据测量位置偏差数据实现实时模型修正。仿真结果表明，相较于完整模型重训练，该方法在提升预测精度的同时显著提高了计算效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Gaussian Processes (GPs) are widely used for robotic field mapping but assume perfect knowledge of measurement locations, an assumption violated by imperfect robot localization which degrades model accuracy. To correct these localization-induced errors without full retraining, the authors propose a real-time refinement method that leverages the differentiability of the GP kernel, using precomputed second-order Taylor series expansions (Jacobians and Hessians) of the mean and covariance functions to update the model based on location discrepancy data. Simulation results show the approach improves prediction accuracy while being computationally more efficient than completely retraining the GP model.</div>
<div class="mono" style="margin-top:8px">高斯过程广泛用于机器人场映射，但其假设测量位置已知，而实际中机器人定位不精确会引入状态不确定性，从而降低模型精度。为解决此问题，本研究提出一种实时修正方法，该方法利用核函数的可微性，基于预计算的高斯过程均值与协方差函数的雅可比矩阵和海森矩阵，对测量位置偏差数据应用二阶泰勒级数校正，从而避免完全重新训练模型。仿真结果表明，与完整模型重训练相比，该方法提高了预测精度并具有更高的计算效率。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
