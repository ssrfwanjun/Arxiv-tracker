<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-04 05:41</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260204_0541</div>
    <div class="row"><div class="card">
<div class="title">Reward-free Alignment for Conflicting Objectives</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Xi Chen, Tianyi Lin</div>
<div class="meta-line">First: 2026-02-02T18:59:52+00:00 · Latest: 2026-02-02T18:59:52+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02495v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02495v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向冲突目标的无奖励对齐方法</div>
<div class="mono" style="margin-top:8px">直接对齐方法正日益用于将大语言模型（LLM）与人类偏好对齐。然而，许多现实世界的对齐问题涉及多个相互冲突的目标，其中对偏好的简单聚合可能导致训练不稳定和权衡效果不佳。具体而言，加权损失方法可能无法识别同时改进所有目标的更新方向，而现有的多目标方法通常依赖显式奖励模型，这引入了额外的复杂性并可能扭曲用户指定的偏好。本文的贡献包括两方面：首先，我们提出了一种面向冲突目标的无奖励对齐框架（RACO），该框架直接利用成对偏好数据，并通过一种新颖的冲突规避梯度下降剪裁变体来解决梯度冲突。我们提供了收敛到尊重用户指定目标权重的帕累托临界点的理论保证，并进一步证明在双目标设置中剪裁能严格提升收敛速率。其次，我们通过启发式方法改进了该方法，并通过实验验证了所提框架在LLM对齐中的适用性。在多目标摘要生成和安全对齐任务上，对多个LLM系列（Qwen 3、Llama 3、Gemma 3）进行的定性与定量评估均表明，相较于现有多目标对齐基线方法，我们的方法能持续实现更优的帕累托权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of aligning large language models with multiple conflicting human preferences, where naive aggregation can cause unstable training and suboptimal trade-offs. The authors propose a reward-free alignment framework called RACO, which directly uses pairwise preference data and resolves gradient conflicts through a novel clipped variant of conflict-averse gradient descent, offering convergence guarantees to Pareto-critical points that respect user-specified weights. Experimental evaluations on multi-objective summarization and safety alignment tasks across models like Qwen 3, Llama 3, and Gemma 3 demonstrate that RACO consistently achieves better Pareto trade-offs compared to existing baselines.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型与多个冲突的人类偏好对齐的挑战，其中简单的偏好聚合会导致训练不稳定和次优权衡。作者提出了RACO，一种无需奖励模型的对齐框架，它利用成对偏好数据，并通过一种新颖的裁剪式冲突规避梯度下降来解决梯度冲突，提供了收敛到帕累托临界点的理论保证。在多目标摘要和安全对齐任务上对Qwen 3、Llama 3和Gemma 3等模型的实验评估表明，与现有基线方法相比，RACO能持续实现更好的帕累托权衡。</div>
</details>
</div>
<div class="card">
<div class="title">MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training</div>
<div class="meta-line">Authors: Dulhan Jayalath, Oiwi Parker Jones</div>
<div class="meta-line">First: 2026-02-02T18:59:50+00:00 · Latest: 2026-02-02T18:59:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02494v1">PDF</a> · <a href="https://github.com/neural-processing-lab/MEG-XL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MEG-XL：通过长上下文预训练实现数据高效型脑到文本转换</div>
<div class="mono" style="margin-top:8px">临床脑到文本接口专为无法提供大量训练记录的瘫痪患者设计。预训练通过跨被试学习统计先验来提升数据效率与泛化能力，但这些先验高度依赖上下文。自然语音可能持续数分钟展开，而现有方法大多仅用数秒上下文进行预训练。为此，我们提出MEG-XL模型，其每个样本使用2.5分钟MEG上下文进行预训练，长度达先前研究的5-300倍，相当于19.1万词元，可捕捉长程神经上下文。在脑数据单词解码任务上微调后，MEG-XL仅用少量数据（如1小时对比50小时）即可达到监督学习性能，且优于现有脑基础模型。我们发现长上下文预训练模型能学习到更适用于单词解码的迁移表征。结果表明，长上下文预训练有助于利用其他方法常被丢弃的扩展神经上下文。代码、模型权重及使用说明详见：https://github.com/neural-processing-lab/MEG-XL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Clinical brain-to-text interfaces for paralyzed patients require data-efficient models due to limited training recordings. This work introduces MEG-XL, which pre-trains on magnetoencephalography (MEG) data using 2.5 minutes of context per sample—5 to 300 times longer than previous methods—to capture extended neural dynamics. When fine-tuned for word decoding, MEG-XL matches supervised performance with drastically less data (e.g., 1 hour versus 50 hours) and outperforms existing brain foundation models, demonstrating that long-context pre-training yields representations that transfer more effectively.</div>
<div class="mono" style="margin-top:8px">针对瘫痪患者的临床脑到文本接口需要数据高效的模型，因为训练记录有限。本研究提出了MEG-XL，该模型在脑磁图数据上进行预训练，每个样本使用2.5分钟的上下文，比先前方法增加了5-300倍，以捕捉更长的神经动态。在单词解码任务上进行微调后，MEG-XL仅用极少数据（例如1小时对比50小时）就达到了监督模型的性能，并超越了现有的脑基础模型，这表明长上下文预训练能产生更有效迁移到解码任务的表征。</div>
</details>
</div>
<div class="card">
<div class="title">RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</div>
<div class="meta-line">Authors: Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang</div>
<div class="meta-line">First: 2026-02-02T18:59:04+00:00 · Latest: 2026-02-02T18:59:04+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Gen-Verse/Open-AgentRL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02488v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02488v1">PDF</a> · <a href="https://github.com/Gen-Verse/Open-AgentRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLAnything：在完全动态强化学习系统中锻造环境、策略与奖励模型</div>
<div class="mono" style="margin-top:8px">我们提出RLAnything，一种通过闭环优化动态锻造环境、策略和奖励模型的强化学习框架，可增强学习信号并强化适用于任何大语言模型或智能体场景的整体强化学习系统。具体而言，策略通过整合来自逐步信号与结果信号的反馈进行训练，而奖励模型则通过一致性反馈联合优化，进而进一步提升策略训练效果。此外，我们基于理论驱动的自动环境适应机制，通过利用来自奖励模型与策略模型的批评反馈，提升两者的训练效果，实现从经验中学习。实证表明，每个新增组件均持续提升系统整体性能，RLAnything在多种代表性大语言模型与智能体任务中取得显著增益：在OSWorld上使Qwen3-VL-8B-Thinking提升9.1%，在AlfWorld和LiveBench上分别使Qwen2.5-7B-Instruct提升18.7%和11.9%。我们还发现优化后的奖励模型信号优于依赖人工标注的结果。代码：https://github.com/Gen-Verse/Open-AgentRL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance reinforcement learning systems by dynamically co-optimizing their core components. The proposed RLAnything framework forges environment, policy, and reward models through closed-loop optimization, where the policy is trained with integrated step-wise and outcome feedback, the reward model is optimized via consistency feedback, and the environment is automatically adapted using critic feedback from both models. Experimental results show that each component contributes to system improvement, with RLAnything delivering substantial performance gains on various LLM and agent tasks, such as boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% on AlfWorld, while also demonstrating that optimized reward signals can surpass those based on human labels.</div>
<div class="mono" style="margin-top:8px">本研究针对强化大型语言模型和智能体场景中强化学习系统的挑战，提出了RLAnything框架，该框架通过闭环优化动态构建环境、策略和奖励模型。该方法整合了逐步和结果反馈进行策略训练，通过一致性反馈联合优化奖励模型，并采用理论驱动的自动环境适应机制，利用来自两个模型的批评反馈。实验结果表明，每个组件都持续提升了系统性能：RLAnything在OSWorld上将Qwen3-VL-8B-Thinking的性能提升了9.1%，在AlfWorld和LiveBench上分别将Qwen2.5-7B-Instruct的性能提升了18.7%和11.9%，同时证明优化的奖励模型信号优于依赖人工标注的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-02T18:56:56+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈拓展强化学习能力</div>
<div class="mono" style="margin-top:8px">强化学习在大型语言模型后训练中的成功源于一个信息量极低的来源：每次训练轮次仅提供单比特信息，如二元奖励或偏好标签。另一极端，蒸馏方法虽能提供密集监督，但需要成本高昂且难以扩展的演示样本。本研究将文本反馈作为一种中间信号进行探索：它比标量奖励更丰富，又比完整演示更经济。文本反馈是人类互动的自然模式，已在众多现实场景中广泛存在，用户、标注者和自动评估系统常以此评判语言模型输出。为规模化利用文本反馈，我们形式化了一个多轮强化学习框架——基于文本反馈的强化学习（RLTF），其中训练阶段可获得文本反馈，但推理阶段则无。因此，模型必须学会内化反馈以提升测试阶段的单轮表现。为此，我们提出两种方法：自蒸馏（RLTF-SD），训练单轮策略以匹配其自身基于反馈的第二轮生成结果；以及反馈建模（RLTF-FM），通过预测反馈作为辅助目标。我们对两种方法进行了理论分析，并在推理谜题、竞赛数学和创意写作任务上进行了实证评估。结果表明，两种方法在多个基准测试中均持续优于强基线，凸显了强化学习结合规模化丰富监督源的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing reinforcement learning (RL) approaches for large language model (LLM) post-training, which rely on sparse binary rewards or costly full demonstrations. To address this, the study introduces RL from Text Feedback (RLTF), a multi-turn RL framework that leverages textual feedback—richer than scalar rewards yet cheaper than demonstrations—as an intermediate training signal. Two methods are proposed: Self Distillation (RLTF-SD), which aligns the single-turn policy with its own feedback-conditioned second-turn outputs, and Feedback Modeling (RLTF-FM), which predicts feedback as an auxiliary task. Experimental evaluations on reasoning puzzles, competition math, and creative writing tasks demonstrate that both methods consistently outperform strong baselines, underscoring the effectiveness of text feedback as a scalable source of supervision for improving LLM performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型（LLM）后训练中强化学习方法的局限性，即依赖稀疏的二元奖励或成本高昂的完整演示。为此，研究提出了基于文本反馈的强化学习（RLTF）框架，利用文本反馈——比标量奖励更丰富、比演示更廉价——作为中间训练信号，且反馈仅在训练阶段可用。提出了两种方法：自蒸馏（RLTF-SD），通过将单轮策略与自身反馈条件下的第二轮生成对齐；以及反馈建模（RLTF-FM），将反馈预测作为辅助任务。在推理谜题、竞赛数学和创意写作任务上的实验评估表明，这两种方法均持续优于强基线，验证了文本反馈作为可扩展监督来源对提升LLM性能的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents</div>
<div class="meta-line">Authors: Haozhen Zhang, Quanyu Long, Jianzhu Bao, Tao Feng, Weizhi Zhang, Haodong Yue, Wenya Wang</div>
<div class="meta-line">First: 2026-02-02T18:53:28+00:00 · Latest: 2026-02-02T18:53:28+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/MemSkill</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02474v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02474v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/MemSkill">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemSkill：面向自进化智能体的记忆技能学习与演化</div>
<div class="mono" style="margin-top:8px">现有大型语言模型（LLM）智能体记忆系统多依赖少量静态、人工设计的记忆提取操作。这些固定流程将人类对存储内容与记忆修订的先验知识硬编码其中，导致其在多样化交互模式下僵化，且在长历史记录中效率低下。为此，我们提出\textbf{MemSkill}，将此类操作重构为可学习、可演化的记忆技能——即结构化、可复用的例行程序，用于从交互轨迹中提取、整合与修剪信息。受智能体技能设计理念启发，MemSkill采用\emph{控制器}学习选择少量相关技能，并搭配基于LLM的\emph{执行器}生成技能引导的记忆。除学习技能选择外，MemSkill引入\emph{设计器}定期审查因所选技能产生错误或不完整记忆的困难案例，并通过优化现有技能或提出新技能来演化技能集。三者共同构成闭环流程，同步提升技能选择策略与技能集本身。在LoCoMo、LongMemEval、HotpotQA和ALFWorld上的实验表明，MemSkill在任务性能上优于强基线模型，且具有良好的跨场景泛化能力。进一步分析揭示了技能的演化机制，为LLM智能体实现更自适应、自进化的记忆管理提供了新思路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current LLM agent memory systems often depend on static, hand-designed operations for memory extraction, which can be inflexible and inefficient across diverse interactions and long histories. To address this, MemSkill introduces learnable and evolvable memory skills—structured routines for extracting, consolidating, and pruning information—through a controller that selects relevant skills and an LLM-based executor that produces skill-guided memories. Additionally, a designer component periodically reviews hard cases to evolve the skill set by refining existing skills and proposing new ones, forming a closed-loop system. Experimental results on benchmarks including LoCoMo, LongMemEval, HotpotQA, and ALFWorld show that MemSkill outperforms strong baselines in task performance and generalizes well, with analyses revealing how skills evolve to enable more adaptive memory management.</div>
<div class="mono" style="margin-top:8px">当前大型语言模型（LLM）智能体的记忆系统通常依赖于静态、人工设计的记忆提取操作，这在不同交互模式和长历史中可能显得僵化和低效。为此，MemSkill提出了可学习和可演化的记忆技能——用于提取、整合和修剪信息的结构化例程——通过控制器选择相关技能，基于LLM的执行器生成技能引导的记忆，并引入设计器定期审查导致错误或不完整记忆的困难案例，以改进和提出新技能，形成一个闭环改进系统。在LoCoMo、LongMemEval、HotpotQA和ALFWorld等基准测试上的实验表明，MemSkill相比强基线提升了任务性能并具有良好的泛化能力，相关分析为更自适应的记忆管理提供了技能演化方面的见解。</div>
</details>
</div>
<div class="card">
<div class="title">HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos</div>
<div class="meta-line">Authors: Yinhuai Wang, Qihan Zhao, Yuen Fui Lau, Runyi Yu, Hok Wai Tsui, Qifeng Chen, Jingbo Wang, Jiangmiao Pang, Ping Tan</div>
<div class="meta-line">First: 2026-02-02T18:53:01+00:00 · Latest: 2026-02-02T18:53:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanX：基于人类视频实现敏捷且可泛化的人形机器人交互技能</div>
<div class="mono" style="margin-top:8px">赋予人形机器人执行敏捷自适应交互任务的能力一直是机器人学的核心挑战。现有方法受限于真实交互数据的稀缺性或需要精细的任务特定奖励工程，制约了其可扩展性。为缩小这一差距，我们提出HumanX——一个全栈框架，可将人类视频编译为无需任务特定奖励、可泛化至现实世界的人形机器人交互技能。HumanX整合了两个协同设计的组件：XGen（通过视频合成多样化且物理合理的机器人交互数据，支持可扩展数据增强的生成流程）和XMimic（学习可泛化交互技能的统一模仿学习框架）。在篮球、足球、羽毛球、货物拾取及反应性对抗五个不同领域的评估中，HumanX成功习得10种技能并零样本迁移至实体Unitree G1人形机器人。习得能力包括无需外部感知的复杂动作（如假动作转身后仰跳投）以及从单段视频演示学习的交互任务（如持续10个周期的人机连续传接序列）。实验表明，HumanX的泛化成功率较现有方法提升8倍以上，为学习多样化现实世界机器人交互技能提供了可扩展且任务无关的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of scarce interaction data and task-specific reward engineering in humanoid robotics, this paper introduces HumanX, a framework that converts human videos into generalizable interaction skills without task-specific rewards. The method combines XGen, a pipeline for generating diverse and physically plausible robot data from videos with scalable augmentation, and XMimic, a unified imitation learning framework for skill acquisition. Experimental results across five domains, including basketball and football, show that HumanX acquires 10 skills and transfers them zero-shot to a physical Unitree G1 humanoid, achieving complex maneuvers like pump-fake turnaround fadeaway jumpshots and sustained human-robot passing sequences, with over 8 times higher generalization success than prior methods.</div>
<div class="mono" style="margin-top:8px">为解决人形机器人交互中数据稀缺和任务特定奖励设计带来的可扩展性限制，本文提出了HumanX框架，该框架将人类视频编译为无需任务特定奖励即可泛化的现实世界技能。该方法整合了XGen（一个从视频合成多样且物理合理的机器人交互数据并支持可扩展增强的流水线）和XMimic（一个用于技能学习的统一模仿学习框架）。在篮球、足球等五个领域的实验结果表明，HumanX成功习得了10种技能，并零样本迁移到Unitree G1实体人形机器人上，实现了如假动作转身后仰跳投等复杂动作以及从单次视频演示中学习持续人机传球序列，其泛化成功率比先前方法高出8倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning</div>
<div class="meta-line">Authors: Qifan Yu, Xinyu Ma, Zhijian Zhuo, Minrui Wang, Deyi Liu, Shiyi Zhan, Yiyuan Ma, Liang Xiang, Xingyan Bin, Di He</div>
<div class="meta-line">First: 2026-02-02T18:52:52+00:00 · Latest: 2026-02-02T18:52:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02472v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02472v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARKLING：在宽度渐进学习中平衡信号保持与对称性打破</div>
<div class="mono" style="margin-top:8px">渐进学习通过逐步增加模型规模来减少预训练的计算开销。现有研究已深入探索深度扩展，但宽度扩展仍研究不足，少数现有方法仅限于训练早期阶段。然而，在训练中期扩展宽度对最大化计算节省至关重要，但由于严重的训练不稳定性，这仍是一项艰巨挑战。实验表明，此阶段的简单初始化会破坏激活统计量，引发损失骤增，而基于复制的初始化则引入梯度对称性，阻碍特征多样性。为解决这些问题，我们提出SPARKLING（在宽度渐进学习中平衡信号保持与对称性打破），一种用于中期宽度扩展的新框架。该方法通过RMS尺度一致性实现信号保持，在扩展过程中稳定激活统计量；通过非对称优化器状态重置和学习率重新预热确保对称性打破。在混合专家模型上的大量实验表明，在多种宽度轴和优化器族中，SPARKLING始终优于从头训练，并在2倍宽度扩展下将训练成本降低高达35%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Progressive learning aims to reduce pre-training costs by scaling models gradually, but width expansion during mid-stage training is challenging due to instability from disrupted activation statistics and gradient symmetry from copy-based initialization. To address this, SPARKLING introduces RMS-scale consistency to preserve signal and stabilize activations, combined with asymmetric optimizer state resetting and learning rate re-warmup to break symmetry and enhance feature diversity. Experiments on Mixture-of-Experts models show that SPARKLING outperforms training from scratch across various width axes and optimizers, reducing training costs by up to 35% under 2× width expansion.</div>
<div class="mono" style="margin-top:8px">渐进式学习通过逐步扩展模型规模来降低预训练成本，但在训练中期进行宽度扩展面临挑战，因为简单的初始化会破坏激活统计导致损失尖峰，而复制初始化则引入梯度对称性阻碍特征多样性。为此，SPARKLING提出了一种框架，通过RMS尺度一致性保持信号以稳定激活，并通过非对称优化器状态重置和学习率重新预热来打破对称性。在混合专家模型上的实验表明，SPARKLING在多种宽度轴和优化器上均优于从头训练，在2倍宽度扩展下最高可减少35%的训练成本。</div>
</details>
</div>
<div class="card">
<div class="title">Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation</div>
<div class="meta-line">Authors: Ahmed M. Elshazly, Ahmed Arafa</div>
<div class="meta-line">First: 2026-02-02T18:50:51+00:00 · Latest: 2026-02-02T18:50:51+00:00</div>
<div class="meta-line">Comments: To appear in IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02469v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02469v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \emph{AgeTop-\(k\)}, which first picks the largest-magnitude entries and then chooses the \(k\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \(k\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\(k\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \(k\) depends on the channel, with smaller \(k\) being better in noisy settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于空口聚合的年龄感知边缘盲联邦学习</div>
<div class="mono" style="margin-top:8px">本文研究无线衰落信道下的联邦学习（FL），其中多设备同时发送模型更新。我们提出一种高效的年龄感知边缘盲空口联邦学习方法，无需设备端信道状态信息（CSI）。参数服务器（PS）采用多天线，基于估计的信道增益总和应用最大比合并（MRC）来检测参数更新。关键挑战在于正交子载波数量有限，传输大量参数需多个正交频分复用（OFDM）符号，导致延迟增加。为此，PS每轮仅通过AgeTop-k算法选择少量模型坐标：该算法先选取幅度最大的条目，再筛选自上次被选后等待时间最长的k个坐标。这确保所有选定参数可容纳于单个OFDM符号，降低延迟。我们给出收敛界，突显更多天线阵列单元的优势，并揭示关键权衡：增加k会以增强信道噪声影响为代价降低压缩误差。实验结果表明：（1）更多PS天线显著提升精度与收敛速度；（2）在较好信道条件下AgeTop-k优于随机选择；（3）最优k值取决于信道状况，高噪声环境下较小k值更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of high latency in wireless federated learning due to limited orthogonal subcarriers, which forces model updates to be split across multiple OFDM symbols. The proposed method, age-aware edge-blind over-the-air FL, eliminates the need for devices to have channel state information by employing a multi-antenna parameter server that uses maximum-ratio combining based on estimated aggregate channel gains. To fit updates into a single OFDM symbol and reduce latency, the server selects a subset of model parameters each round using AgeTop-k, which prioritizes entries with large magnitudes and long waiting times since last selection. Experiments demonstrate that more server antennas significantly boost accuracy and convergence speed, AgeTop-k surpasses random selection in favorable channel conditions, and the optimal subset size k is channel-dependent, with smaller k being preferable in noisy environments.</div>
<div class="mono" style="margin-top:8px">本文针对无线信道中联邦学习因正交子载波有限而导致的高延迟问题，该问题迫使模型更新需分割到多个OFDM符号中传输。所提出的方法——年龄感知的边缘盲空中聚合联邦学习，通过采用配备多天线的参数服务器，基于估计的信道增益使用最大比合并来检测更新，从而消除了设备对信道状态信息的需求。为了将所有选定的参数压缩到单个OFDM符号中以降低延迟，服务器每轮使用AgeTop-k选择模型坐标的一个子集，该策略优先选取幅度最大且自上次选择以来等待时间最长的条目。实验结果表明，增加参数服务器的天线数量能显著提高精度和收敛速度；在信道条件相对较好时，AgeTop-k优于随机选择；最优子集大小k取决于信道噪声，在噪声较大的环境中，较小的k表现更佳。</div>
</details>
</div>
<div class="card">
<div class="title">Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables</div>
<div class="meta-line">Authors: Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Oliver Powell, Benjamin Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Taru Muhonen, Richard Vigars, Louis Berridge</div>
<div class="meta-line">First: 2025-03-10T20:12:06+00:00 · Latest: 2026-02-02T18:50:47+00:00</div>
<div class="meta-line">Comments: 24 pages, 14 figures. Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, and Oliver Powell contributed equally to this paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.07825v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.07825v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\% F1 accuracy and our 6-channel model surpassing 80\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Helios 2.0：面向事件传感器可穿戴设备的鲁棒超低功耗手势识别系统</div>
<div class="mono" style="margin-top:8px">我们提出一种可穿戴技术进展：针对移动设备优化的实时超低功耗事件相机系统，可实现智能眼镜的自然手势控制，显著提升用户体验。尽管计算机视觉中的手势识别已取得重大进展，但构建直观、适应不同用户与环境、且满足可穿戴设备实际能效需求的系统仍面临关键挑战。我们通过精选微手势应对这些挑战：食指侧向拇指滑动（双向）以及拇指与食指指尖的双次捏合。这些以人为本的交互利用自然手部动作，确保直观易用性，无需用户学习复杂指令序列。为克服用户与环境差异性，我们开发了创新的仿真方法，无需大量现实数据采集即可实现全面领域采样。我们的功耗优化架构保持卓越性能，在包含多样用户与环境的基准数据集上取得超过80%的F1分数。所构建模型在采用高通骁龙Hexagon DSP时功耗仅6-8毫瓦：双通道方案在用户研究中实现超70%的F1准确率，六通道模型在所有手势类别上突破80%的F1准确率。这些成果仅使用合成训练数据达成。相较于现有技术，该系统在使用DSP时将F1准确率提升20%，功耗降低至1/25。该进展推动超低功耗视觉系统在可穿戴设备中的部署，为人机无缝交互开辟新可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for intuitive, adaptable, and energy-efficient gesture recognition systems for practical wearable applications like smart glasses. The method introduces a system using an event camera and carefully selected microgestures (lateral thumb swipes and a double pinch), supported by a novel simulation methodology for domain sampling to reduce reliance on extensive real-world data. Experimental results show the power-optimised architecture achieves F1 scores above 80% on benchmark datasets, operates at 6-8 mW using a Qualcomm Snapdragon Hexagon DSP, and improves state-of-the-art F1 accuracy by 20% with a 25x power reduction, using only synthetic training data.</div>
<div class="mono" style="margin-top:8px">本研究旨在为智能眼镜等可穿戴设备开发直观、用户自适应且高能效的手势识别系统。方法上，系统采用事件相机识别特定的微手势（拇指横向滑动和双指捏合），并利用新颖的仿真方法进行模型训练，无需大量真实世界数据。实验结果表明，该功耗优化系统在多样化基准数据集上F1分数超过80%，使用DSP时功耗仅为6-8毫瓦，仅使用合成数据训练就将F1准确率较现有最佳水平提升20%，同时功耗降低25倍。</div>
</details>
</div>
<div class="card">
<div class="title">MentisOculi: Revealing the Limits of Reasoning with Mental Imagery</div>
<div class="meta-line">Authors: Jana Zeller, Thaddäus Wiedemer, Fanfei Li, Thomas Klein, Prasanna Mayilvahanan, Matthias Bethge, Felix Wichmann, Ryan Cotterell, Wieland Brendel</div>
<div class="meta-line">First: 2026-02-02T18:49:06+00:00 · Latest: 2026-02-02T18:49:06+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02465v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02465v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MentisOculi：揭示心智意象推理的局限性</div>
<div class="mono" style="margin-top:8px">前沿模型正从仅接收视觉信息的多模态大语言模型（MLLMs）向能够原生交错生成的统一多模态模型（UMMs）转型。这一转变引发了利用中间可视化作为推理辅助的兴趣，类似于人类的心智意象。该能力的核心在于以目标导向的方式形成、维持和操控视觉表征。为评估和探究此能力，我们开发了MentisOculi——一套程序化、分层化的多步骤推理问题集，适用于视觉化解决方案，并针对前沿模型的挑战性进行调优。通过评估从潜在标记到显式生成图像等多种视觉策略，我们发现它们普遍未能提升性能。对UMMs的具体分析揭示了一个关键局限：尽管它们具备解决任务的文本推理能力，有时也能生成正确的视觉内容，但存在生成误差累积问题，且无法有效利用甚至完全准确的视觉化信息。我们的研究表明，尽管视觉思维具有内在吸引力，但目前尚未对模型推理产生助益。MentisOculi为分析和弥合不同模型家族间的这一差距奠定了必要基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the shift from multimodal large language models (MLLMs) to unified multimodal models (UMMs) capable of generating interleaved content, which raises the question of whether using intermediate visualizations as a reasoning aid—similar to human mental imagery—can enhance model performance. The method involves developing MentisOculi, a procedural and stratified benchmark of multi-step visual reasoning problems, to evaluate various visual strategies, including latent tokens and explicitly generated imagery, across frontier models. Key experimental findings reveal that these visual strategies generally fail to improve reasoning; specifically, UMMs exhibit a critical limitation where they possess the textual reasoning capacity to solve tasks and can sometimes generate correct visuals, but they suffer from compounding generation errors and cannot effectively leverage even ground-truth visualizations, indicating that visual thoughts do not yet benefit model reasoning.</div>
<div class="mono" style="margin-top:8px">本研究动机源于前沿模型正从仅能处理视觉信息的多模态大语言模型（MLLMs）向能够原生交错生成内容的统一多模态模型（UMMs）过渡，这引发了关于使用中间可视化作为推理辅助（类似于人类心理意象）是否能提升模型性能的探讨。方法上，研究者开发了MentisOculi，一个程序化、分层的多步骤视觉推理问题测试套件，用于评估包括潜在标记和显式生成图像在内的多种视觉策略在不同前沿模型上的表现。主要实验结果表明，这些视觉策略普遍未能提升推理性能；具体而言，UMMs暴露出一个关键局限：它们虽具备解决任务的文本推理能力，有时也能生成正确的视觉内容，但会因生成错误累积而失败，且无法有效利用甚至真实的可视化结果，这表明视觉思维目前尚未有益于模型推理。</div>
</details>
</div>
<div class="card">
<div class="title">Conflict-Aware Client Selection for Multi-Server Federated Learning</div>
<div class="meta-line">Authors: Mingwei Hong, Zheng Lin, Zehang Lin, Lin Li, Miao Yang, Xia Du, Zihan Fang, Zhaolu Kang, Dianxin Luan, Shunzhi Zhu</div>
<div class="meta-line">First: 2026-02-02T18:47:16+00:00 · Latest: 2026-02-02T18:47:16+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多服务器联邦学习的冲突感知客户端选择</div>
<div class="mono" style="margin-top:8px">联邦学习作为一种分布式机器学习范式，支持客户端在不暴露原始数据的情况下协同训练模型，从而保护用户隐私并降低通信开销。然而传统单服务器联邦学习因需聚合海量客户端模型而存在高通信延迟问题。多服务器联邦学习虽能将负载分散至边缘服务器，但客户端覆盖重叠与选择失序常引发资源竞争，导致带宽冲突与训练失败。为此，我们提出一种融合冲突风险预测的去中心化强化学习方法（RL-CRP），以优化多服务器联邦学习系统中的客户端选择。具体而言，各服务器基于稀疏历史选择序列，通过分类隐马尔可夫模型预测客户端选择冲突概率，并引入公平感知奖励机制以促进客户端长期参与，从而最小化训练延迟与资源竞争。大量实验表明，所提RL-CRP框架能有效降低服务器间冲突，在收敛速度与通信成本方面显著提升训练效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses communication latency and resource contention in multi-server federated learning (FL) caused by overlapping client coverage and uncoordinated selection. The proposed method, RL-CRP, employs a decentralized reinforcement learning approach with conflict risk prediction, where each server uses a categorical hidden Markov model to estimate client selection conflicts from sparse historical sequences and incorporates a fairness-aware reward mechanism to encourage long-term client participation. Experimental results show that RL-CRP effectively reduces inter-server conflicts and improves training efficiency, including faster convergence and lower communication costs.</div>
<div class="mono" style="margin-top:8px">为解决多服务器联邦学习中因客户端覆盖重叠和选择不协调导致的通信延迟与资源争用问题，本研究提出了RL-CRP，一种结合冲突风险预测的去中心化强化学习框架。该方法在每个服务器上使用分类隐马尔可夫模型，基于稀疏的历史客户端选择序列来预测冲突风险，并引入公平感知奖励机制以促进客户端的长期参与。实验结果表明，RL-CRP能有效减少服务器间冲突，显著提升训练效率，包括更快的收敛速度和更低的通信开销。</div>
</details>
</div>
<div class="card">
<div class="title">Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization</div>
<div class="meta-line">Authors: Patrick Cooper, Alvaro Velasquez</div>
<div class="meta-line">First: 2026-02-02T18:43:52+00:00 · Latest: 2026-02-02T18:43:52+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02451v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p &lt; 0.001, Cohen&#x27;s d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动因果实验者（ACE）：通过直接偏好优化学习干预策略</div>
<div class="mono" style="margin-top:8px">发现因果关系需要受控实验，但实验者面临序贯决策问题：每次干预揭示的信息都应指导后续尝试。传统方法（如随机抽样、贪婪信息最大化、轮询覆盖）孤立处理每个决策，无法从经验中学习自适应策略。我们提出主动因果实验者（ACE），将实验设计学习为序贯策略。核心洞见是：虽然绝对信息增益随知识积累而递减（导致基于价值的强化学习不稳定），但候选干预间的相对比较始终具有意义。ACE通过直接偏好优化利用这一特性，从成对干预比较中学习而非非平稳的奖励幅度。在合成基准测试、物理模拟和经济数据中，ACE在同等干预预算下较基线提升70-71%（p &lt; 0.001，Cohen&#x27;s d ~ 2）。值得注意的是，学习到的策略自主发现碰撞机制需对父变量集中干预——这一理论支撑的策略完全从经验中涌现。这表明基于偏好的学习能恢复原则性实验策略，通过习得的领域适应与理论形成互补。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the sequential decision-making challenge in causal discovery, where traditional methods like random sampling or greedy information gain fail to learn adaptive intervention strategies from experience. The authors propose Active Causal Experimentalist (ACE), which frames experimental design as a sequential policy and leverages Direct Preference Optimization to learn from pairwise comparisons between candidate interventions, circumventing the instability of learning from non-stationary absolute reward magnitudes. In experiments across synthetic benchmarks, physics simulations, and economic data, ACE achieves a 70-71% improvement over baselines under equal intervention budgets, and the learned policy autonomously discovers theoretically-grounded strategies, such as concentrating interventions on parent variables for collider mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究针对因果发现中的序列决策挑战，传统方法如随机采样或贪婪信息增益无法从经验中学习自适应干预策略。提出的主动因果实验者（ACE）通过将干预选择构建为序列问题来学习实验设计策略，并采用直接偏好优化方法，基于候选干预之间的成对比较进行训练，以处理绝对信息增益的非平稳性。在合成基准、物理模拟和经济数据上的实验结果表明，在相同干预预算下，ACE相比基线方法实现了70-71%的性能提升，且学习到的策略能自主发现理论驱动的策略，例如针对碰撞机制集中干预父变量。</div>
</details>
</div>
<div class="card">
<div class="title">Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation</div>
<div class="meta-line">Authors: Seo Taek Kong, R. Srikant</div>
<div class="meta-line">First: 2026-02-02T18:41:06+00:00 · Latest: 2026-02-02T18:41:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02445v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov&#x27;s inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非线性随机逼近的有限样本Wasserstein误差界与集中不等式</div>
<div class="mono" style="margin-top:8px">本文推导了非线性随机逼近算法在Wasserstein-$p$距离下的非渐近误差界。为获得末次迭代的显式有限样本保证，我们构建了一种耦合论证方法，将离散时间过程与极限Ornstein-Uhlenbeck过程进行比较。该分析适用于满足一般噪声条件的算法，包括鞅差序列和遍历马尔可夫链函数。作为补充，我们通过直接分析处理了Polyak-Ruppert平均的收敛速率，该分析在相同一般条件下成立。假设驱动噪声满足非渐近中心极限定理，我们证明归一化末次迭代以$γ_n^{1/6}$阶速率在$p$-Wasserstein距离下收敛于高斯分布（$γ_n$为步长）。类似地，Polyak-Ruppert平均在Wasserstein距离下以$n^{-1/6}$阶速率收敛。这些分布保证导出了高概率集中不等式，其优于基于矩界和马尔可夫不等式推导的结果。我们通过两个应用案例展示该方法实用性：（1）线性随机逼近中显式量化迭代从重尾到高斯行为的转变，从而弥合近期有限样本分析与渐近理论间的差距；（2）随机梯度下降中建立收敛至中心极限定理的速率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to provide explicit, non-asymptotic performance guarantees for nonlinear stochastic approximation algorithms, addressing the gap between finite-sample analyses and asymptotic theory. The method employs a coupling argument to compare the discrete-time algorithm&#x27;s last iterate to a limiting Ornstein-Uhlenbeck process, and separately analyzes the Polyak-Ruppert average, both under general noise conditions like martingale differences and Markovian noise. Key findings show that, assuming a non-asymptotic central limit theorem for the noise, the normalized last iterate converges in the p-Wasserstein distance at a rate of order γ_n^{1/6} (where γ_n is the step size), and the Polyak-Ruppert average converges at order n^{-1/6}, yielding improved high-probability concentration inequalities; these results are demonstrated in applications to linear stochastic approximation and stochastic gradient descent.</div>
<div class="mono" style="margin-top:8px">本文旨在为非线性随机逼近算法提供明确的非渐近性能保证，以弥补有限样本分析与渐近理论之间的差距。该方法采用耦合论证，将离散时间算法的最后迭代与极限的Ornstein-Uhlenbeck过程进行比较，并在包括鞅差和马尔可夫噪声在内的通用噪声条件下，单独分析了Polyak-Ruppert平均。主要实验结果表明，在假设噪声满足非渐近中心极限定理的情况下，归一化的最后迭代以γ_n^{1/6}的速率（其中γ_n为步长）在p-Wasserstein距离中收敛，而Polyak-Ruppert平均则以n^{-1/6}的速率收敛，从而得到了改进的高概率集中不等式；这些结果在线性随机逼近和随机梯度下降的应用中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE</div>
<div class="meta-line">Authors: Yuanteng Chen, Peisong Wang, Nanxin Zeng, Yuantian Shao, Gang Li, Jing Liu, Jian Cheng</div>
<div class="meta-line">First: 2026-02-02T18:39:33+00:00 · Latest: 2026-02-02T18:39:33+00:00</div>
<div class="meta-line">Comments: 24 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02443v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02443v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>确定头部与不确定尾部：细粒度MoE中测试时扩展的专家采样方法</div>
<div class="mono" style="margin-top:8px">测试时扩展通过生成多个候选解提升大语言模型性能，但词元级采样需调节温度参数以权衡多样性与稳定性。细粒度混合专家模型每层包含数百个训练有素的专家，且每个词元激活多个专家，其丰富的路由空间提供了尚未探索的替代方案。我们通过实证分析细粒度MoE路由机制，发现一种信息性模式：路由器评分呈现确定性的高置信度专家头部与不确定的低置信度候选尾部。当激活较少专家时，单次贪婪解码的准确率保持稳定，但多样本pass@n指标显著下降——这表明确定性头部主导核心推理能力，而不确定尾部与推理多样性相关。基于此发现，我们提出专家采样法：一种无需训练的方法，在保留高置信度选择的同时向不确定尾部注入可控随机性，实现多样化生成且不破坏输出稳定性。在数学、知识推理和代码任务的多类细粒度MoE模型评估中，专家采样法持续提升pass@n与基于验证的准确率。以Qwen3-30B-A3B-Instruct模型在GPQA-Diamond数据集测试为例（32个并行样本），pass@32从85.4%提升至91.9%，采用N选最优验证时准确率从59.1%提升至62.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of balancing diversity and stability in test-time scaling for large language models, where token-level sampling often requires careful temperature tuning. The method leverages fine-grained mixture-of-experts (MoE) models by analyzing their routing patterns, identifying a certain head of high-confidence experts and an uncertain tail of low-confidence candidates. The proposed Expert-Sample approach preserves high-confidence selections while injecting stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Experimental results on math, knowledge reasoning, and code tasks show consistent improvements in pass@n and verification-based accuracy; for instance, on Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 samples, pass@32 increased from 85.4% to 91.9% and accuracy rose from 59.1% to 62.6% with Best-of-N verification.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型测试时扩展中平衡多样性与稳定性的挑战，其中令牌级采样需要精细的温度调节。该方法利用细粒度混合专家模型中的路由模式，观察到路由器分数呈现高置信度专家的确定头部和低置信度候选的不确定尾部。提出的Expert-Sample方法保留高置信度选择，同时在不确定尾部注入受控随机性，实现多样化生成而不破坏输出稳定性。在数学、知识推理和代码任务上的实验结果表明，pass@n和基于验证的准确性均获得持续提升，Qwen3-30B-A3B-Instruct在GPQA-Diamond数据集上使用32个并行样本时，pass@32从85.4%提升至91.9%，准确性从59.1%提高至62.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization</div>
<div class="meta-line">Authors: Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz, Duygu Erisken, Rana Irem Turhan</div>
<div class="meta-line">First: 2026-02-02T18:34:48+00:00 · Latest: 2026-02-02T18:34:48+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, 4 tables. Submitted to IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02439v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Edge AI applications increasingly require ultra-low-power, low-latency inference. Neuromorphic computing based on event-driven spiking neural networks (SNNs) offers an attractive path, but practical deployment on resource-constrained devices is limited by training difficulty, hardware-mapping overheads, and sensitivity to temporal dynamics. We present NeuEdge, a framework that combines adaptive SNN models with hardware-aware optimization for edge deployment. NeuEdge uses a temporal coding scheme that blends rate and spike-timing patterns to reduce spike activity while preserving accuracy, and a hardware-aware training procedure that co-optimizes network structure and on-chip placement to improve utilization on neuromorphic processors. An adaptive threshold mechanism adjusts neuron excitability from input statistics, reducing energy consumption without degrading performance. Across standard vision and audio benchmarks, NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency on edge hardware and an estimated 847 GOp/s/W energy efficiency. A case study on an autonomous-drone workload shows up to 312x energy savings relative to conventional deep neural networks while maintaining real-time operation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向边缘AI的能效神经形态计算：基于自适应脉冲神经网络与硬件感知优化的框架</div>
<div class="mono" style="margin-top:8px">边缘AI应用日益需要超低功耗、低延迟的推理能力。基于事件驱动脉冲神经网络（SNN）的神经形态计算提供了有前景的解决方案，但在资源受限设备上的实际部署仍受限于训练难度、硬件映射开销及对时序动态的敏感性。本文提出NeuEdge框架，将自适应SNN模型与硬件感知优化相结合以支持边缘部署。该框架采用融合发放率与脉冲时序模式的时序编码方案，在保持精度的同时降低脉冲活动量；通过硬件感知训练流程协同优化网络结构与片上布局，提升神经形态处理器的利用率；自适应阈值机制根据输入统计量调整神经元兴奋性，在不降低性能的前提下减少能耗。在标准视觉与音频基准测试中，NeuEdge在边缘硬件上实现91-96%的准确率，推理延迟低至2.3毫秒，能效预估达847 GOp/s/W。自动驾驶无人机任务的案例研究表明，相比传统深度神经网络，在保持实时运行的同时可实现高达312倍的节能效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Edge AI applications demand ultra-low-power and low-latency inference, but deploying neuromorphic computing with spiking neural networks (SNNs) on resource-constrained devices faces challenges in training, hardware mapping, and temporal sensitivity. To address this, the authors propose NeuEdge, a framework that integrates adaptive SNN models with hardware-aware optimization, employing a temporal coding scheme mixing rate and spike-timing patterns to reduce spike activity, a hardware-aware training procedure co-optimizing network structure and on-chip placement, and an adaptive threshold mechanism adjusting neuron excitability from input statistics. Experimental results on vision and audio benchmarks show NeuEdge achieves 91-96% accuracy with up to 2.3 ms inference latency and an estimated 847 GOp/s/W energy efficiency, while a case study on an autonomous-drone workload demonstrates up to 312x energy savings compared to conventional deep neural networks while maintaining real-time operation.</div>
<div class="mono" style="margin-top:8px">边缘AI应用需要超低功耗和低延迟推理，但在资源受限设备上部署基于脉冲神经网络（SNN）的神经形态计算面临训练困难、硬件映射开销和时间动态敏感性等挑战。为此，研究者提出了NeuEdge框架，该框架结合自适应SNN模型与硬件感知优化，采用融合速率和脉冲时序模式的时序编码方案以减少脉冲活动，通过硬件感知训练协同优化网络结构和片上布局，并利用自适应阈值机制根据输入统计调整神经元兴奋性。在视觉和音频基准测试中，NeuEdge实现了91-96%的准确率，推理延迟低至2.3毫秒，能效估计达847 GOp/s/W；在自主无人机工作负载的案例研究中，相比传统深度神经网络，NeuEdge在保持实时操作的同时实现了高达312倍的节能效果。</div>
</details>
</div>
<div class="card">
<div class="title">Maximizing Reliability with Bayesian Optimization</div>
<div class="meta-line">Authors: Jack M. Buckingham, Ivo Couckuyt, Juergen Branke</div>
<div class="meta-line">First: 2026-02-02T18:31:58+00:00 · Latest: 2026-02-02T18:31:58+00:00</div>
<div class="meta-line">Comments: 25 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02432v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02432v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯优化的可靠性最大化方法</div>
<div class="mono" style="margin-top:8px">贝叶斯优化是一种针对昂贵黑箱优化问题的样本高效技术。在制造业中，常需在随机扰动下最大化设计可靠性（即最小化失效概率），这类问题可能涉及极低失效概率（$P_\mathrm{fail} = 10^{-6}-10^{-8}$）。本研究提出两种基于汤普森采样与知识梯度的贝叶斯优化方法，后者通过近似一步贝叶斯最优策略来最小化失效概率的对数值。两种方法均采用重要性采样以应对极低失效概率场景。实验结果表明，所提方法在极端与非极端工况下均优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of maximizing design reliability, which involves minimizing extremely small failure probabilities (e.g., 10^{-6} to 10^{-8}) in manufacturing under random perturbations, a problem where standard Bayesian optimization (BO) may struggle with sample efficiency. The method introduces two BO variants: one based on Thompson sampling and another on knowledge gradient, which approximates the optimal policy for minimizing the log failure probability, both enhanced with importance sampling to accurately target rare failure events. Experimental results demonstrate that these proposed methods outperform existing approaches across both extreme and non-extreme failure probability regimes.</div>
<div class="mono" style="margin-top:8px">本研究针对制造业中最大化设计可靠性的挑战，其中故障概率极低（低至10^{-6}至10^{-8}），使得优化变得困难。方法提出了两种贝叶斯优化方法：一种基于汤普森采样，另一种基于知识梯度，后者近似于最小化故障概率对数的最优策略，两者均结合重要性采样以高效针对这些罕见事件。实验结果表明，所提出的方法在极端和非极端故障概率范围内均优于现有技术。</div>
</details>
</div>
<div class="card">
<div class="title">Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning</div>
<div class="meta-line">Authors: Filip Kovačević, Hong Chang Ji, Denny Wu, Mahdi Soltanolkotabi, Marco Mondelli</div>
<div class="meta-line">First: 2026-02-02T18:31:51+00:00 · Latest: 2026-02-02T18:31:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02431v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02431v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It is folklore that reusing training data more than once can improve the statistical efficiency of gradient-based learning. However, beyond linear regression, the theoretical advantage of full-batch gradient descent (GD, which always reuses all the data) over one-pass stochastic gradient descent (online SGD, which uses each data point only once) remains unclear. In this work, we consider learning a $d$-dimensional single-index model with a quadratic activation, for which it is known that one-pass SGD requires $n\gtrsim d\log d$ samples to achieve weak recovery. We first show that this $\log d$ factor in the sample complexity persists for full-batch spherical GD on the correlation loss; however, by simply truncating the activation, full-batch GD exhibits a favorable optimization landscape at $n \simeq d$ samples, thereby outperforming one-pass SGD (with the same activation) in statistical efficiency. We complement this result with a trajectory analysis of full-batch GD on the squared loss from small initialization, showing that $n \gtrsim d$ samples and $T \gtrsim\log d$ gradient steps suffice to achieve strong (exact) recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全批次梯度下降优于单次随机梯度下降：单索引学习中的样本复杂度分离</div>
<div class="mono" style="margin-top:8px">传统观点认为，多次复用训练数据可提升基于梯度的学习的统计效率。然而，在线性回归之外，全批次梯度下降（GD，始终复用所有数据）相对于单次随机梯度下降（在线SGD，每个数据点仅使用一次）的理论优势尚不明确。本研究针对具有二次激活的d维单索引模型进行学习分析，已知单次SGD需要n≳d log d样本才能实现弱恢复。我们首先证明，在相关性损失上，全批次球形GD的样本复杂度中仍存在log d因子；但仅通过截断激活函数，全批次GD在n≃d样本量下即可呈现有利的优化景观，从而在统计效率上超越单次SGD（使用相同激活函数）。我们进一步通过小初始化条件下全批次GD在平方损失上的轨迹分析佐证该结论，表明n≳d样本与T≳log d梯度步数足以实现强（精确）恢复。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates whether full-batch gradient descent (GD) offers a statistical efficiency advantage over one-pass stochastic gradient descent (SGD) beyond linear regression, focusing on learning a d-dimensional single-index model with a quadratic activation. The method analyzes full-batch GD on the correlation loss and squared loss, revealing that while standard GD suffers a log d factor in sample complexity similar to one-pass SGD, a simple activation truncation enables full-batch GD to achieve a favorable optimization landscape with only n ≃ d samples, outperforming one-pass SGD. Key experimental findings show that with truncated activation, full-batch GD requires n ≳ d samples and T ≳ log d gradient steps for strong recovery, demonstrating a clear separation in sample complexity where full-batch GD is more efficient.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在线性回归之外，全批次梯度下降（GD）是否比单次随机梯度下降（SGD）具有统计效率优势，重点关注具有二次激活的单索引模型学习。方法包括分析全批次GD在相关损失和截断激活上的表现，以及对平方损失从小初始化开始的轨迹分析。主要实验结果表明，虽然全批次GD在相关损失上仍需要n≳d log d个样本，与单次SGD相同，但使用截断激活能使全批次GD在仅n≃d个样本下获得有利的优化景观，从而超越单次SGD；此外，轨迹分析证实n≳d个样本和T≳log d梯度步数足以实现强恢复。</div>
</details>
</div>
<div class="card">
<div class="title">Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning</div>
<div class="meta-line">Authors: Qihao Wen, Jiahao Wang, Yang Nan, Pengfei He, Ravi Tandon, Han Xu</div>
<div class="meta-line">First: 2026-02-02T18:27:26+00:00 · Latest: 2026-02-02T18:27:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02427v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02427v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language Models (LLMs) have achieved significant breakthroughs across diverse domains; however, they can still produce unreliable or misleading outputs. For responsible LLM application, Uncertainty Quantification (UQ) techniques are used to estimate a model&#x27;s uncertainty about its outputs, indicating the likelihood that those outputs may be problematic. For LLM reasoning tasks, it is essential to estimate the uncertainty not only for the final answer, but also for the intermediate steps of the reasoning, as this can enable more fine-grained and targeted interventions. In this study, we explore what UQ metrics better reflect the LLM&#x27;s ``intermediate uncertainty&#x27;&#x27;during reasoning. Our study reveals that an LLMs&#x27; incorrect reasoning steps tend to contain tokens which are highly sensitive to the perturbations on the preceding token embeddings. In this way, incorrect (uncertain) intermediate steps can be readily identified using this sensitivity score as guidance in practice. In our experiments, we show such perturbation-based metric achieves stronger uncertainty quantification performance compared with baseline methods such as token (generation) probability and token entropy. Besides, different from approaches that rely on multiple sampling, the perturbation-based metrics offer better simplicity and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>嵌入扰动或能更好反映大语言模型推理中的不确定性</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）已在多个领域取得重大突破，但仍可能产生不可靠或误导性输出。为负责任地应用LLMs，不确定性量化（UQ）技术被用于评估模型对其输出的不确定程度，以提示输出可能存在问题的概率。在LLM推理任务中，不仅需评估最终答案的不确定性，还需评估推理中间步骤的不确定性，这有助于实现更精细化和有针对性的干预。本研究探讨了哪些UQ指标能更好地反映LLM推理过程中的“中间不确定性”。研究发现，LLMs的错误推理步骤往往包含对前序词元嵌入扰动高度敏感的词汇。实践中，可利用这种敏感性评分作为指导，快速识别错误（不确定）的中间步骤。实验表明，相较于词元生成概率、词元熵等基线方法，这种基于扰动的指标实现了更强的不确定性量化性能。此外，与依赖多次采样的方法不同，基于扰动的指标具有更好的简洁性和效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable more targeted interventions in large language model reasoning, this study investigates uncertainty quantification metrics that can better reflect uncertainty at intermediate reasoning steps. The method proposes using the sensitivity of tokens to perturbations on preceding token embeddings as a metric, revealing that incorrect reasoning steps contain tokens with high sensitivity. Experiments demonstrate that this perturbation-based metric outperforms baseline methods like token probability and entropy in uncertainty quantification, while also offering greater simplicity and efficiency compared to multiple-sampling approaches.</div>
<div class="mono" style="margin-top:8px">为了通过对大语言模型推理过程中的中间步骤而不仅仅是最终答案进行不确定性量化，以实现更有针对性的干预，本研究探讨了哪些不确定性量化指标能更好地反映中间不确定性。该方法引入了一种基于扰动的指标，通过测量令牌对前序嵌入扰动的敏感性，发现错误推理步骤中的令牌对此类扰动高度敏感。实验表明，该指标在不确定性量化性能上优于令牌概率和熵等基线方法，同时相比多重采样方法具有更好的简洁性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization</div>
<div class="meta-line">Authors: Amaru Caceres Arroyo, Lea Bogensperger, Ahmed Allam, Michael Krauthammer, Konrad Schindler, Dominik Narnhofer</div>
<div class="meta-line">First: 2026-02-02T18:25:33+00:00 · Latest: 2026-02-02T18:25:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02425v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02425v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Protein fitness optimization is challenged by a vast combinatorial landscape where high-fitness variants are extremely sparse. Many current methods either underperform or require computationally expensive gradient-based sampling. We present CHASE, a framework that repurposes the evolutionary knowledge of pretrained protein language models by compressing their embeddings into a compact latent space. By training a conditional flow-matching model with classifier-free guidance, we enable the direct generation of high-fitness variants without predictor-based guidance during the ODE sampling steps. CHASE achieves state-of-the-art performance on AAV and GFP protein design benchmarks. Finally, we show that bootstrapping with synthetic data can further enhance performance in data-constrained settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于蛋白质语言模型潜空间流匹配的适应性优化方法</div>
<div class="mono" style="margin-top:8px">蛋白质适应性优化面临组合空间巨大且高适应性变异体极度稀疏的挑战。现有方法常存在性能不足或需昂贵梯度采样的局限。本研究提出CHASE框架，通过将预训练蛋白质语言模型的嵌入压缩至紧凑潜空间，利用其进化知识。结合无分类器引导的条件流匹配模型训练，实现在ODE采样步骤中无需预测器引导即可直接生成高适应性变异体。CHASE在AAV和GFP蛋白质设计基准测试中取得最优性能。实验表明，在数据受限场景下，通过合成数据自举可进一步提升模型表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of navigating the vast combinatorial landscape of protein fitness optimization, where high-fitness variants are sparse and many methods are either ineffective or computationally expensive. The method, CHASE, repurposes a pretrained protein language model by compressing its embeddings into a compact latent space and training a conditional flow-matching model with classifier-free guidance, enabling the direct generation of high-fitness variants without predictor-based guidance during ODE sampling. Experiments demonstrate state-of-the-art performance on AAV and GFP protein design benchmarks, and further show that bootstrapping with synthetic data enhances performance in data-constrained settings.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决蛋白质适应性优化中组合空间巨大、高适应性变体极其稀疏的挑战，现有方法往往效率低下或需要计算成本高昂的梯度采样。提出的CHASE框架通过将预训练蛋白质语言模型的嵌入压缩到紧凑的潜在空间中，并训练一个带有无分类器引导的条件流匹配模型，从而能够在ODE采样步骤中无需基于预测器的引导直接生成高适应性变体。实验结果表明，CHASE在AAV和GFP蛋白质设计基准测试中取得了最先进的性能，并且进一步证明在数据受限的设置中，使用合成数据进行自举可以进一步提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">Poly-attention: a general scheme for higher-order self-attention</div>
<div class="meta-line">Authors: Sayak Chakrabarti, Toniann Pitassi, Josh Alman</div>
<div class="meta-line">First: 2026-02-02T18:24:53+00:00 · Latest: 2026-02-02T18:24:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02422v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02422v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The self-attention mechanism, at the heart of the Transformer model, is able to effectively model pairwise interactions between tokens. However, numerous recent works have shown that it is unable to perform basic tasks involving detecting triples of correlated tokens, or compositional tasks where multiple input tokens need to be referenced to generate a result. Some higher-dimensional alternatives to self-attention have been proposed to address this, including higher-order attention and Strassen attention, which can perform some of these polyadic tasks in exchange for slower, superquadratic running times.
  In this work, we define a vast class of generalizations of self-attention, which we call poly-attention mechanisms. Our mechanisms can incorporate arbitrary higher-order (tensor) computations as well as arbitrary relationship structures between the input tokens, and they include the aforementioned alternatives as special cases. We then systematically study their computational complexity and representational strength, including giving new algorithms and matching complexity-theoretic lower bounds on the time complexity of computing the attention matrix exactly as well as approximately, and tightly determining which polyadic tasks they can each perform. Our results give interesting trade-offs between different desiderata for these mechanisms, including a tight relationship between how expressive a mechanism is, and how large the coefficients in the model may be so that the mechanism can be approximated in almost-linear time.
  Notably, we give a new attention mechanism which can be computed exactly in quadratic time, and which can perform function composition for any fixed number of functions. Prior mechanisms, even for just composing two functions, could only be computed in superquadratic time, and our new lower bounds show that faster algorithms for them are not possible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多注意力：高阶自注意力的一般化框架</div>
<div class="mono" style="margin-top:8px">作为Transformer模型核心的自注意力机制能有效建模词元间的成对交互，但近期多项研究表明其无法处理涉及检测三个相关词元的基础任务，或需要参照多个输入词元生成结果的组合任务。为此提出的高阶替代方案包括高阶注意力和Strassen注意力，这些方法能以超二次方运行时间为代价处理部分多元任务。本文定义了一类广泛的自注意力泛化形式——多注意力机制，其能纳入任意高阶（张量）计算及输入词元间的任意关系结构，并将前述替代方案作为特例涵盖。我们系统研究了其计算复杂度与表征能力，包括提出新算法、给出计算注意力矩阵（精确与近似）的时间复杂度匹配的理论下界，并精确判定各类机制可执行的多元任务。研究结果揭示了这些机制在不同需求间的权衡关系，特别是机制表达能力与模型系数大小（以实现近线性时间近似计算）之间的紧致关联。值得注意的是，我们提出了一种可在二次时间内精确计算的新注意力机制，该机制能执行任意固定数量函数的组合运算。此前即使仅组合两个函数的机制也需超二次时间计算，而我们的新下界证明其不存在更快的算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The self-attention mechanism in Transformers is limited to pairwise token interactions, failing at tasks requiring detection of triples or compositional operations. To address this, the paper introduces poly-attention, a broad class of higher-order attention mechanisms that incorporate tensor computations and arbitrary token relationships, encompassing prior methods like higher-order and Strassen attention. Through systematic analysis, the authors establish computational complexity bounds and representational capabilities, demonstrating trade-offs between expressiveness and efficient approximation. A key contribution is a new quadratic-time poly-attention mechanism capable of composing any fixed number of functions, outperforming prior superquadratic methods, with lower bounds proving faster exact computation is impossible.</div>
<div class="mono" style="margin-top:8px">Transformer中的自注意力机制仅限于处理标记间的成对交互，无法完成需要检测三元相关标记或组合操作的任务。为此，本文提出了一类广泛的泛化机制，称为多注意力机制，它融合了高阶张量计算和任意的标记关系结构，涵盖了先前的高阶注意力变体。通过对计算复杂性和表示能力的系统分析，包括新算法和匹配的下界，该研究建立了表达力与高效近似之间的权衡，特别提出了一种新的二次时间精确计算机制，能够组合任意固定数量的函数，而先前的方法需要超二次时间。</div>
</details>
</div>
<div class="card">
<div class="title">How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models</div>
<div class="meta-line">Authors: Parth Asawa, Alan Zhu, Abby O&#x27;Neill, Matei Zaharia, Alexandros G. Dimakis, Joseph E. Gonzalez</div>
<div class="meta-line">First: 2025-10-02T18:02:39+00:00 · Latest: 2026-02-02T18:23:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02453v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.02453v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier language models are deployed as black-box services, where model weights cannot be modified and customization is limited to prompting. We introduce Advisor Models, a method to train small open-weight models to generate dynamic, per-instance natural language advice that improves the capabilities of black-box frontier models. Advisor Models improve GPT-5&#x27;s performance on RuleArena (Taxes) by 71%, reduce Gemini 3 Pro&#x27;s steps taken in SWE agent tasks by 24.6%, and outperform static prompt optimizers in personalizing GPT-5 to user preferences (85-100% vs. 40-60%). We also find that advisors are transferable: an advisor trained with a low-cost student model still transfers improvements to a frontier model. Moreover, Advisor Models are robust: we observe no degradation on other benchmarks than the pipeline is trained on. Our method shows how to perform parametric optimization for black-box frontier models in a practical and cost-effective way.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>如何训练你的顾问模型：利用顾问模型引导黑盒大语言模型</div>
<div class="mono" style="margin-top:8px">前沿语言模型通常以黑盒服务形式部署，其模型权重无法修改，定制化仅限于提示工程。我们提出顾问模型方法，通过训练小型开放权重模型来生成动态的、针对每个实例的自然语言建议，从而提升黑盒前沿模型的能力。该方法使GPT-5在RuleArena（税务）任务上的性能提升71%，将Gemini 3 Pro在SWE智能体任务中的步骤减少24.6%，并在个性化适配GPT-5用户偏好方面显著优于静态提示优化器（85-100%对比40-60%）。研究还发现顾问模型具备可迁移性：基于低成本学生模型训练的顾问仍能提升前沿模型性能。此外，顾问模型具有鲁棒性：在训练流程未涉及的其他基准测试中未观察到性能下降。本方法为黑盒前沿模型提供了一种实用且高性价比的参数化优化路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of customizing black-box frontier language models, where only prompting is available. The method introduces Advisor Models, which are small open-weight models trained to generate dynamic natural language advice per instance to steer black-box models. Experiments show that Advisor Models improve GPT-5&#x27;s performance on RuleArena (Taxes) by 71%, reduce Gemini 3 Pro&#x27;s steps in SWE agent tasks by 24.6%, and outperform static prompt optimizers in personalizing GPT-5 to user preferences (85-100% vs. 40-60%), while also demonstrating transferability and robustness without degrading performance on other benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究针对黑盒前沿语言模型仅能通过提示进行定制的局限性，提出了一种训练小型开放权重模型生成动态自然语言建议的方法，以指导黑盒模型。实验结果表明，该方法将GPT-5在RuleArena（Taxes）上的性能提升71%，使Gemini 3 Pro在SWE代理任务中的步骤减少24.6%，在个性化GPT-5适应用户偏好方面优于静态提示优化器（85-100%对比40-60%），并展示了从低成本学生模型到前沿模型的可迁移性，同时在其他基准测试中保持稳健性而无性能下降。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Knowledge Tracing Models</div>
<div class="meta-line">Authors: Joshua Mitton, Prarthana Bhattacharyya, Ralph Abboud, Simon Woodhead</div>
<div class="meta-line">First: 2025-09-25T20:06:02+00:00 · Latest: 2026-02-02T18:21:09+00:00</div>
<div class="meta-line">Comments: 10 pages, 7 figures. Joshua Mitton and Prarthana Bhattacharyya contributed equally to this paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21514v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21514v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The main focus of research on Knowledge Tracing (KT) models is on model developments with the aim of improving predictive accuracy. Most of these models make the most incorrect predictions when students choose a distractor, leading to student errors going undetected. We present an approach to add new capabilities to KT models by capturing predictive uncertainty and demonstrate that a larger predictive uncertainty aligns with model incorrect predictions. We show that uncertainty in KT models is informative and that this signal would be pedagogically useful for application in an educational learning platform that can be used in a limited resource setting where understanding student ability is necessary.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定性感知的知识追踪模型</div>
<div class="mono" style="margin-top:8px">知识追踪模型的研究主要聚焦于提升预测精度的模型开发。多数模型在学生选择干扰项时会产生最多错误预测，导致学生错误未被察觉。我们提出一种通过捕捉预测不确定性来增强知识追踪模型能力的方法，并证明较大的预测不确定性与模型错误预测具有一致性。研究表明知识追踪模型中的不确定性信息具有参考价值，该信号在教学应用上具有实用性，可应用于需理解学生能力且资源有限的教育学习平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of existing Knowledge Tracing (KT) models, which often make their most incorrect predictions when students select distractors, thereby failing to detect student errors. The proposed method enhances KT models by incorporating the capability to capture predictive uncertainty, demonstrating that higher predictive uncertainty correlates with model prediction errors. Experimental findings indicate that this uncertainty signal is informative and could be pedagogically valuable for educational platforms, particularly in resource-limited settings where accurately assessing student ability is crucial.</div>
<div class="mono" style="margin-top:8px">本研究针对现有知识追踪模型在学生选择干扰项时预测错误率较高、导致学习错误未被察觉的局限性。作者提出通过引入预测不确定性估计来增强知识追踪模型，并证明较高的预测不确定性与模型的预测错误相关。实验结果表明，这种不确定性信号具有信息价值，对于教育平台（特别是在资源有限、需要准确评估学生能力的场景中）具有潜在的教学应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">Trust Region Continual Learning as an Implicit Meta-Learner</div>
<div class="meta-line">Authors: Zekun Wang, Anant Gupta, Christopher J. MacLellan</div>
<div class="meta-line">First: 2026-02-02T18:19:16+00:00 · Latest: 2026-02-02T18:19:16+00:00</div>
<div class="meta-line">Comments: 19 pages, 23 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02417v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02417v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning aims to acquire tasks sequentially without catastrophic forgetting, yet standard strategies face a core tradeoff: regularization-based methods (e.g., EWC) can overconstrain updates when task optima are weakly overlapping, while replay-based methods can retain performance but drift due to imperfect replay. We study a hybrid perspective: \emph{trust region continual learning} that combines generative replay with a Fisher-metric trust region constraint. We show that, under local approximations, the resulting update admits a MAML-style interpretation with a single implicit inner step: replay supplies an old-task gradient signal (query-like), while the Fisher-weighted penalty provides an efficient offline curvature shaping (support-like). This yields an emergent meta-learning property in continual learning: the model becomes an initialization that rapidly \emph{re-converges} to prior task optima after each task transition, without explicitly optimizing a bilevel objective. Empirically, on task-incremental diffusion image generation and continual diffusion-policy control, trust region continual learning achieves the best final performance and retention, and consistently recovers early-task performance faster than EWC, replay, and continual meta-learning baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信任区域持续学习：一种隐式元学习器</div>
<div class="mono" style="margin-top:8px">持续学习旨在顺序学习任务而不发生灾难性遗忘，但标准策略面临核心权衡：当任务最优解重叠度较低时，基于正则化的方法（如EWC）可能过度约束更新；而基于回放的方法虽能保持性能，却会因不完美的回放产生漂移。我们研究了一种混合视角：结合生成式回放与费希尔度量信任区域约束的信任区域持续学习。我们证明，在局部近似下，所得更新可解释为仅含单步隐式内层更新的MAML式框架：回放提供旧任务梯度信号（类查询），费希尔加权惩罚则提供高效的离线曲率塑形（类支持）。这使持续学习呈现出一种新兴的元学习特性：模型成为能在每次任务切换后快速重收敛至先前任务最优解的初始化状态，而无需显式优化双层目标。在任务增量扩散图像生成与持续扩散策略控制的实验中，信任区域持续学习取得了最佳最终性能与保持率，且比EWC、回放及持续元学习基线更快恢复早期任务性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the trade-off in continual learning between regularization methods, which can overly restrict updates when task optima differ, and replay methods, which may suffer from performance drift. The proposed method, trust region continual learning, integrates generative replay with a Fisher-information-matrix-based trust region constraint. Under local approximations, this update is shown to function like a single-step implicit meta-learner, where replay provides gradient signals and the penalty shapes curvature, enabling the model to rapidly reconverge to previous task optima after new tasks. Experiments on task-incremental image generation and policy control with diffusion models demonstrate superior final performance, retention, and faster recovery of early-task performance compared to EWC, replay, and meta-learning baselines.</div>
<div class="mono" style="margin-top:8px">本研究针对持续学习中正则化方法（在任务最优解重叠度低时可能过度约束更新）与回放方法（可能因不完美的回放导致性能漂移）之间的核心权衡问题。提出的方法结合了生成式回放与基于费雪信息矩阵的信任区域约束，形成了信任区域持续学习。在局部近似下，该更新可被解释为类似MAML的单步隐式元学习器，其中回放提供梯度信号，而信任区域则塑造曲率。在任务增量图像生成和持续扩散策略控制的实验中，该方法取得了最佳最终性能与保留率，并且比EWC、回放及持续元学习基线更快地恢复了早期任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">FS-DFM: Fast and Accurate Long Text Generation with Few-Step Diffusion Language Models</div>
<div class="meta-line">Authors: Amin Karimi Monsefi, Nikhil Bhendawade, Manuel Rafael Ciosici, Dominic Culver, Yizhe Zhang, Irina Belousova</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-24T23:59:05+00:00 · Latest: 2026-02-02T18:18:24+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20624v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.20624v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive language models (ARMs) deliver strong likelihoods, but are inherently serial: they generate one token per forward pass, which limits throughput and inflates latency for long sequences. Diffusion Language Models (DLMs) parallelize across positions and thus appear promising for language generation, yet standard discrete diffusion typically needs hundreds to thousands of model evaluations to reach high quality, trading serial depth for iterative breadth. We introduce FS-DFM, Few-Step Discrete Flow-Matching. A discrete flow-matching model designed for speed without sacrificing quality. The core idea is simple: make the number of sampling steps an explicit parameter and train the model to be consistent across step budgets, so one big move lands where many small moves would. We pair this with a reliable update rule that moves probability in the right direction without overshooting, and with strong teacher guidance distilled from long-run trajectories. Together, these choices make few-step sampling stable, accurate, and easy to control. On language modeling benchmarks, FS-DFM with 8 sampling steps achieves perplexity parity with a 1,024-step discrete-flow baseline for generating 1,024 tokens using a similar-size model, delivering up to 128 times faster sampling and corresponding latency/throughput gains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FS-DFM：基于少步扩散语言模型的快速准确长文本生成方法</div>
<div class="mono" style="margin-top:8px">自回归语言模型（ARMs）虽能提供较强的似然性，但其本质是串行生成：每次前向传播仅产生一个词元，导致长序列生成时吞吐量受限且延迟增加。扩散语言模型（DLMs）通过跨位置并行化展现出语言生成潜力，但标准离散扩散通常需数百至数千次模型评估才能达到高质量，实则是以迭代广度换取串行深度。本文提出FS-DFM（少步离散流匹配），这是一种兼顾速度与质量的离散流匹配模型。其核心思想简明：将采样步数设为显式参数，训练模型在不同步数预算下保持一致性，使单次大幅更新能达到多次小幅更新的效果。我们结合了可靠更新规则（确保概率定向移动而不超调）以及从长程轨迹中提炼的强教师指导，共同实现了稳定、准确且易控的少步采样。在语言建模基准测试中，使用8步采样的FS-DFM在生成1,024个词元时，与1,024步离散流基线模型（规模相近）达到同等困惑度，采样速度提升高达128倍，并相应获得延迟降低与吞吐量提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the serial generation bottleneck of autoregressive language models and the high computational cost of standard discrete diffusion models, this paper proposes FS-DFM, a few-step discrete flow-matching model for fast long text generation. The method explicitly parameterizes the number of sampling steps and trains the model to be consistent across different step budgets, enabling large, accurate updates in few steps, supported by a stable update rule and distilled teacher guidance. Experimental results on language modeling benchmarks show that FS-DFM with only 8 sampling steps matches the perplexity of a 1,024-step discrete-flow baseline for generating 1,024 tokens, achieving up to 128× faster sampling with comparable model size.</div>
<div class="mono" style="margin-top:8px">为解决自回归语言模型的串行生成瓶颈和标准离散扩散模型的高计算成本，本文提出了FS-DFM，一种用于快速生成长文本的少步离散流匹配模型。该方法将采样步数显式参数化，并训练模型在不同步数预算下保持一致性，通过稳定的更新规则和蒸馏的教师指导，实现少步内的大幅准确更新。在语言建模基准测试中，仅使用8个采样步的FS-DFM在生成1,024个词元时，达到了与1,024步离散流基线相当的困惑度，在模型规模相近的情况下实现了高达128倍的采样加速。</div>
</details>
</div>
<div class="card">
<div class="title">Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models</div>
<div class="meta-line">Authors: Vivienne Pelletier, Daniel J. Rivera, Obinna Nwokonkwo, Steven A. Wilson, Christopher L. Muhich</div>
<div class="meta-line">First: 2026-02-02T18:15:50+00:00 · Latest: 2026-02-02T18:15:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02415v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02415v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern machine learning has achieved remarkable success on many problems, but this success often depends on the existence of large, labeled datasets. While active learning can dramatically reduce labeling cost when annotations are expensive, early performance is frequently dominated by the initial seed set, typically chosen at random. In many applications, however, related or approximate datasets are readily available and can be leveraged to construct a better seed set. We introduce a new method for selecting the seed data set for active learning, Active-Transfer Bagging (ATBagging). ATBagging estimates the informativeness of candidate data point from a Bayesian interpretation of bagged ensemble models by comparing in-bag and out-of-bag predictive distributions from the labeled dataset, yielding an information-gain proxy. To avoid redundant selections, we impose feature-space diversity by sampling a determinantal point process (DPP) whose kernel uses Random Fourier Features and a quality-diversity factorization that incorporates the informativeness scores. This same blended method is used for selection of new data points to collect during the active learning phase. We evaluate ATBagging on four real-world datasets covering both target-transfer and feature-shift scenarios (QM9, ERA5, Forbes 2000, and Beijing PM2.5). Across seed sizes nseed = 10-100, ATBagging improves or ties early active learning and increases area under the learning-curve relative to alternative seed subset selection methodologies in almost all cases, with strongest benefits in low-data regimes. Thus, ATBagging provides a low-cost, high reward means to initiating active learning-based data collection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动迁移装袋：一种结合迁移学习与装袋模型加速主动学习数据获取的新方法</div>
<div class="mono" style="margin-top:8px">现代机器学习在许多问题上取得了显著成功，但这种成功往往依赖于大规模标注数据集的存在。虽然主动学习能在标注成本高昂时大幅降低标注开销，但其早期性能常受初始种子集（通常随机选择）的支配。然而在许多实际应用中，相关或近似数据集易于获取，可用于构建更优的种子集。本文提出一种为主动学习选择种子数据集的新方法——主动迁移装袋（ATBagging）。该方法通过比较标注数据集的袋内与袋外预测分布，基于装袋集成模型的贝叶斯解释来估计候选数据点的信息量，从而构建信息增益代理。为避免冗余选择，我们通过采样行列式点过程（DPP）来强制特征空间多样性，该过程的核函数采用随机傅里叶特征及融合信息量得分的质量-多样性分解。此混合方法同样适用于主动学习阶段新数据点的选择。我们在涵盖目标迁移与特征偏移场景的四个真实数据集（QM9、ERA5、福布斯2000、北京PM2.5）上评估ATBagging。在种子规模nseed=10-100的设定下，ATBagging在几乎所有案例中均优于或持平早期主动学习方法，并较其他种子子集选择方法提升了学习曲线下面积，在低数据场景中效益尤为显著。因此，ATBagging为启动基于主动学习的数据采集提供了一种低成本、高回报的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the sensitivity of active learning performance to the initial random seed set by proposing a method that leverages related datasets to construct better seed sets. The method, Active-Transfer Bagging (ATBagging), estimates data point informativeness using a Bayesian interpretation of bagged ensembles to compute an information-gain proxy and employs a determinantal point process with Random Fourier Features to enforce feature-space diversity during selection. Experimental evaluation on four real-world datasets (QM9, ERA5, Forbes 2000, Beijing PM2.5) demonstrates that ATBagging consistently improves or matches early active learning performance and increases the area under the learning curve across seed sizes from 10 to 100, with particularly strong benefits in low-data regimes.</div>
<div class="mono" style="margin-top:8px">本研究针对主动学习中早期性能严重依赖通常随机选择的初始种子集的问题，提出利用相关数据集构建更优种子集的方法。该方法名为主动迁移装袋（ATBagging），通过袋装集成模型的贝叶斯解释来估计候选数据点的信息量，比较袋内和袋外预测分布以获取信息增益代理，并利用基于随机傅里叶特征和质-量多样性分解的行列式点过程（DPP）来强制特征空间多样性以避免冗余选择。在四个真实数据集（QM9、ERA5、福布斯2000和北京PM2.5）上的实验评估表明，在种子规模为10至100的情况下，ATBagging在几乎所有情况下都改善或持平了早期主动学习性能，提高了学习曲线下面积，在低数据区域尤其有效。</div>
</details>
</div>
<div class="card">
<div class="title">Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank</div>
<div class="meta-line">Authors: Joshua Mitton, Prarthana Bhattacharyya, Digory Smith, Thomas Christie, Ralph Abboud, Simon Woodhead</div>
<div class="meta-line">First: 2026-02-02T18:14:35+00:00 · Latest: 2026-02-02T18:14:35+00:00</div>
<div class="meta-line">Comments: 21 pages, 8 figures, 8 tables. Joshua Mitton and Prarthana Bhattacharyya contributed equally to this paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02414v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02414v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Timely and accurate identification of student misconceptions is key to improving learning outcomes and pre-empting the compounding of student errors. However, this task is highly dependent on the effort and intuition of the teacher. In this work, we present a novel approach for detecting misconceptions from student-tutor dialogues using large language models (LLMs). First, we use a fine-tuned LLM to generate plausible misconceptions, and then retrieve the most promising candidates among these using embedding similarity with the input dialogue. These candidates are then assessed and re-ranked by another fine-tuned LLM to improve misconception relevance. Empirically, we evaluate our system on real dialogues from an educational tutoring platform. We consider multiple base LLM models including LLaMA, Qwen and Claude on zero-shot and fine-tuned settings. We find that our approach improves predictive performance over baseline models and that fine-tuning improves both generated misconception quality and can outperform larger closed-source models. Finally, we conduct ablation studies to both validate the importance of our generation and reranking steps on misconception generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于师生对话的误解诊断：生成、检索与重排序</div>
<div class="mono" style="margin-top:8px">及时准确地识别学生误解是提升学习成效、防止错误累积的关键。然而，这项任务高度依赖教师的经验与直觉。本研究提出一种利用大语言模型（LLM）从师生对话中检测误解的新方法：首先使用微调后的LLM生成可能的误解假设，再通过嵌入向量相似度从对话中检索最可能的候选误解，最后通过另一微调LLM对候选结果进行相关性评估与重排序。我们在真实教育辅导平台的对话数据上进行了实证评估，测试了包括LLaMA、Qwen和Claude在内的多种基础LLM在零样本和微调设置下的表现。实验表明，该方法相比基线模型提升了预测性能，微调不仅能改善生成误解的质量，甚至可超越规模更大的闭源模型。此外，消融研究验证了生成与重排序步骤对误解生成质量的重要作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To automate the timely and accurate identification of student misconceptions from tutoring dialogues, which traditionally relies on teacher effort and intuition, this work introduces a method using large language models (LLMs). The approach involves generating plausible misconceptions with a fine-tuned LLM, retrieving candidates via embedding similarity to the dialogue, and then reranking them with another fine-tuned LLM to enhance relevance. Evaluation on real tutoring platform dialogues with models like LLaMA, Qwen, and Claude shows that this pipeline improves predictive performance over baselines, with fine-tuning boosting misconception quality and even enabling smaller models to outperform larger closed-source ones; ablation studies confirm the importance of both the generation and reranking steps.</div>
<div class="mono" style="margin-top:8px">为了从辅导对话中自动、及时且准确地识别学生误解，这一传统上依赖教师努力和直觉的任务，本研究提出了一种利用大语言模型（LLM）的方法。该方法首先使用微调后的LLM生成可能的误解，然后通过嵌入相似性从对话中检索候选误解，最后用另一个微调的LLM对这些候选进行重新排序以提高相关性。在真实教育辅导平台对话上的实验评估中，使用了LLaMA、Qwen和Claude等模型在零样本和微调设置下进行测试，结果表明该方法相比基线模型提升了预测性能，微调不仅改善了生成的误解质量，甚至能超越更大的闭源模型；消融研究证实了生成和重排序步骤的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Masked Autoencoders as Universal Speech Enhancer</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Ritwik Giri, Zhiqiang Tang, Kyu Han</div>
<div class="meta-line">First: 2026-02-02T18:13:59+00:00 · Latest: 2026-02-02T18:13:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02413v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02413v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised speech enhancement methods have been very successful. However, in practical scenarios, there is a lack of clean speech, and self-supervised learning-based (SSL) speech enhancement methods that offer comparable enhancement performance and can be applied to other speech-related downstream applications are desired. In this work, we develop a masked autoencoder based universal speech enhancer that is agnostic to the type of distortion affecting speech, can handle multiple distortions simultaneously, and is trained in a self-supervised manner. An augmentation stack adds further distortions to the noisy input data. The masked autoencoder model learns to remove the added distortions along with reconstructing the masked regions of the spectrogram during pre-training. The pre-trained embeddings are then used by fine-tuning models trained on a small amount of paired data for specific downstream tasks. We evaluate the pre-trained features for denoising and dereverberation downstream tasks. We explore different augmentations (like single or multi-speaker) in the pre-training augmentation stack and the effect of different noisy input feature representations (like $log1p$ compression) on pre-trained embeddings and downstream fine-tuning enhancement performance. We show that the proposed method not only outperforms the baseline but also achieves state-of-the-art performance for both in-domain and out-of-domain evaluation datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>掩码自编码器作为通用语音增强器</div>
<div class="mono" style="margin-top:8px">监督式语音增强方法已取得显著成功。然而，在实际场景中，干净语音数据往往匮乏，因此需要基于自监督学习的语音增强方法，既能提供相当的增强性能，又可应用于其他语音相关下游任务。本研究开发了一种基于掩码自编码器的通用语音增强器，该增强器不依赖于语音失真类型，能同时处理多种失真，并以自监督方式进行训练。通过增强堆栈向含噪输入数据添加额外失真，掩码自编码器在预训练中学习消除这些失真并重建频谱图的掩码区域。预训练生成的嵌入特征随后被用于微调模型，这些模型基于少量配对数据针对特定下游任务进行训练。我们评估了预训练特征在去噪和去混响下游任务中的表现，探讨了预训练增强堆栈中不同增强策略（如单说话人与多说话人）以及不同含噪输入特征表示（如$log1p$压缩）对预训练嵌入及下游微调增强性能的影响。实验表明，所提方法不仅优于基线模型，在领域内和跨领域评估数据集上均达到了最先进的性能水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Supervised speech enhancement methods require clean speech data, which is often unavailable in practice, motivating the development of a self-supervised learning approach that can handle various distortions and be applied to downstream tasks. The method employs a masked autoencoder pre-trained on noisy speech augmented with additional distortions, learning to remove distortions and reconstruct masked spectrogram regions; the resulting embeddings are then fine-tuned with limited paired data for specific tasks like denoising and dereverberation. Experimental results demonstrate that this approach surpasses baseline methods and achieves state-of-the-art performance on both in-domain and out-of-domain datasets.</div>
<div class="mono" style="margin-top:8px">监督式语音增强方法需要纯净语音数据，但在实际场景中往往缺乏此类数据，因此需要开发一种自监督学习方法，能够处理多种失真并适用于下游任务。该方法采用掩码自编码器，在添加了额外失真的带噪语音上进行预训练，学习去除失真并重建掩码的频谱区域；随后使用少量配对数据对预训练嵌入进行微调，以完成去噪和去混响等特定任务。实验结果表明，该方法超越了基线方法，并在领域内和领域外评估数据集上均达到了最先进的性能，其性能受到增强策略和输入特征表示选择的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function</div>
<div class="meta-line">Authors: Tung Quoc Le, Anh Tuan Nguyen, Viet Anh Nguyen</div>
<div class="meta-line">First: 2026-02-02T18:04:13+00:00 · Latest: 2026-02-02T18:04:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02406v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02406v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data-driven algorithm design automates hyperparameter tuning, but its statistical foundations remain limited because model performance can depend on hyperparameters in implicit and highly non-smooth ways. Existing guarantees focus on the simple case of a one-dimensional (scalar) hyperparameter. This leaves the practically important, multi-dimensional hyperparameter tuning setting unresolved. We address this open question by establishing the first general framework for establishing generalization guarantees for tuning multi-dimensional hyperparameters in data-driven settings. Our approach strengthens the generalization guarantee framework for semi-algebraic function classes by exploiting tools from real algebraic geometry, yielding sharper, more broadly applicable guarantees. We then extend the analysis to hyperparameter tuning using the validation loss under minimal assumptions, and derive improved bounds when additional structure is available. Finally, we demonstrate the scope of the framework with new learnability results, including data-driven weighted group lasso and weighted fused lasso.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结构化损失函数的数据驱动多超参数调优可证明性研究</div>
<div class="mono" style="margin-top:8px">数据驱动算法设计实现了超参数自动调优，但其统计基础仍存在局限，因为模型性能可能以隐式且高度非光滑的方式依赖于超参数。现有理论保证仅针对一维（标量）超参数的简单情形，尚未解决实践中至关重要的多维超参数调优问题。本研究通过建立首个通用框架，为数据驱动场景下的多维超参数调优提供泛化保证，填补了这一空白。该方法利用实代数几何工具，强化了半代数函数类的泛化保证框架，从而获得更精确、更广泛适用的理论保证。我们进一步将分析拓展至最小假设下基于验证损失的超参数调优，并在获得附加结构信息时推导出更优的界。最后，通过数据驱动的加权群套索和加权融合套索等新可学习性结果，展示了该框架的应用广度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of statistical foundations for multi-dimensional hyperparameter tuning in data-driven algorithm design, where performance can depend on hyperparameters in complex, non-smooth ways, this work establishes a general framework for deriving generalization guarantees. The method strengthens existing analysis for semi-algebraic function classes by employing tools from real algebraic geometry, extends it to validation loss under minimal assumptions, and incorporates additional structural information for improved bounds. Key experimental findings demonstrate the framework&#x27;s applicability through new learnability results for data-driven weighted group lasso and weighted fused lasso.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决数据驱动算法设计中多维超参数调优缺乏统计理论基础的难题，因为模型性能可能以复杂、非光滑的方式依赖于超参数。方法上，通过利用实代数几何工具，强化了半代数函数类的泛化保证框架，将其扩展到最小假设下的验证损失分析，并利用额外结构信息改进边界。主要实验结果展示了该框架的适用性，为数据驱动的加权群套索和加权融合套索等模型提供了新的可学习性结果。</div>
</details>
</div>
<div class="card">
<div class="title">Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning</div>
<div class="meta-line">Authors: Ethan Mendes, Jungsoo Park, Alan Ritter</div>
<div class="meta-line">First: 2026-02-02T18:03:43+00:00 · Latest: 2026-02-02T18:03:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02405v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02405v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improving the reasoning capabilities of large language models (LLMs) typically relies either on the model&#x27;s ability to sample a correct solution to be reinforced or on the existence of a stronger model able to solve the problem. However, many difficult problems remain intractable for even current frontier models, preventing the extraction of valid training signals. A promising alternative is to leverage high-quality expert human solutions, yet naive imitation of this data fails because it is fundamentally out of distribution: expert solutions are typically didactic, containing implicit reasoning gaps intended for human readers rather than computational models. Furthermore, high-quality expert solutions are expensive, necessitating generalizable sample-efficient training methods. We propose Distribution Aligned Imitation Learning (DAIL), a two-step method that bridges the distributional gap by first transforming expert solutions into detailed, in-distribution reasoning traces and then applying a contrastive objective to focus learning on expert insights and methodologies. We find that DAIL can leverage fewer than 1000 high-quality expert solutions to achieve 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improve reasoning efficiency by 2x to 4x, and enable out-of-domain generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从说教到建构：将专家解法转化为可学习的推理过程</div>
<div class="mono" style="margin-top:8px">提升大语言模型（LLMs）的推理能力通常依赖于模型采样正确解以进行强化，或依赖更强模型解决问题的能力。然而，许多难题即使对当前前沿模型仍难以解决，导致无法提取有效的训练信号。一种有前景的替代方案是利用高质量的人类专家解法，但直接模仿这类数据往往失败，因其本质上分布不一致：专家解法通常具有说教性，包含面向人类读者而非计算模型的隐含推理断层。此外，高质量专家解法成本高昂，需要具备泛化能力的高样本效率训练方法。我们提出分布对齐模仿学习（DAIL），该方法通过两步弥合分布差距：先将专家解法转化为详细且分布一致的推理轨迹，再应用对比目标使学习聚焦于专家洞见与方法论。实验表明，DAIL仅需不足1000个高质量专家解法，即可在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k提升，将推理效率提高2至4倍，并具备跨领域泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of improving large language models&#x27; reasoning on difficult problems where neither model self-sampling nor stronger model solutions are available, and where naive imitation of expensive, didactic expert human solutions fails due to a distributional mismatch. The proposed method, Distribution Aligned Imitation Learning (DAIL), first transforms expert solutions into detailed, in-distribution reasoning traces and then applies a contrastive objective to focus learning on expert methodologies. Experimental results show that DAIL, using fewer than 1000 expert solutions, achieves 10-25% pass@k gains on Qwen2.5-Instruct and Qwen3 models, improves reasoning efficiency by 2x to 4x, and enables out-of-domain generalization.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型在复杂推理任务上的训练难题，即前沿模型难以生成正确解以供强化学习，而高质量的人类专家解答成本高昂且具有教学性结构，导致直接模仿时分布外。提出的方法——分布对齐模仿学习（DAIL）——首先将专家解答转化为详细的、分布内的推理轨迹，然后应用对比目标使学习聚焦于专家的见解和方法。实验结果表明，DAIL能够利用少于1000个专家解答，在Qwen2.5-Instruct和Qwen3模型上实现10-25%的pass@k提升，将推理效率提高2至4倍，并实现跨领域泛化。</div>
</details>
</div>
<div class="card">
<div class="title">An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence</div>
<div class="meta-line">Authors: Qizhen Zhang, Ankush Garg, Jakob Foerster, Niladri Chatterji, Kshitiz Malik, Mike Lewis</div>
<div class="meta-line">First: 2026-02-02T17:58:50+00:00 · Latest: 2026-02-02T17:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02400v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02400v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale pretraining datasets drive the success of large language models (LLMs). However, these web-scale corpora inevitably contain large amounts of noisy data due to unregulated web content or randomness inherent in data. Although LLM pretrainers often speculate that such noise contributes to instabilities in large-scale LLM pretraining and, in the worst cases, loss divergence, this phenomenon remains poorly understood.In this work, we present a systematic empirical study of whether noisy data causes LLM pretraining divergences and how it does so. By injecting controlled synthetic uniformly random noise into otherwise clean datasets, we analyze training dynamics across model sizes ranging from 480M to 5.2B parameters. We show that noisy data indeed induces training loss divergence, and that the probability of divergence depends strongly on the noise type, amount of noise, and model scale. We further find that noise-induced divergences exhibit activation patterns distinct from those caused by high learning rates, and we provide diagnostics that differentiate these two failure modes. Together, these results provide a large-scale, controlled characterization of how noisy data affects loss divergence in LLM pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>噪声数据与LLM预训练损失发散实证研究</div>
<div class="mono" style="margin-top:8px">大规模预训练数据集驱动了大语言模型（LLM）的成功。然而，由于网络内容缺乏监管或数据固有的随机性，这些网络规模语料库不可避免地包含大量噪声数据。尽管LLM预训练者常推测此类噪声会导致大规模LLM预训练的不稳定性，最坏情况下引发损失发散，但这一现象仍缺乏深入理解。本研究通过向清洁数据集中注入受控的合成均匀随机噪声，系统实证分析了噪声数据是否及如何导致LLM预训练发散。我们在4.8亿至52亿参数规模的模型上分析训练动态，证明噪声数据确实会诱发训练损失发散，且发散概率高度依赖于噪声类型、噪声量和模型规模。进一步发现噪声引发的发散表现出与高学习率所致发散不同的激活模式，并提供了区分这两种失效模式的诊断方法。这些结果共同构成了关于噪声数据如何影响LLM预训练损失发散的大规模受控特征描述。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the impact of noisy data on large language model (LLM) pretraining, motivated by the prevalence of noise in web-scale corpora and its suspected role in training instability and loss divergence. The method involves injecting controlled synthetic uniformly random noise into clean datasets and analyzing training dynamics across model scales from 480M to 5.2B parameters. Key findings confirm that noisy data induces loss divergence, with divergence probability strongly dependent on noise type, amount, and model scale; the work also identifies distinct activation patterns for noise-induced versus learning-rate-induced failures and provides diagnostic tools to differentiate them.</div>
<div class="mono" style="margin-top:8px">本研究探讨了噪声数据对大型语言模型预训练的影响，其动机在于训练语料库中普遍存在未受监管的网络内容，且推测此类噪声会导致训练不稳定和损失发散。方法包括向干净数据集中注入受控的合成均匀随机噪声，并分析从4.8亿到52亿参数规模模型的训练动态。主要实验结果表明，噪声数据确实会引发训练损失发散，且发散概率强烈依赖于噪声类型、数量及模型规模；研究进一步识别出噪声引起的发散与高学习率导致的发散具有不同的激活模式，并提供了区分这两种失败模式的诊断方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
