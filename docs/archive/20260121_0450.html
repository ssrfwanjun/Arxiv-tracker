<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-21 04:50</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260121_0450</div>
    <div class="row"><div class="card">
<div class="title">ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</div>
<div class="meta-line">Authors: Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu, Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-16T18:51:24+00:00 · Latest: 2026-01-16T18:51:24+00:00</div>
<div class="meta-line">Comments: Project Page: http://facebookresearch.github.io/ShapeR Video: https://www.youtube.com/watch?v=EbY30KAA55I</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11514v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="http://facebookresearch.github.io/ShapeR">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShapeR：基于随意拍摄视频的鲁棒条件式三维形状生成</div>
<div class="mono" style="margin-top:8px">三维形状生成领域近期虽取得显著进展，但现有方法大多依赖干净、无遮挡且分割良好的输入数据，而现实场景很少满足这些条件。本文提出ShapeR——一种从随意拍摄序列生成条件式三维物体形状的新方法。给定图像序列，我们利用现成的视觉-惯性SLAM、三维检测算法和视觉-语言模型，为每个物体提取稀疏SLAM点集、多视角位姿图像及机器生成描述。通过训练整流流变换器有效融合这些模态信息，最终生成高保真度度量三维形状。为应对随意拍摄数据的挑战，我们采用包括动态组合增强、跨物体与场景级数据集的课程训练方案、背景干扰处理策略在内的多项技术。此外，我们构建了包含7个真实场景中178个带几何标注的野外物体的新评估基准。实验表明，ShapeR在此挑战性设定下显著优于现有方法，其倒角距离指标较当前最优技术提升2.7倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for robust 3D shape generation from imperfect, real-world inputs rather than clean, segmented data, this paper introduces ShapeR. The method extracts sparse SLAM points, posed multi-view images, and machine-generated captions from casually captured image sequences using off-the-shelf tools, and then employs a rectified flow transformer conditioned on these modalities to generate metric 3D shapes. Key experimental results, evaluated on a new benchmark of 178 in-the-wild objects, show that ShapeR outperforms existing approaches, achieving a 2.7x improvement in Chamfer distance compared to the state of the art.</div>
<div class="mono" style="margin-top:8px">本研究动机在于现有三维形状生成方法大多依赖干净、无遮挡且分割良好的输入，而这在现实世界的随意拍摄场景中难以满足。所提出的ShapeR方法，首先利用现成的视觉-惯性SLAM、三维检测算法和视觉-语言模型，为每个物体提取稀疏SLAM点、带姿态的多视角图像和机器生成的描述，然后训练一个基于这些模态的整流流变换器来生成高保真度的度量三维形状，并通过组合增强、跨物体与场景数据集的课程训练等策略确保对杂乱数据的鲁棒性。在包含7个真实场景、178个野外物体的新评估基准上的实验表明，ShapeR在此挑战性设定下显著优于现有方法，其倒角距离指标比当前最优方法提升了2.7倍。</div>
</details>
</div>
<div class="card">
<div class="title">MHA2MLA-VLM: Enabling DeepSeek&#x27;s Economical Multi-Head Latent Attention across Vision-Language Models</div>
<div class="meta-line">Authors: Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui</div>
<div class="meta-line">First: 2026-01-16T17:45:34+00:00 · Latest: 2026-01-16T17:45:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11464v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11464v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MHA2MLA-VLM：实现DeepSeek跨视觉语言模型的经济型多头潜在注意力机制</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型处理日益复杂的多模态任务，推理过程中键值缓存的高速增长导致显著的内存与计算瓶颈。虽然多头潜在注意力机制能有效压缩键值缓存并加速推理，但如何在不进行昂贵预训练的情况下将现有视觉语言模型适配至该架构仍属空白。本研究提出MHA2MLA-VLM——一种参数高效且多模态感知的框架，可将现成视觉语言模型转换为多头潜在注意力架构。该框架包含两项核心技术：(1) 模态自适应部分旋转位置编码策略，通过选择性掩蔽非必要维度同时支持传统与多模态场景；(2) 模态解耦的低秩近似方法，独立压缩视觉与文本键值空间。此外，我们引入参数高效微调以最小化适配成本，并证明最小化输出激活误差（而非参数距离）能显著降低性能损失。在三个代表性视觉语言模型上的大量实验表明，MHA2MLA-VLM能以极少量监督数据恢复原始模型性能，显著降低键值缓存占用，并与键值量化技术无缝集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the memory and computational bottlenecks caused by the growing Key-Value (KV) cache in vision-language models (VLMs) during inference. The proposed MHA2MLA-VLM framework efficiently converts standard VLMs to a Multi-Head Latent Attention (MLA) architecture without full pretraining, using a modality-adaptive partial-RoPE strategy and a modality-decoupled low-rank approximation method for compressing visual and textual KV spaces. Experimental results on three VLMs demonstrate that the method restores original performance with minimal supervised data, significantly reduces KV cache size, and works effectively with KV quantization.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型推理过程中，因键值缓存快速增长导致的内存和计算瓶颈问题。为在不进行完整预训练的情况下，将现有视觉语言模型高效适配到多头潜在注意力架构，作者提出了MHA2MLA-VLM这一参数高效框架，其核心包括模态自适应的部分旋转位置编码策略，以及对视觉和文本键值空间进行独立压缩的模态解耦低秩近似方法。在三个代表性视觉语言模型上的实验表明，该方法能以极少的监督数据恢复原始模型性能，显著减少键值缓存占用，并能与键值量化技术无缝集成。</div>
</details>
</div>
<div class="card">
<div class="title">Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation</div>
<div class="meta-line">Authors: Tao Tang, Shijie Xu, Jionglong Su, Zhixiang Lu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-07-04T13:52:16+00:00 · Latest: 2026-01-16T16:16:45+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.03585v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.03585v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The clinical utility of deep learning models for medical image segmentation is severely constrained by their inability to generalize to unseen domains. This failure is often rooted in the models learning spurious correlations between anatomical content and domain-specific imaging styles. To overcome this fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that elevates Large Language Models (LLMs) to the role of causal reasoners. Our framework, built upon a frozen Segment Anything Model (SAM) encoder, incorporates two synergistic innovations. First, Linguistic Adversarial Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual descriptions of confounding image styles. By training the segmentation model&#x27;s features to be contrastively dissimilar to these style descriptions, it learns a representation robustly purged of non-causal information. Second, Test-Time Causal Intervention (TCI) provides an interactive mechanism where an LLM interprets a clinician&#x27;s natural language command to modulate the segmentation decoder&#x27;s features in real-time, enabling targeted error correction. We conduct an extensive empirical evaluation on a composite benchmark from four public datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM establishes a new state of the art in out-of-distribution (OOD) robustness, improving the average Dice score by up to 6.2 points and reducing the Hausdorff Distance by 15.8 mm over the strongest baseline, all while using less than 9% of the full model&#x27;s trainable parameters. Our work charts a new course for building robust, efficient, and interactively controllable medical AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Causal-SAM-LLM：将大语言模型作为因果推理器实现鲁棒医学图像分割</div>
<div class="mono" style="margin-top:8px">深度学习模型在医学图像分割中的临床应用，因其难以泛化至未见域而严重受限。这一缺陷常源于模型习得了解剖内容与域特异性成像风格间的伪相关性。为攻克此根本性挑战，我们提出Causal-SAM-LLM——一个将大语言模型提升为因果推理器的新型框架。该框架基于冻结的Segment Anything Model编码器构建，融合两项协同创新：其一，语言对抗解耦技术通过视觉-语言模型生成丰富的混淆图像风格文本描述，通过训练分割模型特征使其与风格描述形成对比差异，从而学习到彻底剔除非因果信息的鲁棒表征；其二，测试时因果干预提供交互机制，使大语言模型能解析临床医生的自然语言指令，实时调控分割解码器特征，实现针对性误差修正。我们在整合四个公开数据集（BTCV、CHAOS、AMOS、BraTS）的复合基准上开展广泛实证评估，检验跨扫描仪、跨模态及跨解剖结构的泛化能力。Causal-SAM-LLM在分布外鲁棒性上创下新标杆，平均Dice分数最高提升6.2分，Hausdorff距离降低15.8毫米，且仅使用全模型不足9%的可训练参数。本研究为构建鲁棒、高效、可交互控制的医疗AI系统开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The clinical application of deep learning for medical image segmentation is limited by poor generalization to unseen domains, often due to models learning spurious correlations between anatomy and imaging style. To address this, the authors propose Causal-SAM-LLM, a framework that uses Large Language Models (LLMs) as causal reasoners, built upon a frozen Segment Anything Model (SAM) encoder. The method introduces Linguistic Adversarial Disentanglement (LAD) to purge non-causal style information using textual descriptions from a Vision-Language Model, and Test-Time Causal Intervention (TCI), which allows an LLM to interpret clinician commands for real-time feature modulation and error correction. Evaluated on a composite benchmark from four public datasets (BTCV, CHAOS, AMOS, BraTS) under cross-scanner, cross-modality, and cross-anatomy settings, the framework achieves state-of-the-art out-of-distribution robustness, improving the average Dice score by up to 6.2 points and reducing the Hausdorff Distance by 15.8 mm over the strongest baseline while using less than 9% of trainable parameters.</div>
<div class="mono" style="margin-top:8px">深度学习模型在医学图像分割中的临床应用，常因其对未见域泛化能力差而受限，这通常源于模型学习了解剖内容与域特异性成像风格之间的虚假相关性。为解决这一根本挑战，本文提出了Causal-SAM-LLM，该框架将大型语言模型提升为因果推理者，并基于冻结的Segment Anything Model编码器构建。该方法包含两项协同创新：一是语言对抗解缠，它利用视觉语言模型生成丰富的混淆图像风格文本描述，并通过训练分割特征与这些风格描述对比相异，从而学习到稳健去除非因果信息的表示；二是测试时因果干预，它提供了一个交互机制，让大型语言模型解释临床医生的自然语言指令，实时调制分割解码器的特征以实现针对性错误纠正。在由四个公共数据集组成的复合基准上，针对跨扫描仪、跨模态和跨解剖结构的设置进行评估，该框架在分布外鲁棒性上达到了新的最优水平，相比最强基线平均Dice分数提升高达6.2分，Hausdorff距离减少15.8毫米，同时仅使用不到9%的模型可训练参数。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</div>
<div class="meta-line">Authors: Pavana Pradeep, Krishna Kant, Suya Yu</div>
<div class="meta-line">First: 2026-01-16T14:16:38+00:00 · Latest: 2026-01-16T14:16:38+00:00</div>
<div class="meta-line">Comments: Accepted for publication in IEEE Transactions on AI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11322v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11322v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过逻辑推理增强视觉语言模型的情境感知能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）能够从图像和视频中生成复杂活动的高层次、可解释描述，使其在情境感知（SA）应用中具有重要价值。在此类场景中，重点在于以高可靠性和准确性识别罕见但重要的事件，同时提取细粒度细节并评估识别质量。本文提出一种通过显式逻辑推理将VLMs与传统计算机视觉方法相结合的方法，从三个关键方面增强SA：（a）提取细粒度事件细节；（b）采用智能微调（FT）策略，其准确率显著高于无指导选择；（c）在推理过程中为VLM输出生成解释。我们证明，智能FT机制不仅提高了准确性，还在推理过程中提供了确认VLM输出有效性或指出其可能存疑原因的有效手段。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) are promising for situational awareness but require enhanced reliability for detecting infrequent, significant events and extracting fine details. The proposed method integrates VLMs with traditional computer vision via explicit logic reasoning to extract fine-grained event details, employs an intelligent fine-tuning strategy for higher accuracy, and generates justifications for VLM outputs. Experiments show the intelligent fine-tuning mechanism substantially improves accuracy and provides a means to validate or question the VLM&#x27;s inferences during operation.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在情境感知应用中前景广阔，但需提升对偶发重大事件检测的可靠性与细节提取能力。本研究提出一种通过显式逻辑推理将视觉语言模型与传统计算机视觉相结合的方法，旨在增强细节提取、采用智能微调策略以提高准确性，并生成模型输出的合理性解释。实验结果表明，该智能微调机制显著提升了模型精度，并能在推理过程中为验证或质疑模型输出提供依据。</div>
</details>
</div>
<div class="card">
<div class="title">X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning</div>
<div class="meta-line">Authors: Maanping Shao, Feihong Zhang, Gu Zhang, Baiye Cheng, Zhengrong Xue, Huazhe Xu</div>
<div class="meta-line">First: 2026-01-16T13:15:55+00:00 · Latest: 2026-01-16T13:15:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11269v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11269v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>X-Distill：面向视觉运动学习的跨架构视觉蒸馏</div>
<div class="mono" style="margin-top:8px">视觉运动策略常利用预训练的大型视觉变换器（ViTs）以获取强大的泛化能力。然而，在大多数数据稀缺的机器人学习场景中，其庞大的数据需求构成主要挑战，而具有强归纳偏置的紧凑卷积神经网络（CNNs）则更易优化。为平衡这一矛盾，我们提出X-Distill——一种简洁高效的方法，能协同发挥两种架构的优势。该方法通过离线跨架构知识蒸馏，在通用ImageNet数据集上将大型冻结DINOv2教师模型的丰富视觉表征迁移至紧凑的ResNet-18学生模型。经蒸馏的编码器获得强大视觉先验后，与扩散策略头在目标操作任务上联合微调。在34个模拟基准和5项复杂现实任务上的大量实验表明，本方法始终优于采用从头训练ResNet或微调DINOv2编码器的策略。值得注意的是，X-Distill甚至超越了使用特权点云观测或更庞大视觉语言模型的3D编码器。本研究证明，基于严谨设计的简易蒸馏策略可在数据高效的机器人操作中实现最先进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of deploying large Vision Transformers (ViTs) for visuomotor policies in data-scarce robotic settings, where compact CNNs are easier to optimize but lack generalization. The proposed X-Distill method performs offline cross-architecture knowledge distillation, transferring visual representations from a frozen DINOv2 ViT teacher to a compact ResNet-18 student on ImageNet, then fine-tuning the distilled encoder with a diffusion policy head on target tasks. Experiments across 34 simulated and 5 real-world manipulation tasks show that X-Distill consistently outperforms policies using from-scratch ResNet or fine-tuned DINOv2 encoders, and even surpasses 3D encoders with privileged point cloud data or larger vision-language models, demonstrating state-of-the-art data-efficient performance.</div>
<div class="mono" style="margin-top:8px">本研究针对数据稀缺的机器人场景中部署大型视觉Transformer（ViT）进行视动策略学习的挑战，其中紧凑的CNN更易优化但泛化能力不足。提出的X-Distill方法采用离线跨架构知识蒸馏，在ImageNet上将冻结的DINOv2 ViT教师的视觉表征迁移到ResNet-18学生模型，随后在目标任务上将该蒸馏编码器与扩散策略头联合微调。在34个模拟和5个真实世界操作任务上的实验表明，X-Distill consistently outperforms policies using from-scratch ResNets, fine-tuned DINOv2, 3D encoders with point clouds, and larger vision-language models, demonstrating state-of-the-art data-efficient performance。</div>
</details>
</div>
<div class="card">
<div class="title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</div>
<div class="meta-line">Authors: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</div>
<div class="meta-line">First: 2026-01-05T18:56:34+00:00 · Latest: 2026-01-16T13:04:59+00:00</div>
<div class="meta-line">Comments: Project page: https://sotamak1r.github.io/VINO-web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02358v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02358v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sotamak1r.github.io/VINO-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VINO：基于交错式全模态上下文的统一视觉生成器</div>
<div class="mono" style="margin-top:8px">本文提出VINO——一个在单一框架内实现图像与视频生成及编辑的统一视觉生成器。区别于依赖任务专用模型或独立模态模块的传统方案，VINO采用共享的扩散主干网络，可同时接受文本、图像和视频作为条件输入，从而在单一模型中实现广泛的视觉创作与编辑任务。具体而言，VINO将视觉语言模型（VLM）与多模态扩散变换器（MMDiT）耦合，其中多模态输入被编码为交错的条件标记，进而引导扩散过程。该设计支持多参考基准定位、长指令序列跟随以及静态与动态内容间的连贯身份保持，同时避免了模态专用的架构组件。为训练此统一系统，我们提出了多阶段训练流程，逐步将基础视频生成模型扩展为支持图像与视频输入输出的统一多任务生成器。在多样化生成与编辑基准测试中，VINO展现出卓越的视觉质量、精准的指令跟随能力、增强的参考与属性保持特性，以及更可控的多身份编辑效果。本研究为可扩展的统一视觉生成提供了可行路径，并揭示了交错式上下文计算作为通用视觉创作基础技术的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to unify diverse visual generation and editing tasks, which are typically handled by separate, task-specific models, into a single, efficient framework. The proposed method, VINO, integrates a vision-language model with a Multimodal Diffusion Transformer (MMDiT), encoding text, image, and video inputs as interleaved tokens to condition a shared diffusion backbone for generating both images and videos. Experimental results show that VINO achieves strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits across various benchmarks, demonstrating the effectiveness of its unified, interleaved conditioning approach.</div>
<div class="mono" style="margin-top:8px">本研究旨在将通常由独立任务特定模型处理的多样化视觉生成与编辑任务，统一到一个单一高效的框架中。所提出的VINO方法将视觉语言模型与多模态扩散变换器（MMDiT）相结合，将文本、图像和视频输入编码为交错的条件令牌，用以引导一个共享的扩散主干网络，实现图像和视频的生成与编辑。实验结果表明，VINO在各种基准测试中展现出强大的视觉质量、对指令的忠实遵循、改进的参考属性与身份保持能力，以及更可控的多身份编辑效果。</div>
</details>
</div>
<div class="card">
<div class="title">Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval</div>
<div class="meta-line">Authors: Fangke Chen, Tianhao Dong, Sirry Chen, Guobin Zhang, Yishu Zhang, Yining Chen</div>
<div class="meta-line">First: 2026-01-16T12:55:41+00:00 · Latest: 2026-01-16T12:55:41+00:00</div>
<div class="meta-line">Comments: 9 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11248v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11248v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言无关视觉嵌入在跨文字手写检索中的应用</div>
<div class="mono" style="margin-top:8px">手写词检索对数字档案至关重要，但由于手写体差异大和跨语言语义鸿沟，仍面临挑战。尽管大型视觉语言模型提供潜在解决方案，但其高昂计算成本阻碍实际边缘部署。为此，我们提出一种轻量级非对称双编码器框架，学习统一、风格不变的视觉嵌入。通过联合优化实例级对齐和类级语义一致性，该方法将视觉嵌入锚定至语言无关的语义原型，强制实现跨文字和书写风格的不变性。实验表明，本方法在28个基线模型中表现最优，并在同语言检索基准上达到最先进准确率。我们进一步开展显式跨语言检索（查询语言与目标语言不同），验证所学跨语言表征的有效性。仅需现有模型参数量的极小部分即可实现强劲性能，本框架为跨文字手写检索提供了精准且资源高效的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Handwritten word retrieval faces challenges due to handwriting variability and cross-lingual semantic gaps, with existing vision-language models being computationally expensive for edge deployment. To address this, the authors propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings by jointly optimizing instance-level alignment and class-level semantic consistency, anchoring embeddings to language-agnostic semantic prototypes. Experimental results demonstrate that the method outperforms 28 baselines, achieves state-of-the-art accuracy on within-language retrieval benchmarks, and shows strong performance in explicit cross-lingual retrieval scenarios while using only a fraction of the parameters of existing models.</div>
<div class="mono" style="margin-top:8px">本研究针对数字档案中手写词检索的挑战，该挑战因笔迹多变性和跨语言语义鸿沟而复杂化，同时旨在克服大型视觉语言模型在边缘部署中计算成本过高的问题。方法上，提出了一种轻量级非对称双编码器框架，通过联合优化实例级对齐和类级语义一致性来学习统一的、风格不变的视觉嵌入，从而将嵌入锚定到语言无关的语义原型。实验结果表明，该方法在28个基线模型上表现优异，在语言内检索基准上达到了最先进的准确率，并以远少于现有模型的参数量验证了有效的跨语言检索能力。</div>
</details>
</div>
<div class="card">
<div class="title">Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification</div>
<div class="meta-line">Authors: Zhiqi Pang, Lingling Zhao, Yang Liu, Chunyu Wang, Gaurav Sharma</div>
<div class="meta-line">First: 2026-01-16T12:45:01+00:00 · Latest: 2026-01-16T12:45:01+00:00</div>
<div class="meta-line">Comments: 12 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向无监督多场景行人重识别的图文知识建模</div>
<div class="mono" style="margin-top:8px">本文提出无监督多场景行人重识别这一新任务，旨在单一框架内扩展跨分辨率、换装等多场景的ReID能力。为此，我们提出图文知识建模——一种三阶段框架，有效利用视觉语言模型的表征能力。我们以预训练的CLIP模型为基础，包含图像编码器和文本编码器。第一阶段：在图像编码器中引入场景嵌入，通过微调使编码器自适应融合多场景知识。第二阶段：优化一组可学习文本嵌入，使其与第一阶段生成的伪标签关联，并引入多场景分离损失以增强场景间文本表征的差异性。第三阶段：首先通过簇级和实例级异质匹配模块，在各场景内获取可靠的异质正样本对（如同行人的可见光与红外图像）；随后提出动态文本表征更新策略，保持文本与图像监督信号的一致性。多场景实验结果表明，ITKM不仅优于现有场景专用方法，还能通过整合多场景知识提升整体性能，具有优越的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of unsupervised multi-scenario person re-identification (UMS-ReID), which aims to recognize individuals across diverse scenarios like cross-resolution and clothing changes without labeled data. The proposed method, Image-Text Knowledge Modeling (ITKM), employs a three-stage framework built upon a pre-trained CLIP model: first fine-tuning the image encoder with scenario embeddings, then optimizing text embeddings with a multi-scenario separation loss, and finally establishing reliable heterogeneous matches within scenarios while dynamically updating text representations. Experiments show ITKM outperforms existing scenario-specific methods and demonstrates superior generalizability by effectively integrating knowledge from multiple scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对无监督多场景行人重识别（UMS-ReID）的挑战，旨在无需标注数据的情况下，跨分辨率变化、衣物更换等多种条件识别同一行人。所提出的方法——图像-文本知识建模（ITKM）——是一个基于预训练CLIP模型的三阶段框架：首先通过场景嵌入微调图像编码器，然后利用分离损失优化文本嵌入以区分不同场景，最后采用异构匹配和动态文本更新策略来对齐图像与文本表示。实验结果表明，ITKM不仅优于现有的特定场景方法，还能通过有效整合多场景知识提升整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">SceneFoundry: Generating Interactive Infinite 3D Worlds</div>
<div class="meta-line">Authors: ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang</div>
<div class="meta-line">First: 2026-01-09T14:33:10+00:00 · Latest: 2026-01-16T11:20:40+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05810v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05810v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://anc891203.github.io/SceneFoundry-Demo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research. project page: https://anc891203.github.io/SceneFoundry-Demo/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SceneFoundry：生成交互式无限三维世界</div>
<div class="mono" style="margin-top:8px">自动生成大规模、交互式且物理真实的三维环境能力对推进机器人学习与具身智能至关重要。然而，现有生成方法常难以捕捉真实室内环境的功能复杂性，尤其缺乏对操控与导航至关重要的含可动部件的关节式物体的建模。本文提出SceneFoundry——一种语言引导的扩散框架，可为机器人训练生成公寓级三维世界，其中包含功能可动的家具与语义多样的布局。通过自然语言指令，大语言模型模块控制平面布局生成，而基于扩散的后验采样则高效地从大规模三维资源库中选取关节化资产填充场景。为确保物理可用性，SceneFoundry采用可微分引导函数来调控物体数量、防止关节碰撞，并为机器人导航维持充足的可通行空间。大量实验表明，该框架能在多样场景类型与条件下生成结构有效、语义连贯且功能可交互的环境，为可扩展的具身人工智能研究提供支持。项目页面：https://anc891203.github.io/SceneFoundry-Demo/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for automatically generating large-scale, interactive 3D environments that capture functional complexity, such as articulated furniture, for advancing robotic learning and embodied AI. The method, SceneFoundry, is a language-guided diffusion framework that uses an LLM to control floor layout generation from natural language prompts and employs diffusion-based posterior sampling to populate scenes with articulated assets from 3D repositories, incorporating differentiable guidance functions to ensure physical usability by regulating object quantity, preventing collisions, and maintaining walkable space. Experimental results show the framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types, facilitating scalable embodied AI research.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决为机器人学习自动生成大规模交互式3D环境的需求，因为现有方法通常无法捕捉具有可动部件的真实室内环境的功能复杂性。提出的SceneFoundry框架采用语言引导的扩散方法：大型语言模型根据自然语言提示控制平面布局生成，而基于扩散的后验采样从大规模3D资源库中填充场景的可动资产，并通过可微指导函数确保物理可用性，以调节物体数量、防止碰撞并保持机器人导航所需的可行走空间。实验结果表明，该框架能够生成结构有效、语义连贯且功能交互的环境，适用于多种场景类型，支持可扩展的具身人工智能研究。</div>
</details>
</div>
<div class="card">
<div class="title">Attention Debiasing for Token Pruning in Vision Language Models</div>
<div class="meta-line">Authors: Kai Zhao, Wubang Yuan, Yuchen Lin, Liting Ruan, Xiaofeng Lu, Deng-Ping Fan, Ming-Ming Cheng, Dan Zeng</div>
<div class="meta-line">First: 2025-08-25T08:56:32+00:00 · Latest: 2026-01-16T09:19:57+00:00</div>
<div class="meta-line">Comments: https://github.com/intcomp/attention-bias</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.17807v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.17807v2">PDF</a> · <a href="https://github.com/intcomp/attention-bias">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) typically encode substantially more visual tokens than text tokens, resulting in significant token redundancy. Pruning uninformative visual tokens is therefore crucial for improving computational efficiency, and language-to-vision attention has become a widely used importance criterion for this purpose. However, we find that attention in VLMs is systematically biased. It disproportionately favors tokens appearing later in the sequence, manifesting as over-attention to lower image regions, and assigns inflated scores to semantically empty padding tokens. These behaviors stem from intrinsic recency bias and attention sink effects inherited from large language models (LLMs), and they distort attention-based pruning by preserving irrelevant visual content. To derive a pruning criterion better aligned with semantic relevance, we introduce two lightweight yet effective debiasing techniques that restore the reliability of attention. The first compensates for positional distortions by removing recency-induced attention trends, producing a content-aware and position-agnostic importance measure. The second suppresses attention sink effects by eliminating spurious attention on padding tokens. Our method is model-agnostic, pruning-method-agnostic, and task-agnostic, enabling plug-and-play integration with existing VLM pruning models. Despite its simplicity, our approach consistently delivers strong performance gains. We evaluate our method on ten vision-language benchmarks spanning both image-based and video-based tasks, in comparison with seven state-of-the-art visual token pruning methods and across two representative VLM architectures. Our method achieves substantial performance gains, demonstrating strong effectiveness and generalizability. Our code is available at https://github.com/intcomp/attention-bias.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中注意力去偏的令牌剪枝方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型通常编码的视觉令牌远多于文本令牌，导致显著的令牌冗余。因此，剪枝无信息的视觉令牌对提升计算效率至关重要，而语言到视觉的注意力机制已成为广泛采用的重要性评估标准。然而，我们发现视觉语言模型中的注意力存在系统性偏差：它不成比例地偏向序列中靠后的令牌，表现为对图像下部区域的过度关注，并为语义空白的填充令牌分配虚高分数。这些现象源于从大语言模型继承的内在近因偏差和注意力汇聚效应，会扭曲基于注意力的剪枝过程，保留无关视觉内容。为建立更符合语义相关性的剪枝标准，我们提出两种轻量高效的去偏技术以恢复注意力可靠性：第一项技术通过消除近因诱导的注意力趋势来补偿位置失真，生成内容感知且位置无关的重要性度量；第二项技术通过抑制填充令牌的虚假注意力来削弱注意力汇聚效应。本方法具有模型无关性、剪枝方法无关性和任务无关性，可与现有视觉语言模型剪枝方案即插即用。尽管设计简洁，该方法持续带来显著性能提升。我们在涵盖图像与视频任务的十项视觉语言基准测试中，对比七种前沿视觉令牌剪枝方法，并在两种代表性视觉语言模型架构上进行评估。实验表明本方法取得实质性性能增益，展现出强有效性和泛化能力。代码已开源：https://github.com/intcomp/attention-bias。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models encode many redundant visual tokens, and pruning them using language-to-vision attention is common but problematic because attention is systematically biased, favoring later tokens and padding due to recency bias and attention sink effects from large language models. To correct this, the authors introduce two lightweight debiasing techniques: one removes recency-induced positional trends to create a content-aware importance measure, and another suppresses attention on padding tokens, yielding a model-agnostic, plug-and-play method. Evaluated on ten vision-language benchmarks against seven pruning methods and across two VLM architectures, the approach consistently achieves substantial performance gains, demonstrating strong effectiveness and generalizability.</div>
<div class="mono" style="margin-top:8px">视觉语言模型编码了大量冗余的视觉标记，基于注意力的剪枝常用于提升效率，但注意力分数存在系统性偏差，如近因效应和注意力汇现象，导致保留无关标记。为此，研究者提出两种轻量级去偏技术：一是消除位置近因趋势以生成内容感知的重要性度量，二是抑制对填充标记的注意力。该方法在十个基准测试中与七种剪枝方法及两种VLM架构对比，均取得显著性能提升，展现了强大的有效性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification</div>
<div class="meta-line">Authors: Tayyab Rehman, Giovanni De Gasperis, Aly Shmahell</div>
<div class="meta-line">First: 2026-01-08T11:31:47+00:00 · Latest: 2026-01-16T08:21:29+00:00</div>
<div class="meta-line">Comments: Author email changed, Acknowlegement changes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06204v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.06204v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型与嵌入分类的级联多智能体监控系统异常检测</div>
<div class="mono" style="margin-top:8px">动态视觉环境中的智能异常检测需兼顾实时性能与语义可解释性。传统方法仅能应对该挑战的局部：基于重建的模型捕获低级偏差但缺乏上下文推理，目标检测器速度快但语义有限，大型视觉语言系统虽具可解释性但计算成本过高。本研究提出一种级联多智能体框架，将上述互补范式统一为连贯且可解释的架构。早期模块执行重建门控过滤与目标级评估，高层推理智能体则被选择性调用以解释语义模糊事件。系统采用自适应升级阈值与发布-订阅通信主干，支持异构硬件间的异步协调与可扩展部署。基于大规模监控数据的广泛评估表明，相比直接视觉语言推理，所提级联方法将延迟降低三倍，同时保持高感知保真度（PSNR = 38.3 dB, SSIM = 0.965）与一致的语义标注。该框架通过融合早期退出效率、自适应多智能体推理与可解释异常归因，超越了传统检测流程，为可扩展智能视觉监控建立了可复现且高能效的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to address the challenge of achieving both real-time performance and semantic interpretability in intelligent anomaly detection for dynamic visual environments, where conventional methods offer only partial solutions. The proposed method is a cascading multi-agent framework that integrates reconstruction-based filtering, object-level assessment, and selectively invoked high-level reasoning agents, coordinated via adaptive escalation thresholds and a publish-subscribe communication backbone. Experimental results on large-scale monitoring data show the framework reduces inference latency threefold compared to direct vision-language models while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决监控系统中智能异常检测同时实现实时性能与语义可解释性的挑战。所提出的方法是一个级联多智能体框架，它集成了基于重建的过滤、对象级评估以及选择性调用的基于视觉-语言模型的高级推理智能体，通过自适应阈值和发布-订阅通信主干进行协调。在大规模监控数据上的实验结果表明，该框架与直接使用视觉-语言模型相比，将推理延迟降低了三倍，同时保持了高感知保真度（PSNR = 38.3 dB, SSIM = 0.965）和一致的语义标注。</div>
</details>
</div>
<div class="card">
<div class="title">MERGETUNE: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-16T04:31:59+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v2">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, continued fine-tuning (CFT), which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6% on base-novel generalisation without adding parameters. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at https://github.com/Surrey-UP-Lab/MERGETUNE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MERGETUNE：视觉语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">对CLIP等视觉语言模型进行微调常导致预训练知识的灾难性遗忘。先前研究主要致力于缓解适应过程中的遗忘，但该过程中遗忘往往难以避免。本文提出一种新范式——持续微调，旨在零样本模型完成适应后恢复预训练知识。我们提出一种基于线性模式连通性指导的简易模型无关持续微调策略（命名为MERGETUNE），该策略可事后应用于现有微调模型而无需改变架构。给定微调模型，我们持续优化其可训练参数以寻找具有两条低损失路径的持续模型：分别通向零样本解决方案和微调解决方案。通过利用损失景观的几何特性，持续模型隐式融合两种解决方案，恢复微调模型中丢失的预训练知识。传统LMC约束需要预训练任务数据回放，我们通过二阶代理近似零样本模型的约束，无需大规模数据回放。实验表明：MERGETUNE在基础-新类别泛化任务中将CoOp的调和平均值提升5.6%且未增加参数；在鲁棒微调评估中，MERGETUNE的LMC融合模型以更低推理成本超越集成基线，与零样本模型集成时可实现进一步增益并达到最优结果。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the persistent issue of catastrophic forgetting in fine-tuned vision-language models (VLMs), this research introduces a novel continued fine-tuning (CFT) paradigm aimed at recovering lost pretrained knowledge post-adaptation. The proposed method, MERGETUNE, is a model-agnostic strategy that leverages linear mode connectivity (LMC) to guide the continued fine-tuning of a model&#x27;s trainable parameters, seeking a solution with low-loss paths to both the original zero-shot model and the fine-tuned model, thereby implicitly merging their knowledge; a key innovation is approximating the LMC constraint with a second-order surrogate to avoid large-scale data replay. Experimental results demonstrate that MERGETUNE improves the harmonic mean of CoOp by +5.6% on base-novel generalization without adding parameters and, in robust fine-tuning evaluations, the LMC-merged model surpasses ensemble baselines with lower inference cost, achieving state-of-the-art results when further ensembled.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型（VLM）微调中持续存在的灾难性遗忘问题，本研究提出了一种新颖的持续微调（CFT）范式，旨在模型适应后恢复丢失的预训练知识。所提出的方法MERGETUNE是一种模型无关的策略，它利用线性模式连通性（LMC）来指导模型可训练参数的持续微调，寻找一个对原始零样本模型和微调模型都具有低损失路径的解，从而在不改变架构的情况下合并它们的能力；该方法通过二阶近似来模拟零样本模型的LMC约束，避免了大规模数据回放。实验结果表明，MERGETUNE在不增加参数的情况下，将CoOp在基础-新类别泛化上的调和平均提高了5.6%，并且在鲁棒微调评估中，LMC合并模型以更低的推理成本超越了集成基线，当与零样本模型进一步集成时取得了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement</div>
<div class="meta-line">Authors: Meidan Ding, Jipeng Zhang, Wenxuan Wang, Haiqin Zhong, Xiaoling Luo, Wenting Chen, Linlin Shen</div>
<div class="meta-line">First: 2026-01-16T02:32:07+00:00 · Latest: 2026-01-16T02:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10949v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10949v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Vision-Language Models (MedVLMs) excel at perception tasks but struggle with complex clinical reasoning required in real-world scenarios. While reinforcement learning (RL) has been explored to enhance reasoning capabilities, existing approaches face critical mismatches: the scarcity of deep reasoning data, cold-start limits multi-specialty alignment, and standard RL algorithms fail to model clinical reasoning diversity. We propose MMedExpert-R1, a novel reasoning MedVLM that addresses these challenges through domain-specific adaptation and clinical guideline reinforcement. We construct MMedExpert, a high-quality dataset of 10K samples across four specialties with step-by-step reasoning traces. Our Domain-Specific Adaptation (DSA) creates specialty-specific LoRA modules to provide diverse initialization, while Guideline-Based Advantages (GBA) explicitly models different clinical reasoning perspectives to align with real-world diagnostic strategies. Conflict-Aware Capability Integration then merges these specialized experts into a unified agent, ensuring robust multi-specialty alignment. Comprehensive experiments demonstrate state-of-the-art performance, with our 7B model achieving 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, establishing a robust foundation for reliable multimodal medical reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMedExpert-R1：通过领域特定适应与临床指南强化增强多模态医学推理</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型在感知任务上表现优异，但在真实场景所需的复杂临床推理方面存在不足。尽管已有研究探索利用强化学习提升推理能力，现有方法仍面临关键不匹配问题：深度推理数据稀缺、冷启动限制多专科对齐，以及标准强化学习算法难以建模临床推理的多样性。我们提出MMedExpert-R1，一种新型推理医学视觉语言模型，通过领域特定适应和临床指南强化应对这些挑战。我们构建了包含四个专科共1万条样本的高质量数据集MMedExpert，附带逐步推理轨迹。领域特定适应方法创建专科特定的LoRA模块以提供多样化初始化，而基于指南的优势函数显式建模不同临床推理视角，以符合真实诊断策略。冲突感知能力集成随后将这些专科专家模型融合为统一智能体，确保稳健的多专科对齐。综合实验表明其达到最先进性能，我们的70亿参数模型在MedXpert-MM上取得27.50分，在OmniMedVQA上获得83.03分，为可靠的多模态医学推理系统奠定了坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of Medical Vision-Language Models (MedVLMs) in complex clinical reasoning by tackling data scarcity, multi-specialty alignment, and the failure of standard RL to model diverse clinical logic. The proposed MMedExpert-R1 model introduces a Domain-Specific Adaptation (DSA) method using specialty-specific LoRA modules for initialization and a Guideline-Based Advantages (GBA) mechanism to align with real-world diagnostic strategies, followed by Conflict-Aware Capability Integration to unify the experts. Experiments show the 7B model achieves state-of-the-art performance, scoring 27.50 on MedXpert-MM and 83.03 on OmniMedVQA.</div>
<div class="mono" style="margin-top:8px">针对医学视觉语言模型在复杂临床推理中存在的不足，如深度推理数据稀缺、多专科对齐的冷启动问题以及标准强化学习算法无法建模临床推理多样性，本研究提出了MMedExpert-R1。该方法构建了一个包含逐步推理轨迹的高质量数据集（MMedExpert），并采用领域特定适应（DSA）创建专科特定的LoRA模块以实现多样化初始化，同时利用基于指南的优势（GBA）显式建模不同的临床推理视角，最后通过冲突感知能力集成将这些专科专家合并为统一智能体。实验结果表明该模型取得了最先进的性能，其7B版本在MedXpert-MM和OmniMedVQA上分别获得了27.50和83.03的分数。</div>
</details>
</div>
<div class="card">
<div class="title">PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis</div>
<div class="meta-line">Authors: K Lokesh, Abhirama Subramanyam Penamakuri, Uday Agarwal, Apoorva Challa, Shreya K Gowda, Somesh Gupta, Anand Mishra</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-16T02:18:29+00:00 · Latest: 2026-01-16T02:18:29+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2026 Main Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10945v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10945v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PatientVLM与DocVLM的相遇：基于视觉语言模型的预咨询对话实现高效诊断</div>
<div class="mono" style="margin-top:8px">传统医学诊断AI研究主要集中于图像分析，虽取得显著进展，但缺乏患者主诉症状仍制约诊断准确性。为此，我们提出预咨询对话框架，模拟医生在得出结论前对患者进行迭代询问的真实诊断流程。具体而言，我们构建两个视觉语言模型间的诊断对话：DocVLM基于图像和对话历史生成追问，PatientVLM则根据真实诊断推导的症状特征进行应答。通过对框架生成的合成症状开展小规模临床验证，持证临床医生确认了其临床相关性、症状覆盖度和整体真实性。研究表明，DocVLM与PatientVLM的交互可形成连贯的多轮次咨询，并关联图像与诊断结果。我们利用该对话数据对DocVLM进行微调，这种基于对话的监督机制相比纯图像训练取得显著提升，凸显了真实症状采集对诊断的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of image-only AI medical diagnosis, which lacks patient-reported symptoms, this work introduces a Pre-Consultation Dialogue Framework (PCDF) to simulate real-world diagnostic interactions. The method employs two vision-language models: a DocVLM that asks follow-up questions based on medical images and dialogue history, and a PatientVLM that responds using symptom profiles from ground-truth diagnoses. Experimental results include a small-scale clinical validation where clinicians confirmed the clinical relevance and realism of the synthetic symptoms, and fine-tuning the DocVLM with these generated dialogues yielded substantial performance improvements over image-only training.</div>
<div class="mono" style="margin-top:8px">为超越仅依赖图像的AI模型并提升诊断准确性，本研究引入了模拟真实诊断交互的预咨询对话框架（PCDF）。该方法采用两个视觉语言模型：基于医学图像和对话历史生成后续问题的DocVLM，以及利用真实诊断得出的症状档案进行回应的PatientVLM。实验结果表明，该对话生成的合成症状经临床验证具有相关性和真实性，使用这些对话对DocVLM进行微调，相比仅使用图像训练取得了显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Image2Garment: Simulation-ready Garment Generation from a Single Image</div>
<div class="meta-line">Authors: Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu, Yang Zheng, Hugo Bertiche, Menglei Chai, Thabo Beeler, Gordon Wetzstein</div>
<div class="meta-line">First: 2026-01-14T17:47:33+00:00 · Latest: 2026-01-15T21:21:50+00:00</div>
<div class="meta-line">Comments: Project Page: https://image2garment.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09658v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09658v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://image2garment.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Image2Garment：从单张图像生成仿真就绪的服装</div>
<div class="mono" style="margin-top:8px">从单张图像估计物理精确、仿真就绪的服装具有挑战性，原因在于缺乏图像到物理的数据集以及该问题本身的不适定性。现有方法要么需要多视角捕捉和昂贵的可微分仿真，要么仅预测服装几何而缺少真实仿真所需的材料属性。我们提出一种前馈框架，通过先微调视觉语言模型从真实图像推断材料成分与织物属性，再利用小型材料物理测量数据集训练轻量级预测器，将这些属性映射至对应的物理织物参数，从而规避了上述限制。该方法引入了两个新数据集（FTAG与T2P），无需迭代优化即可从单张图像生成仿真就绪的服装。实验表明，我们的估计器在材料成分估计和织物属性预测上均达到更高精度，且通过物理参数估计器处理后，相比现有图像到服装方法能实现更高保真度的仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating physically accurate, simulation-ready 3D garments from a single image, a problem made difficult by the lack of image-to-physics datasets and its ill-posed nature. The proposed method introduces a feed-forward framework that first fine-tunes a vision-language model to infer material composition and fabric attributes from an image, and then uses a lightweight predictor to map these attributes to physical simulation parameters, trained on a new small dataset of material-physics measurements. Experimental results demonstrate that the approach achieves superior accuracy in material composition estimation and fabric attribute prediction, and the resulting physical parameters enable higher-fidelity garment simulations compared to prior state-of-the-art methods, all without requiring iterative optimization or multi-view capture.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从单张图像生成物理精确、可直接仿真的服装这一挑战，该问题因缺乏图像到物理的数据集及其不适定性而变得困难。所提出的方法采用前馈框架，首先微调一个视觉语言模型从图像中推断材料成分和织物属性，然后使用一个在小型材料物理数据集上训练的轻量级预测器，将这些属性映射到物理仿真参数。实验结果表明，该方法在材料和属性估计上实现了更优的准确性，并因此能够生成比现有最先进的图像到服装方法更高保真度的仿真效果。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
