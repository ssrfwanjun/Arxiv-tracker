<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-07 05:34</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260207_0534</div>
    <div class="row"><div class="card">
<div class="title">EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Alan Yuille</div>
<div class="meta-line">Venue: Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pages 649-659</div>
<div class="meta-line">First: 2025-02-07T07:07:04+00:00 · Latest: 2026-02-05T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04700v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.04700v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenLoRAx：通过回收适配器寻找主成分子空间以实现资源高效适应与推理</div>
<div class="mono" style="margin-top:8px">大模型的快速发展因其高昂计算成本引发了环境影响与可访问性公平的担忧。低秩适配器（LoRA）为大模型微调提供了轻量级解决方案，催生了大量面向不同领域的公开适配器。我们提出：能否利用这些预训练适配器进一步简化新任务适应过程并应对相关挑战？本文介绍EigenLoRAx——一种参数高效的微调方法，通过回收现有适配器构建与其共享领域知识对齐的主成分子空间，并可在低资源场景中通过正交基向量进行扩展。该方法仅需学习子空间主成分上的轻量级系数即可快速适应新任务，无需微调完整适配器。EigenLoRAx显著减少了参数量和内存需求，提升了训练与推理效率。实验表明，该方法在多样化领域和任务中均表现优异，为边缘计算应用、个性化部署及资源受限环境下大模型的公平部署提供了可扩展方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the computational and environmental costs of large models, this paper introduces EigenLoRAx, a parameter-efficient finetuning method that recycles existing Low-Rank Adapters (LoRA) to construct a principal subspace capturing shared domain knowledge, which can be expanded with orthogonal basis vectors when data is scarce. The method enables rapid adaptation to new tasks by learning only lightweight coefficients on this subspace&#x27;s principal components, avoiding the need to finetune entire adapters. Experiments show that EigenLoRAx achieves strong performance across diverse domains and tasks while requiring significantly fewer parameters and less memory, improving efficiency for both training and inference in resource-constrained settings.</div>
<div class="mono" style="margin-top:8px">本研究针对大模型的计算成本、环境影响以及大量公开可用的领域特定LoRA适配器，提出了EigenLoRAx方法，这是一种参数高效的微调技术，通过回收利用现有适配器构建一个捕捉共享领域知识的主子空间，并在低资源场景下用正交基向量进行扩展。该方法仅需在该主子空间的主成分上学习轻量级系数即可适应新任务，无需微调整个适配器。实验结果表明，EigenLoRAx在多种领域和任务上均表现出色，同时显著减少了参数数量和内存占用，提升了资源受限环境中训练和推理的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Shared LoRA Subspaces for almost Strict Continual Learning</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>共享LoRA子空间实现近乎严格的持续学习</div>
<div class="mono" style="margin-top:8px">将大型预训练模型高效持续地适配新任务对实际部署至关重要，但因灾难性遗忘和重训练成本高昂而面临挑战。虽然低秩适配（LoRA）等参数高效调优方法降低了计算需求，但缺乏不依赖数据回放或多适配器的严格持续学习与知识整合机制。我们提出Share方法——一种参数高效的持续微调新方案，通过学习和动态更新单一共享低秩子空间，实现跨多任务与多模态的无缝适配。Share构建基础子空间以提取历史任务核心知识，并通过识别关键子空间方向逐步整合新信息。每个新任务的知识均融入这一动态演进的子空间，在促进前向知识迁移的同时最小化灾难性干扰。该方法相比传统LoRA实现高达100倍的参数压缩与281倍的内存节省，性能媲美联合训练模型。单个Share模型可替代数百个任务专用LoRA适配器，支持可扩展的异步持续学习。在图像分类、自然语言理解、3D姿态估计和文生图生成等任务的实验验证了其有效性，使Share成为大规模AI系统中实用且可扩展的终身学习解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting large pretrained models to new tasks continually and efficiently, which is hindered by catastrophic forgetting and high computational costs. The proposed method, Share, introduces a parameter-efficient continual finetuning approach that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across tasks and modalities by extracting core knowledge from past tasks and incrementally integrating new information. Experimental results demonstrate that Share achieves up to 100x parameter reduction and 281x memory savings compared to traditional LoRA methods while maintaining performance comparable to jointly trained models, as validated across image classification, natural language understanding, 3D pose estimation, and text-to-image generation tasks.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决大型预训练模型在持续适应新任务时面临的灾难性遗忘和高计算成本挑战。提出的方法Share采用参数高效的持续微调策略，通过学习和动态更新一个共享的低秩子空间，从过往任务中提取核心知识，并通过识别关键子空间方向逐步整合新信息。实验结果表明，Share相比传统LoRA方法实现了高达100倍的参数减少和281倍的内存节省，同时在图像分类、自然语言理解、3D姿态估计和文本到图像生成等任务上保持了与联合训练模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Pseudo-Invertible Neural Networks</div>
<div class="meta-line">Authors: Yamit Ehrlich, Nimrod Berman, Assaf Shocher</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06042v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06042v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or &quot;Back-Projection&quot;, $x&#x27; = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x&#x27;$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, &quot;degradation&quot; is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伪可逆神经网络</div>
<div class="mono" style="margin-top:8px">Moore-Penrose伪逆（PInv）是线性系统的基本解法。本文提出将PInv自然推广至非线性领域，特别是神经网络。我们引入满射伪可逆神经网络（SPNN），这类架构经显式设计可实现可处理的非线性PInv。所提出的非线性PInv及其在SPNN中的实现满足基本几何特性，例如零空间投影或“反向投影”$x&#x27; = x + A^\dagger(y-Ax)$，该操作将样本$x$移动至满足$Ax=y$的最近一致状态$x&#x27;$。我们形式化非线性反向投影（NLBP），该方法通过定义的PInv保证非线性映射$f(x)=y$具有相同的一致性约束。利用SPNN拓展零样本逆问题的适用范围：基于扩散的零空间投影通过闭式反向投影革新了线性逆问题的零样本求解，我们将此方法延伸至非线性退化问题。此处“退化”广义涵盖任何非线性信息损失，包括从光学畸变到分类等语义抽象。该方法支持复杂退化的零样本逆变换，并能在不重训练扩散先验的情况下实现对生成输出的精确语义控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to generalize the Moore-Penrose pseudo-inverse from linear to nonlinear systems, specifically for neural networks, to solve inverse problems. The method introduces Surjective Pseudo-invertible Neural Networks (SPNN), which are designed to have a tractable nonlinear pseudo-inverse that satisfies geometric properties like non-linear back-projection, ensuring a sample meets a consistency constraint. Key experimental findings show that SPNNs enable zero-shot inversion for complex nonlinear degradations, such as optical distortions and semantic abstractions, allowing precise semantic control over generative outputs without retraining the diffusion prior.</div>
<div class="mono" style="margin-top:8px">本研究旨在将求解线性系统的基本工具——摩尔-彭罗斯伪逆——推广到非线性领域，特别是神经网络中，以解决具有复杂退化的零样本逆问题。方法引入了满射伪可逆神经网络（SPNN），这类架构被设计为允许一个易处理的非线性伪逆，并满足关键的几何性质，如实现非线性反向投影（NLBP）以强制执行一致性约束。实验结果表明，SPNN将基于扩散的零空间投影技术扩展到非线性退化——包括光学畸变和语义抽象（如分类）——从而实现了零样本逆问题求解，并能在不重新训练扩散先验的情况下对生成输出进行精确的语义控制。</div>
</details>
</div>
<div class="card">
<div class="title">CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</div>
<div class="meta-line">Authors: Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:45+00:00 · Latest: 2026-02-05T18:59:45+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://comm-cp.github.io/">Project1</a> · <a href="https://comm-cp.github.io">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CommCP：基于大语言模型与保形预测的高效多智能体协同通信框架</div>
<div class="mono" style="margin-top:8px">为完成人类以自然语言下达的任务，机器人需解析指令、生成并回答场景理解相关问题，并操控目标物体。实际部署常需多个具备不同操控能力的异构机器人协同处理不同任务。除专业操控技能外，有效信息收集对任务完成至关重要。为此，我们将完全协作环境中的信息收集过程形式化为一个尚未充分探索的多智能体多任务具身问答问题，这是经典具身问答任务的新扩展，其中高效通信对避免冗余协作至关重要。针对该问题，我们提出CommCP——专为MM-EQA设计的基于大语言模型的去中心化通信框架。该框架采用保形预测技术校准生成信息，从而最小化接收方干扰并提升通信可靠性。为评估框架性能，我们构建了包含多样化照片级真实家庭场景与具身问题的MM-EQA基准测试。实验表明，CommCP较基线方法显著提升了任务成功率和探索效率。实验视频、代码及数据集详见项目网站：https://comm-cp.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of coordinating multiple heterogeneous robots to complete natural language assignments in household environments, where effective information gathering and communication are critical. The authors formalize this as a Multi-Agent Multi-task Embodied Question Answering (MM-EQA) problem and propose CommCP, a decentralized LLM-based communication framework that uses conformal prediction to calibrate generated messages, reducing receiver distraction and improving reliability. Experiments on a new photo-realistic MM-EQA benchmark show that CommCP significantly improves task success rates and exploration efficiency compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多台异构机器人在家庭环境中协作完成自然语言指令的协调问题，其中有效的信息收集与通信至关重要。作者将此形式化为多智能体多任务具身问答问题，并提出了CommCP框架：一种基于大语言模型的去中心化通信方法，它使用共形预测来校准生成消息的置信度，以减少接收方的干扰。在一个新构建的逼真家庭场景基准测试上的实验结果表明，与基线方法相比，CommCP显著提高了任务成功率和探索效率。</div>
</details>
</div>
<div class="card">
<div class="title">Can vision language models learn intuitive physics from interaction?</div>
<div class="meta-line">Authors: Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz</div>
<div class="meta-line">First: 2026-02-05T18:59:20+00:00 · Latest: 2026-02-05T18:59:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06033v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06033v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型能否通过交互学习直观物理？</div>
<div class="mono" style="margin-top:8px">预训练的视觉语言模型对物理世界缺乏良好的直觉。近期研究表明，监督微调能提升模型在简单物理任务上的表现，但微调后的模型并未学到可泛化至新情境的稳健物理规则。基于认知科学研究，我们假设模型需通过与环境交互来正确学习其物理动态。我们使用强化学习训练模型，使其通过环境交互进行学习。尽管交互学习能提升模型在任务内的表现，但未能形成具有泛化性的物理直觉。研究发现，在单一任务上训练的模型无法可靠地泛化至相关任务，即使这些任务共享视觉统计特征与物理原理，且无论模型是否通过交互训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates whether vision-language models can acquire robust physical intuitions, motivated by their known limitations in understanding the physical world and the hypothesis from cognitive science that interaction is key to learning dynamics. The method involves training models through interaction with an environment using reinforcement learning, comparing this approach to supervised fine-tuning. The key experimental finding is that while interaction-based training improves within-task performance, it fails to produce models with generalizable physical rules; models trained on one task do not reliably transfer to related tasks sharing visual and physical principles, regardless of the training paradigm.</div>
<div class="mono" style="margin-top:8px">本研究探讨视觉语言模型能否获得稳健的物理直觉，其动机在于现有模型缺乏对物理世界的理解，且监督微调方法的泛化能力有限。该方法通过强化学习让模型与环境交互进行训练，假设这种交互对于学习物理动态至关重要。关键的实验结果表明，尽管交互式学习提升了任务内的性能，但未能产生可泛化的物理直觉，因为在一个任务上训练的模型无法可靠地泛化到具有相似视觉统计和物理原理的相关任务上。</div>
</details>
</div>
<div class="card">
<div class="title">PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling</div>
<div class="meta-line">Authors: Kavana Venkatesh, Yinhan He, Jundong Li, Jiaming Cui</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06030v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06030v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysicsAgentABM：基于物理引导的生成式多智能体建模</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的多智能体系统虽能实现表达性强的智能体推理，但扩展成本高昂，且难以校准时间步对齐的状态转移模拟；而经典多智能体模型（ABM）虽具可解释性，却难以整合丰富的个体层面信号与非稳态行为。本文提出PhysicsAgentABM，将推理转向行为一致的智能体集群：状态特化的符号智能体编码机制化转移先验，多模态神经转移模型捕捉时序与交互动态，不确定性感知的认知融合产生校准后的集群级转移分布。个体智能体随后在局部约束下随机实现转移，从而将群体推理与实体级变异性解耦。我们进一步提出ANCHOR——一种基于跨情境行为响应与新型对比损失的LLM智能体驱动聚类策略，将LLM调用减少至多6-8倍。在公共卫生、金融和社会科学领域的实验表明，该方法在事件时间准确性与校准度上持续优于机制模型、神经模型及LLM基线。通过以不确定性感知的神经符号融合为核心重构群体级推理的生成式ABM，PhysicsAgentABM为基于LLM的可扩展校准仿真建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of LLM-based multi-agent systems, which are costly and poorly calibrated for timestep-aligned simulation, and classical agent-based models (ABMs), which lack rich individual-level signals. The proposed PhysicsAgentABM shifts inference to agent clusters, combining state-specialized symbolic agents for mechanistic priors, a multimodal neural transition model for dynamics, and uncertainty-aware epistemic fusion to produce calibrated cluster-level transitions; individual agents then stochastically realize transitions under local constraints. A key component is ANCHOR, an LLM-driven clustering strategy using cross-contextual behavioral responses and a contrastive loss, reducing LLM calls by 6-8 times. Experiments in public health, finance, and social sciences demonstrate improved event-time accuracy and calibration over mechanistic, neural, and LLM baselines.</div>
<div class="mono" style="margin-top:8px">该研究针对基于大语言模型的多智能体系统成本高、仿真校准性差，以及经典基于智能体的模型难以整合丰富个体行为的问题，提出了PhysicsAgentABM方法。该方法将推理转移到智能体集群，结合符号智能体提供机制先验、神经转换模型捕捉动态，以及认知融合生成校准的集群级分布，个体智能体则在局部约束下随机实现状态转换。同时引入ANCHOR这一基于大语言模型的聚类策略，将大语言模型调用减少6-8倍。在公共卫生、金融和社会科学领域的实验表明，该方法在事件时间准确性和校准性上优于机制、神经和大语言模型基线。</div>
</details>
</div>
<div class="card">
<div class="title">AP-OOD: Attention Pooling for Out-of-Distribution Detection</div>
<div class="meta-line">Authors: Claus Hofmann, Christian Huber, Bernhard Lehner, Daniel Klotz, Sepp Hochreiter, Werner Zellinger</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06031v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AP-OOD：基于注意力池化的分布外检测</div>
<div class="mono" style="margin-top:8px">分布外检测旨在将高维数据映射为标量OOD分数，对机器学习模型的可靠部署至关重要。当前研究的核心挑战在于如何有效利用并聚合语言模型中的词元嵌入以获取OOD分数。本文提出AP-OOD——一种面向自然语言的新型OOD检测方法，该方法突破基于简单平均的聚合方式，通过挖掘词元级信息实现更精准的检测。AP-OOD采用半监督框架，可在无监督与有监督设置间灵活切换，从而利用有限的辅助离群数据。实验表明，AP-OOD在文本OOD检测领域达到最新最优水平：在无监督设置下，将XSUM摘要任务的FPR95从27.84%降至4.67%，将WMT15英法翻译任务的FPR95从77.08%降至70.37%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of effectively aggregating token embeddings from language models to compute reliable out-of-distribution (OOD) scores for text data. The proposed method, AP-OOD, introduces an attention pooling mechanism that leverages token-level information, moving beyond simple average-based aggregation, and operates as a semi-supervised approach to utilize limited auxiliary outlier data. Experimental results demonstrate state-of-the-art performance, significantly reducing the FPR95 from 27.84% to 4.67% on the XSUM summarization dataset and from 77.08% to 70.37% on the WMT15 En-Fr translation dataset in unsupervised settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决如何有效聚合语言模型的词元嵌入以计算文本数据可靠分布外检测分数的关键挑战。提出的AP-OOD方法引入了注意力池化机制，利用词元级信息超越简单的基于平均的聚合，并作为一种半监督方法能够利用有限的辅助异常数据。实验结果表明该方法取得了最先进的性能，在无监督设置下，将XSUM摘要数据集的FPR95从27.84%显著降低至4.67%，并将WMT15英法翻译数据集的FPR95从77.08%降低至70.37%。</div>
</details>
</div>
<div class="card">
<div class="title">Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</div>
<div class="meta-line">Authors: Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan</div>
<div class="meta-line">First: 2026-02-05T18:58:32+00:00 · Latest: 2026-02-05T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06029v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>好奇心即知识：基于主动推断的自洽学习与无遗憾优化</div>
<div class="mono" style="margin-top:8px">主动推断通过最小化期望自由能，借助好奇心系数平衡认知价值（信息增益）与实用价值（任务性能），统一了探索与利用。然而，这种平衡何时能同时实现连贯的学习与高效的决策一直不明确：好奇心不足可能导致短视的利用并阻碍不确定性消解，而过度的好奇心则可能引发不必要的探索与遗憾。我们首次为最小化期望自由能的智能体建立了理论保证，证明单一条件——足够的好奇心——即可同时确保自洽学习（贝叶斯后验一致性）和无遗憾优化（累积遗憾有界）。分析揭示了该机制如何依赖于初始不确定性、可识别性与目标对齐性，从而在一个理论框架内将主动推断与经典贝叶斯实验设计及贝叶斯优化联系起来。我们进一步将这些理论转化为实用设计准则，用于在混合学习-优化问题中调节认知-实用权衡，并通过真实实验验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge in active inference of balancing exploration and exploitation via the Expected Free Energy, where improper tuning of the curiosity coefficient can lead to either myopic exploitation or wasteful exploration. The authors provide the first theoretical guarantee that a sufficiently high curiosity coefficient ensures both self-consistent Bayesian learning and no-regret optimization, linking the framework to Bayesian experimental design and optimization. Experimental validation confirms that these theoretical insights yield practical guidelines for tuning the epistemic-pragmatic trade-off in hybrid problems.</div>
<div class="mono" style="margin-top:8px">本研究针对主动推理中通过期望自由能平衡探索与利用的挑战，其中好奇心系数调节不当会导致短视利用或无效探索。方法上建立了理论保证，证明单一充分好奇心条件可同时确保自洽学习（实现贝叶斯后验一致性）和无悔优化（累积遗憾有界），从而将主动推理与贝叶斯实验设计和优化联系起来。实验验证表明，该理论框架为混合学习-优化任务中认知-实用权衡的调节提供了实用设计指导。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</div>
<div class="meta-line">Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang</div>
<div class="meta-line">First: 2026-02-05T18:57:09+00:00 · Latest: 2026-02-05T18:57:09+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/BudgetMem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06025v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/BudgetMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向运行时智能体内存的查询感知预算层级路由学习</div>
<div class="mono" style="margin-top:8px">内存对于超越单上下文窗口运行的大语言模型（LLM）智能体日益关键，但现有系统多依赖离线、查询无关的内存构建方式，效率低下且可能丢失查询关键信息。尽管运行时内存利用是自然替代方案，先前研究常伴随显著开销，且对性能-成本权衡缺乏显式控制。本文提出\textbf{BudgetMem}——一个支持显式查询感知性能-成本控制的运行时智能体内存框架。该框架将内存处理构建为多模块结构，每个模块提供三种预算层级（即\textsc{低}/\textsc{中}/\textsc{高}）。轻量级路由器通过强化学习训练的紧凑神经策略，执行跨模块预算层级路由以平衡任务性能与内存构建成本。以BudgetMem为统一测试平台，我们研究实现预算层级的三种互补策略：实现方式（方法复杂度）、推理机制（推断行为）与容量配置（模块模型规模）。在LoCoMo、LongMemEval和HotpotQA数据集上的实验表明：在优先性能的场景（即高预算设置）中，BudgetMem超越强基线模型；在严格预算限制下，能提供更优的精度-成本边界。进一步分析揭示了不同层级策略的优劣特性，明确了各策略维度在不同预算区间实现最优权衡的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the inefficiency and query-agnostic nature of offline memory construction for LLM agents, and the high overhead or lack of explicit control in prior runtime memory approaches. The method introduces BudgetMem, a runtime agent memory framework that structures memory into modules, each with three budget tiers (Low/Mid/High), and uses a lightweight neural router trained with reinforcement learning to perform query-aware budget-tier routing for performance-cost control. Key experimental results on LoCoMo, LongMemEval, and HotpotQA show that BudgetMem outperforms strong baselines in high-budget settings and achieves better accuracy-cost trade-offs under tight budgets, while analysis reveals the distinct strengths of different tiering strategies (implementation, reasoning, capacity) across budget regimes.</div>
<div class="mono" style="margin-top:8px">该研究的动机源于现有大型语言模型（LLM）智能体内存系统的局限性：它们通常依赖离线、与查询无关的构建方式，效率低下且可能丢弃关键信息，而运行时替代方案则开销高昂且对性能-成本权衡的控制有限。方法上提出了BudgetMem，这是一个运行时智能体内存框架，它将内存处理构建为多个模块，每个模块提供三个预算层级（低/中/高），并采用一个通过强化学习训练的轻量级神经路由器，执行跨模块的查询感知预算层级路由，以实现明确的性能-成本控制。在LoCoMo、LongMemEval和HotpotQA上的关键实验结果表明，BudgetMem在高预算设置下优于强基线，在更严格的预算下实现了更好的准确率-成本权衡，分析还揭示了不同层级策略（实现、推理和容量）在不同预算制度下的各自优势。</div>
</details>
</div>
<div class="card">
<div class="title">Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</div>
<div class="meta-line">Authors: Miranda Muqing Miao, Young-Min Cho, Lyle Ungar</div>
<div class="meta-line">First: 2026-02-05T18:55:56+00:00 · Latest: 2026-02-05T18:55:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06022v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正确性优化的残差激活透镜（CORAL）：可迁移且感知校准的推理时引导方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）存在持续校准偏差，尤其在指令微调和偏好对齐后。修改训练目标可改善校准，但重新训练成本高昂。推理时引导提供了一种轻量级替代方案，但现有方法多优化正确性代理而非正确性本身。本文提出CORAL（正确性优化的残差激活透镜），一种正则化的推理时引导方法，通过权重衰减MLP探针从模型内部激活中捕获分布式正确性信号。我们在三个70亿参数模型上评估CORAL，发现其平均提升准确率10%、降低预期校准误差（ECE）50%。进一步实验表明，这些增益无需重新训练即可迁移至四个保留基准测试（ARC-Challenge、HellaSwag、Math-MC、OpenBookQA）的完整公开测试集，平均提升准确率14%、降低ECE 49%。结果支持以下假设：当单个神经元信息不足时，可通过正则化探针提取模型内部的分布式信息。因此，CORAL为提升推理时多项选择题性能提供了一种计算高效、可迁移且感知校准的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the persistent miscalibration of large language models after instruction tuning without expensive retraining, this paper introduces CORAL, a regularized inference-time steering method that optimizes for correctness directly. The method uses weight-decay MLP probes to extract distributed correctness signals from model internal activations, steering generation during inference. Experimental results on three 7B-parameter models show average improvements of 10% in accuracy and 50% in expected calibration error, with these gains transferring to four held-out benchmarks (averaging 14% accuracy and 49% ECE improvements), demonstrating that regularized probes can effectively extract distributed internal information for compute-efficient performance enhancement.</div>
<div class="mono" style="margin-top:8px">为解决大型语言模型在指令微调和偏好对齐后持续存在的校准偏差问题，同时避免昂贵的重新训练，本文提出了CORAL方法，这是一种正则化的推理时引导方法，使用权重衰减的MLP探针从模型内部激活中提取分布式正确性信号。该方法在三个70亿参数模型上平均将准确率提高10%，将预期校准误差降低50%，且这些改进可迁移到四个保留基准测试集上，平均准确率提升14%，校准误差改善49%。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Model&#x27;s Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold</div>
<div class="meta-line">Authors: Ye He, Yitong Qiu, Molei Tao</div>
<div class="meta-line">First: 2026-02-05T18:55:03+00:00 · Latest: 2026-02-05T18:55:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06021v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06021v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model&#x27;s performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model&#x27;s inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的泛化能力可通过其朝向数据依赖的脊流形的归纳偏置来刻画</div>
<div class="mono" style="margin-top:8px">当扩散模型未记忆训练数据集时，其泛化机制究竟如何？对其生成分布的定量理解有助于评估模型在下游应用中的性能。为此，我们通过提出对数密度脊流形并量化生成数据随推断动态如何关联于该流形，来明确刻画扩散模型的生成行为。具体而言，推断过程围绕脊流形经历“抵达-对齐-滑动”三个阶段：轨迹首先抵达流形邻域，随后在法向被推近或推离流形以完成对齐，最终沿切向在流形上滑动。在此通用行为框架下，不同的训练误差将导致可量化的法向与切向运动差异，这些细节运动刻画了模态间生成现象的出现时机。对训练动态的更深入理解将带来对生成归纳偏置的更精确量化，本文以随机特征模型为例，具体阐释扩散模型的归纳偏置如何源于架构偏置与训练精度的组合，并如何随推断动态演化。在合成多模态分布与MNIST潜在扩散上的实验，均支持低维与高维场景中预测的方向性效应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand how diffusion models generalize beyond memorizing training data, this work characterizes the generated distribution by introducing a data-dependent log-density ridge manifold and analyzing inference dynamics. The method reveals a reach-align-slide process: trajectories first approach the manifold, then align along normal directions, and finally slide tangentially along it, with training errors dictating specific normal and tangent motions that explain inter-mode generation. Experimental results on synthetic multimodal distributions and MNIST latent diffusion confirm the predicted directional effects across dimensions, and a random feature model illustrates how inductive biases arise from architectural bias and training accuracy.</div>
<div class="mono" style="margin-top:8px">为理解扩散模型在非记忆训练数据时的泛化机制，本研究通过引入数据依赖的对数密度脊流形来刻画生成分布。分析表明推理动力学围绕该流形经历到达-对齐-滑动过程：轨迹先接近流形，随后沿法向对齐，最终沿切向滑动，其中训练误差决定了具体的法向和切向运动，从而解释模态间生成现象。在合成多模态分布和MNIST潜在扩散上的实验验证了这些方向性效应，随机特征模型进一步说明归纳偏置如何源于架构偏好与训练精度的组合。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanisms of AI Protein Folding in ESMFold</div>
<div class="meta-line">Authors: Kevin Lu, Jannik Brinkmann, Stefan Huber, Aaron Mueller, Yonatan Belinkov, David Bau, Chris Wendler</div>
<div class="meta-line">First: 2026-02-05T18:54:54+00:00 · Latest: 2026-02-05T18:54:54+00:00</div>
<div class="meta-line">Comments: Our code, data, and results are available at https://folding.baulab.info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06020v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESMFold中AI蛋白质折叠的机制</div>
<div class="mono" style="margin-top:8px">蛋白质结构预测模型如何折叠蛋白质？我们通过追踪ESMFold折叠β发夹（一种普遍存在的结构基序）的过程来研究这个问题。通过对模型潜在变量进行反事实干预，我们在折叠主干中识别出两个计算阶段。第一阶段，早期模块初始化成对生化信号：残基身份及相关生化特征（如电荷）从序列表征流入成对表征。第二阶段，后期模块发展成对空间特征：距离和接触信息在成对表征中累积。我们证明，ESMFold结构决策的机制可被定位、通过可解释表征追踪，并能通过强因果效应进行操控。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand the computational mechanisms underlying protein structure prediction models, this study investigates how ESMFold folds a beta hairpin motif. The method involves tracing the folding process and performing counterfactual interventions on the model&#x27;s latent representations. The key experimental findings reveal a two-stage computational process: early model blocks initialize pairwise biochemical signals like residue identities and charge flow from sequence, while later blocks develop pairwise spatial features such as distance and contact information, demonstrating that structural decisions can be localized, traced, and causally manipulated.</div>
<div class="mono" style="margin-top:8px">本研究通过分析ESMFold如何折叠β发夹这一常见结构基序，探究了蛋白质结构预测模型的内部机制。方法包括追踪折叠过程并对模型的潜在表示进行反事实干预。主要实验结果表明存在两个不同的计算阶段：早期模块初始化残基身份和电荷流等成对生化信号，而晚期模块则发展出距离和接触信息等成对空间特征，证明结构决策可以被定位、追踪并因果性地操控。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Token Prediction via Self-Distillation</div>
<div class="meta-line">Authors: John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein</div>
<div class="meta-line">First: 2026-02-05T18:54:48+00:00 · Latest: 2026-02-05T18:54:48+00:00</div>
<div class="meta-line">Comments: 8 pages and 5 figures in the main body</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $&lt;5\%$ drop in accuracy relative to single token decoding performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自蒸馏的多令牌预测方法</div>
<div class="mono" style="margin-top:8px">现有加速语言模型推理的技术（如推测解码）需要训练辅助推测模型并构建复杂的推理流程。本文提出一种新方法：通过简单的在线蒸馏目标，将预训练的自回归语言模型从缓慢的单令牌预测模型转换为快速的独立多令牌预测模型。最终模型与预训练初始检查点保持完全相同的实现方式，无需添加任何辅助验证器或专用推理代码即可部署。在GSM8K数据集上，本方法生成的模型解码速度平均提升3倍以上，且准确率相较于单令牌解码仅下降不足5%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To simplify and accelerate language model inference without the complexity of auxiliary speculator models and specialized pipelines, this work proposes converting a pretrained autoregressive model into a standalone multi-token predictor via a simple online self-distillation objective. The method retains the original model&#x27;s architecture and deployment simplicity, eliminating the need for verifiers or extra inference code. Experimental results on GSM8K show the converted models achieve over 3x faster decoding on average with less than a 5% drop in accuracy compared to standard single-token decoding.</div>
<div class="mono" style="margin-top:8px">为了简化并加速语言模型推理，避免使用辅助模型或复杂流程，本研究提出通过在线自蒸馏将预训练的自回归模型转换为独立的多令牌预测器。该方法采用简单的蒸馏目标，使模型能在单次前向传播中预测多个未来令牌，同时保持与原始模型完全相同的架构和部署简易性。在GSM8K上的实验结果表明，所得模型的解码速度平均提升超过3倍，且准确率相比标准的单令牌解码下降不到5%。</div>
</details>
</div>
<div class="card">
<div class="title">Optimism Stabilizes Thompson Sampling for Adaptive Inference</div>
<div class="meta-line">Authors: Shunxing Yan, Han Zhong</div>
<div class="meta-line">First: 2026-02-05T18:52:54+00:00 · Latest: 2026-02-05T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm&#x27;s pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观性稳定化汤普森采样用于自适应推断</div>
<div class="mono" style="margin-top:8px">汤普森采样（TS）在随机多臂老虎机问题中被广泛使用，但其在自适应数据收集下的推断性质较为微妙。由于各臂的样本量具有随机性，且通过动作选择规则与奖励耦合，经典样本均值的渐近理论可能失效。我们在$K$臂高斯老虎机中研究这一现象，并指出\emph{乐观性}是恢复\emph{稳定性}的关键机制——稳定性是有效渐近推断的充分条件，要求每个臂的拉动次数集中在一个确定性尺度附近。首先，我们证明方差膨胀TS \citep{halder2025stable} 对任意$K \ge 2$（包括多个臂均为最优的挑战性场景）均具有稳定性，这通过将\citet{halder2025stable} 的双臂结果推广至一般$K$臂设定，解决了其提出的开放性问题。其次，我们分析了一种保持后验方差不变、但显式增加后验均值奖励的乐观修正方法，并得出了相同的稳定性结论。总之，适当实施的乐观性能够稳定汤普森采样，使多臂老虎机中的渐近有效推断成为可能，同时仅产生轻微的额外遗憾代价。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of performing valid statistical inference under adaptive data collection in multi-armed bandits using Thompson sampling (TS), where classical asymptotic theory fails due to the randomness and coupling of arm-specific sample sizes with rewards. The study identifies optimism as a key stabilizing mechanism and analyzes two specific implementations: variance-inflated TS and an alternative method that adds an explicit mean bonus to the posterior mean. The main experimental findings prove that both optimistic modifications ensure stability—where each arm&#x27;s pull count concentrates around a deterministic scale—for any number of arms K ≥ 2, including scenarios with multiple optimal arms, thereby enabling asymptotically valid inference with only a mild additional regret cost.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在多臂赌博机问题中使用汤普森采样进行自适应数据收集时，由于臂特定的样本量随机且与奖励通过动作选择规则耦合，导致经典渐近理论失效，从而难以进行有效统计推断的挑战。研究将乐观主义识别为关键的稳定机制，并分析了两种具体实现：方差膨胀的汤普森采样以及一种保持后验方差不变但对后验均值添加显式奖励的替代方法。主要实验结果证明，这两种乐观主义修改都能确保稳定性——即每个臂的拉动次数围绕一个确定性尺度集中——适用于任意臂数 K ≥ 2，包括存在多个最优臂的情况，从而实现了渐近有效的推断，且仅带来轻微的额外遗憾代价。</div>
</details>
</div>
<div class="card">
<div class="title">AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</div>
<div class="meta-line">Authors: Xianyang Liu, Shangding Gu, Dawn Song</div>
<div class="meta-line">First: 2026-02-05T18:50:36+00:00 · Latest: 2026-02-05T18:50:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06008v1">PDF</a> · <a href="https://github.com/SafeRL-Lab/AgenticPay">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgenticPay：面向买卖交易的多智能体大语言模型协商系统</div>
<div class="mono" style="margin-top:8px">基于大语言模型的智能体被日益期望能自主进行协商、协调与交易，但现有基准缺乏评估多智能体间语言驱动经济交互的原则性设定。本文提出AgenticPay——一个由自然语言驱动的多智能体买卖协商基准与仿真框架。该框架模拟买卖双方拥有私有约束和产品相关估值的市场环境，要求通过多轮语言协商（而非仅数值竞价）达成协议。框架支持涵盖双边议价至多对多市场的110余项任务，具备结构化动作提取机制，并提供可行性、效率与福利等评估指标。对前沿闭源与开源权重大语言模型的基准测试揭示了协商性能的显著差距，凸显了长程战略推理的挑战，从而确立AgenticPay作为研究智能体商业与语言化市场交互的基础平台。代码与数据集可通过链接获取：https://github.com/SafeRL-Lab/AgenticPay。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for principled evaluation settings for language-mediated economic interactions among LLM-based agents, as existing benchmarks lack such frameworks. The method introduces AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language, modeling markets with private constraints and product-dependent valuations that require multi-round linguistic negotiation. Key experimental results from benchmarking state-of-the-art LLMs reveal substantial gaps in negotiation performance and highlight challenges in long-horizon strategic reasoning, establishing the framework as a foundation for studying agentic commerce.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有基准缺乏评估多智能体语言中介经济交互的原则性设置的问题。方法上提出了AgenticPay，这是一个基于自然语言驱动的多智能体买卖双方谈判基准与模拟框架，该框架模拟了具有私有约束和产品依赖估值的市场，支持从双边议价到多对多市场的110多项任务，并包含结构化行动提取和经济指标。对最先进的大语言模型进行基准测试的实验结果揭示了谈判性能上的显著差距，突显了长时程战略推理的挑战，从而确立了AgenticPay作为研究智能体商业和基于语言的市场交互的基础。</div>
</details>
</div>
<div class="card">
<div class="title">On Computation and Reinforcement Learning</div>
<div class="meta-line">Authors: Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-05T18:45:57+00:00 · Latest: 2026-02-05T18:45:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05999v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05999v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论计算与强化学习</div>
<div class="mono" style="margin-top:8px">强化学习策略可用的计算量如何影响其学习效果？使用固定参数量的策略是否仍能从额外计算中获益？标准强化学习框架缺乏正式回答这些问题的理论工具。实践中，深度强化学习策略常被参数化为静态架构的神经网络，混淆了计算量与参数量。本文形式化定义了计算受限策略，并证明使用更多计算的策略能解决计算较少策略无法处理的问题，且能泛化至更长时域的任务。基于算法学习和无模型规划的前期研究，我们提出一种能灵活利用可变计算量的最小化架构。实验与理论相互印证：在涵盖在线与离线强化学习的31项任务中，该架构（1）仅通过增加计算量即可获得更强性能，（2）与使用多达5倍参数的标准前馈网络或深度残差网络相比，在长时域测试任务中展现出更优的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how the computational budget, distinct from parameter count, influences reinforcement learning (RL) policy performance and generalization. To formally address this, the authors introduce a framework for compute-bounded policies and propose a minimal architecture capable of utilizing variable computation, building on concepts from algorithmic learning and model-free planning. Experimental results across 31 diverse online and offline RL tasks demonstrate that this architecture achieves stronger performance with increased compute and exhibits superior generalization to longer-horizon tasks compared to standard feedforward or deep residual networks, even those with significantly more parameters.</div>
<div class="mono" style="margin-top:8px">本研究探讨了计算预算（区别于参数数量）如何影响强化学习策略的性能和泛化能力。为了对此进行形式化分析，作者提出了一个计算受限策略的框架，并设计了一种能够利用可变计算量的最小化架构。在31个在线和离线强化学习任务上的实验结果表明，该架构通过使用更多计算资源实现了更强的性能，并且在更长视野的测试任务上，比参数数量多出5倍的标准前馈网络或深度残差网络具有更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Transmuting prompts into weights</div>
<div class="meta-line">Authors: Hanna Mazzawi, Benoit Dherin, Michael Munn, Michael Wunder, Javier Gonzalvo</div>
<div class="meta-line">First: 2025-10-09T18:40:39+00:00 · Latest: 2026-02-05T18:44:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08734v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08734v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt&#x27;s influence can be mathematically mapped to token-dependent implicit weight updates (Dherin et. al, 2025), we derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector-and-matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将提示转化为权重</div>
<div class="mono" style="margin-top:8px">越来越多的研究表明，通过直接修改大型语言模型的内部状态——无论是通过对其激活值进行向量加法，还是更新其权重矩阵——可以在推理时有效控制其行为。这些技术虽然强大，但通常依赖于经验启发式方法，例如从对比提示的平均激活中推导导向向量。本研究为这些干预措施提供了理论基础，解释了它们如何从Transformer架构的基本计算中自然产生。基于近期关于提示影响可数学映射为依赖令牌的隐式权重更新的发现（Dherin等人，2025），我们推导出一种原则性方法，将此信息压缩为独立于令牌的思维向量和思维矩阵。这些构造为现有基于向量和矩阵的模型编辑技术提供了理论解释，并提供了一种直接、基于计算的将文本输入转化为可复用权重更新的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to provide a theoretical foundation for empirical techniques that control large language model behavior by modifying internal states, such as adding steering vectors to activations or updating weight matrices. The method builds on the finding that a prompt&#x27;s influence can be mapped to token-dependent implicit weight updates, and it derives a principled approach to condense this information into token-independent thought vectors and matrices. The key outcome is that these constructs theoretically explain existing model editing methods and offer a direct, computationally-grounded way to convert textual input into reusable weight updates.</div>
<div class="mono" style="margin-top:8px">本研究旨在为通过修改内部状态（如在激活中添加导向向量或更新权重矩阵）来控制大语言模型行为的经验性技术提供理论基础。该方法基于提示的影响可映射为依赖于令牌的隐式权重更新的发现，推导出一种将此类信息压缩为独立于令牌的思维向量和思维矩阵的原则性方法。主要实验结果表明，这些构造从理论上解释了现有的模型编辑技术，并提供了一种直接的、基于计算的方法，可将文本输入转化为可重复使用的权重更新。</div>
</details>
</div>
<div class="card">
<div class="title">Causal Inference on Stopped Random Walks in Online Advertising</div>
<div class="meta-line">Authors: Jia Yuan Yu</div>
<div class="meta-line">First: 2026-02-05T18:43:29+00:00 · Latest: 2026-02-05T18:43:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05997v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05997v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user&#x27;s interaction-trajectory, and each advertiser&#x27;s bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线广告中停止随机游走的因果推断</div>
<div class="mono" style="margin-top:8px">本文探讨在线广告系统中常见的因果推断问题：发布商（如Instagram、TikTok）通过间歇性向用户展示竞价广告，与用户和广告商重复交互。每种处理对应广告机制的一个参数值（如拍卖保留价），我们通过实验估计其长期处理效应（如年度广告收入）。处理不仅影响广告展示的即时收益，还会改变用户交互轨迹和广告商竞价策略（后者受有限预算约束）。特定处理甚至可能影响用户规模，因为用户对可容忍的广告机制交互更久。我们摒弃经典独立同分布假设，将实验测量值（如广告收入）建模为停止随机游走，结合预算分割实验设计、Anscombe定理、类Wald方程及中心极限定理，构建长期处理效应的置信区间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of estimating long-term causal effects in online advertising systems, where repeated interactions between users, advertisers, and platforms create complex dependencies that violate standard i.i.d. assumptions. The method models experimental measurements, such as advertising revenue, as stopped random walks to account for treatment impacts on user trajectories and advertiser budget-constrained bidding policies, employing a budget-splitting design, Anscombe&#x27;s theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for long-term effects. Key experimental findings demonstrate the approach&#x27;s ability to provide reliable statistical inference for treatment effects, such as changes in annual revenue, even when treatments influence population sizes by altering user engagement duration.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在线广告系统中长期处理效应估计的难题，其中如拍卖保留价等干预不仅影响即时收入，还会改变用户参与轨迹和广告商在预算约束下的出价策略，甚至可能影响用户规模。方法摒弃了传统的独立同分布假设，将实验测量建模为停时随机游走，采用预算分割实验设计，并结合安斯科姆定理、沃尔德类方程和中心极限定理等统计工具，构建长期处理效应的置信区间。关键实验结果表明，该方法能为累积结果（如年度广告收入）提供可靠的统计推断，同时考虑了动态交互和广告商基于预算的适应性行为。</div>
</details>
</div>
<div class="card">
<div class="title">Orthogonal Self-Attention</div>
<div class="meta-line">Authors: Leo Zhang, James Martens</div>
<div class="meta-line">First: 2026-02-05T18:42:57+00:00 · Latest: 2026-02-05T18:42:57+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05996v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05996v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正交自注意力机制</div>
<div class="mono" style="margin-top:8px">软最大自注意力（SSA）是Transformer架构的核心组件。然而，在旨在提升表征学习的无跳跃连接架构中，近期研究揭示了SSA因导致秩塌缩和雅可比矩阵病态性而存在固有稳定性问题。本研究设计了一种新颖的注意力机制：正交自注意力（OSA），旨在规避SSA的这些问题，使无跳跃连接和归一化层的（非因果）Transformer更易于训练。具体而言，OSA通过将查询-键值构成的斜对称矩阵映射至矩阵指数空间，参数化得到正交注意力矩阵。我们证明该机制可利用查询-键值的低秩结构实现线性序列长度计算复杂度与内存开销。此外，我们推导出能确保OSA雅可比矩阵良态的初始化方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the instability issues of Softmax Self-Attention (SSA) in skipless Transformer architectures, such as rank collapse and poorly-conditioned Jacobians, this work introduces Orthogonal Self-Attention (OSA). The method parametrizes an orthogonal attention matrix by applying the matrix exponential to a skew-symmetric matrix constructed from query-key values, and it is implemented efficiently with linear complexity in sequence length by exploiting low-rank structure. Key experimental findings include a proven initialization scheme that ensures a well-conditioned Jacobian for OSA, enabling more stable training of Transformers without skip connections or normalization layers.</div>
<div class="mono" style="margin-top:8px">本研究针对无跳跃连接的Transformer架构中Softmax自注意力机制的不稳定性问题，如秩崩溃和雅可比矩阵病态，提出了正交自注意力机制。该方法通过将查询-键值构成的斜对称矩阵映射为正交注意力矩阵，并利用低秩结构实现线性计算复杂度。关键实验结果表明，所推导的初始化方案能保证正交自注意力机制的雅可比矩阵良态，从而使得无需跳跃连接或归一化层的非因果Transformer能够被有效训练。</div>
</details>
</div>
<div class="card">
<div class="title">Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps</div>
<div class="meta-line">Authors: Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz</div>
<div class="meta-line">First: 2026-02-05T18:42:00+00:00 · Latest: 2026-02-05T18:42:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05993v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05993v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose &quot;Diamond Maps&quot;, stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>钻石映射：基于随机流映射的高效奖励对齐方法</div>
<div class="mono" style="margin-top:8px">流模型与扩散模型能生成高质量样本，但训练后调整以适应用户偏好或约束仍成本高昂且脆弱，这一挑战通常称为奖励对齐。我们认为高效奖励对齐应是生成模型的内在属性而非事后补救，并为此重新设计模型以提升适应性。我们提出“钻石映射”——一种随机流映射模型，能在推理阶段高效精准地对齐任意奖励函数。钻石映射将多步模拟摊销为单步采样器（如流映射），同时保留奖励对齐所需随机性。该设计通过支持价值函数的高效一致估计，使搜索、序列蒙特卡洛和引导方法具备可扩展性。实验表明，钻石映射可通过GLASS流的蒸馏高效学习，获得更强的奖励对齐性能，且比现有方法更具扩展优势。研究结果为构建能在推理时快速适应任意偏好与约束的生成模型提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting flow and diffusion models to user preferences post-training, which is typically costly and brittle. The authors propose Diamond Maps, stochastic flow map models designed for efficient reward alignment at inference time by amortizing multiple simulation steps into a single-step sampler while preserving necessary stochasticity. Experiments demonstrate that Diamond Maps, learned via distillation from GLASS Flows, achieve stronger reward alignment performance and better scalability than existing methods.</div>
<div class="mono" style="margin-top:8px">该研究针对流模型和扩散模型在训练后适应不同用户偏好时成本高昂且效果不稳定的问题，提出了一种名为“钻石映射”的随机流映射模型。该方法将多步模拟过程压缩为单步采样器，同时保留必要的随机性，从而在推理时实现对任意奖励函数的高效准确对齐。实验结果表明，通过从GLASS流模型蒸馏学习，钻石映射能够高效训练，在奖励对齐性能上优于现有方法，且具备更好的可扩展性，为生成模型快速适应任意偏好提供了实用途径。</div>
</details>
</div>
<div class="card">
<div class="title">Layer-wise LoRA fine-tuning: a similarity metric approach</div>
<div class="meta-line">Authors: Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Lucas Pellicer, Rosimeire Pereira Costa, Edson Bollis, Anna Helena Reali Costa, Artur Jordao</div>
<div class="meta-line">First: 2026-02-05T18:38:53+00:00 · Latest: 2026-02-05T18:38:53+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05988v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05988v1">PDF</a> · <a href="https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于相似性度量的分层LoRA微调方法</div>
<div class="mono" style="margin-top:8px">在网络规模数据集上预训练大语言模型已成为推进通用人工智能的基础。相比之下，提升其在下游任务中的预测性能通常需要通过微调来适配其知识。参数高效微调技术（如低秩适配LoRA）通过冻结预训练模型并更新少量参数，旨在降低该过程的计算成本。与全参数微调相比，这些方法根据配置可减少99%以上的可训练参数量。然而，随着大语言模型规模持续扩大，这种降幅可能仍显不足。本研究通过系统性地选择少量层进行LoRA或其变体的微调，以解决上述问题。我们认为并非所有层对模型适配的贡献均等，并基于此通过测量各层对内部表征变化的贡献度来识别最相关的待微调层。本方法与现有低秩适配技术正交且完全兼容，能在保持不同模型和任务预测性能的前提下，将基于LoRA技术的可训练参数量进一步降低50%。具体而言，在仅编码器架构中，这种参数削减在GLUE基准测试中仅导致可忽略的预测性能下降；在仅解码器架构中，数学问题求解与代码任务甚至出现小幅下降或性能提升。该方法同样适用于多模态模型，在所有层使用LoRA模块微调的对比实验中仍表现出竞争力。代码已开源：https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To further reduce the computational cost of fine-tuning large language models beyond standard Low-Rank Adaptation (LoRA), this work proposes a method to systematically select only the most relevant layers for adaptation. The approach measures the contribution of each layer to changes in internal representations to identify which layers to fine-tune with LoRA or its variants, a strategy orthogonal to existing low-rank techniques. Experiments show the method reduces trainable parameters by up to 50% compared to standard LoRA while maintaining predictive performance: on encoder-only models, it leads to a negligible performance drop on the GLUE benchmark, and on decoder-only models, it achieves small drops or even improvements in mathematical and coding tasks, with competitive results also observed for multimodal models.</div>
<div class="mono" style="margin-top:8px">为了在标准低秩适应（LoRA）基础上进一步降低大语言模型微调的计算成本，本研究提出了一种系统性地选择最相关层进行适配的方法。该方法通过相似性度量评估各层对内部表示变化的贡献，从而识别出一个用于LoRA或其变体微调的子层集。实验表明，与标准LoRA相比，该方法将可训练参数减少了高达50%，同时保持了性能：在编码器模型上，GLUE基准测试的性能下降可忽略不计；在解码器模型上，数学和代码任务性能仅有小幅下降甚至有所提升；该方法在多模态模型上也取得了有竞争力的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Clifford Kolmogorov-Arnold Networks</div>
<div class="meta-line">Authors: Matthias Wolff, Francesco Alesiani, Christof Duhme, Xiaoyi Jiang</div>
<div class="meta-line">First: 2026-02-05T18:25:40+00:00 · Latest: 2026-02-05T18:25:40+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05977v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>克利福德-柯尔莫哥洛夫-阿诺德网络</div>
<div class="mono" style="margin-top:8px">本文提出克利福德-柯尔莫哥洛夫-阿诺德网络（ClKAN），一种适用于任意克利福德代数空间的灵活高效函数逼近架构。针对高维代数伴随的指数级计算复杂度，我们采用随机拟蒙特卡洛网格生成方法予以解决。ClKAN还引入新型批归一化策略以处理变域输入。该架构在科学发现与工程领域具有应用价值，并在合成任务及物理启发的任务中得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop an efficient architecture for approximating functions in Clifford algebra spaces, addressing the challenge of exponential scaling in higher dimensions. The method introduces Clifford Kolmogorov-Arnold Networks (ClKAN), which employ Randomized Quasi Monte Carlo grid generation to manage dimensional complexity and incorporate novel batch normalization strategies for handling variable domain inputs. Experimental validation on synthetic and physics-inspired tasks demonstrates the model&#x27;s effectiveness for applications in scientific discovery and engineering.</div>
<div class="mono" style="margin-top:8px">本研究旨在为克利福德代数空间中的函数逼近开发一种高效架构，以解决高维情况下指数级扩展的挑战。该方法引入了克利福德Kolmogorov-Arnold网络（ClKAN），采用随机拟蒙特卡洛网格生成来处理维度问题，并整合了新的批归一化策略以适应可变域输入。在合成和物理启发任务上的实验验证证明了该模型在科学发现和工程领域的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space</div>
<div class="meta-line">Authors: Felipe D. Toro-Hernández, Jesuino Vieira Filho, Rodrigo M. Cabral-Carvalho</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-05T18:23:04+00:00 · Latest: 2026-02-05T18:23:04+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures (excluding refs/appendix). Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05971v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05971v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将人类概念生成中的语义导航表征为嵌入空间中的轨迹</div>
<div class="mono" style="margin-top:8px">语义表征可被构建为结构化、动态的知识空间，人类通过在其中导航来检索和操纵意义。为探究人类如何穿越这一几何空间，我们提出一个将概念生成表征为嵌入空间导航的框架。利用不同的Transformer文本嵌入模型，我们基于累积嵌入构建参与者特定的语义轨迹，并提取几何与动态指标，包括至下一概念距离、至质心距离、熵、速度和加速度。这些指标同时捕捉语义导航的标量与方向性特征，为语义表征搜索提供了基于计算的几何空间运动视角。我们在跨语言的四个数据集上评估该框架，涵盖不同属性生成任务：神经退行性疾病、言语流畅性（脏话）、意大利语属性列举任务和德语属性列举任务。在这些情境中，我们的方法能区分临床组别与概念类型，提供了一个相较于传统高人工强度的语言预处理方法更少依赖人工干预的数学框架。与非累积方法的比较表明，累积嵌入在较长轨迹中表现最优，而较短轨迹可能因语境不足更适合非累积方法。关键的是，不同嵌入模型产生了相似结果，凸显了不同训练流程下习得表征间的共性。通过将语义导航构建为嵌入空间中的结构化轨迹，该框架连接了认知建模与习得表征，从而建立了量化语义表征动态的流程，可应用于临床研究、跨语言分析和人工认知评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand how humans navigate semantic knowledge during concept production, this study frames semantic search as movement through embedding space. The method constructs participant-specific semantic trajectories from cumulative embeddings using transformer models, extracting geometric and dynamical metrics like distance, entropy, velocity, and acceleration to capture both scalar and directional aspects of navigation. Experimental results across four multilingual datasets show the framework effectively distinguishes clinical groups and concept types with minimal human intervention, reveals that cumulative embeddings excel for longer trajectories while non-cumulative suits shorter ones, and demonstrates consistent outcomes across different embedding models, highlighting shared representational structures.</div>
<div class="mono" style="margin-top:8px">为了理解人类在概念产生过程中如何导航语义知识，本研究将语义搜索构建为在嵌入空间中的移动。该方法使用Transformer模型从累积嵌入构建参与者特定的语义轨迹，提取距离、熵、速度和加速度等几何与动态指标，以捕捉导航的标量和方向特征。在跨四种多语言属性生成任务数据集的实验评估表明，该方法能以最少的人工干预有效区分临床组和概念类型，揭示累积嵌入适用于较长轨迹而非累积方法更适合较短轨迹，并且发现不同嵌入模型产生一致结果，突显了不同学习表征之间的共性结构。</div>
</details>
</div>
<div class="card">
<div class="title">Inverse Depth Scaling From Most Layers Being Similar</div>
<div class="meta-line">Authors: Yizhou Liu, Sara Kangaslahti, Ziming Liu, Jeff Gore</div>
<div class="meta-line">First: 2026-02-05T18:22:41+00:00 · Latest: 2026-02-05T18:22:41+00:00</div>
<div class="meta-line">Comments: 23 pages, 24 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05970v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>源于多数层相似性的逆深度缩放</div>
<div class="mono" style="margin-top:8px">神经缩放定律描述了大型语言模型（LLM）中损失与模型规模的关系，但深度与宽度对性能的影响可能不同，需要更细致的研究。本文通过分析LLM与玩具残差网络，量化了深度对损失的影响。我们发现LLM中损失与深度成反比缩放，这可能源于功能相似的层通过集成平均降低误差，而非组合式学习或离散化平滑动态。这种机制虽低效但稳健，可能源自残差网络的结构偏置以及与平滑动态不兼容的目标函数。研究结果表明，提升LLM效率可能需要通过架构创新来促进深度的组合式利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates how depth specifically influences performance in large language models, motivated by the need to move beyond general scaling laws that treat model size uniformly. By analyzing both LLMs and toy residual networks, the method reveals that loss scales inversely with depth, primarily because many layers are functionally similar and act like an ensemble to average errors, rather than enabling compositional learning or discretizing smooth dynamics. Key experimental findings indicate this regime is inefficient but robust, likely stemming from residual network biases and target functions unsuited for smooth dynamics, suggesting architectural innovations are needed to make depth usage more compositionally efficient.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究深度如何具体影响大语言模型的性能，动机是需要超越将模型大小视为统一整体的通用缩放定律。通过分析真实的大语言模型和玩具残差网络，该方法量化了深度与损失之间的关系。关键的实验发现是，损失与深度成反比缩放，这种模式归因于功能相似的层执行了集成平均，而非实现组合学习或离散化平滑动态；这种机制效率低下但鲁棒性强，表明架构偏差可能限制了效率，因此需要创新架构以更好地组合利用深度。</div>
</details>
</div>
<div class="card">
<div class="title">A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders</div>
<div class="meta-line">Authors: Mohamad Amin Jamshidi, Mehrbod Zarifi, Zolfa Anvari, Hamed Ghafarirad, Mohammad Zareinejad</div>
<div class="meta-line">First: 2026-02-05T18:21:28+00:00 · Latest: 2026-02-05T18:21:28+00:00</div>
<div class="meta-line">Comments: Published in: 2025 33rd International Conference on Electrical Engineering (ICEE), Publisher IEEE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05967v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05967v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm&#x27;s performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于液压缸实时摩擦力估计的混合数据驱动算法</div>
<div class="mono" style="margin-top:8px">液压系统因其高功率输出、精确控制及恶劣环境适应能力而广泛应用于工业领域。液压缸作为系统执行器，通过液压油位移实现力和位置控制，但其运行受摩擦力显著影响。实现液压缸精确控制需建立不同工况下的准确摩擦力模型。现有基于实验推导的分析模型需识别影响因素，但存在适应性有限、计算效率不足的问题。本研究提出一种基于长短期记忆网络与随机森林的数据驱动混合算法，用于非线性摩擦力估计。该算法利用液压实验台采集的训练数据，有效融合特征检测与估计过程，在多种工况及外部负载变化下保持模型误差稳定低于10%，确保复杂场景下的鲁棒性。单次估计计算耗时仅1.51毫秒，满足实时性要求。该方法通过高精度与高效计算克服了分析模型的局限。通过详细分析与实验验证（包括与LuGre模型的直接对比），证明该算法性能优越。对比表明：LuGre模型虽提供摩擦力建模理论基础，但无法动态适应液压缸工况变化，进一步凸显了本混合算法在实时应用中的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of traditional analytical friction models in hydraulic cylinders, which lack adaptability and computational efficiency for real-time use, this study proposes a hybrid data-driven algorithm combining Long Short-Term Memory networks and Random Forests for nonlinear friction force estimation. The method integrates feature detection and estimation using training data from an experimental hydraulic test setup, achieving a stable model error below 10% across diverse operating conditions and load variations, with a computational cost of 1.51 milliseconds per estimation suitable for real-time applications. Experimental validation, including comparisons with the LuGre model, demonstrates that the proposed approach offers superior precision and adaptability to dynamic conditions, overcoming the LuGre model&#x27;s limitations in adjusting to varying operational states.</div>
<div class="mono" style="margin-top:8px">针对传统液压缸摩擦分析模型在实时应用中适应性差、计算效率低的局限，本研究提出了一种结合长短期记忆网络和随机森林的混合数据驱动算法，用于非线性摩擦力估计。该方法利用实验液压测试装置获取的训练数据，整合了特征检测与估计过程。实验验证表明，该算法在不同工况和负载变化下均保持模型误差低于10%，每次估计计算成本为1.51毫秒，实现了稳健的实时性能，并在动态适应性方面优于静态的LuGre模型。</div>
</details>
</div>
<div class="card">
<div class="title">Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access</div>
<div class="meta-line">Authors: Daniel Ebi, Gaspard Lambrechts, Damien Ernst, Klemens Böhm</div>
<div class="meta-line">First: 2025-09-30T09:32:20+00:00 · Latest: 2026-02-05T18:21:20+00:00</div>
<div class="meta-line">Comments: 11 pages, 26 pages total, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26000v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26000v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Asymmetric actor-critic methods are widely used in partially observable reinforcement learning, but typically assume full state observability to condition the critic during training, which is often unrealistic in practice. We introduce the informed asymmetric actor-critic framework, allowing the critic to be conditioned on arbitrary state-dependent privileged signals without requiring access to the full state. We show that any such privileged signal yields unbiased policy gradient estimates, substantially expanding the set of admissible privileged information. This raises the problem of selecting the most adequate privileged information in order to improve learning. For this purpose, we propose two novel informativeness criteria: a dependence-based test that can be applied prior to training, and a criterion based on improvements in value prediction accuracy that can be applied post-hoc. Empirical results on partially observable benchmark tasks and synthetic environments demonstrate that carefully selected privileged signals can match or outperform full-state asymmetric baselines while relying on strictly less state information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知情非对称行动者-评论者：利用超越全状态访问的特权信号</div>
<div class="mono" style="margin-top:8px">非对称行动者-评论者方法在部分可观测强化学习中广泛应用，但通常假设评论者在训练期间能基于全状态条件进行更新，这在实践中往往不现实。我们提出知情非对称行动者-评论者框架，允许评论者基于任意与状态相关的特权信号进行条件更新，而无需访问完整状态。我们证明任何此类特权信号都能产生无偏的策略梯度估计，从而大幅扩展了可采纳特权信息的范围。这引出了如何选择最合适的特权信息以提升学习效果的问题。为此，我们提出两种新颖的信息量准则：一种可在训练前应用的基于依赖关系的测试，以及一种基于价值预测精度改进的事后准则。在部分可观测基准任务和合成环境中的实证结果表明，精心选择的特权信号在依赖严格更少状态信息的情况下，能匹配甚至超越全状态非对称基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Asymmetric actor-critic methods in partially observable reinforcement learning typically require the critic to access the full state during training, a condition often impractical in real-world applications. To address this, the authors propose the informed asymmetric actor-critic framework, which allows the critic to be conditioned on arbitrary state-dependent privileged signals without needing the full state, while still providing unbiased policy gradient estimates. They further introduce two informativeness criteria—a dependence-based pre-training test and a post-hoc value prediction accuracy measure—to select effective privileged signals. Experimental results on benchmark tasks and synthetic environments show that well-chosen privileged signals can match or exceed the performance of full-state asymmetric baselines while using strictly less state information.</div>
<div class="mono" style="margin-top:8px">非对称行动者-评论者方法通常要求评论者在训练期间能够访问完整状态信息，但这在实践中往往不切实际。本研究提出了信息增强的非对称行动者-评论者框架，允许评论者基于任意与状态相关的特权信号进行条件化，而无需访问完整状态，同时保持策略梯度估计的无偏性。为了选择有效的特权信息，作者提出了两个信息量标准：基于依赖性的训练前测试和基于价值预测准确性的训练后标准。在部分可观测基准任务和合成环境上的实验表明，精心选择的特权信号能够匹配甚至超越完整状态非对称基线的性能，同时使用更少的状态信息。</div>
</details>
</div>
<div class="card">
<div class="title">Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces</div>
<div class="meta-line">Authors: Arran Carter, Sanghyeok Choi, Kirill Tamogashev, Víctor Elvira, Nikolay Malkin</div>
<div class="meta-line">First: 2026-02-05T18:16:57+00:00 · Latest: 2026-02-05T18:16:57+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/mmacosha/offpolicy-discrete-diffusion-samplers-and-bridges</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05961v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05961v1">PDF</a> · <a href="https://github.com/mmacosha/offpolicy-discrete-diffusion-samplers-and-bridges">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离散扩散采样器与桥：离策略算法及其在隐空间中的应用</div>
<div class="mono" style="margin-top:8px">从已知归一化常数的分布 $p(x) \propto e^{-\mathcal{E}(x)}$ 中采样是统计学中一个重要且具有挑战性的问题。近年来，一类被称为扩散采样器的新型摊销采样算法兴起，能够从非归一化密度中实现快速高效采样。这类算法在连续空间采样任务中已得到广泛研究，但在离散空间问题中的应用仍鲜有探索。尽管该领域已取得一些进展，但现有离散扩散采样器未能充分利用连续空间采样的常用思路。本文通过引入离散扩散采样器的离策略训练技术来弥合这一差距，并证明这些技术能提升离散采样器在既有及新合成基准测试中的性能。进一步，我们将离散扩散采样器推广至两个任意分布间的桥接任务，首次在离散领域引入数据到能量的薛定谔桥训练方法。最后，我们展示了所提出的扩散采样器在图像生成模型离散隐空间中无数据后验采样的应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of sampling from unnormalized discrete distributions, where existing diffusion samplers have not fully leveraged techniques successful in continuous spaces. The method introduces off-policy training for discrete diffusion samplers and extends them to bridge arbitrary distributions via a discrete Schrödinger bridge formulation. Experiments demonstrate improved performance on synthetic benchmarks and successful application to data-free posterior sampling in discrete latent spaces of image generative models.</div>
<div class="mono" style="margin-top:8px">本研究针对从未归一化离散分布中采样的挑战，现有扩散采样器未能充分利用连续空间中的成功技术。方法引入了离散扩散采样器的离策略训练，并通过离散域薛定谔桥公式将其扩展至连接任意分布。实验表明，该方法在合成基准上提升了采样器性能，并成功应用于图像生成模型离散潜空间中的数据无关后验采样。</div>
</details>
</div>
<div class="card">
<div class="title">Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</div>
<div class="meta-line">Authors: Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim</div>
<div class="meta-line">First: 2026-02-05T18:08:20+00:00 · Latest: 2026-02-05T18:08:20+00:00</div>
<div class="meta-line">Comments: Project Page: https://junwankimm.github.io/CSFM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05951v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05951v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://junwankimm.github.io/CSFM">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更优源分布，更佳流匹配：学习条件依赖的源分布以优化流匹配</div>
<div class="mono" style="margin-top:8px">流匹配技术近期已成为扩散生成模型的有力替代方案，尤其在文本到图像生成领域。尽管其允许任意源分布的灵活性，现有方法大多沿用扩散模型的标准高斯分布，鲜少将源分布本身作为优化目标。本研究证明，在现代文本到图像系统规模下，源分布的原则性设计不仅可行且能显著提升性能。具体而言，我们提出在流匹配目标下学习条件依赖的源分布，以更充分利用丰富的条件信号。我们揭示了将条件直接融入源分布时出现的关键失效模式（如分布坍缩与不稳定），并证明适当的方差正则化及源-目标方向对齐对稳定高效学习至关重要。进一步，我们分析了目标表示空间的选择如何影响结构化源分布的流匹配，揭示了此类设计最有效的机制。在多个文本到图像基准上的大量实验表明，该方法能带来持续稳健的改进，包括FID收敛速度提升最高达3倍，凸显了条件流匹配中原则性源分布设计的实用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Flow matching offers a flexible generative modeling alternative to diffusion models, but existing methods typically adopt a standard Gaussian source distribution without optimizing it for conditional tasks like text-to-image generation. This work proposes learning a condition-dependent source distribution under the flow matching objective to better leverage conditioning signals, addressing key challenges such as distributional collapse and instability through variance regularization and directional alignment between source and target. Experiments on text-to-image benchmarks show that this principled source design yields consistent improvements, including up to a 3x faster convergence in FID, demonstrating its practical efficacy.</div>
<div class="mono" style="margin-top:8px">流匹配作为一种灵活的生成建模框架，通常采用标准高斯分布作为源分布，这可能无法充分利用条件信号。本研究提出在流匹配目标中学习一个条件依赖的源分布，并通过方差正则化以及源与目标之间的方向对齐来解决分布坍缩和不稳定性等关键挑战。在文本到图像生成基准上的实验表明，这种原则性的源分布设计带来了持续的改进，包括FID收敛速度最高提升3倍。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking Symmetry Bottlenecks in GNN Readouts</div>
<div class="meta-line">Authors: Mouad Talhi, Arne Wolf, Anthea Monod</div>
<div class="meta-line">First: 2026-02-05T18:08:13+00:00 · Latest: 2026-02-05T18:08:13+00:00</div>
<div class="meta-line">Comments: 23 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05950v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05950v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破图神经网络读出层的对称性瓶颈</div>
<div class="mono" style="margin-top:8px">图神经网络（GNN）广泛应用于结构化数据学习，但其区分非同构图的能力存在根本性局限。这些局限通常归因于消息传递机制；本研究揭示在读出阶段存在独立的瓶颈。通过有限维表示理论，我们证明所有线性置换不变读出操作（包括求和与均值池化）均通过雷诺兹（群平均）算子进行分解，从而将节点嵌入投影到置换作用的不变子空间，无论编码器表达能力如何，都会消除所有非平凡的对称感知分量。这既产生了新的表达能力界限，也对全局池化保留或破坏的信息提供了可解释的特征描述。为克服这种坍缩，我们引入基于投影算子的不变读出方法，将节点表示分解为对称感知通道，并通过非线性不变统计量进行汇总，在保持置换不变性的同时，保留可证明对平均操作不可见的信息。实证表明，仅替换读出层即可使固定编码器区分WL-hard图对，并在多个基准测试中提升性能，证明读出设计是影响GNN表达能力的关键且未被充分重视的因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses a previously overlooked bottleneck in graph neural networks (GNNs) by showing that linear permutation-invariant readouts, such as sum or mean pooling, inherently project node embeddings onto a fixed subspace, erasing symmetry-aware information regardless of encoder expressivity. To overcome this, the authors introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining critical information. Experiments demonstrate that simply replacing the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, highlighting readout design as a decisive factor in GNN expressivity.</div>
<div class="mono" style="margin-top:8px">本研究针对图神经网络（GNN）中一个先前被忽视的表达能力瓶颈，该瓶颈独立于已知的消息传递限制，存在于读出阶段。作者利用有限维表示理论证明，标准的线性置换不变读出（如求和或平均池化）通过雷诺兹算子固有地将节点嵌入投影到一个固定子空间，从而丢弃所有非平凡的对称感知信息，无论编码器能力如何。为克服此问题，他们引入了基于投影子的不变读出方法，将表示分解为对称感知通道并应用非线性不变统计量，在保持置换不变性的同时保留原本会丢失的信息。实验表明，仅替换读出模块就能使固定编码器区分WL-困难的图对，并在多个基准测试中提升性能，凸显了读出设计是影响GNN表达能力的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-02-05T18:03:03+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v2">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题发现新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时执行强化学习，使大型语言模型能够持续训练，但此时训练经验专门针对测试问题。这种持续学习形式非常特殊，因为其目标是产生一个卓越解决方案而非多个平均良好的方案，并且旨在解决当前特定问题而非泛化至其他问题。因此，我们的学习目标和搜索子程序被设计为优先考虑最有潜力的解决方案。我们将此方法称为“测试时训练发现法”。遵循先前研究，我们专注于具有连续奖励的问题。我们在数学、GPU内核工程、算法设计和生物学领域报告了所有尝试问题的结果。该方法在几乎所有问题上都创造了新的最优解：（i）埃尔德什最小重叠问题与自相关不等式；（ii）GPUMode内核竞赛（比现有技术快达2倍）；（iii）过往AtCoder算法竞赛；（iv）单细胞分析中的去噪问题。我们的解决方案均经过专家或组织者评审。所有结果均使用开源模型OpenAI gpt-oss-120b实现，并可通过我们公开的代码复现，而此前的最佳结果需依赖闭源前沿模型。测试时训练通过Thinking Machines的Tinker API运行，每个问题的成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance AI&#x27;s capability to discover novel state-of-the-art solutions for scientific problems, moving beyond prior test-time scaling methods that rely on prompting frozen LLMs. The proposed method, Test-Time Training to Discover (TTT-Discover), employs reinforcement learning at test time, allowing the LLM to continuously train with experience specific to the target problem; this is designed as a form of continual learning focused on generating a single optimal solution for a specific problem rather than achieving average generalization. Key experimental results demonstrate that TTT-Discover sets new state-of-the-art performance across diverse domains, including mathematics (e.g., Erdős&#x27; minimum overlap problem), GPU kernel engineering (achieving up to 2x speedup), algorithm design (past AtCoder competitions), and biology (single-cell analysis denoising), with all solutions validated by experts and achieved using an open model and publicly available code.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升人工智能在科学问题中发现新颖最优解的能力，超越先前依赖提示冻结大型语言模型的测试时扩展方法。所提出的方法——测试时训练发现（TTT-Discover），在测试时采用强化学习，使大型语言模型能够针对特定测试问题持续训练，专注于生成单一最优解而非平均性能。关键实验结果表明，TTT-Discover在多个领域实现了新的最优性能，包括数学（埃尔德什最小重叠问题和自相关不等式）、GPU内核工程（速度提升高达2倍）、算法设计（过往AtCoder竞赛）和生物学（单细胞分析去噪），所有解决方案均经专家验证，并使用开放模型和公开代码实现。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260207_0451.html">20260207_0451</a>
<a href="archive/20260207_0345.html">20260207_0345</a>
<a href="archive/20260206_0629.html">20260206_0629</a>
<a href="archive/20260206_0531.html">20260206_0531</a>
<a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
