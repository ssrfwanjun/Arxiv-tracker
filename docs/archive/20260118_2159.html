<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-18 21:59</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260118_2159</div>
    <div class="row"><div class="card">
<div class="title">See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection</div>
<div class="meta-line">Authors: Amir Mallak, Erfan Aasi, Shiva Sreeram, Tsun-Hsuan Wang, Daniela Rus, Alaa Maalouf</div>
<div class="meta-line">First: 2026-01-15T18:58:33+00:00 · Latest: 2026-01-15T18:58:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少看多开：基于基础模型随机补丁选择的端到端自动驾驶泛化方法</div>
<div class="mono" style="margin-top:8px">端到端自动驾驶的最新进展表明，基于基础模型提取的补丁对齐特征训练的策略在分布外场景中具有更好的泛化能力。我们假设由于自注意力机制，每个补丁特征都以不同方式和强度隐式嵌入/包含了所有其他补丁的信息，导致这些描述符高度冗余。我们通过主成分分析和跨补丁相似性量化了此类（BLIP2）特征的冗余度：90%的方差可由17/64个主成分捕获，且强跨令牌相关性普遍存在。基于这种重叠信息训练会导致策略过拟合虚假相关性，损害分布外鲁棒性。我们提出随机补丁选择方法——一种简单有效的策略学习方案，能提升模型的鲁棒性、泛化性和效率。SPS对每帧图像随机掩码部分补丁描述符（不输入策略模型），同时保持剩余补丁的空间布局。策略因此获得同一场景的不同随机但完整的视图：每个随机补丁子集都构成对世界的不同但合理的连贯投影。策略决策将基于对特定令牌存留具有不变性的特征。大量实验证实，在所有分布外场景中，本方法均优于现有最优技术：平均提升6.2%，闭环仿真中最高提升20.4%，同时提速2.4倍。我们通过掩码率和补丁特征重组进行消融实验，训练评估的9个系统中有8个超越先前最优技术。最后，我们证明学习到的策略无需调整即可迁移至物理世界的真实车辆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the issue of feature redundancy in foundation model-based end-to-end autonomous driving, which leads to overfitting and poor out-of-distribution (OOD) generalization, this paper introduces Stochastic Patch Selection (SPS). The method randomly masks a fraction of patch descriptors at each frame while preserving spatial layout, forcing the policy to rely on invariant features from stochastic yet coherent scene views. Experimental results demonstrate that SPS outperforms state-of-the-art methods across all OOD scenarios, achieving an average 6.2% improvement and up to 20.4% in closed-loop simulations, while being 2.4 times faster, and the learned policy successfully transfers to a real-world car without tuning.</div>
<div class="mono" style="margin-top:8px">针对基于基础模型的自动驾驶中特征冗余导致过拟合和分布外泛化能力差的问题，本文提出了随机补丁选择方法。该方法在每帧随机掩蔽一部分补丁描述符同时保持空间布局，迫使策略依赖于同一场景不同随机视图中的不变特征。实验表明，该方法在所有分布外场景中均优于现有最佳方法，平均提升6.2%，闭环仿真中最高提升20.4%，且速度提升2.4倍，所学策略无需调整即可成功迁移到真实物理车辆上。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Euclidean Distance Field Properties for Fast and Safe 3D planning with a modified Lazy Theta*</div>
<div class="meta-line">Authors: Jose A. Cobano, L. Merino, F. Caballero</div>
<div class="meta-line">Venue: Published in Robotics and Autonomous Systems (RAS), 2025</div>
<div class="meta-line">First: 2025-05-29T21:51:02+00:00 · Latest: 2026-01-15T17:24:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24024v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.24024v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the FS-Planner, a fast graph-search planner based on a modified Lazy Theta* algorithm that exploits the analytical properties of Euclidean Distance Fields (EDFs). We introduce a new cost function that integrates an EDF-based term proven to satisfy the triangle inequality, enabling efficient parent selection and reducing computation time while generating safe paths with smaller heading variations. We also derive an analytic approximation of the EDF integral along a segment and analyze the influence of the line-of-sight limit on the approximation error, motivating the use of a bounded visibility range. Furthermore, we propose a gradient-based neighbour-selection mechanism that decreases the number of explored nodes and improves computational performance without degrading safety or path quality. The FS-Planner produces safe paths with small heading changes without requiring the use of post-processing methods. Extensive experiments and comparisons in challenging 3D indoor simulation environments, complemented by tests in real-world outdoor environments, are used to evaluate and validate the FS-Planner. The results show consistent improvements in computation time, exploration efficiency, safety, and smoothness in a geometric sense compared with baseline heuristic planners, while maintaining sub-optimality within acceptable bounds. Finally, the proposed EDF-based cost formulation is orthogonal to the underlying search method and can be incorporated into other planning paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用欧几里得距离场特性实现基于改进Lazy Theta*算法的快速安全三维规划</div>
<div class="mono" style="margin-top:8px">本文提出FS-Planner，这是一种基于改进Lazy Theta*算法的快速图搜索规划器，充分利用欧几里得距离场的解析特性。我们引入一种新型成本函数，其中整合了经证明满足三角不等式的EDF项，从而实现高效的父节点选择、减少计算时间，同时生成航向变化更小的安全路径。我们还推导出线段上EDF积分的解析近似，并分析视线限制对近似误差的影响，论证了使用有限可见范围的有效性。此外，我们提出基于梯度的邻域选择机制，减少探索节点数量，在不降低安全性或路径质量的前提下提升计算性能。FS-Planner无需后处理方法即可生成航向变化小的安全路径。通过在复杂三维室内仿真环境中的大量实验与对比，并结合真实室外环境测试，对FS-Planner进行评估验证。结果表明，相较于基准启发式规划器，该算法在计算时间、探索效率、安全性及几何平滑度方面均获得持续改进，同时将次优性控制在可接受范围内。最后，所提出的基于EDF的成本公式与底层搜索方法正交，可融入其他规划范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for efficient and safe 3D path planning by developing a fast graph-search planner that leverages the geometric properties of Euclidean Distance Fields (EDFs). The method introduces a modified Lazy Theta* algorithm, termed FS-Planner, which incorporates a novel EDF-based cost function proven to satisfy the triangle inequality for efficient parent selection, an analytic approximation for the EDF integral along a path segment, and a gradient-based neighbor-selection mechanism to reduce node exploration. Experimental evaluations in challenging 3D indoor simulations and real-world outdoor tests demonstrate that the planner achieves consistent improvements in computation time, exploration efficiency, safety, and path smoothness compared to baseline heuristic planners, while maintaining acceptable sub-optimality bounds without requiring post-processing.</div>
<div class="mono" style="margin-top:8px">本研究旨在无需后处理的情况下，提升三维路径规划的计算速度与路径安全性。方法上提出了FS-Planner，该规划器通过改进Lazy Theta*算法，引入了一种利用欧几里得距离场三角不等式特性的新成本函数、对路径段上EDF积分的解析近似，以及一种基于梯度的邻居选择机制以减少节点探索。在模拟三维室内和真实室外环境中的实验结果表明，与基线方法相比，该规划器实现了更快的计算速度、更高的探索效率、更安全且航向变化更小的路径，以及更好的平滑度，同时将路径次优性保持在可接受的范围内。</div>
</details>
</div>
<div class="card">
<div class="title">RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization</div>
<div class="meta-line">Authors: Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang</div>
<div class="meta-line">First: 2025-12-28T03:45:57+00:00 · Latest: 2026-01-15T15:14:21+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00705v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.00705v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://breeze1124.github.io/rgs-slam-project-page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS. Additional details and resources are available at this URL: https://breeze1124.github.io/rgs-slam-project-page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RGS-SLAM：基于单次密集初始化的鲁棒高斯溅射SLAM</div>
<div class="mono" style="margin-top:8px">本文提出RGS-SLAM，一种鲁棒的高斯溅射SLAM框架，其采用免训练的对应-高斯初始化方法替代了GS-SLAM中基于残差的致密化阶段。相较于通过残差逐步添加高斯以补全缺失几何结构的方法，RGS-SLAM通过置信度感知的内点分类器精炼DINOv3描述符，从密集多视角对应关系中执行单次三角测量，在优化前生成分布均匀且结构感知的高斯种子。该初始化策略稳定了早期建图过程，收敛速度提升约20%，在纹理丰富和杂乱场景中实现更高渲染保真度，同时完全兼容现有GS-SLAM流程。在TUM RGB-D和Replica数据集上的评估表明，相较于最先进的高斯与点云SLAM系统，RGS-SLAM在定位与重建精度方面达到竞争性或更优水平，并保持最高925 FPS的实时建图性能。更多细节与资源详见：https://breeze1124.github.io/rgs-slam-project-page/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the instability and slow convergence in Gaussian Splatting SLAM systems by proposing RGS-SLAM, which replaces the traditional residual-driven densification with a one-shot dense initialization method. The core method involves generating dense multi-view correspondences using DINOv3 descriptors, refining them through a confidence-aware inlier classifier, and performing one-shot triangulation to create a well-distributed Gaussian seed prior to optimization. Experimental results on TUM RGB-D and Replica datasets demonstrate that this approach accelerates convergence by approximately 20%, achieves competitive or superior localization and reconstruction accuracy compared to state-of-the-art systems, and maintains real-time performance at up to 925 FPS while improving rendering fidelity in texture-rich and cluttered scenes.</div>
<div class="mono" style="margin-top:8px">本研究针对现有高斯泼溅SLAM系统中渐进式致密化阶段的计算效率低和稳定性问题，提出了一种解决方案。RGS-SLAM框架采用一次性、无需训练的初始化方法，取代了残差驱动的致密化过程，该方法通过对源自DINOv3描述符的密集多视角对应点进行三角测量，并经过置信度感知的内点分类器细化，在优化前生成结构化的高斯种子。在TUM RGB-D和Replica数据集上的实验结果表明，该方法使收敛速度加快约20%，在定位和重建精度上达到或超越了先进系统水平，在纹理丰富和杂乱场景中提高了渲染保真度，同时保持高达925 FPS的实时性能。</div>
</details>
</div>
<div class="card">
<div class="title">SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability</div>
<div class="meta-line">Authors: Ruochen Li, Kun Yuan, Yufei Xia, Yue Zhou, Qingyu Lu, Weihang Li, Youxiang Zhu, Nassir Navab</div>
<div class="meta-line">First: 2026-01-15T14:47:26+00:00 · Latest: 2026-01-15T14:47:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10455v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10455v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SurgGoal：基于目标可满足性的手术规划评估方法重构</div>
<div class="mono" style="margin-top:8px">手术规划融合了视觉感知、长程推理与流程知识，但现有评估方案能否在安全关键场景中可靠评估视觉语言模型仍不明确。本研究从目标导向视角出发，通过阶段目标可满足性定义规划正确性，即规划有效性由专家定义的手术规则判定。基于此定义，我们构建了包含有效流程变体及含顺序/内容错误无效方案的多中心元评估基准。实验表明，序列相似度指标系统性误判规划质量：既惩罚有效方案，又无法识别无效方案。因此，我们采用基于规则的目标可满足性指标作为高精度元评估基准，在渐进约束条件下评估视频大语言模型，揭示了感知错误与欠约束推理导致的失效案例。结构化知识能持续提升性能，而纯语义引导不可靠，仅在与结构化约束结合时才对更大模型产生增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable evaluation of vision-language models in safety-critical surgical planning, this research redefines planning correctness through phase-goal satisfiability based on expert-defined surgical rules. The method introduces a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors, then employs a rule-based goal-satisfiability metric as a high-precision reference to assess Video-LLMs under progressively constrained settings. Key experimental findings show that sequence similarity metrics systematically misjudge planning quality, while the proposed evaluation reveals model failures due to perception errors and under-constrained reasoning, with structural knowledge consistently improving performance whereas semantic guidance alone proves unreliable.</div>
<div class="mono" style="margin-top:8px">本研究出于在安全关键的手术规划中可靠评估视觉语言模型的需求，基于专家定义的手术规则和阶段目标可满足性重新定义了规划正确性。方法引入了一个包含有效程序变体和带有顺序/内容错误的无效计划的多中心元评估基准，然后使用基于规则的目标可满足性指标作为高精度参考，在逐步受限的设置下评估视频大语言模型。关键实验结果表明，传统的序列相似性指标系统性地误判规划质量，而所提出的评估揭示了模型失败源于感知错误和约束不足的推理，其中结构化知识持续提升性能，而单独的语义指导不可靠，仅在与结构化约束结合时才对更大模型有益。</div>
</details>
</div>
<div class="card">
<div class="title">Sampling-Based Constrained Motion Planning with Products of Experts</div>
<div class="meta-line">Authors: Amirreza Razmjoo, Teng Xue, Suhan Shetty, Sylvain Calinon</div>
<div class="meta-line">First: 2024-12-23T10:39:59+00:00 · Latest: 2026-01-15T14:31:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.17462v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.17462v2">PDF</a> · <a href="https://github.com/idiap/smpc_poe">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach to enhance the performance of sampling-based Model Predictive Control (MPC) in constrained optimization by leveraging products of experts. Our methodology divides the main problem into two components: one focused on optimality and the other on feasibility. By combining the solutions from each component, represented as distributions, we apply products of experts to implement a project-then-sample strategy. In this strategy, the optimality distribution is projected into the feasible area, allowing for more efficient sampling. This approach contrasts with the traditional sample-then-project and naive sample-then-reject method, leading to more diverse exploration and reducing the accumulation of samples on the boundaries. We demonstrate an effective implementation of this principle using a tensor train-based distribution model, which is characterized by its non-parametric nature, ease of combination with other distributions at the task level, and straightforward sampling technique. We adapt existing tensor train models to suit this purpose and validate the efficacy of our approach through experiments in various tasks, including obstacle avoidance, non-prehensile manipulation, and tasks involving staying in a restricted volume. Our experimental results demonstrate that the proposed method consistently outperforms known baselines, providing strong empirical support for its effectiveness. Sample codes for this project are available at https://github.com/idiap/smpc_poe.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于采样的约束运动规划与专家乘积方法</div>
<div class="mono" style="margin-top:8px">本文提出一种新颖方法，通过利用专家乘积提升基于采样的模型预测控制在约束优化中的性能。该方法将主问题分解为两个部分：一部分关注最优性，另一部分关注可行性。通过结合表示为分布的各个组件解，我们应用专家乘积实施“先投影后采样”策略。在此策略中，最优性分布被投影至可行区域，从而实现更高效的采样。与传统“先采样后投影”及朴素“先采样后拒绝”方法相比，该方法能促进更广泛的探索并减少边界样本积累。我们使用基于张量链的分布模型有效实现了这一原理，该模型具有非参数特性、易于在任务层面与其他分布结合，且采样技术简便。我们调整现有张量链模型以适应此目的，并通过避障、非抓取操作及受限空间停留等多种任务的实验验证了方法的有效性。实验结果表明，所提方法持续优于已知基线，为其效能提供了有力实证支持。项目示例代码发布于 https://github.com/idiap/smpc_poe。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To improve the performance of sampling-based Model Predictive Control (MPC) in constrained optimization, this work introduces a method that leverages products of experts to separate optimality and feasibility concerns. The approach employs a project-then-sample strategy, where an optimality distribution is projected into the feasible region before sampling, using a tensor train-based non-parametric model for efficient combination and sampling. Experiments in obstacle avoidance, non-prehensile manipulation, and restricted-volume tasks show that the method consistently outperforms baseline techniques like sample-then-project and sample-then-reject, offering more diverse exploration and reduced boundary accumulation.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进基于采样的模型预测控制（MPC）在约束优化问题中的性能，传统方法如先采样后投影可能导致低效的边界采样。所提出的方法将问题分解为最优性和可行性两个独立组件，将其解表示为概率分布，并采用专家乘积框架实现先投影后采样的策略。该策略先将最优性分布投影至可行区域再进行采样，从而促进更丰富的探索并减少边界累积。通过使用为此目的改编的非参数化张量链模型，在避障、非抓取操作和受限空间任务中的实验表明，该方法 consistently 优于已知基线。</div>
</details>
</div>
<div class="card">
<div class="title">Singularity-Free Guiding Vector Field over Bézier&#x27;s Curves Applied to Rovers Path Planning and Path Following</div>
<div class="meta-line">Authors: Alfredo González-Calvin, Lía García-Pérez, Juan Jiménez</div>
<div class="meta-line">Venue: Journal of Field Robotics, 2025. 42:2720-27-39</div>
<div class="meta-line">First: 2024-12-17T15:56:40+00:00 · Latest: 2026-01-15T13:47:35+00:00</div>
<div class="meta-line">Comments: Final version, accepted for publication. 26 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.13033v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.13033v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a guidance algorithm for solving the problem of following parametric paths, as well as a curvature-varying speed setpoint for land-based car-type wheeled mobile robots (WMRs). The guidance algorithm relies on Singularity-Free Guiding Vector Fields SF-GVF. This novel GVF approach expands the desired robot path and the Guiding vector field to a higher dimensional space, in which an angular control function can be found to ensure global asymptotic convergence to the desired parametric path while avoiding field singularities. In SF-GVF, paths should follow a parametric definition. This feature makes using Bezier&#x27;s curves attractive to define the robot&#x27;s desired patch. The curvature-varying speed setpoint, combined with the guidance algorithm, eases the convergence to the path when physical restrictions exist, such as minimal turning radius or maximal lateral acceleration. We provide theoretical results, simulations, and outdoor experiments using a WMR platform assembled with off-the-shelf components.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝塞尔曲线的无奇异性引导矢量场在漫游车路径规划与路径跟踪中的应用</div>
<div class="mono" style="margin-top:8px">本文提出一种用于解决参数化路径跟踪问题的引导算法，以及适用于陆地轮式移动机器人的曲率变化速度设定点。该引导算法基于无奇异性引导矢量场。这种新型GVF方法将期望机器人路径和引导矢量场扩展至高维空间，通过角控制函数确保对目标参数化路径的全局渐近收敛，同时避免场奇异性。SF-GVF要求路径采用参数化定义，该特性使贝塞尔曲线成为定义机器人期望路径的理想选择。曲率变化速度设定点与引导算法相结合，可在存在最小转弯半径或最大横向加速度等物理约束时，促进路径收敛。我们通过理论分析、仿真实验以及采用商用组件组装的WMR平台户外实验验证了该方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for robust path following in wheeled mobile robots, particularly to overcome singularities in guiding vector fields and accommodate physical constraints like turning radius limits. The method introduces a Singularity-Free Guiding Vector Field (SF-GVF) that embeds the desired path, defined parametrically using Bézier curves, into a higher-dimensional space to ensure global asymptotic convergence without singularities, combined with a curvature-varying speed setpoint to handle kinematic restrictions. Experimental validation through simulations and outdoor tests with an off-the-shelf rover platform demonstrates effective path following and convergence under real-world physical limitations.</div>
<div class="mono" style="margin-top:8px">本研究针对类车式轮式移动机器人的路径跟踪问题，提出了一种确保全局收敛且无奇点的引导算法。该方法采用无奇点引导向量场（SF-GVF），将期望路径和向量场嵌入高维空间以定义角度控制函数，并利用贝塞尔曲线进行参数化路径表示，同时结合曲率变化的速度设定点以适应转弯半径等物理约束。通过仿真和户外实验验证，基于定制机器人平台的测试表明，该方法在实际限制下能有效实现路径跟踪和收敛。</div>
</details>
</div>
<div class="card">
<div class="title">Bootstrap Off-policy with World Model</div>
<div class="meta-line">Authors: Guojian Zhan, Likun Wang, Xiangteng Zhang, Jiaxin Gao, Masayoshi Tomizuka, Shengbo Eben Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-01T06:33:04+00:00 · Latest: 2026-01-15T13:38:44+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00423v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00423v3">PDF</a> · <a href="https://github.com/molumitu/BOOM_MBRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy&#x27;s actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner&#x27;s non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner&#x27;s action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于世界模型的引导式离轨策略</div>
<div class="mono" style="margin-top:8px">在线规划在强化学习中已被证明能有效提升样本效率与最终性能。然而，将规划用于环境交互时，不可避免地会导致收集数据与策略实际行为之间的偏差，从而损害模型学习与策略改进。为此，我们提出BOOM（基于世界模型的引导式离轨策略框架），该框架通过引导循环紧密整合规划与离轨学习：策略初始化规划器，规划器则通过行为对齐优化动作以引导策略更新。该循环由联合学习的世界模型支持，使规划器能模拟未来轨迹，并提供价值目标以促进策略改进。BOOM的核心是一种无似然对齐损失，利用规划器的非参数动作分布引导策略，并结合软价值加权机制——该机制优先考虑高回报行为，并缓解回放缓冲区内规划器动作质量的波动性。在DeepMind Control Suite和Humanoid-Bench等高维环境上的实验表明，BOOM在训练稳定性和最终性能方面均达到最先进水平。代码发布于https://github.com/molumitu/BOOM_MBRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Online planning in reinforcement learning improves sample efficiency but creates a mismatch between collected data and actual policy behavior, which harms model learning and policy improvement. To address this, the authors propose BOOM, a framework that integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to align and bootstrap the policy, supported by a jointly learned world model for trajectory simulation and value targets. The method employs a likelihood-free alignment loss and a soft value-weighted mechanism to prioritize high-return behaviors and reduce planner action variability. Experiments on DeepMind Control Suite and Humanoid-Bench demonstrate that BOOM achieves state-of-the-art results in training stability and final performance.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习中在线规划导致收集数据与策略实际行为不一致的性能下降问题，提出了BOOM框架，通过引导循环将规划与离线学习紧密结合：策略初始化规划器，规划器通过行为对齐精化动作来引导策略，并辅以联合学习的世界模型进行轨迹模拟和价值目标提供。在DeepMind Control Suite和Humanoid-Bench上的实验表明，BOOM在训练稳定性和最终性能方面均达到了最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">Online identification of nonlinear time-varying systems with uncertain information</div>
<div class="meta-line">Authors: He Ren, Gaowei Yan, Hang Liu, Lifeng Cao, Zhijun Zhao, Gang Dang</div>
<div class="meta-line">First: 2026-01-15T13:33:48+00:00 · Latest: 2026-01-15T13:33:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10379v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10379v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定信息下非线性时变系统的在线辨识</div>
<div class="mono" style="margin-top:8px">数字孪生作为复杂信息物理系统实时监测与预测性维护的核心使能技术，对其虚拟模型提出了关键要求：高预测精度、强可解释性与在线自适应能力。然而，现有技术难以同时满足这些需求：贝叶斯方法擅长不确定性量化但缺乏模型可解释性，而可解释的符号辨识方法（如SINDy）受限于其离线批处理特性，难以实现实时更新。为弥合这一语义与计算鸿沟，本文提出一种基于贝叶斯回归的符号学习新框架。该框架将在线符号发现构建为统一的概率状态空间模型，通过引入稀疏马蹄先验，将模型选择转化为贝叶斯推断任务，实现系统辨识与不确定性量化的同步处理。进一步，我们推导出带遗忘因子的在线递归算法，并建立保证后验分布适定性的精确递归条件，这些条件同时可作为数据效用的实时监测器以增强算法鲁棒性。此外，严格的收敛性分析证明了参数估计在持续激励条件下的收敛性。案例研究验证了所提框架在实现可解释概率预测与在线学习方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of developing digital twin models that simultaneously achieve high predictive accuracy, strong interpretability, and online adaptive capability, as existing methods like Bayesian inference or symbolic regression often excel in only one aspect. The proposed solution is a Bayesian Regression-based Symbolic Learning (BRSL) framework, which formulates online symbolic discovery as a probabilistic state-space model using sparse horseshoe priors to enable simultaneous system identification and uncertainty quantification, and derives an online recursive algorithm with a forgetting factor and stability monitors. Experimental case studies demonstrate the framework&#x27;s effectiveness in achieving interpretable, probabilistic prediction and successful online learning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决数字孪生模型需同时具备高预测精度、强可解释性和在线自适应能力的挑战，因为现有方法如贝叶斯技术和符号识别（例如SINDy）难以兼顾这些特性。提出的解决方案是一种基于贝叶斯回归的符号学习（BRSL）框架，该框架将在线符号发现构建为概率状态空间模型，利用稀疏马蹄铁先验实现系统辨识和不确定性量化的同步进行，并引入了带遗忘因子的在线递归算法及实时数据效用监测机制。案例研究验证了该框架在实现可解释的概率预测和成功在线学习方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</div>
<div class="meta-line">Authors: Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-10T12:54:04+00:00 · Latest: 2026-01-15T13:22:05+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures, accepted to AAAI 2026. Project page:https://github.com/CASIA-IVA-Lab/UrbanNav</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09607v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09607v2">PDF</a> · <a href="https://github.com/CASIA-IVA-Lab/UrbanNav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanNav：基于网络规模人类轨迹的语言引导城市导航学习</div>
<div class="mono" style="margin-top:8px">利用自然语言指令在复杂城市环境中导航对具身智能体提出了重大挑战，包括噪声语言指令、模糊空间指代、多样化地标和动态街景。现有视觉导航方法通常局限于模拟或非街道环境，且依赖精确目标格式（如特定坐标或图像），限制了其在陌生城市执行末端配送等任务的自主智能体的实用性。为此，我们提出UrbanNav——一个可扩展的框架，通过训练具身智能体遵循自由形式语言指令适应多样化城市场景。基于网络规模的城市步行视频，我们开发了可扩展的标注流程，将人类导航轨迹与基于真实地标的语言指令对齐。UrbanNav包含超过1,500小时导航数据和300万条指令-轨迹-地标三元组，覆盖广泛城市场景。我们的模型通过学习鲁棒的导航策略处理复杂城市情境，展现出卓越的空间推理能力、对噪声指令的鲁棒性以及对未见城市场景的泛化能力。实验结果表明UrbanNav显著优于现有方法，凸显了大规模网络视频数据在实现具身智能体语言引导现实城市导航方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling embodied agents to navigate complex urban environments using free-form natural language instructions, which is hindered by noisy instructions, ambiguous references, and dynamic scenes. The authors introduce UrbanNav, a framework that trains agents by leveraging web-scale city walking videos through a scalable annotation pipeline to align human trajectories with language instructions grounded in real-world landmarks, resulting in a dataset of over 1,500 hours and 3 million instruction-trajectory-landmark triplets. Experiments demonstrate that the model learns robust navigation policies, showing superior spatial reasoning, robustness to instruction noise, and generalization to unseen urban settings, significantly outperforming existing methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身智能体在复杂城市环境中使用自由形式自然语言指令进行导航的挑战，该挑战受限于指令噪声、模糊参照和动态场景。作者提出了UrbanNav框架，通过利用网络规模的城市步行视频和一个可扩展的标注流程，将人类轨迹与基于真实世界地标的语言指令对齐，从而构建了一个包含超过1500小时导航数据和300万个指令-轨迹-地标三元组的数据集。实验结果表明，该模型学习到了鲁棒的导航策略，展现出优越的空间推理能力、对噪声指令的鲁棒性以及对未见城市环境的泛化能力，显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">FastStair: Learning to Run Up Stairs with Humanoid Robots</div>
<div class="meta-line">Authors: Yan Liu, Tao Yu, Haolin Song, Hongbo Zhu, Nianzong Hu, Yuzhi Hao, Xiuyong Yao, Xizhe Zang, Hua Chen, Jie Zhao</div>
<div class="meta-line">First: 2026-01-15T13:14:59+00:00 · Latest: 2026-01-15T13:14:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10365v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10365v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Running up stairs is effortless for humans but remains extremely challenging for humanoid robots due to the simultaneous requirements of high agility and strict stability. Model-free reinforcement learning (RL) can generate dynamic locomotion, yet implicit stability rewards and heavy reliance on task-specific reward shaping tend to result in unsafe behaviors, especially on stairs; conversely, model-based foothold planners encode contact feasibility and stability structure, but enforcing their hard constraints often induces conservative motion that limits speed. We present FastStair, a planner-guided, multi-stage learning framework that reconciles these complementary strengths to achieve fast and stable stair ascent. FastStair integrates a parallel model-based foothold planner into the RL training loop to bias exploration toward dynamically feasible contacts and to pretrain a safety-focused base policy. To mitigate planner-induced conservatism and the discrepancy between low- and high-speed action distributions, the base policy was fine-tuned into speed-specialized experts and then integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across the full commanded-speed range. We deploy the resulting controller on the Oli humanoid robot, achieving stable stair ascent at commanded speeds up to 1.65 m/s and traversing a 33-step spiral staircase (17 cm rise per step) in 12 s, demonstrating robust high-speed performance on long staircases. Notably, the proposed approach served as the champion solution in the Canton Tower Robot Run Up Competition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FastStair：人形机器人学习跑楼梯</div>
<div class="mono" style="margin-top:8px">跑楼梯对人类而言轻而易举，但对人形机器人却极具挑战，因其需同时满足高敏捷性与严格稳定性。无模型强化学习可生成动态步态，但隐式稳定性奖励及对任务特定奖励设计的重度依赖易导致不安全行为（尤其在楼梯上）；反之，基于模型的落脚点规划器虽能编码接触可行性与稳定性结构，但其硬约束常引发保守运动而限制速度。本文提出FastStair——一个规划器引导的多阶段学习框架，融合二者互补优势以实现快速稳定的上楼梯运动。该框架将并行模型落脚点规划器嵌入强化学习训练循环，引导探索朝向动态可行的接触点，并预训练以安全为核心的基策略。为缓解规划器导致的保守性及高低速动作分布差异，基策略被微调为速度专精专家，再通过低秩自适应技术整合，实现全指令速度范围内的平滑操作。我们将最终控制器部署于Oli人形机器人，在指令速度达1.65 m/s时稳定上楼梯，仅用12秒穿越33级螺旋阶梯（每级高17厘米），在长楼梯中展现了鲁棒的高速性能。值得注意的是，该方法在广州塔机器人登塔竞赛中作为冠军解决方案得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Running up stairs is challenging for humanoid robots because it demands both agility and stability, which are often in tension: model-free reinforcement learning can produce dynamic motions but may lead to unsafe behaviors, while model-based planners ensure stability but can be overly conservative and slow. To address this, the authors developed FastStair, a multi-stage learning framework that integrates a parallel model-based foothold planner into RL training to guide exploration toward feasible contacts and pretrain a safety-focused base policy, then fine-tunes this policy into speed-specialized experts and integrates them via Low-Rank Adaptation (LoRA) to enable smooth operation across a wide speed range. Experimental deployment on the Oli humanoid robot demonstrated stable stair ascent at speeds up to 1.65 m/s, successfully traversing a 33-step spiral staircase in 12 seconds, and the approach won the Canton Tower Robot Run Up Competition.</div>
<div class="mono" style="margin-top:8px">人形机器人上楼梯极具挑战性，因为它同时要求高敏捷性和严格稳定性，而现有方法往往难以兼顾：无模型强化学习能生成动态运动但通常不安全，而基于模型的足部规划器虽能保证稳定性却导致动作保守、速度缓慢。为此，FastStair提出一个多阶段学习框架，将并行模型足部规划器集成到强化学习训练循环中，以引导探索朝向可行接触点并预训练一个注重安全的基础策略；随后将该策略微调为速度专精的专家策略，并通过低秩自适应（LoRA）进行整合，以实现宽速度范围内的平滑操作。在Oli人形机器人上的实验表明，该方法能以最高1.65米/秒的指令速度稳定上楼梯，在12秒内成功穿越33级螺旋楼梯，并赢得了广州塔机器人登梯比赛的冠军。</div>
</details>
</div>
<div class="card">
<div class="title">CHORAL: Traversal-Aware Planning for Safe and Efficient Heterogeneous Multi-Robot Routing</div>
<div class="meta-line">Authors: David Morilla-Cabello, Eduardo Montijano</div>
<div class="meta-line">First: 2026-01-15T12:34:22+00:00 · Latest: 2026-01-15T12:34:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10340v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monitoring large, unknown, and complex environments with autonomous robots poses significant navigation challenges, where deploying teams of heterogeneous robots with complementary capabilities can substantially improve both mission performance and feasibility. However, effectively modeling how different robotic platforms interact with the environment requires rich, semantic scene understanding. Despite this, existing approaches often assume homogeneous robot teams or focus on discrete task compatibility rather than continuous routing. Consequently, scene understanding is not fully integrated into routing decisions, limiting their ability to adapt to the environment and to leverage each robot&#x27;s strengths. In this paper, we propose an integrated semantic-aware framework for coordinating heterogeneous robots. Starting from a reconnaissance flight, we build a metric-semantic map using open-vocabulary vision models and use it to identify regions requiring closer inspection and capability-aware paths for each platform to reach them. These are then incorporated into a heterogeneous vehicle routing formulation that jointly assigns inspection tasks and computes robot trajectories. Experiments in simulation and in a real inspection mission with three robotic platforms demonstrate the effectiveness of our approach in planning safer and more efficient routes by explicitly accounting for each platform&#x27;s navigation capabilities. We release our framework, CHORAL, as open source to support reproducibility and deployment of diverse robot teams.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHORAL：面向安全高效异构多机器人路径规划的遍历感知规划方法</div>
<div class="mono" style="margin-top:8px">利用自主机器人监测广阔、未知且复杂的环境面临显著的导航挑战，部署具备互补能力的异构机器人团队可大幅提升任务执行效能与可行性。然而，有效建模不同机器人平台与环境交互需要丰富的语义场景理解。现有方法常假设机器人团队同质化，或聚焦离散任务兼容性而非连续路径规划，导致场景理解未能充分融入路径决策，限制了系统适应环境及发挥各机器人优势的能力。本文提出一种集成语义感知的异构机器人协同框架：通过侦察飞行构建基于开放词汇视觉模型的度量-语义地图，识别需精细巡检的区域，并为各平台规划能力感知路径。这些信息被融入异构车辆路径规划模型，联合分配巡检任务并计算机器人轨迹。在仿真及包含三种机器人平台的实际巡检任务中，实验证明该方法通过显式考量各平台导航能力，能规划出更安全高效的路径。我们将框架CHORAL开源发布，以支持异构机器人团队的可复现研究与应用部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable safe and efficient monitoring of complex environments, this research addresses the limitations of existing multi-robot routing approaches, which often assume homogeneous teams or lack integrated scene understanding. The proposed CHORAL framework first constructs a metric-semantic map from a reconnaissance flight using open-vocabulary vision models, identifies regions for inspection, and generates capability-aware paths for heterogeneous robots. These elements are then integrated into a heterogeneous vehicle routing formulation that jointly assigns tasks and computes trajectories. Experimental results from simulation and a real-world inspection mission with three robotic platforms demonstrate that the method plans safer and more efficient routes by explicitly accounting for each platform&#x27;s navigation capabilities.</div>
<div class="mono" style="margin-top:8px">针对协调异构机器人团队监测复杂环境时现有方法常假设同质性或忽略连续路径规划的挑战，本文提出了CHORAL，一个集成的语义感知框架。该方法从侦察飞行开始，利用开放词汇视觉模型构建度量-语义地图，识别需要检查的区域并为每个平台生成能力感知路径，然后将其纳入异构车辆路径规划公式中，以联合分配任务并计算轨迹。在仿真和包含三个机器人平台的实际检查任务中的实验结果表明，该方法通过明确考虑每个平台的导航能力，规划出了更安全、更高效的路线。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</div>
<div class="meta-line">Authors: Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, Younggyo Seo</div>
<div class="meta-line">First: 2025-05-29T16:41:12+00:00 · Latest: 2026-01-15T11:24:14+00:00</div>
<div class="meta-line">Comments: 29 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00070v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.00070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robot-R1：基于强化学习的机器人具身推理增强框架</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）近期通过结合具身推理与机器人控制，在推动机器人技术发展方面展现出巨大潜力。现有方法通常采用监督微调（SFT）对机器人控制相关的具身推理任务进行训练，但SFT数据集常依赖启发式构建，未针对机器人控制优化，且易导致灾难性遗忘与泛化性能下降。为此，我们提出Robot-R1——一种利用强化学习专门增强机器人控制具身推理能力的新框架。该框架基于专家示范的当前场景图像与环境元数据，学习预测任务完成所需的下一个关键点状态。受DeepSeek-R1学习方法启发，Robot-R1对基于推理的响应进行采样，并强化那些能产生更准确预测的响应。为系统评估Robot-R1，我们还构建了需要多样化具身推理能力的新基准测试。实验表明，采用Robot-R1训练的模型在具身推理任务上优于SFT方法。尽管仅拥有70亿参数，Robot-R1在空间与运动推理等底层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in using Supervised Fine-Tuning (SFT) for training Large Vision-Language Models on embodied reasoning tasks, as SFT datasets are not optimized for robot control and can cause catastrophic forgetting. The proposed Robot-R1 framework employs reinforcement learning to enhance embodied reasoning by learning to predict the next keypoint state from current scene images and environment metadata, sampling and reinforcing reasoning-based responses that lead to accurate predictions. Experimental results on a new benchmark show that Robot-R1 outperforms SFT methods and, despite having only 7B parameters, surpasses GPT-4o on low-level action control reasoning tasks like spatial and movement reasoning.</div>
<div class="mono" style="margin-top:8px">本研究针对使用监督微调训练大型视觉语言模型进行机器人具身推理任务时的局限性，因为监督微调数据集未针对控制优化，且可能导致灾难性遗忘和泛化性能下降。提出的Robot-R1框架采用强化学习来增强具身推理，通过学习预测任务完成所需的下一关键点状态，条件基于当前场景图像和来自专家演示的环境元数据；它采样基于推理的响应并强化那些导致更准确预测的响应，灵感来自DeepSeek-R1方法。在需要多样具身推理能力的新基准测试中，实验结果表明，Robot-R1训练的模型优于监督微调方法，并且尽管仅有70亿参数，在空间和运动推理等低层动作控制推理任务上甚至超越了GPT-4o。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Quadrotor Control From Visual Features Using Differentiable Simulation</div>
<div class="meta-line">Authors: Johannes Heeg, Yunlong Song, Davide Scaramuzza</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2024-10-21T13:06:06+00:00 · Latest: 2026-01-15T10:42:31+00:00</div>
<div class="meta-line">Comments: Accepted for presentation at the IEEE International Conference on Robotics and Automation (ICRA) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.15979v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.15979v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The sample inefficiency of reinforcement learning (RL) remains a significant challenge in robotics. RL requires large-scale simulation and can still cause long training times, slowing research and innovation. This issue is particularly pronounced in vision-based control tasks where reliable state estimates are not accessible. Differentiable simulation offers an alternative by enabling gradient back-propagation through the dynamics model, providing low-variance analytical policy gradients and, hence, higher sample efficiency. However, its usage for real-world robotic tasks has yet been limited. This work demonstrates the great potential of differentiable simulation for learning quadrotor control. We show that training in differentiable simulation significantly outperforms model-free RL in terms of both sample efficiency and training time, allowing a policy to learn to recover a quadrotor in seconds when providing vehicle states and in minutes when relying solely on visual features. The key to our success is two-fold. First, the use of a simple surrogate model for gradient computation greatly accelerates training without sacrificing control performance. Second, combining state representation learning with policy learning enhances convergence speed in tasks where only visual features are observable. These findings highlight the potential of differentiable simulation for real-world robotics and offer a compelling alternative to conventional RL approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可微分模拟从视觉特征学习四旋翼飞行器控制</div>
<div class="mono" style="margin-top:8px">强化学习（RL）的样本效率低下仍是机器人学领域的重大挑战。RL需要大规模仿真且训练周期长，制约了研究创新。该问题在基于视觉的控制任务中尤为突出，因其难以获取可靠状态估计。可微分模拟通过动力学模型的梯度反向传播提供了替代方案，能产生低方差解析策略梯度，从而提升样本效率。然而，该方法在实际机器人任务中的应用仍有限。本研究展示了可微分模拟在学习四旋翼控制方面的巨大潜力：在可微分模拟中的训练，其样本效率与训练时间均显著优于无模型RL——当提供飞行器状态时，策略可在数秒内学会恢复四旋翼姿态；仅依赖视觉特征时，亦可在数分钟内完成学习。成功的关键在于两方面：首先，采用简化代理模型计算梯度，在保持控制性能的同时极大加速训练；其次，在仅能观测视觉特征的任务中，将状态表征学习与策略学习相结合提升了收敛速度。这些发现凸显了可微分模拟在实际机器人应用中的潜力，为传统RL方法提供了具有竞争力的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the sample inefficiency and long training times of reinforcement learning (RL) in robotics, especially for vision-based control where state estimates are unavailable, this work explores differentiable simulation as an alternative. The method employs a differentiable simulator to compute analytical policy gradients, using a simple surrogate model to accelerate gradient computation and combining state representation learning with policy learning for visual-only tasks. Experimental results show that training in differentiable simulation significantly outperforms model-free RL in sample efficiency and speed, enabling a quadrotor policy to learn recovery in seconds with state inputs and in minutes using only visual features.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习在机器人学中样本效率低、训练时间长的问题，特别是在无法获取状态估计的视觉四旋翼控制任务中。该方法采用可微分仿真，通过动力学模型实现梯度反向传播，使用简单的代理模型加速梯度计算，并将状态表征学习与策略学习相结合以处理视觉任务。实验结果表明，在可微分仿真中的训练在样本效率和速度上显著优于无模型强化学习，使得策略能在数秒内学会使用机体状态恢复四旋翼，或在数分钟内仅依靠视觉特征完成学习。</div>
</details>
</div>
<div class="card">
<div class="title">The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation</div>
<div class="meta-line">Authors: Eszter Birtalan, Miklós Koller</div>
<div class="meta-line">First: 2026-01-15T10:38:14+00:00 · Latest: 2026-01-15T10:38:14+00:00</div>
<div class="meta-line">Comments: 13 pages, 6 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10268v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉传感器配置对抓取学习效率的影响——仿真中的对比评估</div>
<div class="mono" style="margin-top:8px">触觉传感器正进入机器人领域，提供与接触表面相关的直接信息，包括接触事件、滑动事件乃至纹理识别。这些事件对机器人手设计（包括假肢）尤为重要，可显著提升抓取稳定性。然而，目前多数已发布的机器人手设计在手掌表面采用差异巨大的密度与布局配置，常占据大部分可用空间。本研究通过仿真评估了6种不同密度与布局的触觉传感器配置对强化学习的影响。我们的双实验系统确保了结果稳健性，不依赖于特定物理模拟器、机器人手模型或机器学习算法。结果显示，6种传感器化仿真中存在特定实验设置的影响及跨实验的普遍效应，并确定出一种在双实验中均持续表现最优的配置。这些结果可为未来机器人手（包括假肢）设计研究提供参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how varying tactile sensor densities and layouts on robotic hands affect grasp learning efficiency, motivated by the need to optimize sensor placement for improved grasp stability in applications like prosthetics. The method employs a simulation-based comparative evaluation of six distinct tactile sensor configurations, using a two-setup system to ensure robustness across different physics simulators, hand models, and reinforcement learning algorithms. Key experimental findings reveal both setup-specific and generalized performance effects, with one configuration consistently outperforming others across both setups, offering guidance for future robotic hand design.</div>
<div class="mono" style="margin-top:8px">本研究探讨了机器人手上不同触觉传感器密度和布局对抓握学习效率的影响，动机在于优化传感器布置以提升如假肢等应用的抓握稳定性。方法基于仿真比较评估了六种不同的触觉传感器配置，采用双设置系统以确保在不同物理模拟器、手部模型和强化学习算法下的鲁棒性。关键实验结果表明了设置特定和跨设置的通用性能效应，其中一种配置在两种设置中均表现最佳，为未来机器人手设计提供了指导。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory</div>
<div class="meta-line">Authors: Johann Licher, Max Bartholdt, Henrik Krauss, Tim-Lukas Habich, Thomas Seel, Moritz Schappler</div>
<div class="meta-line">First: 2025-08-18T07:24:36+00:00 · Latest: 2026-01-15T09:58:58+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Robotics, 20 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12681v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12681v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic control of soft continuum robots (SCRs) holds great potential for expanding their applications, but remains a challenging problem due to the high computational demands of accurate dynamic models. While data-driven approaches like Koopman-operator-based methods have been proposed, they typically lack adaptability and cannot reconstruct the full robot shape, limiting their applicability. This work introduces a real-time-capable nonlinear model-predictive control (MPC) framework for SCRs based on a domain-decoupled physics-informed neural network (DD-PINN) with adaptable bending stiffness. The DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a speed-up factor of 44000. It is also used within an unscented Kalman filter for estimating the model states and bending compliance from end-effector position measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories and setpoint control with end-effector position errors below 3 mm (2.3% of the actuator&#x27;s length). In real-world experiments, the controller achieves similar accuracy and accelerations up to 3.55 m/s2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Cosserat杆理论的物理信息神经网络实现软体连续体机器人自适应模型预测控制</div>
<div class="mono" style="margin-top:8px">软体连续体机器人的动态控制具有广阔应用前景，但精确动力学模型的高计算需求使其成为难题。现有数据驱动方法（如基于Koopman算子的方法）通常缺乏适应性且无法重构完整机器人形态。本研究提出一种基于可调弯曲刚度域解耦物理信息神经网络的实时非线性模型预测控制框架，该神经网络作为动态Cosserat杆模型的替代模型，速度提升达44000倍，并集成无迹卡尔曼滤波器通过末端执行器位置测量估计模型状态与弯曲柔度。在GPU上以70Hz运行的进化非线性模型预测控制器，在仿真中实现动态轨迹精确跟踪（末端位置误差小于3毫米，占执行器长度2.3%），实物实验达到同等精度及3.55m/s²的加速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of dynamic control for soft continuum robots, where accurate modeling is computationally expensive and existing data-driven methods lack adaptability and full shape reconstruction. The authors propose a real-time nonlinear model-predictive control framework using a domain-decoupled physics-informed neural network as a fast surrogate for the Cosserat rod dynamic model, achieving a 44000x speed-up, and integrate it with an unscented Kalman filter for state and stiffness estimation. Experimental results show the controller operates at 70 Hz, achieving end-effector position errors below 3 mm (2.3% of actuator length) in simulation and similar accuracy with accelerations up to 3.55 m/s² in real-world tests.</div>
<div class="mono" style="margin-top:8px">本研究针对软体连续机器人的动态控制难题，其精确模型计算成本高，现有数据驱动方法常缺乏适应性和完整形状重建能力。作者提出了一种实时非线性模型预测控制框架，采用域解耦物理信息神经网络作为动态Cosserat杆模型的快速替代，实现了44000倍的加速，并结合无迹卡尔曼滤波器进行状态和参数估计。实验结果表明，该控制器以70 Hz频率运行，在仿真中实现了末端执行器位置误差低于3毫米的精确轨迹跟踪，在实际测试中也达到了相似精度，加速度高达3.55 m/s²。</div>
</details>
</div>
<div class="card">
<div class="title">Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control</div>
<div class="meta-line">Authors: Yifan Xue, Ze Zhang, Knut Åkesson, Nadia Figueroa</div>
<div class="meta-line">First: 2026-01-15T09:46:03+00:00 · Latest: 2026-01-15T09:46:03+00:00</div>
<div class="meta-line">Comments: Co-first authors: Yifan Xue and Ze Zhang</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动无局部极小值机器人导航：融合运动预测与安全控制</div>
<div class="mono" style="margin-top:8px">本研究致力于解决在具有凹形移动障碍物的复杂动态环境中实现移动机器人安全高效导航的挑战。基于控制屏障函数（CBFs）等反应式安全控制器仅依据障碍物当前状态设计避障策略，存在未来碰撞风险。为缓解此问题，我们采用高斯过程在线学习基于能量学习训练的神经网络生成的多模态障碍物运动预测的屏障函数。随后将学习到的屏障函数输入至采用调制控制屏障函数（MCBFs）——一种无局部极小值的CBF变体——的二次规划中，以实现安全高效导航。该框架作出两项关键贡献：其一，开发了从预测到屏障函数的在线学习流程；其二，提出了自主参数调优算法，使MCBFs能自适应基于预测的可变形屏障函数。通过仿真与实物实验验证，该框架在拥挤动态环境中持续超越基线方法，展现出卓越的安全性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research tackles the challenge of safe and efficient robot navigation among moving obstacles, where purely reactive controllers risk collisions due to a lack of foresight. The method integrates neural network-based multimodal motion predictions of obstacles with online learning of barrier functions via Gaussian processes; these learned functions are then used within a modulated Control Barrier Function (MCBF) framework to formulate a local-minima-free safe controller. Experimental evaluations in simulations and real-world tests show the framework consistently outperforms baseline methods, achieving superior safety and navigation efficiency in crowded, dynamic settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决移动机器人在动态复杂环境中安全高效导航的挑战，其中纯反应式控制器因缺乏前瞻性而存在未来碰撞风险。该方法将基于神经网络的障碍物运动预测与通过高斯过程在线学习屏障函数相结合；然后将学习到的函数用于调制控制屏障函数（MCBF）框架中，以构建一个无局部最优的安全控制器。仿真和真实世界实验结果表明，该框架在拥挤动态环境中持续优于基线方法，实现了更高的安全性和导航效率。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Framework for Kinematic Simulation of Rigid Foldable Structures</div>
<div class="meta-line">Authors: Dongwook Kwak, Geonhee Cho, Jiook Chung, Jinkyu Yang</div>
<div class="meta-line">First: 2026-01-15T09:38:42+00:00 · Latest: 2026-01-15T09:38:42+00:00</div>
<div class="meta-line">Comments: 34 pages (20 pages main text), 11 figures (7 in main text, 4 in appendix)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10225v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Origami-inspired structures with rigid panels now span thick, kirigami, and multi-sheet realizations, making unified kinematic analysis essential. Yet a general method that consolidates their loop constraints has been lacking. We present an automated approach that generates the Pfaffian constraint matrix for arbitrary rigid foldable structures (RFS). From a minimally extended data schema, the tool constructs the facet-hinge graph, extracts a minimum cycle basis that captures all constraints, and assembles a velocity-level constraint matrix via screw theory that encodes coupled rotation and translation loop closure. The framework computes and visualizes deploy and fold motions across diverse RFS while eliminating tedious and error-prone constraint calculations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>刚性可折叠结构运动学模拟的统一框架</div>
<div class="mono" style="margin-top:8px">受折纸启发的刚性面板结构现已涵盖厚板、剪纸及多片层实现形式，使得统一运动学分析至关重要。然而，整合其环路约束的通用方法长期缺失。本文提出一种自动化方法，可为任意刚性可折叠结构生成普法夫约束矩阵。该工具基于最小化扩展数据架构，构建面-铰链图，提取捕获所有约束的最小环基，并通过旋量理论组装编码耦合旋转与平移环路闭合的速度级约束矩阵。该框架能计算并可视化各类刚性可折叠结构的展开与折叠运动，同时消除了繁琐易错的约束计算过程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a unified kinematic analysis method for diverse rigid foldable structures, including thick, kirigami, and multi-sheet realizations, where a general approach to consolidate loop constraints has been lacking. The method introduces an automated framework that generates a Pfaffian constraint matrix for arbitrary structures by constructing a facet-hinge graph from a minimal data schema, extracting a minimum cycle basis to capture all constraints, and assembling a velocity-level constraint matrix via screw theory to encode coupled rotation and translation loop closure. Key experimental results demonstrate that the framework successfully computes and visualizes deployment and folding motions across a variety of rigid foldable structures, effectively eliminating tedious and error-prone manual constraint calculations.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多样化刚性可折叠结构（包括厚板、剪纸和多层折纸启发设计）对统一运动学分析方法的需求，此前缺乏一种整合其环路约束的通用方法。所提出的方法引入了一个自动化框架，通过从最小扩展数据模式构建面-铰链图，提取捕获所有约束的最小环基，并最终借助螺旋理论组装速度级约束矩阵以编码耦合的旋转和平移环路闭合，从而为任意刚性可折叠结构生成普法夫约束矩阵。该框架成功计算并可视化多种刚性可折叠结构的展开和折叠运动，有效消除了通常所需的繁琐且易出错的手动约束计算。</div>
</details>
</div>
<div class="card">
<div class="title">Terrain-Adaptive Mobile 3D Printing with Hierarchical Control</div>
<div class="meta-line">Authors: Shuangshan Nors Li, J. Nathan Kutz</div>
<div class="meta-line">First: 2026-01-15T09:15:06+00:00 · Latest: 2026-01-15T09:15:06+00:00</div>
<div class="meta-line">Comments: Submitted to the 43rd International Symposium on Automation and Robotics in Construction (ISARC 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分层控制的地形自适应移动式3D打印技术</div>
<div class="mono" style="margin-top:8px">非结构化地形上的移动3D打印因平台移动性与沉积精度间的矛盾而面临挑战。现有龙门式系统精度高但缺乏移动性，而移动平台在崎岖地面上难以保持打印质量。本研究提出一种框架，将AI驱动的扰动预测与多模态传感器融合及分层硬件控制紧密结合，形成感知-学习-执行的闭环系统。AI模块通过IMU、视觉与深度传感器学习地形至扰动的映射关系，实现主动补偿而非被动校正。该智能系统嵌入三层控制架构：路径规划、预测性底盘-机械臂协调控制及精密硬件执行。通过在斜坡与不规则表面的户外实验，系统在保持平台全移动性的同时实现了亚厘米级打印精度。这种AI与硬件的集成为非结构化环境下的自主施工奠定了实践基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mobile 3D printing on unstructured terrain is difficult because high mobility often compromises deposition precision. To address this, the authors propose a closed-loop system that integrates AI-driven disturbance prediction from multi-modal sensors (IMU, vision, depth) with a three-layer hierarchical control architecture for path planning, chassis-manipulator coordination, and hardware execution. Experimental results from outdoor tests on sloped and irregular terrain demonstrate that the framework achieves sub-centimeter printing accuracy while preserving full platform mobility.</div>
<div class="mono" style="margin-top:8px">在非结构化地形上进行移动3D打印面临平台移动性与沉积精度之间的冲突，龙门架系统缺乏移动性而移动平台在崎岖地面上难以保持打印质量。为此，本研究提出一个闭环系统，将基于IMU、视觉和深度传感器融合的AI驱动扰动预测，与用于路径规划、预测性底盘-机械臂协调和精密执行的三层分层控制架构相结合。在具有坡度和表面不规则地形的户外实验中，该框架在保持平台完全移动性的同时，实现了亚厘米级的打印精度。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Querying for Reward Learning from Human Feedback</div>
<div class="meta-line">Authors: Yashwanthi Anand, Nnamdi Nwagwu, Kevin Sabbe, Naomi T. Fitter, Sandhya Saisubramanian</div>
<div class="meta-line">First: 2024-12-11T00:02:48+00:00 · Latest: 2026-01-15T09:01:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.07990v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.07990v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from human feedback is a popular approach to train robots to adapt to user preferences and improve safety. Existing approaches typically consider a single querying (interaction) format when seeking human feedback and do not leverage multiple modes of user interaction with a robot. We examine how to learn a penalty function associated with unsafe behaviors using multiple forms of human feedback, by optimizing both the query state and feedback format. Our proposed adaptive feedback selection is an iterative, two-phase approach which first selects critical states for querying, and then uses information gain to select a feedback format for querying across the sampled critical states. The feedback format selection also accounts for the cost and probability of receiving feedback in a certain format. Our experiments in simulation demonstrate the sample efficiency of our approach in learning to avoid undesirable behaviors. The results of our user study with a physical robot highlight the practicality and effectiveness of adaptive feedback selection in seeking informative, user-aligned feedback that accelerate learning. Experiment videos, code and appendices are found on our website: https://tinyurl.com/AFS-learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应查询的人机反馈奖励学习</div>
<div class="mono" style="margin-top:8px">从人类反馈中学习是训练机器人适应用户偏好并提升安全性的常用方法。现有方法在获取人类反馈时通常仅考虑单一查询（交互）形式，未能充分利用用户与机器人的多种交互模式。本研究探讨如何通过优化查询状态与反馈形式，利用多种人类反馈学习与不安全行为相关的惩罚函数。我们提出的自适应反馈选择是一种迭代式两阶段方法：首先选择关键状态进行查询，随后基于信息增益在采样的关键状态中选择反馈形式。反馈形式的选择同时考虑了特定形式反馈的获取成本与概率。仿真实验表明，该方法在学习避免不良行为时具有较高的样本效率。通过实体机器人开展的用户研究结果进一步验证了自适应反馈选择在获取信息丰富、符合用户意图的反馈以加速学习方面的实用性与有效性。实验视频、代码及附录详见项目网站：https://tinyurl.com/AFS-learning。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing methods for learning from human feedback often rely on a single interaction format, limiting their ability to leverage diverse user input. To address this, the authors propose an adaptive feedback selection method that iteratively chooses critical states for querying and then selects the most informative feedback format (e.g., binary comparisons or corrections) based on expected information gain, while also considering query cost and user response likelihood. Experimental simulations and a user study with a physical robot demonstrate that this approach learns to avoid undesirable behaviors more sample-efficiently and obtains more informative, user-aligned feedback compared to non-adaptive baselines.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于提高从人类反馈中学习的样本效率和用户对齐性，因为现有方法通常依赖单一查询格式，未能利用多种交互模式。所提出的自适应反馈选择方法是一种迭代的两阶段方法：首先选择关键状态进行查询，然后基于信息增益选择反馈格式，同时考虑反馈成本和概率。仿真实验结果表明，该方法在学习避免不良行为方面提高了样本效率；而通过物理机器人进行的用户研究证实了其在实际应用中的有效性和实用性，能够获取信息丰富、符合用户偏好的反馈，从而加速学习过程。</div>
</details>
</div>
<div class="card">
<div class="title">RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Yue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie</div>
<div class="meta-line">First: 2026-01-15T08:15:01+00:00 · Latest: 2026-01-15T08:15:01+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10168v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10168v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAG-3DSG：通过重拍引导的检索增强生成优化三维场景图</div>
<div class="mono" style="margin-top:8px">开放词汇三维场景图生成可通过结构化语义表征提升机器人操作、导航等下游任务性能。三维场景图由场景多视角图像构建，以节点表示对象、边表示关系。现有方法因视角受限、遮挡及表面冗余密度等问题，存在对象识别准确率低、速度慢的缺陷。为此，我们提出RAG-3DSG：通过重拍引导的不确定性估计降低聚合噪声，并基于可靠低不确定性对象实现对象级检索增强生成。同时提出动态降采样映射策略，以自适应粒度加速跨图像对象聚合。Replica数据集实验表明，RAG-3DSG在显著提升三维场景图节点标注准确率的同时，较原始版本将建图时间缩短三分之二。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve open-vocabulary 3D Scene Graph (3DSG) generation for robotics applications, as existing methods suffer from low accuracy and speed due to limited viewpoints, occlusions, and redundant data. The proposed RAG-3DSG method introduces a re-shot guided uncertainty estimation to reduce aggregation noise and employs object-level Retrieval-Augmented Generation (RAG) using reliable low-uncertainty objects, alongside a dynamic downsample-mapping strategy for faster cross-image object aggregation. Experimental results on the Replica dataset show that this approach significantly enhances node captioning accuracy and reduces mapping time by two-thirds compared to the baseline.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进面向机器人应用的开放词汇3D场景图生成，因为现有方法因视角受限、遮挡和数据冗余导致准确性和速度较低。提出的RAG-3DSG方法通过重拍引导的不确定性估计来减少聚合噪声，并利用可靠的低不确定性对象进行对象级检索增强生成，同时采用动态下采样映射策略以加速跨图像对象聚合。在Replica数据集上的实验结果表明，该方法显著提高了节点标注准确性，并将建图时间相比基线版本减少了三分之二。</div>
</details>
</div>
<div class="card">
<div class="title">CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments</div>
<div class="meta-line">Authors: Xintong Zhang, Junfeng Chen, Yuxiao Zhu, Bing Luo, Meng Guo</div>
<div class="meta-line">First: 2026-01-15T06:50:21+00:00 · Latest: 2026-01-15T06:50:21+00:00</div>
<div class="meta-line">Comments: 8 pages, 8 figures, published to RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10116v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10116v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoCoPlan：动态未知环境下多机器人系统的自适应协同与通信</div>
<div class="mono" style="margin-top:8px">多机器人系统通过协调协作可显著提升效率，但在实际应用中，全时通信往往难以实现，交互通常仅限于近距离通信。现有方法或维持全时连接、依赖固定调度，或采用成对协议，但均无法在有限通信条件下有效适应动态时空任务分布，导致协同效率欠佳。为此，我们提出CoCoPlan——一个协同优化协作任务规划与团队间歇通信的统一框架。该方法集成了联合编码任务分配与通信事件的分支定界架构、平衡任务效率与通信延迟的自适应目标函数，以及策略性决定何时、何地及如何重建全局连接的通信事件优化模块。大量实验表明，该方法在动态环境中任务完成率提升22.4%，通信开销降低58.6%，并通过支持多达100台机器人显著提升了可扩展性，性能优于现有先进方法。硬件实验涵盖复杂二维办公环境与大规模三维灾害响应场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-robot systems often operate with limited, intermittent communication, but existing coordination methods fail to adapt effectively to dynamic task distributions under such constraints. To solve this, CoCoPlan introduces a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication through a branch-and-bound architecture, an adaptive objective balancing task efficiency and communication latency, and a module for strategically scheduling global connectivity events. Experiments show the method achieves a 22.4% higher task completion rate, reduces communication overhead by 58.6%, and scales to support up to 100 robots in dynamic environments, as validated in 2D office and 3D disaster-response scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决动态未知环境中多机器人系统的协调问题，这些环境中无法维持持续通信且交互仅限于近距离。提出的CoCoPlan框架通过联合优化协作任务规划和团队间歇通信，采用了一种分支定界架构来共同编码任务分配与通信事件、一个平衡任务效率与延迟的自适应目标函数，以及一个战略性重建全局连接的模块。实验结果表明，CoCoPlan优于现有先进方法，任务完成率提高了22.4%，通信开销降低了58.6%，并能在动态环境中扩展至支持多达100个机器人，这些在2D办公室和3D灾难响应场景中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jean Oh, Jihie Kim</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-15T05:09:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全未找到（404）：基于大语言模型的机器人决策隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键场景中，AI系统的一次失误可能危及生命。随着大语言模型（LLMs）成为机器人决策的核心组成部分，风险的物理维度随之扩大；一条错误指令即可直接威胁人身安全。本文针对在微小错误亦可能引发灾难的场景中系统评估LLM性能的迫切需求展开研究。通过对火灾疏散场景的定性评估，我们识别出基于LLM决策的关键失效案例。基于此，我们设计了七项定量评估任务，分为三类：完整信息任务、不完整信息任务、安全导向空间推理（SOSR）任务。完整信息任务采用ASCII地图以最小化解释歧义，将空间推理与视觉处理分离。不完整信息任务要求模型推断缺失上下文，测试空间连续性与幻觉生成。SOSR任务使用自然语言评估危及生命情境下的安全决策能力。我们在此框架下对多种LLM和视觉语言模型（VLMs）进行基准测试。除整体性能外，我们深入分析了1%故障率的影响，揭示“罕见”错误如何升级为灾难性后果。结果暴露出严重漏洞：多个模型在ASCII导航任务中成功率为0%；在模拟消防演练中，模型竟指示机器人朝向危险区域而非紧急出口移动。研究得出警示性结论：当前LLM尚未具备直接部署于安全关键系统的条件。在机器人领域，99%的准确率具有危险的误导性——这意味着每百次执行就可能发生一次灾难性伤害。我们证明即使最先进的模型也无法保证安全，对其绝对依赖将产生不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the severe risks posed by even minor errors when Large Language Models (LLMs) are used for robotics decision-making in safety-critical contexts like fire evacuations. The method involves a qualitative evaluation followed by a quantitative assessment across seven tasks categorized into Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR), benchmarking various LLMs and Vision-Language Models (VLMs). Key experimental findings reveal critical vulnerabilities, including models achieving 0% success in ASCII navigation tasks and, in a simulated fire drill, instructing robots to move toward hazards instead of exits, demonstrating that a 99% accuracy rate is dangerously insufficient as it implies catastrophic failure in one out of a hundred executions.</div>
<div class="mono" style="margin-top:8px">本研究动机源于将大语言模型（LLM）集成到机器人技术中所带来的严重安全风险，其中单个错误决策可能导致灾难性的物理伤害。研究方法采用系统性评估框架，首先通过对火灾疏散场景的定性分析识别故障模式，随后设计了七项定量任务，分为完全信息、不完全信息和安全导向空间推理（SOSR）三类，使用ASCII地图和自然语言测试空间推理、上下文推断和安全关键决策能力。主要实验结果表明存在严重漏洞：多个模型在ASCII导航任务中完全失败（成功率为0%），在模拟消防演练中，模型错误地引导机器人走向危险区域而非紧急出口，这证明即使最先进的模型也无法保证安全，且99%的准确率掩盖了灾难性故障的不可接受风险。</div>
</details>
</div>
<div class="card">
<div class="title">CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications</div>
<div class="meta-line">Authors: Hojung Choi, Jun En Low, Tae Myung Huh, Seongheon Hong, Gabriela A. Uribe, Kenneth A. W. Hoffmann, Julia Di, Tony G. Chen, Andrew A. Stanley, Mark R. Cutkosky</div>
<div class="meta-line">First: 2025-03-25T00:12:48+00:00 · Latest: 2026-01-15T04:10:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.19225v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.19225v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://coin-ft.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is compact, light, low-cost, and robust with an average root-mean-squared error of 0.16N for force and 1.08mNm for moment when the input ranges from 0~14N and 0~5N in normal and shear directions, respectively. CoinFT is a stack of two rigid PCBs with comb-shaped electrodes connected by an array of silicone rubber pillars. The microcontroller interrogates the electrodes in different subsets in order to enhance sensitivity for measuring 6-axis F/T. The combination of features of CoinFT enables various contact-rich robot interactions across different embodiment domains including drones, robot end-effectors, and wearable haptic devices. We demonstrate the utility of CoinFT through two representative applications: a multi-axial contact-probing experiment in which a CoinFT mounted beneath a hemispherical fingertip measures 6-axes of force and torque representative of manipulation scenarios, and an attitude-based force-control task on a drone. The design, fabrication, and firmware of CoinFT are open-sourced at https://coin-ft.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoinFT：一种用于机器人应用的硬币尺寸电容式六维力扭矩传感器</div>
<div class="mono" style="margin-top:8px">我们推出CoinFT，一种紧凑、轻量、低成本且坚固的电容式六维力/扭矩传感器，在法向和剪切方向输入范围分别为0~14N和0~5N时，其平均均方根误差为力0.16N、力矩1.08mNm。CoinFT由两片刚性PCB堆叠构成，通过硅橡胶柱阵列连接梳状电极。微控制器通过分组合询问电极以提升六维力/扭矩测量的灵敏度。CoinFT的特性组合使其能广泛应用于无人机、机器人末端执行器和可穿戴触觉设备等不同形态领域的密集接触式机器人交互。我们通过两个典型应用展示其效用：半球形指尖下安装的CoinFT测量代表操控场景的六维力扭矩的多轴接触探测实验，以及基于姿态的无人机力控任务。CoinFT的设计、制造和固件已在https://coin-ft.github.io/开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable compact, low-cost, and robust six-axis force/torque sensing for diverse robotic applications, this work introduces CoinFT, a coin-sized capacitive sensor. The method employs a stack of two rigid PCBs with comb-shaped electrodes connected by silicone rubber pillars; a microcontroller interrogates electrode subsets to enhance sensitivity for measuring forces and torques. Experimental results show the sensor achieves an average root-mean-squared error of 0.16 N for force and 1.08 mNm for moment over specified ranges, and its utility is demonstrated in contact-probing with a robotic fingertip and in attitude-based force control on a drone.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种紧凑、低成本且坚固的六维力/力矩传感器，以增强机器人在接触丰富场景中的交互能力。方法采用电容式传感设计，其中两个带有梳状电极的刚性PCB通过硅橡胶柱连接，微控制器通过轮询电极子集来提高六维测量的灵敏度。实验结果表明，传感器在指定测量范围内实现了平均均方根误差为0.16 N的力和1.08 mNm的力矩，其应用价值在机器人指尖的多轴接触探测和无人机基于姿态的力控制等任务中得到验证。</div>
</details>
</div>
<div class="card">
<div class="title">UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow</div>
<div class="meta-line">Authors: Nick Truong, Pritam P. Karmokar, William J. Beksi</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2026-01-15T04:10:14+00:00 · Latest: 2026-01-15T04:10:14+00:00</div>
<div class="meta-line">Comments: To be presented at the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshop on Event-Based Vision in the Era of Generative AI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10054v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robotic-vision-lab.github.io/ueof">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UEOF：水下事件光流基准数据集</div>
<div class="mono" style="margin-top:8px">水下成像因波长依赖性光衰减、悬浮颗粒强散射、浑浊导致的模糊及非均匀照明而极具挑战，这些效应不仅损害标准相机性能，也使真实运动数据难以获取。相比之下，事件相机具备微秒级分辨率与高动态范围优势。然而，由于缺乏结合真实水下光学特性与精确光流的数据集，事件相机在水下环境的研究进展受限。为此，我们首次提出基于物理光线追踪RGBD序列生成的合成水下事件光流基准数据集。通过将现代视频-事件转换流程应用于渲染的水下视频，我们生成了包含密集真实光流、深度及相机运动的逼真事件数据流。此外，我们评估了前沿的基于学习与模型的光流预测方法，以探究水下光传输如何影响事件形成与运动估计精度。本数据集为未来水下事件感知算法的开发与评估建立了新基准。项目源代码与数据集已公开于https://robotic-vision-lab.github.io/ueof。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Underwater imaging faces challenges like light attenuation and scattering, which hinder standard cameras and make ground-truth motion capture difficult, while event cameras offer high temporal resolution and dynamic range but lack suitable datasets. To address this, the authors introduce UEOF, a synthetic benchmark dataset for underwater event-based optical flow, generated by applying a video-to-event pipeline to physically-based ray-traced RGBD sequences, providing realistic event streams with dense ground-truth flow, depth, and camera motion. Experimental benchmarking of state-of-the-art optical flow methods reveals how underwater light transport impacts event formation and motion estimation accuracy, establishing a new baseline for developing underwater event-based perception algorithms.</div>
<div class="mono" style="margin-top:8px">水下成像因光衰减、散射和浑浊度而严重退化，这损害了标准相机的性能，并使获取真实运动数据变得困难。为促进事件相机在水下感知中的研究，本研究提出了首个用于水下事件光流的合成基准数据集UEOF，该数据集通过对基于物理光线追踪的水下RGBD序列应用视频到事件的转换流程，生成了具有密集真实光流、深度和相机运动的逼真事件流。在此数据集上对最先进的基于学习和基于模型的光流方法进行基准测试，揭示了水下光传输如何影响事件形成和运动估计精度，从而为开发和评估水下事件感知算法建立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">In-the-Wild Compliant Manipulation with UMI-FT</div>
<div class="meta-line">Authors: Hojung Choi, Yifan Hou, Chuer Pan, Seongheon Hong, Austin Patel, Xiaomeng Xu, Mark R. Cutkosky, Shuran Song</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-01-15T02:00:03+00:00 · Latest: 2026-01-15T02:00:03+00:00</div>
<div class="meta-line">Comments: submitted to ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09988v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://umi-ft.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors on each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. In evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:https://umi-ft.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于UMI-FT的野外环境顺应性操控</div>
<div class="mono" style="margin-top:8px">多数操控任务需精确的力调节：力不足会导致任务失败，力过大则可能造成损坏。商用六维力/力矩传感器因成本高、体积大、易损坏，限制了大规模力感知策略学习。本文提出UMI-FT手持数据采集平台，在每根手指搭载紧凑型六维力/力矩传感器，可同步获取手指级力/力矩数据与RGB、深度及位姿信息。利用该设备采集的多模态数据，我们训练出能预测位置目标、抓握力及刚度的自适应顺应策略，可在标准顺应控制器上执行。在三种高接触、力敏感任务（白板擦拭、西葫芦穿刺、灯泡安装）的评估中，UMI-FT策略能可靠调节外部接触力与内部抓握力，性能优于无顺应性或力感知的基线方法。UMI-FT为从野外演示中学习顺应性操控提供了可扩展路径。我们开源硬件与软件以促进广泛应用：https://umi-ft.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenges of learning force-aware manipulation policies due to the high cost and fragility of commercial force/torque sensors, this research introduces UMI-FT, a handheld data-collection platform equipped with compact six-axis force/torque sensors on each finger. The method involves collecting multimodal data including finger-level wrench measurements, RGB, depth, and pose, which is then used to train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. Experimental results on three contact-rich tasks—whiteboard wiping, skewering zucchini, and lightbulb insertion—demonstrate that policies enabled by UMI-FT reliably regulate both external contact and internal grasp forces, outperforming baselines lacking compliance or force sensing.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决学习力敏感操作策略的挑战，即力不足会导致任务失败，而过大的力会造成损坏，且商用六维力传感器成本高、易损坏，难以用于大规模数据采集。方法上提出了UMI-FT，这是一个手持式数据采集平台，在每个手指上安装了紧凑的六维力传感器，可捕获手指级的力/力矩测量数据以及RGB、深度和位姿信息；利用这些多模态数据训练了一个自适应柔顺策略，该策略可预测位置目标、抓握力和刚度，以便在标准柔顺控制器上执行。在三个接触密集的任务（白板擦拭、串西葫芦和灯泡安装）上的实验结果表明，使用UMI-FT训练的策略能可靠地调节外部接触力和内部抓握力，其性能优于缺乏柔顺控制或力感知的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Monocular Depth Refinement via Neural Radiance Fields</div>
<div class="meta-line">Authors: Arun Muthukkumar</div>
<div class="meta-line">First: 2026-01-07T12:32:39+00:00 · Latest: 2026-01-15T01:46:55+00:00</div>
<div class="meta-line">Comments: IEEE 8th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03869v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03869v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate improvements on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经辐射场的贝叶斯单目深度优化方法</div>
<div class="mono" style="margin-top:8px">单目深度估计在自动驾驶导航与扩展现实等领域具有广泛应用，是计算机视觉中的关键任务。然而，现有方法常生成过度平滑的深度图，缺乏精确场景理解所需的精细几何细节。本文提出MDENeRF迭代框架，利用神经辐射场（NeRF）的深度信息优化单目深度估计。该框架包含三个组件：（1）用于全局结构的初始单目估计；（2）基于扰动视角训练、具有逐像素不确定性的NeRF模型；（3）对含噪声的单目深度与NeRF深度进行贝叶斯融合。我们通过体渲染过程推导NeRF不确定性，以迭代方式注入高频细节，同时通过单目先验保持全局结构。在SUN RGB-D数据集室内场景上的实验表明，该方法在关键指标上均有提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Monocular depth estimation often yields overly smooth results lacking fine geometric details crucial for applications like autonomous navigation. To address this, the authors propose MDENeRF, an iterative framework that refines initial monocular depth estimates by integrating depth information from Neural Radiance Fields (NeRFs). The method combines three components: an initial monocular estimate for global structure, a NeRF trained on perturbed viewpoints with per-pixel uncertainty derived from volume rendering, and Bayesian fusion to merge the noisy sources. Experimental results on the SUN RGB-D indoor dataset demonstrate measurable improvements in key depth estimation metrics, successfully injecting high-frequency details while preserving global scene structure.</div>
<div class="mono" style="margin-top:8px">单目深度估计在自动驾驶导航等领域至关重要，但现有方法常产生过于平滑、缺乏精细几何细节的深度图。为此，研究者提出了MDENeRF，一种迭代框架，通过整合来自扰动视角训练的神经辐射场（NeRF）的深度信息，并利用从体渲染过程中推导的逐像素不确定性进行贝叶斯融合，来优化单目深度估计。在SUN RGB-D数据集的室内场景上的实验表明，该方法能有效注入高频细节，同时保持全局结构，从而在关键深度估计指标上取得了可量化的提升。</div>
</details>
</div>
<div class="card">
<div class="title">OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport</div>
<div class="meta-line">Authors: Zhihua Zhao, Guoqiang Li, Chen Min, Kangping Lu</div>
<div class="meta-line">First: 2026-01-15T00:23:45+00:00 · Latest: 2026-01-15T00:23:45+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures, 6 tables. This work has been submitted to the IEEE for possible publication. Code will be released upon acceptance</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09952v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09952v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OT-Drive：基于最优传输的分布外越野可通行区域分割方法</div>
<div class="mono" style="margin-top:8px">在非结构化环境中实现可靠的可通行区域分割对自动驾驶的规划与决策至关重要。然而，现有数据驱动方法在分布外场景下常出现分割性能下降，进而影响下游驾驶任务。为此，我们提出OT-Drive——一种基于最优传输的多模态融合框架。该方法将RGB与表面法线融合建模为分布传输问题：首先设计场景锚点生成器，将场景信息解耦为天气、时段与道路类型的联合分布，构建可泛化至未见场景的语义锚点；继而设计基于最优传输的多模态融合模块，将RGB与表面法线特征映射至语义锚点定义的流形空间，实现分布外场景下的鲁棒可通行区域分割。实验表明，本方法在ORFD分布外场景达到95.16% mIoU（较现有方法提升6.35%），在跨数据集迁移任务达到89.79% mIoU（较基线提升13.99%）。结果表明，仅需有限训练数据即可实现强分布外泛化能力，显著提升实际部署的实用性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance the reliability of autonomous driving in unstructured environments, this research addresses the performance degradation of data-driven traversable area segmentation in out-of-distribution (OOD) scenarios. The proposed OT-Drive framework formulates multi-modal fusion as an optimal transport problem, introducing a Scene Anchor Generator to decompose scene information into semantic anchors for generalization and an Optimal Transport-based fusion module to align RGB and surface normal features with these anchors. Experiments show the method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset tasks, surpassing baselines by 13.99%, demonstrating strong OOD generalization with limited training data.</div>
<div class="mono" style="margin-top:8px">为解决数据驱动的可通行区域分割在分布外（OOD）越野场景中性能下降、影响自动驾驶的问题，本研究提出了OT-Drive，一种基于最优传输的多模态融合框架。该方法将RGB和表面法线融合表述为分布传输问题，通过场景锚点生成器分解场景信息为可泛化的语义锚点，并利用最优传输融合模块将特征与锚点对齐。实验结果表明，该模型在ORFD OOD场景上达到95.16%的mIoU，优于先前方法6.35%，在跨数据集任务上达到89.79%的mIoU，超越基线13.99%，证明了其在有限训练数据下具备强大的OOD泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">A Taxonomy for Evaluating Generalist Robot Manipulation Policies</div>
<div class="meta-line">Authors: Jensen Gao, Suneel Belkhale, Sudeep Dasari, Ashwin Balakrishna, Dhruv Shah, Dorsa Sadigh</div>
<div class="meta-line">First: 2025-03-03T07:03:00+00:00 · Latest: 2026-01-15T00:06:44+00:00</div>
<div class="meta-line">Comments: IEEE Robotics and Automation Letters (RA-L)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.01238v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.01238v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning for robot manipulation promises to unlock generalization to novel tasks and environments. But how should we measure the progress of these policies towards generalization? Evaluating and quantifying generalization is the Wild West of modern robotics, with each work proposing and measuring different types of generalization in their own, often difficult to reproduce settings. In this work, our goal is (1) to outline the forms of generalization we believe are important for robot manipulation in a comprehensive and fine-grained manner, and (2) to provide reproducible guidelines for measuring these notions of generalization. We first propose STAR-Gen, a taxonomy of generalization for robot manipulation structured around visual, semantic, and behavioral generalization. Next, we instantiate STAR-Gen with two case studies on real-world benchmarking: one based on open-source models and the Bridge V2 dataset, and another based on the bimanual ALOHA 2 platform that covers more dexterous and longer horizon tasks. Our case studies reveal many interesting insights: for example, we observe that open-source vision-language-action models often struggle with semantic generalization, despite pre-training on internet-scale language datasets. We provide videos and other supplementary material at our website stargen-taxonomy.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用机器人操作策略评估分类体系</div>
<div class="mono" style="margin-top:8px">机器人操作的机器学习有望实现对新颖任务和环境的泛化能力。但应如何衡量这些策略在泛化方面的进展？评估和量化泛化是现代机器人学的‘蛮荒之地’，每项研究都以各自难以复现的方式提出和测量不同类型的泛化。本研究旨在：（1）以全面细粒度方式阐述机器人操作所需的重要泛化形式；（2）为测量这些泛化概念提供可复现的指导原则。我们首先提出STAR-Gen——围绕视觉、语义和行为泛化构建的机器人操作泛化分类体系。随后通过两个真实世界基准测试案例进行实例化：一个基于开源模型和Bridge V2数据集，另一个基于覆盖更灵巧长周期任务的双臂ALOHA 2平台。案例研究揭示了诸多重要发现：例如，开源视觉-语言-动作模型尽管经过互联网规模语言数据预训练，仍常面临语义泛化挑战。相关视频与补充材料详见网站stargen-taxonomy.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for standardized evaluation metrics to measure the generalization capabilities of robot manipulation policies, which currently lack consistent and reproducible benchmarks. The method introduces STAR-Gen, a taxonomy categorizing generalization into visual, semantic, and behavioral dimensions, and applies it through two real-world case studies: one using open-source models with the Bridge V2 dataset, and another on the bimanual ALOHA 2 platform for dexterous tasks. Key experimental findings reveal that open-source vision-language-action models often underperform in semantic generalization despite extensive pre-training on language data, highlighting specific areas for improvement in policy development.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要标准化评估指标来衡量机器人操作策略的泛化能力，因为现有方法零散且难以复现。作者提出了STAR-Gen分类法，将泛化分为视觉、语义和行为三个维度，并提供了可复现的评估指南。基于Bridge V2数据集和ALOHA 2平台的实验案例研究表明，开源视觉-语言-动作模型尽管经过大规模预训练，但在语义泛化方面仍常表现不佳。</div>
</details>
</div>
<div class="card">
<div class="title">SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping</div>
<div class="meta-line">Authors: Ruopeng Huang, Boyu Yang, Wenlong Gui, Jeremy Morgan, Erdem Biyik, Jiachen Li</div>
<div class="meta-line">First: 2026-01-14T23:03:43+00:00 · Latest: 2026-01-14T23:03:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09920v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SyncTwin：面向安全机器人抓取的快速数字孪生构建与同步框架</div>
<div class="mono" style="margin-top:8px">在动态变化和视觉遮挡条件下的精准安全抓取，仍是现实世界机器人操作的核心挑战。本文提出SyncTwin数字孪生框架，通过融合快速三维场景重建与实景-仿真同步技术，实现此类环境下鲁棒且具备安全感知的抓取。离线阶段采用VGGT从RGB图像快速重建物体级三维资产，构建可复用的仿真几何库。执行期间，SyncTwin通过点云分割更新追踪真实物体状态，并借助彩色ICP配准进行对齐，持续同步数字孪生体。更新后的孪生模型使运动规划器能在仿真中计算无碰撞且动态可行的轨迹，通过实景-仿真-实景闭环在真实机器人上安全执行。动态遮挡场景实验表明，SyncTwin显著提升了抓取精度与运动安全性，验证了数字孪生同步技术对现实机器人执行的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of accurate and safe robotic grasping in dynamic and visually occluded environments, this research introduces SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction with real-to-sim synchronization. The method involves an offline stage using VGGT to rapidly reconstruct object-level 3D assets from RGB images, and an online stage that continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation and colored-ICP registration. Experimental results in dynamic and occluded scenes demonstrate that SyncTwin improves grasp accuracy and motion safety, validating the effectiveness of the synchronized digital twin for real-world robotic execution.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决动态且视觉遮挡的真实环境中实现精准、安全机器人抓取的核心挑战。所提出的SyncTwin方法构建了一个数字孪生框架：离线阶段使用VGGT从RGB图像快速重建物体级3D资产以形成可复用的几何库；执行阶段则通过点云分割更新和彩色ICP配准持续跟踪并同步真实物体状态。在动态和遮挡场景中的实验表明，SyncTwin提高了抓取精度和运动安全性，验证了其数字孪生同步机制对于真实世界机器人执行的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces</div>
<div class="meta-line">Authors: Andrew Stratton, Phani Teja Singamaneni, Pranav Goyal, Rachid Alami, Christoforos Mavrogiannis</div>
<div class="meta-line">First: 2026-01-14T20:34:34+00:00 · Latest: 2026-01-14T20:34:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09856v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09856v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类运动预测质量如何影响社交机器人在受限空间中的导航性能</div>
<div class="mono" style="margin-top:8px">受将移动机器人更紧密地融入仓库、医院、制造工厂及家庭等场景的愿景驱动，本研究聚焦于动态且空间受限环境中的机器人导航。为确保此类环境中的人类安全、舒适与效率，机器人需具备对人类运动模式的建模能力。由于人类行为的随机性、用户偏好差异及数据稀缺性，机器人对人类运动的预测尤为困难。本文系统探究了人类运动预测质量对机器人导航性能、人类工作效率及主观印象的影响。我们设计了一个受限工作空间中机器人穿梭于两名人类受试者间的场景，并通过一项跨两大洲、涉及两种不同机器人平台、涵盖80名参与者的用户研究进行实证。关键发现包括：1）广泛采用的平均位移误差并非机器人导航性能与人类印象的可靠预测指标；2）在受限环境中，人类会主动合作的普遍假设往往不成立，用户常未回应机器人的协作意图，导致性能下降；3）机器人导航效率的提升常以牺牲人类效率与舒适度为代价。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to integrate mobile robots safely and efficiently into dynamic, constrained human environments like warehouses and hospitals, this work systematically investigates how the quality of human motion prediction impacts robot navigation. The method involves designing a constrained workspace scenario where a robot navigates among two human subjects, and instantiating it in a multi-site user study (N=80) with two different robot platforms. Key experimental findings reveal that the standard average displacement error metric poorly predicts navigation performance and human impressions, the assumption of human cooperation often fails in constrained spaces leading to performance degradation, and gains in robot navigation efficiency frequently reduce human efficiency and comfort.</div>
<div class="mono" style="margin-top:8px">本研究旨在将移动机器人安全高效地集成到仓库、医院等动态、受限的人类环境中，因此系统性地探究了人体运动预测质量对机器人导航性能及人类体验的影响。方法上，设计了一个机器人在受限工作空间中与两名人类受试者交互导航的场景，并通过一项在两个地区开展、涉及两种机器人平台、共80名参与者的用户研究，对各种预测模型进行评估。主要实验结果表明：广泛采用的平均位移误差指标并不能可靠预测机器人导航表现和人类主观印象；在受限环境中，常见的人类合作假设往往失效，用户不常回应机器人的合作行为，导致性能下降；机器人导航效率的提升常常以牺牲人类的效率和舒适度为代价。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
