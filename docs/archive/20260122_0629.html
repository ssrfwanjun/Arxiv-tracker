<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 06:29</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0629</div>
    <div class="row"><div class="card">
<div class="title">Q-learning with Adjoint Matching</div>
<div class="meta-line">Authors: Qiyang Li, Sergey Levine</div>
<div class="meta-line">First: 2026-01-20T18:45:34+00:00 · Latest: 2026-01-20T18:45:34+00:00</div>
<div class="meta-line">Comments: 32 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14234v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14234v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic&#x27;s action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伴随匹配Q学习</div>
<div class="mono" style="margin-top:8px">我们提出伴随匹配Q学习（QAM），这是一种基于时序差分的新型强化学习算法，解决了连续动作强化学习中长期存在的挑战：针对参数化Q函数高效优化表达能力强的扩散或流匹配策略。有效优化需要利用评论家的一阶信息，但对流或扩散策略而言，通过多步去噪过程进行基于梯度的反向传播优化存在数值不稳定性。现有方法要么仅使用价值函数而丢弃梯度信息，要么依赖近似方法牺牲策略表达能力或引入偏差。QAM通过运用生成建模中最新提出的伴随匹配技术，将评论家的动作梯度转化为分步目标函数，既避免了不稳定的反向传播，又在最优解处提供无偏且表达能力强的策略。结合时序差分备份进行评论家学习，QAM在离线及离线到在线强化学习的困难稀疏奖励任务中持续超越现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of efficiently optimizing expressive diffusion or flow-matching policies in continuous-action reinforcement learning, where direct gradient-based optimization through multi-step denoising processes is numerically unstable. The proposed method, Q-learning with Adjoint Matching (QAM), leverages adjoint matching to transform the critic&#x27;s action gradient into a step-wise objective, avoiding unstable backpropagation while maintaining an unbiased and expressive policy. Experimental results show that QAM consistently outperforms prior methods on hard, sparse reward tasks in both offline and offline-to-online RL settings.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决连续动作强化学习中高效优化表达性扩散或流匹配策略的挑战，其中通过多步去噪的直接梯度优化存在数值不稳定性。所提出的方法——伴随匹配Q学习（QAM）——利用伴随匹配技术将评论者的动作梯度转换为逐步目标函数，避免了不稳定的反向传播，同时保持了无偏且表达性强的策略。实验结果表明，在离线及离线到在线强化学习的困难稀疏奖励任务上，QAM一致优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</div>
<div class="meta-line">Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-19T17:55:48+00:00 · Latest: 2026-01-20T18:25:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 25 figures. The first four authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17853v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17853v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTask：面向仿真到现实策略学习的自动化任务与数据生成框架</div>
<div class="mono" style="margin-top:8px">通用机器人学习仍受数据制约：大规模、多样化、高质量的交互数据在现实世界中采集成本高昂。虽然仿真已成为扩展数据采集的可行途径，但相关任务——包括仿真任务设计、任务感知场景生成、专家示范合成及仿真到现实迁移——仍需大量人工投入。本文提出AnyTask框架，通过大规模并行GPU仿真与基础模型结合，自动设计多样化操作任务并合成机器人数据。我们引入三种AnyTask智能体以生成能解决尽可能多任务的专家示范：1) ViPR，一种融合视觉语言模型并行优化的新型任务与运动规划智能体；2) ViPR-Eureka，采用生成式密集奖励与LLM引导接触采样的强化学习智能体；3) ViPR-RL，仅依赖稀疏奖励即可联合生成高质量示范的混合规划学习方法。基于生成数据训练行为克隆策略，在仿真中验证后直接部署于真实机器人硬件。策略能泛化至新物体位姿，在真实世界抓放、抽屉开启、密集接触推动及长时程操作任务套件中平均成功率可达44%。项目网站：https://anytask.rai-inst.com</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the data scarcity challenge in generalist robot learning by automating the generation of diverse manipulation tasks and high-quality interaction data in simulation, which traditionally requires extensive human effort. The proposed AnyTask framework integrates massively parallel GPU simulation with foundation models to create tasks and synthesize robot data, featuring three agents: ViPR for task and motion planning with visual-language model refinement, ViPR-Eureka for reinforcement learning with generated dense rewards and large language model-guided contact sampling, and ViPR-RL for hybrid planning and learning with sparse rewards. Experimental results show that behavior cloning policies trained on the generated data achieve a 44% average success rate when deployed directly on real robots, generalizing to novel object poses across pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks.</div>
<div class="mono" style="margin-top:8px">该研究针对通用机器人学习中的数据瓶颈问题，旨在自动化生成模拟环境中的多样化操作任务和高质量交互数据，这些过程传统上需要大量人力投入。方法提出了AnyTask框架，利用大规模并行GPU模拟和基础模型来创建任务并合成机器人数据，包含三个专用智能体：ViPR用于结合视觉语言模型优化的任务与运动规划，ViPR-Eureka用于基于生成密集奖励和大语言模型引导接触采样的强化学习，以及ViPR-RL用于稀疏奖励下的混合规划与学习。实验结果表明，基于生成数据训练的行为克隆策略在直接部署到真实机器人上时，平均成功率达到44%，能够泛化到新物体位姿，涵盖抓放、抽屉开启、接触丰富的推动和长时程操作任务。</div>
</details>
</div>
<div class="card">
<div class="title">TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers</div>
<div class="meta-line">Authors: Bin Yu, Shijie Lian, Xiaopeng Lin, Yuliang Wei, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Xinming Wang, Bailing Wang, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-20T16:30:07+00:00 · Latest: 2026-01-20T16:30:07+00:00</div>
<div class="meta-line">Comments: GitHub: https://github.com/ZGC-EmbodyAI/TwinBrainVLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14133v1">PDF</a> · <a href="https://github.com/ZGC-EmbodyAI/TwinBrainVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to &quot;catastrophic forgetting&quot; of the model&#x27;s open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen &quot;Left Brain&quot;, which retains robust general visual reasoning, with a trainable &quot;Right Brain&quot;, specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TwinBrainVLA：通过非对称混合变换器架构释放通用视觉语言模型在具身任务中的潜力</div>
<div class="mono" style="margin-top:8px">标准视觉-语言-动作模型通常通过微调单一视觉语言模型主干来适配机器人控制任务，但这种方法在保持高层语义理解与学习低层精细感知运动技能之间存在根本矛盾，常导致模型开放世界能力的灾难性遗忘。为化解这一矛盾，我们提出TwinBrainVLA创新架构，通过协调保持通用语义理解的通用VLM与专精具身本体感知的专用VLM，实现协同机器人控制。该架构通过新型非对称混合变换器机制，将保持通用视觉推理能力的冻结“左脑”与专攻具身感知的可训练“右脑”相融合，使右脑能动态查询左脑的语义知识并与本体感知状态结合，为流匹配动作专家生成精确连续控制提供丰富条件。在SimplerEnv和RoboCasa基准测试中的大量实验表明，TwinBrainVLA在保持预训练VLM完整视觉理解能力的同时，其操作性能显著优于现有先进基线，为构建同时具备高层语义理解与低层物理操控能力的通用机器人提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Standard Vision-Language-Action models face a conflict between preserving high-level semantic understanding from pre-training and learning low-level robotic control, often leading to catastrophic forgetting. To resolve this, TwinBrainVLA introduces an asymmetric architecture coordinating a frozen generalist VLM (Left Brain) for semantic reasoning and a trainable specialist VLM (Right Brain) for embodied proprioception via an Asymmetric Mixture-of-Transformers mechanism, which dynamically fuses semantic knowledge with proprioceptive states to condition a Flow-Matching Action Expert for control. Experiments on SimplerEnv and RoboCasa benchmarks show the model achieves superior manipulation performance while explicitly preserving the pre-trained VLM&#x27;s comprehensive visual understanding.</div>
<div class="mono" style="margin-top:8px">标准视觉-语言-动作模型在为机器人控制进行微调时，常导致通用视觉语言理解能力的灾难性遗忘。为解决此问题，TwinBrainVLA提出了一种非对称架构，协调一个冻结的通用VLM（左脑）提供语义知识，和一个可训练的专用VLM（右脑）处理本体感知，通过非对称混合Transformer机制动态融合信息，以驱动流匹配动作专家生成控制。在SimplerEnv和RoboCasa基准测试上的实验表明，该模型在保持预训练VLM全面视觉理解能力的同时，实现了最先进的操控性能。</div>
</details>
</div>
<div class="card">
<div class="title">SandWorm: Event-based Visuotactile Perception with Active Vibration for Screw-Actuated Robot in Granular Media</div>
<div class="meta-line">Authors: Shoujie Li, Changqing Guo, Junhao Gong, Chenxin Liang, Wenhua Ding, Wenbo Ding</div>
<div class="meta-line">First: 2026-01-20T16:25:50+00:00 · Latest: 2026-01-20T16:25:50+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Transactions on Robotics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14128v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Perception in granular media remains challenging due to unpredictable particle dynamics. To address this challenge, we present SandWorm, a biomimetic screw-actuated robot augmented by peristaltic motion to enhance locomotion, and SWTac, a novel event-based visuotactile sensor with an actively vibrated elastomer. The event camera is mechanically decoupled from vibrations by a spring isolation mechanism, enabling high-quality tactile imaging of both dynamic and stationary objects. For algorithm design, we propose an IMU-guided temporal filter to enhance imaging consistency, improving MSNR by 24%. Moreover, we systematically optimize SWTac with vibration parameters, event camera settings and elastomer properties. Motivated by asymmetric edge features, we also implement contact surface estimation by U-Net. Experimental validation demonstrates SWTac&#x27;s 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm demonstrates versatile locomotion (up to 12.5 mm/s) in challenging terrains, successfully executes pipeline dredging and subsurface exploration in complex granular media (observed 90% success rate). Field experiments further confirm the system&#x27;s practical performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SandWorm：基于事件视觉触觉感知与主动振动的螺旋驱动机器人用于颗粒介质作业</div>
<div class="mono" style="margin-top:8px">颗粒介质中的感知因粒子动力学难以预测而极具挑战。为此，我们提出SandWorm——一种通过蠕动运动增强移动性的仿生螺旋驱动机器人，以及SWTac——一种采用主动振动弹性体的新型事件视觉触觉传感器。事件相机通过弹簧隔振机制与振动机械解耦，实现对动态与静态物体的高质量触觉成像。算法设计方面，我们提出IMU引导的时序滤波器以提升成像一致性，使MSNR提高24%。此外，我们系统优化了SWTac的振动参数、事件相机设置与弹性体特性。基于非对称边缘特征，我们还通过U-Net实现了接触面估计。实验验证表明：SWTac具备0.2mm纹理分辨率、98%石块分类准确率与0.15N力估计误差；SandWorm在复杂地形中实现多模式运动（最高12.5mm/s），成功在复杂颗粒介质中完成管道清淤与地下勘探（观测成功率90%）。现场实验进一步证实了该系统的实用性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of perception in granular media, where unpredictable particle dynamics hinder robotic operation. The authors introduce SandWorm, a biomimetic screw-actuated robot with peristaltic motion for enhanced locomotion, and SWTac, a novel event-based visuotactile sensor featuring an actively vibrated elastomer and a spring-isolated event camera to capture high-quality tactile images of both dynamic and stationary objects. Methodologically, they propose an IMU-guided temporal filter to improve imaging consistency by 24% in MSNR, systematically optimize sensor parameters, and implement a U-Net for contact surface estimation. Key experimental results show SWTac achieves a 0.2 mm texture resolution, 98% stone classification accuracy, and 0.15 N force estimation error, while SandWorm attains locomotion speeds up to 12.5 mm/s in challenging terrains and a 90% success rate in pipeline dredging and subsurface exploration tasks within granular media.</div>
<div class="mono" style="margin-top:8px">由于颗粒介质中粒子动力学的不可预测性，感知任务极具挑战。为此，研究者提出了SandWorm——一种采用蠕动运动增强的仿生螺杆驱动机器人，以及SWTac——一种新型事件驱动的视觉触觉传感器，其采用主动振动的弹性体和弹簧隔离的事件相机以实现高质量触觉成像。在方法上，他们提出了一种IMU引导的时间滤波器以提升成像一致性，并系统优化了传感器参数，同时通过U-Net实现了接触面估计。实验验证表明，SWTac实现了0.2毫米的纹理分辨率、98%的石块分类准确率和0.15牛的力估计误差，而SandWorm在颗粒介质中展现出高达12.5毫米/秒的多种运动能力，在管道疏浚和地下勘探等任务中取得了90%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion-Guided Backdoor Attacks in Real-World Reinforcement Learning</div>
<div class="meta-line">Authors: Tairan Huang, Qingqing Ye, Yulin Jin, Jiawei Lian, Yi Wang, Haibo Hu</div>
<div class="meta-line">First: 2026-01-20T16:03:51+00:00 · Latest: 2026-01-20T16:03:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14104v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14104v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Backdoor attacks embed hidden malicious behaviors in reinforcement learning (RL) policies and activate them using triggers at test time. Most existing attacks are validated only in simulation, while their effectiveness in real-world robotic systems remains unclear. In physical deployment, safety-constrained control pipelines such as velocity limiting, action smoothing, and collision avoidance suppress abnormal actions, causing strong attenuation of conventional backdoor attacks. We study this previously overlooked problem and propose a diffusion-guided backdoor attack framework (DGBA) for real-world RL. We design small printable visual patch triggers placed on the floor and generate them using a conditional diffusion model that produces diverse patch appearances under real-world visual variations. We treat the robot control stack as a black-box system. We further introduce an advantage-based poisoning strategy that injects triggers only at decision-critical training states. We evaluate our method on a TurtleBot3 mobile robot and demonstrate reliable activation of targeted attacks while preserving normal task performance. Demo videos and code are available in the supplementary material.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现实世界强化学习中的扩散引导后门攻击</div>
<div class="mono" style="margin-top:8px">后门攻击在强化学习策略中嵌入隐藏的恶意行为，并在测试时通过触发器激活。现有攻击大多仅在仿真环境中验证，其在现实机器人系统中的有效性尚不明确。在物理部署中，速度限制、动作平滑和碰撞避免等安全约束控制流程会抑制异常动作，导致传统后门攻击大幅衰减。本研究针对这一被忽视的问题，提出面向现实世界强化学习的扩散引导后门攻击框架。我们设计可打印的小型视觉补丁触发器置于地面，并采用条件扩散模型生成适应现实视觉变化的多样化补丁外观。将机器人控制栈视为黑盒系统，进一步提出基于优势的投毒策略，仅在决策关键训练状态注入触发器。在TurtleBot3移动机器人上的实验表明，该方法能可靠激活目标攻击，同时保持正常任务性能。演示视频与代码详见补充材料。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the gap in evaluating backdoor attacks in real-world reinforcement learning (RL) systems, as existing attacks validated in simulation are often attenuated by safety-constrained control pipelines like velocity limiting and action smoothing in physical robotic deployments. The proposed method, Diffusion-Guided Backdoor Attack (DGBA), designs small printable visual patch triggers generated via a conditional diffusion model to ensure diversity under real-world visual variations, treats the robot control stack as a black-box, and employs an advantage-based poisoning strategy to inject triggers only at decision-critical training states. Experimental evaluation on a TurtleBot3 mobile robot demonstrates that the attack reliably activates targeted malicious behaviors while maintaining normal task performance, overcoming the attenuation caused by safety constraints.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习中后门攻击在仿真与真实世界有效性之间的差距，因为传统攻击在物理机器人系统中会被速度限制、动作平滑等安全约束控制流程所削弱。所提出的方法——扩散引导后门攻击（DGBA）——设计了放置于地面的小型可打印视觉补丁触发器，通过条件扩散模型生成以确保在真实世界视觉变化下的多样性，并采用基于优势的投毒策略，仅在决策关键的训练状态注入触发器，同时将控制栈视为黑盒。在TurtleBot3移动机器人上的实验评估表明，该攻击能可靠激活目标恶意行为，同时保持正常的任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">Tube-Based Robust Control Strategy for Vision-Guided Autonomous Vehicles</div>
<div class="meta-line">Authors: Der-Hau Lee</div>
<div class="meta-line">First: 2025-03-24T15:01:00+00:00 · Latest: 2026-01-20T16:00:12+00:00</div>
<div class="meta-line">Comments: 15 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.18752v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.18752v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A robust control strategy for autonomous vehicles can improve system stability, enhance riding comfort, and prevent driving accidents. This paper presents a novel interpolation-tube-based constrained iterative linear quadratic regulator (itube-CILQR) algorithm for autonomous computer-vision-based vehicle lane-keeping. The goal of the algorithm is to enhance robustness during high-speed cornering on tight turns. Compared with standard tube-based approaches, the proposed itube-CILQR algorithm reduces system conservatism and exhibits higher computational speed. Numerical simulations and vision-based experiments were conducted to examine the feasibility of using the proposed algorithm for controlling autonomous vehicles. The results indicated that the proposed algorithm achieved superior vehicle lane-keeping performance to variational CILQR-based methods and model predictive control (MPC) approaches involving the use of a classical interior-point optimizer. Specifically, itube-CILQR required an average runtime of 3.45 ms to generate a control signal for guiding a self-driving vehicle. By comparison, itube-MPC typically required a 4.32 times longer computation time to complete the same task. Moreover, the influence of conservatism on system behavior was investigated by exploring the variations in the interpolation variables derived using the proposed itube-CILQR algorithm during lane-keeping maneuvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于管道的视觉引导自动驾驶车辆鲁棒控制策略</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆的鲁棒控制策略可提升系统稳定性、增强乘坐舒适性并预防驾驶事故。本文提出一种新型基于插值管道的约束迭代线性二次调节器（itube-CILQR）算法，用于基于计算机视觉的自动驾驶车辆车道保持。该算法旨在提升高速急转弯工况下的鲁棒性。与标准管道方法相比，所提出的itube-CILQR算法降低了系统保守性，并展现出更高的计算速度。通过数值仿真和基于视觉的实验验证了该算法控制自动驾驶车辆的可行性。结果表明，该算法在车道保持性能上优于基于变分CILQR的方法及采用经典内点优化器的模型预测控制（MPC）方法。具体而言，itube-CILQR生成自动驾驶车辆控制信号的平均运行时间为3.45毫秒，而itube-MPC完成相同任务的计算时间通常延长4.32倍。此外，通过分析车道保持过程中itube-CILQR算法插值变量的变化，研究了保守性对系统行为的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance autonomous vehicle stability, comfort, and safety, particularly during high-speed cornering, this paper introduces an interpolation-tube-based constrained iterative linear quadratic regulator (itube-CILQR) algorithm for vision-guided lane-keeping. The method reduces the conservatism typical of tube-based approaches and improves computational efficiency. Experimental results demonstrate that itube-CILQR outperforms variational CILQR and model predictive control with an interior-point optimizer in lane-keeping performance, achieving an average control signal generation time of 3.45 ms, which is 4.32 times faster than the itube-MPC baseline, while also analyzing the effect of interpolation variables on system behavior.</div>
<div class="mono" style="margin-top:8px">为提高自动驾驶车辆在高速过弯等场景下的稳定性、舒适性和安全性，本文针对视觉引导的车道保持任务，提出了一种基于插值管的约束迭代线性二次型调节器（itube-CILQR）算法。该方法旨在降低传统管式控制的保守性并提升计算速度。仿真和基于视觉的实验结果表明，该算法在车道保持性能上优于变分CILQR及采用经典内点优化器的模型预测控制方法，其生成控制信号的平均运行时间仅为3.45毫秒，比对比的itube-MPC方法快4.32倍。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems</div>
<div class="meta-line">Authors: Hossein Naderi, Alireza Shojaei, Lifu Huang, Philip Agee, Kereshmeh Afsari, Abiola Akanmu</div>
<div class="meta-line">First: 2026-01-20T15:54:33+00:00 · Latest: 2026-01-20T15:54:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14091v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14091v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自主建造机器人的零样本可适应任务规划：轻量级单智能体与多智能体系统的比较研究</div>
<div class="mono" style="margin-top:8px">机器人有望在未来建筑业发挥重要作用，但面临成本高、动态任务适应性差等挑战。本研究探索基础模型提升建造机器人任务规划适应性与泛化能力的潜力，基于轻量化开源大语言模型（LLM）和视觉语言模型（VLM）提出并实现了四种模型，包括一个单智能体和三个通过协作生成机器人行动方案的多智能体团队。模型在油漆工、安全巡检员、地板铺贴工三种建造角色中进行评估。结果显示：四智能体团队在多数指标上优于当前最优的GPT-4o模型，且成本效益提升十倍；三智能体与四智能体团队还展现出更强的泛化能力。通过分析智能体行为对输出的影响，本研究深化了对AI团队协作机制的理解，为未来拓展至建造领域之外的非结构化环境研究提供支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high costs and limited adaptability of robots in dynamic construction tasks, this study investigates foundation models for flexible task planning. It proposes and implements four lightweight, open-source AI agent systems—one single agent and three multi-agent teams—that utilize large language models and vision language models to generate robot action plans. Experimental evaluation across three construction roles (Painter, Safety Inspector, Floor Tiling) shows that the four-agent team surpasses GPT-4o in most performance metrics while being ten times more cost-effective, with three- and four-agent teams also exhibiting enhanced generalizability, offering insights for AI team behaviors in unstructured environments.</div>
<div class="mono" style="margin-top:8px">为解决建筑机器人成本高、在动态任务中适应性差的问题，本研究探索了利用轻量级基础模型进行零样本自适应任务规划的潜力。方法基于开源大语言模型和视觉语言模型，提出并实现了四种模型，包括一个单智能体和三个多智能体协作系统，用于为油漆工、安全巡检员和地板铺贴工等角色生成动作计划。实验结果表明，四智能体团队在多数性能指标上超越了最先进的GPT-4o，同时成本效益高出十倍，且三智能体和四智能体团队表现出更好的泛化能力，从而增进了对AI团队行为的理解，支持未来在建筑以外多样化非结构化环境中的研究。</div>
</details>
</div>
<div class="card">
<div class="title">FlyPose: Towards Robust Human Pose Estimation From Aerial Views</div>
<div class="meta-line">Authors: Hassaan Farooq, Marvin Brenner, Peter Stütz</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2026-01-09T12:01:36+00:00 · Latest: 2026-01-20T15:33:25+00:00</div>
<div class="meta-line">Comments: 11 pages, 9 figures, IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05747v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05747v2">PDF</a> · <a href="https://github.com/farooqhassaan/FlyPose">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlyPose：面向无人机视角的鲁棒人体姿态估计</div>
<div class="mono" style="margin-top:8px">无人机正日益频繁地部署于人类近身环境，如包裹配送、交通监控、灾害响应和基础设施巡检。为确保此类人机共存场景下的安全可靠运行，需从空中视角精准感知人体姿态与行为。该视角因低分辨率、大俯仰角和（自）遮挡等问题对现有方法构成挑战，尤其在需实时可行模型的应用中。我们训练并部署了FlyPose——一种面向航拍图像的轻量级自上而下人体姿态估计流程。通过多数据集训练，在Manipal-UAV、VisDrone、HIT-UAV及自定义数据集的测试集上，人体检测平均精度提升6.8 mAP；在极具挑战性的UAV-Human数据集上，二维人体姿态估计精度提升16.3 mAP。FlyPose在Jetson Orin AGX开发套件上的推理延迟约20毫秒（含预处理），并已在四旋翼无人机飞行实验中完成机载部署。同时我们开源了FlyPose-104数据集，这是一个规模较小但极具挑战性的航拍人体姿态数据集，包含困难空中视角的人工标注：https://github.com/farooqhassaan/FlyPose。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for accurate human pose estimation from aerial views to ensure safe operation of UAVs in human-populated environments, which is challenged by low resolution, steep angles, and occlusions. The method involves training and deploying FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery, using multi-dataset training to enhance robustness. Key experimental results show an average improvement of 6.8 mAP in person detection across multiple test sets and a 16.3 mAP improvement in 2D pose estimation on the UAV-Human dataset, with the system achieving an inference latency of ~20 milliseconds on a Jetson Orin AGX and being successfully deployed onboard a quadrotor UAV.</div>
<div class="mono" style="margin-top:8px">该研究的动机是无人机在人群环境中安全运行需要从空中视角准确估计人体姿态，但现有方法面临低分辨率、陡峭视角和遮挡等挑战。方法上，通过多数据集训练，开发并部署了FlyPose，一种轻量级的自上而下人体姿态估计流程，专为航空图像设计。主要实验结果表明，在多个测试集上人物检测平均提升6.8 mAP，在UAV-Human数据集上2D姿态估计提升16.3 mAP，且在Jetson Orin AGX上的推理延迟约为20毫秒，实现了飞行实验中的实时板载部署。</div>
</details>
</div>
<div class="card">
<div class="title">Group-Invariant Unsupervised Skill Discovery: Symmetry-aware Skill Representations for Generalizable Behavior</div>
<div class="meta-line">Authors: Junwoo Chang, Joseph Park, Roberto Horowitz, Jongmin Lee, Jongeun Choi</div>
<div class="meta-line">First: 2026-01-20T14:21:18+00:00 · Latest: 2026-01-20T14:21:18+00:00</div>
<div class="meta-line">Comments: 14 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14000v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised skill discovery aims to acquire behavior primitives that improve exploration and accelerate downstream task learning. However, existing approaches often ignore the geometric symmetries of physical environments, leading to redundant behaviors and sample inefficiency. To address this, we introduce Group-Invariant Skill Discovery (GISD), a framework that explicitly embeds group structure into the skill discovery objective. Our approach is grounded in a theoretical guarantee: we prove that in group-symmetric environments, the standard Wasserstein dependency measure admits a globally optimal solution comprised of an equivariant policy and a group-invariant scoring function. Motivated by this, we formulate the Group-Invariant Wasserstein dependency measure, which restricts the optimization to this symmetry-aware subspace without loss of optimality. Practically, we parameterize the scoring function using a group Fourier representation and define the intrinsic reward via the alignment of equivariant latent features, ensuring that the discovered skills generalize systematically under group transformations. Experiments on state-based and pixel-based locomotion benchmarks demonstrate that GISD achieves broader state-space coverage and improved efficiency in downstream task learning compared to a strong baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>群不变无监督技能发现：面向可泛化行为的对称感知技能表示</div>
<div class="mono" style="margin-top:8px">无监督技能发现旨在获取能改善探索并加速下游任务学习的行为基元。然而，现有方法常忽略物理环境的几何对称性，导致行为冗余与样本效率低下。为此，我们提出群不变技能发现框架，将群结构显式嵌入技能发现目标。我们的方法基于理论保证：证明了在群对称环境中，标准Wasserstein依赖度量存在由等变策略与群不变评分函数构成的全局最优解。基于此，我们构建了群不变Wasserstein依赖度量，将优化限制在这一对称感知子空间且不损失最优性。实践中，我们采用群傅里叶表示参数化评分函数，并通过等变潜在特征对齐定义内在奖励，确保发现的技能在群变换下具有系统性泛化能力。在状态空间与像素级运动基准测试中，实验表明该框架相比强基线实现了更广的状态空间覆盖与更优的下游任务学习效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Unsupervised skill discovery methods often fail to account for the geometric symmetries in physical environments, resulting in redundant behaviors and inefficient learning. To address this, the authors propose Group-Invariant Skill Discovery (GISD), a framework that incorporates group structure into the skill discovery objective by formulating a Group-Invariant Wasserstein dependency measure, which is theoretically guaranteed to yield an optimal solution with an equivariant policy and group-invariant scoring function in symmetric environments. Experiments on locomotion benchmarks show that GISD achieves greater state-space coverage and more efficient downstream task learning compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">无监督技能发现方法通常忽略物理环境中存在的几何对称性，导致行为冗余和学习效率低下。为此，研究者提出了群不变技能发现（GISD）框架，该框架通过构建群不变Wasserstein依赖度量，将群结构明确纳入技能发现目标，其理论保证可得到包含等变策略和群不变评分函数的最优解。在基于状态和像素的移动基准测试中，实验表明GISD相比基线方法能实现更广的状态空间覆盖和更高效的下游任务学习。</div>
</details>
</div>
<div class="card">
<div class="title">Active Cross-Modal Visuo-Tactile Perception of Deformable Linear Objects</div>
<div class="meta-line">Authors: Raffaele Mazza, Ciro Natale, Pietro Falco</div>
<div class="meta-line">First: 2026-01-20T13:57:59+00:00 · Latest: 2026-01-20T13:57:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13979v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13979v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a novel cross-modal visuo-tactile perception framework for the 3D shape reconstruction of deformable linear objects (DLOs), with a specific focus on cables subject to severe visual occlusions. Unlike existing methods relying predominantly on vision, whose performance degrades under varying illumination, background clutter, or partial visibility, the proposed approach integrates foundation-model-based visual perception with adaptive tactile exploration. The visual pipeline exploits SAM for instance segmentation and Florence for semantic refinement, followed by skeletonization, endpoint detection, and point-cloud extraction. Occluded cable segments are autonomously identified and explored with a tactile sensor, which provides local point clouds that are merged with the visual data through Euclidean clustering and topology-preserving fusion. A B-spline interpolation driven by endpoint-guided point sorting yields a smooth and complete reconstruction of the cable shape. Experimental validation using a robotic manipulator equipped with an RGB-D camera and a tactile pad demonstrates that the proposed framework accurately reconstructs both simple and highly curved single or multiple cable configurations, even when large portions are occluded. These results highlight the potential of foundation-model-enhanced cross-modal perception for advancing robotic manipulation of deformable objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可变形线性物体的主动跨模态视觉-触觉感知</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的跨模态视觉-触觉感知框架，用于可变形线性物体的三维形状重建，特别关注存在严重视觉遮挡的线缆。与主要依赖视觉的现有方法（其性能在光照变化、背景杂乱或局部可见性差时会下降）不同，所提方法将基于基础模型的视觉感知与自适应触觉探索相结合。视觉流程利用SAM进行实例分割，Florence进行语义细化，随后进行骨架化、端点检测和点云提取。被遮挡的线缆段通过触觉传感器自主识别与探索，获取局部点云后通过欧几里得聚类和拓扑保持融合与视觉数据合并。基于端点引导点排序的B样条插值实现了线缆形状的平滑完整重建。使用配备RGB-D相机和触觉垫的机器人操纵器进行的实验验证表明，该框架能准确重建简单和高度弯曲的单根或多根线缆构型，即使存在大范围遮挡。这些结果凸显了基础模型增强的跨模态感知在推进可变形物体机器人操控方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of vision-only methods in reconstructing the 3D shape of deformable linear objects like cables under severe visual occlusions, this paper introduces a cross-modal visuo-tactile perception framework. The method integrates foundation-model-based visual perception, using SAM for segmentation and Florence for semantic refinement, with adaptive tactile exploration; occluded segments identified visually are explored with a tactile sensor, and the local point clouds are fused with visual data via Euclidean clustering and topology-preserving fusion before B-spline interpolation yields a complete shape. Experiments with a robotic manipulator equipped with an RGB-D camera and tactile pad show the framework accurately reconstructs simple and highly curved single or multiple cable configurations, even with large occlusions, demonstrating the potential of foundation-model-enhanced cross-modal perception for robotic manipulation of deformable objects.</div>
<div class="mono" style="margin-top:8px">本研究针对电缆等可变形线性物体的三维形状重建问题，传统纯视觉方法在严重遮挡、背景杂乱或光照变化下性能会下降。该方法融合了基于基础模型的视觉感知（使用SAM进行实例分割，Florence进行语义细化）与自适应触觉探索：视觉流程提取部分点云与骨架并识别被遮挡段，随后触觉传感器主动探测这些区域以获取局部三维数据，通过欧几里得聚类与拓扑保持融合将触觉与视觉数据合并，最后进行端点引导的B样条插值。搭载RGB-D相机与触觉垫的机械臂实验表明，该框架能准确重建单个或多个高弯曲度电缆的完整三维形状，即使在大部分被遮挡的情况下也有效，证明了跨模态感知对推进可变形物体操控的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation</div>
<div class="meta-line">Authors: Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu, Yonggang Qi</div>
<div class="meta-line">First: 2026-01-20T13:54:10+00:00 · Latest: 2026-01-20T13:54:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13976v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13976v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FantasyVLN：面向视觉语言导航的统一多模态思维链推理框架</div>
<div class="mono" style="margin-top:8px">在视觉语言导航（VLN）中实现人类水平性能，要求智能体同时理解多模态指令与视觉空间上下文，并对长动作序列进行推理。NavCoT、NavGPT-2等近期研究证明了思维链（CoT）推理在提升可解释性与长程规划方面的潜力，OctoNav-R1、CoT-VLA等多模态扩展进一步验证了CoT是实现类人导航推理的有效路径。然而现有方法存在明显缺陷：纯文本CoT缺乏空间基础且易对稀疏标注的推理步骤过拟合，而多模态CoT因生成虚构视觉观测导致严重的令牌膨胀，难以实现实时导航。本研究提出FantasyVLN——一个统一隐式推理框架，在保留CoT推理优势的同时避免显式令牌开销。具体而言，在CoT推理训练阶段，通过预训练视觉自回归器（VAR）将虚构视觉令牌编码至紧凑的潜在空间，模型在统一多CoT策略下联合学习文本、视觉及多模态CoT模式。推理时，模型直接执行指令到动作的映射，同时保持推理感知的表征能力。在LH-VLN数据集上的大量实验表明，本方法实现了兼具推理感知与实时性的导航，在提升成功率与效率的同时，将推理延迟较显式CoT方法降低一个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing Chain-of-Thought (CoT) reasoning methods in Vision-and-Language Navigation (VLN), where purely textual CoTs lack spatial grounding and multimodal CoTs incur impractical computational overhead from generating imagined visual tokens. The proposed method, FantasyVLN, introduces a unified implicit reasoning framework that encodes imagined visual observations into a compact latent space using a pretrained Visual AutoRegressor during training, allowing the model to learn from textual, visual, and multimodal CoT modes without explicit token generation at inference. Experimental results on the LH-VLN benchmark demonstrate that this approach achieves reasoning-aware navigation with significantly improved success rates and efficiency, while reducing inference latency by an order of magnitude compared to explicit CoT methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有思维链推理方法在视觉语言导航中的局限性，即纯文本思维链缺乏空间基础，而多模态思维链则因令牌膨胀导致实时导航不切实际。提出的FantasyVLN框架采用了一种统一的隐式推理方法，在训练过程中使用预训练的视觉自回归模型将想象的视觉令牌编码到紧凑的潜在空间中，并联合学习文本、视觉和多模态思维链模式。在LH-VLN数据集上的实验结果表明，该方法实现了具备推理意识的导航，显著提高了成功率和效率，同时将推理延迟相比显式思维链方法降低了一个数量级。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Coordination with the System-Level Shared State: An Embodied-AI Native Modular Framework</div>
<div class="meta-line">Authors: Yixuan Deng, Tongrun Wu, Donghao Wu, Zeyu Wei, Jiayuan Wang, Zhenglong Sun, Yuqing Tang, Xiaoqiang Ji</div>
<div class="meta-line">First: 2026-01-20T13:21:52+00:00 · Latest: 2026-01-20T13:21:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13945v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13945v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Embodied AI systems move from research prototypes to real world deployments, they tend to evolve rapidly while remaining reliable under workload changes and partial failures. In practice, many deployments are only partially decoupled: middleware moves messages, but shared context and feedback semantics are implicit, causing interface drift, cross-module interference, and brittle recovery at scale. We present ANCHOR, a modular framework that makes decoupling and robustness explicit system-level primitives. ANCHOR separates (i) Canonical Records, an evolvable contract for the standardized shared state, from (ii) a communication bus for many-to-many dissemination and feedback-oriented coordination, forming an inspectable end-to-end loop. We validate closed-loop feasibility on a de-identified workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and demonstrate automatic stream resumption after hard crashes and restarts even with shared-memory loss. Overall, ANCHOR turns ad-hoc integration glue into explicit contracts, enabling controlled degradation under load and self-healing recovery for scalable deployment of closed-loop AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于系统级共享状态的高效协调：一种具身AI原生的模块化框架</div>
<div class="mono" style="margin-top:8px">随着具身AI系统从研究原型转向实际部署，它们需要在保持工作负载变化和局部故障下可靠性的同时快速演进。实践中，许多部署仅实现部分解耦：中间件负责消息传递，但共享上下文与反馈语义仍隐式存在，导致接口漂移、跨模块干扰及大规模场景下的脆弱恢复。本文提出ANCHOR模块化框架，将解耦与鲁棒性明确提升为系统级原语。ANCHOR将（i）标准化共享状态的可演进契约——规范记录，与（ii）支持多对多分发及面向反馈协调的通信总线相分离，形成可检视的端到端闭环。我们通过去标识化工作流实例验证闭环可行性，分析不同负载规模与发布速率下的延迟分布，并演示在共享内存丢失情况下遭遇硬崩溃与重启后的自动流恢复。ANCHOR将临时集成粘合剂转化为显式契约，实现负载下的可控降级与自愈恢复，为闭环AI系统的可扩展部署提供支撑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for Embodied AI systems to evolve rapidly and remain reliable under workload changes and partial failures as they transition from prototypes to real-world deployments, where current partially decoupled approaches often lead to interface drift and brittle recovery. The method introduces ANCHOR, a modular framework that explicitly provides decoupling and robustness as system-level primitives by separating Canonical Records (an evolvable contract for standardized shared state) from a communication bus for many-to-many dissemination and feedback-oriented coordination, creating an inspectable end-to-end loop. Key experimental results demonstrate closed-loop feasibility on a workflow instantiation, characterize latency distributions under varying payload sizes and publish rates, and show automatic stream resumption after hard crashes and restarts, even with shared-memory loss, enabling controlled degradation and self-healing recovery.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身人工智能系统从原型扩展到实际部署时，因隐含的共享上下文和反馈语义常导致接口漂移、模块间干扰和脆弱恢复，从而难以保持可靠性和可演进性的问题。提出的方法ANCHOR引入了一个模块化框架，通过将规范记录（用于标准化共享状态的可演进契约）与用于多对多传播和面向反馈协调的通信总线分离，显式解耦系统级原语，从而形成可检查的端到端循环。在去标识化工作流上的实验验证证明了闭环可行性，描述了不同负载大小和发布速率下的延迟分布，并展示了即使在共享内存丢失的情况下，硬崩溃和重启后也能自动恢复流，实现了可控降级和自愈恢复，以支持可扩展的AI系统部署。</div>
</details>
</div>
<div class="card">
<div class="title">Safety on the Fly: Constructing Robust Safety Filters via Policy Control Barrier Functions at Runtime</div>
<div class="meta-line">Authors: Luzia Knoedler, Oswin So, Ji Yin, Mitchell Black, Zachary Serlin, Panagiotis Tsiotras, Javier Alonso-Mora, Chuchu Fan</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2024-10-15T00:48:00+00:00 · Latest: 2026-01-20T12:00:09+00:00</div>
<div class="meta-line">Comments: Accepted in RAL. The project page can be found at www.oswinso.xyz/rpcbf/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.11157v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.11157v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Control Barrier Functions (CBFs) have proven to be an effective tool for performing safe control synthesis for nonlinear systems. However, guaranteeing safety in the presence of disturbances and input constraints for high relative degree systems is a difficult problem. In this work, we propose the Robust Policy CBF (RPCBF), a practical approach for constructing robust CBF approximations online via the estimation of a value function. We establish conditions under which the approximation qualifies as a valid CBF and demonstrate the effectiveness of the RPCBF-safety filter in simulation on a variety of high relative degree input-constrained systems. Finally, we demonstrate the benefits of our method in compensating for model errors on a hardware quadcopter platform by treating the model errors as disturbances. Website including code: www.oswinso.xyz/rpcbf/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>飞行中的安全保障：通过运行时策略控制屏障函数构建鲁棒安全过滤器</div>
<div class="mono" style="margin-top:8px">控制屏障函数（CBF）已被证明是执行非线性系统安全控制综合的有效工具。然而，对于高相对阶系统，在存在扰动和输入约束的情况下保证安全性是一个难题。本研究提出鲁棒策略CBF（RPCBF），这是一种通过值函数估计在线构建鲁棒CBF近似的实用方法。我们建立了近似结果符合有效CBF标准的条件，并通过仿真在多种高相对阶输入受限系统上验证了RPCBF安全过滤器的有效性。最后，我们将模型误差视为扰动，在硬件四旋翼平台上展示了本方法补偿模型误差的优势。含代码网站：www.oswinso.xyz/rpcbf/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of ensuring safety for high relative degree nonlinear systems under disturbances and input constraints, where traditional Control Barrier Functions (CBFs) are difficult to apply. The method introduces Robust Policy CBF (RPCBF), which constructs robust CBF approximations online by estimating a value function, establishing conditions for validity. Experimental results demonstrate the effectiveness of the RPCBF-safety filter in simulations on various high relative degree, input-constrained systems and in compensating for model errors on a hardware quadcopter platform by treating them as disturbances.</div>
<div class="mono" style="margin-top:8px">本研究针对存在干扰和输入约束的高相对度非线性系统的安全保证难题，传统控制屏障函数（CBF）在此类场景中效果有限。提出的方法——鲁棒策略控制屏障函数（RPCBF）——通过在线估计价值函数来构建鲁棒的CBF近似，并建立了该近似作为有效CBF的充分条件。实验结果表明，RPCBF安全滤波器在多种高相对度、输入受限系统的仿真中表现有效，并在硬件四旋翼平台上通过将模型误差视为干扰，验证了其在补偿模型误差方面的实际优势。</div>
</details>
</div>
<div class="card">
<div class="title">Sequentially Teaching Sequential Tasks $(ST)^2$: Teaching Robots Long-horizon Manipulation Skills</div>
<div class="meta-line">Authors: Zlatan Ajanović, Ravi Prakash, Leandro de Souza Rosa, Jens Kober</div>
<div class="meta-line">First: 2025-10-23T23:00:05+00:00 · Latest: 2026-01-20T11:46:07+00:00</div>
<div class="meta-line">Comments: Accepted for publication in IEEE Robotics and Automation Magazine</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21046v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from demonstration has proved itself useful for teaching robots complex skills with high sample efficiency. However, teaching long-horizon tasks with multiple skills is challenging as deviations tend to accumulate, the distributional shift becomes more evident, and human teachers become fatigued over time, thereby increasing the likelihood of failure. To address these challenges, we introduce $(ST)^2$, a sequential method for learning long-horizon manipulation tasks that allows users to control the teaching flow by specifying key points, enabling structured and incremental demonstrations. Using this framework, we study how users respond to two teaching paradigms: (i) a traditional monolithic approach, in which users demonstrate the entire task trajectory at once, and (ii) a sequential approach, in which the task is segmented and demonstrated step by step. We conducted an extensive user study on the restocking task with $16$ participants in a realistic retail store environment, evaluating the user preferences and effectiveness of the methods. User-level analysis showed superior performance for the sequential approach in most cases (10 users), compared with the monolithic approach (5 users), with one tie. Our subjective results indicate that some teachers prefer sequential teaching -- as it allows them to teach complicated tasks iteratively -- or others prefer teaching in one go due to its simplicity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>序列化教学序列任务$(ST)^2$：教授机器人长时域操作技能</div>
<div class="mono" style="margin-top:8px">示教学习已被证明能以高样本效率教授机器人复杂技能。然而，教授包含多项技能的长期任务具有挑战性：偏差易累积、分布偏移更显著、人类教师随时间推移产生疲劳，从而增加失败概率。为解决这些挑战，我们提出$(ST)^2$——一种学习长时域操作任务的序列化方法，允许用户通过指定关键点控制教学流程，实现结构化增量示教。基于该框架，我们研究用户对两种教学范式的响应：（i）传统整体式教学：用户一次性演示完整任务轨迹；（ii）序列化教学：任务被分段并逐步演示。我们在真实零售环境中对16名参与者开展补货任务的广泛用户研究，评估用户偏好与方法有效性。用户层级分析显示，序列化方法在多数情况下（10名用户）优于整体式方法（5名用户），1名用户评价持平。主观结果表明：部分教师偏好序列化教学（因其支持复杂任务的迭代教学），另一部分则因操作简便性偏好整体式教学。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Teaching robots long-horizon manipulation tasks through demonstration is challenging due to cumulative errors, distributional shift, and teacher fatigue. To address this, the authors introduce $(ST)^2$, a sequential teaching method that allows users to structure demonstrations by specifying key points, enabling incremental learning. In a user study with 16 participants on a restocking task in a realistic retail environment, comparing a monolithic (one-go) demonstration approach with the sequential approach, the sequential method was preferred and performed better for 10 users, while 5 preferred the monolithic approach, and one case was a tie, indicating that sequential teaching is often more effective for complex tasks.</div>
<div class="mono" style="margin-top:8px">通过演示教授机器人长时程操作任务存在误差累积、分布偏移和教师疲劳等挑战。为此，研究者提出了(ST)^2顺序教学方法，允许用户指定关键点以进行结构化、增量式的演示。在一项针对16名参与者在零售环境中执行补货任务的用户研究中，顺序教学方法在10名用户中表现更优，而传统整体教学方法仅在5名用户中更有效，另有一例平局，这表明顺序教学在复杂任务教授中更具优势。</div>
</details>
</div>
<div class="card">
<div class="title">Robotic Tele-Operation for Upper Aerodigestive Tract Microsurgery: System Design and Validation</div>
<div class="meta-line">Authors: Giovani Braglia, José Jair Alves Mendes Junior, Augusto Tetsuo Prado Inafuco, Federico Mariano, Leonardo S. Mattos</div>
<div class="meta-line">First: 2026-01-10T16:50:05+00:00 · Latest: 2026-01-20T10:46:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06617v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.06617v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Upper aerodigestive tract (UADT) treatments frequently employ transoral laser microsurgery (TLM) for procedures such as the removal of tumors or polyps. In TLM, a laser beam is used to cut target tissue, while forceps are employed to grasp, manipulate, and stabilize tissue within the UADT. Although TLM systems may rely on different technologies and interfaces, forceps manipulation is still predominantly performed manually, introducing limitations in ergonomics, precision, and controllability. This paper proposes a novel robotic system for tissue manipulation in UADT procedures, based on a novel end-effector designed for forceps control. The system is integrated within a teleoperation framework that employs a robotic manipulator with a programmed remote center of motion (RCM), enabling precise and constrained instrument motion while improving surgeon ergonomics. The proposed approach is validated through two experimental studies and a dedicated usability evaluation, demonstrating its effectiveness and suitability for UADT surgical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上气道消化道显微手术机器人遥操作系统：设计与验证</div>
<div class="mono" style="margin-top:8px">上气道消化道（UADT）治疗常采用经口激光显微手术（TLM）进行肿瘤或息肉切除等操作。TLM中，激光束用于切割目标组织，而镊子则用于抓取、操纵和稳定UADT内的组织。尽管TLM系统可能依赖不同的技术和界面，镊子操作仍主要依赖人工，存在人机工程学、精度和可控性方面的局限。本文提出一种基于新型末端执行器设计的UADT手术组织操作机器人系统，该执行器专为镊子控制而设计。系统集成于遥操作框架中，采用具有编程远程运动中心（RCM）的机械臂，在提升外科医生人机工程学的同时实现精确受限的器械运动。通过两项实验研究和专项可用性评估验证了所提方法的有效性及其在UADT手术应用中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the ergonomic and precision limitations of manual forceps manipulation in transoral laser microsurgery (TLM) for the upper aerodigestive tract, this paper proposes a novel robotic tele-operation system featuring a custom end-effector for forceps control. The method integrates this end-effector with a robotic manipulator programmed for remote center of motion (RCM) within a teleoperation framework to enable precise, constrained instrument movement. Experimental validation through two studies and a usability evaluation demonstrated the system&#x27;s effectiveness and suitability for UADT surgical applications.</div>
<div class="mono" style="margin-top:8px">针对上气道消化道经口激光显微手术中手动操作钳子存在的工效学、精度和控制性局限，本研究提出了一种新型机器人遥操作系统，其核心是一个用于钳子控制的定制末端执行器。该方法将该执行器与一个编程了远程运动中心的机器人操纵器集成，在遥操作框架内实现受限且精确的器械运动。通过两项实验研究和专门的可用性评估进行验证，结果证明了该系统在上气道消化道手术应用中的有效性和适用性。</div>
</details>
</div>
<div class="card">
<div class="title">GuideTouch: An Obstacle Avoidance Device for Visually Impaired</div>
<div class="meta-line">Authors: Timofei Kozlov, Artem Trandofilov, Georgii Gazaryan, Issatay Tokmurziyev, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-20T10:12:05+00:00 · Latest: 2026-01-20T10:12:05+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13813v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13813v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user&#x27;s shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GuideTouch：面向视障人士的避障设备</div>
<div class="mono" style="margin-top:8px">视障人士的安全导航仍是严峻挑战，尤其针对传统助行工具常无法探测的头部高度障碍物。本文推出GuideTouch——一款紧凑、经济、独立的可穿戴自主避障设备。该系统集成两个垂直排列的飞行时间传感器以实现三维环境感知，以及四个振动触觉执行器以提供定向触觉反馈。通过部署于用户肩部与上胸部的直观四点振动触觉反馈系统传递距离与方向信息。为提升实际环境适应性，设备配备独特的离心式自清洁光学盖板机制及遗失报警系统。我们通过22名参与者（男性17人，女性5人，年龄21-48岁，均值25.7，标准差6.1）评估了触觉感知准确度。统计分析证实不同振动模式的感知准确度存在显著差异。系统展现出高识别准确率：单/双电机（主方向）模式平均达92.9%。此外，14名视障用户的初步实验验证了该交互界面，对主方向线索的识别准确率达93.75%。结果表明，GuideTouch能实现直观空间感知，可显著提升视障用户在独立导航时的安全性、信心与自主性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the critical challenge of safe navigation for visually impaired individuals, particularly the detection of head-level obstacles that traditional aids often miss, this research introduces GuideTouch, a compact and affordable standalone wearable device. The method integrates two vertically aligned Time-of-Flight sensors for 3D environmental perception and four vibrotactile actuators that provide directional haptic feedback across the user&#x27;s shoulders and chest, complemented by a self-cleaning optical cover and a sound alarm for robustness. Experimental results with 22 participants showed high recognition accuracy for haptic patterns, averaging 92.9% for primary directional cues, and preliminary tests with 14 visually impaired users confirmed a 93.75% accuracy, demonstrating the system&#x27;s potential to enhance safety and autonomy during navigation.</div>
<div class="mono" style="margin-top:8px">为解决视障人士安全出行的关键挑战，特别是传统辅助工具常无法检测到的头部高度障碍物，本研究引入了GuideTouch，一款紧凑、经济、独立的可穿戴设备。该方法集成了两个垂直排列的飞行时间传感器以实现三维环境感知，以及四个振动触觉执行器，在用户肩部和胸部提供定向触觉反馈，并配备了自清洁光学盖和声音警报系统以增强鲁棒性。对22名参与者的实验结果显示，触觉模式识别准确率高，主要定向线索的平均准确率达92.9%；初步测试中14名视障用户的准确率为93.75%，表明该系统有望显著提升用户导航时的安全性和自主性。</div>
</details>
</div>
<div class="card">
<div class="title">DroneVLA: VLA based Aerial Manipulation</div>
<div class="meta-line">Authors: Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-20T10:08:00+00:00 · Latest: 2026-01-20T10:08:00+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system&#x27;s efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DroneVLA：基于视觉语言动作模型的空中操控系统</div>
<div class="mono" style="margin-top:8px">随着空中平台从被动观测者演变为主动操控者，设计直观界面让非专业用户自然操控系统成为关键挑战。本研究提出一种新型自主空中操控系统，能够解析高级自然语言指令以抓取物体并递交给人类用户。该系统整合了基于Grounding DINO的MediaPipe模块、视觉-语言-动作模型，以及搭载单自由度夹爪与英特尔实感RGB-D相机的定制无人机。VLA模型通过语义推理解析用户指令意图，生成场景相关物体的优先级抓取任务队列；Grounding DINO与动态A*规划算法协同实现导航与安全物体转移。为确保交接阶段的安全自然交互，系统采用MediaPipe驱动的人体中心控制器，通过实时人体姿态估计使无人机利用视觉伺服保持用户正前方的稳定交接位置。通过实际定位导航实验验证系统效能，最大误差、平均欧氏误差与均方根误差分别为0.164米、0.070米与0.084米，证明了VLA模型在空中操控任务中的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable intuitive control of aerial manipulators by non-experts, this research develops a system that interprets high-level natural language commands for object retrieval and delivery. The method integrates a Vision-Language-Action (VLA) model for semantic reasoning and task planning, Grounding DINO and a dynamic A* algorithm for object localization and navigation, and a MediaPipe-based human-centric controller for visual servoing during handover. Experimental results in real-world tests demonstrate the system&#x27;s localization and navigation accuracy, with maximum, mean Euclidean, and root-mean-squared errors of 0.164m, 0.070m, and 0.084m respectively, confirming the feasibility of using VLA models for aerial manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在为非专业用户设计直观的自然语言接口，以操控空中机械臂系统。提出的DroneVLA系统集成了视觉-语言-动作模型进行语义推理和任务规划，利用Grounding DINO进行物体检测，并采用动态A*算法进行导航，这些模块部署于配备夹爪和RGB-D相机的定制无人机上。系统通过基于MediaPipe的人体姿态估计实现视觉伺服，确保安全、自然的物体交接。真实环境实验验证了定位与导航的有效性，其最大误差、平均欧氏误差和均方根误差分别为0.164米、0.070米和0.084米，证明了该视觉-语言-动作框架在空中操控任务中的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">HoverAI: An Embodied Aerial Agent for Natural Human-Drone Interaction</div>
<div class="meta-line">Authors: Yuhua Jin, Nikita Kuzmin, Georgii Demianchuk, Mariya Lezina, Fawad Mehboob, Issatay Tokmurziyev, Miguel Altamirano Cabrera, Muhammad Ahsan Mustafa, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-20T09:59:49+00:00 · Latest: 2026-01-20T09:59:49+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13801v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13801v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Drones operating in human-occupied spaces suffer from insufficient communication mechanisms that create uncertainty about their intentions. We present HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI into a unified platform. Equipped with a MEMS laser projector, onboard semi-rigid screen, and RGB camera, HoverAI perceives users through vision and voice, responding via lip-synced avatars that adapt appearance to user demographics. The system employs a multimodal pipeline combining VAD, ASR (Whisper), LLM-based intent classification, RAG for dialogue, face analysis for personalization, and voice synthesis (XTTS v2). Evaluation demonstrates high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181). By uniting aerial robotics with adaptive conversational AI and self-contained visual output, HoverAI introduces a new class of spatially-aware, socially responsive embodied agents for applications in guidance, assistance, and human-centered interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HoverAI：一种用于自然人机交互的具身空中智能体</div>
<div class="mono" style="margin-top:8px">在人类活动空间运行的无人机常因通信机制不足而导致其意图不明确。本文提出HoverAI，一种集成了无人机移动性、独立于基础设施的视觉投影和实时对话AI的统一平台具身空中智能体。该系统配备MEMS激光投影仪、机载半刚性屏幕和RGB摄像头，通过视觉与语音感知用户，并利用口型同步的虚拟形象进行响应，其外观可根据用户人口统计特征自适应调整。系统采用多模态流程，结合语音活动检测、自动语音识别、基于大语言模型的意图分类、检索增强生成对话、个性化人脸分析及语音合成技术。评估显示其在指令识别、人口统计特征估计和语音转写方面均表现优异。通过融合空中机器人技术、自适应对话AI与自持式视觉输出，HoverAI开创了一类具有空间感知与社会响应能力的具身智能体，适用于导引、辅助及以人为中心的交互场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the communication gap and uncertainty about drone intentions in human-occupied spaces, this research introduces HoverAI, an embodied aerial agent that integrates drone mobility, infrastructure-independent visual projection, and real-time conversational AI. The method employs a multimodal pipeline combining voice activity detection, automatic speech recognition (Whisper), an LLM for intent classification, retrieval-augmented generation for dialogue, face analysis for personalization, and voice synthesis, all supported by a MEMS laser projector and onboard screen for displaying adaptive, lip-synced avatars. Experimental evaluation shows the system achieves high accuracy in command recognition (F1: 0.90), demographic estimation (gender F1: 0.89, age MAE: 5.14 years), and speech transcription (WER: 0.181), demonstrating its effectiveness as a spatially-aware, socially responsive agent for guidance and human-centered interaction.</div>
<div class="mono" style="margin-top:8px">针对无人机在人类环境中因通信机制不足而导致其意图不明确的问题，本研究提出了HoverAI，一种集成了无人机移动性、无基础设施视觉投影和实时对话AI的具身空中智能体。该方法采用多模态流程，结合MEMS激光投影仪、机载屏幕和RGB摄像头进行感知，并利用语音活动检测、Whisper进行语音识别、基于大语言模型的意图分类、检索增强生成进行对话、面部分析实现个性化以及XTTS v2进行语音合成，通过自适应口型同步的虚拟形象进行交互。实验结果表明，该系统在指令识别（F1分数：0.90）、人口统计估计（性别F1：0.89，年龄平均绝对误差：5.14岁）和语音转写（词错误率：0.181）方面均表现出高性能，验证了其在导引、辅助等空间感知、社会响应型应用中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sample Efficient Learning of Body-Environment Interaction of an Under-Actuated System</div>
<div class="meta-line">Authors: Zvi Chapnik, Yizhar Or, Shai Revzen</div>
<div class="meta-line">First: 2026-01-20T09:33:54+00:00 · Latest: 2026-01-20T09:33:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13777v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13777v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geometric mechanics provides valuable insights into how biological and robotic systems use changes in shape to move by mechanically interacting with their environment. In high-friction environments it provides that the entire interaction is captured by the ``motility map&#x27;&#x27;. Here we compare methods for learning the motility map from motion tracking data of a physical robot created specifically to test these methods by having under-actuated degrees of freedom and a hard to model interaction with its substrate. We compared four modeling approaches in terms of their ability to predict body velocity from shape change within the same gait, across gaits, and across speeds. Our results show a trade-off between simpler methods which are superior on small training datasets, and more sophisticated methods, which are superior when more training data is available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>欠驱动系统体-环境交互的样本高效学习</div>
<div class="mono" style="margin-top:8px">几何力学为理解生物与机器人系统如何通过形态变化与环境进行机械交互而运动提供了重要见解。在高摩擦环境中，整个交互过程可通过&#x27;运动性映射&#x27;完整刻画。本研究通过专门设计的物理机器人（具有欠驱动自由度且与基底存在难以建模的交互）的运动追踪数据，比较了学习该映射的方法。我们从相同步态内、跨步态及跨速度三个维度，评估了四种建模方法根据形态变化预测机体速度的能力。结果表明：简单方法在小训练数据集上表现更优，而复杂方法在训练数据充足时更具优势，二者存在权衡关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to efficiently learn body-environment interactions for under-actuated systems, where modeling is challenging. The method compares four modeling approaches for learning the motility map from motion tracking data of a physical robot, evaluating their ability to predict body velocity from shape changes. Key findings reveal a trade-off: simpler models perform better with small training datasets, while more sophisticated models excel when more data is available.</div>
<div class="mono" style="margin-top:8px">本研究旨在高效学习一个欠驱动机器人系统的体-环境交互模型（即运动性映射），该系统的交互复杂且难以进行解析建模。研究方法基于物理机器人的运动跟踪数据，比较了四种不同的建模方法，以根据形态变化预测身体速度。主要实验结果表明了一种权衡：在小型训练数据集上，更简单的模型表现更优；而当可获得更多训练数据时，更复杂的模型则能实现更优越的预测精度，该评估涵盖了相同步态、不同步态和不同速度下的情况。</div>
</details>
</div>
<div class="card">
<div class="title">RIM Hand : A Robotic Hand with an Accurate Carpometacarpal Joint and Nitinol-Supported Skeletal Structure</div>
<div class="meta-line">Authors: Joon Lee, Jeongyoon Han, Doyoung Kim, Seokhwan Jeong</div>
<div class="meta-line">First: 2026-01-20T08:48:04+00:00 · Latest: 2026-01-20T08:48:04+00:00</div>
<div class="meta-line">Comments: Soft Robotics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13737v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13737v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the flexible RIM Hand, a biomimetic robotic hand that precisely replicates the carpometacarpal (CMC) joints and employs superelastic Nitinol wires throughout its skeletal framework. By modeling the full carpal-to-metacarpal anatomy, the design enables realistic palm deformation through tendon-driven fingers while enhancing joint restoration and supports skeletal structure with Nitinol-based dorsal extensors. A flexible silicone skin further increases contact friction and contact area, enabling stable grasps for diverse objects. Experiments show that the palm can deform up to 28%, matching human hand flexibility, while achieving more than twice the payload capacity and three times the contact area compared to a rigid palm design. The RIM Hand thus offers improved dexterity, compliance, and anthropomorphism, making it promising for prosthetic and service-robot applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RIM Hand：具备精确腕掌关节与镍钛合金支撑骨骼结构的仿生机械手</div>
<div class="mono" style="margin-top:8px">本文提出柔性RIM Hand——一种精确复现腕掌关节并在骨骼框架中全面采用超弹性镍钛合金线的仿生机械手。通过完整建模腕骨至掌骨解剖结构，该设计通过腱驱动手指实现逼真的手掌形变，同时增强关节复位功能，并采用镍钛合金基背侧伸肌支撑骨骼结构。柔性硅胶皮肤进一步增加接触摩擦与接触面积，实现对多样化物体的稳定抓握。实验表明，该手掌形变可达28%，匹配人手灵活性，同时相比刚性手掌设计实现两倍以上负载能力与三倍接触面积。RIM Hand因此展现出更强的灵巧性、顺应性与拟人化特性，在假肢与服务机器人领域具有应用前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance robotic hand dexterity and anthropomorphism for prosthetic and service applications, this study introduces the RIM Hand, which biomimetically replicates the carpometacarpal joint using a skeletal structure with superelastic Nitinol wires and a tendon-driven design for palm deformation. The method integrates a flexible silicone skin to increase contact friction and area. Experimental results demonstrate that the palm achieves 28% deformation, matching human flexibility, and exhibits over twice the payload capacity and three times the contact area compared to rigid palm designs.</div>
<div class="mono" style="margin-top:8px">为提升假肢和服务机器人应用中机械手的灵巧性和拟人化程度，本研究提出了RIM Hand，这是一种仿生设计，精确复制了腕掌关节，并在骨骼结构中采用了超弹性镍钛诺线支撑，同时配备了柔性硅胶皮肤。该方法通过建模完整的腕骨到掌骨解剖结构，利用腱驱动手指实现逼真的手掌形变，而镍钛诺基背侧伸肌则提供关节复位和结构支撑。实验结果表明，该手掌可实现高达28%的形变，与人类手掌的灵活性相匹配，并且与刚性手掌设计相比，其负载能力提高了一倍以上，接触面积增加了三倍，从而能够稳定抓取多种物体。</div>
</details>
</div>
<div class="card">
<div class="title">SUNSET -- A Sensor-fUsioN based semantic SegmEnTation exemplar for ROS-based self-adaptation</div>
<div class="meta-line">Authors: Andreas Wiedholz, Rafael Paintner, Julian Gleißner, Alwin Hoffmann, Tobias Huber</div>
<div class="meta-line">First: 2026-01-20T08:40:57+00:00 · Latest: 2026-01-20T08:40:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13732v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13732v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The fact that robots are getting deployed more often in dynamic environments, together with the increasing complexity of their software systems, raises the need for self-adaptive approaches. In these environments robotic software systems increasingly operate amid (1) uncertainties, where symptoms are easy to observe but root causes are ambiguous, or (2) multiple uncertainties appear concurrently. We present SUNSET, a ROS2-based exemplar that enables rigorous, repeatable evaluation of architecture-based self-adaptation in such conditions. It implements a sensor fusion semantic-segmentation pipeline driven by a trained Machine Learning (ML) model whose input preprocessing can be perturbed to induce realistic performance degradations. The exemplar exposes five observable symptoms, where each can be caused by different root causes and supports concurrent uncertainties spanning self-healing and self-optimisation. SUNSET includes the segmentation pipeline, a trained ML model, uncertainty-injection scripts, a baseline controller, and step-by-step integration and evaluation documentation to facilitate reproducible studies and fair comparison.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SUNSET——基于ROS自适应系统的传感器融合语义分割范例</div>
<div class="mono" style="margin-top:8px">随着机器人在动态环境中的部署日益频繁及其软件系统复杂度的提升，对自适应方法的需求愈发迫切。在此类环境中，机器人软件系统常面临两种情形：(1) 不确定性场景——现象易观测但根源难以追溯；(2) 多重并发的不确定性。本文提出SUNSET，这是一个基于ROS2的范例系统，支持在此类条件下对基于架构的自适应方法进行严谨可复现的评估。该系统实现了由训练完成的机器学习模型驱动的传感器融合语义分割流程，可通过扰动输入预处理模块来模拟真实的性能退化场景。该范例呈现五种可观测现象，每种现象可由不同根源引发，并支持涵盖自修复与自优化的并发不确定性场景。SUNSET包含完整分割流程、预训练ML模型、不确定性注入脚本、基线控制器，以及逐步集成与评估文档，以保障研究的可复现性与公平比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for self-adaptive robotic systems in dynamic environments where multiple or ambiguous uncertainties can arise, this paper introduces SUNSET, a ROS2-based exemplar for evaluating architecture-based self-adaptation. The method implements a sensor fusion semantic segmentation pipeline driven by a trained machine learning model, with scripts to perturb input preprocessing and induce realistic performance degradations, exposing five observable symptoms linked to various root causes. Key experimental findings demonstrate that the exemplar supports concurrent uncertainties for self-healing and self-optimization, providing a reproducible framework with a baseline controller and detailed documentation to facilitate rigorous and fair comparative studies.</div>
<div class="mono" style="margin-top:8px">为应对动态环境中机器人系统面临多重或模糊不确定性时对自适应方法的需求，本文提出了SUNSET，一个基于ROS2的范例，用于评估基于架构的自适应。该方法实现了一个由训练好的机器学习模型驱动的传感器融合语义分割流水线，并通过脚本扰动输入预处理以引发真实的性能退化，从而暴露出与不同根本原因相关的五种可观测症状。实验结果表明，该范例支持针对自愈和自我优化的并发不确定性，并提供了包含基线控制器和详细文档的可复现框架，以促进自适应研究中严谨且公平的比较。</div>
</details>
</div>
<div class="card">
<div class="title">Omni-LIVO: Robust RGB-Colored Multi-Camera Visual-Inertial-LiDAR Odometry via Photometric Migration and ESIKF Fusion</div>
<div class="meta-line">Authors: Yinong Cao, Chenyang Zhang, Xin He, Yuwei Chen, Chengyu Pu, Bingtao Wang, Kaile Wu, Shouzheng Zhu, Fei Han, Shijie Liu, Chunlai Li, Jianyu Wang</div>
<div class="meta-line">First: 2025-09-19T06:48:49+00:00 · Latest: 2026-01-20T07:29:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15673v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.15673v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Wide field-of-view (FoV) LiDAR sensors provide dense geometry across large environments, but existing LiDAR-inertial-visual odometry (LIVO) systems generally rely on a single camera, limiting their ability to fully exploit LiDAR-derived depth for photometric alignment and scene colorization. We present Omni-LIVO, a tightly coupled multi-camera LIVO system that leverages multi-view observations to comprehensively utilize LiDAR geometric information across extended spatial regions. Omni-LIVO introduces a Cross-View direct alignment strategy that maintains photometric consistency across non-overlapping views, and extends the Error-State Iterated Kalman Filter (ESIKF) with multi-view updates and adaptive covariance. The system is evaluated on public benchmarks and our custom dataset, showing improved accuracy and robustness over state-of-the-art LIVO, LIO, and visual-inertial SLAM baselines. Code and dataset will be released upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Omni-LIVO：通过光度迁移与ESIKF融合实现鲁棒性RGB彩色多相机视觉-惯性-激光雷达里程计</div>
<div class="mono" style="margin-top:8px">宽视场激光雷达传感器可在广阔环境中提供密集几何信息，但现有激光雷达-惯性-视觉里程计系统通常依赖单相机，限制了其充分利用激光雷达深度进行光度对齐与场景着色的能力。本文提出Omni-LIVO，一种紧耦合多相机LIVO系统，通过多视角观测全面利用扩展空间区域的激光雷达几何信息。该系统引入跨视角直接对齐策略以保持非重叠视角间的光度一致性，并扩展误差状态迭代卡尔曼滤波器以支持多视角更新与自适应协方差。在公开基准与定制数据集上的评估表明，该系统在精度与鲁棒性上优于当前最先进的LIVO、LIO及视觉-惯性SLAM基线方法。代码与数据集将在发表时开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitation of single-camera LIVO systems in fully utilizing wide-FoV LiDAR geometry for photometric alignment and scene colorization, this work proposes Omni-LIVO, a tightly coupled multi-camera visual-inertial-LiDAR odometry system. The method introduces a Cross-View direct alignment strategy to maintain photometric consistency across non-overlapping camera views and extends the Error-State Iterated Kalman Filter with multi-view updates and adaptive covariance. Experimental evaluations on public benchmarks and a custom dataset demonstrate that Omni-LIVO achieves improved accuracy and robustness compared to state-of-the-art LIVO, LIO, and visual-inertial SLAM baselines.</div>
<div class="mono" style="margin-top:8px">针对现有单相机激光雷达-惯性-视觉里程计系统难以充分利用激光雷达深度进行光度对齐和场景着色的问题，本研究提出了Omni-LIVO，一种紧耦合的多相机激光雷达-惯性-视觉里程计系统。该方法引入了跨视图直接对齐策略，以在非重叠视图间保持光度一致性，并扩展了误差状态迭代卡尔曼滤波器，融入了多视图更新和自适应协方差。在公开基准和自定义数据集上的实验评估表明，该系统相比先进的LIVO、LIO和视觉-惯性SLAM基线方法，在精度和鲁棒性上均有提升。</div>
</details>
</div>
<div class="card">
<div class="title">A0: An Affordance-Aware Hierarchical Model for General Robotic Manipulation</div>
<div class="meta-line">Authors: Rongtao Xu, Jian Zhang, Minghao Guo, Youpeng Wen, Haoting Yang, Min Lin, Jianzheng Huang, Zhe Li, Kaidong Zhang, Liqiong Wang, Yuxuan Kuang, Meng Cao, Feng Zheng, Xiaodan Liang</div>
<div class="meta-line">First: 2025-04-17T04:45:15+00:00 · Latest: 2026-01-20T07:25:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.12636v5">Abs</a> · <a href="https://arxiv.org/pdf/2504.12636v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation faces critical challenges in understanding spatial affordances--the &quot;where&quot; and &quot;how&quot; of object interactions--essential for complex manipulation tasks like wiping a board or stacking objects. Existing methods, including modular-based and end-to-end approaches, often lack robust spatial reasoning capabilities. Unlike recent point-based and flow-based affordance methods that focus on dense spatial representations or trajectory modeling, we propose A0, a hierarchical affordance-aware diffusion model that decomposes manipulation tasks into high-level spatial affordance understanding and low-level action execution. A0 leverages the Embodiment-Agnostic Affordance Representation, which captures object-centric spatial affordances by predicting contact points and post-contact trajectories. A0 is pre-trained on 1 million contact points data and fine-tuned on annotated trajectories, enabling generalization across platforms. Key components include Position Offset Attention for motion-aware feature extraction and a Spatial Information Aggregation Layer for precise coordinate mapping. The model&#x27;s output is executed by the action execution module. Experiments on multiple robotic systems (Franka, Kinova, Realman, and Dobot) demonstrate A0&#x27;s superior performance in complex tasks, showcasing its efficiency, flexibility, and real-world applicability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A0：一种面向通用机器人操作的具身感知分层模型</div>
<div class="mono" style="margin-top:8px">机器人操作在理解空间可供性——即物体交互的“位置”与“方式”——方面面临关键挑战，这对于擦拭白板或堆叠物体等复杂操作任务至关重要。现有方法（包括模块化与端到端方案）常缺乏鲁棒的空间推理能力。不同于近期基于点或流、侧重于稠密空间表征或轨迹建模的可供性方法，我们提出A0——一种分层式可供性感知扩散模型，将操作任务分解为高层空间可供性理解与低层动作执行。A0采用具身无关可供性表征，通过预测接触点及接触后轨迹来捕捉以物体为中心的空间可供性。该模型基于百万级接触点数据预训练，并通过标注轨迹微调，实现了跨平台泛化。其核心组件包括用于运动感知特征提取的位置偏移注意力机制，以及实现精确坐标映射的空间信息聚合层。模型输出由动作执行模块实施。在多种机器人系统（Franka、Kinova、Realman、Dobot）上的实验表明，A0在复杂任务中具有卓越性能，展现出高效性、灵活性与实际应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of robust spatial reasoning in robotic manipulation, where existing methods often lack the ability to understand the spatial affordances of objects, this paper introduces A0, a hierarchical affordance-aware diffusion model. The method decomposes tasks by first understanding object-centric spatial affordances through an Embodiment-Agnostic Affordance Representation, which predicts contact points and trajectories, and then executing actions; it is pre-trained on extensive contact data and fine-tuned on trajectories using components like Position Offset Attention. Experimental results across multiple robotic platforms demonstrate that A0 achieves superior performance in complex manipulation tasks, highlighting its efficiency and real-world applicability.</div>
<div class="mono" style="margin-top:8px">机器人操作需要强大的空间推理能力来理解物体可供性，但现有方法常缺乏这一能力。为此，研究者提出了A0，一种分层可供性感知扩散模型，它将任务分解为高层空间可供性理解和低层动作执行，利用一种与具身无关的可供性表示来预测接触点和接触后轨迹。在多个机器人平台上的实验表明，A0在复杂操作任务中实现了卓越性能，展现了其高效性、灵活性和实际应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Communication-Free Collective Navigation for a Swarm of UAVs via LiDAR-Based Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Myong-Yol Choi, Hankyoul Ko, Hanse Cho, Changseung Kim, Seunghwan Kim, Jaemin Seo, Hyondong Oh</div>
<div class="meta-line">First: 2026-01-20T06:46:09+00:00 · Latest: 2026-01-20T06:46:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13657v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13657v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a deep reinforcement learning (DRL) based controller for collective navigation of unmanned aerial vehicle (UAV) swarms in communication-denied environments, enabling robust operation in complex, obstacle-rich environments. Inspired by biological swarms where informed individuals guide groups without explicit communication, we employ an implicit leader-follower framework. In this paradigm, only the leader possesses goal information, while follower UAVs learn robust policies using only onboard LiDAR sensing, without requiring any inter-agent communication or leader identification. Our system utilizes LiDAR point clustering and an extended Kalman filter for stable neighbor tracking, providing reliable perception independent of external positioning systems. The core of our approach is a DRL controller, trained in GPU-accelerated Nvidia Isaac Sim, that enables followers to learn complex emergent behaviors - balancing flocking and obstacle avoidance - using only local perception. This allows the swarm to implicitly follow the leader while robustly addressing perceptual challenges such as occlusion and limited field-of-view. The robustness and sim-to-real transfer of our approach are confirmed through extensive simulations and challenging real-world experiments with a swarm of five UAVs, which successfully demonstrated collective navigation across diverse indoor and outdoor environments without any communication or external localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于激光雷达深度强化学习的无人机集群无通信协同导航</div>
<div class="mono" style="margin-top:8px">本文提出一种基于深度强化学习（DRL）的控制器，用于无人机集群在通信受限环境中的协同导航，使其能在复杂多障碍物环境中稳定运行。受生物集群中个体无需显式通信即可引导群体的启发，我们采用隐式领导者-跟随者框架：仅领导者掌握目标信息，跟随无人机仅通过机载激光雷达感知学习鲁棒策略，无需个体间通信或领导者识别。系统利用激光雷达点云聚类与扩展卡尔曼滤波器实现稳定的邻居跟踪，提供不依赖外部定位系统的可靠感知。方法核心是通过GPU加速的Nvidia Isaac Sim训练的DRL控制器，使跟随者仅凭局部感知即可学习复杂涌现行为（平衡集群聚集与避障）。这使得集群能隐式跟随领导者，并鲁棒应对遮挡与视场受限等感知挑战。通过大量仿真及五架无人机集群的实地实验，验证了本方法的鲁棒性与仿真到现实的迁移能力，成功实现了在多样室内外环境中的无通信、无外部定位协同导航。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling collective navigation for UAV swarms in communication-denied environments, motivated by the need for robust operation in complex, obstacle-rich settings without relying on inter-agent communication. The method employs a deep reinforcement learning (DRL) controller within an implicit leader-follower framework, where only the leader has goal information; followers learn policies using only onboard LiDAR sensing, enhanced by point clustering and an extended Kalman filter for stable neighbor tracking. Experimental validation through extensive simulations and real-world tests with a five-UAV swarm demonstrated successful collective navigation across diverse indoor and outdoor environments, confirming the system&#x27;s robustness and effective sim-to-real transfer without communication or external localization.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决无人机群在通信受限环境中的集体导航问题，其动机是在复杂、障碍物密集的场景下实现不依赖个体间通信的鲁棒操作。该方法采用基于深度强化学习的控制器，并构建了一个隐式领导者-跟随者框架，其中仅领导者掌握目标信息；跟随者仅利用机载激光雷达感知，通过点云聚类和扩展卡尔曼滤波实现稳定的邻居跟踪来学习策略。在仿真和包含五架无人机的真实世界实验中，该方法成功展示了在多样室内外环境中的集体导航能力，验证了这种无通信方法的鲁棒性及其有效的仿真到现实迁移效果。</div>
</details>
</div>
<div class="card">
<div class="title">A General One-Shot Multimodal Active Perception Framework for Robotic Manipulation: Learning to Predict Optimal Viewpoint</div>
<div class="meta-line">Authors: Deyun Qin, Zezhi Liu, Hanqian Luo, Xiao Liang, Yongchun Fang</div>
<div class="meta-line">First: 2026-01-20T06:12:53+00:00 · Latest: 2026-01-20T06:12:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13639v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active perception in vision-based robotic manipulation aims to move the camera toward more informative observation viewpoints, thereby providing high-quality perceptual inputs for downstream tasks. Most existing active perception methods rely on iterative optimization, leading to high time and motion costs, and are tightly coupled with task-specific objectives, which limits their transferability. In this paper, we propose a general one-shot multimodal active perception framework for robotic manipulation. The framework enables direct inference of optimal viewpoints and comprises a data collection pipeline and an optimal viewpoint prediction network. Specifically, the framework decouples viewpoint quality evaluation from the overall architecture, supporting heterogeneous task requirements. Optimal viewpoints are defined through systematic sampling and evaluation of candidate viewpoints, after which large-scale training datasets are constructed via domain randomization. Moreover, a multimodal optimal viewpoint prediction network is developed, leveraging cross-attention to align and fuse multimodal features and directly predict camera pose adjustments. The proposed framework is instantiated in robotic grasping under viewpoint-constrained environments. Experimental results demonstrate that active perception guided by the framework significantly improves grasp success rates. Notably, real-world evaluations achieve nearly double the grasp success rate and enable seamless sim-to-real transfer without additional fine-tuning, demonstrating the effectiveness of the proposed framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向机器人操作的单次多模态主动感知通用框架：学习预测最优视点</div>
<div class="mono" style="margin-top:8px">基于视觉的机器人操作中的主动感知旨在将相机移动到信息更丰富的观测视点，从而为下游任务提供高质量的感知输入。现有主动感知方法多依赖迭代优化，导致时间和运动成本高昂，且与任务特定目标紧密耦合，限制了其可迁移性。本文提出一种面向机器人操作的单次多模态主动感知通用框架，可直接推断最优视点，包含数据收集流程和最优视点预测网络。该框架将视点质量评估与整体架构解耦，支持异构任务需求：通过系统采样和评估候选视点定义最优视点，再通过领域随机化构建大规模训练数据集。同时开发了多模态最优视点预测网络，利用交叉注意力对齐融合多模态特征，直接预测相机位姿调整。该框架在视点受限环境下的机器人抓取任务中实例化，实验表明框架引导的主动感知显著提升了抓取成功率。真实环境评估中抓取成功率提升近一倍，且无需额外微调即可实现仿真到实物的无缝迁移，验证了框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing active perception methods in robotic manipulation, which often rely on iterative optimization and are tightly coupled with specific tasks, leading to high computational costs and poor transferability. The authors propose a general one-shot framework that decouples viewpoint quality evaluation from the overall architecture, enabling direct inference of optimal camera viewpoints through a data collection pipeline involving systematic sampling and domain randomization, and a multimodal prediction network that uses cross-attention to fuse features and predict pose adjustments. Experimental results in robotic grasping within viewpoint-constrained environments show that the framework significantly improves grasp success rates, nearly doubling performance in real-world evaluations and enabling effective sim-to-real transfer without fine-tuning.</div>
<div class="mono" style="margin-top:8px">针对基于视觉的机器人主动感知中迭代优化方法效率低、可迁移性差的问题，本文提出了一种通用的单次多模态框架来预测最优相机视点。该方法将视点质量评估与整体架构解耦，通过系统化视点采样和领域随机化构建大规模训练数据集，并采用一个利用交叉注意力融合特征、直接预测相机位姿调整的多模态网络。在视点受限环境下的机器人抓取任务中进行的实验表明，该框架指导的主动感知显著提升了抓取成功率，在真实世界测试中性能提升近一倍，且无需微调即可实现有效的仿真到现实迁移。</div>
</details>
</div>
<div class="card">
<div class="title">DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition</div>
<div class="meta-line">Authors: Yuki Kadokawa, Jonas Frey, Takahiro Miki, Takamitsu Matsubara, Marco Hutter</div>
<div class="meta-line">First: 2025-05-09T18:01:56+00:00 · Latest: 2026-01-20T04:10:00+00:00</div>
<div class="meta-line">Comments: Accepted for IEEE Robotics &amp; Automation Magazine (RAM)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.06357v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.06357v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-based Reinforcement Learning (PbRL) enables policy learning through simple queries comparing trajectories from a single policy. While human responses to these queries make it possible to learn policies aligned with human preferences, PbRL suffers from low query efficiency, as policy bias limits trajectory diversity and reduces the number of discriminable queries available for learning preferences. This paper identifies preference discriminability, which quantifies how easily a human can judge which trajectory is closer to their ideal behavior, as a key metric for improving query efficiency. To address this, we move beyond comparisons within a single policy and instead generate queries by comparing trajectories from multiple policies, as training them from scratch promotes diversity without policy bias. We propose Discriminability-Aware Policy-to-Policy Preference-Based Efficient Reinforcement Learning (DAPPER), which integrates preference discriminability with trajectory diversification achieved by multiple policies. DAPPER trains new policies from scratch after each reward update and employs a discriminator that learns to estimate preference discriminability, enabling the prioritized sampling of more discriminable queries. During training, it jointly maximizes the preference reward and preference discriminability score, encouraging the discovery of highly rewarding and easily distinguishable policies. Experiments in simulated and real-world legged robot environments demonstrate that DAPPER outperforms previous methods in query efficiency, particularly under challenging preference discriminability conditions. A supplementary video that facilitates understanding of the proposed framework and its experimental results is available at: https://youtu.be/lRwX8FNN8n4</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DAPPER：面向查询高效机器人技能习得的可区分性感知策略间偏好强化学习</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PbRL）通过比较单一策略产生的轨迹进行简单查询来实现策略学习。虽然人类对这些查询的反馈使得学习符合人类偏好的策略成为可能，但PbRL存在查询效率低下的问题，因为策略偏差会限制轨迹多样性，并减少可用于学习偏好的可区分查询数量。本文提出偏好可区分性作为提升查询效率的关键指标，该指标量化了人类判断哪条轨迹更接近理想行为的难易程度。为解决此问题，我们超越单一策略内的比较，转而通过比较多个策略产生的轨迹生成查询，因为从头训练这些策略可在无策略偏差的情况下促进多样性。我们提出可区分性感知策略间偏好高效强化学习（DAPPER），将偏好可区分性与多策略实现的轨迹多样化相结合。DAPPER在每次奖励更新后从头训练新策略，并采用学习估计偏好可区分性的判别器，从而优先采样更具可区分性的查询。在训练过程中，它同时最大化偏好奖励和偏好可区分性分数，鼓励发现高奖励且易于区分的策略。在模拟和真实世界足式机器人环境中的实验表明，DAPPER在查询效率上优于现有方法，尤其在具有挑战性的偏好可区分性条件下。辅助视频详见：https://youtu.be/lRwX8FNN8n4</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Preference-based Reinforcement Learning (PbRL) enables policy learning from human comparisons but suffers from low query efficiency due to limited trajectory diversity from a single policy, which reduces the availability of easily discriminable queries. To address this, the paper introduces DAPPER, a method that generates queries by comparing trajectories from multiple policies trained from scratch to enhance diversity and integrates a learned discriminator to estimate and prioritize sampling of queries with high preference discriminability, thereby jointly maximizing reward and discriminability during training. Experiments in simulated and real-world legged robot tasks show that DAPPER achieves superior query efficiency compared to prior methods, especially under conditions where preferences are difficult to distinguish.</div>
<div class="mono" style="margin-top:8px">本研究针对基于偏好的强化学习（PbRL）查询效率低的问题，指出偏好可区分性是关键因素，其中策略偏差会限制轨迹多样性并减少易于区分的查询数量。所提出的DAPPER方法通过比较多个从头开始训练的、旨在增强多样性的策略所产生的轨迹来生成查询，从而提高效率；该方法使用一个学习到的判别器来估计并优先采样高度可区分的查询，同时联合最大化奖励和可区分性。在模拟和真实世界足式机器人任务中的实验结果表明，与现有方法相比，DAPPER实现了更高的查询效率，尤其在偏好可区分性较低的情况下表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">Highly Deformable Proprioceptive Membrane for Real-Time 3D Shape Reconstruction</div>
<div class="meta-line">Authors: Guanyu Xu, Jiaqi Wang, Dezhong Tong, Xiaonan Huang</div>
<div class="meta-line">First: 2026-01-20T03:59:21+00:00 · Latest: 2026-01-20T03:59:21+00:00</div>
<div class="meta-line">Comments: 13 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13574v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13574v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing the three-dimensional (3D) geometry of object surfaces is essential for robot perception, yet vision-based approaches are generally unreliable under low illumination or occlusion. This limitation motivates the design of a proprioceptive membrane that conforms to the surface of interest and infers 3D geometry by reconstructing its own deformation. Conventional shape-aware membranes typically rely on resistive, capacitive, or magneto-sensitive mechanisms. However, these methods often encounter challenges such as structural complexity, limited compliance during large-scale deformation, and susceptibility to electromagnetic interference. This work presents a soft, flexible, and stretchable proprioceptive silicone membrane based on optical waveguide sensing. The membrane sensor integrates edge-mounted LEDs and centrally distributed photodiodes (PDs), interconnected via liquid-metal traces embedded within a multilayer elastomeric composite. Rich deformation-dependent light intensity signals are decoded by a data-driven model to recover the membrane geometry as a 3D point cloud. On a customized 140 mm square membrane, real-time reconstruction of large-scale out-of-plane deformation is achieved at 90 Hz with an average reconstruction error of 1.3 mm, measured by Chamfer distance, while maintaining accuracy for indentations up to 25 mm. The proposed framework provides a scalable, robust, and low-profile solution for global shape perception in deformable robotic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于实时三维形状重建的高变形本体感知膜</div>
<div class="mono" style="margin-top:8px">重建物体表面的三维几何形态对机器人感知至关重要，但基于视觉的方法在低光照或遮挡条件下通常不可靠。这一局限促使设计一种能贴合目标表面、通过重建自身变形来推断三维几何的本体感知膜。传统形状感知膜通常依赖电阻、电容或磁敏机制，但这些方法常面临结构复杂、大尺度变形时顺应性有限、易受电磁干扰等挑战。本研究提出一种基于光波导传感的柔软、柔性且可拉伸的本体感知硅胶膜。该膜传感器集成边缘安装的LED和中心分布的光电二极管，通过嵌入多层弹性体复合材料内的液态金属迹线相互连接。通过数据驱动模型解码丰富的变形相关光强信号，将膜几何形态重建为三维点云。在定制的140毫米方形膜上，实现了90赫兹的大尺度离面变形实时重建，以倒角距离测量的平均重建误差为1.3毫米，同时对深达25毫米的压痕保持精度。该框架为可变形机器人系统的全局形状感知提供了可扩展、鲁棒且低剖面的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitations of vision-based 3D reconstruction in poor lighting or occluded conditions, this research develops a soft, proprioceptive membrane for inferring object surface geometry by sensing its own deformation. The method employs a silicone membrane with embedded optical waveguides, using edge-mounted LEDs and centrally distributed photodiodes connected by liquid-metal traces; a data-driven model decodes deformation-dependent light signals to reconstruct the 3D shape as a point cloud. Experimental results on a 140 mm square membrane demonstrate real-time reconstruction at 90 Hz with an average error of 1.3 mm (Chamfer distance), accurately capturing indentations up to 25 mm.</div>
<div class="mono" style="margin-top:8px">为解决基于视觉的三维形状重建在弱光或遮挡条件下不可靠的问题，本研究开发了一种柔软的本体感知膜，通过感知自身形变来推断物体表面几何形状。该方法采用多层硅胶膜，内部嵌入由液态金属迹线连接、边缘安装LED和中心分布光电二极管的光波导传感网络，并通过数据驱动模型解码形变调制光信号以点云形式重建三维形状。在140毫米方形膜上的实验结果表明，该系统能以90赫兹频率实时重建形状，平均误差为1.3毫米（Chamfer距离），并能准确捕捉高达25毫米的平面外压痕。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Fine-Grained Correspondence with Cross-Perspective Perception for Open-Vocabulary 6D Object Pose Estimation</div>
<div class="meta-line">Authors: Yu Qin, Shimeng Fan, Fan Yang, Zixuan Xue, Zijie Mai, Wenrui Chen, Kailun Yang, Zhiyong Li</div>
<div class="meta-line">First: 2026-01-20T03:48:54+00:00 · Latest: 2026-01-20T03:48:54+00:00</div>
<div class="meta-line">Comments: The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13565v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13565v1">PDF</a> · <a href="https://github.com/zjjqinyu/FiCoP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary 6D object pose estimation empowers robots to manipulate arbitrary unseen objects guided solely by natural language. However, a critical limitation of existing approaches is their reliance on unconstrained global matching strategies. In open-world scenarios, trying to match anchor features against the entire query image space introduces excessive ambiguity, as target features are easily confused with background distractors. To resolve this, we propose Fine-grained Correspondence Pose Estimation (FiCoP), a framework that transitions from noise-prone global matching to spatially-constrained patch-level correspondence. Our core innovation lies in leveraging a patch-to-patch correlation matrix as a structural prior to narrowing the matching scope, effectively filtering out irrelevant clutter to prevent it from degrading pose estimation. Firstly, we introduce an object-centric disentanglement preprocessing to isolate the semantic target from environmental noise. Secondly, a Cross-Perspective Global Perception (CPGP) module is proposed to fuse dual-view features, establishing structural consensus through explicit context reasoning. Finally, we design a Patch Correlation Predictor (PCP) that generates a precise block-wise association map, acting as a spatial filter to enforce fine-grained, noise-resilient matching. Experiments on the REAL275 and Toyota-Light datasets demonstrate that FiCoP improves Average Recall by 8.0% and 6.1%, respectively, compared to the state-of-the-art method, highlighting its capability to deliver robust and generalized perception for robotic agents operating in complex, unconstrained open-world environments. The source code will be made publicly available at https://github.com/zjjqinyu/FiCoP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于跨视角感知的细粒度对应学习用于开放词汇6D物体姿态估计</div>
<div class="mono" style="margin-top:8px">开放词汇6D物体姿态估计使机器人能够仅通过自然语言指导操作任意未见物体。然而，现有方法的关键局限在于依赖无约束的全局匹配策略。在开放世界场景中，尝试将锚点特征与整个查询图像空间匹配会引入过度歧义，因为目标特征易与背景干扰物混淆。为解决此问题，我们提出细粒度对应姿态估计框架FiCoP，将匹配方式从易受噪声影响的全局匹配转变为空间受限的块级对应。核心创新在于利用块间关联矩阵作为结构先验来缩小匹配范围，有效滤除无关杂波以防止其降低姿态估计精度。首先，我们引入以物体为中心的解缠预处理以分离语义目标与环境噪声。其次，提出跨视角全局感知模块融合双视图特征，通过显式上下文推理建立结构共识。最后，设计块关联预测器生成精确的块级关联图，作为空间过滤器实现细粒度且抗噪声的匹配。在REAL275和Toyota-Light数据集上的实验表明，FiCoP相比最先进方法将平均召回率分别提升8.0%和6.1%，凸显了其在复杂无约束开放世界环境中为机器人提供鲁棒泛化感知的能力。源代码将在https://github.com/zjjqinyu/FiCoP公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing open-vocabulary 6D object pose estimation methods suffer from ambiguity due to unconstrained global feature matching, where target features are easily confused with background clutter. To address this, the proposed FiCoP framework shifts to spatially-constrained patch-level correspondence by using a patch-to-patch correlation matrix as a structural prior to narrow the matching scope. The method involves object-centric disentanglement preprocessing, a Cross-Perspective Global Perception module for dual-view feature fusion, and a Patch Correlation Predictor to generate a block-wise association map for fine-grained matching. Experiments on REAL275 and Toyota-Light datasets show FiCoP improves Average Recall by 8.0% and 6.1% respectively over the state-of-the-art, demonstrating robust performance in complex open-world environments.</div>
<div class="mono" style="margin-top:8px">现有的开放词汇6D物体姿态估计方法因采用无约束的全局特征匹配而存在歧义问题，目标特征易受背景干扰。为解决此问题，本文提出的FiCoP框架转向基于块级对应的方法，利用块到块的相关性矩阵作为结构先验来约束匹配范围并滤除无关噪声。该方法包括以物体为中心的解缠预处理、用于双视角特征融合的跨视角全局感知模块，以及生成精确空间关联图的块相关性预测器。在REAL275和Toyota-Light数据集上的实验表明，FiCoP将平均召回率分别提升了8.0%和6.1%，优于现有最优方法，展现了在复杂开放世界环境中的鲁棒性能。</div>
</details>
</div>
<div class="card">
<div class="title">LogicEnvGen: Task-Logic Driven Generation of Diverse Simulated Environments for Embodied AI</div>
<div class="meta-line">Authors: Jianan Wang, Siyang Zhang, Bin Li, Juan Chen, Jingtao Qi, Zhuo Zhang, Chen Qian</div>
<div class="meta-line">First: 2026-01-20T03:28:07+00:00 · Latest: 2026-01-20T03:28:07+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13556v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13556v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulated environments play an essential role in embodied AI, functionally analogous to test cases in software engineering. However, existing environment generation methods often emphasize visual realism (e.g., object diversity and layout coherence), overlooking a crucial aspect: logical diversity from the testing perspective. This limits the comprehensive evaluation of agent adaptability and planning robustness in distinct simulated environments. To bridge this gap, we propose LogicEnvGen, a novel method driven by Large Language Models (LLMs) that adopts a top-down paradigm to generate logically diverse simulated environments as test cases for agents. Given an agent task, LogicEnvGen first analyzes its execution logic to construct decision-tree-structured behavior plans and then synthesizes a set of logical trajectories. Subsequently, it adopts a heuristic algorithm to refine the trajectory set, reducing redundant simulation. For each logical trajectory, which represents a potential task situation, LogicEnvGen correspondingly instantiates a concrete environment. Notably, it employs constraint solving for physical plausibility. Furthermore, we introduce LogicEnvEval, a novel benchmark comprising four quantitative metrics for environment evaluation. Experimental results verify the lack of logical diversity in baselines and demonstrate that LogicEnvGen achieves 1.04-2.61x greater diversity, significantly improving the performance in revealing agent faults by 4.00%-68.00%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LogicEnvGen：面向具身AI的任务逻辑驱动多样化模拟环境生成</div>
<div class="mono" style="margin-top:8px">模拟环境在具身AI中扮演着关键角色，其功能类似于软件工程中的测试用例。然而，现有环境生成方法往往侧重于视觉真实性（如物体多样性和布局一致性），却忽视了测试视角下的一个关键维度：逻辑多样性。这限制了对智能体在不同模拟环境中适应性与规划鲁棒性的全面评估。为弥补这一不足，我们提出LogicEnvGen——一种由大语言模型驱动的创新方法，采用自上而下的范式，生成逻辑多样化的模拟环境作为智能体测试用例。给定智能体任务后，LogicEnvGen首先分析其执行逻辑以构建决策树结构的行为规划，进而合成一组逻辑轨迹；随后采用启发式算法优化轨迹集以减少冗余模拟。每条表征潜在任务情境的逻辑轨迹，都会对应实例化为具体环境，并特别通过约束求解确保物理合理性。此外，我们提出包含四项量化指标的新型评估基准LogicEnvEval。实验结果验证了基线方法在逻辑多样性上的不足，并表明LogicEnvGen可实现1.04-2.61倍的多样性提升，在揭示智能体缺陷方面的性能显著提高4.00%-68.00%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to evaluate embodied AI agents more comprehensively, this research addresses the limitation of existing environment generation methods that prioritize visual realism over logical diversity. The proposed LogicEnvGen method employs a top-down approach using Large Language Models to analyze task execution logic, construct decision-tree behavior plans, synthesize logical trajectories, and instantiate physically plausible environments through constraint solving. Experimental results demonstrate that LogicEnvGen generates environments with 1.04-2.61 times greater logical diversity than baselines and improves fault detection in agents by 4.00% to 68.00%, as measured by the introduced LogicEnvEval benchmark.</div>
<div class="mono" style="margin-top:8px">受启发于需要逻辑多样化的模拟环境来全面评估具身智能体，类似于软件工程中的测试用例，本文提出了LogicEnvGen，一种利用大语言模型从任务逻辑角度自上而下生成环境的方法。该方法首先分析任务执行逻辑以构建决策树行为计划并合成逻辑轨迹，然后通过启发式算法进行优化以减少冗余，最后通过约束求解实例化具体环境以确保物理合理性。实验结果表明，基线方法缺乏逻辑多样性，而LogicEnvGen实现了1.04-2.61倍的更高多样性，并将智能体故障检测性能提升了4.00%-68.00%，这一结果通过包含四个定量指标的新基准LogicEnvEval得到验证。</div>
</details>
</div>
<div class="card">
<div class="title">The OncoReach Stylet for Brachytherapy: Design Evaluation and Pilot Study</div>
<div class="meta-line">Authors: Pejman Kheradmand, Kent K. Yamamoto, Emma Webster, Keith Sowards, Gianna Hatheway, Katharine L. Jackson, Sabino Zani, Julie A. Raffi, Diandra N. Ayala-Peacock, Scott R. Silva, Joanna Deaton Bertram, Yash Chitalia</div>
<div class="meta-line">First: 2026-01-20T02:25:47+00:00 · Latest: 2026-01-20T02:25:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13529v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13529v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cervical cancer accounts for a significant portion of the global cancer burden among women. Interstitial brachytherapy (ISBT) is a standard procedure for treating cervical cancer; it involves placing a radioactive source through a straight hollow needle within or in close proximity to the tumor and surrounding tissue. However, the use of straight needles limits surgical planning to a linear needle path. We present the OncoReach stylet, a handheld, tendon-driven steerable stylet designed for compatibility with standard ISBT 15- and 13-gauge needles. Building upon our prior work, we evaluated design parameters like needle gauge, spherical joint count and spherical joint placement, including an asymmetric disk design to identify a configuration that maximizes bending compliance while retaining axial stiffness. Free space experiments quantified tip deflection across configurations, and a two-tube Cosserat rod model accurately predicted the centerline shape of the needle for most trials. The best performing configuration was integrated into a reusable handheld prototype that enables manual actuation. A patient-derived, multi-composite phantom model of the uterus and pelvis was developed to conduct a pilot study of the OncoReach steerable stylet with one expert user. Results showed the ability to steer from less-invasive, medial entry points to reach the lateral-most targets, underscoring the significance of steerable stylets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OncoReach可操控导针在近距离放射治疗中的应用：设计评估与初步研究</div>
<div class="mono" style="margin-top:8px">宫颈癌在全球女性癌症负担中占据重要比例。组织间近距离放射治疗（ISBT）是治疗宫颈癌的标准方法，其通过直型空心针将放射源植入肿瘤及周围组织内部或邻近区域。然而，直针的使用限制了手术规划仅能采用线性针道。本文介绍OncoReach导针——一种手持式、肌腱驱动的可操控导针，设计兼容标准的15号和13号ISBT针具。基于前期研究，我们评估了针具规格、球关节数量与位置等设计参数，包括采用非对称圆盘设计以确定在保持轴向刚度的同时最大化弯曲顺应性的构型。自由空间实验量化了不同构型的针尖偏转，双管Cosserat杆模型准确预测了多数试验中针具的中心线形态。最优构型被集成至可重复使用的手持式原型机，支持手动操控。研究开发了基于患者数据的子宫与骨盆多材料复合体模，由一位专家操作者对OncoReach可操控导针开展初步研究。结果表明，该导针能够从侵入性较低的中间入路点转向抵达最外侧靶区，凸显了可操控导针的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitation of linear needle paths in interstitial brachytherapy for cervical cancer, which restricts surgical planning, this work presents the OncoReach stylet, a handheld, tendon-driven steerable stylet compatible with standard needles. The method involved evaluating design parameters like needle gauge and spherical joint configuration to maximize bending compliance while retaining axial stiffness, using free-space experiments and a Cosserat rod model for shape prediction. Experimental results from a pilot study with a patient-derived phantom showed the stylet&#x27;s ability to steer from medial entry points to reach lateral targets, demonstrating the potential of steerable stylets for improved access.</div>
<div class="mono" style="margin-top:8px">为了解决宫颈癌组织间近距离放射治疗中直线针道限制手术规划的问题，本研究开发了OncoReach，一种手持式、肌腱驱动的可操纵导芯，可与标准针具兼容。方法包括评估针规和球关节配置等设计参数，以在保持轴向刚度的同时最大化弯曲柔顺性，并通过自由空间实验和Cosserat杆模型进行形状预测。在使用患者衍生体模的初步研究中，原型成功实现了从内侧入路点向最外侧靶点的转向，证明了可操纵导芯在改善到达能力方面的潜力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
