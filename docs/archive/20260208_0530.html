<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-08 05:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260208_0530</div>
    <div class="row"><div class="card">
<div class="title">EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Alan Yuille</div>
<div class="meta-line">Venue: Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pages 649-659</div>
<div class="meta-line">First: 2025-02-07T07:07:04+00:00 · Latest: 2026-02-05T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04700v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.04700v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenLoRAx：通过回收适配器寻找主成分子空间以实现资源高效的适应与推理</div>
<div class="mono" style="margin-top:8px">大模型的快速发展因其高昂计算成本引发了环境影响与可访问性公平的担忧。低秩适配器（LoRA）为大模型微调提供了轻量级解决方案，催生了大量面向不同领域的公开适配器。我们提出：能否利用这些预训练适配器进一步简化新任务适应过程并应对相关挑战？本文介绍EigenLoRAx——一种参数高效的微调方法，通过回收现有适配器构建与其共享领域知识对齐的主成分子空间，并可在低资源场景中通过正交基向量进行扩展。该方法仅需学习子空间主成分上的轻量级系数即可快速适应新任务，无需微调完整适配器。EigenLoRAx显著减少了参数量和内存占用，提升了训练与推理效率。实验表明，该方法在多样化领域和任务中均表现优异，为边缘计算应用、个性化部署及资源受限环境下大模型的公平部署提供了可扩展方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid expansion of large models raises concerns about their computational cost and environmental impact, prompting a search for more efficient adaptation methods. This work introduces EigenLoRAx, a parameter-efficient finetuning technique that recycles a collection of pre-existing Low-Rank Adapters (LoRA) to construct a principal subspace capturing shared domain knowledge, which can be expanded with orthogonal basis vectors when data is scarce. Adaptation to a new task is achieved by learning only lightweight coefficients on this subspace, avoiding the need to finetune entire adapters. Experiments show the method achieves strong performance across various domains and tasks while requiring significantly fewer parameters and less memory, enhancing efficiency for both training and inference in resource-constrained settings.</div>
<div class="mono" style="margin-top:8px">本研究针对大模型的计算与环境成本以及公开可用的低秩适配器（LoRA）的激增，提出了EigenLoRAx以简化新任务的适配。该方法通过回收现有适配器构建一个捕捉共享领域知识的主子空间，在资源匮乏时可用正交基向量扩展，从而仅需学习主子空间上的轻量系数即可完成适配，无需微调整个适配器。实验结果表明，EigenLoRAx在多种领域和任务上均表现出色，同时显著减少了参数需求和内存占用，提升了在资源受限环境中训练和推理的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Shared LoRA Subspaces for almost Strict Continual Learning</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>共享LoRA子空间实现近乎严格的持续学习</div>
<div class="mono" style="margin-top:8px">将大型预训练模型高效持续地适配新任务对实际部署至关重要，但灾难性遗忘和重训练的高成本使其仍具挑战性。虽然低秩适配（LoRA）等参数高效调优方法降低了计算需求，但缺乏不依赖数据回放或多适配器的严格持续学习与知识整合机制。我们提出Share方法——一种参数高效的持续微调新范式，通过学习和动态更新单一共享低秩子空间，实现跨任务与跨模态的无缝适配。Share构建基础子空间以提取历史任务核心知识，并通过识别关键子空间方向逐步整合新信息。每个新任务的知识均融入这一动态演进的子空间，在促进前向知识迁移的同时最小化灾难性干扰。该方法相比传统LoRA实现最高100倍的参数压缩与281倍的内存节省，性能媲美联合训练模型。单个Share模型可替代数百个任务专用LoRA适配器，支持可扩展的异步持续学习。在图像分类、自然语言理解、3D姿态估计和文生图生成等任务的实验验证了其有效性，使Share成为大规模AI系统终身学习的实用可扩展解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting large pretrained models to new tasks continually and efficiently, which is hindered by catastrophic forgetting and high computational costs. The proposed method, Share, introduces a parameter-efficient continual finetuning approach that learns and dynamically updates a single, shared low-rank subspace, extracting core knowledge from past tasks and incrementally integrating new information by identifying essential subspace directions. Experimental results demonstrate that Share achieves up to 100x parameter reduction and 281x memory savings compared to traditional LoRA methods while maintaining performance comparable to jointly trained models, with validation across tasks including image classification, natural language understanding, 3D pose estimation, and text-to-image generation.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决大型预训练模型在持续学习场景中适应新任务时面临的灾难性遗忘和高计算成本问题，因为现有参数高效方法如LoRA缺乏严格的持续学习能力。提出的方法Share通过学习和动态更新一个共享的低秩子空间，从过去任务中提取核心知识，并通过识别关键子空间方向逐步整合新信息，从而实现前向知识转移并最小化干扰。实验结果表明，Share相比传统LoRA方法实现了高达100倍的参数减少和281倍的内存节省，性能与联合训练模型相当，并在图像分类、自然语言理解、3D姿态估计和文本到图像生成等任务中验证了其可扩展的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Pseudo-Invertible Neural Networks</div>
<div class="meta-line">Authors: Yamit Ehrlich, Nimrod Berman, Assaf Shocher</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06042v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06042v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or &quot;Back-Projection&quot;, $x&#x27; = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x&#x27;$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, &quot;degradation&quot; is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伪可逆神经网络</div>
<div class="mono" style="margin-top:8px">Moore-Penrose伪逆（PInv）是线性系统的基本解法。本文提出将PInv自然推广至非线性领域，特别是神经网络。我们引入满射伪可逆神经网络（SPNN），这类架构经显式设计可实现可处理的非线性PInv。所提出的非线性PInv及其在SPNN中的实现满足基本几何特性，例如零空间投影或“反向投影”$x&#x27; = x + A^\dagger(y-Ax)$，该操作将样本$x$移动至满足$Ax=y$的最近一致状态$x&#x27;$。我们形式化非线性反向投影（NLBP），该方法通过定义的PInv保证非线性映射$f(x)=y$具有相同的一致性约束。利用SPNN拓展零样本逆问题的适用范围：基于扩散的零空间投影通过闭式反向投影革新了线性逆问题的零样本求解，我们将其扩展至非线性退化问题。此处“退化”广义涵盖任何非线性信息损失，包括从光学畸变到分类等语义抽象。该方法支持复杂退化的零样本逆变换，并能在不重训练扩散先验的情况下实现对生成输出的精确语义控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to generalize the Moore-Penrose pseudo-inverse from linear to nonlinear systems, particularly for neural networks, to solve inverse problems. The method introduces Surjective Pseudo-invertible Neural Networks (SPNN), which are explicitly designed to have a tractable nonlinear pseudo-inverse that satisfies key geometric properties like non-linear back-projection, ensuring a sample can be moved to the closest state consistent with a given output. Experimental findings demonstrate that this framework extends zero-shot solving to nonlinear inverse problems, enabling inversion of complex degradations like optical distortions and semantic abstractions, and allows precise semantic control over generative outputs without retraining the underlying diffusion prior.</div>
<div class="mono" style="margin-top:8px">本研究旨在将摩尔-彭罗斯伪逆从线性系统推广到非线性系统，特别是神经网络，以解决逆问题。该方法引入了满射伪可逆神经网络（SPNN），其设计具有可处理的非线性伪逆，满足非线性反向投影等几何特性，确保映射的一致性。关键实验结果表明，SPNN能够实现复杂非线性退化（如光学畸变和语义抽象）的零样本逆变换，从而在不重新训练扩散先验的情况下，对生成输出进行精确的语义控制。</div>
</details>
</div>
<div class="card">
<div class="title">CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</div>
<div class="meta-line">Authors: Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:45+00:00 · Latest: 2026-02-05T18:59:45+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://comm-cp.github.io/">Project1</a> · <a href="https://comm-cp.github.io">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CommCP：基于大语言模型与保形预测的高效多智能体协同通信框架</div>
<div class="mono" style="margin-top:8px">为完成人类以自然语言下达的任务，机器人需解析指令、生成并回答相关问题以理解场景，并操控目标物体。实际部署常需多个具备不同操控能力的异构机器人协同处理不同任务。除专业操控技能外，有效的信息收集对任务完成至关重要。为此，我们将完全协作环境中的信息收集过程形式化为一个尚未充分探索的多智能体多任务具身问答问题，这是经典具身问答任务的新扩展，其中高效通信对避免冗余协作至关重要。针对该问题，我们提出CommCP——一个专为MM-EQA设计的基于大语言模型的去中心化通信框架。该框架采用保形预测技术校准生成信息，从而最小化接收方干扰并提升通信可靠性。为评估框架性能，我们构建了包含多样化照片级真实家庭场景与具身问题的MM-EQA基准测试。实验结果表明，CommCP较基线方法显著提升了任务成功率和探索效率。实验视频、代码及数据集详见项目网站：https://comm-cp.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of coordinating multiple heterogeneous robots to complete natural language assignments in household environments, where effective information gathering and communication are critical. The authors formalize this as a Multi-Agent Multi-task Embodied Question Answering (MM-EQA) problem and propose CommCP, a decentralized LLM-based communication framework that uses conformal prediction to calibrate generated messages, reducing receiver distractions and improving reliability. Experiments on a new photo-realistic MM-EQA benchmark show that CommCP significantly improves task success rates and exploration efficiency compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多台异构机器人在家庭环境中协作完成自然语言指令的协调难题，其中有效的信息收集与通信至关重要。作者将此问题形式化为多智能体多任务具身问答问题，并提出了CommCP框架：一种基于大语言模型的去中心化通信方法，利用共形预测校准生成的消息，以减少接收方干扰并提升通信可靠性。在新建的真实感多智能体多任务具身问答基准测试上的实验结果表明，与基线方法相比，CommCP显著提高了任务成功率和探索效率。</div>
</details>
</div>
<div class="card">
<div class="title">Can vision language models learn intuitive physics from interaction?</div>
<div class="meta-line">Authors: Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz</div>
<div class="meta-line">First: 2026-02-05T18:59:20+00:00 · Latest: 2026-02-05T18:59:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06033v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06033v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型能否通过交互学习直观物理？</div>
<div class="mono" style="margin-top:8px">预训练的视觉语言模型对物理世界缺乏良好的直觉。近期研究表明，监督微调能提升模型在简单物理任务上的表现，但微调后的模型并未学到可泛化至新情境的稳健物理规则。基于认知科学研究，我们假设模型需通过与环境交互来正确学习其物理动态。我们使用强化学习训练模型，使其通过环境交互进行学习。尽管交互学习能提升模型在任务内的表现，但未能形成具有泛化性的物理直觉。研究发现，在单一任务上训练的模型无法可靠地泛化至相关任务，即使这些任务共享视觉统计特征与物理原理，且无论模型是否通过交互训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether vision-language models can acquire robust physical intuitions, motivated by their current lack of such understanding and the limited generalization of supervised fine-tuning. The method involves training models through interaction with an environment using reinforcement learning, hypothesizing that this approach is key to learning physical dynamics. The key experimental finding is that while interactive learning improves within-task performance, it fails to produce generalizable physical intuitions, as models trained on one task do not reliably generalize to related tasks sharing visual and physical principles.</div>
<div class="mono" style="margin-top:8px">本研究探讨视觉语言模型能否获得稳健的物理直觉，其动机在于现有模型缺乏此类理解，且监督微调的泛化能力有限。方法基于认知科学的假设，即交互是学习的关键，通过强化学习训练模型与物理环境进行交互。核心实验结果表明，交互式学习虽能提升任务内性能，但无法产生可泛化的物理规则，因为在一个任务上训练的模型无法可靠地迁移到共享视觉统计和物理原理的相关任务上。</div>
</details>
</div>
<div class="card">
<div class="title">PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling</div>
<div class="meta-line">Authors: Kavana Venkatesh, Yinhan He, Jundong Li, Jiaming Cui</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06030v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06030v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysicsAgentABM：基于物理引导的生成式多智能体建模</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的多智能体系统虽能实现表达性强的智能体推理，但扩展成本高昂，且难以校准时间步对齐的状态转移模拟；而经典多智能体模型（ABM）虽具可解释性，却难以整合丰富的个体层面信号与非稳态行为。本文提出PhysicsAgentABM，将推理转向行为一致的智能体集群：状态特化的符号智能体编码机制化转移先验，多模态神经转移模型捕捉时序与交互动态，不确定性感知的认知融合产生校准后的集群级转移分布。个体智能体随后在局部约束下随机实现转移，从而将群体推理与实体级变异性解耦。我们进一步提出ANCHOR——一种基于跨情境行为响应与新型对比损失的LLM智能体驱动聚类策略，将LLM调用减少至多6-8倍。在公共卫生、金融和社会科学领域的实验表明，该方法在事件时间准确性与校准度上持续优于机制模型、神经模型及LLM基线。通过以不确定性感知的神经符号融合为核心重构群体级推理的生成式ABM，PhysicsAgentABM为基于LLM的可扩展校准仿真建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of LLM-based multi-agent systems, which are costly and poorly calibrated for timestep-aligned simulation, and classical agent-based models (ABMs), which lack rich individual-level signals. The proposed PhysicsAgentABM shifts inference to agent clusters, combining state-specialized symbolic agents for mechanistic priors, a multimodal neural transition model for dynamics, and uncertainty-aware epistemic fusion to produce calibrated cluster-level transitions; individual agents then stochastically realize transitions under local constraints. A key component is ANCHOR, an LLM-driven clustering strategy using cross-contextual behavioral responses and a contrastive loss, reducing LLM calls by 6-8 times. Experiments in public health, finance, and social sciences demonstrate improved event-time accuracy and calibration over mechanistic, neural, and LLM baselines.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决基于大语言模型的多智能体系统成本高、仿真校准性差，以及经典基于智能体的模型难以整合丰富个体行为的问题。提出的PhysicsAgentABM方法将推理转移到智能体集群，结合了用于机制先验的符号智能体、用于动态的多模态神经模型，以及不确定性感知融合以产生校准的集群级转移；个体智能体随后在局部约束下实现这些转移。关键组件ANCHOR是一种基于大语言驱动的聚类策略，可将大语言模型调用减少6-8倍。在公共卫生、金融和社会科学领域的实验表明，该方法在事件时间准确性和校准性上优于机制、神经和大语言模型基线。</div>
</details>
</div>
<div class="card">
<div class="title">AP-OOD: Attention Pooling for Out-of-Distribution Detection</div>
<div class="meta-line">Authors: Claus Hofmann, Christian Huber, Bernhard Lehner, Daniel Klotz, Sepp Hochreiter, Werner Zellinger</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06031v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AP-OOD：基于注意力池化的分布外检测方法</div>
<div class="mono" style="margin-top:8px">分布外检测通过将高维数据映射为标量OOD分数，对机器学习模型的可靠部署至关重要。当前研究的核心挑战在于如何有效利用并聚合语言模型中的词元嵌入以获取OOD分数。本文提出AP-OOD——一种面向自然语言的新型OOD检测方法，该方法突破基于简单平均的聚合方式，充分利用词元级信息。AP-OOD作为半监督方法，可在无监督与有监督设置间灵活切换，并能利用有限的辅助异常数据。实验表明，AP-OOD在文本OOD检测领域取得最先进成果：在无监督设置下，XSUM摘要任务的FPR95从27.84%降至4.67%，WMT15英法翻译任务的FPR95从77.08%降至70.37%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of effectively aggregating token embeddings from language models to compute reliable out-of-distribution (OOD) scores for text data. The proposed method, AP-OOD, introduces an attention pooling mechanism that leverages token-level information, moving beyond simple average-based aggregation, and operates as a semi-supervised approach to utilize limited auxiliary outlier data. Experimental results demonstrate state-of-the-art performance, significantly reducing the FPR95 from 27.84% to 4.67% on the XSUM summarization dataset and from 77.08% to 70.37% on the WMT15 En-Fr translation dataset in unsupervised settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决如何有效聚合语言模型的词元嵌入以计算文本数据可靠分布外检测分数的关键挑战。提出的AP-OOD方法引入了注意力池化机制，利用词元级信息超越简单的基于平均的聚合，并作为一种半监督方法能够利用有限的辅助异常数据。实验结果表明该方法取得了最先进的性能，在无监督设置下，将XSUM摘要数据集的FPR95从27.84%显著降低至4.67%，并将WMT15英法翻译数据集的FPR95从77.08%降低至70.37%。</div>
</details>
</div>
<div class="card">
<div class="title">Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</div>
<div class="meta-line">Authors: Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan</div>
<div class="meta-line">First: 2026-02-05T18:58:32+00:00 · Latest: 2026-02-05T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06029v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>好奇心即知识：基于主动推断的自洽学习与无悔优化</div>
<div class="mono" style="margin-top:8px">主动推断通过最小化期望自由能量，借助好奇心系数平衡认知价值（信息增益）与实用价值（任务性能），从而统一探索与利用。然而，这种平衡何时能同时实现连贯的学习与高效的决策尚不明确：好奇心不足可能导致短视的利用行为并阻碍不确定性消解，而过度的好奇心则可能引发不必要的探索与遗憾。我们首次为最小化期望自由能量的智能体建立了理论保证，证明单一条件——足够的好奇心——即可同时确保自洽学习（贝叶斯后验一致性）与无悔优化（累积遗憾有界）。分析揭示了该机制如何依赖于初始不确定性、可识别性与目标对齐性，从而在一个理论框架内将主动推断与经典贝叶斯实验设计及贝叶斯优化联系起来。我们进一步将这些理论转化为混合学习-优化问题中调节认知-实用权衡的实用设计准则，并通过真实场景实验验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge in active inference of balancing epistemic and pragmatic values via the Expected Free Energy (EFE) to achieve both coherent learning and efficient decision-making, as improper tuning can lead to myopic exploitation or wasteful exploration. The method establishes a theoretical guarantee that sufficient curiosity ensures self-consistent Bayesian learning and no-regret optimization, linking active inference to Bayesian experimental design and optimization. Experimental validation demonstrates that this framework provides practical guidelines for tuning the trade-off in hybrid problems, with results showing bounded cumulative regret and posterior consistency under identified conditions.</div>
<div class="mono" style="margin-top:8px">本研究针对主动推理中探索与利用的平衡问题，其中期望自由能（EFE）目标通过好奇心系数权衡信息增益与任务性能，但此前缺乏何时能实现有效学习和决策的理论保证。作者首次提出理论分析，表明确保足够的好奇心能同时实现自洽的贝叶斯学习（即后验一致性）和无悔优化（累积遗憾有界），从而将主动推理与经典贝叶斯实验设计和贝叶斯优化联系起来。在真实世界问题上的实验验证表明，这些理论见解为混合学习-优化任务中认知-实用权衡的调参提供了实用指导。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</div>
<div class="meta-line">Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang</div>
<div class="meta-line">First: 2026-02-05T18:57:09+00:00 · Latest: 2026-02-05T18:57:09+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/BudgetMem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06025v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/BudgetMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向运行时智能体内存的查询感知预算层级路由学习</div>
<div class="mono" style="margin-top:8px">内存对超越单上下文窗口运行的大语言模型（LLM）智能体日益关键，但现有系统多依赖离线、查询无关的内存构建机制，效率低下且可能丢失查询关键信息。尽管运行时内存利用是自然替代方案，但先前研究常伴随显著开销，且对性能-成本的权衡缺乏显式控制。本研究提出\textbf{BudgetMem}——一种支持显式查询感知性能-成本控制的运行时智能体内存框架。该框架将内存处理构建为多模块体系，每个模块提供三种预算层级（即\textsc{低}/\textsc{中}/\textsc{高}）。轻量级路由器通过强化学习训练的紧凑神经策略，执行跨模块的预算层级路由以平衡任务性能与内存构建成本。以BudgetMem为统一实验平台，我们研究实现预算层级的三种互补策略：实现方式（方法复杂度）、推理机制（推断行为）与容量配置（模块模型规模）。在LoCoMo、LongMemEval和HotpotQA数据集上的实验表明：当优先考虑性能时（即高预算设置），BudgetMem超越现有基线；在严格预算限制下能提供更优的精度-成本边界。进一步分析揭示了不同层级策略的优劣特性，明确了各维度在不同预算区间实现最佳权衡的适用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses inefficiencies in offline, query-agnostic memory systems for LLM agents by proposing BudgetMem, a runtime memory framework for explicit performance-cost control. The method structures memory into modules with three budget tiers and uses a lightweight neural router trained with reinforcement learning to allocate resources. Experiments on LoCoMo, LongMemEval, and HotpotQA show that BudgetMem outperforms baselines in high-budget settings and achieves better accuracy-cost trade-offs under constrained budgets, while analysis reveals the distinct advantages of different tiering strategies like implementation, reasoning, and capacity.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型智能体中离线、查询无关内存系统的低效问题，提出了BudgetMem这一运行时内存框架，以实现显式的性能-成本控制。该方法将内存组织为具有低、中、高三个预算层级的模块，并采用强化学习训练的轻量级神经路由器进行资源分配。在LoCoMo、LongMemEval和HotpotQA数据集上的实验表明，BudgetMem在高预算设置下优于基线模型，在严格预算下实现了更好的准确率-成本权衡，同时分析揭示了实现、推理和容量等不同层级策略在不同预算制度下的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</div>
<div class="meta-line">Authors: Miranda Muqing Miao, Young-Min Cho, Lyle Ungar</div>
<div class="meta-line">First: 2026-02-05T18:55:56+00:00 · Latest: 2026-02-05T18:55:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06022v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正确性优化的残差激活透镜（CORAL）：可迁移且感知校准的推理时引导方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）存在持续校准偏差，尤其在指令微调和偏好对齐后。修改训练目标可改善校准，但重新训练成本高昂。推理时引导提供了一种轻量级替代方案，但现有方法多优化正确性代理而非正确性本身。本文提出CORAL（正确性优化的残差激活透镜），一种正则化的推理时引导方法，通过权重衰减MLP探针从模型内部激活中捕获分布式正确性信号。我们在三个70亿参数模型上评估CORAL，发现其平均提升准确率10%、降低预期校准误差（ECE）50%。进一步证明这些增益无需重新训练即可迁移至四个保留基准测试（ARC-Challenge、HellaSwag、Math-MC、OpenBookQA）的完整公开测试集，平均提升准确率14%、降低ECE 49%。结果支持以下假设：当单个神经元信息不足时，可通过正则化探针提取模型内部的分布式信息。因此，CORAL为推理时提升多项选择题性能提供了一种计算高效、可迁移且感知校准的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the persistent miscalibration of large language models after alignment without expensive retraining, this paper introduces CORAL, an inference-time steering method that optimizes for correctness directly. The method employs weight-decay regularized MLP probes to extract distributed correctness signals from model internal activations. Experimental results on three 7B-parameter models show CORAL consistently improves accuracy by 10% and reduces expected calibration error by 50% on average, with these gains transferring to four held-out benchmarks, yielding average accuracy improvements of 14% and ECE improvements of 49%.</div>
<div class="mono" style="margin-top:8px">为解决大语言模型在指令微调和偏好对齐后存在的校准偏差问题，同时避免昂贵的重新训练，本研究提出了CORAL，一种在推理时直接优化正确性的引导方法。该方法使用权重衰减正则化的多层感知机探针，从模型内部激活中提取分布式的正确性信号。在三个70亿参数模型上的实验表明，CORAL平均将准确率提升10%，预期校准误差降低50%，且这些改进能迁移到四个保留基准测试集上，平均带来14%的准确率提升和49%的校准误差改善。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Model&#x27;s Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold</div>
<div class="meta-line">Authors: Ye He, Yitong Qiu, Molei Tao</div>
<div class="meta-line">First: 2026-02-05T18:55:03+00:00 · Latest: 2026-02-05T18:55:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06021v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06021v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model&#x27;s performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model&#x27;s inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的泛化能力可通过其朝向数据依赖的脊流形的归纳偏置来刻画</div>
<div class="mono" style="margin-top:8px">当扩散模型未记忆训练数据集时，其泛化机制究竟如何？对其生成分布的定量理解有助于评估模型在下游应用中的性能。为此，我们通过提出对数密度脊流形并量化生成数据随推断动态如何关联于该流形，来明确刻画扩散模型的生成行为。具体而言，推断过程围绕脊流形经历“抵达-对齐-滑动”三个阶段：轨迹首先抵达流形邻域，随后在法向被推近或推离流形以完成对齐，最终沿切向在流形上滑动。在此通用行为框架下，不同的训练误差将导致可量化的法向与切向运动差异，这些细节运动刻画了模态间生成现象的出现时机。对训练动态的更深入理解将带来对生成归纳偏置的更精确量化，我们将以随机特征模型为例，具体阐释扩散模型的归纳偏置如何源于架构偏置与训练精度的组合，并如何随推断动态演化。在合成多模态分布与MNIST潜在扩散上的实验，均支持了低维与高维场景中预测的方向性效应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand how diffusion models generalize beyond memorizing training data, this work characterizes the generated distribution by introducing a data-dependent log-density ridge manifold and analyzing inference dynamics. The method reveals a reach-align-slide process: trajectories first approach the manifold, then align along normal directions, and finally slide tangentially along it, with training errors dictating specific normal and tangent motions that explain inter-mode generation. Experimental results on synthetic multimodal distributions and MNIST latent diffusion confirm the predicted directional effects across dimensions, and a random feature model illustrates how inductive biases arise from architectural bias and training accuracy.</div>
<div class="mono" style="margin-top:8px">为了理解扩散模型在非记忆训练数据时的泛化机制，本研究通过引入一个数据依赖的对数密度脊流形来刻画生成分布。分析表明，推理动力学围绕该流形经历一个“到达-对齐-滑动”过程：轨迹首先接近流形，然后沿法向对齐，最后沿切向滑动，其中训练误差决定了具体的法向和切向运动，从而解释了模态间生成现象。在合成多模态分布和MNIST潜在扩散上的实验验证了这些方向性效应，而随机特征模型进一步阐明了归纳偏置如何源于架构偏置和训练精度。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanisms of AI Protein Folding in ESMFold</div>
<div class="meta-line">Authors: Kevin Lu, Jannik Brinkmann, Stefan Huber, Aaron Mueller, Yonatan Belinkov, David Bau, Chris Wendler</div>
<div class="meta-line">First: 2026-02-05T18:54:54+00:00 · Latest: 2026-02-05T18:54:54+00:00</div>
<div class="meta-line">Comments: Our code, data, and results are available at https://folding.baulab.info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06020v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESMFold中AI蛋白质折叠的机制</div>
<div class="mono" style="margin-top:8px">蛋白质结构预测模型如何折叠蛋白质？我们通过追踪ESMFold折叠β发夹这一普遍结构基序的过程来研究这一问题。通过对模型潜在变量进行反事实干预，我们在折叠主干中识别出两个计算阶段。第一阶段，早期模块初始化成对生化信号：残基身份及相关生化特征（如电荷）从序列表征流入成对表征。第二阶段，晚期模块发展成对空间特征：距离与接触信息在成对表征中累积。我们证明，ESMFold结构决策的机制可被定位、通过可解释表征追踪，并能通过强因果效应进行操控。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand how AI models predict protein structures, this study investigates the folding mechanisms of ESMFold using a beta hairpin motif as a case study. The method involves performing counterfactual interventions on the model&#x27;s latent representations to trace the computational process. Key experimental findings reveal a two-stage mechanism: early model blocks initialize pairwise biochemical signals like residue identities and charge flow from the sequence, while later blocks develop pairwise spatial features such as distance and contact information, demonstrating that structural decisions can be localized, traced, and causally manipulated.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究蛋白质结构预测模型的内部机制，通过分析ESMFold如何折叠β发夹这一常见结构基序。研究方法包括追踪折叠过程并对模型的潜在表示进行反事实干预。主要实验结果表明存在两个不同的计算阶段：早期模块初始化残基身份和电荷流等成对生化信号，而晚期模块则发展出距离和接触信息等成对空间特征，证明结构决策可以被定位、追踪并因果性地操控。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Token Prediction via Self-Distillation</div>
<div class="meta-line">Authors: John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein</div>
<div class="meta-line">First: 2026-02-05T18:54:48+00:00 · Latest: 2026-02-05T18:54:48+00:00</div>
<div class="meta-line">Comments: 8 pages and 5 figures in the main body</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $&lt;5\%$ drop in accuracy relative to single token decoding performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自蒸馏的多令牌预测方法</div>
<div class="mono" style="margin-top:8px">现有加速语言模型推理的技术（如推测解码）需要训练辅助推测模型并构建复杂的推理流程。本文提出一种新方法，通过简单的在线蒸馏目标，将预训练的自回归语言模型从缓慢的单令牌预测模型转换为快速的独立多令牌预测模型。最终模型保持与预训练初始检查点完全相同的实现方式，无需添加任何辅助验证器或专用推理代码即可部署。在GSM8K数据集上，本方法生成的模型解码速度平均提升超过3倍，且准确率下降幅度低于5%（相对于单令牌解码性能）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To simplify and accelerate language model inference without the complexity of auxiliary speculator models and specialized pipelines, this work proposes converting a pretrained autoregressive model into a standalone multi-token predictor via a simple online self-distillation objective. The method retains the original model&#x27;s architecture and deployment simplicity, eliminating the need for verifiers or extra inference code. Experimental results on GSM8K show the converted models achieve over 3x faster decoding on average with less than a 5% drop in accuracy compared to standard single-token decoding.</div>
<div class="mono" style="margin-top:8px">为了简化并加速语言模型推理，避免使用辅助模型或复杂流程，本研究提出通过在线自蒸馏将预训练的自回归模型转换为独立的多令牌预测器。该方法采用简单的蒸馏目标，使模型能在单次前向传播中预测多个未来令牌，同时保持与原始模型完全相同的架构和部署简易性。在GSM8K上的实验结果表明，所得模型的解码速度平均提升超过3倍，且准确率相比标准的单令牌解码下降不到5%。</div>
</details>
</div>
<div class="card">
<div class="title">Optimism Stabilizes Thompson Sampling for Adaptive Inference</div>
<div class="meta-line">Authors: Shunxing Yan, Han Zhong</div>
<div class="meta-line">First: 2026-02-05T18:52:54+00:00 · Latest: 2026-02-05T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm&#x27;s pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观性稳定自适应推断中的汤普森采样</div>
<div class="mono" style="margin-top:8px">汤普森采样（TS）广泛用于随机多臂老虎机问题，但其在自适应数据收集下的推断性质较为微妙。由于各臂的样本量具有随机性，且通过动作选择规则与奖励耦合，经典样本均值的渐近理论可能失效。我们在$K$臂高斯老虎机中研究这一现象，并发现\emph{乐观性}是恢复\emph{稳定性}的关键机制——稳定性是有效渐近推断的充分条件，要求各臂的拉动次数集中在确定性尺度附近。首先，我们证明方差膨胀TS \citep{halder2025stable}对任意$K \ge 2$（包括多臂同时最优的挑战性场景）均具有稳定性，这通过将\citet{halder2025stable}的双臂结果推广至一般$K$臂情形，解决了其提出的开放性问题。其次，我们分析了一种保持后验方差不变但对后验均值添加显式奖励加成的乐观修正方案，并得出了相同的稳定性结论。总之，适当实施的乐观性能够稳定汤普森采样，使多臂老虎机中的渐近有效推断成为可能，且仅产生轻微的额外遗憾代价。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of performing valid statistical inference under adaptive data collection in multi-armed bandits using Thompson sampling (TS), where classical asymptotic theory fails due to the randomness and coupling of arm-specific sample sizes with rewards. The study identifies optimism as a key stabilizing mechanism and analyzes two specific implementations: variance-inflated TS and an alternative method that adds an explicit mean bonus to the posterior mean. The main experimental findings prove that both optimistic modifications ensure stability—where each arm&#x27;s pull count concentrates around a deterministic scale—for any number of arms K ≥ 2, including cases with multiple optimal arms, thereby enabling asymptotically valid inference with only a mild additional regret cost.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在多臂赌博机问题中使用汤普森采样进行自适应数据收集时，由于臂特定的样本量随机且与奖励通过动作选择规则耦合，导致经典渐近理论失效，从而难以进行有效统计推断的挑战。研究将乐观主义识别为关键的稳定机制，并分析了两种具体实现：方差膨胀的汤普森采样以及一种保持后验方差不变但对后验均值添加显式奖励的替代方法。主要实验结果证明，这两种乐观主义修改都能确保稳定性——即每个臂的拉动次数围绕一个确定性尺度集中——适用于任意臂数 K ≥ 2，包括存在多个最优臂的情况，从而实现了渐近有效的统计推断，且仅带来轻微的额外遗憾成本。</div>
</details>
</div>
<div class="card">
<div class="title">AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</div>
<div class="meta-line">Authors: Xianyang Liu, Shangding Gu, Dawn Song</div>
<div class="meta-line">First: 2026-02-05T18:50:36+00:00 · Latest: 2026-02-05T18:50:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06008v1">PDF</a> · <a href="https://github.com/SafeRL-Lab/AgenticPay">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgenticPay：面向买卖交易的多智能体大语言模型协商系统</div>
<div class="mono" style="margin-top:8px">基于大语言模型的智能体日益被期望能自主协商、协调与交易，但现有基准缺乏评估多智能体间语言驱动经济交互的原则性框架。我们提出AgenticPay——一个由自然语言驱动的多智能体买卖协商基准与仿真框架。该框架模拟买卖双方拥有私有约束和产品相关估值的市场环境，要求通过多轮语言协商（而非单纯数值竞价）达成协议。系统支持涵盖双边议价至多对多市场的110余项任务，具备结构化行为提取机制及可行性、效率与福利评估指标。对前沿闭源与开源大语言模型的基准测试揭示了协商性能的显著差距，凸显了长程战略推理的挑战，从而确立AgenticPay作为研究智能体商业与语言化市场交互的基础平台。代码与数据集可通过链接获取：https://github.com/SafeRL-Lab/AgenticPay。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for principled evaluation settings for language-mediated economic interactions among LLM-based agents, as existing benchmarks lack such frameworks. The method introduces AgenticPay, a benchmark and simulation framework that models buyer-seller negotiations where agents with private constraints and valuations reach agreements through multi-round natural language dialogue, supporting over 110 tasks from bilateral to many-to-many markets with structured action extraction and economic metrics. Experimental results from benchmarking state-of-the-art LLMs reveal significant performance gaps in negotiation, particularly in long-horizon strategic reasoning, establishing the framework as a foundation for studying agentic commerce.</div>
<div class="mono" style="margin-top:8px">本研究动机源于现有基准缺乏对基于大语言模型的智能体之间语言中介经济交互的原则性评估设置。方法上提出了AgenticPay，这是一个基准和模拟框架，模拟买卖双方在拥有私有约束和产品相关估值的条件下，通过多轮自然语言对话达成协议，支持从双边到多对多市场的110多项任务，并采用结构化行动提取和经济指标。对最先进的大语言模型进行基准测试的实验结果揭示了谈判性能存在显著差距，特别是在长视野战略推理方面存在挑战，从而确立了该框架作为研究智能体商业和基于语言的市场交互的基础。</div>
</details>
</div>
<div class="card">
<div class="title">On Computation and Reinforcement Learning</div>
<div class="meta-line">Authors: Raj Ghugare, Michał Bortkiewicz, Alicja Ziarko, Benjamin Eysenbach</div>
<div class="meta-line">First: 2026-02-05T18:45:57+00:00 · Latest: 2026-02-05T18:45:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05999v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05999v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How does the amount of compute available to a reinforcement learning (RL) policy affect its learning? Can policies using a fixed amount of parameters, still benefit from additional compute? The standard RL framework does not provide a language to answer these questions formally. Empirically, deep RL policies are often parameterized as neural networks with static architectures, conflating the amount of compute and the number of parameters. In this paper, we formalize compute bounded policies and prove that policies which use more compute can solve problems and generalize to longer-horizon tasks that are outside the scope of policies with less compute. Building on prior work in algorithmic learning and model-free planning, we propose a minimal architecture that can use a variable amount of compute. Our experiments complement our theory. On a set 31 different tasks spanning online and offline RL, we show that $(1)$ this architecture achieves stronger performance simply by using more compute, and $(2)$ stronger generalization on longer-horizon test tasks compared to standard feedforward networks or deep residual network using up to 5 times more parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论计算与强化学习</div>
<div class="mono" style="margin-top:8px">强化学习策略可用的计算量如何影响其学习效果？使用固定参数量的策略是否仍能从额外计算中获益？标准强化学习框架缺乏正式回答这些问题的理论工具。实践中，深度强化学习策略常被参数化为静态架构的神经网络，混淆了计算量与参数量。本文形式化定义了计算受限策略，并证明使用更多计算的策略能解决计算较少策略无法处理的问题，且能泛化至更长时域的任务。基于算法学习和无模型规划的前期研究，我们提出一种能灵活利用可变计算量的最小化架构。实验与理论相互印证：在涵盖在线与离线强化学习的31项任务中，该架构（1）仅通过增加计算量即可获得更强性能，（2）与使用多达5倍参数的标准前馈网络或深度残差网络相比，在长时域测试任务中展现出更优的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how the computational budget, distinct from parameter count, influences reinforcement learning (RL) policy performance and generalization, addressing a gap in the standard RL framework. The authors formalize compute-bounded policies and propose a minimal architecture that can dynamically utilize variable amounts of compute, building on concepts from algorithmic learning and model-free planning. Experimental results across 31 online and offline RL tasks demonstrate that this architecture achieves stronger performance with increased compute and better generalization to longer-horizon tasks compared to standard feedforward or deep residual networks with significantly more parameters.</div>
<div class="mono" style="margin-top:8px">本研究探讨了计算预算（区别于参数数量）如何影响强化学习策略的性能与泛化能力。为正式分析此问题，作者提出了一个计算受限策略的框架，并基于算法学习和无模型规划的概念，设计了一种能够利用可变计算量的最小化架构。在31个在线和离线强化学习任务上的实验结果表明，该架构通过使用更多计算资源，不仅获得了更强的性能，而且在更长视野的测试任务上比标准前馈网络或参数多出5倍的深度残差网络具有更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Transmuting prompts into weights</div>
<div class="meta-line">Authors: Hanna Mazzawi, Benoit Dherin, Michael Munn, Michael Wunder, Javier Gonzalvo</div>
<div class="meta-line">First: 2025-10-09T18:40:39+00:00 · Latest: 2026-02-05T18:44:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08734v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08734v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing body of research has demonstrated that the behavior of large language models can be effectively controlled at inference time by directly modifying their internal states, either through vector additions to their activations or through updates to their weight matrices. These techniques, while powerful, are often guided by empirical heuristics, such as deriving steering vectors from the average activations of contrastive prompts. This work provides a theoretical foundation for these interventions, explaining how they emerge from the fundamental computations of the transformer architecture. Building on the recent finding that a prompt&#x27;s influence can be mathematically mapped to token-dependent implicit weight updates (Dherin et. al, 2025), we derive a principled method for condensing this information into token-independent thought vectors and thought matrices. These constructs provide a theoretical explanation for existing vector-and-matrix-based model editing techniques and offer a direct, computationally-grounded method for transmuting textual input into reusable weight updates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将提示转化为权重</div>
<div class="mono" style="margin-top:8px">越来越多的研究表明，通过直接修改大型语言模型的内部状态——无论是通过对其激活值进行向量加法，还是更新其权重矩阵——可以在推理时有效控制其行为。这些技术虽然强大，但通常依赖于经验启发式方法，例如从对比提示的平均激活中推导导向向量。本研究为这些干预措施提供了理论基础，解释了它们如何从Transformer架构的基本计算中自然产生。基于近期关于提示影响可数学映射为依赖令牌的隐式权重更新的发现（Dherin等人，2025），我们推导出一种原则性方法，将此信息压缩为独立于令牌的思维向量和思维矩阵。这些构造为现有基于向量和矩阵的模型编辑技术提供了理论解释，并提供了一种直接、基于计算的将文本输入转化为可复用权重更新的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to provide a theoretical foundation for existing empirical techniques that control large language model behavior by modifying internal states, such as adding steering vectors to activations or updating weight matrices. The method builds on recent work showing that a prompt&#x27;s influence can be mathematically mapped to token-dependent implicit weight updates, and it derives a principled approach to condense this information into token-independent thought vectors and matrices. The key finding is that these constructs theoretically explain prior model editing techniques and offer a direct, computationally-grounded method for converting textual input into reusable weight updates.</div>
<div class="mono" style="margin-top:8px">本研究旨在为通过修改内部状态（如向激活添加导向向量或更新权重矩阵）来控制大语言模型行为的经验性技术提供理论基础。该方法基于提示的影响可映射为依赖于令牌的隐式权重更新的发现，推导出一种将此类信息压缩为独立于令牌的思维向量和思维矩阵的原则性方法。主要实验结果表明，这些构造从理论上解释了现有的模型编辑技术，并提供了一种直接的、基于计算的方法，可将文本输入转化为可重复使用的权重更新。</div>
</details>
</div>
<div class="card">
<div class="title">Causal Inference on Stopped Random Walks in Online Advertising</div>
<div class="meta-line">Authors: Jia Yuan Yu</div>
<div class="meta-line">First: 2026-02-05T18:43:29+00:00 · Latest: 2026-02-05T18:43:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05997v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05997v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider a causal inference problem frequently encountered in online advertising systems, where a publisher (e.g., Instagram, TikTok) interacts repeatedly with human users and advertisers by sporadically displaying to each user an advertisement selected through an auction. Each treatment corresponds to a parameter value of the advertising mechanism (e.g., auction reserve-price), and we want to estimate through experiments the corresponding long-term treatment effect (e.g., annual advertising revenue). In our setting, the treatment affects not only the instantaneous revenue from showing an ad, but also changes each user&#x27;s interaction-trajectory, and each advertiser&#x27;s bidding policy -- as the latter is constrained by a finite budget. In particular, each a treatment may even affect the size of the population, since users interact longer with a tolerable advertising mechanism. We drop the classical i.i.d. assumption and model the experiment measurements (e.g., advertising revenue) as a stopped random walk, and use a budget-splitting experimental design, the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to construct confidence intervals for the long-term treatment effect.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线广告中停止随机游走的因果推断</div>
<div class="mono" style="margin-top:8px">本文探讨在线广告系统中常见的因果推断问题：发布商（如Instagram、TikTok）通过间歇性向用户展示竞价广告，与用户和广告商重复交互。每种处理对应广告机制的一个参数值（如拍卖保留价），我们通过实验估计其长期处理效应（如年度广告收入）。处理不仅影响广告展示的即时收益，还会改变用户交互轨迹和广告商竞价策略（后者受有限预算约束）。特定处理甚至可能影响用户规模，因为用户对可容忍的广告机制交互更久。我们摒弃经典独立同分布假设，将实验测量值（如广告收入）建模为停止随机游走，结合预算分割实验设计、Anscombe定理、类Wald方程及中心极限定理，构建长期处理效应的置信区间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of estimating long-term treatment effects in online advertising systems, where interventions like auction reserve-price adjustments influence not only immediate revenue but also user engagement trajectories and advertiser bidding behaviors under budget constraints, potentially altering the user population size. The method departs from traditional i.i.d. assumptions by modeling experimental measurements as stopped random walks, employing a budget-splitting experimental design alongside statistical tools including the Anscombe Theorem, a Wald-like equation, and a Central Limit Theorem to derive confidence intervals for long-term effects. Key experimental findings demonstrate the approach&#x27;s ability to provide reliable confidence intervals for cumulative outcomes such as annual advertising revenue, accounting for dynamic interactions and budget-limited advertiser adaptations.</div>
<div class="mono" style="margin-top:8px">本研究针对在线广告系统中长期处理效应估计的挑战，其中如拍卖保留价等干预不仅影响即时收入，还会改变用户参与轨迹和广告商在预算约束下的出价策略，甚至可能影响用户规模。方法摒弃了传统的独立同分布假设，将实验测量建模为停止随机游走，采用预算分割实验设计，并结合安斯科姆定理、沃尔德类方程和中心极限定理等统计工具，构建长期效应的置信区间。关键实验结果表明，该方法能为累计结果（如年度广告收入）提供可靠的统计推断，同时考虑了动态交互和广告商有限预算的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Orthogonal Self-Attention</div>
<div class="meta-line">Authors: Leo Zhang, James Martens</div>
<div class="meta-line">First: 2026-02-05T18:42:57+00:00 · Latest: 2026-02-05T18:42:57+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05996v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05996v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Softmax Self-Attention (SSA) is a key component of Transformer architectures. However, when utilised within skipless architectures, which aim to improve representation learning, recent work has highlighted the inherent instability of SSA due to inducing rank collapse and poorly-conditioned Jacobians. In this work, we design a novel attention mechanism: Orthogonal Self-Attention (OSA), which aims to bypass these issues with SSA, in order to allow for (non-causal) Transformers without skip connections and normalisation layers to be more easily trained. In particular, OSA parametrises the attention matrix to be orthogonal via mapping a skew-symmetric matrix, formed from query-key values, through the matrix exponential. We show that this can be practically implemented, by exploiting the low-rank structure of our query-key values, resulting in the computational complexity and memory cost of OSA scaling linearly with sequence length. Furthermore, we derive an initialisation scheme for which we prove ensures that the Jacobian of OSA is well-conditioned.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正交自注意力机制</div>
<div class="mono" style="margin-top:8px">Softmax自注意力（SSA）是Transformer架构的核心组件。然而，在旨在提升表征学习的无跳跃连接架构中，近期研究指出SSA因引发秩崩溃和病态雅可比矩阵而存在固有稳定性问题。本文设计了一种新型注意力机制：正交自注意力（OSA），旨在规避SSA的这些问题，使无跳跃连接和归一化层的（非因果）Transformer更易于训练。具体而言，OSA通过将查询-键值构成的斜对称矩阵映射至矩阵指数空间，使注意力矩阵参数化保持正交性。我们证明该方法可利用查询-键值的低秩结构实现线性序列长度的计算复杂度与内存开销。此外，我们推导出能确保OSA雅可比矩阵良态的初始化方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the instability of Softmax Self-Attention (SSA) in skipless Transformer architectures, which suffers from rank collapse and poorly-conditioned Jacobians, hindering training. The proposed method, Orthogonal Self-Attention (OSA), parametrizes an orthogonal attention matrix by mapping a skew-symmetric matrix, constructed from query-key values, through the matrix exponential; this is implemented efficiently with linear complexity in sequence length by exploiting low-rank structure. Key experimental findings include a proven initialization scheme that ensures a well-conditioned Jacobian for OSA, enabling the successful training of non-causal Transformers without skip connections or normalization layers.</div>
<div class="mono" style="margin-top:8px">本研究针对无跳跃连接的Transformer架构中Softmax自注意力机制的不稳定性问题，该机制会导致秩塌陷和雅可比矩阵条件数不佳，从而阻碍训练。作者提出了正交自注意力机制，通过将查询-键值形成的斜对称矩阵映射到矩阵指数，使注意力矩阵正交化。实验结果表明，该机制可利用查询-键值的低秩结构实现线性序列长度复杂度的高效计算，且推导出的初始化方案理论上保证了雅可比矩阵的良好条件数，从而能够稳定训练无需跳跃连接或归一化层的Transformer模型。</div>
</details>
</div>
<div class="card">
<div class="title">Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps</div>
<div class="meta-line">Authors: Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz</div>
<div class="meta-line">First: 2026-02-05T18:42:00+00:00 · Latest: 2026-02-05T18:42:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05993v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05993v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose &quot;Diamond Maps&quot;, stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>钻石映射：基于随机流映射的高效奖励对齐方法</div>
<div class="mono" style="margin-top:8px">流模型与扩散模型能生成高质量样本，但训练后调整以适应用户偏好或约束仍成本高昂且脆弱，这一挑战通常称为奖励对齐。我们认为高效奖励对齐应是生成模型的内在属性而非事后补救，并为此重新设计模型以提升适应性。我们提出“钻石映射”——一种随机流映射模型，能在推理阶段高效精准地对齐任意奖励函数。钻石映射将多步模拟摊销为单步采样器（如流映射），同时保留奖励对齐所需随机性。该设计通过支持价值函数的高效一致估计，使搜索、序列蒙特卡洛和引导方法具备可扩展性。实验表明，钻石映射可通过GLASS流的蒸馏高效学习，获得更强的奖励对齐性能，且比现有方法更具扩展优势。研究结果为构建能在推理时快速适应任意偏好与约束的生成模型提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Adapting flow and diffusion models to user preferences or constraints after training is often inefficient and unstable, a problem known as reward alignment. To address this, the authors propose Diamond Maps, a stochastic flow map model designed from the outset for adaptability, which amortizes multiple simulation steps into a single-step sampler while retaining necessary stochasticity. Experiments demonstrate that Diamond Maps, efficiently distilled from GLASS Flows, achieve superior reward alignment performance and better scalability compared to existing methods, offering a practical path for generative models that can be rapidly adapted to arbitrary preferences at inference time.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决流模型和扩散模型在训练后难以高效适应不同用户偏好的问题。作者提出了Diamond Maps，这是一种随机流映射模型，通过将多步模拟压缩为单步采样器来在推理时实现高效的奖励对齐，同时保留了必要的随机性。实验表明，通过从GLASS Flows蒸馏学习的Diamond Maps，在奖励对齐性能和可扩展性方面均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Layer-wise LoRA fine-tuning: a similarity metric approach</div>
<div class="meta-line">Authors: Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Lucas Pellicer, Rosimeire Pereira Costa, Edson Bollis, Anna Helena Reali Costa, Artur Jordao</div>
<div class="meta-line">First: 2026-02-05T18:38:53+00:00 · Latest: 2026-02-05T18:38:53+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05988v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05988v1">PDF</a> · <a href="https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-training Large Language Models (LLMs) on web-scale datasets becomes fundamental for advancing general-purpose AI. In contrast, enhancing their predictive performance on downstream tasks typically involves adapting their knowledge through fine-tuning. Parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA), aim to reduce the computational cost of this process by freezing the pre-trained model and updating a smaller number of parameters. In comparison to full fine-tuning, these methods achieve over 99\% reduction in trainable parameter count, depending on the configuration. Unfortunately, such a reduction may prove insufficient as LLMs continue to grow in scale. In this work, we address the previous problem by systematically selecting only a few layers to fine-tune using LoRA or its variants. We argue that not all layers contribute equally to the model adaptation. Leveraging this, we identify the most relevant layers to fine-tune by measuring their contribution to changes in internal representations. Our method is orthogonal to and readily compatible with existing low-rank adaptation techniques. We reduce the trainable parameters in LoRA-based techniques by up to 50\%, while maintaining the predictive performance across different models and tasks. Specifically, on encoder-only architectures, this reduction in trainable parameters leads to a negligible predictive performance drop on the GLUE benchmark. On decoder-only architectures, we achieve a small drop or even improvements in the predictive performance on mathematical problem-solving capabilities and coding tasks. Finally, this effectiveness extends to multimodal models, for which we also observe competitive results relative to fine-tuning with LoRA modules in all layers. Code is available at: https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于相似性度量的分层LoRA微调方法</div>
<div class="mono" style="margin-top:8px">在网络规模数据集上预训练大语言模型已成为推进通用人工智能的基础。相比之下，提升其在下游任务中的预测性能通常需要通过微调来适配其知识。参数高效微调技术（如低秩适配LoRA）通过冻结预训练模型并更新少量参数，旨在降低该过程的计算成本。与全参数微调相比，这些方法根据配置可减少99%以上的可训练参数量。然而，随着大语言模型规模持续增长，这种降幅可能仍显不足。本研究通过系统性地选择少量层进行LoRA或其变体的微调，以解决上述问题。我们认为并非所有层对模型适配的贡献均等，据此通过度量各层对内部表征变化的贡献来识别最相关的微调层。该方法与现有低秩适配技术正交且兼容，能在保持不同模型和任务预测性能的前提下，将基于LoRA技术的可训练参数进一步减少50%。具体而言，在仅编码器架构中，这种参数削减在GLUE基准测试中仅导致可忽略的预测性能下降；在仅解码器架构中，数学问题求解和代码任务的预测性能仅出现微小下降甚至有所提升。该有效性还延伸至多模态模型，与全层LoRA微调相比仍能保持竞争力。代码发布于：https://github.com/c2d-usp/Layer-wise-LoRA-with-CKA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To further reduce the computational cost of fine-tuning large language models beyond standard Low-Rank Adaptation (LoRA), this work proposes a method to systematically select only the most relevant layers for adaptation. The approach measures the contribution of each layer to changes in internal representations using a similarity metric, identifying a subset of layers to fine-tune with LoRA or its variants. Experimental results show the method reduces trainable parameters by up to 50% compared to standard LoRA while maintaining performance: on encoder-only models, it leads to a negligible performance drop on the GLUE benchmark, and on decoder-only models, it achieves small drops or even improvements in mathematical and coding tasks, with competitive results also observed for multimodal models.</div>
<div class="mono" style="margin-top:8px">为了在标准低秩适应（LoRA）基础上进一步降低大语言模型微调的计算成本，本研究提出了一种系统性地选择最相关层进行适配的方法。该方法通过相似性度量评估各层对内部表示变化的贡献，从而识别出一个用于LoRA或其变体微调的子层集。实验表明，与标准LoRA相比，该方法将可训练参数减少了高达50%，同时保持了性能：在编码器模型上，GLUE基准测试的性能下降可忽略不计；在解码器模型上，数学和代码任务性能仅有小幅下降甚至有所提升；该方法在多模态模型上也取得了有竞争力的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Clifford Kolmogorov-Arnold Networks</div>
<div class="meta-line">Authors: Matthias Wolff, Francesco Alesiani, Christof Duhme, Xiaoyi Jiang</div>
<div class="meta-line">First: 2026-02-05T18:25:40+00:00 · Latest: 2026-02-05T18:25:40+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05977v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>克利福德-柯尔莫哥洛夫-阿诺德网络</div>
<div class="mono" style="margin-top:8px">我们提出克利福德-柯尔莫哥洛夫-阿诺德网络（ClKAN），这是一种在任意克利福德代数空间中用于函数逼近的灵活高效架构。针对高维代数伴随的指数级计算复杂度问题，我们提出采用随机拟蒙特卡洛网格生成方法。ClKAN还引入了新的批归一化策略以处理变域输入。该架构在科学发现与工程领域具有应用潜力，并在合成任务及物理启发的任务中得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop an efficient architecture for function approximation in Clifford algebra spaces, addressing the challenge of exponential scaling in higher dimensions. The method introduces Clifford Kolmogorov-Arnold Networks (ClKAN), which employ Randomized Quasi Monte Carlo grid generation to manage dimensional complexity and incorporate novel batch normalization strategies for handling variable domain inputs. Experimental validation on synthetic and physics-inspired tasks demonstrates the model&#x27;s applicability in scientific discovery and engineering.</div>
<div class="mono" style="margin-top:8px">本研究旨在为克利福德代数空间中的函数逼近开发一种高效架构，以解决高维情况下指数级扩展的挑战。该方法引入了克利福德Kolmogorov-Arnold网络（ClKAN），采用随机拟蒙特卡洛网格生成来管理维度复杂性，并整合了新的批归一化策略以处理可变域输入。在合成和物理启发任务上的实验验证证明了该模型在科学发现和工程领域的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space</div>
<div class="meta-line">Authors: Felipe D. Toro-Hernández, Jesuino Vieira Filho, Rodrigo M. Cabral-Carvalho</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-05T18:23:04+00:00 · Latest: 2026-02-05T18:23:04+00:00</div>
<div class="meta-line">Comments: 10 pages, 6 figures (excluding refs/appendix). Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05971v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05971v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic representations can be framed as a structured, dynamic knowledge space through which humans navigate to retrieve and manipulate meaning. To investigate how humans traverse this geometry, we introduce a framework that represents concept production as navigation through embedding space. Using different transformer text embedding models, we construct participant-specific semantic trajectories based on cumulative embeddings and extract geometric and dynamical metrics, including distance to next, distance to centroid, entropy, velocity, and acceleration. These measures capture both scalar and directional aspects of semantic navigation, providing a computationally grounded view of semantic representation search as movement in a geometric space. We evaluate the framework on four datasets across different languages, spanning different property generation tasks: Neurodegenerative, Swear verbal fluency, Property listing task in Italian, and in German. Across these contexts, our approach distinguishes between clinical groups and concept types, offering a mathematical framework that requires minimal human intervention compared to typical labor-intensive linguistic pre-processing methods. Comparison with a non-cumulative approach reveals that cumulative embeddings work best for longer trajectories, whereas shorter ones may provide too little context, favoring the non-cumulative alternative. Critically, different embedding models yielded similar results, highlighting similarities between different learned representations despite different training pipelines. By framing semantic navigation as a structured trajectory through embedding space, bridging cognitive modeling with learned representation, thereby establishing a pipeline for quantifying semantic representation dynamics with applications in clinical research, cross-linguistic analysis, and the assessment of artificial cognition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将人类概念生成中的语义导航表征为嵌入空间中的轨迹</div>
<div class="mono" style="margin-top:8px">语义表征可被视作一个结构化、动态的知识空间，人类通过在其中导航来检索和操纵意义。为探究人类如何在此几何结构中穿行，我们提出一个框架，将概念生成表征为嵌入空间中的导航过程。利用不同的Transformer文本嵌入模型，我们基于累积嵌入构建参与者特定的语义轨迹，并提取几何与动态指标，包括至下一概念的距离、至质心的距离、熵、速度及加速度。这些度量同时捕捉语义导航的标量与方向性特征，为语义表征搜索提供了基于计算的几何空间运动视角。我们在跨语言的四个数据集上评估该框架，涵盖不同属性生成任务：神经退行性疾病、言语流畅性（脏话）、意大利语属性列举任务及德语属性列举任务。在这些情境中，我们的方法能区分临床组别与概念类型，提供了一个相比传统高人工强度的语言预处理方法更少依赖人工干预的数学框架。与非累积方法的比较表明，累积嵌入在较长轨迹中效果最佳，而较短轨迹可能因语境不足而更适合非累积方法。关键的是，不同嵌入模型得出相似结果，凸显了尽管训练流程不同，但不同习得表征间存在共性。通过将语义导航框架化为嵌入空间中的结构化轨迹，本研究连接了认知建模与习得表征，从而建立了一个量化语义表征动态的流程，可应用于临床研究、跨语言分析及人工认知评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand how humans navigate semantic knowledge during concept production, this study frames semantic search as movement through embedding space. The method constructs participant-specific semantic trajectories from cumulative embeddings using transformer models, extracting geometric and dynamical metrics like distance, entropy, velocity, and acceleration to capture both scalar and directional aspects of navigation. Experimental results across four multilingual datasets show the framework effectively distinguishes clinical groups and concept types with minimal human intervention, reveals that cumulative embeddings outperform non-cumulative ones for longer trajectories, and demonstrates consistent outcomes across different embedding models, highlighting shared representational structures.</div>
<div class="mono" style="margin-top:8px">为理解人类在概念生成过程中如何导航语义知识，本研究将语义搜索构建为在嵌入空间中的移动。该方法使用累积的Transformer文本嵌入，基于概念序列构建参与者特定的语义轨迹，并提取距离、熵、速度和加速度等几何与动态指标。在四个多语言数据集上的实验结果表明，该框架能以最少的人工干预有效区分临床组和概念类型，揭示累积嵌入适用于较长轨迹而非累积方法更适合短序列，并证明了不同嵌入模型间的稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">Inverse Depth Scaling From Most Layers Being Similar</div>
<div class="meta-line">Authors: Yizhou Liu, Sara Kangaslahti, Ziming Liu, Jeff Gore</div>
<div class="meta-line">First: 2026-02-05T18:22:41+00:00 · Latest: 2026-02-05T18:22:41+00:00</div>
<div class="meta-line">Comments: 23 pages, 24 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05970v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>源于多数层相似性的逆深度缩放规律</div>
<div class="mono" style="margin-top:8px">神经缩放定律描述了大型语言模型（LLMs）中损失与模型规模的关系，但深度与宽度对性能的影响机制可能存在差异，需要更精细的研究。本文通过分析LLMs和玩具残差网络，量化了深度对损失的影响。研究发现LLMs中的损失与深度成反比缩放，这可能源于功能相似的层通过集成平均降低误差，而非组合式学习或离散化平滑动态过程。这种机制虽效率较低但具有鲁棒性，可能源自残差网络的结构偏置以及与平滑动态不兼容的目标函数。研究结果表明，提升LLM效率可能需要通过架构创新来促进深度的组合式利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates how depth specifically influences performance in large language models, motivated by the need to move beyond general scaling laws that treat model size uniformly. By analyzing both real LLMs and toy residual networks, the method quantifies the relationship between loss and depth. The key experimental finding is that loss scales inversely with depth, a pattern attributed to functionally similar layers performing an inefficient ensemble averaging rather than enabling compositional learning or discretizing smooth dynamics; this suggests that architectural biases and incompatible target functions lead to this robust but inefficient regime, pointing to a need for architectural innovations to improve efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究深度如何具体影响大语言模型的性能，动机在于需要超越将模型尺寸统一对待的通用缩放定律。通过分析大语言模型和玩具残差网络，方法揭示了损失与深度成反比缩放，这主要是因为许多层在功能上相似，像集成一样平均误差，而非实现组合学习或离散化平滑动态。关键实验结果表明，这种机制效率低下但鲁棒性强，可能源于残差网络的结构偏差和目标函数不适用于平滑动态，这表明需要通过架构创新来促使深度得到更高效的组合性利用。</div>
</details>
</div>
<div class="card">
<div class="title">A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders</div>
<div class="meta-line">Authors: Mohamad Amin Jamshidi, Mehrbod Zarifi, Zolfa Anvari, Hamed Ghafarirad, Mohammad Zareinejad</div>
<div class="meta-line">First: 2026-02-05T18:21:28+00:00 · Latest: 2026-02-05T18:21:28+00:00</div>
<div class="meta-line">Comments: Published in: 2025 33rd International Conference on Electrical Engineering (ICEE), Publisher IEEE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05967v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05967v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hydraulic systems are widely utilized in industrial applications due to their high force generation, precise control, and ability to function in harsh environments. Hydraulic cylinders, as actuators in these systems, apply force and position through the displacement of hydraulic fluid, but their operation is significantly influenced by friction force. Achieving precision in hydraulic cylinders requires an accurate friction model under various operating conditions. Existing analytical models, often derived from experimental tests, necessitate the identification or estimation of influencing factors but are limited in adaptability and computational efficiency. This research introduces a data-driven, hybrid algorithm based on Long Short-Term Memory (LSTM) networks and Random Forests for nonlinear friction force estimation. The algorithm effectively combines feature detection and estimation processes using training data acquired from an experimental hydraulic test setup. It achieves a consistent and stable model error of less than 10% across diverse operating conditions and external load variations, ensuring robust performance in complex situations. The computational cost of the algorithm is 1.51 milliseconds per estimation, making it suitable for real-time applications. The proposed method addresses the limitations of analytical models by delivering high precision and computational efficiency. The algorithm&#x27;s performance is validated through detailed analysis and experimental results, including direct comparisons with the LuGre model. The comparison highlights that while the LuGre model offers a theoretical foundation for friction modeling, its performance is limited by its inability to dynamically adjust to varying operational conditions of the hydraulic cylinder, further emphasizing the advantages of the proposed hybrid approach in real-time applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于液压缸实时摩擦力估计的混合数据驱动算法</div>
<div class="mono" style="margin-top:8px">液压系统因其高力输出、精确控制及恶劣环境适应能力而广泛应用于工业领域。液压缸作为系统中的执行器，通过液压油位移实现力和位置控制，但其运行受摩擦力显著影响。实现液压缸精确控制需要不同工况下的准确摩擦力模型。现有解析模型多基于实验推导，需识别或估计影响因素，但适应性与计算效率有限。本研究提出一种基于长短期记忆网络与随机森林的数据驱动混合算法，用于非线性摩擦力估计。该算法利用实验液压测试平台获取的训练数据，有效结合特征检测与估计过程，在多种工况及外部负载变化下实现稳定且低于10%的模型误差，确保复杂场景下的鲁棒性。算法单次估计计算成本为1.51毫秒，适用于实时应用。该方法通过高精度与计算效率克服了解析模型的局限性。通过详细分析与实验结果（包括与LuGre模型的直接对比）验证了算法性能：对比表明，LuGre模型虽为摩擦建模提供理论基础，但其性能受限于无法动态适应液压缸工况变化，进一步凸显了所提混合方法在实时应用中的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of traditional analytical friction models in hydraulic cylinders, which lack adaptability and computational efficiency for real-time use, this study proposes a hybrid data-driven algorithm combining Long Short-Term Memory networks and Random Forests for nonlinear friction force estimation. The method integrates feature detection and estimation using training data from an experimental hydraulic test setup. Experimental validation shows the algorithm maintains a model error below 10% across diverse operating conditions and load variations, with a computational cost of 1.51 milliseconds per estimation, enabling real-time application and outperforming the static LuGre model in dynamic adaptability.</div>
<div class="mono" style="margin-top:8px">针对液压缸中传统解析摩擦模型适应性差、计算效率低、难以实时应用的问题，本研究提出了一种结合长短期记忆网络和随机森林的混合数据驱动算法，用于非线性摩擦力估计。该方法利用从实验液压测试装置获取的训练数据，整合特征检测与估计过程，在不同工况和负载变化下实现了低于10%的稳定模型误差。实验验证表明，该算法每次估计耗时仅1.51毫秒，适用于实时应用，并且通过动态适应变化条件，性能优于LuGre模型。</div>
</details>
</div>
<div class="card">
<div class="title">Informed Asymmetric Actor-Critic: Leveraging Privileged Signals Beyond Full-State Access</div>
<div class="meta-line">Authors: Daniel Ebi, Gaspard Lambrechts, Damien Ernst, Klemens Böhm</div>
<div class="meta-line">First: 2025-09-30T09:32:20+00:00 · Latest: 2026-02-05T18:21:20+00:00</div>
<div class="meta-line">Comments: 11 pages, 26 pages total, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26000v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26000v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Asymmetric actor-critic methods are widely used in partially observable reinforcement learning, but typically assume full state observability to condition the critic during training, which is often unrealistic in practice. We introduce the informed asymmetric actor-critic framework, allowing the critic to be conditioned on arbitrary state-dependent privileged signals without requiring access to the full state. We show that any such privileged signal yields unbiased policy gradient estimates, substantially expanding the set of admissible privileged information. This raises the problem of selecting the most adequate privileged information in order to improve learning. For this purpose, we propose two novel informativeness criteria: a dependence-based test that can be applied prior to training, and a criterion based on improvements in value prediction accuracy that can be applied post-hoc. Empirical results on partially observable benchmark tasks and synthetic environments demonstrate that carefully selected privileged signals can match or outperform full-state asymmetric baselines while relying on strictly less state information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知情非对称行动者-评论者：利用超越全状态访问的特权信号</div>
<div class="mono" style="margin-top:8px">非对称行动者-评论者方法在部分可观测强化学习中广泛应用，但通常假设评论者在训练期间能基于全状态条件进行学习，这在实践中往往不现实。我们提出知情非对称行动者-评论者框架，允许评论者基于任意与状态相关的特权信号进行条件化，而无需访问完整状态。我们证明任何此类特权信号均可产生无偏策略梯度估计，从而显著扩展了可采纳特权信息的范围。这引出了如何选择最合适特权信息以提升学习效果的问题。为此，我们提出两种新颖的信息量准则：一种可在训练前应用的基于依赖关系的检验方法，以及一种基于价值预测精度改进的事后评估准则。在部分可观测基准任务和合成环境中的实验结果表明，精心选择的特权信号在依赖严格更少状态信息的情况下，能够达到或超越全状态非对称基线方法的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Asymmetric actor-critic methods in partially observable reinforcement learning typically require the critic to have access to the full state during training, an assumption that is often impractical. To address this, the authors propose the informed asymmetric actor-critic framework, which allows the critic to be conditioned on arbitrary state-dependent privileged signals without needing the full state, and prove this yields unbiased policy gradient estimates. They further introduce two informativeness criteria—a dependence-based pre-training test and a post-hoc value prediction accuracy criterion—to select effective privileged signals. Experiments on partially observable benchmarks and synthetic environments show that well-chosen privileged signals can perform as well as or better than full-state baselines while using strictly less state information.</div>
<div class="mono" style="margin-top:8px">非对称行动者-评论家方法通常要求评论家在训练期间能够访问完整状态，但这在实践中往往不现实。本研究提出了信息非对称行动者-评论家框架，允许评论家基于任意与状态相关的特权信号进行条件化，而无需访问完整状态，同时保持策略梯度估计的无偏性。为了选择有效的特权信号，作者提出了两个信息量标准：一种可在训练前应用的基于依赖性的测试，以及一种基于价值预测准确性改进的事后标准。在部分可观测基准任务和合成环境上的实验表明，精心选择的特权信号能够匹配甚至超越完整状态的非对称基线方法，同时依赖更少的状态信息。</div>
</details>
</div>
<div class="card">
<div class="title">Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces</div>
<div class="meta-line">Authors: Arran Carter, Sanghyeok Choi, Kirill Tamogashev, Víctor Elvira, Nikolay Malkin</div>
<div class="meta-line">First: 2026-02-05T18:16:57+00:00 · Latest: 2026-02-05T18:16:57+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/mmacosha/offpolicy-discrete-diffusion-samplers-and-bridges</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05961v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05961v1">PDF</a> · <a href="https://github.com/mmacosha/offpolicy-discrete-diffusion-samplers-and-bridges">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sampling from a distribution $p(x) \propto e^{-\mathcal{E}(x)}$ known up to a normalising constant is an important and challenging problem in statistics. Recent years have seen the rise of a new family of amortised sampling algorithms, commonly referred to as diffusion samplers, that enable fast and efficient sampling from an unnormalised density. Such algorithms have been widely studied for continuous-space sampling tasks; however, their application to problems in discrete space remains largely unexplored. Although some progress has been made in this area, discrete diffusion samplers do not take full advantage of ideas commonly used for continuous-space sampling. In this paper, we propose to bridge this gap by introducing off-policy training techniques for discrete diffusion samplers. We show that these techniques improve the performance of discrete samplers on both established and new synthetic benchmarks. Next, we generalise discrete diffusion samplers to the task of bridging between two arbitrary distributions, introducing data-to-energy Schrödinger bridge training for the discrete domain for the first time. Lastly, we showcase the application of the proposed diffusion samplers to data-free posterior sampling in the discrete latent spaces of image generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离散扩散采样器与桥接：离轨策略算法及其在隐空间的应用</div>
<div class="mono" style="margin-top:8px">从已知归一化常数的分布 $p(x) \propto e^{-\mathcal{E}(x)}$ 中采样是统计学中一个重要且具有挑战性的问题。近年来，一类被称为扩散采样器的摊销采样算法兴起，能够从非归一化密度中实现快速高效采样。这类算法在连续空间采样任务中已得到广泛研究，但在离散空间问题中的应用仍鲜有探索。尽管该领域已取得一些进展，但现有离散扩散采样器未能充分利用连续空间采样的常用思想。本文通过为离散扩散采样器引入离轨策略训练技术来弥合这一差距，实验表明该技术能提升离散采样器在经典及新型合成基准测试中的性能。进一步，我们将离散扩散采样器推广至两个任意分布间的桥接任务，首次在离散域提出数据到能量的薛定谔桥训练方法。最后，我们展示了所提扩散采样器在图像生成模型离散隐空间中无数据后验采样的应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of sampling from unnormalized discrete distributions, where existing diffusion samplers have not fully leveraged techniques successful in continuous spaces. The method introduces off-policy training for discrete diffusion samplers and extends them to bridge arbitrary distributions via a discrete Schrödinger bridge formulation. Experiments demonstrate improved performance on synthetic benchmarks and successful application to data-free posterior sampling in discrete latent spaces of image generative models.</div>
<div class="mono" style="margin-top:8px">本研究针对从未归一化离散分布中采样的挑战，现有离散扩散采样器未能充分利用连续空间中的成功技术。作者引入了离策略训练方法来增强离散扩散采样器，并将其推广到解决连接任意分布的离散薛定谔桥问题。实验表明，该方法在合成基准测试中提升了采样器性能，并成功应用于图像生成模型离散潜空间中的数据无关后验采样。</div>
</details>
</div>
<div class="card">
<div class="title">Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</div>
<div class="meta-line">Authors: Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim</div>
<div class="meta-line">First: 2026-02-05T18:08:20+00:00 · Latest: 2026-02-05T18:08:20+00:00</div>
<div class="meta-line">Comments: Project Page: https://junwankimm.github.io/CSFM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05951v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05951v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://junwankimm.github.io/CSFM">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更优源分布，更佳流匹配：学习条件依赖的源分布用于流匹配</div>
<div class="mono" style="margin-top:8px">流匹配技术近期已成为扩散生成模型的有力替代方案，尤其在文本到图像生成领域。尽管该方法允许使用任意源分布，但现有研究大多沿用扩散模型的标准高斯分布，鲜少将源分布本身作为优化目标。本研究表明，在现代文本到图像系统中，源分布的原则性设计不仅可行且能带来显著收益。我们提出在流匹配目标下学习条件依赖的源分布，以更充分利用丰富的条件信号。通过分析直接引入条件导致的分布坍缩与不稳定等关键失效模式，我们发现适当的方差正则化及源-目标分布的方向对齐对稳定高效学习至关重要。进一步探究目标表示空间选择对结构化源分布流匹配的影响，揭示了该设计最具效能的适用场景。在多个文本到图像基准测试中的广泛实验表明，该方法能带来持续稳健的性能提升，包括FID指标收敛速度最高提升3倍，彰显了条件流匹配中原则性源分布设计的实用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Flow matching offers a flexible generative modeling alternative to diffusion models, but existing methods typically adopt a standard Gaussian source distribution without optimizing it for conditional tasks like text-to-image generation. This work proposes learning a condition-dependent source distribution under the flow matching objective to better leverage conditioning signals, addressing key challenges such as distributional collapse and instability through variance regularization and directional alignment between source and target. Experiments on text-to-image benchmarks show that this principled source design yields consistent improvements, including up to a 3x faster convergence in FID, demonstrating its practical efficacy.</div>
<div class="mono" style="margin-top:8px">流匹配作为一种灵活的生成建模框架，通常采用标准高斯分布作为源分布，这可能无法充分利用条件信号。本研究提出在流匹配目标中学习一个条件依赖的源分布，并通过方差正则化以及源与目标之间的方向对齐来解决分布坍缩和不稳定性等关键问题。在多个文本到图像基准上的实验表明，这种原则性的源分布设计带来了持续的改进，包括FID收敛速度最高提升3倍。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking Symmetry Bottlenecks in GNN Readouts</div>
<div class="meta-line">Authors: Mouad Talhi, Arne Wolf, Anthea Monod</div>
<div class="meta-line">First: 2026-02-05T18:08:13+00:00 · Latest: 2026-02-05T18:08:13+00:00</div>
<div class="meta-line">Comments: 23 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05950v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05950v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural networks (GNNs) are widely used for learning on structured data, yet their ability to distinguish non-isomorphic graphs is fundamentally limited. These limitations are usually attributed to message passing; in this work we show that an independent bottleneck arises at the readout stage. Using finite-dimensional representation theory, we prove that all linear permutation-invariant readouts, including sum and mean pooling, factor through the Reynolds (group-averaging) operator and therefore project node embeddings onto the fixed subspace of the permutation action, erasing all non-trivial symmetry-aware components regardless of encoder expressivity. This yields both a new expressivity barrier and an interpretable characterization of what global pooling preserves or destroys. To overcome this collapse, we introduce projector-based invariant readouts that decompose node representations into symmetry-aware channels and summarize them with nonlinear invariant statistics, preserving permutation invariance while retaining information provably invisible to averaging. Empirically, swapping only the readout enables fixed encoders to separate WL-hard graph pairs and improves performance across multiple benchmarks, demonstrating that readout design is a decisive and under-appreciated factor in GNN expressivity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>突破图神经网络读出层的对称性瓶颈</div>
<div class="mono" style="margin-top:8px">图神经网络（GNN）广泛应用于结构化数据学习，但其区分非同构图的能力存在根本性局限。这些局限通常归因于消息传递机制；本研究揭示读出阶段存在独立的瓶颈。通过有限维表示理论，我们证明所有线性置换不变读出操作（包括求和与均值池化）均通过雷诺兹（群平均）算子分解，从而将节点嵌入投影至置换作用的不变子空间，无论编码器表达能力如何，都会消除所有非平凡的对称感知分量。这既产生了新的表达能力界限，也对全局池化保留或破坏的信息提供了可解释的特征描述。为克服此坍缩问题，我们提出基于投影算子的不变读出方法：将节点表示分解为对称感知通道，并通过非线性不变统计量进行汇总，在保持置换不变性的同时，保留可证明无法被平均操作捕获的信息。实证表明，仅替换读出模块即可使固定编码器区分WL-hard图对，并在多个基准测试中提升性能，证明读出设计是影响GNN表达能力的关键且未被充分重视的因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses a previously overlooked expressivity bottleneck in graph neural networks (GNNs) that arises from permutation-invariant readout functions like sum or mean pooling, which can erase symmetry-aware information regardless of encoder power. Using finite-dimensional representation theory, the authors prove that linear permutation-invariant readouts project node embeddings onto a fixed subspace, losing non-trivial components; they then propose projector-based invariant readouts that decompose representations into symmetry-aware channels and apply nonlinear invariant statistics to preserve critical information. Experiments show that replacing only the readout enables fixed encoders to separate Weisfeiler-Lehman-hard graph pairs and improves performance across multiple benchmarks, highlighting readout design as a decisive factor for GNN expressivity.</div>
<div class="mono" style="margin-top:8px">本研究发现了图神经网络（GNN）中一个先前被忽视的表达能力瓶颈，该瓶颈独立于已知的消息传递限制，存在于读出阶段。利用有限维表示理论，研究证明标准的线性置换不变读出（如求和或平均池化）通过雷诺兹算子将节点嵌入投影到一个固定子空间，无论编码器能力如何，都会丢弃所有非平凡的对称感知信息。为克服此问题，作者引入了基于投影子的不变读出方法，该方法将表示分解为对称感知通道并应用非线性不变统计量，在保持置换不变性的同时保留原本会丢失的信息。实验表明，仅替换读出模块就能使固定编码器区分WL-困难的图对，并在多个基准测试中提升性能，从而确立了读出设计是影响GNN表达能力的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-02-05T18:03:03+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v2">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题探索新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时实施强化学习，使大型语言模型能够持续训练，并针对具体测试问题积累经验。这种持续学习形式具有特殊性：其目标是产生单一卓越解决方案而非平均意义上的多个良好方案，且专注于解决当前问题而非泛化至其他问题。因此，我们的学习目标和搜索子程序均优先考虑最具潜力的解决方案。我们将此方法称为“测试时训练发现法”。延续先前研究，我们聚焦于具有连续奖励的问题，并在数学、GPU内核工程、算法设计和生物学领域报告所有尝试问题的结果。该方法在几乎所有领域均创下新纪录：（1）埃尔德什最小重叠问题与自相关不等式；（2）GPUMode内核竞赛（较现有技术提速达2倍）；（3）历史AtCoder算法竞赛；（4）单细胞分析去噪问题。所有解决方案均经专家或主办方评审。与先前依赖封闭前沿模型的最佳成果不同，本研究全部结果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现。测试时训练通过Thinking Machines的Tinker API执行，每个问题成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance AI&#x27;s capability to discover novel state-of-the-art solutions for scientific problems, moving beyond prior test-time scaling methods that rely on prompting frozen LLMs. The proposed method, Test-Time Training to Discover (TTT-Discover), employs reinforcement learning at test time, allowing the LLM to continuously train with experience specific to the test problem, focusing on generating a single optimal solution rather than average performance across tasks. Key experimental results demonstrate that TTT-Discover achieves new state-of-the-art performance across diverse domains, including mathematics (Erdős&#x27; minimum overlap problem and an autocorrelation inequality), GPU kernel engineering (up to 2× faster than prior art), algorithm design (past AtCoder competitions), and biology (denoising in single-cell analysis), with all solutions validated by experts and achieved using an open model and publicly available code.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升人工智能在科学问题中发现新最优解的能力，超越先前依赖提示冻结大语言模型的测试时扩展方法。提出的方法“测试时训练发现”（TTT-Discover）在测试时采用强化学习，使大语言模型能够针对特定测试问题持续训练，专注于生成单一最优解而非平均性能。关键实验结果表明，TTT-Discover在多个领域实现了新的最优性能，包括数学（埃尔德什最小重叠问题和自相关不等式）、GPU内核工程（速度提升高达2倍）、算法设计（过往AtCoder竞赛）和生物学（单细胞分析去噪），且使用开源模型和可复现代码，成本较低。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260208_0444.html">20260208_0444</a>
<a href="archive/20260208_0331.html">20260208_0331</a>
<a href="archive/20260207_0630.html">20260207_0630</a>
<a href="archive/20260207_0534.html">20260207_0534</a>
<a href="archive/20260207_0451.html">20260207_0451</a>
<a href="archive/20260207_0345.html">20260207_0345</a>
<a href="archive/20260206_0629.html">20260206_0629</a>
<a href="archive/20260206_0531.html">20260206_0531</a>
<a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
