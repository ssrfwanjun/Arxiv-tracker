<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 04:39</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0439</div>
    <div class="row"><div class="card">
<div class="title">Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge</div>
<div class="meta-line">Authors: Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen, Ziye Wang, Ximeng Meng, Stone Tao, Yiran Qin, Xiaohong Liu, Ruimao Zhang, Lei Bai, Yilun Du, Hao Su, Philip Torr, Zhenfei Yin, Ruihao Gong, Yejun Zeng, Fengjun Zhong, Shenghao Jin, Jinyang Guo, Xianglong Liu, Xiaojun Jia, Tianqi Shan, Wenqi Ren, Simeng Qin, Jialing Yang, Xiaoyu Ma, Tianxing Chen, Zixuan Li, Zijian Cai, Yan Qin, Yusen Qin, Qiangyu Chen, Kaixuan Wang, Zhaoming Han, Yao Mu, Ping Luo, Yuanqi Yao, Haoming Song, Jan-Nico Zaech, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-26T17:56:19+00:00 · Latest: 2026-01-26T17:56:19+00:00</div>
<div class="meta-line">Comments: MARS Challenge @ NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI. Challenge page: https://mars-eai.github.io/MARS-Challenge-Webpage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18733v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mars-eai.github.io/MARS-Challenge-Webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体机器人系统（MARS）挑战赛的进展与创新</div>
<div class="mono" style="margin-top:8px">多模态大语言模型与视觉-语言-动作模型的最新进展显著推动了具身人工智能领域的发展。随着该领域向更复杂的任务场景过渡，多智能体系统框架正成为实现可扩展、高效协作解决方案的关键。这一转变主要受三大因素驱动：智能体能力持续增强、通过任务委派提升系统效率，以及实现更先进的人机交互。为应对多智能体协作带来的挑战，我们在NeurIPS 2025 SpaVLE研讨会上提出举办多智能体机器人系统（MARS）挑战赛。竞赛聚焦两大关键领域：规划与控制——参赛者将探索利用视觉语言模型进行多智能体具身规划以协调任务，并通过策略执行在动态环境中完成机器人操控。通过评估参赛方案，本挑战赛为具身多智能体系统的设计与协调提供宝贵见解，助力未来高级协作式人工智能系统的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work arises from the need to address complex task scenarios in Embodied AI, where multi-agent systems are essential for scalable and collaborative solutions. The method involves organizing the Multi-Agent Robotic System (MARS) Challenge at the NeurIPS 2025 Workshop on SpaVLE, focusing on multi-agent embodied planning using vision-language models and policy execution for robotic manipulation. Key experimental findings from evaluating participant submissions provide insights into the design and coordination of embodied multi-agent systems, advancing the development of collaborative AI.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于具身人工智能中复杂任务场景的需求，多智能体系统对于实现可扩展和协作的解决方案至关重要。方法包括组织多智能体机器人系统（MARS）挑战赛，该赛事专注于利用视觉语言模型进行多智能体具身规划，以及在动态环境中执行机器人操作策略。通过评估参赛者提交的方案，关键实验结果为具身多智能体系统的设计和协调提供了宝贵见解，推动了协作式人工智能系统的未来发展。</div>
</details>
</div>
<div class="card">
<div class="title">MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</div>
<div class="meta-line">Authors: Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Zihan Dong, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Linjun Zhang, Shujie Liu, Yan Lu, Huaxiu Yao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-31T13:22:55+00:00 · Latest: 2026-01-26T17:15:26+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00555v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00555v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6% over strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMedAgent-RL：优化多智能体协作以实现多模态医学推理</div>
<div class="mono" style="margin-top:8px">医学大型视觉语言模型（Med-LVLMs）在多模态诊断任务中展现出强大潜力。然而，现有单智能体模型难以泛化至不同医学专科，限制了其性能。近期研究借鉴临床工作流程，引入了全科医生与专科医生按固定顺序交互的多智能体协作框架。尽管有所改进，这些静态流程在推理中缺乏灵活性与适应性。为此，我们提出MMedAgent-RL——一种基于强化学习（RL）的多智能体框架，可实现医学智能体间的动态优化协作。具体而言，我们通过RL训练两个基于Qwen2.5-VL的全科医生智能体：分诊医生学习将患者分配至合适专科，而主治医生则整合多专科判断与自身知识做出最终决策。针对专科医生输出的不一致性，我们引入课程学习（CL）引导的RL策略，通过动态熵调节逐步指导主治医生在模仿专科医生与纠正其错误之间取得平衡。在五个医学VQA基准测试上的实验表明，MMedAgent-RL性能优于开源及专有Med-LVLMs，较基线模型平均性能提升达23.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing single-agent medical large vision-language models face challenges in generalizing across diverse medical specialties, and recent multi-agent frameworks with fixed collaboration sequences lack flexibility. To address this, the authors propose MMedAgent-RL, a reinforcement learning-based multi-agent framework that enables dynamic collaboration, featuring a triage doctor agent that assigns patients to specialties and an attending physician agent that integrates multi-specialist judgments. They employ a curriculum learning-guided RL strategy with dynamic entropy regulation to help the attending physician balance imitating specialists and correcting errors. Experiments on five medical VQA benchmarks show that MMedAgent-RL outperforms both open-source and proprietary models, achieving an average performance gain of 23.6% over strong baselines.</div>
<div class="mono" style="margin-top:8px">该研究针对单一智能体医学大视觉语言模型在不同专科领域泛化能力有限的问题，提出了基于强化学习的多智能体协作框架MMedAgent-RL，以实现动态优化的协作。该方法通过强化学习训练两个全科医生智能体：一个负责分诊以将患者分配给专科医生，另一个作为主治医生整合专科医生的判断，并采用课程学习引导的强化学习策略与动态熵调节来处理专科医生输出的不一致性。在五个医学视觉问答基准测试上的实验结果表明，MMedAgent-RL超越了现有的开源和专有模型，相对于强基线平均性能提升了23.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge</div>
<div class="meta-line">Authors: Xiao Liu, Jiawei Zhang</div>
<div class="meta-line">First: 2026-01-26T17:14:57+00:00 · Latest: 2026-01-26T17:14:57+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18698v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成模型是否具有地理公平性？基于景点中心的全球视觉知识评估</div>
<div class="mono" style="margin-top:8px">近期文本到视频生成技术取得了视觉上引人注目的进展，但这些模型是否编码了地理公平的视觉知识仍不明确。本研究通过以景点为中心的评估，探究文本到视频模型的地理公平性与地理锚定视觉知识。我们提出地理景点地标探测框架——一种系统评估模型对全球不同地区旅游景点合成忠实度的框架，并构建了GEOATTRACTION-500基准数据集，涵盖500个分布全球、覆盖不同区域与知名度层级的景点。该框架整合了互补性指标，将整体视频质量与景点特定知识解耦，包括全局结构对齐、基于细粒度关键点的对齐以及视觉语言模型判断，所有指标均通过人工评估验证。将该框架应用于当前最先进的文本到视频模型Sora 2后，我们发现：与普遍存在强烈地理偏见的假设相反，该模型在不同地区、发展水平和文化群体间展现出相对均匀的地理锚定视觉知识水平，仅与景点知名度呈微弱关联。这些结果表明，当前文本到视频模型表达的全球视觉知识比预期更为均衡，既凸显了其在全球部署应用中的潜力，也揭示了此类系统演进过程中持续评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates whether text-to-video generation models encode geographically equitable visual knowledge, motivated by the need to assess potential biases as these models are deployed globally. The authors propose the Geo-Attraction Landmark Probing (GAP) framework, which systematically evaluates how models synthesize tourist attractions from diverse regions using a new benchmark, GEOATTRACTION-500, and complementary metrics for structural, keypoint-based, and vision-language alignment. Applying GAP to Sora 2 reveals that the model exhibits a relatively uniform level of geographically grounded visual knowledge across different regions, development levels, and cultural groups, with only weak dependence on attraction popularity, suggesting more even global knowledge representation than commonly assumed.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究文本到视频生成模型是否编码了地理上公平的视觉知识，其动机在于评估这些模型在全球部署时潜在的偏见。作者提出了地理吸引力地标探测框架，通过构建包含500个全球景点的基准数据集，并采用结构对齐、关键点对齐和视觉语言模型判断等互补指标进行系统评估。将该框架应用于Sora 2模型的结果表明，该模型在不同地区、发展水平和文化群体中表现出相对一致的地理视觉知识水平，对景点知名度的依赖较弱，这提示其全球知识表征比通常假设的更为均衡。</div>
</details>
</div>
<div class="card">
<div class="title">A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models</div>
<div class="meta-line">Authors: Shihab Aaqil Ahamed, Udaya S. K. P. Miriya Thanthrige, Ranga Rodrigo, Muhammad Haris Khan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-30T12:45:24+00:00 · Latest: 2026-01-26T17:12:54+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26441v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26441v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs&#x27; reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A-TPT：视觉语言模型测试时提示调优的角向多样性校准特性</div>
<div class="mono" style="margin-top:8px">测试时提示调优（TPT）已成为一种无需依赖标注数据即可将大型视觉语言模型（VLM）适配到未见任务的有效技术。然而，文本特征间缺乏分散性会损害校准性能，引发对VLM可靠性、可信度与安全性的担忧。现有TPT方法主要通过最大化平均文本特征分散性或施加正交约束以促进角向分离来改进提示校准，但这些方法可能无法始终实现类间文本特征的最优角向分离，忽视了角向多样性的关键作用。为此，我们提出A-TPT——一种新颖的TPT框架，通过引入角向多样性来促进由可学习提示生成的归一化文本特征在单位超球面上均匀分布。该均匀性通过最大化特征间最小成对角向距离实现。我们在多个数据集上使用不同骨干网络进行大量实验，结果表明该方法在保持相当准确度的同时，能持续超越现有最优TPT方法以降低聚合平均校准误差。值得注意的是，该方法在自然分布偏移上表现出卓越的零样本校准性能，并能良好泛化至医学数据集。我们通过包含理论分析在内的系统性研究确立了A-TPT的基础。这些结果凸显了促进角向多样性以实现充分分散的文本特征对于显著提升VLM在测试时适配过程中校准性能的有效性。代码将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Test-time prompt tuning adapts vision-language models to new tasks without labeled data, but poor textual feature dispersion can degrade calibration, affecting model reliability. This work introduces A-TPT, a framework that explicitly promotes angular diversity by maximizing the minimum pairwise angular distance between normalized textual features on a unit hypersphere to achieve a more uniform distribution. Experiments across various backbones and datasets show that A-TPT consistently reduces average calibration error compared to state-of-the-art methods while maintaining accuracy, and it demonstrates strong zero-shot calibration performance under distribution shifts and on medical datasets.</div>
<div class="mono" style="margin-top:8px">测试时提示调优（TPT）可在无标注数据下使视觉语言模型适应新任务，但文本特征分散性不足会损害校准性能，影响模型可靠性。为此，A-TPT通过最大化单位超球面上归一化文本特征之间的最小成对角距离，引入角度多样性以确保均匀分布。在不同骨干网络和数据集上的实验表明，A-TPT在保持准确性的同时持续降低了平均校准误差，在自然分布偏移和医学数据上表现出优异的零样本校准性能，证明了角度多样性对提升校准的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Pragmatic VLA Foundation Model</div>
<div class="meta-line">Authors: Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng</div>
<div class="meta-line">First: 2026-01-26T17:08:04+00:00 · Latest: 2026-01-26T17:08:04+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18692v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18692v1">PDF</a> · <a href="https://github.com/Robbyant/lingbot-vla/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种实用的VLA基础模型</div>
<div class="mono" style="margin-top:8px">在机器人操作领域展现出巨大潜力，一种强大的视觉-语言-动作（VLA）基础模型有望在任务和平台间实现忠实泛化，同时确保成本效益（例如适应所需的数据和GPU时数）。为此，我们利用来自9种主流双机械臂配置约20,000小时的真实世界数据开发了LingBot-VLA。通过对3个机器人平台进行系统评估（每个平台完成100项任务，每项任务包含130次训练后测试），我们的模型展现出明显优于同类模型的性能，体现了其卓越的执行力和广泛的泛化能力。我们还构建了高效代码库，在8-GPU训练配置下实现每秒每GPU处理261个样本的吞吐量，相比现有VLA专用代码库提速1.5~2.8倍（具体取决于所基于的VLM基础模型）。上述特性确保我们的模型非常适合实际部署。为推进机器人学习领域发展，我们公开提供代码、基础模型和基准数据，重点关注支持更具挑战性的任务并促进健全的评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for a cost-efficient and generalizable Vision-Language-Action (VLA) foundation model for robotic manipulation, this work introduces LingBot-VLA, trained on approximately 20,000 hours of real-world data from nine dual-arm robot configurations. The method is evaluated through a systematic assessment on three robotic platforms, each performing 100 tasks with 130 post-training episodes per task, where the model demonstrates clear superiority over competitors, indicating strong performance and broad generalizability. Additionally, the developed efficient codebase achieves a throughput of 261 samples per second per GPU on an 8-GPU setup, representing a 1.5 to 2.8 times speedup over existing VLA-oriented codebases, facilitating real-world deployment.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种成本高效且泛化能力强的视觉-语言-动作基础模型，用于机器人操作。方法包括在来自九种双臂机器人配置的大约20,000小时真实世界数据上训练LingBot-VLA模型，并实现一个高效代码库以实现高吞吐量训练。实验结果表明，在三个机器人平台上对100项任务的系统评估中，该模型明显优于竞争对手，且代码库相比现有VLA框架实现了1.5至2.8倍的加速，确保了强大的性能和对实际部署的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP&#x27;s Visual Embedding Projector is a Few-shot Cornucopia</div>
<div class="meta-line">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2024-10-07T17:59:59+00:00 · Latest: 2026-01-26T14:50:34+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05270v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.05270v4">PDF</a> · <a href="https://github.com/astra-vision/ProLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ProLIP, a simple and architecture-agnostic method for adapting contrastively pretrained vision-language models, such as CLIP, to few-shot classification. ProLIP fine-tunes the vision encoder&#x27;s projection matrix with Frobenius norm regularization on its deviation from the pretrained weights. It achieves state-of-the-art performance on 11 few-shot classification benchmarks under both ``few-shot validation&#x27;&#x27; and ``validation-free&#x27;&#x27; settings. Moreover, by rethinking the non-linear CLIP-Adapter through ProLIP&#x27;s lens, we design a Regularized Linear Adapter (RLA) that performs better, requires no hyperparameter tuning, is less sensitive to learning rate values, and offers an alternative to ProLIP in black-box scenarios where model weights are inaccessible. Beyond few-shot classification, ProLIP excels in cross-dataset transfer, domain generalization, base-to-new class generalization, and test-time adaptation--where it outperforms prompt tuning while being an order of magnitude faster to train. Code is available at https://github.com/astra-vision/ProLIP .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP视觉嵌入投影器：小样本学习的丰饶之角</div>
<div class="mono" style="margin-top:8px">我们提出ProLIP——一种简单且架构无关的方法，用于将对比预训练的视觉-语言模型（如CLIP）适配到小样本分类任务。ProLIP通过Frobenius范数正则化微调视觉编码器的投影矩阵，约束其与预训练权重的偏差。该方法在11个小样本分类基准测试中，于“小样本验证”和“免验证”两种设置下均达到最先进性能。此外，通过ProLIP的视角重新审视非线性CLIP-Adapter，我们设计了正则化线性适配器（RLA），其性能更优、无需超参数调优、对学习率值更不敏感，并在模型权重不可访问的黑盒场景中为ProLIP提供了替代方案。除小样本分类外，ProLIP在跨数据集迁移、领域泛化、基类到新类泛化及测试时适配方面表现卓越——其训练速度比提示调优快一个数量级的同时性能更优。代码发布于https://github.com/astra-vision/ProLIP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to efficiently adapt contrastively pretrained vision-language models like CLIP for few-shot classification tasks. The proposed method, ProLIP, fine-tunes the vision encoder&#x27;s projection matrix with Frobenius norm regularization on its deviation from the pretrained weights, offering a simple and architecture-agnostic approach. Experiments show ProLIP achieves state-of-the-art performance on 11 few-shot classification benchmarks, and its analysis leads to a Regularized Linear Adapter (RLA) that is more robust and requires less hyperparameter tuning; the method also demonstrates strong performance in cross-dataset transfer, domain generalization, and test-time adaptation, outperforming prompt tuning while training much faster.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决如何高效地将CLIP等预训练视觉语言模型适配到少样本分类任务中的问题。所提出的ProLIP方法仅对视觉编码器的投影矩阵进行微调，并对其与预训练权重的偏差施加Frobenius范数正则化，使其简单且与架构无关。实验结果表明，ProLIP在11个少样本基准测试中取得了最先进的性能，其分析还催生了一个更鲁棒、无需超参数调优的正则化线性适配器（RLA）；该方法在跨数据集迁移、领域泛化和测试时适应方面也表现出色，性能优于提示微调且训练速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment</div>
<div class="meta-line">Authors: Sara Tehrani, Yonghao Xu, Leif Haglund, Amanda Berg, Michael Felsberg</div>
<div class="meta-line">First: 2026-01-26T13:48:11+00:00 · Latest: 2026-01-26T13:48:11+00:00</div>
<div class="meta-line">Comments: Under review at ICPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18493v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18493v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DisasterInsight：面向功能感知与实体化灾害评估的多模态基准</div>
<div class="mono" style="margin-top:8px">卫星影像的及时解译对灾害响应至关重要，但现有遥感领域的视觉-语言基准大多聚焦粗粒度标签和图像级识别，忽视了实际人道主义工作流所需的功能性理解和指令鲁棒性。本文提出DisasterInsight——一个为评估视觉-语言模型在真实灾害分析任务表现而设计的多模态基准。该基准将xBD数据集重构为约11.2万个以建筑物为中心的实例，支持跨多任务的指令多样性评估，包括建筑功能分类、损毁程度与灾害类型分类、计数，以及符合人道主义评估指南的结构化报告生成。为建立领域自适应基线，我们提出DI-Chat模型，通过对现有视觉-语言模型骨干网络使用参数高效的LoRA方法进行灾害专项指令数据微调获得。在先进通用模型与遥感专用模型上的大量实验表明，各任务间存在显著性能差距，尤其在损毁理解和结构化报告生成方面。DI-Chat在损毁程度分类、灾害类型分类及报告生成质量上取得显著提升，而建筑功能分类对所有评估模型仍具挑战。DisasterInsight为研究灾害影像的实体化多模态推理提供了统一基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap in existing vision-language benchmarks for remote sensing, which often lack functional understanding and instruction robustness needed for real-world disaster response. The authors introduce DisasterInsight, a multimodal benchmark built from the xBD dataset with approximately 112K building-centered instances, supporting diverse tasks such as building-function classification, damage assessment, and structured report generation. They also propose DI-Chat, a model fine-tuned using Low-Rank Adaptation (LoRA) on disaster-specific data, which shows significant improvements in damage-level and disaster-type classification and report generation over generic and remote-sensing VLMs, though building-function classification remains a challenge.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于灾害响应中需要及时解读卫星图像，而现有的视觉-语言基准缺乏实际人道主义工作流程所需的功能性理解和指令鲁棒性。方法上引入了DisasterInsight，这是一个多模态基准，将xBD数据集重构为约112K个以建筑物为中心的实例，支持跨任务的指令多样化评估，包括建筑功能分类、损坏程度和灾害类型分类、计数以及符合人道主义评估指南的结构化报告生成。主要实验结果表明，对先进视觉-语言模型的广泛实验揭示了显著的性能差距，特别是在损坏理解和报告生成方面；而提出的DI-Chat模型通过LoRA微调，在损坏程度和灾害类型分类以及报告质量上取得了显著改进，但建筑功能分类对所有模型而言仍具挑战性。</div>
</details>
</div>
<div class="card">
<div class="title">Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone</div>
<div class="meta-line">Authors: Shaivi Malik, Hasnat Md Abdullah, Sriparna Saha, Amit Sheth</div>
<div class="meta-line">First: 2025-08-26T12:41:35+00:00 · Latest: 2026-01-26T12:18:24+00:00</div>
<div class="meta-line">Comments: Accepted to the Findings of EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.18989v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.18989v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Vision Language Models (VLMs) become integral to real-world applications, understanding their demographic biases is critical. We introduce GRAS, a benchmark for uncovering demographic biases in VLMs across gender, race, age, and skin tone, offering the most diverse coverage to date. We further propose the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark five state-of-the-art VLMs and reveal concerning bias levels, with the least biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings also reveal a methodological insight: evaluating bias in VLMs with visual question answering (VQA) requires considering multiple formulations of a question. Our code, data, and evaluation results are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>换个方式再问：用于测量视觉语言模型在性别、种族、年龄和肤色方面偏见的GRAS基准</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLMs）在现实应用中日益重要，理解其人口统计学偏见至关重要。我们提出GRAS基准，用于揭示VLMs在性别、种族、年龄和肤色方面的偏见，提供迄今最多样化的覆盖范围。我们进一步提出可解释的量化指标——GRAS偏见分数。通过对五个前沿VLM的评测，发现其偏见程度令人担忧，偏见最小的模型GRAS分数仅为2分（满分100）。研究还揭示方法学洞见：通过视觉问答（VQA）评估VLM偏见时，需考虑问题的多种表述形式。代码、数据及评估结果已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand demographic biases in Vision Language Models (VLMs) as they are increasingly deployed in real-world applications. The method introduces the GRAS benchmark, which provides diverse coverage across gender, race, age, and skin tone, and proposes an interpretable GRAS Bias Score for quantification. Key experimental results from benchmarking five state-of-the-art VLMs reveal concerning bias levels, with the least biased model scoring only 2 out of 100, and highlight that bias evaluation in visual question answering requires considering multiple question formulations.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型（VLM）在现实应用中存在的关键人口统计偏差问题，本研究引入了GRAS基准，该基准在性别、种族、年龄和肤色方面提供了迄今为止最多样化的覆盖，并提出了一种可解释的GRAS偏差分数进行量化。该方法使用该框架对五个最先进的VLM进行基准测试，其关键方法学见解是，通过视觉问答进行偏差评估必须考虑多种问题表述形式。实验结果揭示了令人担忧的偏差水平，其中偏差最小的模型仅获得了100分中的2分。</div>
</details>
</div>
<div class="card">
<div class="title">ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks</div>
<div class="meta-line">Authors: Gabriel Lee Jun Rong, Christos Korgialas, Dion Jia Xu Ho, Pai Chet Ng, Xiaoxiao Miao, Konstantinos N. Plataniotis</div>
<div class="meta-line">First: 2026-01-26T11:36:34+00:00 · Latest: 2026-01-26T11:36:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18386v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18386v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk&quot;. Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARMOR：面向鲁棒对抗攻击的方法编排与参数重配置的智能体推理框架</div>
<div class="mono" style="margin-top:8px">现有自动化攻击套件多采用静态集成与固定流程，缺乏策略适应性与语义感知能力。本文提出ARMOR框架以解决这些局限。该框架通过视觉语言模型引导的智能体，协同编排三种经典对抗攻击原语——Carlini-Wagner、基于雅可比显著图攻击与空间变换攻击，并借助共享的“混音台”机制生成合成扰动。大型语言模型在实时闭环系统中自适应调参与重配置并行攻击智能体，以挖掘图像特定语义漏洞。在标准测试集上，ARMOR实现了跨架构迁移性的提升，可稳定欺骗黑白盒场景：对盲目标输出混合攻击结果，对白盒目标则基于置信度与结构相似性评分选择最优攻击或混合攻击策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing automated adversarial attack ensembles are static and lack adaptive strategy, limiting their effectiveness. To address this, the ARMOR framework introduces an agentic reasoning system that orchestrates three attack primitives—Carlini-Wagner, JSMA, and Spatially Transformed Attacks—using Vision Language Model-guided agents. These agents collaboratively generate and synthesize perturbations via a shared &#x27;Mixing Desk&#x27;, while Large Language Models adaptively tune the agents in real-time to exploit image-specific semantic vulnerabilities. Experimental results on standard benchmarks show that ARMOR improves cross-architecture transferability and reliably fools both white-box and blind attack settings, using a confidence-and-SSIM score to select or blend optimal attacks.</div>
<div class="mono" style="margin-top:8px">针对现有自动化对抗攻击套件采用静态集成、缺乏策略适应和语义感知的局限，本文提出了ARMOR框架，该框架利用视觉语言模型（VLM）和大语言模型（LLM）引导智能体，通过共享的“混合台”协同编排并动态重参数化三种核心攻击原语（CW、JSMA和STA）。该方法通过利用图像特定的语义漏洞，实现了实时、语义感知的自适应攻击。在标准基准测试上的实验结果表明，ARMOR提升了跨架构的可迁移性，能可靠地欺骗模型，针对盲目标有效生成混合扰动输出，并基于置信度和结构相似性分数为白盒目标选择最佳攻击或混合攻击。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Rigid: Benchmarking Non-Rigid Video Editing</div>
<div class="meta-line">Authors: Bingzheng Qu, Kehai Chen, Xuefeng Bai, Jun Yu, Min Zhang</div>
<div class="meta-line">First: 2026-01-26T10:28:09+00:00 · Latest: 2026-01-26T10:28:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18340v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越刚性：非刚性视频编辑基准评测</div>
<div class="mono" style="margin-top:8px">尽管文本驱动视频编辑已取得显著进展，但生成连贯的非刚性形变仍是关键挑战，常受物理失真与时间闪烁问题困扰。为填补这一空白，我们提出首个专用于评估非刚性视频编辑的综合性基准NRVBench。首先，我们构建了包含六个物理类别180个非刚性运动视频的高质量数据集，配备2340条细粒度任务指令和360道选择题。其次，我们提出基于视觉语言模型的新型评估指标NRVE-Acc，可严格评估物理合规性、时间一致性与指令对齐性，克服了通用指标在捕捉复杂动态方面的局限。第三，我们引入免训练基线方法VM-Edit，通过双区域去噪机制实现结构感知控制，平衡结构保持与动态形变。大量实验表明，当前方法在保持物理合理性方面存在不足，而我们的方法在标准指标与新建指标上均表现优异。我们相信该基准可作为推进物理感知视频编辑的标准测试平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating coherent non-rigid deformations in text-driven video editing, which often suffers from physical distortion and temporal flicker. To evaluate this, the authors introduce NRVBench, a benchmark comprising a curated dataset of 180 non-rigid motion videos with fine-grained instructions, and propose NRVE-Acc, a novel Vision-Language Model-based metric for assessing physical compliance, temporal consistency, and instruction alignment. They also present VM-Edit, a training-free baseline method using a dual-region denoising mechanism for structure-aware control. Experimental results show that existing methods struggle with physical plausibility, while VM-Edit performs excellently across both standard and the proposed metrics, positioning the benchmark as a standard platform for advancing physics-aware video editing.</div>
<div class="mono" style="margin-top:8px">本研究针对文本驱动视频编辑中生成连贯非刚性变形所面临的物理失真和时间闪烁挑战。为此，作者提出了NRVBench基准，包含一个精心策划的、涵盖六个物理类别共180个视频的数据集及详细指令，并设计了基于视觉语言模型的新评估指标NRVE-Acc，用于严格评估物理合规性、时间一致性和指令对齐。同时，他们引入了无需训练的基线方法VM-Edit，该方法采用双区域去噪机制实现结构感知控制。实验结果表明，现有方法在保持物理合理性方面存在不足，而VM-Edit在标准和所提指标上均表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Coding the Visual World: From Image to Simulation Using Vision Language Models</div>
<div class="meta-line">Authors: Sagi Eppel</div>
<div class="meta-line">First: 2026-01-08T19:49:05+00:00 · Latest: 2026-01-26T10:11:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05344v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.05344v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) have the ability to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>编码视觉世界：基于视觉语言模型从图像到仿真的实现</div>
<div class="mono" style="margin-top:8px">构建世界的心智模型是理解能力的核心体现。类似地，视觉理解可视为构建图像所描绘系统的表征模型的能力。本研究通过Im2Sim方法，探索视觉语言模型识别并仿真图像中系统与机制的能力。模型接收真实世界系统（如城市、云层、植被）的自然图像，需描述系统并编写仿真生成代码。执行该生成代码产生合成图像后，与原始图像进行比对。该方法在多种复杂涌现系统上得到验证，涵盖物理系统（波浪、光线、云层）至植被、城市、材料及地质构造。通过分析模型生成的代码与图像，我们检验了其对图像系统的理解程度。结果表明，主流视觉语言模型（GPT、Gemini）具备跨抽象层级与多领域理解并建模复杂多组件系统的能力，但在复现图像精细细节与底层模式排列方面存在局限。这一发现揭示了有趣的不对称性：视觉语言模型兼具高层次深度视觉理解能力与对细节的有限感知。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether Vision Language Models (VLMs) can achieve a deeper, model-based understanding of visual scenes by moving beyond mere recognition to simulation. The proposed Im2Sim method tasks a VLM with analyzing a natural image of a complex system, describing it, and then generating executable code that simulates the system to produce a synthetic image for comparison with the original. Experiments across diverse domains like physical phenomena, vegetation, and cities show that leading VLMs (GPT, Gemini) can understand and model complex systems at a high, abstract level, but they struggle to replicate fine-grained details and low-level pattern arrangements, revealing an asymmetry between high-level conceptual grasp and detailed perceptual fidelity.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究视觉语言模型（VLMs）能否通过构建图像中描绘系统的可执行心智模型来实现更深层的视觉理解。所提出的Im2Sim方法提示VLM分析自然图像、描述所描绘的系统，并生成模拟该系统的代码；执行此代码会产生合成图像，用于与原始图像进行比较。在物理系统、植被和城市等多个领域的实验表明，GPT和Gemini等领先的VLM能够在高层次、抽象的层面上成功建模复杂的多组件系统，但它们在复制图像的精细细节和低层模式排列方面能力有限。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language-Model-Guided Differentiable Ray Tracing for Fast and Accurate Multi-Material RF Parameter Estimation</div>
<div class="meta-line">Authors: Zerui Kang, Yishen Lim, Zhouyou Gu, Seung-Woo Ko, Tony Q. S. Quek, Jihong Park</div>
<div class="meta-line">First: 2026-01-26T07:54:53+00:00 · Latest: 2026-01-26T07:54:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate radio-frequency (RF) material parameters are essential for electromagnetic digital twins in 6G systems, yet gradient-based inverse ray tracing (RT) remains sensitive to initialization and costly under limited measurements. This paper proposes a vision-language-model (VLM) guided framework that accelerates and stabilizes multi-material parameter estimation in a differentiable RT (DRT) engine. A VLM parses scene images to infer material categories and maps them to quantitative priors via an ITU-R material table, yielding informed conductivity initializations. The VLM further selects informative transmitter/receiver placements that promote diverse, material-discriminative paths. Starting from these priors, the DRT performs gradient-based refinement using measured received signal strengths. Experiments in NVIDIA Sionna on indoor scenes show 2-4$\times$ faster convergence and 10-100$\times$ lower final parameter error compared with uniform or random initialization and random placement baselines, achieving sub-0.1\% mean relative error with only a few receivers. Complexity analyses indicate per-iteration time scales near-linearly with the number of materials and measurement setups, while VLM-guided placement reduces the measurements required for accurate recovery. Ablations over RT depth and ray counts confirm further accuracy gains without significant per-iteration overhead. Results demonstrate that semantic priors from VLMs effectively guide physics-based optimization for fast and reliable RF material estimation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型引导的可微分射线追踪实现快速精准的多材料射频参数估计</div>
<div class="mono" style="margin-top:8px">精确的射频材料参数对6G系统中的电磁数字孪生至关重要，但基于梯度的逆向射线追踪对初始化敏感且在有限测量下计算成本高昂。本文提出一种视觉语言模型引导的框架，在可微分射线追踪引擎中加速并稳定多材料参数估计。VLM通过解析场景图像推断材料类别，并借助ITU-R材料表映射为定量先验，从而生成有依据的电导率初始化值。VLM进一步选择信息量丰富的发射/接收器布局，以促进多样化、具有材料区分度的传播路径。基于这些先验，DRT利用实测接收信号强度进行梯度优化。在NVIDIA Sionna平台上的室内场景实验表明：相较于均匀/随机初始化与随机布局基线，本方法收敛速度提升2-4倍，最终参数误差降低10-100倍，仅需少量接收器即可实现低于0.1%的平均相对误差。复杂度分析显示每次迭代时间与材料数量和测量设置呈近线性关系，而VLM引导的布局策略减少了精确重建所需的测量次数。针对射线追踪深度与射线数量的消融实验证实了在不显著增加单次迭代开销的前提下可进一步提升精度。结果表明，VLM提供的语义先验能有效指导基于物理的优化过程，实现快速可靠的射频材料参数估计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate radio-frequency material parameters are crucial for electromagnetic digital twins in 6G, but gradient-based inverse ray tracing is slow and sensitive to initialization with sparse measurements. To address this, the authors propose a framework that uses a vision-language model to parse scene images, infer material categories, and map them to quantitative conductivity priors via an ITU-R material table, while also selecting informative transmitter/receiver placements to promote diverse propagation paths. This initialization guides a differentiable ray tracing engine that performs gradient-based refinement using measured signal strengths. Experiments in NVIDIA Sionna on indoor scenes show the method achieves 2-4 times faster convergence and 10-100 times lower final parameter error compared to baselines, reaching sub-0.1% mean relative error with few receivers, and complexity analysis indicates near-linear scaling with materials and setups.</div>
<div class="mono" style="margin-top:8px">精确的射频材料参数对于6G电磁数字孪生至关重要，但基于梯度的逆向射线追踪存在对初始化敏感且在有限测量下计算成本高的问题。本研究提出了一种视觉语言模型引导的框架，在可微分射线追踪引擎中加速并稳定多材料参数估计。该方法利用VLM解析场景图像，推断材料类别，并通过ITU-R材料表将其映射为定量电导率先验，从而提供有信息的初始化；同时选择信息丰富的发射器/接收器布置，以促进具有材料区分性的传播路径。在NVIDIA Sionna中对室内场景的实验评估表明，与均匀/随机初始化和随机布置基线相比，该方法实现了2-4倍的收敛加速和10-100倍的最终参数误差降低，仅用少量接收器即可达到低于0.1%的平均相对误差。复杂度分析显示每次迭代时间随材料数量和测量设置近线性增长，而VLM引导的布置减少了准确恢复所需的测量次数。</div>
</details>
</div>
<div class="card">
<div class="title">GeoVLMath: Enhancing Geometry Reasoning in Vision-Language Models via Cross-Modal Reward for Auxiliary Line Creation</div>
<div class="meta-line">Authors: Shasha Guo, Liang Pang, Xi Wang, Yanling Wang, Huawei Shen, Jing Zhang</div>
<div class="meta-line">First: 2025-10-13T05:33:51+00:00 · Latest: 2026-01-26T07:35:43+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11020v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11020v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Auxiliary lines are essential for solving complex geometric problems but remain challenging for large vision-language models (LVLMs). Recent attempts construct auxiliary lines via code-driven rendering, a strategy that relies on accurate and executable code generation to produce visual renderings of the auxiliary lines for subsequent reasoning. However, in complex solid geometry settings, such a strong dependence on precise specifications substantially restricts the robustness of this strategy. Alternatively, we turn to a simpler and more stable solution, representing auxiliary-line constructions as structured textual descriptions. To bridge the gap between textual descriptions and spatial structure, we propose a reinforcement learning framework that enhances diagram-text alignment. The core is a cross-modal reward model that evaluates how well the generated auxiliary-line description matches the ground-truth auxiliary-line diagram. The reward signal drives a GRPO-based RL stage to yield informative auxiliary-line descriptions for the reasoning. To support the training and evaluation, we develop a scalable data pipeline and construct AuxSolidMath, a dataset of 3,018 real-exam geometry problems with paired diagrams and aligned textual fields. Based on this framework, we derive GeoVLMath, an LVLM for solving complex solid geometry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoVLMath：通过跨模态奖励增强视觉语言模型的几何推理能力——基于辅助线构建</div>
<div class="mono" style="margin-top:8px">辅助线对解决复杂几何问题至关重要，但对大型视觉语言模型（LVLM）仍具挑战。现有方法通过代码驱动渲染构建辅助线，该策略依赖精确可执行的代码生成来呈现辅助线视觉化以支持后续推理。然而在复杂立体几何场景中，这种对精确规范的高度依赖严重限制了策略的鲁棒性。为此，我们转向更简洁稳定的解决方案：将辅助线构建表示为结构化文本描述。为弥合文本描述与空间结构的鸿沟，我们提出强化学习框架以增强图文对齐，其核心是评估生成辅助线描述与真实辅助线图示匹配度的跨模态奖励模型。奖励信号驱动基于GRPO的强化学习阶段，生成具有信息量的辅助线描述以支持推理。为支撑训练与评估，我们开发了可扩展数据流水线，构建了包含3,018道真实考题、配备配对图示与对齐文本字段的数据集AuxSolidMath。基于此框架，我们最终得到用于解决复杂立体几何问题的LVLM模型GeoVLMath。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Auxiliary lines are crucial for solving complex geometry problems, but existing large vision-language models struggle to generate them robustly, especially in solid geometry where code-driven rendering methods are brittle due to their reliance on precise executable code. To address this, the authors propose representing auxiliary line constructions as structured textual descriptions instead and introduce a reinforcement learning framework enhanced by a cross-modal reward model that evaluates the alignment between generated textual descriptions and ground-truth auxiliary line diagrams; this reward drives a GRPO-based RL stage to produce informative descriptions for reasoning. The method is supported by a new dataset, AuxSolidMath, containing 3,018 real-exam geometry problems with aligned diagrams and text, and experimental results show that the derived model, GeoVLMath, effectively enhances geometry reasoning for complex solid geometry problems.</div>
<div class="mono" style="margin-top:8px">辅助线对于解决复杂几何问题至关重要，但大型视觉语言模型在可靠生成辅助线方面仍面临挑战。针对代码驱动渲染方法在立体几何场景中鲁棒性不足的问题，本研究提出将辅助线构造表示为结构化文本描述。其核心方法是一个强化学习框架，通过跨模态奖励模型评估生成的文本描述与真实辅助线图的对齐程度，并利用基于GRPO的强化学习优化描述以提升推理能力。在包含3,018道真实考题的新建数据集AuxSolidMath上的实验表明，所得到的GeoVLMath模型能有效提升复杂立体几何问题的求解性能。</div>
</details>
</div>
<div class="card">
<div class="title">The Art of Saying &quot;Maybe&quot;: A Conformal Lens for Uncertainty Benchmarking in VLMs</div>
<div class="meta-line">Authors: Asif Azad, Mohammad Sadat Hossain, MD Sadik Hossain Shanto, M Saifur Rahman, Md Rizwan Parvez</div>
<div class="meta-line">First: 2025-09-16T08:17:39+00:00 · Latest: 2026-01-26T07:17:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.13379v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.13379v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved remarkable progress in complex visual understanding across scientific and reasoning tasks. While performance benchmarking has advanced our understanding of these capabilities, the critical dimension of uncertainty quantification has received insufficient attention. Therefore, unlike prior conformal prediction studies that focused on limited settings, we conduct a comprehensive uncertainty benchmarking study, evaluating 18 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets with 3 distinct scoring functions. For closed-source models lacking token-level logprob access, we develop and validate instruction-guided likelihood proxies. Our findings demonstrate that larger models consistently exhibit better uncertainty quantification; models that know more also know better what they don&#x27;t know. More certain models achieve higher accuracy, while mathematical and reasoning tasks elicit poorer uncertainty performance across all models compared to other domains. This work establishes a foundation for reliable uncertainty evaluation in multimodal systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>“或许”的艺术：基于共形预测视角的视觉语言模型不确定性基准测试</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在科学和推理任务的复杂视觉理解方面取得了显著进展。虽然性能基准测试增进了我们对这些能力的理解，但不确定性量化这一关键维度尚未得到充分关注。为此，与先前局限于有限场景的共形预测研究不同，我们开展了全面的不确定性基准测试研究，通过3种不同的评分函数，在6个多模态数据集上评估了18个前沿VLM模型（包括开源和闭源）。针对缺乏词元级对数概率访问权限的闭源模型，我们开发并验证了基于指令引导的似然代理方法。研究发现：更大规模的模型始终表现出更优的不确定性量化能力——知识更丰富的模型也更清楚自身的认知局限；确定性更高的模型能获得更准确的预测结果；与其他领域相比，所有模型在数学和推理任务中的不确定性表现均较差。本研究为多模态系统的可靠不确定性评估奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the lack of comprehensive uncertainty quantification in Vision-Language Models (VLMs), which is critical for reliable deployment despite their strong performance on complex tasks. The authors conduct a large-scale benchmark, evaluating 18 state-of-the-art open and closed-source VLMs across six multimodal datasets using three scoring functions, and they develop instruction-guided likelihood proxies for models without direct token-level probability access. Key experimental results show that larger models consistently provide better uncertainty estimates, that higher model certainty correlates with higher accuracy, and that mathematical and reasoning tasks present a greater challenge for uncertainty performance compared to other domains.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型缺乏全面不确定性量化基准的问题，这对于可靠的多模态系统至关重要。研究方法包括使用三种评分函数对18个最先进的开源和闭源VLM在六个多模态数据集上进行大规模评估，并为无法访问词元级概率的闭源模型开发了新颖的指令引导似然代理。关键实验结果表明：更大规模的模型持续表现出更好的不确定性校准；模型确定性越高，其准确性也越高；与其它领域相比，所有模型在数学和推理任务上的不确定性表现都显著更差。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Conditioned Reasoning in Long-Egocentric Videos</div>
<div class="meta-line">Authors: James Tribble, Hao Wang, Si-En Hong, Chaoyi Zhou, Ashish Bastola, Siyu Huang, Abolfazl Razi</div>
<div class="meta-line">First: 2026-01-26T03:21:35+00:00 · Latest: 2026-01-26T03:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-horizon egocentric video presents significant challenges for visual navigation due to viewpoint drift and the absence of persistent geometric context. Although recent vision-language models perform well on image and short-video reasoning, their spatial reasoning capability in long egocentric sequences remains limited. In this work, we study how explicit spatial signals influence VLM-based video understanding without modifying model architectures or inference procedures. We introduce Sanpo-D, a fine-grained re-annotation of the Google Sanpo dataset, and benchmark multiple VLMs on navigation-oriented spatial queries. To examine input-level inductive bias, we further fuse depth maps with RGB frames and evaluate their impact on spatial reasoning. Our results reveal a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware and spatially grounded representations can improve performance on safety-critical tasks such as pedestrian and obstruction detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长时序第一人称视频中的空间条件推理</div>
<div class="mono" style="margin-top:8px">长时序第一人称视频因视角漂移和缺乏持续几何上下文，给视觉导航带来显著挑战。尽管当前视觉语言模型在图像和短视频推理中表现良好，但其在长时序第一人称序列中的空间推理能力仍有限。本研究在不改变模型架构或推理流程的前提下，探究显式空间信号如何影响基于VLM的视频理解。我们提出Sanpo-D——对Google Sanpo数据集的细粒度重标注，并在面向导航的空间查询任务上对多个VLM进行基准测试。为探究输入层归纳偏置，我们进一步将深度图与RGB帧融合，评估其对空间推理的影响。实验结果表明通用精度与空间特化能力之间存在权衡，深度感知与空间基础表征能提升行人检测、障碍物检测等安全关键任务的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of spatial reasoning in long-horizon egocentric videos, where viewpoint drift and lack of persistent geometric context hinder visual navigation. The study investigates how explicit spatial signals, such as depth maps fused with RGB frames, influence vision-language model (VLM) understanding without altering model architectures, using a newly annotated dataset, Sanpo-D, for benchmarking. Experimental results demonstrate a trade-off between general-purpose accuracy and spatial specialization, showing that depth-aware representations can enhance performance on safety-critical tasks like pedestrian and obstruction detection.</div>
<div class="mono" style="margin-top:8px">本研究针对长时序第一人称视频中视角漂移和缺乏持久几何上下文所带来的空间推理挑战。该方法在不改变模型架构的前提下，通过在新引入的细粒度标注数据集Sanpo-D上对多种视觉语言模型进行基准测试，并将深度图与RGB帧融合作为输入层面的归纳偏置，来探究显式空间信号对模型理解的影响。实验结果表明，通用准确性与空间专门化之间存在权衡，深度感知和空间基础的表征能够提升如行人和障碍物检测等安全关键任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Too Many Frames, Not All Useful: Efficient Strategies for Long-Form Video QA</div>
<div class="meta-line">Authors: Jongwoo Park, Kanchana Ranasinghe, Kumara Kahatapitiya, Wonjeong Ryu, Donghyun Kim, Michael S. Ryoo</div>
<div class="meta-line">First: 2024-06-13T17:59:16+00:00 · Latest: 2026-01-26T00:29:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.09396v6">Abs</a> · <a href="https://arxiv.org/pdf/2406.09396v6">PDF</a> · <a href="https://github.com/jongwoopark7978/LVNet">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-form videos that span across wide temporal intervals are highly information redundant and contain multiple distinct events or entities that are often loosely related. Therefore, when performing long-form video question answering (LVQA), all information necessary to generate a correct response can often be contained within a small subset of frames. Recent literature leverage large language models (LLMs) in LVQA benchmarks, achieving exceptional performance, while relying on vision language models (VLMs) to convert all visual content within videos into natural language. Such VLMs often independently caption a large number of frames uniformly sampled from long videos, which is not efficient and can mostly be redundant. Motivated by this inefficiency, we propose LVNet, a modular and training-free framework featuring a novel Hierarchical Keyframe Selector (HKS) that efficiently selects a minimal set of informative frames tailored to each question. LVNet&#x27;s modularity allows easy integration with existing approaches for more efficient LVQA. We achieve state-of-the-art performance among similarly configured models across four benchmark LVQA datasets: EgoSchema, NExT-QA, IntentQA, VideoMME. The code can be found at https://github.com/jongwoopark7978/LVNet</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>帧数过多，并非全有用：长视频问答的高效策略</div>
<div class="mono" style="margin-top:8px">跨越广泛时间区间的长视频信息冗余度高，包含多个松散关联的独立事件或实体。因此，在执行长视频问答（LVQA）时，生成正确答案所需的全部信息通常仅包含于少量关键帧中。近期研究利用大语言模型（LLM）在LVQA基准测试中取得卓越性能，但依赖视觉语言模型（VLM）将视频内所有视觉内容转换为自然语言。此类VLM通常对长视频均匀采样的大量帧进行独立描述，效率低下且多为冗余。基于此低效问题，我们提出LVNet——一个模块化、免训练的框架，其新颖的分层关键帧选择器（HKS）能针对每个问题高效筛选最小化的信息帧集合。LVNet的模块化特性便于与现有方法集成，实现更高效的LVQA。我们在四个LVQA基准数据集（EgoSchema、NExT-QA、IntentQA、VideoMME）上取得了同配置模型中的最优性能。代码可见：https://github.com/jongwoopark7978/LVNet</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Long-form video question answering (LVQA) faces inefficiency because existing methods often rely on vision-language models to caption many uniformly sampled frames, leading to redundancy. To address this, the authors propose LVNet, a modular and training-free framework featuring a Hierarchical Keyframe Selector (HKS) that dynamically selects a minimal set of informative frames tailored to each question. Experiments on four LVQA benchmarks (EgoSchema, NExT-QA, IntentQA, VideoMME) show that LVNet achieves state-of-the-art performance among similarly configured models, demonstrating its effectiveness in reducing redundancy while maintaining accuracy.</div>
<div class="mono" style="margin-top:8px">长视频问答任务存在效率低下的问题，因为现有方法通常依赖视觉语言模型对大量均匀采样的帧进行描述，导致信息冗余。为此，研究者提出了LVNet，这是一个模块化且无需训练的新框架，其核心是分层关键帧选择器，能够根据每个问题动态筛选出最少数量的信息帧。在EgoSchema、NExT-QA、IntentQA和VideoMME四个基准数据集上的实验表明，LVNet在同等配置模型中取得了最先进的性能，验证了其在减少冗余的同时保持准确性的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Mitigating the Modality Gap: Few-Shot Out-of-Distribution Detection with Multi-modal Prototypes and Image Bias Estimation</div>
<div class="meta-line">Authors: Yimu Wang, Evelien Riddell, Adrian Chow, Sean Sedwards, Krzysztof Czarnecki</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-02-02T04:30:51+00:00 · Latest: 2026-01-25T22:18:42+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.00662v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.00662v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language model (VLM)-based methods for out-of-distribution (OOD) detection typically rely on similarity scores between input images and in-distribution (ID) text prototypes. However, the modality gap between image and text often results in high false positive rates, as OOD samples can exhibit high similarity to ID text prototypes. To mitigate the impact of this modality gap, we propose incorporating ID image prototypes along with ID text prototypes. We present theoretical analysis and empirical evidence indicating that this approach enhances VLM-based OOD detection performance without any additional training. To further reduce the gap between image and text, we introduce a novel few-shot tuning framework, SUPREME, comprising biased prompts generation (BPG) and image-text consistency (ITC) modules. BPG enhances image-text fusion and improves generalization by conditioning ID text prototypes on the Gaussian-based estimated image domain bias; ITC reduces the modality gap by minimizing intra- and inter-modal distances. Moreover, inspired by our theoretical and empirical findings, we introduce a novel OOD score $S_{\textit{GMP}}$, leveraging uni- and cross-modal similarities. Finally, we present extensive experiments to demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缓解模态鸿沟：基于多模态原型与图像偏差估计的少样本分布外检测</div>
<div class="mono" style="margin-top:8px">现有基于视觉语言模型（VLM）的分布外（OOD）检测方法通常依赖于输入图像与分布内（ID）文本原型之间的相似度评分。然而，图像与文本之间的模态鸿沟常导致高误报率，因为OOD样本可能表现出与ID文本原型的高度相似性。为减轻模态鸿沟的影响，我们提出在ID文本原型基础上引入ID图像原型。理论分析与实验证据表明，该方法无需额外训练即可提升基于VLM的OOD检测性能。为进一步缩小图像与文本间的差距，我们提出新型少样本调优框架SUPREME，包含偏差提示生成（BPG）和图文一致性（ITC）模块：BPG通过基于高斯估计的图像域偏差条件化ID文本原型，增强图文融合并提升泛化能力；ITC通过最小化模态内与模态间距离来减小模态鸿沟。此外，基于理论与实证发现，我们提出新型OOD评分$S_{\textit{GMP}}$，综合利用单模态与跨模态相似性。最后，大量实验证明SUPREME在多种基准测试中持续优于现有基于VLM的OOD检测方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the high false positive rates in vision-language model (VLM)-based out-of-distribution (OOD) detection, which stem from the modality gap causing OOD images to appear similar to in-distribution (ID) text prototypes. The method first incorporates ID image prototypes alongside text prototypes to improve detection without training. It then introduces a few-shot tuning framework, SUPREME, which includes a biased prompts generation module to fuse image and text by conditioning text on estimated image bias, and an image-text consistency module to reduce intra- and inter-modal distances. A novel OOD score, S_GMP, leverages uni- and cross-modal similarities. Experimental results demonstrate that SUPREME consistently outperforms existing VLM-based OOD detection methods.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在分布外检测中因图像与文本的模态差距导致的高误报率问题，即分布外样本可能错误地与分布内文本原型表现出高相似性。为缓解此问题，该方法首先在零样本检测中结合了分布内图像原型与文本原型以提升性能。随后提出了一个名为SUPREME的小样本微调框架，包含偏置提示生成模块（通过将文本条件于估计的图像偏置来融合模态）和图像-文本一致性模块（通过最小化模态内与模态间距离来减小差距）。基于理论和实证发现，引入了一种利用单模态与跨模态相似性的新型分布外分数S_GMP。大量实验表明，SUPREME在无需额外训练的情况下，持续优于现有的基于视觉语言模型的分布外检测方法。</div>
</details>
</div>
<div class="card">
<div class="title">RemEdit: Efficient Diffusion Editing with Riemannian Geometry</div>
<div class="meta-line">Authors: Eashan Adhikarla, Brian D. Davison</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-25T17:58:57+00:00 · Latest: 2026-01-25T17:58:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17927v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17927v1">PDF</a> · <a href="https://www.github.com/eashanadhikarla/RemEdit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Controllable image generation is fundamental to the success of modern generative AI, yet it faces a critical trade-off between semantic fidelity and inference speed. The RemEdit diffusion-based framework addresses this trade-off with two synergistic innovations. First, for editing fidelity, we navigate the latent space as a Riemannian manifold. A mamba-based module efficiently learns the manifold&#x27;s structure, enabling direct and accurate geodesic path computation for smooth semantic edits. This control is further refined by a dual-SLERP blending technique and a goal-aware prompt enrichment pass from a Vision-Language Model. Second, for additional acceleration, we introduce a novel task-specific attention pruning mechanism. A lightweight pruning head learns to retain tokens essential to the edit, enabling effective optimization without the semantic degradation common in content-agnostic approaches. RemEdit surpasses prior state-of-the-art editing frameworks while maintaining real-time performance under 50% pruning. Consequently, RemEdit establishes a new benchmark for practical and powerful image editing. Source code: https://www.github.com/eashanadhikarla/RemEdit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RemEdit：基于黎曼几何的高效扩散编辑框架</div>
<div class="mono" style="margin-top:8px">可控图像生成是现代生成式AI成功的基石，但始终面临语义保真度与推理速度间的关键权衡。RemEdit扩散框架通过两项协同创新应对这一挑战：首先，为提升编辑保真度，我们将潜空间建模为黎曼流形，基于Mamba的模块高效学习流形结构，实现直接精确的测地线路径计算以完成平滑语义编辑。该控制机制通过双SLERP混合技术与视觉语言模型的目标感知提示增强得到进一步优化。其次，为加速推理，我们提出新型任务感知注意力剪枝机制——轻量级剪枝头学习保留编辑关键令牌，在避免内容无关方法常见语义退化前提下实现高效优化。RemEdit在保持50%剪枝率下实时性能的同时，超越了现有最优编辑框架，为实用化高性能图像编辑确立了新基准。源代码：https://www.github.com/eashanadhikarla/RemEdit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the trade-off between semantic fidelity and inference speed in controllable image generation, this paper introduces RemEdit, a diffusion-based framework that leverages Riemannian geometry for efficient editing. The method navigates the latent space as a Riemannian manifold, using a mamba-based module to learn its structure and compute geodesic paths for smooth edits, enhanced by dual-SLERP blending and a vision-language model for prompt enrichment; it also employs a task-specific attention pruning mechanism to accelerate inference by retaining essential tokens. Experimental results show that RemEdit outperforms prior state-of-the-art editing frameworks while maintaining real-time performance even with 50% pruning, establishing a new benchmark for practical image editing.</div>
<div class="mono" style="margin-top:8px">本研究针对可控图像生成中语义保真度与推理速度之间的权衡问题，提出了RemEdit框架。该方法包含两项创新：首先，将潜在空间建模为黎曼流形，使用基于mamba的模块学习其结构以实现直接测地线路径计算，并通过双SLERP混合和视觉语言模型提示增强进行优化；其次，采用任务特定的注意力剪枝机制，通过轻量级剪枝头保留关键标记。实验结果表明，RemEdit在保持50%剪枝下实时性能的同时，超越了现有最先进的编辑框架。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Direct Preference Optimization for Medical Large Vision-Language Models</div>
<div class="meta-line">Authors: Dain Kim, Jiwoo Lee, Jaehoon Yun, Yong Hoe Koo, Qingyu Chen, Hyunjae Kim, Jaewoo Kang</div>
<div class="meta-line">First: 2026-01-25T17:36:53+00:00 · Latest: 2026-01-25T17:36:53+00:00</div>
<div class="meta-line">Comments: EACL 2026 (Findings)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17918v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17918v1">PDF</a> · <a href="https://github.com/dmis-lab/med-vlm-dpo">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) hold significant promise for medical applications, yet their deployment is often constrained by insufficient alignment and reliability. While Direct Preference Optimization (DPO) has emerged as a potent framework for refining model responses, its efficacy in high-stakes medical contexts remains underexplored, lacking the rigorous empirical groundwork necessary to guide future methodological advances. To bridge this gap, we present the first comprehensive examination of diverse DPO variants within the medical domain, evaluating nine distinct formulations across two medical LVLMs: LLaVA-Med and HuatuoGPT-Vision. Our results reveal several critical limitations: current DPO approaches often yield inconsistent gains over supervised fine-tuning, with their efficacy varying significantly across different tasks and backbones. Furthermore, they frequently fail to resolve fundamental visual misinterpretation errors. Building on these insights, we present a targeted preference construction strategy as a proof-of-concept that explicitly addresses visual misinterpretation errors frequently observed in existing DPO models. This design yields a 3.6% improvement over the strongest existing DPO baseline on visual question-answering tasks. To support future research, we release our complete framework, including all training data, model checkpoints, and our codebase at https://github.com/dmis-lab/med-vlm-dpo.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>医疗大型视觉语言模型直接偏好优化的基准测试研究</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在医疗领域具有重要应用前景，但其部署常受限于对齐不足与可靠性问题。直接偏好优化虽已成为优化模型响应的有效框架，但在高风险医疗场景中的效能尚未得到充分探索，缺乏指导未来方法学发展的严谨实证基础。为填补这一空白，本研究首次在医疗领域系统评估了多种DPO变体，在LLaVA-Med和HuatuoGPT-Vision两个医疗LVLM上测试了九种不同方案。结果显示当前DPO方法存在关键局限：相较于监督微调常产生不一致的增益效果，其效能随任务与骨干网络差异显著波动，且往往无法修正根本性的视觉误判错误。基于此，我们提出针对性偏好构建策略作为概念验证，专门解决现有DPO模型中常见的视觉误判问题。该设计在视觉问答任务上较现有最强DPO基线提升3.6%。为促进后续研究，我们开源了完整框架，包括训练数据、模型检查点及代码库（https://github.com/dmis-lab/med-vlm-dpo）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the insufficient alignment and reliability of Large Vision-Language Models (LVLMs) for high-stakes medical applications by comprehensively evaluating the efficacy of Direct Preference Optimization (DPO) variants. The authors benchmark nine distinct DPO formulations on two medical LVLMs, LLaVA-Med and HuatuoGPT-Vision, revealing that current DPO approaches yield inconsistent gains over supervised fine-tuning, show variable performance across tasks and backbones, and frequently fail to correct visual misinterpretation errors. As a proof-of-concept, they propose a targeted preference construction strategy that explicitly addresses visual errors, achieving a 3.6% improvement over the strongest DPO baseline on visual question-answering tasks.</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在医疗领域的应用常因对齐不足和可靠性问题而受限，直接偏好优化作为一种有前景的模型精调框架，其在高风险医疗场景中的有效性缺乏严格评估。为此，本研究首次在LLaVA-Med和HuatuoGPT-Vision两个医疗模型上，对九种DPO变体进行了全面基准测试。实验结果揭示了关键局限：DPO方法相比监督微调往往提升不一致，其效果在不同任务和模型骨干间差异显著，且通常无法纠正根本性的视觉误解错误。作为概念验证，一种针对视觉错误设计的定向偏好构建策略，在视觉问答任务上比现有最强DPO基线提升了3.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Image2Garment: Simulation-ready Garment Generation from a Single Image</div>
<div class="meta-line">Authors: Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu, Yang Zheng, Hugo Bertiche, Menglei Chai, Thabo Beeler, Gordon Wetzstein</div>
<div class="meta-line">First: 2026-01-14T17:47:33+00:00 · Latest: 2026-01-25T14:13:38+00:00</div>
<div class="meta-line">Comments: Project Page: https://image2garment.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09658v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.09658v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://image2garment.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Image2Garment：从单张图像生成仿真就绪的服装</div>
<div class="mono" style="margin-top:8px">从单张图像估计物理精确、仿真就绪的服装具有挑战性，原因在于缺乏图像到物理的数据集以及该问题本身的不适定性。现有方法要么需要多视角捕捉和昂贵的可微分仿真，要么仅预测服装几何而缺少真实仿真所需的材料属性。我们提出一种前馈框架，通过先微调视觉语言模型从真实图像推断材料成分与织物属性，再利用小型材料物理测量数据集训练轻量级预测器，将这些属性映射至对应的物理织物参数，从而规避了上述限制。该方法引入了两个新数据集（FTAG与T2P），无需迭代优化即可从单张图像生成仿真就绪的服装。实验表明，我们的估计器在材料成分估计与织物属性预测上均达到更高精度，且通过物理参数估计器处理后，相比现有图像到服装方法能实现更高保真度的仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating physically accurate, simulation-ready garments from a single image, a problem made difficult by the lack of image-to-physics datasets and its ill-posed nature. The proposed method first fine-tunes a vision-language model to infer material composition and fabric attributes from an image, then uses a lightweight predictor trained on a small material-physics dataset to map these attributes to physical simulation parameters. Experimental results demonstrate that this feed-forward framework outperforms prior methods in material composition estimation and fabric attribute prediction, and subsequently produces higher-fidelity garment simulations without requiring iterative optimization.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从单张图像生成物理精确、可直接用于仿真的服装这一挑战，该问题因缺乏图像到物理的数据集及其不适定性而变得困难。所提出的方法采用前馈框架，首先微调一个视觉语言模型从图像中推断材料成分和织物属性，然后使用一个在小型材料物理数据集上训练的轻量级预测器，将这些属性映射到物理仿真参数。实验结果表明，该方法在材料和属性预测上实现了更高的准确性，并且与现有的图像到服装生成方法相比，能实现更高保真度的服装仿真，整个过程无需迭代优化或多视角捕捉。</div>
</details>
</div>
<div class="card">
<div class="title">ViTCoP: Accelerating Large Vision-Language Models via Visual and Textual Semantic Collaborative Pruning</div>
<div class="meta-line">Authors: Wen Luo, Peng Chen, Xiaotao Huang, LiQun Huang</div>
<div class="meta-line">First: 2026-01-25T12:47:30+00:00 · Latest: 2026-01-25T12:47:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17818v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17818v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) incur high computational costs due to significant redundancy in their visual tokens. To effectively reduce this cost, researchers have proposed various visual token pruning methods. However, existing methods are generally limited, either losing critical visual information prematurely due to pruning in the vision encoder, or leading to information redundancy among the selected tokens due to pruning in the Large Language Models (LLMs). To address these challenges, we propose a Visual and Textual Semantic Collaborative Pruning framework (ViTCoP) that combines redundancy filtering in the vision encoder with step-wise co-pruning within the LLM based on its hierarchical characteristics, to efficiently preserve critical and informationally diverse visual tokens. Meanwhile, to ensure compatibility with acceleration techniques like FlashAttention, we introduce the L2 norm of K-vectors as the token saliency metric in the LLM. Extensive experiments on various Large Vision-Language Models demonstrate that ViTCoP not only achieves state-of-the-art performance surpassing existing methods on both image and video understanding tasks, but also significantly reduces model inference latency and GPU memory consumption. Notably, its performance advantage over other methods becomes even more pronounced under extreme pruning rates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViTCoP：通过视觉与文本语义协同剪枝加速大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型因其视觉标记存在显著冗余而导致高昂计算成本。为有效降低该成本，研究者已提出多种视觉标记剪枝方法。然而现有方法普遍存在局限：或因在视觉编码器中剪枝而过早丢失关键视觉信息，或因在大型语言模型内剪枝导致所选标记间信息冗余。为解决这些问题，我们提出视觉与文本语义协同剪枝框架（ViTCoP），该框架结合视觉编码器的冗余过滤与基于大型语言模型层级特性的渐进协同剪枝，以高效保留关键且信息多元的视觉标记。同时，为兼容FlashAttention等加速技术，我们引入K向量L2范数作为大型语言模型中的标记显著性度量。在多种大型视觉语言模型上的大量实验表明，ViTCoP不仅在图像与视频理解任务上取得超越现有方法的先进性能，还显著降低模型推理延迟与GPU内存消耗。值得注意的是，在极端剪枝率下，其性能优势较其他方法更为突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high computational cost of Large Vision-Language Models (LVLMs) caused by redundant visual tokens, this research proposes ViTCoP, a framework that collaboratively prunes tokens based on visual and textual semantics. The method first filters redundancy in the vision encoder and then performs step-wise co-pruning within the Large Language Model (LLM) using its hierarchical structure, employing the L2 norm of K-vectors as a saliency metric to ensure compatibility with acceleration techniques like FlashAttention. Experimental results on various LVLMs show that ViTCoP achieves state-of-the-art performance on image and video understanding tasks while significantly reducing inference latency and GPU memory usage, with its advantages becoming more pronounced under high pruning rates.</div>
<div class="mono" style="margin-top:8px">为解决大型视觉语言模型因视觉令牌冗余导致的计算效率低下问题，本研究提出了ViTCoP，一种视觉与文本语义协同剪枝框架。该方法结合了视觉编码器中的冗余过滤与基于大语言模型层次特性的逐步协同剪枝，使用K向量的L2范数作为令牌显著性度量，以保留关键且信息多样的视觉令牌，同时确保与加速技术的兼容性。在多种大型视觉语言模型上的实验表明，ViTCoP在图像和视频理解任务上取得了领先性能，显著降低了推理延迟和GPU内存消耗，且在极高剪枝率下优势更为明显。</div>
</details>
</div>
<div class="card">
<div class="title">PVLM: Parsing-Aware Vision Language Model with Dynamic Contrastive Learning for Zero-Shot Deepfake Attribution</div>
<div class="meta-line">Authors: Yaning Zhang, Jiahe Zhang, Chunjie Ma, Weili Guan, Tian Gan, Zan Gao</div>
<div class="meta-line">First: 2025-04-19T01:11:46+00:00 · Latest: 2026-01-25T09:45:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.14129v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.14129v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The challenge of tracing the source attribution of forged faces has gained significant attention due to the rapid advancement of generative models. However, existing deepfake attribution (DFA) works primarily focus on the interaction among various domains in vision modality, and other modalities such as texts and face parsing are not fully explored. Besides, they tend to fail to assess the generalization performance of deepfake attributors to unseen advanced generators like diffusion in a fine-grained manner. In this paper, we propose a novel parsing-aware vision language model with a dynamic contrastive learning (PVLM) method for zero-shot deepfake attribution (ZSDFA), which facilitates effective and fine-grained traceability to unseen advanced generators. Specifically, we conduct a novel and fine-grained ZS-DFA benchmark to evaluate the attribution performance of deepfake attributors to unseen advanced generators like diffusion. Besides, we propose an innovative PVLM attributor based on the vision-language model to capture general and diverse attribution features. We are motivated by the observation that the preservation of source face attributes in facial images generated by GAN and diffusion models varies significantly. We propose to employ the inherent facial attributes preservation differences to capture face parsing-aware forgery representations. Therefore, we devise a novel parsing encoder to focus on global face attribute embeddings, enabling parsing-guided DFA representation learning via dynamic vision-parsing matching. Additionally, we present a novel deepfake attribution contrastive center loss to pull relevant generators closer and push irrelevant ones away, which can be introduced into DFA models to enhance traceability. Experimental results show that our model exceeds the state-of-the-art on the ZS-DFA benchmark via various protocol evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PVLM：基于动态对比学习的解析感知视觉语言模型用于零样本深度伪造溯源</div>
<div class="mono" style="margin-top:8px">随着生成模型的快速发展，伪造人脸图像的溯源问题受到广泛关注。现有深度伪造溯源方法主要关注视觉模态中不同领域的交互，未能充分探索文本、人脸解析等其他模态，且难以细粒度评估模型对扩散模型等未知先进生成器的泛化性能。本文提出一种基于动态对比学习的解析感知视觉语言模型，用于零样本深度伪造溯源，实现对未知先进生成器的有效细粒度溯源。具体而言，我们构建了新颖的细粒度零样本深度伪造溯源基准，以评估模型对扩散模型等未知生成器的溯源性能；并提出基于视觉语言模型的创新性PVLM溯源器，以捕捉通用且多样化的溯源特征。我们观察到GAN与扩散模型生成的人脸图像在源属性保留程度上存在显著差异，据此利用固有面部属性保留差异来捕获解析感知的伪造表征。为此，我们设计了新型解析编码器以聚焦全局人脸属性嵌入，通过动态视觉-解析匹配实现解析引导的深度伪造溯源表征学习。此外，我们提出新型深度伪造溯源对比中心损失函数，拉近相关生成器表征并推远无关生成器表征，该损失可集成至现有深度伪造溯源模型以增强可追溯性。实验结果表明，通过多种协议评估，我们的模型在零样本深度伪造溯源基准上超越了现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of tracing source attribution for deepfake faces, particularly as generative models advance, by proposing a zero-shot deepfake attribution method that generalizes to unseen generators like diffusion models. The method introduces a parsing-aware vision-language model with dynamic contrastive learning, which leverages differences in facial attribute preservation across generators to capture forgery representations through a parsing encoder and dynamic vision-parsing matching, enhanced by a contrastive center loss for better traceability. Experiments on a novel zero-shot benchmark demonstrate that the model outperforms state-of-the-art methods across various evaluation protocols.</div>
<div class="mono" style="margin-top:8px">生成模型的快速发展使得追踪伪造人脸来源的需求日益凸显，但现有深度伪造溯源方法主要局限于视觉模态，且对扩散模型等未见先进生成器的细粒度泛化能力不足。为此，研究者提出了一种结合动态对比学习的解析感知视觉语言模型（PVLM），用于零样本深度伪造溯源；该方法建立了一个细粒度评估基准，并利用GAN与扩散模型在生成人脸时保留源属性差异的特性，通过新颖的解析编码器和动态视觉-解析匹配来捕获解析感知的伪造表征。实验结果表明，该模型在提出的零样本溯源基准上，通过多种协议评估均超越了现有最优方法。</div>
</details>
</div>
<div class="card">
<div class="title">SenseNova-MARS: Empowering Multimodal Agentic Reasoning and Search via Reinforcement Learning</div>
<div class="meta-line">Authors: Yong Xien Chng, Tao Hu, Wenwen Tong, Xueheng Li, Jiandong Chen, Haojia Yu, Jiefan Lu, Hewei Guo, Hanming Deng, Chengjun Xie, Gao Huang, Dahua Lin, Lewei Lu</div>
<div class="meta-line">First: 2025-12-30T16:31:45+00:00 · Latest: 2026-01-25T08:36:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24330v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24330v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) can solve complex tasks through agentic reasoning, their capabilities remain largely constrained to text-oriented chain-of-thought or isolated tool invocation. They fail to exhibit the human-like proficiency required to seamlessly interleave dynamic tool manipulation with continuous reasoning, particularly in knowledge-intensive and visually complex scenarios that demand coordinated external tools such as search and image cropping. In this work, we introduce SenseNova-MARS, a novel Multimodal Agentic Reasoning and Search framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities via reinforcement learning (RL). Specifically, SenseNova-MARS dynamically integrates the image search, text search, and image crop tools to tackle fine-grained and knowledge-intensive visual understanding challenges. In the RL stage, we propose the Batch-Normalized Group Sequence Policy Optimization (BN-GSPO) algorithm to improve the training stability and advance the model&#x27;s ability to invoke tools and reason effectively. To comprehensively evaluate the agentic VLMs on complex visual tasks, we introduce the HR-MMSearch benchmark, the first search-oriented benchmark composed of high-resolution images with knowledge-intensive and search-driven questions. Experiments demonstrate that SenseNova-MARS achieves state-of-the-art performance on open-source search and fine-grained image understanding benchmarks. Specifically, on search-oriented benchmarks, SenseNova-MARS-32B scores 74.3 on MMSearch and 54.4 on HR-MMSearch, surpassing proprietary models such as Gemini-3-Pro and GPT-5.2. SenseNova-MARS represents a promising step toward agentic VLMs by providing effective and robust tool-use capabilities. To facilitate further research in this field, we will release all code, models, and datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SenseNova-MARS：通过强化学习赋能多模态智能体推理与搜索</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）能通过智能体推理解决复杂任务，但其能力仍主要局限于文本导向的思维链或孤立工具调用，无法展现人类般流畅的动态工具操作与连续推理交织的能力，尤其在需要协调外部工具（如搜索和图像裁剪）的知识密集型、视觉复杂场景中。本研究提出SenseNova-MARS，一种新型多模态智能体推理与搜索框架，通过强化学习（RL）赋予VLMs交织的视觉推理与工具使用能力。该框架动态整合图像搜索、文本搜索和图像裁剪工具，以应对细粒度、知识密集型的视觉理解挑战。在RL阶段，我们提出批量归一化组序列策略优化（BN-GSPO）算法，以提升训练稳定性并增强模型调用工具与有效推理的能力。为全面评估智能体VLMs在复杂视觉任务上的表现，我们构建了HR-MMSearch基准——首个由高分辨率图像与知识密集型、搜索驱动问题构成的搜索导向基准。实验表明，SenseNova-MARS在开源搜索与细粒度图像理解基准上达到最先进性能：在搜索导向基准中，SenseNova-MARS-32B在MMSearch上得分为74.3，在HR-MMSearch上得分为54.4，超越Gemini-3-Pro和GPT-5.2等专有模型。SenseNova-MARS通过提供高效稳健的工具使用能力，为智能体VLMs的发展迈出关键一步。为促进该领域研究，我们将公开全部代码、模型与数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of current Vision-Language Models in seamlessly interleaving dynamic tool manipulation with continuous reasoning for complex multimodal tasks. The authors propose SenseNova-MARS, a framework that empowers VLMs with interleaved visual reasoning and tool-use capabilities through reinforcement learning, specifically using a novel Batch-Normalized Group Sequence Policy Optimization algorithm to improve training stability. Experimental results on the newly introduced HR-MMSearch benchmark and other search-oriented benchmarks show that SenseNova-MARS-32B achieves state-of-the-art performance, scoring 74.3 on MMSearch and 54.4 on HR-MMSearch, surpassing proprietary models like Gemini-3-Pro and GPT-5.2.</div>
<div class="mono" style="margin-top:8px">本研究针对当前视觉语言模型在复杂多模态任务中难以无缝交织动态工具使用与连续推理的局限性。提出的SenseNova-MARS框架采用强化学习，具体是一种新颖的批归一化组序列策略优化算法，训练视觉语言模型动态整合图像搜索、文本搜索和图像裁剪等工具。在新引入的HR-MMSearch基准测试等实验结果表明，该方法取得了最先进的性能，其中320亿参数模型在MMSearch上得分为74.3，在HR-MMSearch上得分为54.4，超越了Gemini-3-Pro和GPT-5.2等专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">A Computational Approach to Visual Metonymy</div>
<div class="meta-line">Authors: Saptarshi Ghosh, Linfeng Liu, Tianyu Jiang</div>
<div class="meta-line">First: 2026-01-25T05:36:03+00:00 · Latest: 2026-01-25T05:36:03+00:00</div>
<div class="meta-line">Comments: EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17706v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17706v1">PDF</a> · <a href="https://github.com/cincynlp/ViMET">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Images often communicate more than they literally depict: a set of tools can suggest an occupation and a cultural artifact can suggest a tradition. This kind of indirect visual reference, known as visual metonymy, invites viewers to recover a target concept via associated cues rather than explicit depiction. In this work, we present the first computational investigation of visual metonymy. We introduce a novel pipeline grounded in semiotic theory that leverages large language models and text-to-image models to generate metonymic visual representations. Using this framework, we construct ViMET, the first visual metonymy dataset comprising 2,000 multiple-choice questions to evaluate the cognitive reasoning abilities in multimodal language models. Experimental results on our dataset reveal a significant gap between human performance (86.9%) and state-of-the-art vision-language models (65.9%), highlighting limitations in machines&#x27; ability to interpret indirect visual references. Our dataset is publicly available at: https://github.com/cincynlp/ViMET.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉转喻的计算方法研究</div>
<div class="mono" style="margin-top:8px">图像传达的信息常超越其直观描绘：一组工具可暗示某种职业，一件文化器物可代表某种传统。这种间接的视觉指代被称为视觉转喻，它引导观者通过关联线索而非直接描绘来理解目标概念。本研究首次对视觉转喻展开计算化探索，提出基于符号学理论的新型框架，利用大语言模型与文生图模型生成转喻性视觉表征。基于该框架，我们构建了首个视觉转喻数据集ViMET，包含2000道选择题，用于评估多模态语言模型的认知推理能力。实验结果显示：人类表现（86.9%）与前沿视觉语言模型（65.9%）存在显著差距，揭示了机器在解读间接视觉指代方面的局限。数据集已开源：https://github.com/cincynlp/ViMET。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the computational challenge of interpreting visual metonymy, where images convey concepts indirectly through associated cues rather than direct depiction. The method introduces a novel pipeline based on semiotic theory, utilizing large language models and text-to-image models to generate metonymic visual representations, and constructs the ViMET dataset with 2,000 multiple-choice questions to evaluate multimodal reasoning. Experimental results show a significant performance gap, with humans achieving 86.9% accuracy compared to 65.9% for state-of-the-art vision-language models, highlighting current limitations in machines&#x27; ability to process indirect visual references.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉转喻的计算挑战展开，即图像通过关联线索间接传达概念而非直接描绘。作者基于符号学理论提出了一种新方法，利用大语言模型和文生图模型生成转喻视觉表征，并构建了包含2000道选择题的ViMET数据集。在ViMET上的实验结果显示，人类准确率达到86.9%，而最先进的多模态语言模型仅为65.9%，存在显著性能差距，揭示了机器在理解此类间接视觉参照方面的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">StyleDecoupler: Generalizable Artistic Style Disentanglement</div>
<div class="meta-line">Authors: Zexi Jia, Jinchao Zhang, Jie Zhou</div>
<div class="meta-line">First: 2026-01-25T05:09:30+00:00 · Latest: 2026-01-25T05:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17697v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representing artistic style is challenging due to its deep entanglement with semantic content. We propose StyleDecoupler, an information-theoretic framework that leverages a key insight: multi-modal vision models encode both style and content, while uni-modal models suppress style to focus on content-invariant features. By using uni-modal representations as content-only references, we isolate pure style features from multi-modal embeddings through mutual information minimization. StyleDecoupler operates as a plug-and-play module on frozen Vision-Language Models without fine-tuning. We also introduce WeART, a large-scale benchmark of 280K artworks across 152 styles and 1,556 artists. Experiments show state-of-the-art performance on style retrieval across WeART and WikiART, while enabling applications like style relationship mapping and generative model evaluation. We release our method and dataset at this url.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StyleDecoupler：可泛化的艺术风格解耦方法</div>
<div class="mono" style="margin-top:8px">艺术风格因其与语义内容的深度纠缠而难以表征。我们提出StyleDecoupler——一个基于信息论的框架，其核心洞见在于：多模态视觉模型同时编码风格与内容，而单模态模型会抑制风格以聚焦内容不变特征。通过将单模态表征作为纯内容参照，我们借助互信息最小化从多模态嵌入中分离出纯净风格特征。StyleDecoupler可作为即插即用模块应用于冻结的视觉语言模型而无需微调。我们还构建了WeART大规模基准数据集，涵盖152种风格、1,556位艺术家的28万件艺术作品。实验表明，该方法在WeART和WikiART的风格检索任务上达到最优性能，并能支持风格关系图谱构建、生成模型评估等应用。相关方法与数据集已通过指定网址开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of disentangling artistic style from semantic content, which are deeply entangled in visual representations. The method, StyleDecoupler, is an information-theoretic framework that isolates style by leveraging uni-modal vision models as content-only references to extract pure style features from multi-modal embeddings through mutual information minimization, operating as a plug-and-play module on frozen Vision-Language Models without fine-tuning. Key experimental results demonstrate state-of-the-art performance on style retrieval across the introduced WeART benchmark (280K artworks) and WikiART, while also enabling applications such as style relationship mapping and generative model evaluation.</div>
<div class="mono" style="margin-top:8px">由于艺术风格与语义内容深度纠缠，其表征颇具挑战，这促使了StyleDecoupler这一信息论框架的开发。该方法基于一个关键洞见：多模态视觉模型编码了风格和内容，而单模态模型则抑制风格以专注于内容不变特征；它通过将单模态表征作为内容参考并最小化互信息，从冻结的视觉语言模型嵌入中分离出纯风格特征，且无需微调。主要实验结果表明，在新引入的大规模WeART基准（28万件艺术品）和WikiART上，该方法在风格检索任务上取得了最先进的性能，并能支持风格关系映射和生成模型评估等应用。</div>
</details>
</div>
<div class="card">
<div class="title">BigTokDetect: A Clinically-Informed Vision-Language Modeling Framework for Detecting Pro-Bigorexia Videos on TikTok</div>
<div class="meta-line">Authors: Minh Duc Chu, Kshitij Pawar, Zihao He, Roxanna Sharifi, Ross Sonnenblick, Magdalayna Curry, Laura D&#x27;Adamo, Lindsay Young, Stuart B Murray, Kristina Lerman</div>
<div class="meta-line">First: 2025-07-30T08:14:14+00:00 · Latest: 2026-01-25T02:11:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.06515v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.06515v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Social media platforms face escalating challenges in detecting harmful content that promotes muscle dysmorphic behaviors and cognitions (bigorexia). This content can evade moderation by camouflaging as legitimate fitness advice and disproportionately affects adolescent males. We address this challenge with BigTokDetect, a clinically informed framework for identifying pro-bigorexia content on TikTok. We introduce BigTok, the first expert-annotated multimodal benchmark dataset of over 2,200 TikTok videos labeled by clinical psychiatrists across five categories and eighteen fine-grained subcategories. Comprehensive evaluation of state-of-the-art vision-language models reveals that while commercial zero-shot models achieve the highest accuracy on broad primary categories, supervised fine-tuning enables smaller open-source models to perform better on fine-grained subcategory detection. Ablation studies show that multimodal fusion improves performance by 5 to 15 percent, with video features providing the most discriminative signals. These findings support a grounded moderation approach that automates detection of explicit harms while flagging ambiguous content for human review, and they establish a scalable framework for harm mitigation in emerging mental health domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BigTokDetect：基于临床知识的视觉语言建模框架，用于检测TikTok上宣扬大块头症的视频</div>
<div class="mono" style="margin-top:8px">社交媒体平台在检测宣扬肌肉畸形行为和认知（大块头症）的有害内容方面面临日益严峻的挑战。此类内容常伪装成合法的健身建议以规避审核，并对青少年男性群体造成尤为严重的影响。为此，我们提出BigTokDetect——一个基于临床知识的框架，用于识别TikTok上宣扬大块头症的内容。我们构建了BigTok数据集，这是首个由临床精神科医生标注的多模态基准数据集，包含2200余个TikTok视频，涵盖五大类别和十八个细分子类别。对前沿视觉语言模型的综合评估表明：商业零样本模型在宽泛主类别上准确率最高，而监督微调能使小型开源模型在细分子类别检测上表现更优。消融实验显示，多模态融合将性能提升5%至15%，其中视频特征最具判别力。这些发现支持一种基于证据的审核策略：自动化检测显性危害内容，同时将模糊内容标记为人工审核对象，从而为新兴心理健康领域的危害缓解建立了可扩展的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of detecting harmful pro-bigorexia content on TikTok, which often masquerades as fitness advice and poses risks to adolescent males. The authors propose BigTokDetect, a clinically-informed vision-language framework, and introduce BigTok, a novel expert-annotated multimodal dataset of over 2,200 videos labeled across five primary categories and eighteen subcategories. Experimental results show that commercial zero-shot models achieve the highest accuracy on broad categories, while supervised fine-tuning enables smaller open-source models to excel at fine-grained subcategory detection, with multimodal fusion improving performance by 5–15% and video features proving most discriminative.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决社交媒体上检测促进肌肉变形症内容的问题，这类内容常伪装成健身建议以逃避审核，对青少年男性构成风险。作者提出了BigTokDetect，一个基于临床信息的视觉-语言框架，并引入了BigTok，这是一个由专家标注的多模态数据集，包含超过2200个TikTok视频，标注了五个类别和十八个子类别。实验结果表明，商业零样本模型在宽泛类别上准确率最高，而监督微调使较小的开源模型在细粒度子类别检测上表现更优，多模态融合将性能提升5-15%，其中视频特征最具区分性。</div>
</details>
</div>
<div class="card">
<div class="title">Will It Zero-Shot?: Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</div>
<div class="meta-line">Authors: Kevin Robbins, Xiaotong Liu, Yu Wu, Le Sun, Grady McPeak, Abby Stylianou, Robert Pless</div>
<div class="meta-line">First: 2026-01-24T17:30:23+00:00 · Latest: 2026-01-24T17:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.17535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>能否实现零样本分类？：预测任意查询的零样本分类性能</div>
<div class="mono" style="margin-top:8px">如CLIP等视觉-语言模型构建了文本与图像的对齐嵌入空间，使得用户仅需命名待区分类别即可构建视觉分类器。然而，在某一领域表现优异的模型可能在另一领域失效，非专业用户缺乏直接评估所选VLM是否适用于其问题的方法。本研究基于先前仅使用文本比较评估模型在自然语言任务中表现的工作，进一步探索生成与任务相关的合成图像以评估和优化零样本准确率预测的方法。实验表明，在基线纯文本评分基础上加入生成图像能显著提升预测质量，同时为用户提供用于评估的图像类型反馈。在标准CLIP基准数据集上的实验证明，这种基于图像的方法能帮助用户无需标注样本即可预测VLM是否适用于其应用场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge for non-expert users in predicting whether a Vision-Language Model (VLM) like CLIP will perform effectively for their specific zero-shot classification task, as model performance can vary unpredictably across different domains. The method improves upon prior text-only evaluation by generating synthetic images relevant to the user&#x27;s query and using these images alongside text comparisons to assess and refine the prediction of zero-shot classification accuracy. Experimental results on standard CLIP benchmarks show that incorporating generated imagery significantly enhances prediction quality compared to text-only baselines and provides users with visual feedback on the assessment process.</div>
<div class="mono" style="margin-top:8px">本研究针对非专家用户难以预测视觉语言模型（如CLIP）在其特定零样本分类任务中是否有效的问题，因为模型性能在不同领域可能不可预测地变化。该方法改进了先前仅基于文本的评估，通过生成与用户查询相关的合成图像，并结合文本比较来评估和优化零样本准确率的预测。在标准CLIP基准数据集上的实验结果表明，引入生成图像相比仅文本基线显著提高了预测质量，并为用户提供了关于评估所用图像类型的视觉反馈。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-scale Temporal Prediction via Incremental Generation and Multi-agent Collaboration</div>
<div class="meta-line">Authors: Zhitao Zeng, Guojian Yuan, Junyuan Mao, Yuxuan Wang, Xiaoshuang Jia, Yueming Jin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-22T07:22:27+00:00 · Latest: 2026-01-24T15:15:35+00:00</div>
<div class="meta-line">Comments: 20 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.17429v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.17429v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate temporal prediction is the bridge between comprehensive scene understanding and embodied artificial intelligence. However, predicting multiple fine-grained states of a scene at multiple temporal scales is difficult for vision-language models. We formalize the Multi-Scale Temporal Prediction (MSTP) task in general and surgical scenes by decomposing multi-scale into two orthogonal dimensions: the temporal scale, forecasting states of humans and surgery at varying look-ahead intervals, and the state scale, modeling a hierarchy of states in general and surgical scenes. For example, in general scenes, states of contact relationships are finer-grained than states of spatial relationships. In surgical scenes, medium-level steps are finer-grained than high-level phases yet remain constrained by their encompassing phase. To support this unified task, we introduce the first MSTP Benchmark, featuring synchronized annotations across multiple state scales and temporal scales. We further propose a method, Incremental Generation and Multi-agent Collaboration (IG-MC), which integrates two key innovations. First, we present a plug-and-play incremental generation module that continuously synthesizes up-to-date visual previews at expanding temporal scales to inform multiple decision-making agents, keeping decisions and generated visuals synchronized and preventing performance degradation as look-ahead intervals lengthen. Second, we present a decision-driven multi-agent collaboration framework for multi-state prediction, comprising generation, initiation, and multi-state assessment agents that dynamically trigger and evaluate prediction cycles to balance global coherence and local fidelity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于增量生成与多智能体协作的多尺度时序预测</div>
<div class="mono" style="margin-top:8px">精准的时序预测是全面场景理解与具身人工智能之间的桥梁。然而，视觉语言模型难以在多个时间尺度上预测场景的多个细粒度状态。我们通过将多尺度分解为两个正交维度——时间尺度（预测人类与手术在不同前瞻间隔下的状态）和状态尺度（建模通用场景与手术场景中的状态层级），形式化定义了通用场景与手术场景中的多尺度时序预测任务。例如，在通用场景中，接触关系状态比空间关系状态更细粒度；在手术场景中，中层级步骤比高层级阶段更细粒度，但仍受其所属阶段的约束。为支持这一统一任务，我们提出了首个MSTP基准测试，具备跨多状态尺度与时间尺度的同步标注。我们进一步提出一种增量生成与多智能体协作方法，该方法融合了两项关键创新：其一，我们设计了一个即插即用的增量生成模块，可在扩展的时间尺度上持续合成最新的视觉预览，为多个决策智能体提供信息，保持决策与生成视觉内容的同步，防止因前瞻间隔延长导致的性能下降；其二，我们提出了一个决策驱动的多智能体协作框架用于多状态预测，包含生成、启动和多状态评估智能体，这些智能体动态触发并评估预测周期，以平衡全局一致性与局部保真度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling vision-language models to predict multiple fine-grained scene states across varying temporal horizons, a capability crucial for bridging scene understanding and embodied AI. The authors formalize the Multi-Scale Temporal Prediction (MSTP) task, decomposing it into temporal and state scales, and introduce a corresponding benchmark with synchronized annotations. Their proposed method, Incremental Generation and Multi-agent Collaboration (IG-MC), employs a plug-and-play module to generate synchronized visual previews for expanding look-ahead intervals and a multi-agent framework with generation, initiation, and assessment agents to dynamically manage prediction cycles. Experimental results demonstrate that this approach effectively maintains prediction performance over long intervals and balances global coherence with local fidelity in state predictions.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决视觉语言模型难以预测多时间尺度下细粒度场景状态的问题，这是实现具身人工智能的关键。研究者形式化了多尺度时间预测任务，将其分解为时间尺度和状态尺度，并引入了具有同步标注的对应基准。他们提出的增量生成与多智能体协作方法，采用了一个即插即用的模块来为扩展的时间范围生成同步视觉预览，并利用一个由生成、启动和评估智能体组成的多智能体框架来动态管理预测周期。实验结果表明，该方法能有效维持长时距预测的性能，并在状态预测中平衡全局一致性与局部保真度。</div>
</details>
</div>
<div class="card">
<div class="title">BLEnD-Vis: Benchmarking Multimodal Cultural Understanding in Vision Language Models</div>
<div class="meta-line">Authors: Bryan Chen Zhengyu Tan, Zheng Weihua, Zhengyuan Liu, Nancy F. Chen, Hwaran Lee, Kenny Tsu Wei Choo, Roy Ka-Wei Lee</div>
<div class="meta-line">First: 2025-10-13T09:10:05+00:00 · Latest: 2026-01-24T13:12:46+00:00</div>
<div class="meta-line">Comments: EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11178v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11178v2">PDF</a> · <a href="https://github.com/Social-AI-Studio/BLEnD-Vis">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As vision-language models (VLMs) are deployed globally, their ability to understand culturally situated knowledge becomes essential. Yet, existing evaluations largely assess static recall or isolated visual grounding, leaving unanswered whether VLMs possess robust and transferable cultural understanding. We introduce BLEnD-Vis, a multimodal, multicultural benchmark designed to evaluate the robustness of everyday cultural knowledge in VLMs across linguistic rephrasings and visual modalities. Building on the BLEnD dataset, BLEnD-Vis constructs 313 culturally grounded question templates spanning 16 regions and generates three aligned multiple-choice formats: (i) a text-only baseline querying from Region $\rightarrow$ Entity, (ii) an inverted text-only variant (Entity $\rightarrow$ Region), and (iii) a VQA-style version of (ii) with generated images. The resulting benchmark comprises 4,916 images and over 21,000 multiple-choice questions (MCQ) instances, validated through human annotation. BLEnD-Vis reveals significant fragility in current VLM cultural knowledge; models exhibit performance drops under linguistic rephrasing. While visual cues often aid performance, low cross-modal consistency highlights the challenges of robustly integrating textual and visual understanding, particularly in lower-resource regions. BLEnD-Vis thus provides a crucial testbed for systematically analysing cultural robustness and multimodal grounding, exposing limitations and guiding the development of more culturally competent VLMs. Code is available at https://github.com/Social-AI-Studio/BLEnD-Vis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BLEnD-Vis：视觉语言模型多模态文化理解基准评测</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLMs）在全球范围内部署，其理解文化情境知识的能力变得至关重要。然而，现有评估主要关注静态记忆或孤立的视觉基础能力，未能回答VLMs是否具备稳健且可迁移的文化理解力。我们提出BLEnD-Vis——一个多模态、跨文化的基准评测体系，旨在通过语言重述和视觉模态评估VLMs对日常文化知识的鲁棒性。基于BLEnD数据集，BLEnD-Vis构建了涵盖16个地区的313个文化情境问题模板，并生成三种对齐的多选题形式：（i）纯文本基线查询（地区→实体），（ii）逆向纯文本变体（实体→地区），以及（iii）结合生成图像的VQA风格版本（基于ii）。最终基准包含4,916张图像和超过21,000道多选题实例，并经过人工标注验证。BLEnD-Vis揭示了当前VLM文化知识的显著脆弱性：模型在语言重述下表现明显下降。虽然视觉线索常能提升性能，但跨模态一致性较低凸显了文本与视觉理解稳健融合的挑战，在资源匮乏地区尤为突出。因此，BLEnD-Vis为系统分析文化鲁棒性和多模态基础能力提供了关键测试平台，既能暴露现有局限，也能指导开发更具文化适应性的VLMs。代码发布于https://github.com/Social-AI-Studio/BLEnD-Vis。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To assess whether vision-language models (VLMs) possess robust and transferable cultural understanding beyond static recall, this work introduces BLEnD-Vis, a multimodal, multicultural benchmark. The method builds on the BLEnD dataset to construct 313 culturally grounded question templates across 16 regions, generating three aligned multiple-choice formats: a text-only baseline, an inverted text-only variant, and a VQA-style version with generated images, resulting in over 21,000 questions and 4,916 images validated by human annotation. Experimental results reveal significant fragility in current VLM cultural knowledge, with performance dropping under linguistic rephrasing; while visual cues sometimes help, low cross-modal consistency highlights challenges in robustly integrating textual and visual understanding, especially for lower-resource regions.</div>
<div class="mono" style="margin-top:8px">该研究旨在评估视觉语言模型是否具备稳健且可迁移的文化理解能力，因为现有基准主要关注静态记忆或孤立的视觉基础。方法上引入了BLEnD-Vis，这是一个多模态、多文化的基准，构建了涵盖16个地区的313个文化相关的问题模板，生成了三种对齐的多选题格式，包括纯文本和带有生成图像的视觉问答版本，最终产生了4916张图像和超过21000个经过人工验证的问题。主要实验结果表明，当前视觉语言模型的文化知识存在显著脆弱性，在语言重述下性能下降，虽然视觉线索通常有助于提升表现，但跨模态一致性较低，突显了在整合文本和视觉理解方面的挑战，尤其是在资源较少的地区。</div>
</details>
</div>
<div class="card">
<div class="title">WebGym: Scaling Training Environments for Visual Web Agents with Realistic Tasks</div>
<div class="meta-line">Authors: Hao Bai, Alexey Taymanov, Tong Zhang, Aviral Kumar, Spencer Whitehead</div>
<div class="meta-line">First: 2026-01-05T09:35:11+00:00 · Latest: 2026-01-24T12:36:56+00:00</div>
<div class="meta-line">Comments: Fixed typo in Figure 2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02439v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.02439v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WebGym, the largest-to-date open-source environment for training realistic visual web agents. Real websites are non-stationary and diverse, making artificial or small-scale task sets insufficient for robust policy learning. WebGym contains nearly 300,000 tasks with rubric-based evaluations across diverse, real-world websites and difficulty levels. We train agents with a simple reinforcement learning (RL) recipe, which trains on the agent&#x27;s own interaction traces (rollouts), using task rewards as feedback to guide learning. To enable scaling RL, we speed up sampling of trajectories in WebGym by developing a high-throughput asynchronous rollout system, designed specifically for web agents. Our system achieves a 4-5x rollout speedup compared to naive implementations. Second, we scale the task set breadth, depth, and size, which results in continued performance improvement. Fine-tuning a strong base vision-language model, Qwen-3-VL-8B-Instruct, on WebGym results in an improvement in success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models such as GPT-4o and GPT-5-Thinking that achieve 27.1% and 29.8%, respectively. This improvement is substantial because our test set consists only of tasks on websites never seen during training, unlike many other prior works on training visual web agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WebGym：面向视觉网页代理的现实任务训练环境规模化平台</div>
<div class="mono" style="margin-top:8px">我们推出WebGym，这是迄今为止最大的开源现实视觉网页代理训练环境。真实网站具有非稳态和多样性特征，使得人工或小规模任务集难以支撑稳健的策略学习。WebGym包含近30万个任务，通过基于量规的评估体系覆盖多样化的真实网站与难度层级。我们采用简单的强化学习方案训练代理：利用代理自身交互轨迹（rollout）进行训练，以任务奖励作为学习反馈。为实现强化学习的规模化，我们专门为网页代理开发了高吞吐量异步轨迹采样系统，使WebGym的轨迹采样速度较原始实现提升4-5倍。其次，我们通过拓展任务集的广度、深度与规模实现持续性能提升。在WebGym上对强基础视觉语言模型Qwen-3-VL-8B-Instruct进行微调后，其在分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的代理。这一提升尤为显著，因为我们的测试集仅包含训练阶段未见的网站任务，这与多数现有视觉网页代理训练研究形成鲜明对比。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of training robust visual web agents, as real websites are non-stationary and diverse, rendering artificial or small-scale task sets insufficient. The method introduces WebGym, a large-scale open-source environment with nearly 300,000 tasks and rubric-based evaluations, and employs a reinforcement learning recipe that trains on agent interaction traces using task rewards, accelerated by a high-throughput asynchronous rollout system achieving a 4-5x speedup. Key experimental findings show that fine-tuning the Qwen-3-VL-8B-Instruct model on WebGym improves the success rate on an out-of-distribution test set from 26.2% to 42.9%, significantly outperforming agents based on proprietary models like GPT-4o and GPT-5-Thinking, which achieved 27.1% and 29.8% respectively, demonstrating the effectiveness of scaling task breadth, depth, and size.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决训练鲁棒视觉网页代理的挑战，因为真实网站具有非平稳性和多样性，使得人工或小规模任务集不足以进行有效的策略学习。方法提出了WebGym，这是一个大规模开源环境，包含近30万个任务和基于量规的评估，并采用一种强化学习方案，利用任务奖励对代理交互轨迹进行训练；通过一个实现4-5倍加速的高吞吐量异步轨迹采样系统来支持规模化。主要实验结果表明，在WebGym上微调Qwen-3-VL-8B-Instruct模型，将分布外测试集上的成功率从26.2%提升至42.9%，显著优于基于GPT-4o（27.1%）和GPT-5-Thinking（29.8%）等专有模型的代理。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
