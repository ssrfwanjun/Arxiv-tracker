<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-30 05:33</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260130_0533</div>
    <div class="row"><div class="card">
<div class="title">Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</div>
<div class="meta-line">Authors: Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli</div>
<div class="meta-line">First: 2026-01-28T18:59:34+00:00 · Latest: 2026-01-28T18:59:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20861v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20861v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化策略导致大语言模型灾难性遗忘</div>
<div class="mono" style="margin-top:8px">当前AI系统最显著的能力缺失之一是部署后持续学习的能力。实现此类持续学习系统面临多项挑战，其中一项是训练先进大语言模型所用的基于梯度的算法对内存的巨大需求。进化策略作为传统学习算法的无梯度替代方案近期重新兴起，并在大语言模型的特定任务中展现出令人鼓舞的性能。本文对进化策略进行了全面分析，重点评估其在增加更新步数训练时的遗忘曲线。我们首先发现，在可比较的计算预算下，进化策略在数学和推理任务上能达到接近GRPO的性能指标。然而，对于持续学习至关重要的发现是：进化策略的性能提升伴随着对先前能力的显著遗忘，这限制了其在在线训练模型中的适用性。我们还探究了该现象背后的原因，结果表明与相应的GRPO更新相比，进化策略的更新稀疏度显著更低且其$\ell_2$范数量级更大，这解释了两类算法间截然不同的遗忘曲线。本研究旨在揭示进化策略等无梯度算法的遗忘问题，以期启发未来研究缓解这些缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for AI systems capable of continuous learning after deployment, a challenge partly due to the high memory demands of gradient-based methods. The study comprehensively analyzes Evolutionary Strategies (ES) as a gradient-free alternative, specifically evaluating its forgetting behavior over increasing training steps on math and reasoning tasks. The key experimental findings reveal that while ES can achieve performance close to GRPO with comparable compute, it suffers from significant catastrophic forgetting of prior abilities, which limits its online applicability; this is attributed to ES updates being less sparse and having a much larger ℓ₂ norm than GRPO updates.</div>
<div class="mono" style="margin-top:8px">本研究探讨了进化策略作为无梯度替代方法在大型语言模型持续学习中的潜力，其动机是梯度方法内存需求高，阻碍了部署后的持续学习。该方法对进化策略进行了全面分析，通过增加更新步数训练模型在数学和推理任务上，特别评估其遗忘曲线，并与GRPO进行比较。主要实验结果表明，尽管在可比计算预算下进化策略能达到接近GRPO的性能，但它会导致对先前能力的显著灾难性遗忘，严重限制了其在线训练的适用性；进一步分析表明，这与进化策略的更新不如GRPO稀疏且具有数量级更大的ℓ₂范数有关。</div>
</details>
</div>
<div class="card">
<div class="title">LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</div>
<div class="meta-line">Authors: Piyush Jha, Arnav Arora, Vijay Ganesh</div>
<div class="meta-line">Venue: AAAI 2025</div>
<div class="meta-line">First: 2024-11-13T18:44:30+00:00 · Latest: 2026-01-28T18:58:57+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.08862v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.08862v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLMStinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLMStinger across open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMStinger：利用强化学习微调的大语言模型实现越狱攻击</div>
<div class="mono" style="margin-top:8px">本文提出LLMStinger创新方法，利用大语言模型自动生成对抗性后缀实施越狱攻击。与传统依赖复杂提示工程或白盒访问的方法不同，LLMStinger采用强化学习循环微调攻击者大语言模型，基于HarmBench基准中的恶意问题及现有攻击生成新型后缀。本方法显著优于现有红队测试方案（已对比15种最新方法），在具备严格安全措施的LLaMA2-7B-chat模型上实现攻击成功率提升57.2%，在Claude 2模型上提升50.3%。此外，在GPT-3.5和Gemma-2B-it模型上分别达到94.97%和99.4%的攻击成功率，证明了LLMStinger在开源与闭源模型间的强鲁棒性与适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To automate the generation of effective adversarial suffixes for jailbreaking large language models (LLMs) without relying on complex manual prompt engineering or white-box model access, this research introduces LLMStinger. The method employs a reinforcement learning (RL) loop to fine-tune an attacker LLM, enabling it to iteratively generate new adversarial suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Experimental results demonstrate that LLMStinger significantly outperforms 15 existing red-teaming methods, achieving improvements in Attack Success Rate (ASR) of +57.2% on LLaMA2-7B-chat and +50.3% on Claude 2, while also attaining high ASRs of 94.97% on GPT-3.5 and 99.4% on Gemma-2B-it, showcasing its robustness across both open and closed-source models.</div>
<div class="mono" style="margin-top:8px">为了在不依赖复杂手动提示工程或白盒访问的情况下，自动化生成用于越狱大型语言模型（LLM）的有效对抗性后缀，本研究提出了LLMStinger。该方法采用强化学习（RL）循环对攻击者LLM进行微调，使其能够基于HarmBench基准中的有害问题，从现有攻击中迭代生成新的对抗性后缀。实验结果表明，LLMStinger显著优于15种现有的红队测试方法，在LLaMA2-7B-chat和Claude 2上的攻击成功率（ASR）分别提升了57.2%和50.3%，同时在GPT-3.5和Gemma-2B-it上分别达到了94.97%和99.4%的ASR，证明了其在各种开源和闭源模型上的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</div>
<div class="meta-line">Authors: Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</div>
<div class="meta-line">First: 2026-01-28T18:54:27+00:00 · Latest: 2026-01-28T18:54:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20854v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20854v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索Transformer在变分自编码器中用于表格数据生成的位置配置</div>
<div class="mono" style="margin-top:8px">表格数据对生成模型而言仍是一个具有挑战性的领域。特别是，标准变分自编码器（VAE）架构通常由多层感知机构成，难以建模特征间的关系，尤其是在处理混合数据类型时。相比之下，Transformer通过其注意力机制更擅长捕捉复杂的特征交互。本文通过实证研究探讨了将Transformer集成到VAE不同组件中的影响。我们在OpenML CC18套件的57个数据集上进行了实验，并得出两个主要结论：首先，结果表明将Transformer置于利用潜在表示和解码器表示的位置会导致保真度与多样性之间的权衡；其次，我们观察到Transformer在所有组件中连续块之间具有高度相似性，特别是在解码器中，Transformer的输入与输出关系近似线性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of generating high-quality tabular data, where standard Variational Autoencoders (VAEs) with multilayer perceptrons often fail to capture complex feature interactions. The authors empirically explore the impact of integrating Transformer modules into different components of a VAE to leverage their attention mechanism for modeling relationships. Experiments on 57 datasets from the OpenML CC18 suite reveal that placing Transformers to utilize latent and decoder representations creates a trade-off between fidelity and diversity in generated data, and they observe high similarity between consecutive Transformer blocks, with the decoder&#x27;s input-output relationship being approximately linear.</div>
<div class="mono" style="margin-top:8px">本研究针对表格数据生成的挑战，即标准变分自编码器（VAE）因使用多层感知机而难以建模特征间复杂关系。方法上，通过将Transformer模块集成到VAE的不同组件中，利用其注意力机制捕捉特征交互。在OpenML CC18套件的57个数据集上的实验表明，将Transformer置于利用潜在表示和解码器表示的位置时，会在生成保真度和多样性之间产生权衡，并发现所有组件中连续Transformer块的高度相似性，特别是解码器的输入输出关系近似线性。</div>
</details>
</div>
<div class="card">
<div class="title">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</div>
<div class="meta-line">Authors: Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-06-09T17:46:47+00:00 · Latest: 2026-01-28T18:52:54+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07972v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07972v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HeuriGym：面向组合优化中LLM启发式算法生成的智能体基准</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLMs）在推理和基于智能体的问题解决方面取得了显著进展，但现有评估方法未能充分衡量其能力：当前基准测试要么依赖易于饱和和记忆的封闭式问题，要么采用缺乏一致性和严谨性的主观比较。本研究提出HeuriGym——一个用于评估LLMs为组合优化问题生成启发式算法的智能体框架，其特点在于目标定义明确且解空间广阔。该框架支持LLMs提出启发式策略、通过代码执行获得评估反馈，并迭代优化解决方案。我们在计算机系统、物流、生物学等领域的九类问题上测试了九个前沿模型，揭示了其在工具使用、规划和自适应推理方面的持续局限。为量化性能，我们提出质量-产出指数（QYI），该指标同时捕捉解决方案的通过率和质量。即使如GPT-4o-mini-high和Gemini-2.5-Pro等顶级模型，其QYI得分也仅为0.6，远低于专家基线水平1.0。这一开源基准旨在引导LLMs朝着更有效、更贴近实际的科学与工程问题解决方向发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inadequacy of current evaluation methods for large language models (LLMs) in assessing their reasoning and problem-solving capabilities, which are often limited by closed-ended questions or subjective comparisons. The authors introduce HeuriGym, an agentic framework where LLMs propose heuristic algorithms for combinatorial optimization problems, receive feedback through code execution, and iteratively refine their solutions. Experimental evaluation of nine state-of-the-art models across nine problems reveals persistent limitations in tool use, planning, and adaptive reasoning, with even top models like GPT-4o-mini-high and Gemini-2.5-Pro achieving a Quality-Yield Index (QYI) score of only 0.6, significantly below the expert baseline of 1.</div>
<div class="mono" style="margin-top:8px">本研究针对当前大语言模型评估方法依赖封闭式问题或主观比较的局限性，提出了HeuriGym这一智能体框架，用于评估组合优化中由大语言模型生成的启发式算法。该方法使大语言模型能够提出启发式算法，通过代码执行获得反馈，并迭代优化解决方案，同时使用结合通过率和质量的“质量-产出指数”来量化性能。在计算机系统、物流和生物学等领域的九个问题上的实验结果表明，模型在工具使用和规划方面仍存在不足，即使是GPT-4-mini-high和Gemini-2.5-Pro等顶级模型的QYI得分也仅为0.6左右，远低于专家基准的1。</div>
</details>
</div>
<div class="card">
<div class="title">C3Box: A CLIP-based Class-Incremental Learning Toolbox</div>
<div class="meta-line">Authors: Hao Sun, Da-Wei Zhou</div>
<div class="meta-line">First: 2026-01-28T18:52:36+00:00 · Latest: 2026-01-28T18:52:36+00:00</div>
<div class="meta-line">Comments: The code is available at https://github.com/LAMDA-CL/C3Box</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20852v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20852v1">PDF</a> · <a href="https://github.com/LAMDA-CL/C3Box">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>C3Box：基于CLIP的类增量学习工具箱</div>
<div class="mono" style="margin-top:8px">传统机器学习系统通常针对静态数据分布设计，在处理动态数据流时易受灾难性遗忘影响。类增量学习（CIL）通过使学习系统在持续学习新类别时保留已有知识，以应对这一挑战。随着CLIP等预训练模型（PTMs）的兴起，利用其强大的泛化能力与语义对齐特性已成为CIL领域的重要方向。然而，现有基于CLIP的CIL方法常分散于不同代码库，依赖不一致的配置，阻碍了公平比较、可复现性及实际应用。为此，我们提出C3Box（基于CLIP的类增量学习工具箱），这是一个模块化、功能全面的Python工具箱。C3Box将代表性传统CIL方法、基于ViT的CIL方法以及前沿的基于CLIP的CIL方法整合至统一的CLIP框架中。通过继承PyCIL的简洁设计，C3Box提供基于JSON的配置与标准化执行流程，能以较低工程开销实现可复现的实验，并成为持续学习研究的可靠基准平台。C3Box设计注重易用性，仅依赖广泛使用的开源库并支持主流操作系统。代码发布于https://github.com/LAMDA-CL/C3Box。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of catastrophic forgetting in class-incremental learning (CIL) and the lack of a unified framework for evaluating CLIP-based CIL methods, this work introduces C3Box, a modular Python toolbox. It integrates traditional, ViT-based, and state-of-the-art CLIP-based CIL methods into a single framework with a JSON-based configuration and standardized pipeline, inheriting the design of PyCIL. The toolbox facilitates reproducible and fair benchmarking with low engineering overhead, as demonstrated through its comprehensive integration and support for major operating systems using common open-source libraries.</div>
<div class="mono" style="margin-top:8px">为解决机器学习系统从动态数据流中学习时面临的灾难性遗忘问题，本研究提出了C3Box，一个基于预训练CLIP模型的模块化Python工具箱，用于类增量学习。该方法将传统、基于ViT以及先进的基于CLIP的类增量学习方法整合到一个统一框架中，采用了基于JSON的配置和源自PyCIL的标准化执行流程。实验结果表明，该工具箱能够以较低的工程开销实现可复现的基准测试，为持续学习研究提供了一个可靠的公平比较和实践应用平台。</div>
</details>
</div>
<div class="card">
<div class="title">Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation</div>
<div class="meta-line">Authors: Weixin Chen, Li Chen, Yuhan Zhao</div>
<div class="meta-line">Venue: WWW 2026 Oral Presentation</div>
<div class="meta-line">First: 2026-01-28T18:48:43+00:00 · Latest: 2026-01-28T18:48:43+00:00</div>
<div class="meta-line">Comments: Accepted to WWW 2026 Workshop on HCRS (Oral Presentation)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20848v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20848v1">PDF</a> · <a href="https://github.com/weixinchen98/Cofair">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>训练后公平性控制：推荐系统中动态公平性的单次训练框架</div>
<div class="mono" style="margin-top:8px">尽管缓解推荐系统不公平性的研究日益增多，现有公平性方法通常在训练时固定公平性要求，训练后灵活性有限。然而实际场景中，不同利益相关方可能随时间提出差异化的公平性需求，为每种需求重新训练模型成本过高。为此，我们提出Cofair——一个支持训练后公平性控制的单次训练框架。该框架通过共享表征层与公平性条件适配器模块，生成适应不同公平性等级的用户嵌入，并引入用户级正则化项确保跨等级的用户公平性单调提升。理论证明Cofair的对抗目标上界约束人口统计均等性，正则化项则在用户层面实现渐进公平性。在多数据集与骨干模型上的实验表明，本框架能在不同等级提供动态公平性，其公平性-准确性曲线优于或媲美前沿基线方法，且无需为每个新公平性需求重新训练。代码已开源：https://github.com/weixinchen98/Cofair。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing fairness-aware recommendation methods often fix fairness constraints during training, lacking flexibility to adapt to diverse and evolving stakeholder requirements without costly retraining. To address this, this work proposes Cofair, a single-train framework that enables post-training fairness control by using a shared representation layer with fairness-conditioned adapter modules to generate user embeddings for different fairness levels, alongside a user-level regularization term that ensures monotonic fairness improvements. Experiments on multiple datasets and backbone models show that Cofair provides dynamic fairness control at various levels, achieving fairness-accuracy trade-off curves comparable to or better than state-of-the-art baselines without requiring retraining for each new fairness specification.</div>
<div class="mono" style="margin-top:8px">现有的公平性推荐方法通常在训练时固定公平性约束，缺乏灵活性，难以在不进行昂贵重训练的情况下适应多样且动态变化的利益相关者需求。为解决此问题，本研究提出了Cofair，一个单次训练框架，通过使用带有公平性条件适配器模块的共享表示层来生成针对不同公平性水平的用户嵌入，并辅以一个用户级正则化项以确保公平性的单调改进，从而实现了训练后的公平性控制。理论分析表明该框架的对抗性目标上界了人口统计均等性，在多个数据集和骨干模型上的实验证明，该框架能提供动态的公平性控制，在不需为每个新公平性目标重训练的情况下，达到了与最先进方法相当或更优的公平性-准确性权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</div>
<div class="meta-line">Authors: Rui Pan, Zhuofu Chen, Hongyi Liu, Arvind Krishnamurthy, Ravi Netravali</div>
<div class="meta-line">First: 2025-12-23T18:16:58+00:00 · Latest: 2026-01-28T18:48:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20573v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.20573v3">PDF</a> · <a href="https://github.com/ruipeterpan/failfast">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM&#x27;s speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It &quot;fails fast&quot; by spending minimal compute in hard-to-speculate regions to shrink speculation latency and &quot;wins big&quot; by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.7$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>快速失败，大获全胜：通过扩散大语言模型重新思考推测解码中的草稿策略</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）能够实现快速、并行的令牌生成，但其独立使用存在固有的效率与质量权衡问题。我们证明，若精心应用，dLLMs的特性实际上可以成为推测解码中自回归（AR）验证器草稿生成的优势。我们的核心见解是：dLLMs通过并行解码获得的速度，显著降低了代价高昂的拒绝风险，为实现推测解码中（难以捉摸的）长草稿提供了实用机制，从而带来大幅加速。我们提出了FailFast，这是一个基于dLLM的推测解码框架，通过动态调整推测长度来实现该方法。它在难以推测的区域“快速失败”，以最小计算成本缩短推测延迟；在较易区域则“大获全胜”，通过积极延长草稿长度来减少验证延迟（在许多情况下，可一次性推测并接受多达70个令牌！）。无需任何微调，FailFast即可实现AR LLMs的无损加速，相比原始解码最高达4.9倍加速，优于最佳朴素dLLM草稿生成器1.7倍，并在多种模型和工作负载上超越EAGLE-3达1.7倍。我们已在https://github.com/ruipeterpan/failfast开源FailFast。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the efficiency-quality tradeoff in diffusion large language models (dLLMs) by re-purposing them as drafters in speculative decoding with autoregressive verifiers. The proposed FailFast framework leverages dLLMs&#x27; fast parallel token generation to produce lengthy draft sequences with low rejection risk, dynamically adapting speculation length to fail quickly in difficult regions and extend aggressively in easy ones. Experimental results show that FailFast achieves lossless acceleration with up to 4.9× speedup over vanilla decoding, 1.7× over naive dLLM drafters, and 1.7× over EAGLE-3 across diverse models and workloads, often accepting 70-token drafts at once.</div>
<div class="mono" style="margin-top:8px">本研究针对扩散大语言模型（dLLM）固有的效率与质量权衡问题，提出将其重新用作推测解码框架中的草稿模型。所提出的FailFast方法利用dLLM快速并行生成令牌的能力，以最小的拒绝风险生成长草稿序列，并动态调整推测长度，在困难区域实现“快速失败”，在容易区域通过生成长度被接受的草稿实现“大获全胜”。实验结果表明，FailFast实现了无损加速，在不同模型和任务上相比原始解码速度提升最高达4.9倍，相比朴素dLLM草稿方法提升1.7倍，相比EAGLE-3基线提升1.7倍，单次推测常可接受多达70个令牌。</div>
</details>
</div>
<div class="card">
<div class="title">PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting</div>
<div class="meta-line">Authors: Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz</div>
<div class="meta-line">First: 2026-01-28T18:45:45+00:00 · Latest: 2026-01-28T18:45:45+00:00</div>
<div class="meta-line">Comments: 5 pages; 2 figures; 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20845v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20845v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PatchFormer：一种基于补丁的时间序列基础模型，采用分层掩码重建与跨领域迁移学习实现零样本多步预测</div>
<div class="mono" style="margin-top:8px">时间序列预测是气候、能源、医疗和金融等领域的基础问题。现有方法通常需要针对特定领域进行特征工程，且依赖大量标注数据。本文提出PatchFormer，一种基于补丁的时间序列基础模型，通过分层掩码重建进行自监督预训练，并利用轻量适配器实现高效迁移。该模型将时间序列分割为补丁，通过可学习的跨时间尺度聚合学习多尺度时序表示。预训练采用动态掩码的补丁重建目标，兼顾局部精度与全局一致性，并结合跨领域知识蒸馏。在涵盖天气、能源、交通、金融和医疗的24个基准数据集上的实验表明，PatchFormer在零样本多步预测任务中达到最优性能，相较于强基线平均平方误差降低27.3%，且所需任务特定训练数据减少94%。模型在千亿级预训练数据下呈现近似对数线性扩展，对长度为512的序列处理速度比全序列Transformer快3.8倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for domain-specific feature engineering and large labeled datasets in time series forecasting, this paper proposes PatchFormer, a patch-based foundation model that employs hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. The method segments time series into patches, learns multiscale temporal representations with learnable aggregation, and uses masked patch reconstruction with dynamic masking and cross-domain knowledge distillation. Experimental results on 24 benchmark datasets across weather, energy, traffic, finance, and healthcare show state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3% compared to strong baselines while requiring 94% less task-specific training data, with efficient scaling and faster processing than full-sequence transformers.</div>
<div class="mono" style="margin-top:8px">针对时间序列预测中需要领域特定特征工程和大量标注数据的问题，本文提出了PatchFormer，一种基于片段的基础模型，采用分层掩码重建进行自监督预训练，并使用轻量级适配器实现高效迁移。该方法将时间序列分割为片段，通过可学习的聚合学习多尺度时序表示，并利用动态掩码和兼顾局部精度与全局一致性的目标进行掩码片段重建，随后进行跨领域知识蒸馏。在涵盖天气、能源、交通、金融和医疗的24个基准数据集上的实验结果表明，该模型在零样本多步预测上达到了最先进水平，相比强基线平均平方误差降低了27.3%，同时所需任务特定训练数据减少了94%，在高达1000亿点的预训练数据上呈现近对数线性扩展，且处理长度为512的序列比全序列变换器快3.8倍。</div>
</details>
</div>
<div class="card">
<div class="title">$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval</div>
<div class="meta-line">Authors: Zihao Wang, Hang Yin, Lihui Liu, Hanghang Tong, Yangqiu Song, Ginny Wong, Simon See</div>
<div class="meta-line">First: 2026-01-28T18:45:43+00:00 · Latest: 2026-01-28T18:45:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20844v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20844v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of &quot;distances&quot; or &quot;similarities,&quot; including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理论上，$\mathbb{R}^{2k}$ 足以支持基于嵌入的 Top-$k$ 检索</div>
<div class="mono" style="margin-top:8px">本文研究了将子集成员关系（$m$ 个元素和最多 $k$ 个元素的 ${m\choose k}$ 个子集）嵌入向量空间所需的最小维度，称为最小可嵌入维度（MED）。我们理论上推导了 MED 的紧致界，并通过实验验证了其在多种“距离”或“相似度”度量（包括 $\ell_2$ 度量、内积和余弦相似度）下的有效性。此外，我们在一个更易实现的设定下进行了数值模拟，其中 ${m\choose k}$ 个子集的嵌入被选为所含元素嵌入的质心。模拟结果轻松实现了 MED 与待嵌入元素数量之间的对数依赖关系。这些发现表明，基于嵌入的检索限制主要源于可学习性挑战，而非几何约束，从而为未来算法设计提供了指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the minimal dimension needed to embed subset memberships into vector spaces, known as the Minimal Embeddable Dimension (MED), to understand geometric constraints in embedding-based top-k retrieval. Theoretically, tight bounds for MED are derived for various distance or similarity measures, including the ℓ₂ metric, inner product, and cosine similarity, and these are empirically validated. Numerical simulations in a centroid-based embedding setting demonstrate a logarithmic dependency between MED and the number of elements, indicating that retrieval limitations arise more from learnability issues than geometric ones, thus informing future algorithm design.</div>
<div class="mono" style="margin-top:8px">本研究探讨了将子集成员关系嵌入向量空间所需的最小维度，即最小可嵌入维度（MED），以理解基于嵌入的top-k检索的几何约束。理论上，针对ℓ₂度量、内积和余弦相似性等多种距离或相似性度量，推导了MED的紧致界，并通过实验验证。在基于质心的嵌入设置中进行数值模拟，结果表明MED与元素数量呈对数关系，这表明检索限制主要源于可学习性问题而非几何约束，从而为未来算法设计提供了指导。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Models Inherit Value Biases from Pretraining</div>
<div class="meta-line">Authors: Brian Christian, Jessica A. F. Thompson, Elle Michelle Yang, Vincent Adam, Hannah Rose Kirk, Christopher Summerfield, Tsvetomira Dumbalska</div>
<div class="meta-line">First: 2026-01-28T18:40:29+00:00 · Latest: 2026-01-28T18:40:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20838v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20838v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the &quot;Big Two&quot; psychological axes, we show a robust preference of Llama RMs for &quot;agency&quot; and a corresponding robust preference of Gemma RMs for &quot;communion.&quot; This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers&#x27; choice of base model is as much a consideration of values as of performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励模型从预训练中继承价值偏好</div>
<div class="mono" style="margin-top:8px">奖励模型（RMs）是将大语言模型（LLMs）与人类价值观对齐的核心组件，但其受关注程度低于预训练及后训练的LLMs本身。由于RMs基于LLMs初始化，它们继承了影响其行为的表征，但这种影响的性质与程度尚未被充分研究。通过对10个主流开源权重RMs使用经过验证的心理语言学语料库进行综合研究，我们发现RMs在人类价值观的多个维度上表现出显著差异，且差异与其基础模型相关。基于心理学“两大维度”理论，我们证明Llama系列RMs持续偏向“能动性”，而Gemma系列RMs则持续偏向“共生性”。即使在使用相同偏好数据与微调流程的情况下，该现象依然存在，我们将其溯源至对应指令微调模型与预训练模型的逻辑输出差异。这些对数概率差异本身可构建为隐式奖励模型；我们推导出可用的隐式奖励分数，并证明其呈现完全相同的能动性/共生性差异。通过控制偏好数据来源与数量的消融实验，我们发现该效应不仅可复现且具有惊人的持久性。尽管RMs被设计为代表人类偏好，但证据表明其输出仍受底层预训练LLMs的影响。本研究强调了预训练阶段安全对齐工作的重要性，并明确指出开源开发者选择基础模型时，需同等考量价值取向与性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how reward models (RMs), which are crucial for aligning large language models with human values, inherit value biases from their underlying pre-trained language models. The researchers conducted a comprehensive analysis of 10 leading open-weight RMs using validated psycholinguistic corpora, specifically measuring preferences along the &quot;Big Two&quot; psychological axes of agency and communion. The key experimental findings reveal robust and systematic biases: RMs based on Llama models consistently prefer agency, while those based on Gemma models prefer communion, even when trained on identical preference data. This effect is traced back to log-probability differences in the base models and is shown to be durable across ablations of training data source and quantity, highlighting that pretraining choices fundamentally shape RM values.</div>
<div class="mono" style="margin-top:8px">本研究探讨了用于对齐大语言模型与人类偏好的奖励模型如何从其底层预训练语言模型中继承价值偏见。研究人员使用心理语言学语料库对10个开源权重奖励模型进行了全面分析，发现基础模型显著影响了奖励模型的价值判断；具体而言，基于Llama的奖励模型表现出对&#x27;能动性&#x27;的强烈偏好，而基于Gemma的模型则倾向于&#x27;共生性&#x27;，即使在用相同偏好数据训练的情况下也是如此。关键实验结果表明，这些偏见源于预训练或指令微调基础模型的逻辑值，可被表述为表现出相同价值差异的隐式奖励分数，并且在训练数据来源和数量的消融实验中表现出惊人的持久性，这凸显了预训练选择将价值观固有地嵌入到了对齐系统中。</div>
</details>
</div>
<div class="card">
<div class="title">Discrete Variational Autoencoding via Policy Search</div>
<div class="meta-line">Authors: Michael Drolet, Firas Al-Hafez, Aditya Bhatt, Jan Peters, Oleg Arenz</div>
<div class="meta-line">First: 2025-09-29T12:44:05+00:00 · Latest: 2026-01-28T18:33:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24716v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.24716v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略搜索的离散变分自编码方法</div>
<div class="mono" style="margin-top:8px">变分自编码器（VAE）中的离散潜在瓶颈具有高比特效率，可通过自回归离散分布建模，实现基于Transformer的参数高效多模态搜索。然而，离散随机变量无法进行精确可微参数化；因此离散VAE通常依赖近似方法，如Gumbel-Softmax重参数化或直通梯度估计，或采用高方差的无梯度方法（如REINFORCE），这些方法在图像重建等高维任务中成效有限。受策略搜索常用技术启发，我们提出一种离散VAE训练框架，利用非参数编码器的自然梯度更新参数编码器，无需重参数化。该方法结合自动步长适应和基于Transformer的编码器，可扩展至ImageNet等挑战性数据集，在从紧凑潜在空间重建高维数据方面，性能优于近似重参数化方法和基于量化的离散自编码器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Discrete latent bottlenecks in variational autoencoders offer efficiency and multimodal modeling but are difficult to train due to non-differentiability, with existing methods like Gumbel-Softmax or REINFORCE facing approximation errors or high variance. To address this, the authors propose a training framework inspired by policy search that uses the natural gradient of a non-parametric encoder to update a parametric encoder, avoiding reparameterization. When combined with automatic step size adaptation and a transformer-based encoder, the method scales to ImageNet and outperforms both approximate reparameterization techniques and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.</div>
<div class="mono" style="margin-top:8px">变分自编码器中的离散潜在瓶颈具有高比特效率并支持多模态搜索，但由于离散变量的不可微性，其训练面临挑战，现有方法如Gumbel-Softmax或REINFORCE通常存在近似或高方差问题。为此，作者受策略搜索技术启发，提出一种训练框架，利用非参数编码器的自然梯度来更新参数编码器，避免了重参数化，并结合自动步长适应和基于Transformer的编码器。在ImageNet等数据集上的实验表明，该方法在从紧凑潜在空间重建高维数据方面，优于近似重参数化方法和基于量化的离散自编码器。</div>
</details>
</div>
<div class="card">
<div class="title">Linear representations in language models can change dramatically over a conversation</div>
<div class="meta-line">Authors: Andrew Kyle Lampinen, Yuxuan Li, Eghbal Hosseini, Sangnie Bhardwaj, Murray Shanahan</div>
<div class="meta-line">First: 2026-01-28T18:33:17+00:00 · Latest: 2026-01-28T18:33:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20834v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20834v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型中的线性表征在对话过程中会发生显著变化</div>
<div class="mono" style="margin-top:8px">语言模型的表征常包含对应高层概念的线性方向。本研究探讨了这些表征在（模拟）对话语境中沿特定维度的动态演变。研究发现，线性表征在对话过程中会发生显著变化：例如，对话初期表征为事实的信息可能在对话末期被表征为非事实，反之亦然。这种变化具有内容依赖性：对话相关信息可能改变，而通用信息通常保持稳定。即使对于将事实性与表层响应模式分离的维度，这种变化依然稳健，且在不同模型家族和模型层中均有出现。表征变化无需依赖在线策略对话；即使重播由完全不同的模型编写的对话脚本也能产生类似变化。然而，若语境仅为明确标注的科幻故事，适应效应则显著减弱。研究还表明，沿表征方向进行引导在对话不同阶段可能产生截然不同的效果。这些结果与&#x27;模型根据对话提示扮演特定角色时表征随之演变&#x27;的观点一致。本研究对可解释性与引导技术提出挑战：静态的特征/方向解释或假设特定特征始终对应特定真实值的探测方法可能产生误导。但此类表征动态也为理解模型如何适应语境开辟了新的研究方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how linear representations of concepts in language models evolve during conversations, motivated by the need to understand the dynamic nature of model interpretability and steering. The method involves analyzing how linear directions corresponding to high-level concepts, such as factuality, change across simulated conversational contexts, including replaying conversation scripts from different models. Key experimental findings reveal that these linear representations can shift dramatically within a conversation, with factual information becoming non-factual and vice versa, depending on the conversation&#x27;s content; these changes are robust across model families and layers, occur even with off-policy script replay, and show that steering along a representational direction yields varying effects at different conversation points, challenging static interpretations of model features.</div>
<div class="mono" style="margin-top:8px">本研究探讨了语言模型中概念的线性表示在对话过程中如何演变，其动机在于静态解释模型特征可能产生误导。方法包括分析在模拟对话语境中，特定线性维度上的表示如何变化，例如重播来自不同模型的对话脚本。主要实验结果表明，事实性信息的表示会随对话内容发生显著变化（例如从事实性变为非事实性），这些变化在不同模型家族和层中均稳健，但当语境被明确框定为虚构时，适应性较弱。这些发现表明模型会根据对话角色调整表示，这对依赖静态特征含义的可解释性和导向技术提出了挑战。</div>
</details>
</div>
<div class="card">
<div class="title">VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring</div>
<div class="meta-line">Authors: Waldyn G. Martinez</div>
<div class="meta-line">First: 2026-01-28T18:30:48+00:00 · Latest: 2026-01-28T18:30:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20830v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20830v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings. VSCOUT combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) architecture with ensemble-based latent outlier filtering and changepoint detection. The ARD prior isolates the most informative latent dimensions, while the ensemble and changepoint filters identify pointwise and structural contamination within the determined latent space. A second-stage retraining step removes flagged observations and re-estimates the latent structure using only the retained inliers, mitigating masking and stabilizing the IC latent manifold. This two-stage refinement produces a clean and reliable IC baseline suitable for subsequent Phase II deployment. Extensive experiments across benchmark datasets demonstrate that VSCOUT achieves superior sensitivity to special-cause structure while maintaining controlled false alarms, outperforming classical SPC procedures, robust estimators, and modern machine-learning baselines. Its scalability, distributional flexibility, and resilience to complex contamination patterns position VSCOUT as a practical and effective method for retrospective modeling and anomaly detection in AI-enabled environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VSCOUT：一种用于高维回顾性监测中异常值检测的混合变分自编码器方法</div>
<div class="mono" style="margin-top:8px">现代工业和服务流程产生的高维、非高斯且易受污染的数据，对经典统计过程控制（SPC）的基本假设构成挑战。重尾分布、多模态性、非线性依赖关系以及稀疏的特殊原因观测可能扭曲基线估计、掩盖真实异常，并阻碍可靠识别受控（IC）参考集。为解决这些问题，我们提出VSCOUT——一个专为高维环境下的回顾性（第一阶段）监测设计的无分布框架。VSCOUT将自动相关性确定变分自编码器（ARD-VAE）架构与基于集成的潜在异常值过滤及变点检测相结合。ARD先验分离出信息量最大的潜在维度，而集成与变点过滤器则在确定的潜在空间内识别逐点污染和结构性污染。第二阶段通过重训练步骤移除标记观测，并仅使用保留的正常样本重新估计潜在结构，从而减轻掩盖效应并稳定IC潜在流形。这种两阶段优化产生适用于后续第二阶段部署的洁净可靠IC基线。在多个基准数据集上的广泛实验表明，VSCOUT在保持可控误报率的同时，对特殊原因结构具有卓越的检测灵敏度，其性能超越经典SPC流程、鲁棒估计器及现代机器学习基线。其可扩展性、分布灵活性以及对复杂污染模式的鲁棒性，使VSCOUT成为AI赋能环境中回顾性建模与异常检测的实用高效方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of classical Statistical Process Control (SPC) in handling modern high-dimensional, non-Gaussian, and contaminated data, where issues like heavy tails and sparse anomalies can distort baseline estimation. The proposed method, VSCOUT, is a distribution-free framework for retrospective monitoring that combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) with ensemble-based latent outlier filtering and changepoint detection to isolate informative dimensions and identify contamination, followed by a second-stage retraining step to refine the in-control baseline. Experimental results on benchmark datasets show that VSCOUT achieves superior sensitivity to anomalies with controlled false alarms, outperforming classical SPC, robust estimators, and modern machine-learning baselines.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于经典统计过程控制（SPC）在处理现代高维、非高斯和受污染数据时存在局限，其中重尾分布和稀疏异常等问题会扭曲基线估计。所提出的方法VSCOUT是一种用于回顾性监控的无分布框架，它结合了自动相关性确定变分自编码器（ARD-VAE）与基于集成的潜在离群值过滤和变点检测，以隔离信息维度并识别污染，随后通过第二阶段的重训练步骤来细化受控基线。在基准数据集上的实验结果表明，VSCOUT在控制误报的同时实现了对异常的更高灵敏度，其性能优于经典SPC、鲁棒估计器和现代机器学习基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Online Conformal Model Selection for Nonstationary Time Series</div>
<div class="meta-line">Authors: Shibo Li, Yao Zheng</div>
<div class="meta-line">First: 2025-06-05T19:45:52+00:00 · Latest: 2026-01-28T18:29:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05544v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.05544v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces the MPS (Model Prediction Set), a novel framework for online model selection for nonstationary time series. Classical model selection methods, such as information criteria and cross-validation, rely heavily on the stationarity assumption and often fail in dynamic environments which undergo gradual or abrupt changes over time. Yet real-world data are rarely stationary, and model selection under nonstationarity remains a largely open problem. To tackle this challenge, we combine conformal inference with model confidence sets to develop a procedure that adaptively selects models best suited to the evolving dynamics at any given time. Concretely, the MPS updates in real time a confidence set of candidate models that covers the best model for the next time period with a specified long-run probability, while adapting to nonstationarity of unknown forms. Through simulations and real-world data analysis, we demonstrate that MPS reliably and efficiently identifies optimal models under nonstationarity, an essential capability lacking in offline methods. Moreover, MPS frequently produces high-quality sets with small cardinality, whose evolution offers deeper insights into changing dynamics. As a generic framework, MPS accommodates any data-generating process, data structure, model class, training method, and evaluation metric, making it broadly applicable across diverse problem settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非平稳时间序列的在线共形模型选择</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为MPS（模型预测集）的新框架，用于非平稳时间序列的在线模型选择。传统模型选择方法（如信息准则和交叉验证）严重依赖平稳性假设，在随时间发生渐变或突变动态变化的环境中常失效。然而现实数据极少平稳，非平稳下的模型选择仍是一个未解决的难题。为应对此挑战，我们将共形推断与模型置信集结合，开发了一种能自适应选择最适合当前演化动态模型的方法。具体而言，MPS实时更新候选模型的置信集，以指定长期概率覆盖下一时间段的最佳模型，同时适应未知形式的非平稳性。通过仿真和实际数据分析，我们证明MPS能在非平稳条件下可靠高效地识别最优模型，这是离线方法所缺乏的关键能力。此外，MPS常生成基数较小的高质量集合，其演化过程为动态变化提供了更深入的洞察。作为一个通用框架，MPS兼容任意数据生成过程、数据结构、模型类别、训练方法和评估指标，使其广泛适用于多样化的问题场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the failure of classical model selection methods under nonstationarity, this paper introduces the Model Prediction Set (MPS) framework for online model selection in nonstationary time series. The method combines conformal inference with model confidence sets to adaptively maintain a real-time confidence set of candidate models that covers the best model for the next time period with a specified long-run probability, accommodating unknown forms of nonstationarity. Experimental results on simulations and real-world data show that MPS reliably identifies optimal models under nonstationarity, often producing small, high-quality sets that provide insights into changing dynamics.</div>
<div class="mono" style="margin-top:8px">针对经典模型选择方法在非平稳性下失效的问题，本文提出了模型预测集（MPS）框架，用于非平稳时间序列的在线模型选择。该方法将共形推理与模型置信集相结合，自适应地维护一个实时更新的候选模型置信集，该集合以指定的长期概率覆盖下一时间段的最佳模型，并能处理未知形式的非平稳性。在模拟和真实数据上的实验结果表明，MPS能够在非平稳条件下可靠地识别最优模型，且通常能产生小而高质量的集合，从而为动态变化提供更深入的洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning</div>
<div class="meta-line">Authors: Minwu Kim, Safal Shrestha, Keith Ross</div>
<div class="meta-line">First: 2026-01-28T18:29:21+00:00 · Latest: 2026-01-28T18:29:21+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model&#x27;s robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于失败前缀条件化的饱和问题推理模型训练</div>
<div class="mono" style="margin-top:8px">可验证奖励强化学习（RLVR）显著提升了大型语言模型（LLM）的推理能力，但随着问题趋于饱和，训练常陷入停滞。我们发现核心挑战在于信息性失败的可及性差：学习信号存在，但在标准推演中极少出现。为此，我们提出失败前缀条件化——一种从饱和问题中学习的简洁高效方法。该方法不再从原始问题出发，而是通过将训练条件化于罕见错误推理轨迹衍生的前缀，重新分配探索范围，使模型暴露于易失败状态。实验表明，失败前缀条件化带来的性能提升与中等难度问题训练相当，同时保持标记效率。进一步分析模型鲁棒性发现，该方法能降低误导性失败前缀下的性能衰减，仅需以轻微牺牲对早期正确推理的遵循度为代价。最后，我们证明在训练中迭代更新失败前缀的策略，能在性能平台期后实现额外增益。总体而言，本研究结果表明失败前缀条件化为延续饱和问题的RLVR训练提供了有效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) improves large language model reasoning but struggles when problems become saturated, as informative failure signals are rarely encountered. To address this, the authors propose failure-prefix conditioning, which reallocates exploration by training models on prefixes derived from rare incorrect reasoning trajectories, exposing them to failure-prone states. Experiments show this method yields performance gains comparable to training on medium-difficulty problems while maintaining token efficiency, reduces performance degradation under misleading prefixes, and an iterative approach with refreshed prefixes unlocks further gains after plateaus.</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）能提升大语言模型的推理能力，但在问题趋于饱和时训练常陷入停滞，核心挑战在于标准训练中难以接触到有信息量的失败信号。为此，研究者提出失败前缀条件化方法，通过将训练过程条件化于从罕见错误推理轨迹中提取的前缀，使模型暴露于易失败状态。实验结果表明，该方法在保持标记效率的同时，取得了与中等难度问题训练相当的性能提升，降低了模型在误导性失败前缀下的性能衰减，且采用迭代更新失败前缀的版本能在性能平台期后实现额外增益。</div>
</details>
</div>
<div class="card">
<div class="title">In-Context Bias Propagation in LLM-Based Tabular Data Generation</div>
<div class="meta-line">Authors: Pol G. Recasens, Alberto Gutierrez, Jordi Torres, Josep. Ll Berral, Javier Carnerero-Cano, Anisa Halimi, Kieran Fraser</div>
<div class="meta-line">First: 2025-06-11T11:39:29+00:00 · Latest: 2026-01-28T18:25:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09630v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.09630v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly used for synthetic tabular data generation through in-context learning (ICL), offering a practical solution for data augmentation in data scarce scenarios. While prior work has shown the potential of LLMs to improve downstream task performance through augmenting underrepresented groups, these benefits often assume access to a subset of unbiased in-context examples, representative of the real dataset. In real-world settings, however, data is frequently noisy and demographically skewed. In this paper, we systematically study how statistical biases within in-context examples propagate to the distribution of synthetic tabular data, showing that even mild in-context biases lead to global statistical distortions. We further introduce an adversarial scenario where a malicious contributor can inject bias into the synthetic dataset via a subset of in-context examples, ultimately compromising the fairness of downstream classifiers for a targeted and protected subgroup. Finally, we evaluate mitigation strategies based on preprocessing in-context examples, demonstrating that while such interventions can attenuate disparity, the inherent sensitivity of LLMs to adversarial prompts remains a persistent challenge. Our findings highlight a critical new vulnerability in LLM-based data generation pipelines within sensitive domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的表格数据生成中的上下文偏见传播</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正日益通过上下文学习（ICL）被用于生成合成表格数据，为数据稀缺场景下的数据增强提供了实用解决方案。尽管先前研究已展示LLMs通过增强代表性不足群体来提升下游任务性能的潜力，但这些优势通常假设能获取一组代表真实数据集的无偏上下文示例。然而在现实场景中，数据往往存在噪声且人口统计学分布失衡。本文系统研究了上下文示例中的统计偏见如何传播至合成表格数据的分布，证明即使轻微的上下文偏见也会导致全局统计失真。我们进一步提出一种对抗场景：恶意贡献者可通过部分上下文示例向合成数据集注入偏见，最终损害针对特定受保护子群体的下游分类器的公平性。最后，我们评估了基于上下文示例预处理的缓解策略，表明虽然此类干预能减轻差异，但LLMs对对抗性提示的固有敏感性仍是持续存在的挑战。我们的研究结果揭示了敏感领域中基于LLM的数据生成流程存在新的关键脆弱性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the vulnerability of large language models (LLMs) used for synthetic tabular data generation via in-context learning, motivated by the fact that real-world data is often noisy and demographically skewed, contrary to the common assumption of unbiased examples. The method involves a systematic study of how statistical biases in the provided in-context examples propagate to the generated synthetic data, including an adversarial scenario where bias is intentionally injected. Key experimental findings show that even mild in-context biases cause global statistical distortions in the synthetic data and can compromise downstream classifier fairness for protected subgroups, while preprocessing mitigation strategies only partially attenuate these disparities due to the models&#x27; persistent sensitivity to adversarial prompts.</div>
<div class="mono" style="margin-top:8px">本研究调查了基于大语言模型（LLM）通过上下文学习生成合成表格数据时统计偏差的传播问题，其动机在于现实世界中数据常存在噪声和人口统计倾斜，这与通常假设存在无偏上下文示例的情况相矛盾。研究方法包括系统性地分析上下文示例中的偏差如何扭曲生成数据的整体统计分布，并引入一种对抗性场景，即恶意贡献者通过一部分示例注入偏差以损害下游分类器对特定受保护子群的公平性。主要实验结果表明，即使轻微的上下文偏差也会导致显著的统计失真，虽然基于预处理的缓解策略可以减弱差异，但LLM对对抗性提示的固有敏感性仍然是一个持续挑战，这揭示了敏感领域数据生成流程中的一个关键脆弱性。</div>
</details>
</div>
<div class="card">
<div class="title">Demystifying Prediction Powered Inference</div>
<div class="meta-line">Authors: Yilin Song, Dan M. Kluger, Harsh Parikh, Tian Gu</div>
<div class="meta-line">First: 2026-01-28T18:16:02+00:00 · Latest: 2026-01-28T18:16:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20819v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from large unlabeled datasets to improve statistical efficiency while maintaining valid inference through explicit bias correction using a smaller labeled subset. Despite its potential, the growing PPI variants and the subtle distinctions between them have made it challenging for practitioners to determine when and how to apply these methods responsibly. This paper demystifies PPI by synthesizing its theoretical foundations, methodological extensions, connections to existing statistics literature, and diagnostic tools into a unified practical workflow. Using the Mosaiks housing price data, we show that PPI variants produce tighter confidence intervals than complete-case analysis, but that double-dipping, i.e. reusing training data for inference, leads to anti-conservative confidence intervals and coverages. Under missing-not-at-random mechanisms, all methods, including classical inference using only labeled data, yield biased estimates. We provide a decision flowchart linking assumption violations to appropriate PPI variants, a summary table of selective methods, and practical diagnostic strategies for evaluating core assumptions. By framing PPI as a general recipe rather than a single estimator, this work bridges methodological innovation and applied practice, helping researchers responsibly integrate predictions into valid inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭秘预测驱动推断</div>
<div class="mono" style="margin-top:8px">机器学习预测在生物医学研究、环境科学和社会科学等领域中，正日益被用于补充不完整或测量成本高昂的结果。然而，将预测视为真实值会引入偏差，而忽略它们则会浪费宝贵信息。预测驱动推断（PPI）提供了一个原则性框架，利用来自大型未标记数据集的预测来提高统计效率，同时通过使用较小标记子集进行显式偏差校正来保持有效推断。尽管PPI具有潜力，但其不断增长的变体及它们之间的细微差别使得实践者难以确定何时以及如何负责任地应用这些方法。本文通过将PPI的理论基础、方法扩展、与现有统计文献的联系以及诊断工具综合成一个统一的实践工作流程，来揭秘PPI。使用Mosaiks房价数据，我们展示了PPI变体比完整案例分析产生更紧密的置信区间，但双重利用（即重复使用训练数据进行推断）会导致非保守的置信区间和覆盖率。在非随机缺失机制下，所有方法（包括仅使用标记数据的经典推断）都会产生有偏估计。我们提供了一个决策流程图，将假设违反与适当的PPI变体联系起来，一份选择性方法的汇总表，以及用于评估核心假设的实用诊断策略。通过将PPI构建为一个通用方案而非单一估计器，这项工作在方法创新与应用实践之间架起桥梁，帮助研究人员负责任地将预测整合到有效推断中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to leverage machine learning predictions for statistical inference without introducing bias or wasting information, this paper synthesizes Prediction-Powered Inference (PPI) into a unified practical workflow. The method corrects prediction bias using a small labeled dataset to enable valid inference from large unlabeled data, with extensions and diagnostics for various scenarios. Key experimental findings using housing price data show that PPI variants yield tighter confidence intervals than complete-case analysis, but reusing training data for inference leads to anti-conservative intervals, and all methods, including classical approaches, produce biased estimates under missing-not-at-random mechanisms.</div>
<div class="mono" style="margin-top:8px">该研究针对生物医学和社会科学等领域中利用机器学习预测补充不完整结果时面临的挑战：将预测视为真实值会引入偏差，而忽略预测则会浪费信息。本文通过整合预测驱动推断的理论基础、方法扩展和诊断工具，将其统一为实用工作流程，利用小规模标注子集校正偏差，同时从大规模未标注数据集的预测中获益。在Mosaiks房价数据上的实验表明，PPI变体能产生比完整案例分析更紧的置信区间，但数据重复使用会导致区间反保守，且在非随机缺失机制下所有方法均产生有偏估计，为此作者提供了决策流程图和诊断策略以指导负责任的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Sharpness of Minima in Deep Matrix Factorization</div>
<div class="meta-line">Authors: Anil Kamber, Rahul Parhi</div>
<div class="meta-line">First: 2025-09-30T04:50:28+00:00 · Latest: 2026-01-28T18:09:28+00:00</div>
<div class="meta-line">Comments: 18 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25783v5">Abs</a> · <a href="https://arxiv.org/pdf/2509.25783v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding the geometry of the loss landscape near a minimum is key to explaining the implicit bias of gradient-based methods in non-convex optimization problems such as deep neural network training and deep matrix factorization. A central quantity to characterize this geometry is the maximum eigenvalue of the Hessian of the loss. Currently, its precise role has been obfuscated because no exact expressions for this sharpness measure were known in general settings. In this paper, we present the first exact expression for the maximum eigenvalue of the Hessian of the squared-error loss at any minimizer in deep matrix factorization/deep linear neural network training problems, resolving an open question posed by Mulayoff &amp; Michaeli (2020). This expression reveals a fundamental property of the loss landscape in deep matrix factorization: Having a constant product of the spectral norms of the left and right intermediate factors across layers is a sufficient condition for flatness. Most notably, in both depth-$2$ matrix and deep overparameterized scalar factorization, we show that this condition is both necessary and sufficient for flatness, which implies that flat minima are spectral-norm balanced even though they are not necessarily Frobenius-norm balanced. To complement our theory, we provide the first empirical characterization of an escape phenomenon during gradient-based training near a minimizer of a deep matrix factorization problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度矩阵分解中极小值的锐度</div>
<div class="mono" style="margin-top:8px">理解损失函数在极小值附近的几何结构，对于解释梯度方法在非凸优化问题（如深度神经网络训练和深度矩阵分解）中的隐式偏差至关重要。表征该几何结构的一个核心量是损失函数Hessian矩阵的最大特征值。目前，由于该锐度度量在一般设置下缺乏精确表达式，其确切作用尚不明确。本文首次给出了深度矩阵分解/深度线性神经网络训练问题中，任意极小值点处平方误差损失Hessian矩阵最大特征值的精确表达式，解决了Mulayoff &amp; Michaeli (2020)提出的一个开放性问题。该表达式揭示了深度矩阵分解损失函数的一个基本性质：各层左右中间因子的谱范数乘积保持恒定是平坦性的充分条件。特别值得注意的是，在深度为2的矩阵分解和深度超参数化标量分解中，我们证明该条件同时是平坦性的充要条件，这意味着平坦极小值点具有谱范数平衡性，而不一定具有Frobenius范数平衡性。作为理论补充，我们首次实证描述了深度矩阵分解问题在极小值点附近基于梯度训练时的逃逸现象。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to clarify the role of loss landscape geometry, specifically the sharpness measured by the maximum Hessian eigenvalue, in explaining the implicit bias of gradient-based optimization for non-convex problems like deep matrix factorization. The authors derive the first exact expression for this sharpness measure at any minimizer, revealing that a constant product of the spectral norms of intermediate factors across layers is a sufficient condition for flat minima. Key experimental findings show that in depth-2 and deep overparameterized scalar factorization, this condition is also necessary, establishing that flat minima are spectral-norm balanced, and they empirically characterize an escape phenomenon during training near a minimizer.</div>
<div class="mono" style="margin-top:8px">本研究旨在阐明Hessian锐度在解释深度矩阵分解等非凸问题中梯度优化隐式偏置的作用。方法上首次推导出任意极小值点处Hessian矩阵最大特征值的精确表达式，揭示了各层谱范数乘积为常数是平坦性的充分条件。关键实验结果表明，在深度为2的矩阵分解和深度过参数化标量分解中，该条件也是必要的，说明平坦极小值是谱范数平衡的，并且首次对极小值点附近的逃逸现象进行了实证表征。</div>
</details>
</div>
<div class="card">
<div class="title">ProToken: Token-Level Attribution for Federated Large Language Models</div>
<div class="meta-line">Authors: Waris Gill, Ahmad Humayun, Ali Anwar, Muhammad Ali Gulzar</div>
<div class="meta-line">First: 2026-01-27T14:53:12+00:00 · Latest: 2026-01-28T18:05:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19672v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19672v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) across distributed data sources while preserving privacy. However, when federated LLMs are deployed in critical applications, it remains unclear which client(s) contributed to specific generated responses, hindering debugging, malicious client identification, fair reward allocation, and trust verification. We present ProToken, a novel Provenance methodology for Token-level attribution in federated LLMs that addresses client attribution during autoregressive text generation while maintaining FL privacy constraints. ProToken leverages two key insights to enable provenance at each token: (1) transformer architectures concentrate task-specific signals in later blocks, enabling strategic layer selection for computational tractability, and (2) gradient-based relevance weighting filters out irrelevant neural activations, focusing attribution on neurons that directly influence token generation. We evaluate ProToken across 16 configurations spanning four LLM architectures (Gemma, Llama, Qwen, SmolLM) and four domains (medical, financial, mathematical, coding). ProToken achieves 98% average attribution accuracy in correctly localizing responsible client(s), and maintains high accuracy when the number of clients are scaled, validating its practical viability for real-world deployment settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProToken：联邦大语言模型的词元级贡献溯源方法</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）支持在分布式数据源上协同训练大语言模型（LLM）并保护隐私。然而，当联邦LLM部署于关键应用时，特定生成响应由哪些客户端贡献仍不明确，这阻碍了调试、恶意客户端识别、公平奖励分配和信任验证。本文提出ProToken——一种面向联邦LLM词元级贡献溯源的新型溯源方法，在自回归文本生成过程中实现客户端贡献归因，同时保持联邦学习的隐私约束。ProToken基于两个关键洞见实现词元级溯源：（1）Transformer架构将任务相关信号集中于深层模块，可通过策略性层选择实现计算可行性；（2）基于梯度的相关性加权能过滤无关神经激活，使归因聚焦于直接影响词元生成的神经元。我们在涵盖四种LLM架构（Gemma、Llama、Qwen、SmolLM）和四个领域（医疗、金融、数学、编程）的16种配置中评估ProToken。该方法在准确定位责任客户端方面达到98%的平均归因准确率，并在客户端数量扩展时保持高精度，验证了其在实际部署场景中的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Federated Learning (FL) enables collaborative training of Large Language Models (LLMs) while preserving data privacy, but attributing specific generated tokens to contributing clients remains a challenge for debugging, fairness, and trust. To address this, ProToken introduces a token-level provenance method that operates within FL privacy constraints by strategically selecting later transformer layers where task-specific signals are concentrated and applying gradient-based relevance weighting to filter neural activations, thereby focusing attribution on neurons directly influencing token generation. Experimental evaluation across 16 configurations involving four LLM architectures and four domains demonstrates that ProToken achieves an average attribution accuracy of 98% in correctly identifying responsible clients and maintains high accuracy as the number of clients scales, confirming its practical viability.</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）能够在保护数据隐私的前提下协同训练大语言模型（LLM），但将生成的特定文本归因于贡献客户端仍然是一个挑战，这阻碍了调试、恶意客户端识别和公平奖励分配。为此，ProToken提出了一种令牌级别的溯源方法，该方法在FL隐私约束下运行，其核心是策略性选择任务信号集中的深层Transformer模块，并应用基于梯度的相关性加权来过滤不相关的神经激活。在涵盖四种LLM架构和四个领域的16种配置上的实验评估表明，ProToken在正确识别责任客户端方面达到了98%的平均归因准确率，并且在客户端数量增加时仍保持高准确率，验证了其实际部署的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">GNN Explanations that do not Explain and How to find Them</div>
<div class="meta-line">Authors: Steve Azzolin, Stefano Teso, Bruno Lepri, Andrea Passerini, Sagar Malhotra</div>
<div class="meta-line">First: 2026-01-28T18:05:17+00:00 · Latest: 2026-01-28T18:05:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20815v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20815v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Explanations provided by Self-explainable Graph Neural Networks (SE-GNNs) are fundamental for understanding the model&#x27;s inner workings and for identifying potential misuse of sensitive attributes. Although recent works have highlighted that these explanations can be suboptimal and potentially misleading, a characterization of their failure cases is unavailable. In this work, we identify a critical failure of SE-GNN explanations: explanations can be unambiguously unrelated to how the SE-GNNs infer labels. We show that, on the one hand, many SE-GNNs can achieve optimal true risk while producing these degenerate explanations, and on the other, most faithfulness metrics can fail to identify these failure modes. Our empirical analysis reveals that degenerate explanations can be maliciously planted (allowing an attacker to hide the use of sensitive attributes) and can also emerge naturally, highlighting the need for reliable auditing. To address this, we introduce a novel faithfulness metric that reliably marks degenerate explanations as unfaithful, in both malicious and natural settings. Our code is available in the supplemental.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无法解释的图神经网络解释及其发现方法</div>
<div class="mono" style="margin-top:8px">自解释图神经网络（SE-GNNs）提供的解释对于理解模型内部机制及识别敏感属性的潜在误用至关重要。尽管近期研究指出这些解释可能并非最优且具有误导性，但其失效案例的特征尚未明确。本研究揭示了SE-GNN解释的一个关键缺陷：解释可能与SE-GNN推理标签的方式完全无关。我们证明，一方面许多SE-GNN在产生这些退化解释时仍能达到最优真实风险；另一方面，多数忠实性度量方法无法识别这些失效模式。实证分析表明，退化解释既可能被恶意植入（使攻击者得以隐藏敏感属性的使用），也可能自然产生，这凸显了可靠审计的必要性。为此，我们提出一种新颖的忠实性度量方法，能在恶意与自然场景下均可靠地将退化解释标记为不忠实。代码已附于补充材料中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the critical issue that self-explainable graph neural networks (SE-GNNs) can produce explanations that are fundamentally unrelated to the model&#x27;s actual reasoning process, which is problematic for understanding model behavior and detecting misuse of sensitive attributes. The method involves characterizing this failure mode, demonstrating that SE-GNNs can achieve optimal accuracy while generating such degenerate explanations, and showing that existing faithfulness metrics often fail to detect them. Key experimental findings reveal that these misleading explanations can arise both from malicious manipulation to conceal sensitive attribute usage and naturally during training, prompting the development of a novel faithfulness metric that reliably identifies degenerate explanations in both scenarios.</div>
<div class="mono" style="margin-top:8px">本研究解决了自解释图神经网络（SE-GNNs）可能产生与模型实际推理过程根本无关的解释这一关键问题，这损害了其用于模型审计和理解的可信度。方法包括描述这种失败模式，证明SE-GNNs在生成此类退化解释的同时仍能实现最优预测性能，并表明现有的忠实性度量标准往往无法检测它们。关键实验发现揭示，这些误导性解释既可能恶意产生以隐藏敏感属性的使用，也可能自然出现，这凸显了改进审计的必要性；作者还提出了一种新的忠实性度量标准，能有效识别这两种情况下的退化解释。</div>
</details>
</div>
<div class="card">
<div class="title">Context-Augmented Code Generation Using Programming Knowledge Graphs</div>
<div class="meta-line">Authors: Shahd Seddik, Fahd Seddik, Iman Saberi, Fatemeh Fard, Minh Hieu Huynh, Patanamon Thongtanunam</div>
<div class="meta-line">First: 2026-01-28T17:58:30+00:00 · Latest: 2026-01-28T17:58:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20810v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20810v1">PDF</a> · <a href="https://github.com/iamshahd/ProgrammingKnowledgeGraph">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) excel at code generation but struggle with complex problems. Retrieval-Augmented Generation (RAG) mitigates this issue by integrating external knowledge, yet retrieval models often miss relevant context, and generation models hallucinate with irrelevant data. We propose Programming Knowledge Graph (PKG) for semantic representation and fine-grained retrieval of code and text. Our approach enhances retrieval precision through tree pruning and mitigates hallucinations via a re-ranking mechanism that integrates non-RAG solutions. Structuring external data into finer-grained nodes improves retrieval granularity. Evaluations on HumanEval and MBPP show up to 20% pass@1 accuracy gains and a 34% improvement over baselines on MBPP. Our findings demonstrate that our proposed PKG approach along with re-ranker effectively address complex problems while maintaining minimal negative impact on solutions that are already correct without RAG. The replication package is published at https://github.com/iamshahd/ProgrammingKnowledgeGraph</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于编程知识图谱的上下文增强代码生成</div>
<div class="mono" style="margin-top:8px">大语言模型在代码生成方面表现出色，但在处理复杂问题时仍面临挑战。检索增强生成通过整合外部知识缓解了这一问题，但检索模型常遗漏相关上下文，生成模型则易受无关数据干扰产生幻觉。我们提出编程知识图谱，用于代码与文本的语义表示和细粒度检索。该方法通过树剪枝提升检索精度，并采用融合非RAG解决方案的重排序机制抑制幻觉。将外部数据组织为更细粒度的节点提高了检索粒度。在HumanEval和MBPP上的评估显示，pass@1准确率最高提升20%，在MBPP上较基线提升34%。结果表明，所提出的PKG方法结合重排序器能有效处理复杂问题，同时对无需RAG已正确的解决方案产生最小负面影响。复现资源已发布于https://github.com/iamshahd/ProgrammingKnowledgeGraph</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of Large Language Models in complex code generation, where Retrieval-Augmented Generation often retrieves irrelevant context and causes hallucinations. The proposed method constructs a Programming Knowledge Graph for semantic representation and fine-grained retrieval, employing tree pruning for precision and a re-ranking mechanism to integrate non-RAG solutions. Experimental results on HumanEval and MBPP benchmarks demonstrate up to 20% pass@1 accuracy improvement and a 34% gain over baselines on MBPP, effectively solving complex problems while minimizing negative impact on already-correct solutions.</div>
<div class="mono" style="margin-top:8px">针对大语言模型在生成复杂问题代码时的局限性，以及检索增强生成方法常检索到无关上下文并导致幻觉的问题，本研究提出了一种用于代码和文本语义表示与细粒度检索的编程知识图谱（PKG）。该方法通过树剪枝提高检索精度，并引入重排序机制整合非RAG解决方案，通过将外部数据结构化为更细粒度的节点来减少幻觉。在HumanEval和MBPP基准上的实验评估显示，该方法取得了显著提升，包括pass@1准确率最高提升20%，在MBPP上比基线模型提高34%，表明PKG方法能有效处理复杂问题，且对原本无需RAG即正确的解决方案产生的负面影响最小。</div>
</details>
</div>
<div class="card">
<div class="title">Spiking Brain Compression: Post-Training Second-order Compression for Spiking Neural Networks</div>
<div class="meta-line">Authors: Lianfeng Shi, Ao Li, Benjamin Ward-Cherrier</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-04T14:23:05+00:00 · Latest: 2026-01-28T17:49:10+00:00</div>
<div class="meta-line">Comments: Preliminary work accepted at non-archival OPT-ML workshop at NeurIPS 2025. The workshop version is available in an earlier version of this arXiv paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.03996v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.03996v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking Neural Networks (SNNs) have emerged as a new generation of energy-efficient neural networks suitable for implementation on neuromorphic hardware. As neuromorphic hardware has limited memory and computational resources, parameter pruning and quantization have recently been explored to improve the efficiency of SNNs. State-of-the-art SNN pruning/quantization methods employ multiple compression and training iterations, increasing the cost for pre-trained or very large SNNs. In this paper, we propose a novel one-shot post-training compression framework, Spiking Brain Compression (SBC), that extends the classical Optimal Brain Surgeon method to SNNs. SBC replaces the current-based objective found in the common layer-wise compression method with a spike-train-based objective whose Hessian is cheaply computable, allowing a single backward pass to compress parameters and analytically rescale the rest. Applying SBC to SNN pruning and quantization across event-based and static datasets (up to ImageNet), including SEW-ResNet152 and spike-driven Transformers, we achieve state-of-the-art one-shot post-training compression for SNNs, with single- to double-digit accuracy gains over ANN compression baselines ported to SNNs. We further report a synaptic-operation-based energy proxy and a calibration-size ablation, demonstrating robust performance under sub-one-sample-per-class calibration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>脉冲大脑压缩：面向脉冲神经网络的后训练二阶压缩方法</div>
<div class="mono" style="margin-top:8px">脉冲神经网络（SNNs）已成为新一代高能效神经网络，适用于神经形态硬件实现。由于神经形态硬件内存和计算资源有限，近期研究探索通过参数剪枝和量化提升SNNs效率。现有先进SNN剪枝/量化方法需多次压缩与训练迭代，增加了预训练或大型SNN的成本。本文提出一种新颖的单次后训练压缩框架——脉冲大脑压缩（SBC），将经典最优脑外科医生方法扩展至SNNs。SBC将常见逐层压缩方法中的电流目标替换为基于脉冲序列的目标，其海森矩阵计算成本低廉，仅需单次反向传播即可压缩参数并解析重缩放其余参数。将SBC应用于事件驱动与静态数据集（最高至ImageNet）的SNN剪枝与量化，包括SEW-ResNet152和脉冲驱动Transformer，我们实现了当前最优的SNN单次后训练压缩，相比移植至SNNs的人工神经网络压缩基线获得个位数至两位数的精度提升。我们进一步报告了基于突触操作的能耗代理指标和校准规模消融实验，证明在每类亚单样本校准条件下仍具稳健性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high computational cost of iterative compression methods for Spiking Neural Networks (SNNs) on resource-constrained neuromorphic hardware, this paper proposes Spiking Brain Compression (SBC), a one-shot post-training compression framework. The method extends the Optimal Brain Surgeon approach to SNNs by replacing the current-based objective with a spike-train-based objective, enabling efficient Hessian computation and parameter rescaling in a single backward pass. Experimental results on event-based and static datasets, including ImageNet with SEW-ResNet152 and spike-driven Transformers, show state-of-the-art one-shot compression performance, achieving single- to double-digit accuracy gains over ported ANN baselines while maintaining robust performance with minimal calibration data.</div>
<div class="mono" style="margin-top:8px">为解决脉冲神经网络在资源受限的神经形态硬件上迭代压缩方法计算成本高的问题，本文提出了脉冲大脑压缩（SBC），一种单次训练后压缩框架。该方法将最优脑外科医生方法扩展到SNN，通过用脉冲序列目标替代基于电流的目标，实现了高效的Hessian计算，从而在单次反向传播中完成参数压缩和分析性重缩放。在基于事件和静态数据集（包括使用SEW-ResNet152和脉冲驱动Transformer的ImageNet）上的实验结果表明，该方法实现了最先进的单次压缩性能，相比移植的ANN基线获得了单位数到两位数的精度提升，同时展现出鲁棒的能量效率和校准性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning via Self-Distillation</div>
<div class="meta-line">Authors: Jonas Hübotter, Frederike Lübeck, Lejs Behric, Anton Baumann, Marco Bagatella, Daniel Marta, Ido Hakimi, Idan Shenfeld, Thomas Kleine Buening, Carlos Guestrin, Andreas Krause</div>
<div class="meta-line">First: 2026-01-28T17:45:12+00:00 · Latest: 2026-01-28T17:45:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20802v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly post-trained with reinforcement learning in verifiable domains such as code and math. Yet, current methods for reinforcement learning with verifiable rewards (RLVR) learn only from a scalar outcome reward per attempt, creating a severe credit-assignment bottleneck. Many verifiable environments actually provide rich textual feedback, such as runtime errors or judge evaluations, that explain why an attempt failed. We formalize this setting as reinforcement learning with rich feedback and introduce Self-Distillation Policy Optimization (SDPO), which converts tokenized feedback into a dense learning signal without any external teacher or explicit reward model. SDPO treats the current model conditioned on feedback as a self-teacher and distills its feedback-informed next-token predictions back into the policy. In this way, SDPO leverages the model&#x27;s ability to retrospectively identify its own mistakes in-context. Across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6, SDPO improves sample efficiency and final accuracy over strong RLVR baselines. Notably, SDPO also outperforms baselines in standard RLVR environments that only return scalar feedback by using successful rollouts as implicit feedback for failed attempts. Finally, applying SDPO to individual questions at test time accelerates discovery on difficult binary-reward tasks, achieving the same discovery probability as best-of-k sampling or multi-turn conversations with 3x fewer attempts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自蒸馏的强化学习</div>
<div class="mono" style="margin-top:8px">大型语言模型越来越多地在代码和数学等可验证领域通过强化学习进行后训练。然而，当前基于可验证奖励的强化学习方法仅从每次尝试的标量结果奖励中学习，形成了严重的信用分配瓶颈。许多可验证环境实际上提供了丰富的文本反馈（如运行时错误或评判评估），用以解释尝试失败的原因。我们将此设定形式化为具有丰富反馈的强化学习，并引入自蒸馏策略优化方法，该方法将标记化反馈转化为密集学习信号，无需任何外部教师或显式奖励模型。SDPO将基于反馈调节的当前模型视为自教师，并将其反馈感知的下一标记预测蒸馏回策略中。通过这种方式，SDPO利用了模型在上下文中回溯识别自身错误的能力。在科学推理、工具使用以及LiveCodeBench v6的竞技编程任务中，SDPO相比强RLVR基线在样本效率和最终准确率上均有提升。值得注意的是，在仅返回标量反馈的标准RLVR环境中，SDPO通过将成功轨迹作为失败尝试的隐式反馈，同样超越了基线方法。最后，在测试时对单个问题应用SDPO可加速困难二元奖励任务的探索发现，以比最佳k采样或多轮对话少3倍的尝试次数达到相同的发现概率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the credit-assignment bottleneck in reinforcement learning with verifiable rewards (RLVR), where models only receive scalar outcome rewards, by proposing to leverage the rich textual feedback often available in verifiable environments like runtime errors. The method introduces Self-Distillation Policy Optimization (SDPO), which uses the current model conditioned on tokenized feedback as a self-teacher to distill feedback-informed next-token predictions back into the policy, thereby converting feedback into a dense learning signal without external models. Experimental results across scientific reasoning, tool use, and competitive programming on LiveCodeBench v6 show that SDPO improves sample efficiency and final accuracy over strong RLVR baselines, outperforms them even in scalar-feedback settings by using successful rollouts as implicit feedback, and accelerates discovery on difficult tasks, achieving the same probability as best-of-k sampling with three times fewer attempts.</div>
<div class="mono" style="margin-top:8px">本研究针对可验证奖励强化学习（RLVR）中的信用分配瓶颈问题，即模型仅接收标量结果奖励，提出利用可验证环境（如运行时错误）中常提供的丰富文本反馈。方法引入了自蒸馏策略优化（SDPO），它将当前模型基于标记化反馈生成自教师，然后将其反馈感知的下一个标记预测蒸馏回策略中，从而在没有外部监督的情况下将反馈转化为密集学习信号。在科学推理、工具使用和LiveCodeBench v6的竞争性编程上的实验结果表明，SDPO相比强RLVR基线提高了样本效率和最终准确率，即使在仅提供标量反馈的环境中，通过使用成功轨迹作为隐式反馈也优于基线，并在困难任务上加速了发现过程，以三分之一的尝试次数达到了与最佳k采样相同的成功概率。</div>
</details>
</div>
<div class="card">
<div class="title">Conditional PED-ANOVA: Hyperparameter Importance in Hierarchical &amp; Dynamic Search Spaces</div>
<div class="meta-line">Authors: Kaito Baba, Yoshihiko Ozaki, Shuhei Watanabe</div>
<div class="meta-line">First: 2026-01-28T17:44:36+00:00 · Latest: 2026-01-28T17:44:36+00:00</div>
<div class="meta-line">Comments: 16 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20800v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20800v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose conditional PED-ANOVA (condPED-ANOVA), a principled framework for estimating hyperparameter importance (HPI) in conditional search spaces, where the presence or domain of a hyperparameter can depend on other hyperparameters. Although the original PED-ANOVA provides a fast and efficient way to estimate HPI within the top-performing regions of the search space, it assumes a fixed, unconditional search space and therefore cannot properly handle conditional hyperparameters. To address this, we introduce a conditional HPI for top-performing regions and derive a closed-form estimator that accurately reflects conditional activation and domain changes. Experiments show that naive adaptations of existing HPI estimators yield misleading or uninterpretable importance estimates in conditional settings, whereas condPED-ANOVA consistently provides meaningful importances that reflect the underlying conditional structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>条件PED-ANOVA：层次与动态搜索空间中的超参数重要性分析</div>
<div class="mono" style="margin-top:8px">本文提出条件PED-ANOVA（condPED-ANOVA），这是一个用于估计条件搜索空间中超参数重要性（HPI）的原则性框架。在条件搜索空间中，超参数的存在或定义域可能依赖于其他超参数。尽管原始PED-ANOVA能在搜索空间的高性能区域快速有效地估计HPI，但其假设搜索空间固定且无条件，因此无法正确处理条件超参数。为解决此问题，我们针对高性能区域引入条件HPI概念，并推导出能准确反映条件激活与定义域变化的闭式估计量。实验表明，在条件设置下，现有HPI估计量的简单适配会产生误导性或不可解释的重要性估计，而condPED-ANOVA始终能提供反映底层条件结构的意义明确的重要性度量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of existing hyperparameter importance (HPI) estimators, which assume fixed search spaces and fail to properly handle conditional hyperparameters whose presence or domain depends on other hyperparameters. The method introduces conditional PED-ANOVA, a framework that defines a conditional HPI for top-performing regions and derives a closed-form estimator to accurately account for conditional activation and domain changes. Experimental results demonstrate that naive adaptations of prior estimators produce misleading or uninterpretable importance scores in conditional settings, while the proposed condPED-ANOVA consistently yields meaningful importance estimates that reflect the underlying conditional structure.</div>
<div class="mono" style="margin-top:8px">本研究针对现有超参数重要性估计方法的局限性，这些方法假设固定的搜索空间，无法正确处理其存在或定义域依赖于其他超参数的条件超参数。该方法提出了条件PED-ANOVA框架，为高性能区域定义了条件超参数重要性，并推导出一个闭式估计器，以准确反映条件激活和定义域变化。实验结果表明，在条件设置下，对现有估计器的简单改编会产生误导性或难以解释的重要性评分，而所提出的条件PED-ANOVA则能持续提供反映底层条件结构的有意义的重要性估计。</div>
</details>
</div>
<div class="card">
<div class="title">EVEREST: An Evidential, Tail-Aware Transformer for Rare-Event Time-Series Forecasting</div>
<div class="meta-line">Authors: Antanas Zilinskas, Robert N. Shorten, Jakub Marecek</div>
<div class="meta-line">Venue: 14th International Conference on Learning Representations, 2026</div>
<div class="meta-line">First: 2026-01-26T23:15:20+00:00 · Latest: 2026-01-28T17:40:06+00:00</div>
<div class="meta-line">Comments: Updated author affiliation. No changes to technical content</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19022v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19022v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Forecasting rare events in multivariate time-series data is challenging due to severe class imbalance, long-range dependencies, and distributional uncertainty. We introduce EVEREST, a transformer-based architecture for probabilistic rare-event forecasting that delivers calibrated predictions and tail-aware risk estimation, with auxiliary interpretability via attention-based signal attribution. EVEREST integrates four components: (i) a learnable attention bottleneck for soft aggregation of temporal dynamics; (ii) an evidential head for estimating aleatoric and epistemic uncertainty via a Normal--Inverse--Gamma distribution; (iii) an extreme-value head that models tail risk using a Generalized Pareto Distribution; and (iv) a lightweight precursor head for early-event detection. These modules are jointly optimized with a composite loss (focal loss, evidential NLL, and a tail-sensitive EVT penalty) and act only at training time; deployment uses a single classification head with no inference overhead (approximately 0.81M parameters). On a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic (TSS) of 0.973/0.970/0.966 at 24/48/72-hour horizons for C-class flares. The model is compact, efficient to train on commodity hardware, and applicable to high-stakes domains such as industrial monitoring, weather, and satellite diagnostics. Limitations include reliance on fixed-length inputs and exclusion of image-based modalities, motivating future extensions to streaming and multimodal forecasting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVEREST：一种用于罕见事件时间序列预测的证据性尾部感知Transformer</div>
<div class="mono" style="margin-top:8px">由于严重的类别不平衡、长程依赖性和分布不确定性，多元时间序列数据中的罕见事件预测具有挑战性。我们提出EVEREST，一种基于Transformer的概率性罕见事件预测架构，能够提供校准预测和尾部感知风险估计，并通过基于注意力的信号归因实现辅助可解释性。EVEREST集成四个组件：(i) 可学习的注意力瓶颈用于时间动态的软聚合；(ii) 证据性头部通过正态-逆伽马分布估计偶然性和认知不确定性；(iii) 极值头部使用广义帕累托分布建模尾部风险；(iv) 轻量级前兆头部用于早期事件检测。这些模块通过复合损失函数（焦点损失、证据性负对数似然和尾部敏感极值理论惩罚）联合优化，仅在训练时激活；部署时使用单一分类头部且无推理开销（约81万参数）。在十年空间天气数据上，EVEREST对C级耀斑在24/48/72小时预测窗口实现了0.973/0.970/0.966的最优真技巧统计量。该模型结构紧凑，可在商用硬件高效训练，适用于工业监测、气象和卫星诊断等高风险领域。局限性包括依赖固定长度输入及未纳入图像模态，为未来流式和多模态预测扩展提供方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of forecasting rare events in multivariate time-series data, which is hindered by severe class imbalance, long-range dependencies, and distributional uncertainty. The proposed method, EVEREST, is a transformer-based architecture that integrates a learnable attention bottleneck for temporal dynamics, an evidential head for uncertainty estimation, an extreme-value head for tail risk modeling, and a precursor head for early detection; these components are jointly optimized with a composite loss. Key experimental results show that on a decade of space-weather data, EVEREST achieves state-of-the-art True Skill Statistic scores of 0.973, 0.970, and 0.966 for forecasting C-class solar flares at 24, 48, and 72-hour horizons, respectively, while remaining compact and efficient for deployment.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决多元时间序列数据中罕见事件预测的挑战，这些挑战主要源于严重的类别不平衡、长程依赖性和分布不确定性。所提出的方法EVEREST是一种基于Transformer的架构，它集成了一个用于时序动态学习的注意力瓶颈、一个用于不确定性估计的证据头、一个用于尾部风险建模的极值头以及一个用于早期检测的前兆头；这些组件通过复合损失函数联合优化。关键实验结果表明，在长达十年的空间天气数据上，EVEREST在预测C级太阳耀斑时，于24小时、48小时和72小时的前瞻时间上分别达到了0.973、0.970和0.966的最优真技巧统计分数，同时模型保持紧凑，参数量约为0.81M。</div>
</details>
</div>
<div class="card">
<div class="title">Dissecting Multimodal In-Context Learning: Modality Asymmetries and Circuit Dynamics in modern Transformers</div>
<div class="meta-line">Authors: Yiran Huang, Karsten Roth, Quentin Bouniot, Wenjia Xu, Zeynep Akata</div>
<div class="meta-line">First: 2026-01-28T17:37:28+00:00 · Latest: 2026-01-28T17:37:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20796v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20796v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based multimodal large language models often exhibit in-context learning (ICL) abilities. Motivated by this phenomenon, we ask: how do transformers learn to associate information across modalities from in-context examples? We investigate this question through controlled experiments on small transformers trained on synthetic classification tasks, enabling precise manipulation of data statistics and model architecture. We begin by revisiting core principles of unimodal ICL in modern transformers. While several prior findings replicate, we find that Rotary Position Embeddings (RoPE) increases the data complexity threshold for ICL. Extending to the multimodal setting reveals a fundamental learning asymmetry: when pretrained on high-diversity data from a primary modality, surprisingly low data complexity in the secondary modality suffices for multimodal ICL to emerge. Mechanistic analysis shows that both settings rely on an induction-style mechanism that copies labels from matching in-context exemplars; multimodal training refines and extends these circuits across modalities. Our findings provide a mechanistic foundation for understanding multimodal ICL in modern transformers and introduce a controlled testbed for future investigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>剖析多模态上下文学习：现代Transformer中的模态不对称性与电路动态</div>
<div class="mono" style="margin-top:8px">基于Transformer的多模态大语言模型常展现出上下文学习能力。受此现象启发，我们探究：Transformer如何通过上下文示例学习跨模态信息关联？我们通过在合成分类任务上训练的小型Transformer进行控制实验，精确操纵数据统计与模型架构，以研究该问题。首先重新审视现代Transformer中单模态上下文学习的核心原理：部分先前结论可复现，但发现旋转位置编码提高了上下文学习的数据复杂度阈值。扩展至多模态场景后，揭示出一种根本性学习不对称性：当在主要模态的高多样性数据上预训练时，次要模态仅需极低数据复杂度即可催生多模态上下文学习。机理分析表明，两种场景均依赖基于归纳的机制——从匹配的上下文示例中复制标签；多模态训练则跨模态优化并扩展了这些电路。本研究为理解现代Transformer中的多模态上下文学习提供了机理基础，并为未来探索引入了可控测试平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the in-context learning (ICL) abilities of multimodal transformers, this study investigates how these models learn to associate information across different modalities from in-context examples. The method employs controlled experiments on small transformers trained on synthetic classification tasks, allowing precise manipulation of data and architecture, beginning with an analysis of unimodal ICL principles and extending to multimodal settings. Key experimental findings reveal that Rotary Position Embeddings increase the data complexity needed for ICL, and a fundamental learning asymmetry exists: high-diversity pretraining in a primary modality enables multimodal ICL to emerge with surprisingly low data complexity in a secondary modality, mediated by an induction-style mechanism that copies labels from exemplars, with multimodal training refining these circuits across modalities.</div>
<div class="mono" style="margin-top:8px">本研究受多模态Transformer的上下文学习能力启发，旨在探究模型如何从上下文示例中学习跨模态的信息关联。方法是在合成分类任务上训练小型Transformer进行受控实验，从而精确操控数据统计和架构，首先回顾了单模态上下文学习原理，然后扩展到多模态设置。关键实验结果表明，旋转位置编码提高了上下文学习的数据复杂度阈值，并且存在一种根本性的学习不对称性：当在具有高多样性的主要模态数据上进行预训练后，次要模态仅需极低的数据复杂度即可出现多模态上下文学习，其机制依赖于从匹配的上下文示例中复制标签的归纳式电路。</div>
</details>
</div>
<div class="card">
<div class="title">DiffRatio: Training One-Step Diffusion Models Without Teacher Supervision</div>
<div class="meta-line">Authors: Wenlin Chen, Mingtian Zhang, Jiajun He, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber</div>
<div class="meta-line">First: 2025-02-11T23:02:14+00:00 · Latest: 2026-01-28T17:35:55+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures, 5 tables, 2 algorithms</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.08005v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.08005v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Score-based distillation methods (e.g., variational score distillation) train one-step diffusion models by first pre-training a teacher score model and then distilling it into a one-step student model. However, the gradient estimator in the distillation stage usually suffers from two sources of bias: (1) biased teacher supervision due to score estimation error incurred during pre-training, and (2) the student model&#x27;s score estimation error during distillation. These biases can degrade the quality of the resulting one-step diffusion model. To address this, we propose DiffRatio, a new framework for training one-step diffusion models: instead of estimating the teacher and student scores independently and then taking their difference, we directly estimate the score difference as the gradient of a learned log density ratio between the student and data distributions across diffusion time steps. This approach greatly simplifies the training pipeline, significantly reduces gradient estimation bias, and improves one-step generation quality. Additionally, it also reduces auxiliary network size by using a lightweight density-ratio network instead of two full score networks, which improves computational and memory efficiency. DiffRatio achieves competitive one-step generation results on CIFAR-10 and ImageNet (64x64 and 512x512), outperforming most teacher-supervised distillation methods. Moreover, the learned density ratio naturally serves as a verifier, enabling a principled inference-time parallel scaling scheme that further improves the generation quality without external rewards or additional sequential computation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffRatio：无需教师监督的一步扩散模型训练方法</div>
<div class="mono" style="margin-top:8px">基于分数的蒸馏方法（如变分分数蒸馏）通过先预训练教师分数模型，再将其蒸馏为一步学生模型来训练一步扩散模型。然而，蒸馏阶段的梯度估计器通常存在两种偏差来源：（1）预训练过程中分数估计误差导致的教师监督偏差；（2）蒸馏过程中学生模型的分数估计误差。这些偏差会降低最终一步扩散模型的质量。为此，我们提出DiffRatio这一训练一步扩散模型的新框架：不再独立估计教师和学生分数后计算差值，而是直接通过学生分布与数据分布在扩散时间步上的对数密度比梯度来估计分数差。该方法大幅简化了训练流程，显著降低梯度估计偏差，并提升一步生成质量。此外，通过使用轻量级密度比网络替代两个完整分数网络，减少了辅助网络规模，提升了计算与内存效率。DiffRatio在CIFAR-10和ImageNet（64×64及512×512分辨率）数据集上取得具有竞争力的一步生成结果，优于多数教师监督蒸馏方法。所学习的密度比还可作为验证器，实现原则性的推理时并行缩放方案，无需外部奖励或额外序列计算即可进一步提升生成质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of score-based distillation methods for training one-step diffusion models, which suffer from gradient estimation bias due to errors in both teacher and student score models. The proposed method, DiffRatio, directly estimates the score difference by learning the log density ratio between the student and data distributions across diffusion time steps, using a lightweight density-ratio network instead of separate teacher and student score networks. Experimental results show that DiffRatio achieves competitive one-step generation quality on CIFAR-10 and ImageNet at resolutions up to 512x512, outperforming most teacher-supervised distillation methods, while also improving computational efficiency and enabling a principled inference-time scaling scheme.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决基于分数的蒸馏方法中因教师分数估计误差和学生蒸馏过程中的分数误差导致的偏差问题，以改进一步扩散模型的训练。提出的DiffRatio框架通过直接估计学生分布与数据分布之间对数密度比在扩散时间步上的梯度作为分数差，从而无需单独的教师监督。实验结果表明，DiffRatio在CIFAR-10和ImageNet数据集上（分辨率最高达512x512）实现了一步生成的竞争性质量，优于大多数教师监督方法，同时通过轻量级密度比网络降低了计算成本，并支持推理时并行缩放以进一步提升生成质量。</div>
</details>
</div>
<div class="card">
<div class="title">Federated k-Means over Networks</div>
<div class="meta-line">Authors: Xu Yang, Salvatore Rastelli, Alexander Jung</div>
<div class="meta-line">First: 2025-10-10T06:32:28+00:00 · Latest: 2026-01-28T17:32:20+00:00</div>
<div class="meta-line">Comments: Xu Yang and Salvatore Rastelli contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09718v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09718v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study federated clustering, where interconnected devices collaboratively cluster the data points of private local datasets. Focusing on hard clustering via the k-means principle, we formulate federated k-means as an instance of generalized total variation minimization (GTVMin). This leads to a federated k-means algorithm in which each device updates its local cluster centroids by solving a regularized k-means problem with a regularizer that enforces consistency between neighbouring devices. The resulting algorithm is privacy-friendly, as only aggregated information is exchanged.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>网络联邦k均值聚类</div>
<div class="mono" style="margin-top:8px">本研究探讨联邦聚类问题，即互联设备通过协作对私有本地数据集的数据点进行聚类。聚焦于基于k均值原理的硬聚类，我们将联邦k均值建模为广义全变分最小化（GTVMin）的实例。由此推导出联邦k均值算法：每个设备通过求解带正则项的k均值问题来更新本地聚类中心，该正则项确保相邻设备间的一致性。该算法仅交换聚合信息，具有隐私友好特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of performing clustering across interconnected devices while preserving data privacy, motivated by the need for collaborative analysis of distributed datasets without centralizing sensitive information. The method formulates federated k-means as a generalized total variation minimization problem, developing an algorithm where each device updates local cluster centroids by solving a regularized k-means problem that incorporates a consistency regularizer to align neighboring devices. Experimental results demonstrate that the algorithm effectively clusters data across networks while maintaining privacy, as only aggregated information is exchanged between devices.</div>
<div class="mono" style="margin-top:8px">该研究针对互联设备间协同聚类同时保护数据隐私的挑战，其动机是在不集中原始数据的情况下对分布式私有数据集执行k均值聚类。方法将联邦k均值表述为广义全变分最小化问题，开发了一种算法，其中每个设备通过求解带有正则项的k均值问题来更新本地聚类中心，该正则项强制相邻设备的中心保持一致。实验结果表明，该算法通过仅交换聚合信息而非原始数据，在保持隐私的同时，有效实现了与集中式方法相当的聚类性能。</div>
</details>
</div>
<div class="card">
<div class="title">SERA: Soft-Verified Efficient Repository Agents</div>
<div class="meta-line">Authors: Ethan Shen, Danny Tormoen, Saurabh Shah, Ali Farhadi, Tim Dettmers</div>
<div class="meta-line">First: 2026-01-28T17:27:08+00:00 · Latest: 2026-01-28T17:27:08+00:00</div>
<div class="meta-line">Comments: 21 main pages, 7 pages appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20789v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for training coding agents that enables the rapid and cheap creation of agents specialized to private codebases. Using only supervised finetuning (SFT), SERA achieves state-of-the-art results among fully open-source (open data, method, code) models while matching the performance of frontier open-weight models like Devstral-Small-2. Creating SERA models is 26x cheaper than reinforcement learning and 57x cheaper than previous synthetic data methods to reach equivalent performance. Our method, Soft Verified Generation (SVG), generates thousands of trajectories from a single code repository. Combined with cost-efficiency, this enables specialization to private codebases. Beyond repository specialization, we apply SVG to a larger corpus of codebases, generating over 200,000 synthetic trajectories. We use this dataset to provide detailed analysis of scaling laws, ablations, and confounding factors for training coding agents. Overall, we believe our work will greatly accelerate research on open coding agents and showcase the advantage of open-source models that can specialize to private codebases. We release SERA as the first model in Ai2&#x27;s Open Coding Agents series, along with all our code, data, and Claude Code integration to support the research community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SERA：软验证高效代码库智能体</div>
<div class="mono" style="margin-top:8px">开源权重编程智能体本应具备超越闭源系统的根本优势：可针对私有代码库进行专业化训练，将仓库特定信息直接编码至权重中。然而训练成本与复杂性使这一优势长期停留于理论层面。本研究证明其已具备实践可行性。我们提出软验证高效代码库智能体（SERA），一种高效的编程智能体训练方法，能够快速低成本创建专用于私有代码库的智能体。仅通过监督微调（SFT），SERA即在完全开源（开放数据、方法、代码）模型中取得最先进成果，同时达到如Devstral-Small-2等前沿开源权重模型的性能水平。创建SERA模型的成本比强化学习降低26倍，比先前合成数据方法降低57倍即可获得同等性能。我们的软验证生成（SVG）方法能从单一代码库生成数千条轨迹，结合成本效益优势，实现了对私有代码库的专业化适配。除仓库专业化外，我们将SVG应用于更大规模的代码库语料，生成超过20万条合成轨迹，并利用该数据集详细分析了编程智能体训练的缩放规律、消融实验及混杂因素。总体而言，我们相信这项工作将极大加速开源编程智能体的研究进程，并彰显可适配私有代码库的开源模型优势。我们将SERA作为Ai2开源编程智能体系列的首个模型发布，同步开放全部代码、数据及Claude Code集成方案以支持研究社区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the theoretical advantage of open-weight coding agents in specializing to private codebases, which has been hindered by high training costs and complexity. The method introduces Soft-Verified Efficient Repository Agents (SERA), which employs Soft Verified Generation (SVG) to efficiently produce thousands of synthetic trajectories from a single repository, combined with supervised fine-tuning (SFT) for training. Experimental results show that SERA achieves state-of-the-art performance among fully open-source models, matches the performance of frontier open-weight models like Devstral-Small-2, and reduces training costs by 26x compared to reinforcement learning and 57x compared to previous synthetic data methods, enabling practical specialization to private codebases.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，需要将开放权重编码智能体的理论优势——即通过权重编码实现针对私有代码库的专门化——变得实用，以克服训练的高成本与复杂性。该方法提出了软验证高效仓库智能体（SERA），其采用软验证生成（SVG）技术，从单个代码库高效生成数千条轨迹，并仅使用监督微调（SFT）进行训练。关键实验结果表明，SERA在完全开源的模型中取得了最先进的性能，匹配了如Devstral-Small-2等前沿开放权重模型的性能，且达到同等性能的成本比强化学习低26倍，比先前的合成数据方法低57倍，从而实现了对私有仓库的实际专门化。</div>
</details>
</div>
<div class="card">
<div class="title">Neural Quantum States in Mixed Precision</div>
<div class="meta-line">Authors: Massimo Solinas, Agnes Valenti, Nawaf Bou-Rabee, Roeland Wiersema</div>
<div class="meta-line">First: 2026-01-28T17:15:58+00:00 · Latest: 2026-01-28T17:15:58+00:00</div>
<div class="meta-line">Comments: 22 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20782v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scientific computing has long relied on double precision (64-bit floating point) arithmetic to guarantee accuracy in simulations of real-world phenomena. However, the growing availability of hardware accelerators such as Graphics Processing Units (GPUs) has made low-precision formats attractive due to their superior performance, reduced memory footprint, and improved energy efficiency. In this work, we investigate the role of mixed-precision arithmetic in neural-network based Variational Monte Carlo (VMC), a widely used method for solving computationally otherwise intractable quantum many-body systems. We first derive general analytical bounds on the error introduced by reduced precision on Metropolis-Hastings MCMC, and then empirically validate these bounds on the use-case of VMC. We demonstrate that significant portions of the algorithm, in particular, sampling the quantum state, can be executed in half precision without loss of accuracy. More broadly, this work provides a theoretical framework to assess the applicability of mixed-precision arithmetic in machine-learning approaches that rely on MCMC sampling. In the context of VMC, we additionally demonstrate the practical effectiveness of mixed-precision strategies, enabling more scalable and energy-efficient simulations of quantum many-body systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合精度下的神经量子态</div>
<div class="mono" style="margin-top:8px">科学计算长期依赖双精度（64位浮点）运算以保证现实世界现象模拟的准确性。然而，随着图形处理器（GPU）等硬件加速器的普及，低精度格式因其卓越性能、更低内存占用和更高能效而备受关注。本研究探讨混合精度运算在基于神经网络的变分蒙特卡洛（VMC）方法中的作用——这是求解计算上难以处理的量子多体系统的常用方法。我们首先推导了降低精度对Metropolis-Hastings MCMC引入误差的通用解析边界，随后在VMC应用场景中实证验证这些边界。研究表明，该算法的重要环节（特别是量子态采样）可在半精度下执行且不损失精度。更广泛而言，本研究为评估混合精度运算在依赖MCMC采样的机器学习方法中的适用性提供了理论框架。在VMC背景下，我们还验证了混合精度策略的实际效能，能够实现更具可扩展性和更高能效的量子多体系统模拟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the performance and efficiency advantages of low-precision arithmetic on modern hardware accelerators, this work investigates the use of mixed-precision computing in neural-network-based Variational Monte Carlo (VMC) for quantum many-body systems. The method involves deriving analytical error bounds for reduced precision in Metropolis-Hastings MCMC and empirically validating them within VMC. Key experimental findings show that significant parts of the algorithm, particularly sampling the quantum state, can be executed in half precision without sacrificing accuracy, enabling more scalable and energy-efficient simulations.</div>
<div class="mono" style="margin-top:8px">本研究受现代硬件加速器中低精度计算在性能和能效方面优势的驱动，探究了混合精度计算在基于神经网络的变分蒙特卡洛方法求解量子多体系统中的应用。方法包括推导降低精度的Metropolis-Hastings MCMC的解析误差界，并在VMC中进行实证验证。主要实验结果表明，该算法的关键部分，特别是量子态采样，可以在半精度下执行而不损失精度，从而实现了更具可扩展性和能效的模拟。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
