<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-18 22:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260118_2225</div>
    <div class="row"><div class="card">
<div class="title">Alterbute: Editing Intrinsic Attributes of Objects in Images</div>
<div class="meta-line">Authors: Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-15T18:59:53+00:00 · Latest: 2026-01-15T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page is available at https://talreiss.github.io/alterbute/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://talreiss.github.io/alterbute/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Alterbute, a diffusion-based method for editing an object&#x27;s intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., &#x27;&#x27;Porsche 911 Carrera&#x27;&#x27;) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alterbute：编辑图像中物体的内在属性</div>
<div class="mono" style="margin-top:8px">我们提出Alterbute，一种基于扩散模型的图像物体内在属性编辑方法。该方法支持改变物体的颜色、纹理、材质甚至形状，同时保持其感知身份与场景上下文。现有方法要么依赖无法保持身份的无监督先验，要么采用过度限制的监督方式阻碍有意义的属性变化。我们的方法基于：（1）松弛训练目标，允许模型根据身份参考图像、描述目标内在属性的文本提示、以及定义外在背景的背景图像与物体掩码，同时改变内在与外在属性。在推理阶段，通过复用原始背景与物体掩码限制外在变化，确保仅修改目标内在属性；（2）视觉命名实体——细粒度视觉身份类别（如“保时捷911卡雷拉”），将具有身份定义特征但允许内在属性变化的物体分组。我们利用视觉语言模型从大型公共图像数据集中自动提取VNE标签与内在属性描述，实现可扩展的身份保持监督。Alterbute在身份保持的物体内在属性编辑任务上优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of editing intrinsic attributes like color, texture, material, and shape of objects in images while preserving the object&#x27;s identity and scene context, as existing methods often fail to maintain identity or restrict meaningful variations. The proposed method, Alterbute, employs a diffusion-based approach with a relaxed training objective that conditions changes on an identity reference image, a textual prompt for target attributes, and a background image with an object mask; at inference, it reuses the original background and mask to restrict extrinsic changes. Key experimental results show that Alterbute outperforms existing methods in identity-preserving object intrinsic attribute editing, facilitated by the use of Visual Named Entities for fine-grained identity categorization and scalable supervision from a large dataset.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决图像中物体内在属性（如颜色、纹理、材质和形状）编辑的挑战，同时保持物体的身份和场景上下文，因为现有方法往往无法保持身份或限制了有意义的变异。所提出的方法Alterbute采用基于扩散的模型，通过宽松的训练目标，将变化条件化于身份参考图像、描述目标属性的文本提示以及带有物体掩码的背景图像；在推理时，它重用原始背景和掩码以限制外在变化。关键实验结果表明，Alterbute在保持身份的对象内在属性编辑方面优于现有方法，这得益于使用视觉命名实体进行细粒度身份分类，并从大型数据集中获得可扩展的监督。</div>
</details>
</div>
<div class="card">
<div class="title">From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion</div>
<div class="meta-line">Authors: Cheng Chen, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Peng Di, Hang Yu, Lianli Gao</div>
<div class="meta-line">First: 2026-01-15T18:59:10+00:00 · Latest: 2026-01-15T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10710v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从单对单到多对多：面向深度视觉-语言融合的动态跨层注入</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通过仅将视觉编码器输出与大型语言模型（LLM）输入相连的粗糙非对称连接，造成了严重的视觉特征瓶颈。这种静态架构从根本上限制了LLM与层次化视觉知识实现全面对齐的能力，削弱了其将局部细节与全局语义准确整合为连贯推理的效能。为此，我们提出跨层注入（CLI）——一种新颖轻量的框架，可在两种模态间构建动态的多对多桥梁。CLI包含两个高效协同的参数化组件：自适应多投影（AMP）模块（用于协调来自不同视觉层的特征）和自适应门控融合（AGF）机制（使LLM能依据实时解码上下文选择性注入最相关的视觉信息）。通过将CLI集成至LLaVA-OneVision和LLaVA-1.5，我们在18个多样化基准测试中验证了其有效性与通用性，实验表明性能显著提升。CLI作为一种可扩展范式，通过赋予LLM按需访问完整视觉层次的能力，开启了更深层次的多模态理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the visual feature bottleneck in Vision-Language Models (VLMs), where a static, one-to-one connection between the vision encoder and the large language model (LLM) limits comprehensive alignment with hierarchical visual knowledge. To overcome this, the authors propose Cross-Layer Injection (CLI), a lightweight framework that establishes a dynamic many-to-many bridge via two components: an Adaptive Multi-Projection (AMP) module to harmonize features from different vision layers, and an Adaptive Gating Fusion (AGF) mechanism that allows the LLM to selectively inject relevant visual information based on its decoding context. Experimental integration into LLaVA-OneVision and LLaVA-1.5 across 18 benchmarks shows significant performance improvements, demonstrating CLI&#x27;s effectiveness in enabling deeper multimodal understanding through on-demand access to the full visual hierarchy.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（VLM）中存在的视觉特征瓶颈问题，即视觉编码器与大语言模型（LLM）之间静态的一对一连接限制了模型与层次化视觉知识的全面对齐。为解决此问题，作者提出了跨层注入（CLI）这一轻量级框架，通过两个协同组件构建动态的多对多桥梁：自适应多投影（AMP）模块用于协调来自不同视觉层的特征，以及自适应门控融合（AGF）机制，使LLM能够根据其实时解码上下文有选择地注入最相关的视觉信息。通过在LLaVA-OneVision和LLaVA-1.5模型上进行集成，并在18个多样化基准测试上的广泛实验表明，性能得到显著提升，验证了CLI作为一种可扩展范式，能够通过按需访问完整视觉层次来解锁更深层次的多模态理解能力。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</div>
<div class="meta-line">Authors: Jorge Ortiz</div>
<div class="meta-line">First: 2025-12-31T23:27:32+00:00 · Latest: 2026-01-15T17:31:17+00:00</div>
<div class="meta-line">Comments: Preprint. Diagnostic study of confidence-based abstention under evidence truncation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00138v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00138v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频问答中可预测可靠性的显式弃权调控机制</div>
<div class="mono" style="margin-top:8px">视觉语言模型在高风险场景部署需采用选择性预测机制，使系统在不确定时主动弃权以避免代价高昂的错误。本研究探究基于置信度的弃权机制能否在视频问答任务中实现对错误率的可靠控制，以及该控制在分布偏移下是否保持稳健。通过NExT-QA数据集与Gemini 2.0 Flash模型实验，我们获得两项发现：首先，置信度阈值能在分布内提供机制性控制；调节阈值ε可产生平滑的风险-覆盖权衡曲线，有效降低错误率</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether confidence-based abstention can provide reliable and predictable control over error rates for video question answering (VideoQA) systems, a critical requirement for high-stakes deployment. The method involves a diagnostic analysis using the NExT-QA dataset and the Gemini 2.0 Flash model, where a confidence threshold is systematically varied to examine the risk-coverage trade-off. The key experimental findings show that, in-distribution, adjusting this threshold provides smooth and mechanistic control over error rates, effectively reducing them, though the robustness of this control under distribution shift is a central question of the research.</div>
<div class="mono" style="margin-top:8px">本研究旨在为高风险场景下的视觉语言模型部署提供可靠的选择性预测，即在不确定时选择弃答以避免代价高昂的错误。方法上，该研究使用NExT-QA数据集和Gemini 2.0 Flash模型，探究了基于置信度的弃答机制（通过调整置信度阈值）是否能在视频问答中提供对错误率的可预测控制。主要实验结果表明，置信度阈值法在分布内提供了机制性的控制，能够产生平滑的风险-覆盖权衡并降低错误率，但其在分布变化下的鲁棒性也是该诊断研究的核心问题。</div>
</details>
</div>
<div class="card">
<div class="title">Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</div>
<div class="meta-line">Authors: Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna</div>
<div class="meta-line">First: 2026-01-15T17:27:44+00:00 · Latest: 2026-01-15T17:27:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10611v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10611v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Today&#x27;s strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&amp;F on video tracking).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Molmo2：具备视频理解与定位能力的视觉语言模型开源权重与数据集</div>
<div class="mono" style="margin-top:8px">当前最先进的视频语言模型（VLM）仍为闭源。最强的开源权重模型要么依赖闭源VLM生成的合成数据（本质上是其知识蒸馏），要么未公开训练数据或方法。这导致开源社区缺乏改进前沿视频（及图像）语言模型的基础。关键的是，许多下游应用不仅需要高层次视频理解，还需具备定位能力——通过指向或像素级追踪实现。即使闭源模型也缺乏此功能。我们推出Molmo2系列VLM，在开源模型中达到领先水平，并在单图、多图及视频任务中展现出卓越的指向驱动定位能力。核心贡献是发布了7个新视频数据集和2个多图数据集，包括用于预训练的高细节视频描述数据集、用于微调的自由形式视频问答数据集、含复杂查询的新物体追踪数据集，以及创新的视频指向数据集——所有数据均未使用闭源VLM。我们还提出了采用高效打包与消息树编码方案的数据训练方法，并证明视觉令牌的双向注意力机制与新颖的令牌权重策略能提升性能。我们8B规模的顶尖模型在短视频、计数和描述任务上优于同类开源权重与数据模型，在长视频任务中表现相当；在视频定位方面，Molmo2显著超越Qwen3-VL等开源模型（视频计数准确率35.5对29.6），并在部分任务上超过Gemini 3 Pro等闭源模型（视频指向F1分数38.4对20.0，视频追踪J&amp;F分数56.2对41.1）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of open-source, state-of-the-art video-language models (VLMs) with grounding capabilities, as current leading models are either proprietary or rely on distilled data from them. The method introduces Molmo2, a family of VLMs trained on a novel collection of seven video and two multi-image datasets collected without using closed VLMs, employing an efficient packing and message-tree encoding scheme, bi-directional attention on vision tokens, and a novel token-weight strategy. Key experimental results show that the 8B model outperforms other open-weight models on tasks like short video understanding, counting, and captioning, and significantly surpasses models like Qwen3-VL and even proprietary ones like Gemini 3 Pro in video grounding tasks such as pointing and tracking.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，当前强大的视频-语言模型要么是专有的，要么依赖于从专有模型蒸馏的数据，导致开源社区缺乏具备先进视频理解和像素级定位（如指向或跟踪）能力的模型基础。方法上，研究提出了Molmo2模型系列，基于七个新视频数据集和两个多图像数据集（均未使用封闭模型收集）进行训练，并采用了一种包含高效打包、消息树编码、视觉令牌双向注意力和新颖令牌权重策略的训练方案。主要实验结果表明，其8B模型在短视频、计数和字幕生成上优于其他开源模型，在长视频上具有竞争力，并在视频定位任务（如指向和跟踪）上显著超越了Qwen3-VL等开源模型以及Gemini 3 Pro等专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Misalignment in Vision-Language Models under Perceptual Degradation</div>
<div class="meta-line">Authors: Guo Cheng</div>
<div class="meta-line">First: 2026-01-13T09:13:05+00:00 · Latest: 2026-01-15T17:10:05+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08355v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08355v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知退化下视觉语言模型的语义失配研究</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）正日益应用于自动驾驶和具身人工智能系统，其可靠的感知能力对安全的语义推理与决策至关重要。尽管当前VLMs在多模态基准测试中表现优异，但其对现实感知退化的鲁棒性仍缺乏深入研究。本文通过以Cityscapes数据集语义分割作为代表性感知模块，系统研究了上游视觉感知受控退化时VLMs的语义失配现象。我们引入了仅导致传统分割指标适度下降但能引发下游VLM行为严重失效的感知现实型干扰，包括幻觉对象提及、安全关键实体遗漏及不一致的安全判断。为量化这些影响，我们提出了一套语言级失配度量指标，用于捕捉幻觉、关键遗漏和安全误判，并分析了多种对比式与生成式VLMs中这些指标与分割质量的关系。研究结果揭示了像素级鲁棒性与多模态语义可靠性之间的明显脱节，凸显了当前基于VLM系统的关键局限，表明亟需在安全关键应用中建立能显式考量感知不确定性的评估框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) are critical for safety in autonomous systems, but their robustness to degraded visual inputs is not well understood. This work systematically studies semantic misalignment in VLMs by applying controlled, perception-realistic corruptions to the semantic segmentation output from the Cityscapes dataset, which serves as the upstream visual perception module. The experiments reveal that even moderate drops in standard segmentation metrics lead to severe downstream VLM failures, including object hallucinations, omissions of safety-critical entities, and inconsistent safety judgments, as quantified by newly proposed language-level misalignment metrics. The findings demonstrate a significant disconnect between pixel-level perception robustness and the semantic reliability of multimodal reasoning in current VLMs.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于确保如自动驾驶等安全关键应用中视觉-语言模型（VLMs）语义推理的可靠性，因为这类模型虽被部署，但其对现实感知退化的鲁棒性尚不明确。研究方法是通过对Cityscapes数据集上的语义分割模块施加受控的、感知真实的退化，系统性地研究VLMs中的语义错位，并提出语言级指标来量化幻觉、关键遗漏和安全误判。主要实验结果表明，即使传统分割指标仅出现适度下降，也会导致严重的下游VLM故障，包括幻觉对象和遗漏安全关键实体，这揭示了像素级鲁棒性与多模态语义可靠性之间的脱节。</div>
</details>
</div>
<div class="card">
<div class="title">Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Mikel Williams-Lekuona, Georgina Cosma</div>
<div class="meta-line">First: 2025-12-17T12:19:54+00:00 · Latest: 2026-01-15T16:58:39+00:00</div>
<div class="meta-line">Comments: Camera-ready version for ECIR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15372v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15372v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision transformers in vision-language models typically use the same amount of compute for every image, regardless of whether it is simple or complex. We propose ICAR (Image Complexity-Aware Retrieval), an adaptive computation approach that enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both the early-exit and full-depth paths. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance, attaining a Pearson correlation coefficient of 0.959 with human labelling whilst delivering 4.4x faster complexity prediction. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% faster image encoding while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效视觉语言模型的图像复杂度感知自适应检索</div>
<div class="mono" style="margin-top:8px">视觉语言模型中的视觉Transformer通常对每张图像使用相同的计算量，无论其简单或复杂。我们提出ICAR（图像复杂度感知检索），这是一种自适应计算方法，使视觉Transformer能够对简单图像使用较少计算，同时让复杂图像通过完整的网络深度进行处理。关键挑战在于保持跨模态对齐：来自不同处理深度的嵌入必须保持与文本匹配的兼容性。ICAR通过双路径训练解决这一问题，该训练从早期退出路径和完整深度路径生成兼容的嵌入。这确保了无论图像是早期退出还是完整处理，其表征与文本嵌入都能在同一语义空间中保持兼容。与现有需要昂贵重排序的两阶段方法不同，ICAR无需额外开销即可实现直接的图文匹配。为确定计算量，我们开发了ConvNeXt-IC，将图像复杂度评估视为分类任务。通过采用现代分类器主干而非专用架构，ConvNeXt-IC实现了最先进的性能，与人工标注的皮尔逊相关系数达0.959，同时复杂度预测速度提升4.4倍。在结合真实网络数据的标准基准测试中，ICAR实现了20%的图像编码加速，在保持类别级性能的同时达到实例级性能的95%，为视觉语言系统的可持续扩展提供了可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of vision transformers in vision-language models applying uniform computational cost to all images, regardless of complexity. The proposed method, ICAR, introduces an adaptive computation framework where a ConvNeXt-based classifier (ConvNeXt-IC) first assesses image complexity to route simple images for early exit and complex images for full-depth processing; dual-path training ensures cross-modal alignment, making embeddings from different depths compatible for direct image-text matching without reranking. Experimental results show that ConvNeXt-IC achieves a 0.959 Pearson correlation with human complexity labels and is 4.4x faster than prior methods, while ICAR overall enables 20% faster image encoding while maintaining category-level performance and 95% of instance-level retrieval accuracy on benchmarks augmented with web data.</div>
<div class="mono" style="margin-top:8px">针对视觉语言模型中不同复杂度图像使用统一计算资源导致的效率低下问题，本文提出了ICAR自适应计算方法，该方法减少简单图像的计算量，同时完整处理复杂图像。该方法采用双路径训练确保早期退出与全深度图像嵌入的跨模态对齐，实现无需昂贵重排的直接图文匹配，并集成ConvNeXt-IC进行高效的图像复杂度分类。实验结果表明，ConvNeXt-IC与人工标注的皮尔逊相关系数达0.959且预测速度快4.4倍，而ICAR在增强网络数据的基准测试中实现了图像编码速度提升20%，同时保持类别级性能及95%的实例级性能。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure</div>
<div class="meta-line">Authors: Luxuan Fu, Chong Liu, Bisheng Yang, Zhen Dong</div>
<div class="meta-line">First: 2026-01-15T16:16:34+00:00 · Latest: 2026-01-15T16:16:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10551v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10551v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>释放大型视觉语言模型在路侧基础设施智能感知中的潜力</div>
<div class="mono" style="margin-top:8px">城市路侧基础设施的自动化感知对智慧城市管理至关重要，但通用模型往往难以捕捉必要的细粒度属性与领域规则。尽管大型视觉语言模型在开放世界识别方面表现优异，却常无法依据工程标准准确解析复杂的设施状态，导致实际应用可靠性不足。为此，我们提出一种领域自适应框架，将视觉语言模型转化为专业化的基础设施智能分析智能体。该方法融合了数据高效微调策略与知识驱动的推理机制：首先基于Grounding DINO进行开放词汇微调，以最少监督实现多样化资产的鲁棒定位；随后通过Qwen-VL的LoRA适配实现深层语义属性推理。为减少幻觉并确保专业合规性，我们设计了双模态检索增强生成模块，在推理过程中动态检索权威行业标准与视觉范例。基于新构建的城市路侧场景数据集评估，该框架实现了58.9%的检测平均精度与95.5%的属性识别准确率，为基础设施智能监测提供了稳健解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of general-purpose vision-language models in accurately perceiving and interpreting fine-grained attributes of urban roadside infrastructure according to engineering standards, which is crucial for reliable smart city management. The proposed method transforms large vision-language models into specialized agents through a domain-adapted framework that combines open-vocabulary fine-tuning on Grounding DINO for robust asset localization and LoRA-based adaptation on Qwen-VL for semantic reasoning, enhanced by a dual-modality retrieval-augmented generation module to incorporate authoritative standards and mitigate hallucinations. Experimental evaluation on a comprehensive urban roadside dataset shows the framework achieves 58.9 mAP for detection and 95.5% accuracy for attribute recognition, demonstrating its effectiveness for intelligent infrastructure monitoring.</div>
<div class="mono" style="margin-top:8px">本研究针对通用视觉语言模型在城市路边基础设施自动感知中难以捕捉细粒度属性并遵循领域特定工程标准的问题，提出了一种领域自适应框架。该方法通过基于Grounding DINO的数据高效微调策略实现开放词汇目标定位，并利用基于LoRA的Qwen-VL适配进行深度语义属性推理，同时引入双模态检索增强生成模块动态检索权威行业标准和视觉示例以减少幻觉。在新型城市路边场景数据集上的评估表明，该框架实现了58.9%的mAP检测性能和95.5%的属性识别准确率，为智能基础设施监控提供了可靠解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery</div>
<div class="meta-line">Authors: Chong Liu, Luxuan Fu, Yang Jia, Zhen Dong, Bisheng Yang</div>
<div class="meta-line">First: 2026-01-15T15:57:18+00:00 · Latest: 2026-01-15T15:57:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVII-3D：基于稀疏街景图像实现分米级三维定位与理解的道路基础设施盘点技术进展</div>
<div class="mono" style="margin-top:8px">数字孪生与精确资产盘点的自动化创建是智慧城市建设和设施全生命周期管理的关键任务。然而，受限于鲁棒性不足、定位不精确及细粒度状态理解缺失，利用经济高效的稀疏图像仍具挑战。为此，本文提出SVII-3D——一个面向资产整体数字化的统一框架：首先融合LoRA微调的开集检测与空间注意力匹配网络，实现稀疏视角观测的鲁棒关联；其次引入几何引导优化机制以修正结构误差，达成分米级精度的三维定位；最后突破静态几何映射局限，集成基于多模态提示的视觉-语言模型智能体，自动诊断细粒度运行状态。实验表明SVII-3D显著提升识别精度并最小化定位误差，为高保真基础设施数字化提供了可扩展的经济解决方案，有效弥合稀疏感知与自动化智能维护间的鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to automate the creation of digital twins and precise asset inventories for smart cities, addressing challenges in using cost-effective sparse street imagery, which often suffers from limited robustness, inaccurate localization, and a lack of fine-grained state understanding. The proposed SVII-3D framework integrates three key methods: fusing LoRA fine-tuned open-set detection with a spatial-attention matching network for robust cross-view association, introducing a geometry-guided refinement mechanism for precise decimeter-level 3D localization, and incorporating a Vision-Language Model agent with multi-modal prompting to diagnose fine-grained operational states. Experimental results show that SVII-3D significantly improves identification accuracy and minimizes localization errors, offering a scalable, cost-effective solution for high-fidelity infrastructure digitization that bridges the gap between sparse perception and automated maintenance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决利用稀疏街景图像进行自动化数字孪生创建和资产盘点时面临的挑战，包括鲁棒性不足、定位不准确以及细粒度状态理解缺乏。提出的SVII-3D框架融合了LoRA微调的开集检测与空间注意力匹配网络以实现鲁棒的跨视角关联，采用几何引导的细化机制实现分米级精度的三维定位，并集成一个利用多模态提示的视觉-语言模型代理来自动诊断细粒度运行状态。实验结果表明，SVII-3D显著提高了识别精度并最小化了定位误差，为高保真基础设施数字化提供了一个可扩展且经济高效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">mergetune: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-15T15:15:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v1">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mergetune：视觉语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">微调视觉语言模型（如CLIP）常导致预训练知识的灾难性遗忘。先前研究主要旨在缓解适应过程中的遗忘，但遗忘在此过程中往往不可避免。我们提出一种新范式——持续微调（CFT），旨在零样本模型适应后恢复预训练知识。我们提出一种基于线性模式连通性（LMC）的简单、模型无关的CFT策略（命名为MERGETUNE），可事后应用于现有微调模型而无需架构修改。给定微调模型，我们持续微调其可训练参数（如软提示或线性头），以寻找一个具有两条低损失路径的持续模型：分别通向零样本（如CLIP）和微调（如CoOp）解。通过利用损失景观的几何特性，持续模型隐式融合两个解，恢复微调模型中丢失的预训练知识。挑战在于原始LMC约束需要预训练任务的数据回放。我们通过二阶代理近似零样本模型的约束，无需大规模数据回放。实验表明，MERGETUNE在不增加参数的情况下，将CoOp的基类-新类泛化调和平均值提升+5.6%。在跨数据集迁移中，我们首次在DTD和EuroSAT上实现优于CLIP的性能。在鲁棒微调评估中，MERGETUNE的LMC融合模型以更低推理成本超越集成基线，与零样本模型集成时获得进一步增益和最优结果。代码发布于https://github.com/Surrey-UP-Lab/MERGETUNE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the issue of catastrophic forgetting in fine-tuned vision-language models (VLMs) like CLIP, this paper introduces a novel continued fine-tuning (CFT) paradigm aimed at recovering lost pretrained knowledge post-adaptation. The proposed method, MERGETUNE, is a model-agnostic strategy that leverages linear mode connectivity (LMC) to guide the continued fine-tuning of trainable parameters, searching for a model with low-loss paths to both the original zero-shot and the fine-tuned solutions, thereby merging their knowledge without architectural changes; it approximates the LMC constraint with a second-order surrogate to avoid large-scale data replay. Experimental results demonstrate that MERGETUNE improves the harmonic mean of CoOp by +5.6% on base-novel generalization, achieves superior performance to CLIP on datasets like DTD and EuroSAT in cross-dataset transfer, and, in robust fine-tuning evaluations, surpasses ensemble baselines with lower inference cost while setting state-of-the-art results when ensembled with the zero-shot model.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型（VLM）微调中持续存在的灾难性遗忘问题，本研究提出了一种新颖的持续微调（CFT）范式，旨在模型适配后恢复丢失的预训练知识。所提出的方法MERGETUNE是一种模型无关的策略，它利用线性模式连通性（LMC）来指导模型可训练参数的持续微调，寻找一个同时与原始零样本模型和微调模型具有低损失路径的解，从而在不改变架构的情况下合并两者的能力；该方法通过二阶近似来模拟LMC约束，避免了大规模数据回放。实验结果表明，MERGETUNE在基础-新类泛化上将CoOp的调和平均值提高了5.6%，在跨数据集迁移任务中（如DTD和EuroSAT）性能优于CLIP，并且在鲁棒微调评估中，以更低的推理成本超越了集成基线，与零样本模型集成时进一步取得了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Urban Socio-Semantic Segmentation with Vision-Language Reasoning</div>
<div class="meta-line">Authors: Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li</div>
<div class="meta-line">First: 2026-01-15T15:00:36+00:00 · Latest: 2026-01-15T15:00:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10477v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10477v1">PDF</a> · <a href="https://github.com/AMAP-ML/SocioReasoner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&#x27;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉-语言推理的城市社会语义分割</div>
<div class="mono" style="margin-top:8px">作为人类活动的枢纽，城市地表包含丰富的语义实体。从卫星图像中分割这些多样实体对一系列下游应用至关重要。当前先进的语义分割模型能可靠分割由物理属性定义的实体（如建筑、水体），但在处理社会定义类别（如学校、公园）时仍面临困难。本研究通过视觉-语言模型推理实现了社会语义分割。为此，我们构建了名为SocioSeg的城市社会语义分割数据集，该新型资源包含卫星影像、数字地图及按层级结构组织的社会语义实体像素级标注。此外，我们提出名为SocioReasoner的创新视觉-语言推理框架，通过跨模态识别与多阶段推理模拟人类识别标注社会语义实体的认知过程。采用强化学习优化这一不可微分流程，以激发视觉-语言模型的推理潜能。实验表明，该方法在性能上超越现有先进模型，并展现出强大的零样本泛化能力。数据集与代码已开源：https://github.com/AMAP-ML/SocioReasoner。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of segmenting socially defined categories like schools and parks from satellite imagery, which existing models struggle with despite being proficient at segmenting physical entities. The authors introduce the SocioSeg dataset, which provides hierarchical pixel-level labels for social semantic entities, and propose the SocioReasoner framework that uses vision-language reasoning to mimic human annotation through cross-modal recognition and multi-stage reasoning, optimized via reinforcement learning. Experimental results show that the method outperforms state-of-the-art models and exhibits strong zero-shot generalization capabilities.</div>
<div class="mono" style="margin-top:8px">本研究针对现有分割模型难以从卫星图像中识别学校、公园等社会定义类别的问题，尽管这些模型对物理定义实体已表现良好。作者引入了包含分层像素级标签的社会语义实体数据集SocioSeg，并提出了SocioReasoner框架，该框架通过视觉-语言模型推理模拟人类标注过程，利用跨模态识别和多阶段推理，并通过强化学习进行优化。实验结果表明，该方法优于现有先进模型，并展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning</div>
<div class="meta-line">Authors: Guoqiang Liang, Jianyi Wang, Zhonghua Wu, Shangchen Zhou</div>
<div class="meta-line">First: 2026-01-06T11:00:17+00:00 · Latest: 2026-01-15T14:19:47+00:00</div>
<div class="meta-line">Comments: Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02918v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02918v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ethanliang99.github.io/ZOOMIQA-Projectpage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or providing low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA by jointly generating quality descriptions and scores. However, existing VLM-based IQA methods often suffer from unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions, and 2) reinforcement learning (RL) for dynamic policy exploration, stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, with a Progressive Re-sampling Strategy for mitigating annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Zoom-IQA：基于可靠区域感知推理的图像质量评估</div>
<div class="mono" style="margin-top:8px">图像质量评估是计算机视觉领域的长期难题。现有方法通常仅预测数值分数而缺乏解释，或提供缺乏精确分数的低层次描述。近期基于推理的视觉语言模型通过联合生成质量描述与分数，展现出解决IQA问题的潜力，但现有基于VLM的方法常因视觉与文本线索融合能力有限而产生不可靠推理。本研究提出Zoom-IQA模型，显式模拟关键认知行为：不确定性感知、区域推理与迭代优化。具体采用两阶段训练流程：1）在Grounded-Rationale-IQA数据集上进行监督微调，使模型将评估依据锚定于关键区域；2）通过强化学习进行动态策略探索，辅以KL-Coverage正则化器稳定训练以防止推理与评分多样性坍缩，并采用渐进重采样策略缓解标注偏差。大量实验表明Zoom-IQA在鲁棒性、可解释性与泛化性方面均有提升，在图像修复等下游任务中的应用进一步验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing vision-language models (VLMs) for image quality assessment (IQA), which often produce unreliable reasoning due to insufficient integration of visual and textual cues. The proposed Zoom-IQA model explicitly emulates key cognitive behaviors—uncertainty awareness, region reasoning, and iterative refinement—through a two-stage training pipeline: first, supervised fine-tuning on a Grounded-Rationale-IQA dataset to ground assessments in key regions, followed by reinforcement learning stabilized by a KL-Coverage regularizer and a Progressive Re-sampling Strategy to prevent diversity collapse and mitigate annotation bias. Experimental results demonstrate that Zoom-IQA achieves improved robustness, explainability, and generalization, with effectiveness further validated in downstream tasks like image restoration.</div>
<div class="mono" style="margin-top:8px">该研究针对现有图像质量评估（IQA）方法的局限性：要么提供无解释的数值分数，要么给出缺乏精确分数的低级描述，以及近期视觉语言模型（VLM）因视觉与文本线索整合不佳而导致推理不可靠的问题。提出的Zoom-IQA模型通过模拟关键认知行为——不确定性感知、区域推理和迭代优化——采用两阶段训练流程：首先，在Grounded-Rationale-IQA数据集上进行监督微调，将评估基于关键区域；其次，通过KL-Coverage正则化稳定的强化学习防止多样性崩溃，并结合渐进重采样策略减轻标注偏差。实验结果表明，Zoom-IQA在鲁棒性、可解释性和泛化性方面均有提升，其在下游任务（如图像恢复）中的有效性进一步得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Global Context Compression with Interleaved Vision-Text Transformation</div>
<div class="meta-line">Authors: Dian Jiao, Jiaxin Duan, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang</div>
<div class="meta-line">First: 2026-01-15T13:29:16+00:00 · Latest: 2026-01-15T13:29:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10378v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer&#x27;s input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全局上下文压缩与交错视觉-文本转换</div>
<div class="mono" style="margin-top:8px">视觉语言模型在端到端OCR中的近期成就，为文本信息的低损耗压缩开辟了新途径。这启发了早期研究将Transformer输入渲染为图像进行预填充，通过视觉编码有效减少令牌数量，从而缓解注意力计算的二次增长。然而，这种局部压缩在逐令牌推理阶段无法节省计算或内存成本。本文研究全局上下文压缩，在预填充和推理阶段均节省令牌。为此，我们提出VIST2——一种新型Transformer，它将输入文本块与其视觉编码交错排列，并仅依赖前文中的视觉令牌来预测下一个文本令牌分布。基于此思路，我们将文本块渲染为草图图像，并分多阶段训练VIST2：从课程调度的光学语言建模预训练开始，再到模态交错指令微调。通过规模从0.6B到8B的VIST2系列模型进行大量实验，探索训练方案和超参数。在4倍压缩比下，所得模型在长文本生成任务上显著优于基线，平均实现首令牌生成速度提升3倍、内存使用降低77%、浮点运算量减少74%。代码与数据集将公开以支持后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency of partial compression methods that only reduce tokens during prefilling but not during token-by-token inference, this research introduces VIST2, a Transformer model designed for global context compression. The method interleaves input text chunks with their visual encodings, using exclusively visual tokens from the pre-context to predict subsequent text token distributions; it involves rendering text into sketch images and employs a multi-stage training regimen including curriculum-scheduled pretraining and modal-interleaved instruction tuning. Experimental results from models scaled from 0.6B to 8B parameters show that with a 4× compression ratio, VIST2 significantly outperforms baselines on long writing tasks, delivering on average a 3× speedup in first-token generation, a 77% reduction in memory usage, and a 74% reduction in FLOPS.</div>
<div class="mono" style="margin-top:8px">该研究旨在降低Transformer模型在端到端OCR中的计算和内存成本，现有方法将输入压缩为图像仅能减轻预填充阶段的成本，而无法在逐令牌推理时节省开销。为此，作者提出了VIST2，这是一种新颖的Transformer模型，它将文本块与其视觉编码交错排列，并仅依赖前文中的视觉令牌来预测下一个文本令牌的分布，从而实现全局上下文压缩。通过在0.6B到8B参数规模的模型上进行广泛实验，结果表明，在4倍压缩比下，VIST2在长文本生成任务上显著优于基线模型，平均实现了首次令牌生成速度提升3倍、内存使用减少77%以及浮点运算减少74%的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models</div>
<div class="meta-line">Authors: Peng-Fei Zhang, Zi Huang</div>
<div class="meta-line">First: 2026-01-15T11:45:56+00:00 · Latest: 2026-01-15T11:45:56+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10313v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10313v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型通用多模态攻击的层次化精炼</div>
<div class="mono" style="margin-top:8px">现有视觉语言预训练模型的对抗攻击多为样本特异性方法，在大规模数据集或新场景中扩展时会产生巨大计算开销。为克服此限制，本文提出层次化精炼攻击——一种面向VLP模型的通用多模态攻击框架。该框架在样本层面与优化层面同步精炼通用对抗扰动：在图像模态中，通过解耦对抗样本为干净图像与扰动分量，实现各组件独立处理以更有效破坏跨模态对齐；引入ScMix增强策略，通过多样化视觉上下文强化通用扰动的全局与局部效用，降低对伪特征的依赖；在优化路径上，利用历史梯度与预估未来梯度的时间层次结构避免局部极小值，稳定通用扰动学习。在文本模态中，通过融合句内与句间重要性度量识别全局影响力词汇，并将其作为通用文本扰动。跨下游任务、VLP模型及数据集的广泛实验验证了所提通用多模态攻击的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency of sample-specific adversarial attacks on vision-language pretraining (VLP) models, this work proposes the Hierarchical Refinement Attack (HRA), a universal multimodal attack framework. The method refines universal adversarial perturbations (UAPs) hierarchically: for images, it disentangles clean images from perturbations and employs a ScMix augmentation strategy to diversify visual contexts and reduce reliance on spurious features, while also using a temporal hierarchy of gradients to stabilize optimization; for text, it identifies globally influential words as universal perturbations. Experimental results across various VLP models, datasets, and downstream tasks demonstrate the attack&#x27;s superior effectiveness.</div>
<div class="mono" style="margin-top:8px">针对视觉语言预训练模型样本特异性对抗攻击计算开销大的问题，本研究提出了分层精炼攻击（HRA），一种通用的多模态攻击框架。该方法分层优化通用对抗扰动：对于图像，它将干净图像与扰动解耦，并采用ScMix增强策略以多样化视觉上下文并减少对虚假特征的依赖，同时利用梯度的时序层次结构来稳定优化；对于文本，它识别具有全局影响力的词作为通用扰动。在多种下游任务、模型和数据集上的实验表明，HRA相比现有方法实现了更优的攻击性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Study of Commonsense Reasoning over Visual Object Properties</div>
<div class="meta-line">Authors: Abhishek Kolari, Mohammadhossein Khojasteh, Yifan Jiang, Floris den Hengst, Filip Ilievski</div>
<div class="meta-line">First: 2025-08-14T11:28:40+00:00 · Latest: 2026-01-15T11:10:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10956v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10956v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by human categorization, object property reasoning involves identifying and recognizing low-level details and higher-level abstractions. While current visual question answering (VQA) studies consider multiple object properties, such as size, they typically blend perception and reasoning and lack representativeness in terms of reasoning and image categories, making it unclear whether and how vision-language models (VLMs) abstract and reason over depicted objects. To this end, we introduce a systematic evaluation framework comprising images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions, informed by prior work on common sense. We develop a procedure to instantiate this framework in two VQA object reasoning benchmarks: OPTICS-CNT, comprising 360 images paired with 1,080 multi-level, count-based questions, and OPTICS-CMP, with 2.1k comparison questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations relative to humans, with the best-performing model achieving below 40% counting and 70% comparison accuracy. VLMs struggle particularly with photographic images, counterfactual reasoning, physical and functional properties, and higher counts. We make the OPTICS benchmark data and code available to support future work on scalable benchmarking methods, generalized annotation guidelines, and advanced reasoning VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉物体属性常识推理研究</div>
<div class="mono" style="margin-top:8px">受人类分类能力启发，物体属性推理涉及对底层细节与高层抽象特征的识别认知。现有视觉问答研究虽考虑尺寸等多重物体属性，但常混淆感知与推理过程，且在推理维度与图像类别上缺乏代表性，导致视觉语言模型对描绘物体的抽象与推理机制尚不明确。为此，我们基于常识研究构建系统性评估框架，包含三种代表性图像类型、三个递进复杂度推理层级及四个物体属性维度。通过两套视觉问答基准测试实现该框架：OPTICS-CNT（含360张图像及1080道多层级计数问题）与OPTICS-CMP（含2100道对比问题）。对12个前沿视觉语言模型的零样本实验显示其与人类能力存在显著差距，最优模型在计数与对比任务中准确率分别低于40%和70%。模型在真实照片、反事实推理、物理功能属性及高数量计数场景中表现尤显不足。我们公开OPTICS基准数据与代码，以支持可扩展评测方法、通用标注准则及高阶推理视觉语言模型的后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the unclear capability of vision-language models (VLMs) in abstracting and reasoning over object properties, as existing visual question answering (VQA) benchmarks often blend perception with reasoning and lack representativeness. To systematically evaluate VLMs, the authors introduce a framework based on three image types, three reasoning levels, and four property dimensions, instantiated in two benchmarks: OPTICS-CNT with 1,080 count-based questions and OPTICS-CMP with 2.1k comparison questions. In zero-shot experiments with 12 state-of-the-art VLMs, results show significant limitations compared to humans, with the best model achieving below 40% accuracy on counting and 70% on comparison, particularly struggling with photographic images, counterfactual reasoning, physical/functional properties, and higher counts.</div>
<div class="mono" style="margin-top:8px">本研究针对当前视觉问答（VQA）基准常将感知与推理混淆且缺乏代表性，导致视觉语言模型（VLMs）在图像对象属性上的抽象与推理能力不明确的问题。作者基于常识推理提出了一个系统评估框架，按图像类型（图标、剪贴画、照片）、推理层级（描述性、推断性、反事实）和属性维度（大小、颜色、材料、功能）对任务进行分类。该框架被实例化为两个基准：OPTICS-CNT包含1080个计数问题，OPTICS-CMP包含2100个比较问题。对12个先进VLMs进行的零样本实验揭示了显著局限性，最佳模型在计数任务上准确率低于40%，在比较任务上低于70%；模型尤其在照片图像、反事实推理、物理/功能属性以及较高计数上表现困难，凸显了与人类性能之间的巨大差距。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation</div>
<div class="meta-line">Authors: Zirui Zhao, Boye Niu, David Hsu, Wee Sun Lee</div>
<div class="meta-line">First: 2025-12-01T03:38:44+00:00 · Latest: 2026-01-15T07:18:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01242v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.01242v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成对抗Gumbel蒙特卡洛树搜索用于抽象视觉构图生成</div>
<div class="mono" style="margin-top:8px">本研究聚焦抽象视觉构图，其本质特征由少量几何基元（如部件、对称性、拓扑结构）的空间配置与关系决定，对纹理和写实细节具有不变性。在几何约束与模糊目标（如文本）条件下，从固定组件组合此类结构面临组合爆炸、数据稀缺及离散可行性（无重叠、允许朝向）等挑战，形成稀疏解空间，不适用于纯统计像素空间生成器。我们提出约束引导框架，融合显式几何推理与神经语义理解：采用AlphaGo式搜索确保可行性，微调视觉语言模型评估语义对齐作为奖励信号。算法以策略网络作为蒙特卡洛树搜索启发函数，并通过搜索生成方案微调网络。受生成对抗网络启发，利用生成实例进行对抗性奖励优化，使生成结果在奖励模型无法区分生成实例与真实数据时渐近逼近真实分布。在七巧板拼图任务中，本方法在约束收紧时相比扩散模型与自回归基线具有更高的有效性与语义保真度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating abstract visual compositions, where the identity is defined by spatial arrangements of geometric primitives under constraints like overlap avoidance and allowable orientations, which creates a sparse solution space difficult for standard pixel-based generative models. The proposed method integrates explicit geometric reasoning with neural semantics by employing an AlphaGo-style Monte Carlo Tree Search (MCTS) guided by a policy network to enforce feasibility, while a fine-tuned vision-language model provides semantic alignment rewards; the policy network is further refined using search-generated plans, and an adversarial reward mechanism, inspired by Generative Adversarial Networks, iteratively improves generation quality by distinguishing generated instances from ground truth. Experimental results on the Tangram Assembly task demonstrate that this approach achieves higher validity and semantic fidelity compared to diffusion and auto-regressive baselines, particularly under tighter constraints.</div>
<div class="mono" style="margin-top:8px">本研究针对抽象视觉组合生成的挑战，其中身份主要由几何基元（如部件、对称性、拓扑）的空间配置和关系决定，且需满足无重叠、允许方向等约束，由于组合选择复杂、可行解稀疏，纯统计像素空间方法难以处理。所提方法结合显式几何推理与神经语义，采用类似AlphaGo的蒙特卡洛树搜索（MCTS），通过策略网络确保可行性，同时利用微调的视觉语言模型提供语义对齐分数作为奖励；策略网络通过搜索生成的计划进行微调，并引入受生成对抗网络启发的对抗奖励机制，通过使生成实例与真实数据难以区分来迭代提升生成质量。在七巧板组装任务上的实验结果表明，该方法相比扩散和自回归基线，在约束收紧时能实现更高的有效性和语义保真度。</div>
</details>
</div>
<div class="card">
<div class="title">Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets</div>
<div class="meta-line">Authors: Huy M. Le, Dat Tien Nguyen, Phuc Binh Nguyen, Gia Bao Le Tran, Phu Truong Thien, Cuong Dinh, Minh Nguyen, Nga Nguyen, Thuy T. N. Nguyen, Tan Nhat Nguyen, Binh T. Nguyen</div>
<div class="meta-line">First: 2025-11-15T15:23:44+00:00 · Latest: 2026-01-15T06:23:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12255v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12255v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fusionista2.0：面向大规模数据集的效率检索系统</div>
<div class="mono" style="margin-top:8px">视频浏览器大赛（VBS）要求系统在严格时间限制下提供准确结果。为满足这一需求，我们推出Fusionista2.0——一个针对速度与可用性优化的精简视频检索系统。所有核心模块均经过效率重构：预处理改用ffmpeg实现快速关键帧提取，光学字符识别采用Vintern-1B-v3.5实现鲁棒的多语言文本识别，自动语音识别使用faster-whisper进行实时转录。在问答环节，轻量级视觉语言模型以较低成本提供快速响应。除技术升级外，Fusionista2.0还重新设计了用户界面，提升响应速度、可访问性与工作流效率，使非专业用户也能快速检索相关内容。评估显示，系统检索时间最高减少75%，准确率与用户满意度同步提升，证实Fusionista2.0是具备竞争力且用户友好的大规模视频检索系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Video Browser Showdown (VBS) requires systems to provide accurate video retrieval under strict time limits, motivating the development of Fusionista2.0, a system optimized for speed and usability. The method involves re-engineering core modules for efficiency: using ffmpeg for fast keyframe extraction, Vintern-1B-v3.5 for robust multilingual OCR, faster-whisper for real-time ASR, and lightweight vision-language models for quick question answering, alongside a redesigned user interface for better workflow. Experimental evaluations show that retrieval time was reduced by up to 75% while maintaining or improving accuracy and user satisfaction, confirming its competitiveness for large-scale video search.</div>
<div class="mono" style="margin-top:8px">视频浏览器大赛（VBS）要求系统在严格的时间限制下进行准确的视频检索，这推动了针对速度和可用性优化的Fusionista2.0系统的开发。该方法通过重构核心模块以实现高效处理：使用ffmpeg进行快速关键帧提取，Vintern-1B-v3.5进行多语言OCR，faster-whisper进行实时语音识别，以及采用轻量级视觉语言模型进行问答，并重新设计了用户界面。实验评估表明，检索时间减少了高达75%，同时准确率和用户满意度均得到提升，证实了该系统在大规模视频搜索中的竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation</div>
<div class="meta-line">Authors: Han Wang, Yi Yang, Jingyuan Hu, Minfeng Zhu, Wei Chen</div>
<div class="meta-line">First: 2026-01-15T05:47:43+00:00 · Latest: 2026-01-15T05:47:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10094v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10094v1">PDF</a> · <a href="https://github.com/SatonoDia/V-Zero">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-Zero：无需标注的自演进多模态推理</div>
<div class="mono" style="margin-top:8px">多模态学习的最新进展显著提升了视觉语言模型的推理能力。然而，当前最先进的方法严重依赖大规模人工标注数据集，其获取成本高昂且耗时。为突破此限制，我们提出了V-Zero——一个通用的后训练框架，仅利用未标注图像实现自我演进。该框架通过实例化提问者与求解者两个角色建立协同进化循环：提问者通过对比直觉猜测与推理结果的双轨推理奖励机制，学习生成高质量挑战性问题；求解者则利用对其自身采样响应进行多数投票产生的伪标签进行优化。二者通过组相对策略优化进行迭代训练，形成相互增强的循环。值得注意的是，在完全无需人工标注的情况下，V-Zero在Qwen2.5-VL-7B-Instruct模型上实现了持续性能提升，视觉数学推理能力提高+1.7，通用视觉中心任务提升+2.6，彰显了多模态系统自我演进机制的潜力。代码已开源：https://github.com/SatonoDia/V-Zero</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high cost and time required for human-annotated datasets in multimodal learning, this paper introduces V-Zero, a self-improving post-training framework that uses only unlabeled images. The method establishes a co-evolutionary loop with a Questioner that synthesizes challenging questions using a dual-track reasoning reward and a Solver optimized via pseudo-labels from majority voting of its own responses, both trained iteratively with Group Relative Policy Optimization. Experimental results show that without human annotation, V-Zero improves the Qwen2.5-VL-7B-Instruct model, achieving gains of +1.7 on visual mathematical reasoning and +2.6 on general vision-centric tasks.</div>
<div class="mono" style="margin-top:8px">为解决多模态学习中人工标注数据集成本高、耗时长的问题，本文提出了V-Zero，一种仅使用未标注图像实现自我改进的后训练框架。该方法建立了一个协同进化循环，包含两个角色：提问者通过对比直觉猜测与推理结果的双轨推理奖励来合成高质量挑战性问题，解答者则利用自身响应多数投票产生的伪标签进行优化，两者通过组相对策略优化（GRPO）进行迭代训练。实验结果表明，在没有任何人工标注的情况下，V-Zero在Qwen2.5-VL-7B-Instruct模型上取得了持续性能提升，视觉数学推理能力提高+1.7，通用视觉中心推理能力提高+2.6，证明了多模态系统自我改进的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</div>
<div class="meta-line">Authors: Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</div>
<div class="meta-line">First: 2026-01-12T16:26:42+00:00 · Latest: 2026-01-15T03:58:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07695v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07695v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes &quot;near-miss&quot; samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑算子：可验证平滑奖励激活视觉语言模型的空间推理能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型在实现三维场景理解的精确数值预测方面面临关键瓶颈。传统基于相对排序的强化学习方法常受奖励稀疏性和梯度不稳定性困扰，难以有效利用三维物理约束提供的可验证信号。尤其在标准GRPO框架中，相对归一化会导致&#x27;近失&#x27;样本（具有微小非零误差特征）遭遇优势坍缩，造成宝贵边界样本在优化过程中被丢弃的严重数据利用瓶颈。为此，我们提出平滑数值奖励激活算子与绝对保持GRPO框架。SNRA采用动态参数化Sigmoid函数将原始反馈转化为稠密连续的奖励连续体；AP-GRPO则集成绝对标量梯度以缓解传统相对排序机制固有的数值信息损失。基于此构建的Numerical3D-50k数据集包含5万个可验证三维子任务。实验表明AP-GRPO在保持更高数据效率的同时达到与大规模监督方法相当的性能，无需修改模型架构即可有效激活视觉语言模型的潜在三维推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the bottleneck in Vision-Language Models (VLMs) for precise numerical prediction in 3D scene understanding, where traditional reinforcement learning methods suffer from reward sparsity and gradient instability, particularly causing &#x27;near-miss&#x27; samples to be poorly utilized. The proposed solution is the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework, which transforms raw feedback into a dense reward continuum using a parameterized Sigmoid and integrates absolute scalar gradients to preserve numerical information. Experiments on the constructed Numerical3D-50k dataset show that AP-GRPO matches the performance of large-scale supervised methods with higher data efficiency, effectively activating latent 3D reasoning in VLMs without architectural changes.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型在三维场景理解中进行精确数值预测的瓶颈问题，传统基于相对排序的强化学习方法存在奖励稀疏和梯度不稳定的缺陷，尤其导致“接近正确”的样本无法被有效利用。所提出的方法引入了平滑数值奖励激活算子，它使用参数化的Sigmoid函数生成密集的奖励信号，以及绝对保留GRPO框架，该框架整合绝对标量梯度以保留数值信息。在构建的Numerical3D-50k数据集上的实验结果表明，该框架在不改变模型架构的情况下，以更高的数据效率达到了大规模监督方法的性能水平，有效激活了视觉语言模型的潜在三维推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）发展迅速，但其捕捉空间关系的能力仍存在盲点。当前VLM通常基于对比语言-图像预训练（CLIP）风格的图像编码器构建，其训练方法常将图像展平为一维补丁序列，丢弃了空间推理所需的二维结构。我们认为，这种空间感知能力的缺失是VLM设计中的一个空白维度，也是机器人学与具身AI等需要空间基础的应用瓶颈。为此，我们研究了（i）采用替代目标训练的图像编码器与（ii）二维位置编码。实验表明，这些架构选择可在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models (VLMs) often fail to capture spatial relationships, which is a critical limitation for applications like robotics and embodied AI. This spatial blindspot is attributed to the prevalent use of CLIP-style image encoders that flatten images into 1D patch sequences, discarding essential 2D structure. To address this, the study investigates alternative image encoder training objectives and the incorporation of 2D positional encodings. Experimental results demonstrate that these architectural modifications lead to measurable improvements in spatial reasoning performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在捕捉空间关系方面存在明显缺陷，这成为机器人学和具身智能等应用的关键瓶颈。本研究指出，常见的CLIP风格图像编码器将图像展平为一维序列是主要原因，为此通过探索替代的图像编码器训练目标并引入二维位置编码来增强空间推理能力。实验结果表明，这些架构上的改进在多个空间推理基准测试上带来了可衡量的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation</div>
<div class="meta-line">Authors: Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao, Kuang Gong</div>
<div class="meta-line">First: 2026-01-14T21:21:00+00:00 · Latest: 2026-01-14T21:21:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09879v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedVL-SAM2：用于多模态推理与提示驱动分割的统一三维医学视觉语言模型</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型（VLM）近期在图像级文本中心任务（如报告生成和视觉问答）上取得了显著进展。然而，在三维医学VLM中实现细粒度视觉定位和体空间推理仍具挑战性，尤其是在单一通用框架内统一这些能力。为此，我们提出MedVL-SAM2——一个统一的三维医学多模态模型，同步支持报告生成、视觉问答及多范式分割（包括语义分割、指代分割和交互式分割）。该模型通过专为三维医学影像设计的协同架构，整合图像级推理与像素级感知，并引入基于SAM2的体分割模块以实现精准多粒度空间推理。训练采用多阶段流程：首先在大规模三维CT图文对语料库上进行预训练，以对齐体视觉特征与放射学语言嵌入；随后使用综合三维CT分割数据集，联合优化语言理解与分割目标。这种联合训练支持通过语言、点或框提示进行灵活交互，从而统一高层视觉推理与空间精确定位。我们的统一架构在报告生成、视觉问答及多项三维分割任务中均达到最先进性能。深入分析进一步表明，该模型能提供可靠的三维视觉定位、可控交互式分割及鲁棒跨模态推理，证明高层语义推理与精准三维定位可在统一的三维医学VLM中协同实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of integrating fine-grained visual grounding and volumetric spatial reasoning within a single, generalizable 3D medical vision-language model, as existing models excel at image-level tasks but lack unified capabilities for precise 3D localization. The method introduces MedVL-SAM2, a unified model that combines image-level reasoning and pixel-level perception through a cohesive architecture, incorporating a SAM2-based volumetric segmentation module; it is trained via a multi-stage pipeline involving pre-training on 3D CT image-text pairs and joint optimization with language-understanding and segmentation objectives. Experimental results demonstrate state-of-the-art performance in report generation, VQA, and multiple 3D segmentation tasks, with analyses confirming reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning.</div>
<div class="mono" style="margin-top:8px">现有医学视觉语言模型在报告生成和视觉问答等图像级任务上表现良好，但在细粒度三维视觉定位和体积空间推理方面仍面临挑战。为此，本研究提出了MedVL-SAM2，一个统一的三维医学多模态模型，通过整合图像级推理和像素级感知的架构，并引入基于SAM2的体积分割模块，以同时支持报告生成、视觉问答及多范式分割。模型采用多阶段训练流程：首先在大规模三维CT图像-文本对上进行预训练以实现特征对齐，随后结合语言理解和分割目标进行联合优化。实验结果表明，该模型在报告生成、视觉问答和多种三维分割任务上均达到了最先进的性能，展现出可靠的三维视觉定位、可控的交互式分割和稳健的跨模态推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning</div>
<div class="meta-line">Authors: Po-han Li, Shenghui Chen, Ufuk Topcu, Sandeep Chinchali</div>
<div class="meta-line">First: 2026-01-14T20:14:47+00:00 · Latest: 2026-01-14T20:14:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09851v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSIL：多模态视频描述中信息损失的统一评估</div>
<div class="mono" style="margin-top:8px">多模态视频描述将密集视频内容压缩为关键帧与自然语言的结构化格式。通过创建统一的多模态摘要，该方法将生成式AI锚定于丰富的语义证据，并作为高效检索的轻量级代理。然而，传统指标（如BLEU或ROUGE）无法量化跨不同模态的信息覆盖度，例如比较文本段落与关键帧序列。为此，我们提出视频摘要信息损失（ViSIL）分数，这是一个基于信息论的框架，通过视觉语言模型（VLM）推理量化摘要未捕获的视频信息。通过测量信息损失，ViSIL作为一种统一指标，可在结构差异显著的多模态摘要格式间直接比较。实验结果表明，ViSIL分数与人类及VLM在视频问答（VQA）任务上的表现均呈现统计显著相关性。ViSIL还能通过摘要选择优化信息损失与处理速度的权衡，构建帕累托最优前沿，在VQA准确率上超越文本摘要7%，且未增加处理负载。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inability of traditional metrics like BLEU or ROUGE to quantify information coverage across different modalities in multimodal video captioning, which combines keyframes and text. To solve this, the authors propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that uses vision-language model inference to measure the video information not captured by a summary. Experimental results show ViSIL scores correlate significantly with human and VLM performance on Video Question Answering tasks, and using ViSIL for summary selection establishes a Pareto-optimal frontier that improves VQA accuracy by 7% over text summaries without increasing processing load.</div>
<div class="mono" style="margin-top:8px">本研究针对多模态视频描述（结合关键帧和文本）中，传统指标如BLEU或ROUGE无法量化跨模态信息覆盖的问题。作者提出了视频摘要信息损失（ViSIL）分数，这是一个基于信息论的框架，利用视觉语言模型推理来测量摘要未捕获的视频信息。实验结果表明，ViSIL分数与人类和视觉语言模型在视频问答任务上的表现存在显著相关性，并且使用ViSIL进行摘要选择可以建立一个帕累托最优前沿，在视频问答准确率上比纯文本摘要高出7%，且不增加处理负载。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Animal Identification for Long Videos</div>
<div class="meta-line">Authors: Xuyang Fang, Sion Hannuna, Edwin Simpson, Neill Campbell</div>
<div class="meta-line">First: 2026-01-14T17:53:59+00:00 · Latest: 2026-01-14T17:53:59+00:00</div>
<div class="meta-line">Comments: 11 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09663v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09663v1">PDF</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves}{here">Code1</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($&gt;$97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长视频的自监督动物个体识别方法</div>
<div class="mono" style="margin-top:8px">在长时间视频中识别个体动物对行为生态学、野生动物监测和畜牧管理至关重要。传统方法需要大量人工标注，而现有自监督方法因内存限制和时间误差传播问题，计算需求大且不适用于长序列。我们提出一种高效的自监督方法，将动物识别重构为全局聚类任务而非序列跟踪问题。该方法基于单视频中已知固定个体数的常见实际场景，仅需边界框检测和总数信息。通过采样帧对、使用冻结预训练主干网络，并采用匈牙利算法进行批内伪标签分配的自举机制，可在无身份标签情况下学习判别性特征。我们借鉴视觉语言模型的二元交叉熵损失，在每批次GPU内存消耗低于1GB（比标准对比方法低一个数量级）的同时实现最先进准确率（&gt;97%）。在具有挑战性的真实数据集（3D-POP鸽群和8头犊牛进食视频）评估中，本框架达到或超越了基于1000多帧标注数据的监督基线，有效消除了人工标注瓶颈。该工作使消费级硬件能够实现高精度动物识别，在资源受限的研究场景中具有广泛适用性。本文所有代码已发布于指定链接。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for efficient individual animal identification in long videos, crucial for behavioral ecology and wildlife monitoring, by overcoming the limitations of manual annotation and computationally intensive self-supervised methods. The proposed method reframes identification as a global clustering task, assuming a fixed number of individuals, and uses only bounding box detections and the total count; it employs frame pair sampling, a frozen pre-trained backbone, and a self-bootstrapping mechanism with the Hungarian algorithm for pseudo-label assignment to learn discriminative features without identity labels. Experimental results on datasets like 3D-POP pigeons and 8-calves feeding videos show the method achieves over 97% accuracy, matches or surpasses supervised baselines trained on over 1,000 labeled frames, and consumes less than 1 GB of GPU memory per batch, enabling practical use on consumer-grade hardware.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决长视频中个体动物识别效率低下的问题，这对于行为生态学和野生动物监测至关重要，以克服手动标注和计算密集型自监督方法的局限。该方法将识别重新定义为全局聚类任务，假设个体数量固定，仅使用边界框和总数；它采用帧对采样、冻结预训练主干网络，以及结合匈牙利算法的自引导机制进行伪标签分配，从而无需身份标签即可学习特征。在3D-POP鸽子和8头小牛进食视频等数据集上的实验结果表明，该方法实现了超过97%的最先进准确率，每批次GPU内存消耗低于1 GB，匹配或超越了基于超过1,000个标注帧训练的监督基线，有效消除了手动标注瓶颈，使得在消费级硬件上的实际应用成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">LiteEmbed: Adapting CLIP to Rare Classes</div>
<div class="meta-line">Authors: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</div>
<div class="meta-line">First: 2026-01-14T17:53:11+00:00 · Latest: 2026-01-14T17:53:11+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiteEmbed：将CLIP适配至稀有类别</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（如CLIP）虽在零样本识别中表现优异，但在处理预训练阶段罕见类别（包括新兴实体与文化特定类别）时存在局限。本文提出LiteEmbed——一种轻量级CLIP少样本个性化框架，无需重新训练编码器即可扩展新类别。该方法通过基于PCA的分解解耦粗粒度语义方向与细粒度变化，在CLIP词汇空间内执行文本嵌入的子空间引导优化。通过粗粒度对齐与细粒度分离两个互补目标，在保持全局语义一致性的同时增强视觉相似类别间的区分度。优化后的嵌入即插即用，可无缝替代CLIP原始文本特征，适用于分类、检索、分割与检测任务。大量实验表明，该方法较现有技术取得显著提升，为CLIP适配低代表性、稀有或未见类别提供了有效方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large vision-language models like CLIP perform poorly on rare or emerging classes not well-represented in their pretraining data. To address this, LiteEmbed introduces a lightweight few-shot personalization method that optimizes new text embeddings within CLIP&#x27;s vocabulary without retraining the model&#x27;s encoders. The method uses a PCA-based decomposition to guide optimization with two objectives: coarse alignment for semantic consistency and fine separation for discriminability among similar classes. Experiments show LiteEmbed achieves significant performance improvements over prior methods on adapting CLIP to rare classes across various vision tasks.</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型如CLIP在预训练中罕见或新出现的类别上表现不佳。为此，LiteEmbed提出了一种轻量级的少样本个性化框架，通过优化新的文本嵌入来适配CLIP，而无需重新训练模型编码器。该方法基于PCA分解引导优化，将粗粒度语义方向与细粒度变化解耦，并采用粗粒度对齐和细粒度分离的目标，在保持全局语义一致性的同时增强相似类别间的区分性。实验表明，LiteEmbed在多种任务上显著优于现有方法，能有效将CLIP适配到代表性不足、罕见或未见过的类别。</div>
</details>
</div>
<div class="card">
<div class="title">Image2Garment: Simulation-ready Garment Generation from a Single Image</div>
<div class="meta-line">Authors: Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu, Yang Zheng, Hugo Bertiche, Menglei Chai, Thabo Beeler, Gordon Wetzstein</div>
<div class="meta-line">First: 2026-01-14T17:47:33+00:00 · Latest: 2026-01-14T17:47:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09658v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09658v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Image2Garment：从单张图像生成仿真就绪的服装</div>
<div class="mono" style="margin-top:8px">从单张图像估计物理精确、仿真就绪的服装具有挑战性，原因在于缺乏图像到物理的数据集以及该问题本身的不适定性。现有方法要么需要多视角捕捉和昂贵的可微分仿真，要么仅预测服装几何形状而缺乏真实仿真所需的材料属性。我们提出一种前馈框架，通过以下方式规避这些限制：首先微调视觉语言模型以从真实图像推断材料成分和织物属性，随后利用小型材料物理测量数据集训练轻量级预测器，将这些属性映射至相应的物理织物参数。我们的方法引入了两个新数据集（FTAG与T2P），无需迭代优化即可从单张图像生成仿真就绪的服装。实验表明，该估计器在材料成分估计和织物属性预测方面具有更优的准确性，且通过物理参数估计器处理后，相较于当前最先进的图像到服装方法，能实现更高保真度的仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating physically accurate, simulation-ready garments from a single image, a problem made difficult by the lack of image-to-physics datasets and its ill-posed nature. The method involves a feed-forward framework that first fine-tunes a vision-language model to infer material composition and fabric attributes from images, and then trains a lightweight predictor to map these attributes to physical fabric parameters using a small dataset of material-physics measurements. Experimental results demonstrate that the approach achieves superior accuracy in material composition and fabric attribute prediction, and by using these predictions to estimate physics parameters, it enables higher-fidelity garment simulations compared to existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从单张图像生成物理精确、可直接用于仿真的服装这一挑战，该问题因缺乏图像到物理的数据集及其不适定性而变得困难。所提出的方法采用了一个两阶段前馈框架：首先，一个微调的视觉语言模型从输入图像推断材料成分和织物属性；其次，一个轻量级预测器利用一个小型的材料物理测量数据集，将这些属性映射到物理织物参数。实验结果表明，该方法在材料成分估计和织物属性预测方面实现了更高的准确性，并且通过使用这些推断出的参数，与现有的先进图像到服装方法相比，能够实现更高保真度的服装仿真，整个过程无需迭代优化或多视角捕捉。</div>
</details>
</div>
<div class="card">
<div class="title">Head Pursuit: Probing Attention Specialization in Multimodal Transformers</div>
<div class="meta-line">Authors: Lorenzo Basile, Valentino Maiorca, Diego Doimo, Francesco Locatello, Alberto Cazzaniga</div>
<div class="meta-line">Venue: NeurIPS 2025 spotlight</div>
<div class="meta-line">First: 2025-10-24T14:41:47+00:00 · Latest: 2026-01-14T15:47:59+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025 (spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21518v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21518v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意力头追踪：探究多模态Transformer中的注意力专业化机制</div>
<div class="mono" style="margin-top:8px">语言模型与视觉语言模型在广泛任务中展现出卓越性能，但其内部机制尚未完全明晰。本研究聚焦于文本生成模型中单个注意力头对特定语义或视觉属性的专业化机制。基于既有可解释性方法，我们通过信号处理视角重新阐释了利用最终解码层探测中间激活的实践，从而建立多样本分析框架，并依据注意力头与目标概念的相关性进行排序。研究结果表明，单模态与多模态Transformer在注意力头层面均存在一致的专业化模式。值得注意的是，采用本方法筛选仅1%的注意力头进行编辑，即可可靠地抑制或增强模型输出中的目标概念。我们在问答、毒性缓解等语言任务，以及图像分类、描述生成等视觉语言任务上验证了该方法的有效性。本研究揭示了注意力层内可解释且可控的结构，为理解与编辑大规模生成模型提供了简洁工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how individual attention heads in text-generative and multimodal transformers specialize in processing specific semantic or visual concepts, aiming to better understand their internal mechanisms. The method reinterprets the probing of intermediate activations through a signal processing lens, enabling principled analysis across multiple samples to rank heads by their relevance to target attributes. Experimental results reveal consistent patterns of head-level specialization and demonstrate that editing as few as 1% of heads, selected via this approach, can reliably suppress or enhance targeted concepts in outputs, validated across language tasks like question answering and toxicity mitigation, and vision-language tasks including image classification and captioning.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究文本生成和多模态Transformer模型中单个注意力头如何专门处理特定语义或视觉概念，以更好地理解其内部机制。该方法通过信号处理的视角重新解释了中间激活的探测，从而能够对多个样本进行原理性分析，并根据注意力头与目标属性的相关性进行排序。关键的实验结果表明，注意力头存在一致的专门化模式，并且通过该方法选择并编辑少至1%的注意力头，即可可靠地抑制或增强输出中的目标概念，这一发现在语言任务（如问答和毒性缓解）以及视觉语言任务（如图像分类和字幕生成）中均得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</div>
<div class="meta-line">Authors: Sheng-Yu Huang, Jaesung Choe, Yu-Chiang Frank Wang, Cheng Sun</div>
<div class="meta-line">First: 2026-01-14T15:45:57+00:00 · Latest: 2026-01-14T15:45:57+00:00</div>
<div class="meta-line">Comments: project page: https://peterjohnsonhuang.github.io/openvoxel-pages/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09575v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09575v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://peterjohnsonhuang.github.io/openvoxel-pages/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenVoxel：面向开放词汇3D场景理解的免训练体素分组与描述方法</div>
<div class="mono" style="margin-top:8px">本文提出OpenVoxel——一种免训练算法，用于为开放词汇3D场景理解任务进行稀疏体素的分组与描述。基于从多视角图像获取的稀疏体素栅格化模型，OpenVoxel能够生成描述场景中不同物体的有意义分组。通过结合强大的视觉语言模型与多模态大语言模型，该方法通过对每个分组进行文字描述构建信息丰富的场景地图，从而支持开放词汇分割、指代表达分割等进阶3D场景理解任务。与现有方法不同，本方法无需训练且不依赖CLIP/BERT文本编码器的嵌入向量，而是直接利用多模态大语言模型进行文本到文本的检索。大量实验表明，该方法在复杂指代表达分割任务中性能显著优于近期研究。代码将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for open-vocabulary 3D scene understanding without requiring task-specific training. The proposed OpenVoxel method is a training-free algorithm that first groups sparse voxels from a 3D scene rasterization into meaningful object segments, then captions each group by leveraging Vision Language Models and Multi-modal Large Language Models for text-to-text search, avoiding the use of CLIP/BERT text encoders. Experimental results show that this approach achieves superior performance compared to recent methods, particularly excelling in complex referring expression segmentation tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对开放词汇3D场景理解中无需训练方法的需求，提出了OpenVoxel，能够在无需模型训练或CLIP/BERT文本嵌入的情况下对稀疏体素进行分组和描述。该方法基于多视角图像生成的稀疏体素栅格化模型，利用视觉语言模型和多模态大语言模型通过文本到文本搜索实现体素聚类分组和描述。实验结果表明，OpenVoxel在复杂指代表达分割任务中优于现有方法，同时能为下游应用创建信息丰富的场景地图。</div>
</details>
</div>
<div class="card">
<div class="title">PrivLEX: Detecting legal concepts in images through Vision-Language Models</div>
<div class="meta-line">Authors: Darya Baranouskaya, Andrea Cavallaro</div>
<div class="meta-line">First: 2026-01-14T12:51:48+00:00 · Latest: 2026-01-14T12:51:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09449v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX&#x27;s ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PrivLEX：通过视觉语言模型检测图像中的法律概念</div>
<div class="mono" style="margin-top:8px">本文提出PrivLEX——一种基于法律定义的个人数据概念进行决策的新型图像隐私分类器。作为首个与法律概念对齐的可解释隐私分类器，PrivLEX利用视觉语言模型（VLM）的识别能力，通过零样本VLM概念检测实现无需训练阶段显式概念标注的无标签概念瓶颈模型，从而提供可解释的分类结果。我们验证了PrivLEX识别图像中个人数据概念的能力，并进一步分析了图像隐私数据集人工标注者对这些概念的敏感度认知。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop an interpretable image privacy classifier grounded in legally defined personal data concepts, addressing the need for transparency in automated privacy decisions. The method, named PrivLEX, employs a zero-shot Vision-Language Model for concept detection and integrates this into a label-free Concept Bottleneck Model, eliminating the requirement for explicit concept labels during training. Experimental results demonstrate that PrivLEX can effectively identify personal data concepts within images and provides an analysis of how sensitive these concepts are perceived by human annotators in existing privacy datasets.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种基于法律定义的个人数据概念的可解释图像隐私分类器，以解决自动化隐私决策中透明度的需求。该方法名为PrivLEX，利用零样本视觉语言模型进行概念检测，并将其与无需标签的概念瓶颈模型结合，从而在训练过程中无需显式的概念标注。实验结果表明，PrivLEX能够有效识别图像中的个人数据概念，进一步分析揭示了隐私数据集中人类标注者对这些概念敏感度的认知。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models</div>
<div class="meta-line">Authors: Junjie Li, Ziao Wang, Jianghong Ma, Xiaofeng Zhang</div>
<div class="meta-line">First: 2025-09-27T02:57:37+00:00 · Latest: 2026-01-14T12:33:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.00040v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.00040v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) achieve strong benchmark performance, but controlling their behavior through instruction tuning remains difficult. Reducing the budget of instruction tuning dataset often causes regressions, as heuristic strategies treat models as black boxes and overlook the latent capabilities that govern learning. We introduce Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. CADC discovers intrinsic capabilities in an unsupervised manner from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. This transforms black-box instruction tuning into a controllable, capability-driven process. With as little as 5% of the original data, CADC surpasses full-data training on multimodal benchmarks. These results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a principle paradigm for instruction data curation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示内在能力：视觉语言模型数据策展的新范式</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLM）在基准测试中表现优异，但通过指令微调控制其行为仍具挑战。减少指令微调数据集的预算常导致性能衰退，因为启发式策略将模型视为黑箱，忽视了主导学习的潜在能力。我们提出能力归因数据策展（CADC）框架，将策展从任务特定启发式转向内在能力分析。CADC通过基于梯度的学习轨迹无监督地发现内在能力，借助影响估计将训练数据归因于这些能力，并通过平衡选择与分阶段排序构建能力感知的课程。这使黑箱指令微调转变为可控的、能力驱动的过程。仅使用原始数据5%的情况下，CADC在多模态基准测试中超越了全数据训练。这些结果验证了内在能力作为模型学习的基本构建单元，并确立了CADC作为指令数据策展的原则性范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the difficulty of controlling vision-language models via instruction tuning and the performance regressions from reduced data budgets, this work introduces Capability-Attributed Data Curation (CADC), a framework that moves from task-specific heuristics to analyzing intrinsic model capabilities. The method discovers these capabilities unsupervised from gradient-based learning trajectories, attributes training data to them via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. Experiments show that using only 5% of the original data, CADC outperforms full-data training on multimodal benchmarks, validating intrinsic capabilities as fundamental learning components and establishing CADC as a principled data curation paradigm.</div>
<div class="mono" style="margin-top:8px">针对视觉语言模型在指令微调中行为控制困难以及减少训练数据量导致性能退化的问题，本研究提出了能力归因数据筛选框架，将数据筛选从任务特定启发式方法转向内在能力分析。该方法通过基于梯度的学习轨迹无监督地发现模型内在能力，利用影响估计将训练数据归因于这些能力，并通过平衡选择和分阶段排序构建能力感知的课程学习方案。实验结果表明，仅使用原始训练数据的5%，该方法在多项多模态基准测试上超越了全数据训练的性能，验证了内在能力作为模型学习基本构建单元的重要性，并确立了该框架作为指令数据筛选的原则性范式。</div>
</details>
</div>
<div class="card">
<div class="title">ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation</div>
<div class="meta-line">Authors: Edoardo Bianchi, Jacopo Staiano, Antonio Liotta</div>
<div class="meta-line">First: 2025-09-30T14:00:41+00:00 · Latest: 2026-01-14T12:30:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26278v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26278v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing approaches treat action quality assessment and skill proficiency estimation as classification problems, outputting discrete labels without interpretable reasoning. We reformulate this task as generative vision language modeling, introducing ProfVLM, a compact model that jointly predicts proficiency levels and generates expert-like natural language feedback from multi-view videos. ProfVLM leverages conditional language generation to provide actionable insights along with quantitative evaluation scores. Central to our method is an AttentiveGatedProjector that dynamically fuses and projects multi-view egocentric and exocentric features from a frozen TimeSformer backbone into a language model fine-tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60% compared to existing classification-based methods. By providing natural language critiques aligned with performance levels, this work shows that generative vision-language modeling offers a powerful and efficient paradigm shift for interpretable action quality assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProfVLM：用于多视角技能熟练度评估的轻量级视频语言模型</div>
<div class="mono" style="margin-top:8px">现有方法将动作质量评估与技能熟练度估计视为分类问题，仅输出离散标签而缺乏可解释的推理过程。本研究将该任务重构为生成式视觉语言建模，提出紧凑模型ProfVLM，能够从多视角视频中联合预测熟练度等级并生成类专家的自然语言反馈。该模型通过条件语言生成机制，在提供量化评估分数的同时输出可操作的改进建议。方法的核心是注意力门控投影器，其动态融合并投影来自冻结TimeSformer骨干网络的多视角第一人称与第三人称特征，输入至专为反馈生成微调的语言模型。基于EgoExo4D数据集及专家注释训练的ProfVLM，在参数量减少高达20倍、训练时间降低60%的情况下，性能仍超越现有最优方法。通过生成与表现水平对应的自然语言评析，本研究表明生成式视觉语言建模为可解释的动作质量评估提供了高效且强大的范式转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To move beyond discrete classification labels and enable interpretable reasoning for action quality assessment and skill proficiency estimation, this work reformulates the task as generative vision-language modeling. The proposed ProfVLM model uses an AttentiveGatedProjector to dynamically fuse multi-view egocentric and exocentric video features from a frozen TimeSformer backbone and projects them into a language model fine-tuned to jointly predict proficiency levels and generate expert-like natural language feedback. Experiments on the EgoExo4D dataset show that ProfVLM outperforms state-of-the-art methods while being up to 20 times more parameter-efficient and reducing training time by up to 60%, demonstrating that generative modeling provides an efficient and interpretable paradigm for the task.</div>
<div class="mono" style="margin-top:8px">为了超越离散的分类标签，实现动作质量评估与技能熟练度估计的可解释推理，本研究将该任务重新定义为生成式视觉-语言建模。提出的ProfVLM是一个轻量级模型，它通过一个注意力门控投影器，动态融合来自冻结TimeSformer骨干网络的多视角第一人称和第三人称视频特征，并将其投影到一个为生成熟练度分数和专家式自然语言反馈而微调的语言模型中。在EgoExo4D数据集上的实验表明，ProfVLM在性能上超越了现有最佳方法，同时参数量减少了多达20倍，训练时间缩短了高达60%，证明了生成式建模为熟练度估计提供了一种高效且可解释的新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Frequency Is What You Need: Considering Word Frequency When Text Masking Benefits Vision-Language Model Pre-training</div>
<div class="meta-line">Authors: Mingliang Liang, Martha Larson</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2024-12-20T18:51:41+00:00 · Latest: 2026-01-14T11:07:46+00:00</div>
<div class="meta-line">Comments: Accepted by WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.16148v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.16148v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of strategies (truncation, random masking, block masking and syntax masking) and has reported syntax masking as the top performer. In this paper, we analyze the impact of different text masking strategies on the word frequency in the training data, and show that this impact is connected to model success. This finding motivates Contrastive Language-Image Pre-training with Word Frequency Masking (CLIPF), our proposed masking approach, which directly leverages word frequency. Extensive experiments demonstrate the advantages of CLIPF over syntax masking and other existing approaches, particularly when the number of input tokens decreases. We show that not only CLIPF, but also other existing masking strategies, outperform syntax masking when enough epochs are used during training, a finding of practical importance for selecting a text masking method for VLM training. Our code is available online.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>频率即所需：文本掩码时考虑词频对视觉语言模型预训练的增益</div>
<div class="mono" style="margin-top:8px">若训练集规模可缩减，视觉语言模型（VLM）的训练效率将得以提升。近期研究表明，在VLM训练中采用多种文本掩码策略（截断、随机掩码、块掩码及句法掩码）具有显著效益，其中句法掩码表现最佳。本文通过分析不同文本掩码策略对训练数据词频分布的影响，揭示该影响与模型性能存在关联。基于此发现，我们提出基于词频掩码的对比语言-图像预训练方法（CLIPF），该方法直接利用词频信息进行掩码。大量实验证明，CLIPF在输入标记数减少时，性能优于句法掩码及其他现有方法。研究还发现，当训练周期足够时，不仅CLIPF，其他现有掩码策略也能超越句法掩码，这一结论对VLM训练中文本掩码方法的选择具有重要实践意义。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how text masking strategies affect word frequency distributions during vision-language model pre-training, finding that this impact correlates with model performance. The authors propose Contrastive Language-Image Pre-training with Word Frequency Masking (CLIPF), a method that directly utilizes word frequency information for masking. Experiments show CLIPF outperforms prior approaches like syntax masking, especially with fewer input tokens, and reveal that with sufficient training epochs, other existing masking strategies can also surpass syntax masking.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在视觉语言模型预训练中，文本掩码策略如何影响词频分布，并发现这种影响与模型性能相关。作者提出了CLIPF，一种直接利用词频的掩码方法，并通过大量实验证明，该方法在输入标记减少时优于语法掩码及其他现有方法。研究还表明，在足够训练轮次下，其他掩码策略也能超越语法掩码，这为VLM训练中选择文本掩码方法提供了实用指导。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
