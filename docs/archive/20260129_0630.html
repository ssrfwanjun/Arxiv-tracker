<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-29 06:30</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260129_0630</div>
    <div class="row"><div class="card">
<div class="title">VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</div>
<div class="meta-line">Authors: Dominic Maggio, Luca Carlone</div>
<div class="meta-line">First: 2026-01-27T18:54:29+00:00 · Latest: 2026-01-27T18:54:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19887v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19887v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VGGT-SLAM 2.0：实时密集前馈场景重建</div>
<div class="mono" style="margin-top:8px">我们提出VGGT-SLAM 2.0，一种实时RGB前馈SLAM系统，显著改进了基于VGGT增量对齐子地图的VGGT-SLAM。首先，通过设计新的因子图，我们消除了VGGT-SLAM中的高维15自由度漂移和平面退化问题，同时解决了在相机内参未知情况下VGGT的重建模糊性。其次，通过研究VGGT的注意力层，我们发现其中一层无需额外训练即可有效辅助图像检索验证，既能拒绝误匹配，又能完成更多闭环检测。最后，我们进行了一系列实验，包括展示VGGT-SLAM 2.0可轻松适配开放集目标检测，并在搭载Jetson Thor的地面机器人上实现在线实时运行。测试环境涵盖杂乱室内公寓、办公室场景至4200平方英尺的仓库，同时实验表明VGGT-SLAM 2.0在TUM数据集上达到最高精度，位姿误差较VGGT-SLAM降低约23%。代码将在论文发表时开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the real-time dense scene reconstruction capabilities of the original VGGT-SLAM system by addressing its issues with high-dimensional drift, planar degeneracy, and reconstruction ambiguity under unknown camera intrinsics. The method introduces a new factor graph design to correct these alignment problems and leverages an existing attention layer within the VGGT model to improve image retrieval verification for more reliable loop closure without requiring additional training. Experimental results demonstrate that VGGT-SLAM 2.0 achieves real-time performance on a ground robot, operates effectively in diverse environments from indoor spaces to large barns, and reduces pose error by approximately 23% compared to its predecessor on the TUM dataset, while also showing adaptability for open-set object detection.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过改进先前的VGGT-SLAM系统，解决其高维漂移、平面退化以及在未知相机内参下的重建模糊性等局限，以提升实时稠密场景重建的性能。方法上，设计了一种新的因子图来校正漂移和退化，并利用VGGT模型中现有的注意力层来增强图像检索验证，从而无需额外训练即可改善闭环检测。实验结果表明，该系统在Jetson Thor平台上实现了地面机器人的实时运行，在从杂乱室内到大型谷仓等多种环境中均能有效工作，并在TUM数据集上比VGGT-SLAM减少了约23%的位姿误差，同时展示了其对开放集物体检测的适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">Estimating Trust in Human-Robot Collaboration through Behavioral Indicators and Explainability</div>
<div class="meta-line">Authors: Giulio Campagna, Marta Lagomarsino, Marta Lorenzini, Dimitrios Chrysostomou, Matthias Rehm, Arash Ajoudani</div>
<div class="meta-line">Venue: IEEE Robotics and Automation Letters (Volume: 10, Issue: 10, October 2025)</div>
<div class="meta-line">First: 2026-01-27T18:12:44+00:00 · Latest: 2026-01-27T18:12:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19856v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19856v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industry 5.0 focuses on human-centric collaboration between humans and robots, prioritizing safety, comfort, and trust. This study introduces a data-driven framework to assess trust using behavioral indicators. The framework employs a Preference-Based Optimization algorithm to generate trust-enhancing trajectories based on operator feedback. This feedback serves as ground truth for training machine learning models to predict trust levels from behavioral indicators. The framework was tested in a chemical industry scenario where a robot assisted a human operator in mixing chemicals. Machine learning models classified trust with over 80\% accuracy, with the Voting Classifier achieving 84.07\% accuracy and an AUC-ROC score of 0.90. These findings underscore the effectiveness of data-driven methods in assessing trust within human-robot collaboration, emphasizing the valuable role behavioral indicators play in predicting the dynamics of human trust.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于行为指标与可解释性评估人机协作中的信任度</div>
<div class="mono" style="margin-top:8px">工业5.0聚焦以人为中心的人机协作，强调安全、舒适与信任。本研究提出一种数据驱动框架，通过行为指标评估信任度。该框架采用基于偏好的优化算法，根据操作员反馈生成增强信任的轨迹。该反馈作为训练机器学习模型的真实标签，以从行为指标预测信任水平。框架在化工行业场景中测试，机器人协助操作员混合化学品。机器学习模型对信任的分类准确率超过80%，其中投票分类器达到84.07%的准确率及0.90的AUC-ROC分数。这些发现凸显了数据驱动方法在评估人机协作信任度方面的有效性，并强调了行为指标在预测人类信任动态中的重要作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to quantify trust in Industry 5.0&#x27;s human-robot collaboration, where safety and comfort are prioritized. The authors propose a data-driven framework that first uses a Preference-Based Optimization algorithm to generate robot trajectories that enhance trust based on direct operator feedback; this feedback then serves as ground truth to train machine learning models that predict trust levels from observed behavioral indicators. In a chemical mixing scenario, the models successfully classified trust with over 80% accuracy, where a Voting Classifier achieved 84.07% accuracy and an AUC-ROC score of 0.90, demonstrating the practical efficacy of using behavioral data for trust assessment.</div>
<div class="mono" style="margin-top:8px">为推进以人为中心的工业5.0协作，本研究旨在解决如何量化和增强人对机器人信任的问题。提出的数据驱动框架首先使用基于偏好的优化算法，根据操作员的直接反馈生成能提升信任的机器人轨迹，该反馈同时作为真实标签。随后训练机器学习模型，从行为指标预测信任水平。在化学品混合协作场景中，模型的分类准确率超过80%，其中投票分类器达到84.07%的准确率和0.90的AUC-ROC分数，证明了行为数据在信任评估中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">How Does Delegation in Social Interaction Evolve Over Time? Navigation with a Robot for Blind People</div>
<div class="meta-line">Authors: Rayna Hata, Masaki Kuribayashi, Allan Wang, Hironobu Takagi, Chieko Asakawa</div>
<div class="meta-line">First: 2026-01-27T18:00:00+00:00 · Latest: 2026-01-27T18:00:00+00:00</div>
<div class="meta-line">Comments: Pre-print of paper accepted into CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19851v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomy and independent navigation are vital to daily life but remain challenging for individuals with blindness. Robotic systems can enhance mobility and confidence by providing intelligent navigation assistance. However, fully autonomous systems may reduce users&#x27; sense of control, even when they wish to remain actively involved. Although collaboration between user and robot has been recognized as important, little is known about how perceptions of this relationship change with repeated use. We present a repeated exposure study with six blind participants who interacted with a navigation-assistive robot in a real-world museum. Participants completed tasks such as navigating crowds, approaching lines, and encountering obstacles. Findings show that participants refined their strategies over time, developing clearer preferences about when to rely on the robot versus act independently. This work provides insights into how strategies and preferences evolve with repeated interaction and offers design implications for robots that adapt to user needs over time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>社交互动中的委托关系如何随时间演变？——面向视障人士的机器人导航研究</div>
<div class="mono" style="margin-top:8px">自主性与独立导航对日常生活至关重要，但对视障人士而言仍具挑战。机器人系统可通过智能导航辅助提升行动能力与信心。然而，完全自主的系统可能削弱用户控制感，即使他们希望保持主动参与。尽管人机协作的重要性已被认识，但人们对这种关系认知如何随重复使用而变化仍知之甚少。本研究对六位视障参与者在真实博物馆场景中与导航辅助机器人进行重复接触实验，任务包括人群导航、排队接近和障碍应对。结果显示，参与者随时间优化策略，对何时依赖机器人或自主行动形成更明确的偏好。这项工作揭示了策略与偏好如何随重复互动演变，并为适应长期用户需求的机器人设计提供了启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need to understand how collaboration between blind users and navigation-assistive robots evolves with repeated use, as fully autonomous systems can diminish user control. The researchers conducted a repeated exposure study in a real-world museum, where six blind participants performed navigation tasks like maneuvering through crowds and around obstacles with a robot. Key findings reveal that participants refined their interaction strategies over time, developing clearer preferences for when to delegate tasks to the robot versus acting independently, offering design insights for adaptive robotic systems.</div>
<div class="mono" style="margin-top:8px">为应对在盲人辅助导航中平衡机器人自主性与用户控制权的挑战，本研究在真实博物馆环境中进行了一项重复暴露实验。六名盲人参与者与导航辅助机器人进行了多次交互，执行了如穿越人群、避开障碍等任务。实验结果表明，参与者的策略和偏好随经验而演变，形成了更清晰的关于何时依赖机器人或自主行动的决策，这为设计自适应机器人系统提供了重要启示。</div>
</details>
</div>
<div class="card">
<div class="title">HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs</div>
<div class="meta-line">Authors: Jeanne Malécot, Hamed Rahimi, Jeanne Cattoni, Marie Samson, Mouad Abrini, Mahdi Khoramshahi, Maribel Pino, Mohamed Chetouani</div>
<div class="meta-line">First: 2026-01-27T17:45:04+00:00 · Latest: 2026-01-27T17:45:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19839v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HARMONI：基于大语言模型的多用户人机交互多模态个性化框架</div>
<div class="mono" style="margin-top:8px">现有人机交互系统在多用户环境中常缺乏持续个性化与动态适应机制，限制了实际部署效果。本文提出HARMONI多模态个性化框架，利用大语言模型使社交辅助机器人能够管理长期多用户交互。该框架集成四个核心模块：(i)感知模块识别活跃说话者并提取多模态输入；(ii)世界建模模块维护环境表征与短期对话上下文；(iii)用户建模模块更新长期说话者特定档案；(iv)生成模块产生情境化且符合伦理的响应。通过在四个数据集上的广泛评估与消融研究，以及在养老院真实场景驱动的用户研究，我们证明HARMONI支持鲁棒的说话者识别、在线记忆更新和伦理对齐的个性化，在用户建模准确性、个性化质量和用户满意度方面优于基线LLM驱动方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing human-robot interaction systems in sustaining personalization and adapting dynamically in multi-user settings, this research introduces HARMONI, a multimodal personalization framework powered by large language models. The method integrates four core modules for perception, world modeling, long-term user modeling, and response generation, enabling socially assistive robots to manage interactions by maintaining speaker-specific profiles and contextual grounding. Experimental results from four datasets and a real-world nursing home user study show that HARMONI outperforms baseline LLM-driven approaches in speaker identification accuracy, personalization quality, and user satisfaction, while ensuring ethically aligned responses.</div>
<div class="mono" style="margin-top:8px">针对现有人机交互系统在多用户环境中缺乏持续个性化与动态适应能力的问题，本研究提出了HARMONI，一个基于大语言模型的多模态个性化框架。该方法整合了感知、世界建模、用户建模和响应生成四个模块，使机器人能够维护长期用户档案并生成情境化、符合伦理的回应。在四个数据集和真实养老院场景的用户研究中，实验结果表明HARMONI在说话人识别准确率、个性化质量和用户满意度方面均优于基于大语言模型的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Information-Theoretic Detection of Bimanual Interactions for Dual-Arm Robot Plan Generation</div>
<div class="meta-line">Authors: Elena Merlo, Marta Lagomarsino, Arash Ajoudani</div>
<div class="meta-line">Venue: in IEEE Robotics and Automation Letters, vol. 10, no. 5, pp. 4532-4539, May 2025</div>
<div class="meta-line">First: 2026-01-27T17:38:58+00:00 · Latest: 2026-01-27T17:38:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19832v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Programming by demonstration is a strategy to simplify the robot programming process for non-experts via human demonstrations. However, its adoption for bimanual tasks is an underexplored problem due to the complexity of hand coordination, which also hinders data recording. This paper presents a novel one-shot method for processing a single RGB video of a bimanual task demonstration to generate an execution plan for a dual-arm robotic system. To detect hand coordination policies, we apply Shannon&#x27;s information theory to analyze the information flow between scene elements and leverage scene graph properties. The generated plan is a modular behavior tree that assumes different structures based on the desired arms coordination. We validated the effectiveness of this framework through multiple subject video demonstrations, which we collected and made open-source, and exploiting data from an external, publicly available dataset. Comparisons with existing methods revealed significant improvements in generating a centralized execution plan for coordinating two-arm systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信息论的双臂交互检测及其在双臂机器人规划生成中的应用</div>
<div class="mono" style="margin-top:8px">示教编程是一种通过人类演示简化非专家机器人编程过程的策略。然而，由于手部协调的复杂性，其在双手任务中的应用尚未得到充分探索，这也阻碍了数据记录。本文提出了一种新颖的单次学习方法，通过处理双手任务演示的单个RGB视频，为双臂机器人系统生成执行计划。为检测手部协调策略，我们应用香农信息论分析场景元素间的信息流，并利用场景图特性。生成的计划是一种模块化行为树，其结构根据所需的双臂协调方式而变化。我们通过多个受试者视频演示（已收集并开源）以及外部公开数据集的数据验证了该框架的有效性。与现有方法的比较表明，本方法在生成协调双臂系统的集中式执行计划方面具有显著改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of simplifying robot programming for bimanual tasks via demonstration, which is complex due to hand coordination and data recording difficulties. The method processes a single RGB video of a human demonstration to generate an execution plan for a dual-arm robot, applying Shannon&#x27;s information theory to detect hand coordination policies by analyzing information flow between scene elements and leveraging scene graph properties, resulting in a modular behavior tree. Experimental validation using newly collected open-source subject videos and an external public dataset showed that the framework significantly improves the generation of centralized execution plans for two-arm coordination compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决通过示教简化机器人双手任务编程的挑战，该任务因手部协调和数据采集的复杂性而困难重重。该方法处理单段人类演示的RGB视频，应用香农信息论通过分析场景元素间的信息流并利用场景图属性来检测手部协调策略，从而为双机械臂生成模块化的行为树执行计划。实验验证使用了新收集的开源受试者视频和外部公开数据集，结果表明，与现有方法相比，该框架在生成用于双臂协调的集中式执行计划方面有显著改进。</div>
</details>
</div>
<div class="card">
<div class="title">Whether We Care, How We Reason: The Dual Role of Anthropomorphism and Moral Foundations in Robot Abuse</div>
<div class="meta-line">Authors: Fan Yang, Renkai Ma, Yaxin Hu, Lingyao Li</div>
<div class="meta-line">First: 2026-01-27T17:34:31+00:00 · Latest: 2026-01-27T17:34:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19826v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19826v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As robots become increasingly integrated into daily life, understanding responses to robot mistreatment carries important ethical and design implications. This mixed-methods study (N = 201) examined how anthropomorphic levels and moral foundations shape reactions to robot abuse. Participants viewed videos depicting physical mistreatment of robots varying in humanness (Spider, Twofoot, Humanoid) and completed measures assessing moral foundations, anger, and social distance. Results revealed that anthropomorphism determines whether people extend moral consideration to robots, while moral foundations shape how they reason about such consideration. Qualitative analysis revealed distinct reasoning patterns: low-progressivism individuals employed character-based judgments, while high-progressivism individuals engaged in future-oriented moral deliberation. Findings offer implications for robot design and policy communication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我们是否在意，如何推理：拟人化与道德基础在机器人虐待中的双重作用</div>
<div class="mono" style="margin-top:8px">随着机器人日益融入日常生活，理解对机器人虐待行为的反应具有重要的伦理与设计意义。本研究采用混合方法（N = 201），探讨拟人化程度与道德基础如何影响对机器人虐待的反应。参与者观看描绘不同拟人化程度机器人（蜘蛛型、双足型、人形）遭受物理虐待的视频，并完成道德基础、愤怒感与社会距离的评估。结果显示：拟人化决定人们是否对机器人给予道德关怀，而道德基础则影响其推理方式。质性分析揭示了两种推理模式：低进步主义倾向者采用基于特质的判断，高进步主义倾向者则进行面向未来的道德思辨。研究结果为机器人设计与政策传播提供了启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how people respond to robot mistreatment, motivated by the ethical and design implications of robots&#x27; growing integration into daily life. Using a mixed-methods approach with 201 participants, it examined the effects of robot anthropomorphism (Spider, Twofoot, Humanoid forms) and individual moral foundations on reactions to abuse, measured through anger, social distance, and qualitative reasoning. Key findings show that anthropomorphism primarily determines whether people grant robots moral consideration, while moral foundations shape the reasoning behind that consideration, with distinct patterns observed between individuals with low versus high progressivism.</div>
<div class="mono" style="margin-top:8px">本研究动机源于机器人日益融入社会生活，需要理解对其虐待行为的伦理反应。通过混合方法，该研究调查了机器人的拟人化程度和个体的道德基础如何影响对机器人虐待的反应。方法上，参与者（N=201）观看了针对不同拟人化程度机器人（蜘蛛形、双足形、人形）的物理虐待视频，并完成了道德基础、愤怒感和社会距离的测量。主要实验结果表明，拟人化程度主要决定人们是否对机器人给予道德关怀，而道德基础则塑造了这种关怀背后的推理方式；定性分析进一步揭示，低进步主义倾向的个体采用基于品格的判断，而高进步主义倾向的个体则进行面向未来的道德思辨。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals</div>
<div class="meta-line">Authors: Octavio Pappalardo</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T17:10:29+00:00 · Latest: 2026-01-27T17:10:29+00:00</div>
<div class="meta-line">Comments: To appear at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19810v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19810v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream tasks where solving every task zero-shot is infeasible. Such settings naturally arise when the target tasks lie outside of the pre-training distribution or when their identities are unknown to the agent. In this work, we (i) optimize for efficient multi-episode exploration and adaptation within a meta-learning framework, and (ii) guide the training curriculum with evolving estimates of the agent&#x27;s post-adaptation performance. We present ULEE, an unsupervised meta-learning method that combines an in-context learner with an adversarial goal-generation strategy that maintains training at the frontier of the agent&#x27;s capabilities. On XLand-MiniGrid benchmarks, ULEE pre-training yields improved exploration and adaptation abilities that generalize to novel objectives, environment dynamics, and map structures. The resulting policy attains improved zero-shot and few-shot performance, and provides a strong initialization for longer fine-tuning processes. It outperforms learning from scratch, DIAYN pre-training, and alternative curricula.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效探索的无监督学习：通过自设目标预训练自适应策略</div>
<div class="mono" style="margin-top:8px">无监督预训练可为强化学习智能体提供先验知识，加速下游任务学习。基于人类发展机制的研究方向关注智能体通过自主设定并追求目标进行学习，其核心挑战在于如何有效生成、选择并从中学习。本研究聚焦于广泛的下游任务分布场景——当目标任务超出预训练分布范围或任务身份未知时，零样本解决所有任务并不可行。本工作（i）在元学习框架内优化多回合探索与适应效率，（ii）通过动态评估智能体适应后表现来指导训练课程。我们提出ULEE方法：结合上下文学习器与对抗性目标生成策略的无监督元学习方法，使训练始终处于智能体能力边界。在XLand-MiniGrid基准测试中，ULEE预训练获得的探索与适应能力可泛化至新目标、环境动态和地图结构，其策略在零样本/少样本场景表现更优，并为长时微调提供优质初始化，性能显著优于从头学习、DIAYN预训练及其他课程学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling reinforcement learning agents to efficiently explore and adapt in downstream tasks that may be outside their pre-training distribution or unknown during pre-training. The proposed method, ULEE, combines unsupervised meta-learning with an adversarial goal-generation strategy that continuously pushes the agent&#x27;s learning frontier, optimizing for multi-episode exploration and adaptation. Experimental results on XLand-MiniGrid benchmarks demonstrate that ULEE pre-training enhances generalization to novel objectives, dynamics, and map structures, leading to superior zero-shot and few-shot performance, and providing a strong initialization for fine-tuning, outperforming learning from scratch, DIAYN, and other curricula.</div>
<div class="mono" style="margin-top:8px">为了在下游任务中加速强化学习，特别是在零样本求解不可行的情况下，本研究提出了ULEE，一种无监督元学习方法，通过优化多回合探索和适应能力来预训练智能体。该方法结合了上下文学习器和对抗性目标生成策略，利用智能体适应后性能的演化估计来维持其能力前沿的训练课程。在XLand-MiniGrid基准测试中的实验结果表明，ULEE预训练提升了探索和适应能力，带来了更好的零样本和少样本性能，并为微调过程提供了更强的初始化，其表现优于从头学习、DIAYN预训练以及其他课程学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Reimagining Social Robots as Recommender Systems: Foundations, Framework, and Applications</div>
<div class="meta-line">Authors: Jin Huang, Fethiye Irmak Doğan, Hatice Gunes</div>
<div class="meta-line">First: 2026-01-27T16:25:56+00:00 · Latest: 2026-01-27T16:25:56+00:00</div>
<div class="meta-line">Comments: HRI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19761v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19761v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Personalization in social robots refers to the ability of the robot to meet the needs and/or preferences of an individual user. Existing approaches typically rely on large language models (LLMs) to generate context-aware responses based on user metadata and historical interactions or on adaptive methods such as reinforcement learning (RL) to learn from users&#x27; immediate reactions in real time. However, these approaches fall short of comprehensively capturing user preferences-including long-term, short-term, and fine-grained aspects-, and of using them to rank and select actions, proactively personalize interactions, and ensure ethically responsible adaptations. To address the limitations, we propose drawing on recommender systems (RSs), which specialize in modeling user preferences and providing personalized recommendations. To ensure the integration of RS techniques is well-grounded and seamless throughout the social robot pipeline, we (i) align the paradigms underlying social robots and RSs, (ii) identify key techniques that can enhance personalization in social robots, and (iii) design them as modular, plug-and-play components. This work not only establishes a framework for integrating RS techniques into social robots but also opens a pathway for deep collaboration between the RS and HRI communities, accelerating innovation in both fields.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将社交机器人重构为推荐系统：基础、框架与应用</div>
<div class="mono" style="margin-top:8px">社交机器人中的个性化指机器人满足个体用户需求和/或偏好的能力。现有方法通常依赖大型语言模型（LLM）基于用户元数据和历史交互生成情境感知响应，或采用强化学习（RL）等自适应方法从用户实时反应中学习。然而，这些方法未能全面捕捉用户偏好（包括长期、短期及细粒度层面），也未能利用这些偏好进行行动排序与选择、主动个性化交互及确保符合伦理的适应性调整。为突破这些局限，我们建议借鉴专门用于建模用户偏好并提供个性化推荐的推荐系统（RS）。为确保RS技术在整个社交机器人流程中实现有据可依且无缝的整合，我们（i）对齐社交机器人与RS的基础范式，（ii）识别能增强社交机器人个性化的关键技术，（iii）将其设计为模块化即插即用组件。本研究不仅建立了将RS技术整合至社交机器人的框架，更为RS与人机交互（HRI）领域的深度协作开辟道路，加速两个领域的创新进程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitations of existing personalization approaches in social robots, which struggle to comprehensively model user preferences and proactively rank actions, this work proposes integrating recommender systems (RSs) that specialize in user preference modeling. The method involves aligning the paradigms of social robots and RSs, identifying key RS techniques for enhancement, and designing them as modular components. The framework establishes a foundation for applying RS techniques to improve personalization in social robots and fosters collaboration between the RS and human-robot interaction communities.</div>
<div class="mono" style="margin-top:8px">本研究针对当前社交机器人个性化方法的局限性，这些方法通常无法全面捕捉和利用用户的长期、短期及细粒度偏好，以实现主动且符合伦理的自适应。所提出的方法将社交机器人重新构想为推荐系统，通过对齐两者的基础范式、识别关键的推荐系统增强技术，并将其设计为模块化、即插即用的组件，以无缝集成到机器人流程中。这项工作建立了一个集成框架，旨在通过推荐系统与人机交互社区之间更深度的合作来加速两个领域的创新。</div>
</details>
</div>
<div class="card">
<div class="title">SCOPE: Smooth Convex Optimization for Planned Evolution of Deformable Linear Objects</div>
<div class="meta-line">Authors: Ali Jnadi, Hadi Salloum, Yaroslav Kholodov, Alexander Gasnikov, Karam Almaghout</div>
<div class="meta-line">First: 2026-01-27T16:01:56+00:00 · Latest: 2026-01-27T16:01:56+00:00</div>
<div class="meta-line">Comments: Proceedings of Machine Learning Research tbd:1_13, 2025 International Conference on Computational Optimization</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19742v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19742v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SCOPE, a fast and efficient framework for modeling and manipulating deformable linear objects (DLOs). Unlike conventional energy-based approaches, SCOPE leverages convex approximations to significantly reduce computational cost while maintaining smooth and physically plausible deformations. This trade-off between speed and accuracy makes the method particularly suitable for applications requiring real-time or near-real-time response. The effectiveness of the proposed framework is demonstrated through comprehensive simulation experiments, highlighting its ability to generate smooth shape trajectories under geometric and length constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCOPE：可变形线性物体规划演化的平滑凸优化方法</div>
<div class="mono" style="margin-top:8px">本文提出SCOPE框架，用于快速高效地建模与操控可变形线性物体。与传统基于能量的方法不同，SCOPE采用凸近似技术，在保持平滑且物理可信变形的同时显著降低计算成本。这种速度与精度的平衡使该方法特别适用于需要实时或近实时响应的应用场景。通过综合仿真实验验证了该框架的有效性，突显其在几何与长度约束下生成平滑形状轨迹的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable fast and physically plausible manipulation of deformable linear objects (DLOs) for real-time applications, this work introduces SCOPE, a framework that replaces conventional energy-based models with convex approximations to drastically reduce computational cost. The method efficiently generates smooth deformation trajectories while respecting geometric and length constraints. Simulation experiments confirm that SCOPE achieves a favorable trade-off, providing smooth shape trajectories with significantly improved speed compared to traditional approaches.</div>
<div class="mono" style="margin-top:8px">本研究针对可变形线性物体的建模与操控中的计算挑战，传统基于能量的方法通常速度过慢，难以满足实时应用需求。提出的SCOPE框架采用凸近似方法来模拟物体变形，在保持平滑合理形变轨迹的同时，以部分物理精度为代价显著降低了计算成本。综合仿真实验结果表明，SCOPE能够在几何和长度约束下有效生成平滑的形变路径，达到了机器人实时或近实时操控任务所需的速度要求。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Exploration for High-Dimensional Continuous Control via Value-Guided Flow</div>
<div class="meta-line">Authors: Yunyue Wei, Chenhui Zuo, Yanan Sui</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T15:30:10+00:00 · Latest: 2026-01-27T15:30:10+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Controlling high-dimensional systems in biological and robotic applications is challenging due to expansive state-action spaces, where effective exploration is critical. Commonly used exploration strategies in reinforcement learning are largely undirected with sharp degradation as action dimensionality grows. Many existing methods resort to dimensionality reduction, which constrains policy expressiveness and forfeits system flexibility. We introduce Q-guided Flow Exploration (Qflex), a scalable reinforcement learning method that conducts exploration directly in the native high-dimensional action space. During training, Qflex traverses actions from a learnable source distribution along a probability flow induced by the learned value function, aligning exploration with task-relevant gradients rather than isotropic noise. Our proposed method substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional continuous-control benchmarks. Qflex also successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency in very high-dimensional settings. Our results indicate that value-guided flows offer a principled and practical route to exploration at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于价值引导流的高维连续控制可扩展探索方法</div>
<div class="mono" style="margin-top:8px">在生物与机器人应用中，高维系统的控制因庞大的状态-动作空间而极具挑战，其中高效探索至关重要。强化学习中常用的探索策略大多缺乏方向性，且随动作维度增加性能急剧下降。现有方法多采用降维技术，但这会限制策略表达能力并牺牲系统灵活性。本文提出Q引导流探索（Qflex），这是一种可在原生高维动作空间直接进行探索的可扩展强化学习方法。训练过程中，Qflex沿习得价值函数诱导的概率流，从可学习的源分布中遍历动作，使探索与任务相关梯度而非各向同性噪声对齐。该方法在多种高维连续控制基准测试中显著优于代表性在线强化学习基线。Qflex还成功控制了全身人体肌肉骨骼模型以执行敏捷复杂动作，在超高维场景中展现出卓越的可扩展性与样本效率。结果表明，价值引导流为大规模探索提供了原则性且实用的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of effective exploration in high-dimensional continuous control tasks, where traditional reinforcement learning methods suffer from undirected exploration and performance degradation as action dimensionality increases. The proposed method, Q-guided Flow Exploration (Qflex), performs exploration directly in the native high-dimensional action space by guiding actions from a learnable source distribution along a probability flow induced by the learned value function, thereby aligning exploration with task-relevant gradients. Experimental results show that Qflex substantially outperforms representative online reinforcement learning baselines across diverse high-dimensional benchmarks and successfully controls a full-body human musculoskeletal model to perform agile, complex movements, demonstrating superior scalability and sample efficiency.</div>
<div class="mono" style="margin-top:8px">本研究针对高维连续控制任务中探索效率低下的问题，传统强化学习方法在动作维度增加时存在探索无方向性和性能下降的挑战。作者提出了Q引导流探索方法，该方法通过从可学习的源分布生成动作，并沿价值函数诱导的概率流进行传输，从而在高维原生动作空间直接进行探索，使探索方向与任务相关梯度对齐。实验结果表明，该方法在多种高维基准测试中显著优于代表性在线强化学习基线，并成功控制全身人体肌肉骨骼模型执行敏捷复杂运动，展现了卓越的可扩展性和样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">LangForce: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-27T14:51:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose LangForce, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, LangForce significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LangForce：基于潜在动作查询的视觉语言动作模型贝叶斯分解方法</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在机器人操作任务中展现出潜力，但常难以泛化至新指令或复杂多任务场景。本文指出当前训练范式存在根本缺陷：目标驱动的数据收集导致数据集偏差，其中仅凭视觉观测即可高度预测语言指令，造成指令与动作间条件互信息消失的“信息坍缩”现象。这使模型退化为忽略语言约束的纯视觉策略，在分布外（OOD）场景中失效。为此，我们提出LangForce框架，通过贝叶斯分解强化指令跟随能力。该框架引入可学习的潜在动作查询，构建双分支架构分别估计纯视觉先验$p(a \mid v)$与语言条件后验$π(a \mid v, \ell)$，并通过最大化动作与指令间条件点互信息（PMI）优化策略。该目标有效抑制视觉捷径，奖励显式解释语言指令的动作。在不需新数据的情况下，LangForce显著提升泛化能力。在SimplerEnv和RoboCasa的广泛实验中取得显著效果，其中在挑战性OOD基准SimplerEnv上提升11.3%，验证了本方法在动作中鲁棒锚定语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions are predictable from visual context, leading to Information Collapse where models ignore language and act as vision-only policies. To counteract this, LangForce introduces a Bayesian decomposition framework using learnable Latent Action Queries to separately model a vision-only prior and a language-conditioned posterior, then optimizes the policy to maximize the conditional Pointwise Mutual Information between actions and instructions, thereby penalizing the vision shortcut. Experiments on SimplerEnv and RoboCasa show the method substantially improves generalization without new data, achieving an 11.3% performance gain on an out-of-distribution benchmark.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差而泛化能力不足，其中语言指令可仅从视觉上下文预测，导致信息坍缩并使模型退化为纯视觉策略。为解决此问题，LangForce通过贝叶斯分解强制指令跟随，引入潜在动作查询来估计纯视觉先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息。在SimplerEnv和RoboCasa上的实验表明泛化性能显著提升，包括在分布外基准测试上获得11.3%的改进，验证了该方法能鲁棒地将语言信息落实到动作中。</div>
</details>
</div>
<div class="card">
<div class="title">Problem Space Transformations for Out-of-Distribution Generalisation in Behavioural Cloning</div>
<div class="meta-line">Authors: Kiran Doshi, Marco Bagatella, Stelian Coros</div>
<div class="meta-line">First: 2024-11-06T17:05:58+00:00 · Latest: 2026-01-27T14:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.04056v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.04056v3">PDF</a> · <a href="https://github.com/kirandoshi/pst_ood_gen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The combination of behavioural cloning and neural networks has driven significant progress in robotic manipulation. As these algorithms may require a large number of demonstrations for each task of interest, they remain fundamentally inefficient in complex scenarios, in which finite datasets can hardly cover the state space. One of the remaining challenges is thus out-of-distribution (OOD) generalisation, i.e. the ability to predict correct actions for states with a low likelihood with respect to the state occupancy induced by the dataset. This issue is aggravated when the system to control is treated as a black-box, ignoring its physical properties. This work highlights widespread properties of robotic manipulation, specifically pose equivariance and locality. We investigate the effect of the choice of problem space on OOD performance of BC policies and how transformations arising from characteristic properties of manipulation can be employed for its improvement. Through controlled, simulated and real-world experiments, we empirically demonstrate that these transformations allow behaviour cloning policies, using either standard MLP-based one-step action prediction or diffusion-based action-sequence prediction, to generalise better to certain OOD problem instances. Code is available at https://github.com/kirandoshi/pst_ood_gen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为克隆中用于分布外泛化的问题空间变换</div>
<div class="mono" style="margin-top:8px">行为克隆与神经网络的结合推动了机器人操作领域的显著进展。然而，这些算法通常需要针对每个任务收集大量演示数据，在复杂场景中效率低下，因为有限的数据集难以覆盖完整状态空间。分布外泛化仍是关键挑战，即对数据集中低概率状态预测正确动作的能力。当被控系统被视为黑箱而忽略其物理特性时，这一问题更为突出。本研究聚焦机器人操作的普遍特性——姿态等变性与局部性，探讨问题空间选择对行为克隆策略分布外性能的影响，并利用操作任务的特征变换提升泛化能力。通过仿真与真实实验验证，这些变换能使基于标准MLP的单步动作预测或基于扩散模型的动作序列预测策略，在特定分布外问题实例中实现更优泛化。代码发布于https://github.com/kirandoshi/pst_ood_gen。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of out-of-distribution (OOD) generalization in behavioral cloning for robotic manipulation, where limited demonstration datasets fail to cover the state space adequately, especially when treating the system as a black-box. The method leverages inherent properties of robotic manipulation—specifically pose equivariance and locality—by applying problem space transformations to improve OOD performance. Experimental results from controlled simulations and real-world tasks show that these transformations enhance generalization for both standard MLP-based action prediction and diffusion-based action-sequence prediction models.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人模仿学习中分布外泛化的挑战，即基于有限演示数据训练的策略在面对训练数据未充分覆盖的状态时容易失效，尤其是在将系统视为黑盒的情况下。方法提出利用机器人操作固有的特性——特别是姿态等变性和局部性——来变换问题空间，从而提升策略的分布外鲁棒性。通过受控仿真和真实世界实验验证，结果表明应用这些变换能使基于标准MLP的单步动作预测器和基于扩散模型的动作序列预测器在特定分布外问题实例上实现更好的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Worker Safety in Harbors Using Quadruped Robots</div>
<div class="meta-line">Authors: Zoe Betta, Davide Corongiu, Carmine Tommaso Recchiuto, Antonio Sgorbissa</div>
<div class="meta-line">First: 2026-01-27T14:19:55+00:00 · Latest: 2026-01-27T14:19:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19643v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19643v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Infrastructure inspection is becoming increasingly relevant in the field of robotics due to its significant impact on ensuring workers&#x27; safety. The harbor environment presents various challenges in designing a robotic solution for inspection, given the complexity of daily operations. This work introduces an initial phase to identify critical areas within the port environment. Following this, a preliminary solution using a quadruped robot for inspecting these critical areas is analyzed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用四足机器人提升港口作业人员安全</div>
<div class="mono" style="margin-top:8px">基础设施检测因其对保障作业人员安全的重要影响，在机器人领域日益受到关注。港口环境的日常作业复杂性给检测机器人方案设计带来诸多挑战。本研究首先提出识别港口环境关键区域的初始阶段，随后分析采用四足机器人检测这些关键区域的初步解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to improve worker safety in harbor environments through robotic infrastructure inspection, addressing the operational complexities of such settings. The method involves an initial phase to identify critical areas within the port, followed by the analysis of a preliminary solution employing a quadruped robot for inspecting these identified zones. The experimental findings demonstrate the feasibility of using quadruped robots for targeted inspections in challenging harbor conditions, though specific performance metrics are not detailed in the abstract.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过机器人基础设施检查来提高港口环境中的工人安全，以应对此类场景的运营复杂性。方法采用两阶段方案：首先识别港口内的关键区域，然后分析使用四足机器人检查这些区域的初步解决方案。实验结果表明，在具有挑战性的港口条件下，利用四足机器人进行针对性检查是可行的，为加强安全规程奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">AC^2-VLA: Action-Context-Aware Adaptive Computation in Vision-Language-Action Models for Efficient Robotic Manipulation</div>
<div class="meta-line">Authors: Wenda Yu, Tianshi Wang, Fengling Li, Jingjing Li, Lei Zhu</div>
<div class="meta-line">First: 2026-01-27T14:10:39+00:00 · Latest: 2026-01-27T14:10:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19634v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have demonstrated strong performance in robotic manipulation, yet their closed-loop deployment is hindered by the high latency and compute cost of repeatedly running large vision-language backbones at every timestep. We observe that VLA inference exhibits structured redundancies across temporal, spatial, and depth dimensions, and that most existing efficiency methods ignore action context, despite its central role in embodied tasks. To address this gap, we propose Action-Context-aware Adaptive Computation for VLA models (AC^2-VLA), a unified framework that conditions computation on current visual observations, language instructions, and previous action states. Based on this action-centric context, AC^2-VLA adaptively performs cognition reuse across timesteps, token pruning, and selective execution of model components within a unified mechanism. To train the adaptive policy, we introduce an action-guided self-distillation scheme that preserves the behavior of the dense VLA policy while enabling structured sparsification that transfers across tasks and settings. Extensive experiments on robotic manipulation benchmarks show that AC^2-VLA achieves up to a 1.79\times speedup while reducing FLOPs to 29.4% of the dense baseline, with comparable task success.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AC^2-VLA：面向高效机器人操作的可视-语言-动作模型中的动作-上下文感知自适应计算</div>
<div class="mono" style="margin-top:8px">可视-语言-动作（VLA）模型在机器人操作中展现出强大性能，但其闭环部署因需在每个时间步重复运行大型视觉-语言骨干网络而产生的高延迟与计算成本而受阻。我们观察到，VLA推理在时间、空间和深度维度上存在结构化冗余，且现有多数效率优化方法忽视了动作上下文在具身任务中的核心作用。为填补这一空白，我们提出面向VLA模型的动作-上下文感知自适应计算框架（AC^2-VLA），该统一框架以当前视觉观测、语言指令及历史动作状态为条件进行动态计算。基于这一以动作为核心的上下文，AC^2-VLA通过统一机制自适应地实现跨时间步的认知复用、令牌剪枝及模型组件的选择性执行。为训练自适应策略，我们引入动作引导的自蒸馏方案，在保持稠密VLA策略行为的同时，实现可跨任务与场景迁移的结构化稀疏化。在机器人操作基准测试上的大量实验表明，AC^2-VLA在达到可比任务成功率的同时，实现了最高1.79倍的加速，并将计算量降至稠密基准模型的29.4%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of Vision-Language-Action (VLA) models in robotic manipulation, where repeatedly running large backbones at every timestep causes high latency and compute cost. The proposed method, AC^2-VLA, introduces a unified framework that conditions computation on visual observations, language instructions, and previous action states to adaptively perform cognition reuse, token pruning, and selective execution of model components. Experimental results on robotic manipulation benchmarks demonstrate that AC^2-VLA achieves up to a 1.79× speedup and reduces FLOPs to 29.4% of the dense baseline while maintaining comparable task success.</div>
<div class="mono" style="margin-top:8px">用于机器人操作的视觉-语言-动作模型因在每个时间步运行大型骨干网络而存在高计算延迟问题。为此，研究者提出了AC^2-VLA框架，该框架基于当前视觉观察、语言指令和先前动作状态自适应地调整计算，实现跨时间步的认知重用、令牌剪枝和模型组件的选择性执行。在机器人操作基准测试上的实验表明，该方法在保持任务成功率相当的同时，实现了最高1.79倍的加速，并将计算量降至基线模型的29.4%。</div>
</details>
</div>
<div class="card">
<div class="title">Studying the Effect of Explicit Interaction Representations on Learning Scene-level Distributions of Human Trajectories</div>
<div class="meta-line">Authors: Anna Mészáros, Javier Alonso-Mora, Jens Kober</div>
<div class="meta-line">First: 2025-11-06T14:01:47+00:00 · Latest: 2026-01-27T14:02:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04375v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.04375v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effectively capturing the joint distribution of all agents in a scene is relevant for predicting the true evolution of the scene and in turn providing more accurate information to the decision processes of autonomous vehicles. While new models have been developed for this purpose in recent years, it remains unclear how to best represent the joint distributions particularly from the perspective of the interactions between agents. Thus far there is no clear consensus on how best to represent interactions between agents; whether they should be learned implicitly from data by neural networks, or explicitly modeled using the spatial and temporal relations that are more grounded in human decision-making. This paper aims to study various means of describing interactions within the same network structure and their effect on the final learned joint distributions. Our findings show that more often than not, simply allowing a network to establish interactive connections between agents based on data has a detrimental effect on performance. Instead, having well defined interactions (such as which agent of an agent pair passes first at an intersection) can often bring about a clear boost in performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>显式交互表征对学习人类轨迹场景级分布的影响研究</div>
<div class="mono" style="margin-top:8px">有效捕捉场景中所有智能体的联合分布对于预测场景的真实演化至关重要，进而能为自动驾驶决策过程提供更准确的信息。尽管近年来已为此开发出新模型，但如何最佳表征联合分布——尤其是从智能体间交互的视角——仍不明确。目前对智能体间交互的最佳表征方式尚无明确共识：应通过神经网络从数据中隐式学习，还是采用更贴近人类决策的时空关系进行显式建模？本文旨在研究同一网络结构内描述交互的不同方式及其对最终学习到的联合分布的影响。研究发现，单纯让网络基于数据建立智能体间的交互连接往往会对性能产生负面影响；相反，明确定义的交互（如路口通行中哪一方优先通过）通常能显著提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how different representations of interactions between agents affect the learning of joint trajectory distributions in multi-agent scenes, which is crucial for accurate scene evolution prediction in autonomous driving. The study compares implicit interaction learning, where neural networks autonomously derive connections from data, with explicit modeling that incorporates spatial and temporal relations grounded in human decision-making, all within a consistent network framework. Experimental results indicate that implicit interaction learning often harms performance, whereas explicitly defined interactions, such as determining which agent passes first at an intersection, consistently lead to significant performance improvements.</div>
<div class="mono" style="margin-top:8px">本研究探讨了不同交互表示方式如何影响多智能体场景中联合轨迹分布的学习，这对于自动驾驶中准确预测场景演化至关重要。该研究在相同网络结构下，比较了神经网络隐式学习交互与基于时空关系的显式建模方法。实验结果表明，隐式学习交互通常会损害性能，而显式定义的交互（例如确定交叉路口哪个智能体优先通过）则能显著提升性能。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Exploration via Policy Priors</div>
<div class="meta-line">Authors: Manuel Wendl, Yarden As, Manish Prajapat, Anton Pollak, Stelian Coros, Andreas Krause</div>
<div class="meta-line">First: 2026-01-27T13:45:28+00:00 · Latest: 2026-01-27T13:45:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19612v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19612v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe exploration is a key requirement for reinforcement learning (RL) agents to learn and adapt online, beyond controlled (e.g. simulated) environments. In this work, we tackle this challenge by utilizing suboptimal yet conservative policies (e.g., obtained from offline data or simulators) as priors. Our approach, SOOPER, uses probabilistic dynamics models to optimistically explore, yet pessimistically fall back to the conservative policy prior if needed. We prove that SOOPER guarantees safety throughout learning, and establish convergence to an optimal policy by bounding its cumulative regret. Extensive experiments on key safe RL benchmarks and real-world hardware demonstrate that SOOPER is scalable, outperforms the state-of-the-art and validate our theoretical guarantees in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略先验的安全探索</div>
<div class="mono" style="margin-top:8px">安全探索是强化学习智能体在受控环境（如仿真环境）之外进行在线学习与适应的关键需求。本研究通过利用次优但保守的策略（例如从离线数据或模拟器中获得）作为先验来应对这一挑战。我们提出的SOOPER方法采用概率动力学模型进行乐观探索，同时在必要时悲观地回退至保守策略先验。我们证明SOOPER能在整个学习过程中保障安全性，并通过界定其累积遗憾来确保收敛至最优策略。在关键安全强化学习基准和实际硬件上的大量实验表明，SOOPER具备可扩展性，性能优于现有最优方法，并在实践中验证了我们的理论保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable safe online reinforcement learning beyond simulated environments, this work leverages suboptimal but conservative policies as priors. The proposed method, SOOPER, employs probabilistic dynamics models to explore optimistically while pessimistically reverting to the prior policy when necessary to ensure safety. Experiments on safe RL benchmarks and real-world hardware show that SOOPER is scalable, outperforms state-of-the-art methods, and validates its theoretical safety and convergence guarantees.</div>
<div class="mono" style="margin-top:8px">为使强化学习能在模拟环境之外安全地进行在线学习，本研究提出了SOOPER方法，它利用来自离线数据或模拟器的次优但保守的策略先验。该方法采用概率动力学模型进行乐观探索，同时允许在需要时悲观地回退到先验策略以确保安全性。在安全强化学习基准和真实硬件上的实验表明，SOOPER具有可扩展性，性能优于现有先进方法，并验证了其理论上的安全性和收敛性保证。</div>
</details>
</div>
<div class="card">
<div class="title">Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation</div>
<div class="meta-line">Authors: Ojasva Mishra, Xiaolong Wu, Min Xu</div>
<div class="meta-line">First: 2026-01-26T16:11:05+00:00 · Latest: 2026-01-27T13:24:25+00:00</div>
<div class="meta-line">Comments: Pending IEEE Transactions on Robotics Publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18639v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18639v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>考虑执行器饱和约束的机器人关节离散时间PID增益优化控制</div>
<div class="mono" style="margin-top:8px">旋转驱动的精确调节是自主机器人技术的基础，然而实际PID控制回路因离散时间执行、执行器饱和以及微小延迟和测量误差而偏离连续时间理论。本文提出一种面向饱和离散时间关节控制的实现感知分析与整定流程：首先利用朱里判据推导欧拉和精确零阶保持离散化下的PI稳定性区域；其次评估饱和主导工况下的离散反向计算抗饱和实现方案；最后提出一种混合认证贝叶斯优化流程，在优化具有超调与饱和占空比软惩罚的鲁棒IAE指标同时，筛选解析不稳定候选参数和行为不安全暂态过程。基准扫描（τ=1.0秒，Δt=0.01秒，u∈[-10,10]）量化了P/PI/PID的上升/稳定趋势。在模拟不确定性、延迟、噪声、量化及更严格饱和的随机模型族下，鲁棒导向整定将IAE中位数从0.843提升至0.430，同时保持超调中位数低于2%。在纯仿真整定中，认证筛选机制可在完整鲁棒评估前剔除边界内11.6%的随机采样增益，无需硬件实验即提升了采样效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the practical challenges in robotic joint control where discrete-time execution, actuator saturation, and small delays cause PID controllers to deviate from ideal continuous-time theory. The authors develop an implementation-aware tuning workflow that analytically derives PI stability regions under Euler and exact zero-order-hold discretizations, evaluates a discrete anti-windup scheme, and proposes a hybrid-certified Bayesian optimization method that screens for instability and unsafe transients while optimizing a robust integral absolute error objective. Experimental results show that, under a randomized model family emulating real-world uncertainties, the robustness-oriented tuning improves the median IAE from 0.843 to 0.430 while maintaining median overshoot below 2%, and the certification screen preemptively rejects 11.6% of sampled gains to improve optimization efficiency.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人关节控制中因离散时间执行、执行器饱和及微小延迟导致PID控制环偏离理想连续时间理论的实践挑战。方法包括使用Jury判据推导不同离散化下的PI稳定性区域，评估离散抗饱和方案，并提出一种混合认证的贝叶斯优化流程，该流程在优化鲁棒积分绝对误差目标的同时筛选不稳定和不安全瞬态。实验结果表明，在随机模型族上进行鲁棒性导向的调参将中位数IAE从0.843改善至0.430，同时保持中位数超调低于2%，且认证筛选在仿真中拒绝了11.6%的采样增益，提高了样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">The S3LI Vulcano Dataset: A Dataset for Multi-Modal SLAM in Unstructured Planetary Environments</div>
<div class="meta-line">Authors: Riccardo Giubilato, Marcus Gerhard Müller, Marco Sewtz, Laura Alejandra Encinar Gonzalez, John Folkesson, Rudolph Triebel</div>
<div class="meta-line">First: 2026-01-27T12:49:40+00:00 · Latest: 2026-01-27T12:49:40+00:00</div>
<div class="meta-line">Comments: Accepted submission to the 2026 IEEE Aerospace Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19557v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19557v1">PDF</a> · <a href="http://github.com/DLR-RM/s3li-toolkit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We release the S3LI Vulcano dataset, a multi-modal dataset towards development and benchmarking of Simultaneous Localization and Mapping (SLAM) and place recognition algorithms that rely on visual and LiDAR modalities. Several sequences are recorded on the volcanic island of Vulcano, from the Aeolian Islands in Sicily, Italy. The sequences provide users with data from a variety of environments, textures and terrains, including basaltic or iron-rich rocks, geological formations from old lava channels, as well as dry vegetation and water. The data (rmc.dlr.de/s3li_dataset) is accompanied by an open source toolkit (github.com/DLR-RM/s3li-toolkit) providing tools for generating ground truth poses as well as preparation of labelled samples for place recognition tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S3LI Vulcano数据集：面向非结构化行星环境的多模态SLAM数据集</div>
<div class="mono" style="margin-top:8px">我们发布了S3LI Vulcano数据集，这是一个面向依赖视觉与激光雷达模态的同步定位与建图（SLAM）及地点识别算法开发与基准测试的多模态数据集。该数据集在意大利西西里埃奥利群岛的武尔卡诺火山岛采集了多条序列，为用户提供涵盖多种环境、纹理与地形特征的数据，包括玄武岩/富铁岩石、古熔岩通道地质构造、干燥植被及水域。数据集（rmc.dlr.de/s3li_dataset）配套开源工具包（github.com/DLR-RM/s3li-toolkit），提供生成真实轨迹位姿的工具及面向地点识别任务的标注样本制备功能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To support the development and benchmarking of multi-modal SLAM and place recognition algorithms for unstructured planetary-like environments, this work introduces the S3LI Vulcano dataset. The method involves collecting sequences on the volcanic island of Vulcano, capturing diverse visual and LiDAR data from varied terrains such as basaltic rocks, old lava channels, dry vegetation, and water. The main experimental outcome is the release of this dataset, which is accompanied by an open-source toolkit for generating ground truth poses and preparing labeled samples, facilitating algorithm evaluation in challenging, texture-poor landscapes.</div>
<div class="mono" style="margin-top:8px">本研究旨在为无结构行星类似环境中的SLAM与位置识别算法提供多模态数据集，以推动其发展，因为现有数据集常缺乏此类环境特有的纹理与地形。方法涉及收集并发布S3LI Vulcano数据集，该数据集包含在意大利武尔卡诺火山岛使用视觉和激光雷达传感器记录的序列，捕捉了玄武岩、古老熔岩通道、干燥植被和水体等多种环境。主要实验成果包括数据集本身以及配套的开源工具包，该工具包可用于生成真实轨迹位姿和准备标注样本，从而为机器人学社区的发展与基准测试提供支持。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Inverse Perspective Mapping for Automatic Vectorized Road Map Generation</div>
<div class="meta-line">Authors: Hongji Liu, Linwei Zheng, Yongjian Li, Mingkai Tang, Xiaoyang Yan, Ming Liu, Jun Ma</div>
<div class="meta-line">First: 2026-01-27T12:27:27+00:00 · Latest: 2026-01-27T12:27:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19536v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19536v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this study, we present a low-cost and unified framework for vectorized road mapping leveraging enhanced inverse perspective mapping (IPM). In this framework, Catmull-Rom splines are utilized to characterize lane lines, and all the other ground markings are depicted using polygons uniformly. The results from instance segmentation serve as references to refine the three-dimensional position of spline control points and polygon corner points. In conjunction with this process, the homography matrix of IPM and vehicle poses are optimized simultaneously. Our proposed framework significantly reduces the mapping errors associated with IPM. It also improves the accuracy of the initial IPM homography matrix and the predicted vehicle poses. Furthermore, it addresses the limitations imposed by the coplanarity assumption in IPM. These enhancements enable IPM to be effectively applied to vectorized road mapping, which serves a cost-effective solution with enhanced accuracy. In addition, our framework generalizes road map elements to include all common ground markings and lane lines. The proposed framework is evaluated in two different practical scenarios, and the test results show that our method can automatically generate high-precision maps with near-centimeter-level accuracy. Importantly, the optimized IPM matrix achieves an accuracy comparable to that of manual calibration, while the accuracy of vehicle poses is also significantly improved.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>增强逆透视映射以实现自动矢量化道路地图生成</div>
<div class="mono" style="margin-top:8px">本研究提出了一种低成本、统一的矢量化道路制图框架，利用增强的逆透视映射（IPM）。该框架采用Catmull-Rom样条表征车道线，并统一使用多边形描绘其他地面标记。实例分割结果作为参考，用于优化样条控制点和多边形角点的三维位置。在此过程中，IPM的单应性矩阵与车辆位姿同步优化。所提框架显著降低了IPM相关的制图误差，提升了初始IPM单应性矩阵和预测车辆位姿的精度，同时克服了IPM共面假设的局限性。这些改进使IPM能有效应用于矢量化道路制图，成为兼具成本效益与高精度的解决方案。此外，本框架将地图要素泛化至涵盖所有常见地面标记与车道线。通过在两种实际场景中的评估，测试结果表明该方法能自动生成接近厘米级精度的高精度地图。值得注意的是，优化后的IPM矩阵精度可达人工校准水平，车辆位姿精度亦得到显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to develop a low-cost, automated method for generating vectorized road maps, addressing the accuracy limitations of traditional inverse perspective mapping (IPM) due to its coplanarity assumption. The proposed framework enhances IPM by using Catmull-Rom splines for lane lines and polygons for other ground markings, then jointly optimizing the IPM homography matrix and vehicle poses using instance segmentation results to refine 3D control points. Experimental evaluation in two practical scenarios demonstrates that the method automatically generates high-precision maps with near-centimeter-level accuracy, with the optimized IPM matrix achieving manual calibration-level accuracy and vehicle pose estimates significantly improved.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种低成本、自动化的矢量化道路地图生成系统，以解决传统逆透视映射（IPM）存在的映射误差和严格的共面性假设限制。该方法提出了一个统一框架，使用Catmull-Rom样条建模车道线，使用多边形建模其他地面标记，并基于实例分割结果优化其三维位置，同时联合优化IPM单应矩阵和车辆位姿。在两个实际场景中的实验评估表明，该框架显著减少了IPM误差，实现了接近厘米级的地图精度，生成的IPM矩阵精度可与手动校准相媲美，并大幅提升了车辆位姿估计的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Rhombot: Rhombus-shaped Modular Robots for Stable, Medium-Independent Reconfiguration Motion</div>
<div class="meta-line">Authors: Jie Gu, Yirui Sun, Zhihao Xia, Tin Lun Lam, Chunxu Tian, Dan Zhang</div>
<div class="meta-line">First: 2026-01-27T12:15:10+00:00 · Latest: 2026-01-27T12:15:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19529v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19529v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we present Rhombot, a novel deformable planar lattice modular self-reconfigurable robot (MSRR) with a rhombus shaped module. Each module consists of a parallelogram skeleton with a single centrally mounted actuator that enables folding and unfolding along its diagonal. The core design philosophy is to achieve essential MSRR functionalities such as morphing, docking, and locomotion with minimal control complexity. This enables a continuous and stable reconfiguration process that is independent of the surrounding medium, allowing the system to reliably form various configurations in diverse environments. To leverage the unique kinematics of Rhombot, we introduce morphpivoting, a novel motion primitive for reconfiguration that differs from advanced MSRR systems, and propose a strategy for its continuous execution. Finally, a series of physical experiments validate the module&#x27;s stable reconfiguration ability, as well as its positional and docking accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Rhombot：菱形模块化机器人实现稳定且介质无关的重构运动</div>
<div class="mono" style="margin-top:8px">本文提出Rhombot，一种采用菱形模块的新型可变形平面晶格模块化自重构机器人。每个模块由平行四边形骨架和单个中央执行器构成，可沿对角线折叠与展开。其核心设计理念是以最小控制复杂度实现变形、对接与运动等基本功能，从而实现连续稳定的重构过程，且不受周围介质影响，使系统能在多样环境中可靠形成多种构型。基于Rhombot独特的运动学特性，我们提出区别于先进模块化自重构机器人系统的“形态轴转”新型重构运动基元，并设计其连续执行策略。最后，通过系列物理实验验证了模块的稳定重构能力及其定位与对接精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research introduces Rhombot, a rhombus-shaped modular self-reconfigurable robot designed to achieve essential functionalities like morphing, docking, and locomotion with minimal control complexity. The method employs a module with a parallelogram skeleton and a single central actuator for folding, enabling a medium-independent reconfiguration process, and introduces a novel &#x27;morphpivoting&#x27; motion primitive for continuous execution. Experimental results demonstrate the module&#x27;s stable reconfiguration ability, along with validated positional and docking accuracy in physical tests.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种模块化自重构机器人，以最小的控制复杂度实现稳定且与环境介质无关的重构。提出的Rhombot采用菱形模块，通过单个中央驱动器沿对角线折叠，实现变形、对接和移动等核心功能。物理实验验证了该系统在不同环境中的稳定重构能力、定位精度和可靠的对接性能。</div>
</details>
</div>
<div class="card">
<div class="title">PALM: Enhanced Generalizability for Local Visuomotor Policies via Perception Alignment</div>
<div class="meta-line">Authors: Ruiyu Wang, Zheyu Zhuang, Danica Kragic, Florian T. Pokorny</div>
<div class="meta-line">Venue: IEEE Robotics and Automation Letters 2026</div>
<div class="meta-line">First: 2026-01-27T11:55:23+00:00 · Latest: 2026-01-27T11:55:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19514v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalizing beyond the training domain in image-based behavior cloning remains challenging. Existing methods address individual axes of generalization, workspace shifts, viewpoint changes, and cross-embodiment transfer, yet they are typically developed in isolation and often rely on complex pipelines. We introduce PALM (Perception Alignment for Local Manipulation), which leverages the invariance of local action distributions between out-of-distribution (OOD) and demonstrated domains to address these OOD shifts concurrently, without additional input modalities, model changes, or data collection. PALM modularizes the manipulation policy into coarse global components and a local policy for fine-grained actions. We reduce the discrepancy between in-domain and OOD inputs at the local policy level by enforcing local visual focus and consistent proprioceptive representation, allowing the policy to retrieve invariant local actions under OOD conditions. Experiments show that PALM limits OOD performance drops to 8% in simulation and 24% in the real world, compared to 45% and 77% for baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PALM：通过感知对齐增强局部视觉运动策略的泛化能力</div>
<div class="mono" style="margin-top:8px">基于图像的行为克隆在训练域外的泛化仍具挑战。现有方法分别处理工作空间偏移、视角变化和跨具身迁移等单一泛化维度，但通常孤立开发且依赖复杂流程。本文提出PALM（局部操作的感知对齐），利用分布外（OOD）域与演示域间局部动作分布的不变性，在不增加输入模态、修改模型或收集数据的前提下，同步处理这些OOD偏移。PALM将操作策略模块化为粗粒度全局组件和细粒度动作的局部策略，通过强化局部视觉聚焦与一致本体表征，减少局部策略层面域内与OOD输入的差异，使策略能在OOD条件下检索不变的局部动作。实验表明：基线方法在仿真和现实中的OOD性能下降分别为45%和77%，而PALM仅下降8%和24%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generalizing image-based behavior cloning policies beyond their training domain, particularly for visuomotor manipulation tasks, where existing methods often tackle individual axes of generalization like workspace shifts or viewpoint changes in isolation with complex pipelines. The proposed method, PALM (Perception Alignment for Local Manipulation), modularizes the policy into coarse global and fine-grained local components, then reduces the discrepancy between in-domain and out-of-distribution inputs by enforcing a local visual focus and consistent proprioceptive representation to retrieve invariant local actions. Experimental results demonstrate that PALM limits performance drops to 8% in simulation and 24% in real-world settings, significantly outperforming baseline methods which showed drops of 45% and 77%, respectively.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决基于图像的行为克隆策略在训练域外泛化的挑战，现有方法通常孤立处理工作空间偏移、视角变化或跨具身迁移等单一泛化轴，且依赖复杂流程。提出的PALM方法将操作策略模块化为粗略全局组件和细粒度动作的局部策略，通过强制局部视觉聚焦和一致的本体感知表示来减少域内与分布外输入的差异，从而检索不变局部动作。实验结果表明，PALM将分布外性能下降限制在模拟环境8%和现实世界24%，而基线方法分别下降45%和77%。</div>
</details>
</div>
<div class="card">
<div class="title">ALRM: Agentic LLM for Robotic Manipulation</div>
<div class="meta-line">Authors: Vitor Gaboardi dos Santos, Ibrahim Khadraoui, Ibrahim Farhat, Hamza Yous, Samy Teffahi, Hakim Hacid</div>
<div class="meta-line">First: 2026-01-27T11:54:14+00:00 · Latest: 2026-01-27T11:54:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19510v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19510v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have recently empowered agentic frameworks to exhibit advanced reasoning and planning capabilities. However, their integration in robotic control pipelines remains limited in two aspects: (1) prior \ac{llm}-based approaches often lack modular, agentic execution mechanisms, limiting their ability to plan, reflect on outcomes, and revise actions in a closed-loop manner; and (2) existing benchmarks for manipulation tasks focus on low-level control and do not systematically evaluate multistep reasoning and linguistic variation. In this paper, we propose Agentic LLM for Robot Manipulation (ALRM), an LLM-driven agentic framework for robotic manipulation. ALRM integrates policy generation with agentic execution through a ReAct-style reasoning loop, supporting two complementary modes: Code-asPolicy (CaP) for direct executable control code generation, and Tool-as-Policy (TaP) for iterative planning and tool-based action execution. To enable systematic evaluation, we also introduce a novel simulation benchmark comprising 56 tasks across multiple environments, capturing linguistically diverse instructions. Experiments with ten LLMs demonstrate that ALRM provides a scalable, interpretable, and modular approach for bridging natural language reasoning with reliable robotic execution. Results reveal Claude-4.1-Opus as the top closed-source model and Falcon-H1-7B as the top open-source model under CaP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ALRM：面向机器人操作的智能体化大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）近期赋能智能体框架展现出高级推理与规划能力，但在机器人控制流程中的集成仍存在两方面局限：（1）现有基于LLM的方法常缺乏模块化、智能体化的执行机制，难以实现闭环式的规划、结果反思与动作修正；（2）现有操作任务基准多聚焦底层控制，未系统评估多步推理与语言多样性。本文提出面向机器人操作的智能体化大语言模型（ALRM），该LLM驱动的智能体框架通过ReAct式推理循环将策略生成与智能体执行相融合，支持两种互补模式：直接生成可执行控制代码的“代码即策略”（CaP）模式，以及迭代规划与工具化动作执行的“工具即策略”（TaP）模式。为建立系统化评估体系，我们同时构建了包含多环境56项任务的新型仿真基准，涵盖语言多样性指令。基于十种LLM的实验表明，ALRM为连接自然语言推理与可靠机器人执行提供了可扩展、可解释、模块化的解决方案。结果显示，在CaP模式下Claude-4.1-Opus为最优闭源模型，Falcon-H1-7B为最优开源模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in integrating Large Language Models (LLMs) into robotic control, where prior approaches lack modular, agentic closed-loop execution and existing benchmarks fail to systematically evaluate multistep reasoning and linguistic variation. The proposed method, Agentic LLM for Robot Manipulation (ALRM), is an LLM-driven framework that integrates policy generation with agentic execution via a ReAct-style reasoning loop, featuring two modes: Code-as-Policy for generating direct control code and Tool-as-Policy for iterative planning using tools. Experimental evaluation on a novel simulation benchmark of 56 linguistically diverse tasks shows that ALRM provides a scalable, interpretable, and modular approach, with Claude-4.1-Opus and Falcon-H1-7B identified as top-performing closed-source and open-source models, respectively, under the Code-as-Policy mode.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大语言模型在机器人应用中存在的两个局限：现有基于大语言模型的方法缺乏模块化、闭环的智能体执行机制，以及缺乏能系统评估多步推理和语言多样性的操作任务基准。提出的解决方案是用于机器人操作的智能体大语言模型（ALRM），它通过ReAct式推理循环将策略生成与智能体执行相结合，提供两种互补模式：用于生成直接控制代码的“代码即策略”模式，以及用于使用工具进行迭代规划的“工具即策略”模式。在一个包含56个语言多样化任务的新型仿真基准上的实验评估表明，ALRM提供了一个可扩展且可解释的框架，其中Claude-4.1-Opus和Falcon-H1-7B分别在“代码即策略”模式下被识别为表现最佳的闭源和开源模型。</div>
</details>
</div>
<div class="card">
<div class="title">A DVL Aided Loosely Coupled Inertial Navigation Strategy for AUVs with Attitude Error Modeling and Variance Propagation</div>
<div class="meta-line">Authors: Jin Huang, Zichen Liu, Haoda Li, Zhikun Wang, Ying Chen</div>
<div class="meta-line">First: 2026-01-27T11:53:33+00:00 · Latest: 2026-01-27T11:53:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19509v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19509v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In underwater navigation systems, strap-down inertial navigation system/Doppler velocity log (SINS/DVL)-based loosely coupled architectures are widely adopted. Conventional approaches project DVL velocities from the body coordinate system to the navigation coordinate system using SINS-derived attitude; however, accumulated attitude estimation errors introduce biases into velocity projection and degrade navigation performance during long-term operation. To address this issue, two complementary improvements are introduced. First, a vehicle attitude error-aware DVL velocity transformation model is formulated by incorporating attitude error terms into the observation equation to reduce projection-induced velocity bias. Second, a covariance matrix-based variance propagation method is developed to transform DVL measurement uncertainty across coordinate systems, introducing an expectation-based attitude error compensation term to achieve statistically consistent noise modeling. Simulation and field experiment results demonstrate that both improvements individually enhance navigation accuracy and confirm that accumulated attitude errors affect both projected velocity measurements and their associated uncertainty. When jointly applied, long-term error divergence is effectively suppressed. Field experimental results show that the proposed approach achieves a 78.3% improvement in 3D position RMSE and a 71.8% reduction in the maximum component-wise position error compared with the baseline IMU+DVL method, providing a robust solution for improving long-term SINS/DVL navigation performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向AUV的DVL辅助松耦合惯性导航策略：姿态误差建模与方差传播</div>
<div class="mono" style="margin-top:8px">在水下导航系统中，基于捷联惯性导航系统/多普勒计程仪（SINS/DVL）的松耦合架构被广泛采用。传统方法利用SINS解算的姿态将DVL速度从载体坐标系投影至导航坐标系，但长期运行中累积的姿态估计误差会引入速度投影偏差，导致导航性能下降。针对此问题，本文提出两项互补改进：首先，通过将姿态误差项引入观测方程，建立载体姿态误差感知的DVL速度转换模型，以降低投影引起的速度偏差；其次，开发基于协方差矩阵的方差传播方法，实现DVL测量不确定性在坐标系间的转换，并引入基于期望的姿态误差补偿项以建立统计一致的噪声模型。仿真与实地实验结果表明，两项改进均能独立提升导航精度，并证实累积姿态误差同时影响投影速度测量值及其关联不确定性。联合应用时能有效抑制长期误差发散。实地实验数据显示，相较于基准IMU+DVL方法，所提方案的三维位置均方根误差改善78.3%，最大分量位置误差降低71.8%，为提升SINS/DVL长期导航性能提供了稳健解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To mitigate the degradation of long-term navigation accuracy in SINS/DVL systems caused by accumulated attitude errors during DVL velocity projection, this work proposes two complementary improvements. The method introduces an attitude error-aware DVL velocity transformation model and a covariance-based variance propagation technique that incorporates an expectation-based compensation term for statistically consistent noise modeling. Experimental results demonstrate that both enhancements individually improve accuracy and, when combined, effectively suppress long-term error divergence, achieving a 78.3% reduction in 3D position RMSE and a 71.8% decrease in maximum component-wise position error compared to the baseline.</div>
<div class="mono" style="margin-top:8px">本研究针对自主水下航行器（AUV）采用松耦合SINS/DVL导航系统时，惯性导航系统累积的姿态估计误差导致多普勒计程仪（DVL）速度向导航坐标系投影产生偏差，从而降低长期导航精度的问题。提出的方法包含两项关键改进：一是建立一种载体姿态误差感知的DVL速度转换模型，将姿态误差项纳入观测方程以减少投影偏差；二是开发一种基于协方差矩阵的方差传播方法，通过引入基于期望的姿态误差补偿项，实现跨坐标系DVL测量不确定性的统计一致噪声建模。仿真和现场实验结果表明，两项改进各自提升了导航精度，联合应用能有效抑制长期误差发散，与基线IMU+DVL方法相比，所提方法实现了三维位置均方根误差78.3%的提升和最大分量位置误差71.8%的降低。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Goal-Reaching Control with Guaranteed Lyapunov-Like Stabilizer for Mobile Robots</div>
<div class="meta-line">Authors: Mehdi Heydari Shahna, Seyed Adel Alizadeh Kolagar, Jouni Mattila</div>
<div class="meta-line">First: 2026-01-27T11:35:00+00:00 · Latest: 2026-01-27T11:35:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19499v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19499v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) can be highly effective at learning goal-reaching policies, but it typically does not provide formal guarantees that the goal will always be reached. A common approach to provide formal goal-reaching guarantees is to introduce a shielding mechanism that restricts the agent to actions that satisfy predefined safety constraints. The main challenge here is integrating this mechanism with RL so that learning and exploration remain effective without becoming overly conservative. Hence, this paper proposes an RL-based control framework that provides formal goal-reaching guarantees for wheeled mobile robots operating in unstructured environments. We first design a real-time RL policy with a set of 15 carefully defined reward terms. These rewards encourage the robot to reach both static and dynamic goals while generating sufficiently smooth command signals that comply with predefined safety specifications, which is critical in practice. Second, a Lyapunov-like stabilizer layer is integrated into the benchmark RL framework as a policy supervisor to formally strengthen the goal-reaching control while preserving meaningful exploration of the state action space. The proposed framework is suitable for real-time deployment in challenging environments, as it provides a formal guarantee of convergence to the intended goal states and compensates for uncertainties by generating real-time control signals based on the current state, while respecting real-world motion constraints. The experimental results show that the proposed Lyapunov-like stabilizer consistently improves the benchmark RL policies, boosting the goal-reaching rate from 84.6% to 99.0%, sharply reducing failures, and improving efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>移动机器人强化学习目标到达控制与保证李雅普诺夫类稳定器</div>
<div class="mono" style="margin-top:8px">强化学习（RL）在学习目标到达策略方面非常有效，但通常无法提供目标必然达成的形式化保证。提供形式化目标到达保证的常见方法是引入屏蔽机制，限制智能体仅执行满足预设安全约束的动作。主要挑战在于将该机制与RL结合，使学习和探索保持高效而不至于过于保守。为此，本文提出一种基于RL的控制框架，为在非结构化环境中运行的轮式移动机器人提供形式化目标到达保证。首先，我们设计了一个包含15项精心定义奖励项的实时RL策略，这些奖励鼓励机器人同时到达静态和动态目标，并生成符合预设安全规范的足够平滑指令信号，这在实际应用中至关重要。其次，将李雅普诺夫类稳定器层集成至基准RL框架中作为策略监督器，以形式化强化目标到达控制，同时保持对状态动作空间的有意义探索。该框架适用于在挑战性环境中实时部署，既能提供向目标状态收敛的形式化保证，又能基于当前状态生成实时控制信号以补偿不确定性，同时遵循实际运动约束。实验结果表明，所提出的李雅普诺夫类稳定器持续改进基准RL策略，将目标到达率从84.6%提升至99.0%，显著减少失败案例并提高效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of formal goal-reaching guarantees in reinforcement learning (RL) for mobile robots by integrating a shielding mechanism that ensures safety without overly restricting exploration. The method combines a real-time RL policy, trained with 15 reward terms to encourage smooth, safe navigation toward static and dynamic goals, with a Lyapunov-like stabilizer layer that supervises the policy to provide formal convergence guarantees. Experimental results demonstrate that this stabilizer consistently enhances benchmark RL policies, increasing the goal-reaching rate from 84.6% to 99.0% while reducing failures and improving efficiency.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习在移动机器人目标到达任务中缺乏形式化保证的问题，提出了一种集成李雅普诺夫类稳定器的控制框架。该方法首先设计了一个包含15项奖励项的强化学习策略，以鼓励机器人到达静态和动态目标并生成平滑、安全的控制指令；其次，引入一个作为策略监督器的形式化稳定层，在确保目标收敛的同时不过度限制探索。实验结果表明，该框架显著提升了基准强化学习策略的性能，将目标到达率从84.6%提高到99.0%，有效减少了失败案例并提升了运行效率。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Reconfiguration Planning for Deformable Quadrilateral Modular Robots</div>
<div class="meta-line">Authors: Jie Gu, Hongrun Gao, Zhihao Xia, Yirun Sun, Chunxu Tian, Dan Zhang</div>
<div class="meta-line">First: 2026-01-27T11:32:04+00:00 · Latest: 2026-01-27T11:32:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19496v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19496v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For lattice modular self-reconfigurable robots (MSRRs), maintaining stable connections during reconfiguration is crucial for physical feasibility and deployability. This letter presents a novel self-reconfiguration planning algorithm for deformable quadrilateral MSRRs that guarantees stable connection. The method first constructs feasible connect/disconnect actions using a virtual graph representation, and then organizes these actions into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves interdependencies. We also prove that reconfiguration sequences satisfying motion characteristics exist for any pair of configurations with seven or more modules (excluding linear topologies). Finally, comparisons with a modified BiRRT algorithm highlight the superior efficiency and stability of our approach, while deployment on a physical robotic platform confirms its practical feasibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可变形四边形模块化机器人的自重构规划</div>
<div class="mono" style="margin-top:8px">对于晶格型模块化自重构机器人，在重构过程中保持稳定连接对其物理可行性和部署能力至关重要。本文提出了一种新颖的可变形四边形模块化自重构机器人自重构规划算法，该算法能确保稳定连接。该方法首先利用虚拟图表示构建可行的连接/断开动作，然后通过基于依赖关系的反向树组织这些动作，形成有效的执行序列以解决相互依赖问题。我们还证明了对于任意包含七个或更多模块（不包括线性拓扑）的配置对，均存在满足运动特性的重构序列。最后，与改进的BiRRT算法对比突显了本方法在效率和稳定性上的优势，同时在物理机器人平台上的部署验证了其实际可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the critical need for stable connection maintenance during reconfiguration of lattice modular self-reconfigurable robots (MSRRs) to ensure physical feasibility. The proposed method develops a self-reconfiguration planning algorithm for deformable quadrilateral MSRRs by first constructing feasible connect/disconnect actions using a virtual graph representation, then organizing them into a valid execution sequence through a Dependence-based Reverse Tree (DRTree) that resolves action interdependencies. Experimental results demonstrate the algorithm&#x27;s existence for any pair of configurations with seven or more modules (excluding linear topologies), show superior efficiency and stability compared to a modified BiRRT algorithm, and confirm practical feasibility through deployment on a physical robotic platform.</div>
<div class="mono" style="margin-top:8px">本研究针对晶格模块化自重构机器人在重构过程中保持稳定连接以确保物理可行性的需求。所提出的方法为可变形四边形模块化自重构机器人开发了一种自重构规划算法，通过虚拟图构建可行的连接/断开动作，并利用基于依赖关系的反向树来组织这些动作以解决相互依赖性。实验结果表明，对于任意具有七个或更多模块（不包括线性拓扑）的配置对，都存在满足运动特性的重构序列；与改进的BiRRT算法相比，该方法展现出更高的效率和稳定性，物理平台部署也验证了其实际可行性。</div>
</details>
</div>
<div class="card">
<div class="title">A Riemannian Take on Distance Fields and Geodesic Flows in Robotics</div>
<div class="meta-line">Authors: Yiming Li, Jiacheng Qiu, Sylvain Calinon</div>
<div class="meta-line">First: 2024-12-06T17:22:18+00:00 · Latest: 2026-01-27T10:58:17+00:00</div>
<div class="meta-line">Comments: 27 pages, 20 figures. International Journal of Robotics Research (IJRR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.05197v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.05197v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distance functions are crucial in robotics for representing spatial relationships between a robot and its environment. They provide an implicit, continuous, and differentiable representation that integrates seamlessly with control, optimization, and learning. While standard distance fields rely on the Euclidean metric, many robotic tasks inherently involve non-Euclidean structures. To this end, we generalize Euclidean distance fields to more general metric spaces by solving the Riemannian eikonal equation, a first-order partial differential equation whose solution defines a distance field and its associated gradient flow on the manifold, enabling the computation of geodesics and globally length-minimizing paths. We demonstrate that geodesic distance fields, the classical Riemannian distance function represented as a global, continuous, and queryable field, are effective for a broad class of robotic problems where Riemannian geometry naturally arises. To realize this, we present a neural Riemannian eikonal solver (NES) that solves the equation as a mesh-free implicit representation without grid discretization, scaling to high-dimensional robot manipulators. Training leverages a physics-informed neural network (PINN) objective that constrains spatial derivatives via the PDE residual and boundary and metric conditions, so the model is supervised by the governing equation and requires no labeled distances or geodesics. We propose two NES variants, conditioned on boundary data and on spatially varying Riemannian metrics, underscoring the flexibility of the neural parameterization. We validate the effectiveness of our approach through extensive examples, yielding minimal-length geodesics across diverse robot tasks involving Riemannian geometry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人学中距离场与测地流：黎曼几何视角</div>
<div class="mono" style="margin-top:8px">距离函数在机器人学中对表示机器人与环境的空间关系至关重要，它提供了一种隐式、连续且可微的表征，能与控制、优化和学习无缝集成。传统距离场依赖欧几里得度量，但许多机器人任务本质涉及非欧结构。为此，我们通过求解黎曼程函方程——一个一阶偏微分方程，其解定义了流形上的距离场及其梯度流——将欧氏距离场推广至更一般的度量空间，从而能够计算测地线与全局长度最小路径。我们证明，测地距离场（即表示为全局连续可查询场的经典黎曼距离函数）对黎曼几何自然涌现的广泛机器人问题具有有效性。为实现此目标，我们提出一种神经黎曼程函求解器，以无网格隐式表征求解方程，可扩展至高维机器人机械臂。训练采用物理信息神经网络目标函数，通过偏微分方程残差、边界条件与度量条件约束空间导数，使模型受控于控制方程且无需标注距离或测地线数据。我们提出两种求解器变体：基于边界数据与空间变化黎曼度量的条件化版本，突显神经参数化的灵活性。通过大量实验验证了方法的有效性，在涉及黎曼几何的多种机器人任务中实现了最小长度测地线计算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for distance functions that capture non-Euclidean structures inherent in many robotic tasks, this work generalizes Euclidean distance fields by solving the Riemannian eikonal equation to compute geodesic distance fields on manifolds. The method introduces a neural Riemannian eikonal solver (NES), a mesh-free implicit representation trained with a physics-informed neural network objective that enforces the PDE, boundary, and metric conditions without requiring labeled distance data. Experimental validation across diverse robot tasks demonstrates that the approach successfully yields minimal-length geodesics, effectively handling high-dimensional manipulators and spatially varying Riemannian metrics.</div>
<div class="mono" style="margin-top:8px">针对许多机器人任务中固有的非欧几里得结构，本研究通过求解黎曼程函方程，将欧氏距离场推广到更一般的度量空间，以计算流形上的测地距离场。方法提出了一种神经黎曼程函求解器，这是一种无网格的隐式表示，采用物理信息神经网络目标进行训练，通过约束偏微分方程残差和边界条件来监督模型，无需标注的距离或测地线数据。在涉及黎曼几何的各种机器人任务上的实验验证表明，该方法能有效生成全局长度最小化的路径，并可扩展到高维机器人操纵器。</div>
</details>
</div>
<div class="card">
<div class="title">Physical Human-Robot Interaction: A Critical Review of Safety Constraints</div>
<div class="meta-line">Authors: Riccardo Zanella, Federico Califano, Stefano Stramigioli</div>
<div class="meta-line">First: 2026-01-27T10:45:50+00:00 · Latest: 2026-01-27T10:45:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19462v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper aims to provide a clear and rigorous understanding of commonly recognized safety constraints in physical human-robot interaction, i.e. ISO/TS 15066, by examining how they are obtained and which assumptions support them. We clarify the interpretation and practical impact of key simplifying assumptions, show how these modeling choices affect both safety and performance across the system, and indicate specific design parameters that can be adjusted in safety-critical control implementations. Numerical examples are provided to quantify performance degradation induced by common approximations and simplifying design choices. Furthermore, the fundamental role of energy in safety assessment is emphasized, and focused insights are offered on the existing body of work concerning energy-based safety methodologies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理人机交互：安全约束的批判性审视</div>
<div class="mono" style="margin-top:8px">本文旨在通过考察物理人机交互中普遍认可的安全约束（即ISO/TS 15066）的推导过程及其支撑假设，提供清晰而严谨的理解。我们阐释了关键简化假设的内涵与实际影响，展示这些建模选择如何影响整个系统的安全性与性能，并指出在安全关键控制实施中可调整的具体设计参数。通过数值算例量化了常见近似与简化设计选择导致的性能衰减，同时强调了能量在安全评估中的核心作用，并对现有基于能量的安全方法研究体系提供了聚焦性见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review critically examines the safety constraints for physical human-robot interaction, particularly those in ISO/TS 15066, to clarify their underlying assumptions and practical implications. The method involves analyzing how key simplifying assumptions in these models are derived and interpreting their impact on system safety and performance, while identifying adjustable design parameters for safety-critical control. Experimental findings from numerical examples quantify the performance degradation caused by common approximations, and the analysis underscores the fundamental role of energy-based methodologies in safety assessment.</div>
<div class="mono" style="margin-top:8px">本文旨在通过审视安全约束的获取方式及其支撑假设，对物理人机交互中普遍认可的安全约束（即ISO/TS 15066）提供清晰而严谨的理解。研究方法包括澄清关键简化假设的解释与实际影响，展示这些建模选择如何影响整个系统的安全性与性能，并指出在安全关键控制实施中可调整的具体设计参数。数值示例量化了常见近似和简化设计选择导致的性能下降，同时强调了能量在安全评估中的根本作用，并对现有基于能量的安全方法论提供了聚焦性见解。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Gold-Standard Depth Estimation for Tree Branches in UAV Forestry: Benchmarking Deep Stereo Matching Methods</div>
<div class="meta-line">Authors: Yida Lin, Bing Xue, Mengjie Zhang, Sam Schofield, Richard Green</div>
<div class="meta-line">First: 2026-01-27T10:45:29+00:00 · Latest: 2026-01-27T10:45:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19461v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19461v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous UAV forestry operations require robust depth estimation with strong cross-domain generalization, yet existing evaluations focus on urban and indoor scenarios, leaving a critical gap for vegetation-dense environments. We present the first systematic zero-shot evaluation of eight stereo methods spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms. All methods use officially released pretrained weights (trained on Scene Flow) and are evaluated on four standard benchmarks (ETH3D, KITTI 2012/2015, Middlebury) plus a novel 5,313-pair Canterbury Tree Branches dataset ($1920 \times 1080$). Results reveal scene-dependent patterns: foundation models excel on structured scenes (BridgeDepth: 0.23 px on ETH3D; DEFOM: 4.65 px on Middlebury), while iterative methods show variable cross-benchmark performance (IGEV++: 0.36 px on ETH3D but 6.77 px on Middlebury; IGEV: 0.33 px on ETH3D but 4.99 px on Middlebury). Qualitative evaluation on the Tree Branches dataset establishes DEFOM as the gold-standard baseline for vegetation depth estimation, with superior cross-domain consistency (consistently ranking 1st-2nd across benchmarks, average rank 1.75). DEFOM predictions will serve as pseudo-ground-truth for future benchmarking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向无人机林业树枝深度估计的黄金标准：深度立体匹配方法基准测试</div>
<div class="mono" style="margin-top:8px">自主无人机林业作业需要具备强大跨域泛化能力的鲁棒深度估计技术，然而现有评估主要集中于城市和室内场景，对植被密集环境存在关键空白。本研究首次对涵盖迭代优化、基础模型、扩散模型和3D CNN范式的八种立体方法进行系统性零样本评估。所有方法均采用官方发布的预训练权重（基于Scene Flow数据集训练），并在四个标准基准（ETH3D、KITTI 2012/2015、Middlebury）及包含5,313对图像的坎特伯雷树枝数据集（1920×1080分辨率）上进行测试。结果呈现场景依赖性规律：基础模型在结构化场景表现优异（BridgeDepth在ETH3D上误差0.23像素；DEFOM在Middlebury上误差4.65像素），而迭代方法在不同基准间性能波动显著（IGEV++在ETH3D上误差0.36像素但在Middlebury上达6.77像素；IGEV在ETH3D上误差0.33像素但在Middlebury上达4.99像素）。树枝数据集的定性评估确立DEFOM为植被深度估计的黄金标准基线，其跨域一致性表现卓越（在所有基准中稳定位列第1-2名，平均排名1.75）。DEFOM的预测结果将作为未来基准测试的伪真值参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the lack of robust depth estimation methods for autonomous UAV forestry operations, particularly in vegetation-dense environments where existing evaluations are limited to urban and indoor scenes. The authors conduct a systematic zero-shot evaluation of eight deep stereo matching methods, spanning iterative refinement, foundation model, diffusion-based, and 3D CNN paradigms, using their officially released pretrained weights trained on Scene Flow. The evaluation on four standard benchmarks and a novel Canterbury Tree Branches dataset reveals that foundation models perform best on structured scenes, while iterative methods show variable cross-benchmark performance, with DEFOM emerging as the gold-standard baseline for vegetation depth estimation due to its superior cross-domain consistency.</div>
<div class="mono" style="margin-top:8px">本研究针对自主无人机林业作业中缺乏鲁棒深度估计的问题，现有立体匹配评估主要集中于城市和室内场景，在植被密集环境中存在关键空白。该研究对八种深度立体方法进行了系统性零样本评估，包括迭代优化、基础模型、扩散模型和3D CNN范式，均使用其在Scene Flow上预训练的官方权重。方法在四个标准基准（ETH3D、KITTI 2012/2015、Middlebury）和一个包含5,313对图像的新型坎特伯雷树枝数据集上进行测试。主要发现揭示了场景依赖性性能：基础模型在结构化场景中表现优异，而迭代方法在不同基准间表现波动。DEFOM成为植被深度估计的黄金标准基线，展现出卓越的跨域一致性，在所有基准中排名第一或第二，其预测结果被指定为未来基准测试的伪真值。</div>
</details>
</div>
<div class="card">
<div class="title">CBMC-V3: A CNS-inspired Control Framework Towards Agile Manipulation with SNN</div>
<div class="meta-line">Authors: Yanbo Pang, Qingkai Li, Mingguo Zhao</div>
<div class="meta-line">First: 2025-11-06T06:48:29+00:00 · Latest: 2026-01-27T10:28:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04109v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.04109v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As robotic arm applications expand beyond traditional industrial settings into service-oriented domains such as catering, household and retail, existing control algorithms struggle to achieve the level of agile manipulation required in unstructured environments characterized by dynamic trajectories, unpredictable interactions, and diverse objects. This paper presents a biomimetic control framework based on Spiking Neural Network (SNN), inspired by the human Central Nervous System (CNS), to address these challenges. The proposed framework comprises five control modules-cerebral cortex, cerebellum, thalamus, brainstem, and spinal cord-organized into three hierarchical control levels (first-order, second-order, and third-order) and two information pathways (ascending and descending). All modules are fully implemented using SNN. The framework is validated through both simulation and experiments on a commercial robotic arm platform across a range of control tasks. The results demonstrate that the proposed method outperforms the baseline in terms of agile motion control capability, offering a practical and effective solution for achieving agile manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CBMC-V3：基于脉冲神经网络的仿中枢神经系统控制框架实现灵巧操作</div>
<div class="mono" style="margin-top:8px">随着机械臂应用从传统工业领域扩展到餐饮、家居、零售等服务业场景，现有控制算法难以在具有动态轨迹、不可预测交互和多样物体的非结构化环境中实现所需的灵巧操作水平。本文提出一种受人类中枢神经系统启发的、基于脉冲神经网络的仿生控制框架以应对这些挑战。该框架包含大脑皮层、小脑、丘脑、脑干和脊髓五个控制模块，组织为三个层级控制阶（一阶、二阶、三阶）和两条信息通路（上行与下行），所有模块均采用脉冲神经网络完整实现。通过在商用机械臂平台上进行多类控制任务的仿真与实验验证，结果表明所提方法在灵巧运动控制能力上优于基线方案，为实现灵巧操作提供了实用有效的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable robotic arms to perform agile manipulation in unstructured service environments like catering and retail, where dynamic trajectories and unpredictable interactions challenge existing control algorithms, this paper proposes a biomimetic control framework inspired by the human Central Nervous System (CNS) and implemented entirely with Spiking Neural Networks (SNNs). The method organizes five SNN-based modules—cerebral cortex, cerebellum, thalamus, brainstem, and spinal cord—into three hierarchical control levels and two information pathways to mimic CNS functionality. Experimental validation on a commercial robotic arm platform shows that this framework surpasses baseline methods in agile motion control capability, providing a practical solution for complex manipulation tasks.</div>
<div class="mono" style="margin-top:8px">为解决现有控制算法在非结构化服务环境中实现机械臂灵巧操作的局限性，本文受人类中枢神经系统启发，提出了一种基于脉冲神经网络的仿生控制框架。该方法将五个基于脉冲神经网络的模块——大脑皮层、小脑、丘脑、脑干和脊髓——组织为三个层次和两条信息通路，以模拟中枢神经系统的功能。在商用机械臂平台上的实验验证表明，该框架在灵巧运动控制能力上优于基线方法，为动态任务提供了有效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Automating Box Folding: Sequence Extraction and Ranking Methodologies</div>
<div class="meta-line">Authors: Giuseppe Fabio Preziosa, Davide Ferloni, Andrea Maria Zanchettin, Marco Faroni, Paolo Rocco</div>
<div class="meta-line">Venue: IFAC-PapersOnLine, vol. 59(18), 2025, pp. 241-246</div>
<div class="meta-line">First: 2025-05-07T09:02:09+00:00 · Latest: 2026-01-27T09:53:30+00:00</div>
<div class="meta-line">Comments: Accepted at IFAC ROBOTICS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.04257v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.04257v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Box folding represents a crucial challenge for automated packaging systems. This work bridges the gap between existing methods for folding sequence extraction and approaches focused on the adaptability of automated systems to specific box types. An innovative method is proposed to identify and rank folding sequences, enabling the transformation of a box from an initial state to a desired final configuration. The system evaluates and ranks these sequences based on their feasibility and compatibility with available hardware, providing recommendations for real-world implementations. Finally, an illustrative use case is presented, where a robot performs the folding of a box.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动化纸盒折叠：序列提取与排序方法研究</div>
<div class="mono" style="margin-top:8px">纸盒折叠是自动化包装系统的关键挑战。本研究填补了现有折叠序列提取方法与自动化系统针对特定盒型适应能力之间的空白，提出了一种创新方法用于识别和排序折叠序列，实现纸盒从初始状态到目标构型的转换。该系统基于序列的可行性与硬件兼容性进行评估排序，为实际应用提供实施方案建议。最后通过机器人执行纸盒折叠的案例进行演示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of automating box folding in packaging systems by bridging the gap between sequence extraction methods and hardware adaptability. The proposed method identifies and ranks feasible folding sequences to transform a box from an initial to a final state, evaluating them based on hardware compatibility for real-world implementation. In an illustrative use case, a robot successfully performed the box folding task, demonstrating the method&#x27;s practical viability.</div>
<div class="mono" style="margin-top:8px">本研究针对包装系统中纸盒折叠自动化的挑战，旨在弥合折叠序列提取方法与硬件适应性之间的差距。所提出的方法能识别并排序将纸盒从初始状态变换至目标构型的可行折叠序列，根据与可用硬件的兼容性进行评估，以提供实际实施建议。在一个实验性用例中，该系统成功实现了机器人执行纸盒折叠任务。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
