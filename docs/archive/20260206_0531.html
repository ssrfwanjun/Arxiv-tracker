<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 05:31</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0531</div>
    <div class="row"><div class="card">
<div class="title">Reinforced Attention Learning</div>
<div class="meta-line">Authors: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen, Derek Zhiyuan Cheng</div>
<div class="meta-line">First: 2026-02-04T18:59:52+00:00 · Latest: 2026-02-04T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04884v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化注意力学习</div>
<div class="mono" style="margin-top:8px">通过测试时扩展，基于强化学习（RL）的后训练显著提升了大语言模型（LLM）的推理能力。然而，将这一范式通过冗长推理过程扩展到多模态大语言模型（MLLM）时，对感知能力的提升有限，甚至可能导致性能下降。我们提出强化注意力学习（RAL），这是一种直接优化内部注意力分布而非输出词元序列的策略梯度框架。通过将优化目标从&#x27;生成什么&#x27;转向&#x27;关注何处&#x27;，RAL促进了复杂多模态输入中的有效信息分配与更优的语义基础。在多样化图像与视频基准测试中的实验表明，该方法相较GRPO及其他基线模型均取得稳定提升。我们进一步提出同策略注意力蒸馏，证明迁移潜在注意力行为比标准知识蒸馏能产生更强的跨模态对齐效果。我们的研究确立了注意力策略作为多模态后训练的一种原理性通用替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of applying reinforcement learning-based post-training to multimodal large language models, where verbose rationales yield minimal perception gains and can degrade performance. The proposed Reinforced Attention Learning (RAL) framework directly optimizes internal attention distributions using policy-gradient methods, shifting optimization from output token sequences to where the model should attend, thereby improving information allocation and grounding in multimodal inputs. Experimental results across diverse image and video benchmarks demonstrate consistent improvements over GRPO and other baselines, while the introduced On-Policy Attention Distillation shows that transferring latent attention behaviors achieves stronger cross-modal alignment than standard knowledge distillation.</div>
<div class="mono" style="margin-top:8px">本研究针对传统强化学习后训练在多模态大语言模型中的局限性展开，该方法通过优化冗长的输出推理过程往往只能带来有限的感知能力提升，甚至可能导致性能下降。提出的强化注意力学习方法采用策略梯度框架直接优化模型内部的注意力分布，将优化目标从生成的标记序列转向潜在的注意力分配机制。在多种图像和视频基准测试上的实验结果表明，该方法相比GRPO及其他基线模型取得了持续的性能提升，同时引入的策略上注意力蒸馏技术表明，迁移优化后的注意力行为比标准知识蒸馏方法能实现更强的跨模态对齐效果。</div>
</details>
</div>
<div class="card">
<div class="title">Protein Autoregressive Modeling via Multiscale Structure Generation</div>
<div class="meta-line">Authors: Yanru Qu, Cheng-Yen Hsieh, Zaixiang Zheng, Ge Liu, Quanquan Gu</div>
<div class="meta-line">First: 2026-02-04T18:59:49+00:00 · Latest: 2026-02-04T18:59:49+00:00</div>
<div class="meta-line">Comments: ByteDance Seed Tech Report; Page: https://par-protein.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04883v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04883v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://par-protein.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>蛋白质自回归建模：基于多尺度结构生成</div>
<div class="mono" style="margin-top:8px">我们提出了蛋白质自回归建模（PAR），这是首个通过从粗到细的跨尺度预测实现蛋白质骨架生成的多尺度自回归框架。PAR利用蛋白质的层次化特性，通过模拟雕塑过程生成结构：先形成粗粒度拓扑，再逐级细化结构细节。该框架包含三个核心组件：（1）多尺度下采样操作，在训练中表征跨尺度的蛋白质结构；（2）自回归Transformer编码器，整合多尺度信息并生成条件嵌入以指导结构生成；（3）基于流的骨架解码器，根据条件嵌入生成骨架原子。针对自回归模型因训练与生成过程不匹配导致的暴露偏差问题，我们通过噪声上下文学习与计划采样策略有效缓解了其对结构生成质量的负面影响。值得注意的是，PAR展现出强大的零样本泛化能力，支持无需微调的人类提示条件生成与基序支架构建。在无条件生成基准测试中，PAR能有效学习蛋白质分布，生成具有高设计质量的骨架，并呈现良好的扩展特性。这些优势共同确立了PAR作为蛋白质结构生成框架的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to generate high-quality protein backbone structures by addressing the limitations of existing methods that may not fully exploit the hierarchical nature of proteins or suffer from exposure bias in autoregressive modeling. The proposed method, Protein Autoregressive Modeling (PAR), introduces a multi-scale autoregressive framework that generates structures from coarse topology to fine details using multi-scale downsampling, an autoregressive transformer for encoding and conditional embedding, and a flow-based backbone decoder, with noisy context learning and scheduled sampling to mitigate exposure bias. Experimental results demonstrate that PAR effectively learns protein distributions, produces backbones of high design quality in unconditional generation, exhibits strong zero-shot generalization for conditional tasks like motif scaffolding, and shows favorable scaling behavior.</div>
<div class="mono" style="margin-top:8px">该研究旨在生成高质量的蛋白质主链结构，以解决现有方法未能充分利用蛋白质的层次结构特性以及自回归建模中暴露偏差的问题。所提出的方法，即蛋白质自回归建模（PAR），采用了一个多尺度自回归框架，首先生成粗略的拓扑结构，然后细化结构细节，利用了多尺度下采样、用于编码的自回归变换器以及基于流的骨架解码器进行原子生成；同时通过噪声上下文学习和计划采样来减轻暴露偏差。实验结果表明，PAR在无条件生成基准测试中能有效学习蛋白质分布，生成具有高设计质量的主链，展现出良好的扩展行为，并在无需微调的情况下，对条件生成和基序支架表现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Continual Learning for Model Adaptability in Internet of Things</div>
<div class="meta-line">Authors: Ajesh Koyatan Chathoth</div>
<div class="meta-line">First: 2026-02-04T18:59:14+00:00 · Latest: 2026-02-04T18:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04881v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04881v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向物联网模型适应性的对比持续学习</div>
<div class="mono" style="margin-top:8px">物联网部署运行于非平稳的动态环境中，传感器漂移、用户行为演变及异构隐私需求等因素均可能影响应用效能。持续学习通过使模型随时间适应而避免灾难性遗忘来应对这一问题。与此同时，对比学习已成为一种强大的表征学习范式，能以自监督方式提升模型鲁棒性与样本效率。本文综述了对比持续学习在物联网领域的应用，将算法设计（回放、正则化、蒸馏、提示）与物联网系统现实（微型机器学习约束、间歇连接、隐私保护）相结合。我们提出了统一的问题形式化框架，推导出融合对比损失与蒸馏损失的通用目标函数，设计了面向设备端、边缘侧与云端的物联网对比持续学习参考架构，并提供了评估协议与指标的指导原则。最后，我们重点阐述了物联网领域特有的开放性挑战，包括表格数据与流式数据的协同处理、概念漂移、联邦学习场景及能耗感知训练等问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the need for adaptable machine learning models in Internet of Things (IoT) environments, which are characterized by nonstationary data, sensor drift, and privacy constraints. The method focuses on contrastive continual learning (CCL), which integrates contrastive learning&#x27;s self-supervised representation benefits with continual learning techniques like replay and distillation to prevent catastrophic forgetting. The work formulates a unifying problem, proposes an IoT-specific architecture for on-device, edge, and cloud deployment, and identifies key challenges such as handling tabular and streaming data, concept drift, and federated learning in energy-aware settings.</div>
<div class="mono" style="margin-top:8px">本文针对物联网环境中机器学习模型需要适应非平稳条件（如传感器漂移和用户行为演变）的需求展开研究。作者提出将对比学习与持续学习相结合，以增强模型的鲁棒性和样本效率，综述了回放和蒸馏等算法设计，并将其与物联网系统的计算限制和间歇性连接等现实约束相结合。主要贡献包括统一的问题表述、面向设备端和边缘-云部署的参考架构设计，以及评估指标指导，同时指出了在物联网场景中处理流数据、概念漂移和联邦学习等独特开放挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Robust inverse material design with physical guarantees using the Voigt-Reuss Net</div>
<div class="meta-line">Authors: Sanath Keshav, Felix Fritzen</div>
<div class="meta-line">First: 2025-11-14T15:17:37+00:00 · Latest: 2026-02-04T18:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11388v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $&gt;\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2&gt;0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Voigt-Reuss网络且具有物理保证的鲁棒逆向材料设计</div>
<div class="mono" style="margin-top:8px">我们提出了一种具有严格物理保证的正向与逆向力学均匀化谱归一化代理模型。利用Voigt-Reuss界限，通过类Cholesky算子分解其差值，学习得到特征值位于$[0,1]$区间内的无量纲对称半正定表示；逆向映射返回的对称正定预测在Löwner意义下严格位于界限之间。在随机双相微结构开放数据集的三维线弹性问题中，采用236个各向同性不变量描述符和三个对比度参数，基于超过$7.5\times 10^{5}$个FFT标签训练的全连接Voigt-Reuss网络，以近乎完美的保真度恢复了各向同性投影（各向同性相关条目：$R^2 \ge 0.998$），而揭示各向异性的耦合项无法从$SO(3)$不变输入中识别。张量级相对Frobenius误差在数据划分中位数约$1.7\%$，均值约$3.4\%$。在阈值化三角微结构的二维平面应变问题中，将谱归一化与可微分渲染器及CNN结合，所有分量均实现$R^2&gt;0.99$，归一化损失低于百分之一，精确追踪渗流诱导的特征值跃变，并对分布外图像具有鲁棒泛化能力。将参数化微结构作为设计变量，采用单一代理模型的批量一阶优化可在数个百分点内匹配目标张量，并返回多样化的近最优设计。总体而言，Voigt-Reuss网络统一了精确且物理可接受的正向预测与批量化的约束一致逆向设计，并适用于椭圆算子和耦合物理场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for physically guaranteed surrogate models in mechanical homogenization to enable reliable inverse material design. The method introduces the Voigt-Reuss Net, which leverages the Voigt-Reuss bounds to construct a spectrally normalized surrogate; it learns a dimensionless, symmetric positive semi-definite representation with eigenvalues in [0,1], ensuring that inverse predictions are symmetric positive-definite and lie between the bounds in the Löwner sense. Key experimental results show that in 3D linear elasticity on stochastic biphasic microstructures, the model trained on over 750,000 FFT-based labels achieves near-perfect fidelity for isotropic projections (R² ≥ 0.998) with median relative Frobenius errors around 1.7%, while in 2D plane strain on trigonometric microstructures, it attains R² &gt; 0.99 on all components, accurately tracks percolation-induced eigenvalue jumps, and generalizes robustly to out-of-distribution images, enabling batched optimization for inverse design with diverse near-optimal designs.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决机械均质化中需要物理保证的代理模型问题，以确保预测符合已知的材料界限。方法提出了Voigt-Reuss Net，利用Voigt-Reuss界限，通过类Cholesky算子分解其差异，学习一个特征值在[0,1]范围内的无量纲、对称半正定表示；这确保了逆预测是对称正定的，并在Löwner意义上位于界限之间。在三维线性弹性实验中，基于随机双相微结构数据集，模型以高保真度恢复了各向同性投影（各向同性相关条目的R² ≥ 0.998），中位相对Frobenius误差约为1.7%；在二维平面应变实验中，基于三角微结构，所有组分的R² &gt; 0.99，准确追踪了渗透引起的特征值跳跃，并实现了稳健的逆设计，匹配目标张量误差在几个百分点内。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Trust Region in LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee</div>
<div class="meta-line">First: 2026-02-04T18:59:04+00:00 · Latest: 2026-02-04T18:59:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04879v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考大语言模型强化学习中的信任区域</div>
<div class="mono" style="margin-top:8px">强化学习已成为微调大语言模型的核心技术，其中近端策略优化算法是实际上的标准算法。尽管应用广泛，我们认为PPO核心的概率比截断机制在结构上并不适合大语言模型固有的巨大词汇量。PPO基于采样词元的概率比约束策略更新，该比值是对真实策略散度的噪声单样本蒙特卡洛估计。这导致了次优的学习动态：对低概率词元的更新被过度惩罚，而高概率词元可能出现的灾难性偏移却约束不足，从而造成训练效率低下和稳定性问题。为此，我们提出散度近端策略优化算法，用基于策略散度直接估计（如总变差或KL散度）的原则性约束替代启发式截断。为避免巨大内存开销，我们引入高效的二元与Top-K近似方法，以可忽略的开销捕捉核心散度。大量实验评估表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于强化学习的大语言模型微调提供了更稳健的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research identifies a structural limitation in Proximal Policy Optimization (PPO) for fine-tuning Large Language Models, where its token-level ratio clipping creates noisy, sub-optimal constraints that over-penalize low-probability tokens and under-constrain high-probability ones, leading to instability. The proposed Divergence Proximal Policy Optimization (DPPO) replaces heuristic clipping with a direct constraint on policy divergence (e.g., Total Variation or KL) and introduces efficient Binary and Top-K approximations to manage computational overhead. Experimental results show that DPPO achieves superior training stability and efficiency compared to existing methods, providing a more robust foundation for RL-based LLM fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究指出了近端策略优化（PPO）在微调大语言模型时存在的结构缺陷：其基于词元概率比的裁剪机制会产生噪声大、次优的约束，过度惩罚低概率词元而对高概率词元的潜在灾难性偏移约束不足，导致训练不稳定和低效。为此，作者提出了散度近端策略优化（DPPO），用基于策略散度（如总变差或KL散度）的直接估计约束取代启发式裁剪，并引入了高效的二值化和Top-K近似方法来控制计算开销。大量实验表明，DPPO相比现有方法实现了更优的训练稳定性和效率，为基于强化学习的大语言模型微调提供了更鲁棒的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning</div>
<div class="meta-line">Authors: Nicholas Barnfield, Subhabrata Sen, Pragya Sur</div>
<div class="meta-line">First: 2026-02-04T18:57:30+00:00 · Latest: 2026-02-04T18:57:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04872v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04872v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多层交叉注意力被证明是多模态上下文学习的最优方案</div>
<div class="mono" style="margin-top:8px">近期研究快速推进了我们对现代基于注意力的神经网络中上下文学习机制的理解。然而，现有成果仅聚焦于单模态数据；相比之下，多模态数据上下文学习的理论基础仍知之甚少。我们引入一个数学上可处理的框架来研究多模态学习，并探究类Transformer架构何时能在上下文中恢复贝叶斯最优性能。为建模多模态问题，我们假设观测数据源自潜在因子模型。首个结果呈现了表达能力的负面结论：我们证明单层线性自注意力无法在任务分布上一致地恢复贝叶斯最优预测器。为突破此局限，我们提出一种新颖的线性化交叉注意力机制，并在交叉注意力层数与上下文长度均较大的场景下进行研究。结果表明，该交叉注意力机制在使用梯度流优化时可被证明是贝叶斯最优的。我们的研究凸显了深度对上下文学习的益处，并确立了交叉注意力机制对多模态分布的可证明效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of theoretical understanding for multi-modal in-context learning, which contrasts with recent progress in unimodal settings. The authors propose a mathematical framework where data follows a latent factor model and analyze transformer-like architectures. They first prove that single-layer linear self-attention cannot achieve Bayes-optimal performance, then demonstrate that a novel linearized cross-attention mechanism, when optimized with gradient flow across many layers and with long context, provably recovers the Bayes-optimal predictor. The findings highlight the necessity of architectural depth and establish the theoretical utility of cross-attention for multi-modal tasks.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，尽管单模态上下文学习机制已取得进展，但多模态数据的上下文学习理论基础仍不明确。方法上，研究引入了一个基于潜在因子模型的可数学处理的多模态学习框架，分析了类Transformer架构，证明了单层线性自注意力无法一致地恢复贝叶斯最优预测器。为解决此限制，研究提出了一种新颖的线性化交叉注意力机制，并在多层和长上下文长度的条件下进行分析。主要实验结果表明，通过梯度流优化，该交叉注意力机制可证明地达到贝叶斯最优性能，这强调了深度对于上下文学习的益处，并确立了交叉注意力在多模态分布中的可证明效用。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism</div>
<div class="meta-line">Authors: Chenwei Cui, Rockwell Jackson, Benjamin Joseph Herrera, Ana María Tárano, Hannah Kerner</div>
<div class="meta-line">First: 2026-02-04T18:57:19+00:00 · Latest: 2026-02-04T18:57:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04870v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04870v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多头隐式专家混合与头并行：通信高效且确定性的专家混合并行方法</div>
<div class="mono" style="margin-top:8px">大语言模型已改变众多应用场景，但其训练成本依然高昂。稀疏专家混合模型通过条件计算应对此问题，其中专家并行是标准的分布式训练方法。然而专家并行存在三个局限：通信成本随激活专家数$k$线性增长、负载不均衡影响延迟与内存使用、数据依赖型通信需交换元数据。我们提出多头隐式专家混合与头并行——这种新架构与并行方法可实现与$k$无关的$O(1)$通信成本、完全均衡的通信流量及确定性通信，同时保持与专家并行的兼容性。为加速多头隐式专家混合，我们提出I/O感知路由与专家计算机制。相较于采用专家并行的专家混合模型，结合头并行的多头隐式专家混合在保持同等性能的同时训练速度提升达$1.61\times$；当粒度加倍时，其综合性能更优且仍保持$1.11\times$的速度优势。该方法使数十亿参数基础模型的研究更具可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the communication inefficiencies and load imbalance issues in standard Expert Parallel (EP) training of sparse Mixture of Experts (MoE) models, this work introduces the Multi-Head LatentMoE architecture and a corresponding Head Parallel (HP) method. The approach employs IO-aware routing and expert computation to achieve deterministic, load-balanced communication with constant cost independent of the number of activated experts, while maintaining compatibility with EP. Experiments show that Multi-Head LatentMoE with HP trains up to 1.61 times faster than a standard MoE with EP while delivering identical model performance, and with increased granularity, it achieves higher overall performance while remaining 1.11 times faster.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型训练成本高昂的问题，旨在改进稀疏专家混合模型（MoE）中标准的专家并行（EP）方法，该方法存在通信成本随激活专家数增长、负载不均衡和数据依赖通信等局限。作者提出了一种新架构——多头潜在MoE，并搭配头并行（HP）策略，通过引入IO感知路由和专家计算，实现了恒定的O(1)通信成本、均衡的流量和确定性的通信模式。实验结果表明，该方法相比标准MoE与EP训练速度最高提升1.61倍且性能相同；在增加粒度后，能在保持1.11倍加速的同时获得更高的整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation</div>
<div class="meta-line">Authors: Yannick Denker, Alexander Gepperth</div>
<div class="meta-line">First: 2026-02-04T18:54:26+00:00 · Latest: 2026-02-04T18:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRoSS：面向高任务多样性与真实物理仿真的可扩展强化学习持续机器人仿真套件</div>
<div class="mono" style="margin-top:8px">持续强化学习要求智能体在任务序列中学习且不遗忘已习得的策略。本研究基于Gazebo仿真器中的高拟真机器人，提出一种新型持续强化学习基准套件。该持续机器人仿真套件采用两种机器人平台：配备激光雷达、摄像头及碰撞传感器的两轮差速驱动机器人，以及七关节机械臂。前者用于线跟随与物体推动场景，通过视觉与结构参数变化可生成大量差异化任务；后者应用于两种目标抵达场景：基于笛卡尔手部位置的高层控制（延续Continual World基准设计）与基于关节角度的底层控制。针对机械臂基准，我们额外提供仅需运动学计算的变体方案，在无需传感器读数时可绕过物理仿真，运行速度提升两个数量级。本套件具备易扩展性，支持在高度物理真实的机器人场景中对持续强化学习进行受控研究，尤其支持近乎任意的仿真传感器配置。为确保可复现性与易用性，我们提供开箱即用的容器化部署方案（Apptainer），并报告深度Q网络、策略梯度等标准强化学习算法的性能表现，验证其作为可扩展、可复现的持续强化学习研究基准的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces CRoSS, a benchmark suite for continual reinforcement learning (CRL) that addresses the need for scalable, realistic, and diverse robotic simulation environments. The method leverages the Gazebo simulator to model two robotic platforms: a differential-drive robot for line-following and object-pushing tasks with varied visual and structural parameters, and a 7-joint robotic arm for goal-reaching tasks under both high-level Cartesian and low-level joint control, with kinematics-only variants provided for faster computation. Key experimental findings demonstrate the suite&#x27;s extensibility and physical realism, supporting controlled CRL studies with arbitrary sensors, and benchmark performances from standard algorithms like DQN and policy gradient methods confirm its suitability as a scalable and reproducible research tool.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决机器人持续强化学习领域缺乏可扩展、可复现基准的问题，其中智能体需在不遗忘的情况下进行序列学习。方法上引入了持续机器人仿真套件（CRoSS），该套件基于Gazebo仿真器，使用两个高真实度机器人平台构建基准：一个配备激光雷达、摄像头和碰撞传感器的差速驱动机器人，用于具有视觉和结构参数变化的循线和物体推动任务；另一个七关节机械臂用于基于笛卡尔手部位置控制或关节角度控制的目标到达任务。关键实验结果包括该套件易于扩展、支持近乎任意的仿真传感器，并为机械臂提供了无需物理仿真、运行速度快两个数量级的运动学变体；通过深度Q网络和策略梯度等标准强化学习算法的性能基线，验证了其作为基准的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Subliminal Effects in Your Data: A General Mechanism via Log-Linearity</div>
<div class="meta-line">Authors: Ishaq Aden-Ali, Noah Golowich, Allen Liu, Abhishek Shetty, Ankur Moitra, Nika Haghtalab</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/ishaqadenali/logit-linear-selection</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04863v1">PDF</a> · <a href="https://github.com/ishaqadenali/logit-linear-selection">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model&#x27;s properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据中的潜意识效应：一种基于对数线性的通用机制</div>
<div class="mono" style="margin-top:8px">训练现代大型语言模型已成为融合多种算法与数据集的复杂过程，这些设计旨在激发特定行为，因此开发理解数据集对模型特性影响的技术至关重要。近期实验进一步表明，数据集可能传递无法从单个数据点直接观测的信号，这对以数据集为中心理解LLM训练提出了概念性挑战，并暗示此类现象缺乏根本性理论解释。为探究此类效应，我们受近期关于LLM线性结构研究的启发，揭示了一种通用机制，可解释隐藏潜文本如何在通用数据集中产生。
我们提出对数线性选择法——一种通过筛选通用偏好数据集的子集来激发多种隐藏效应的方法。应用该方法从现实数据集中发现的子集，能使训练出的模型展现出从特定偏好、响应数据集中未出现的外语提示到呈现不同人格特征等一系列行为。关键的是，这种效应在不同架构的模型中均对选定子集保持稳定，印证了其普适性与通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand how datasets influence large language models (LLMs), especially given that datasets can transmit signals not observable in individual data points, challenging dataset-centric explanations of model behavior. The method introduces Logit-Linear-Selection (LLS), a technique for selecting subsets from generic preference datasets to elicit hidden effects, such as specific preferences, responses in an unseen language, or different personas. Key experimental findings show that models trained on these LLS-selected subsets consistently exhibit the intended behaviors, with the effect persisting across various model architectures, demonstrating the generality and universality of the mechanism.</div>
<div class="mono" style="margin-top:8px">该研究的动机源于需要理解数据集如何影响大语言模型（LLM）的行为，特别是考虑到有证据表明数据集可以传递在单个数据点中不明显的信号，这对以数据集为中心的解释提出了挑战。方法上引入了Logit-Linear-Selection（LLS），这是一种从通用偏好数据集中选择子集以引发隐藏效应（如特定偏好、跨语言响应或角色扮演）的技术。关键实验结果表明，在这些子集上训练的模型在不同架构中均能一致地表现出目标行为，证明了该效应的普遍性和持久性。</div>
</details>
</div>
<div class="card">
<div class="title">From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures</div>
<div class="meta-line">Authors: Ryan Liu, Eric Qu, Tobias Kreiman, Samuel M. Blau, Aditi S. Krishnapriyan</div>
<div class="meta-line">First: 2026-02-04T18:50:10+00:00 · Latest: 2026-02-04T18:50:10+00:00</div>
<div class="meta-line">Comments: 13 pages main text, 10 pages reference &amp; appendix, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04861v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04861v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an &quot;in-the-loop&quot; model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从评估到设计：利用势能面平滑度指标指导机器学习原子间势能函数架构</div>
<div class="mono" style="margin-top:8px">机器学习原子间势能函数有时无法再现量子势能面的物理平滑性，导致下游模拟出现标准能量和力回归评估可能遗漏的错误行为。现有评估方法（如微正则分子动力学）计算成本高昂且主要探测近平衡态。为改进MLIP的评估指标，我们提出键平滑性表征测试。该高效基准通过受控键变形探测势能面，检测平衡态附近及远处的非平滑现象，包括不连续性、虚假极小值和伪力场。我们证明BSCT与MD稳定性强相关，而计算成本仅为MD的极小部分。为展示BSCT如何指导迭代模型设计，我们以无约束Transformer架构为测试平台，阐明通过可微分k近邻算法和温控注意力机制等改进措施，可有效减少该指标识别的伪影。基于BSCT系统优化模型设计后，所得MLIP同时实现了较低的传统能量/力回归误差、稳定的MD模拟和稳健的原子性质预测。本研究确立BSCT既可作为验证指标，也可作为“循环内”模型设计代理，帮助MLIP开发者识别当前基准测试无法高效评估的物理挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Machine Learning Interatomic Potentials (MLIPs) often fail to capture the physical smoothness of quantum potential energy surfaces, a flaw that standard regression metrics miss but causes errors in simulations. To address this, the authors introduce the Bond Smoothness Characterization Test (BSCT), an efficient benchmark that probes the potential energy surface through controlled bond deformations to detect non-smoothness, discontinuities, and spurious forces both near and far from equilibrium. Experimental results show BSCT strongly correlates with costly molecular dynamics stability tests while being far cheaper, and its application in guiding model design—such as incorporating a differentiable k-nearest neighbors algorithm and temperature-controlled attention into a Transformer backbone—yields an MLIP with low regression error, stable simulations, and robust property predictions.</div>
<div class="mono" style="margin-top:8px">机器学习原子间势能（MLIPs）有时无法再现量子势能面的物理平滑性，导致下游模拟出现错误，而标准的能量和力回归评估可能无法发现这些问题，且现有的评估方法如分子动力学计算成本高昂且主要探测近平衡态。为此，本文引入了键平滑性表征测试（BSCT），这是一种通过受控键变形来探测势能面的高效基准测试，能够检测包括不连续性、人工最小值和虚假力在内的非平滑现象，无论处于平衡态附近还是远离平衡态。实验结果表明，BSCT与分子动力学稳定性强相关，同时成本大幅降低；通过使用BSCT迭代指导基于Transformer的MLIP设计——例如引入可微k近邻算法和温度控制注意力等改进——最终得到的模型同时实现了较低的传统回归误差、稳定的分子动力学模拟和可靠的原子性质预测，从而将BSCT确立为一种验证指标和MLIP开发的“在环”设计代理。</div>
</details>
</div>
<div class="card">
<div class="title">Combining Residual U-Net and Data Augmentation for Dense Temporal Segmentation of Spike Wave Discharges in Single-Channel EEG</div>
<div class="meta-line">Authors: Saurav Sengupta, Scott Kilianski, Suchetha Sharma, Sakina Lashkeri, Ashley McHugh, Mark Beenhakker, Donald E. Brown</div>
<div class="meta-line">First: 2026-01-01T19:58:20+00:00 · Latest: 2026-02-04T18:43:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00459v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00459v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manual annotation of spike-wave discharges (SWDs), the electrographic hallmark of absence seizures, is labor-intensive for long-term electroencephalography (EEG) monitoring studies. While machine learning approaches show promise for automated detection, they often struggle with cross-subject generalization due to high inter-individual variability in seizure morphology and signal characteristics. In this study we compare the performance of 15 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs and find that a 1D U-Net performs the best. We then improve its performance by employing residual connections and data augmentation strategies combining amplitude scaling, Gaussian noise injection, and signal inversion during training to enhance cross-subject generalization. We also compare our method, named AugUNet1D, to a recently published time- and frequency-based algorithmic approach called &quot;Twin Peaks&quot; and show that AugUNet1D performs better on our dataset. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for other users.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结合残差U-Net与数据增强的单通道脑电棘慢波密集时序分割方法</div>
<div class="mono" style="margin-top:8px">棘慢波放电（SWD）作为失神发作的脑电图标志，其人工标注在长期脑电监测研究中极为耗时。尽管机器学习方法在自动检测方面展现出潜力，但由于发作形态和信号特征存在显著的个体间差异，这些方法在跨被试泛化方面常面临挑战。本研究基于自建的C3H/HeJ小鼠961小时脑电数据集（包含22,637个标注SWD），比较了15种机器学习分类器的性能，发现一维U-Net表现最佳。通过引入残差连接及结合幅度缩放、高斯噪声注入和信号反转的数据增强策略，我们进一步提升了模型在跨被试泛化方面的性能。将本方法（命名为AugUNet1D）与近期发表的时频域算法“Twin Peaks”进行比较，结果显示AugUNet1D在我们的数据集上表现更优。我们公开了基于标注数据预训练及未训练的AugUNet1D模型供其他研究者使用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Manual annotation of spike-wave discharges (SWDs) in long-term EEG is laborious, and existing machine learning methods often fail to generalize across subjects due to high inter-individual variability. To address this, the authors evaluated 15 classifiers on a manually annotated dataset of 961 hours of mouse EEG and found a 1D U-Net performed best; they then enhanced it with residual connections and data augmentation (amplitude scaling, Gaussian noise, and signal inversion) to improve cross-subject generalization, naming the method AugUNet1D. Experimental results show that AugUNet1D outperforms both the baseline U-Net and a recent algorithmic approach called &quot;Twin Peaks&quot; on their dataset, and the model is made publicly available.</div>
<div class="mono" style="margin-top:8px">长程脑电图中棘波放电的手动标注十分耗时，且由于个体间差异，现有机器学习方法的跨被试泛化能力往往不佳。为此，作者首先在961小时手动标注的小鼠脑电图数据集上评估了15种分类器，确定1D U-Net性能最佳；随后通过引入残差连接和数据增强（包括幅度缩放、高斯噪声注入和信号反转）来提升跨被试泛化能力，该方法被命名为AugUNet1D。实验结果表明，AugUNet1D在其数据集上优于基线U-Net和近期发表的“Twin Peaks”算法，该模型已公开提供使用。</div>
</details>
</div>
<div class="card">
<div class="title">Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations</div>
<div class="meta-line">Authors: Aroon Sankoh, Victor Wickerhauser</div>
<div class="meta-line">First: 2025-05-06T21:07:53+00:00 · Latest: 2026-02-04T18:42:42+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.03980v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.03980v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stochastic differential equations such as the Ornstein-Uhlenbeck process have long been used to model realworld probablistic events such as stock prices and temperature fluctuations. While statistical methods such as Maximum Likelihood Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have historically been used to estimate the parameters of stochastic differential equations, the recent explosion of deep learning technology suggests that models such as a Recurrent Neural Network (RNN) could produce more precise estimators. We present a series of experiments that compare the estimation accuracy and computational expensiveness of a statistical method (MLE) with a deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续时间随机微分方程参数估计的统计方法与深度学习技术比较</div>
<div class="mono" style="margin-top:8px">随机微分方程（如奥恩斯坦-乌伦贝克过程）长期用于模拟股票价格和温度波动等现实世界概率事件。传统上常采用最大似然估计、卡尔曼滤波、逆变量法等统计方法估计随机微分方程参数，而近期深度学习技术的兴起表明，循环神经网络等模型可能产生更精确的估计量。本文通过系列实验，比较了统计方法（最大似然估计）与深度学习模型（循环神经网络）在奥恩斯坦-乌伦贝克过程参数估计中的精度与计算成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study compares traditional statistical methods with deep learning approaches for estimating parameters of continuous-time stochastic differential equations, motivated by the need to model real-world probabilistic phenomena like stock prices and temperature fluctuations. The authors conduct experiments that pit Maximum Likelihood Estimation (MLE) against a Recurrent Neural Network (RNN) to estimate the parameters of the Ornstein-Uhlenbeck process. The main experimental findings evaluate and compare the estimation accuracy and computational cost of these two techniques.</div>
<div class="mono" style="margin-top:8px">本研究比较了传统统计方法与深度学习方法在连续时间随机微分方程参数估计上的性能，其动机在于像奥恩斯坦-乌伦贝克过程这样的模型在金融和气候科学等领域的广泛应用。方法上，通过实验直接对比了最大似然估计与循环神经网络在奥恩斯坦-乌伦贝克过程参数估计中的准确性和计算成本。主要实验结果表明，循环神经网络模型能够产生更精确的参数估计器，同时其计算开销的权衡也得到了评估。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization</div>
<div class="meta-line">Authors: Luca Della Libera, Cem Subakan, Mirco Ravanelli</div>
<div class="meta-line">First: 2026-01-30T16:58:40+00:00 · Latest: 2026-02-04T18:42:12+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23174v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.23174v2">PDF</a> · <a href="https://github.com/lucadellalib/dycast">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越固定帧率：动态字符对齐的语音分词技术</div>
<div class="mono" style="margin-top:8px">神经音频编解码器是现代对话语音技术的核心，它将连续语音转换为可由大语言模型处理的离散标记序列。然而，现有编解码器通常以固定帧率运行，在时间上均匀分配标记，导致序列长度不必要地增加。本研究提出DyCAST，一种动态字符对齐的语音分词器，通过软字符级对齐和显式时长建模实现可变帧率分词。DyCAST在训练中学习将标记与字符级语言单元关联，并在解码时支持无需对齐的推理，直接控制标记时长。为提升低帧率下的语音重合成质量，我们进一步引入检索增强解码机制，在不增加比特率的情况下提高重建保真度。实验表明，DyCAST在使用显著少于固定帧率编解码器的标记数时，仍能实现具有竞争力的语音重合成质量与下游任务性能。代码与模型检查点将在https://github.com/lucadellalib/dycast公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the inefficiency of fixed-frame-rate neural audio codecs that produce unnecessarily long token sequences, this work introduces DyCAST, a dynamic character-aligned speech tokenizer. The method enables variable-frame-rate tokenization by learning soft character-level alignments and explicit duration modeling during training, allowing for alignment-free inference with direct control over token durations, and incorporates a retrieval-augmented decoding mechanism to enhance resynthesis quality at low bitrates. Experimental results demonstrate that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.</div>
<div class="mono" style="margin-top:8px">针对固定帧率神经音频编解码器产生不必要长令牌序列的低效问题，本研究提出了DyCAST，一种动态字符对齐的语音分词器。该方法通过训练时学习软字符级对齐和显式时长建模，实现可变帧率分词，支持解码时直接控制时长的无对齐推理，并进一步引入检索增强解码机制以提升低码率下的重建保真度。实验结果表明，DyCAST在使用显著更少令牌的同时，实现了与固定帧率编解码器相竞争的语音重建质量和下游任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">The Key to State Reduction in Linear Attention: A Rank-based Perspective</div>
<div class="meta-line">Authors: Philipp Nazari, T. Konstantin Rusch</div>
<div class="meta-line">First: 2026-02-04T18:39:38+00:00 · Latest: 2026-02-04T18:39:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04852v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04852v1">PDF</a> · <a href="https://github.com/camail-official/LinearAttentionPruning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>线性注意力中状态缩减的关键：基于秩的视角</div>
<div class="mono" style="margin-top:8px">线性注意力作为一种计算高效且表达能力强的替代方案，相较于softmax注意力具有优势。然而，近期实证研究表明，训练后的线性注意力模型状态常呈现低秩结构，暗示这些模型在实践中未能充分利用其容量。为阐明此现象，我们从理论角度分析了秩在线性注意力中的作用，揭示低有效秩可能通过放大查询噪声影响检索误差。除理论洞见外，我们推测低秩状态可在训练后大幅缩减，仅伴随轻微性能损失，从而获得更快、内存效率更高的模型。为此，我们提出一种新颖的硬件感知方法，通过结构化剪枝关键矩阵与查询矩阵来缩减状态规模，同时保持与现有CUDA内核的兼容性。我们调整了多种现有剪枝策略以适应本框架，并基于理论分析提出一种基于秩揭示QR分解的新型结构化剪枝方法。在不同规模模型及多样下游任务中的实证结果表明，我们的状态缩减框架具有显著效果。我们特别指出，该框架能在困惑度仅边际上升的情况下移除50%的查询与关键通道。项目代码详见：https://github.com/camail-official/LinearAttentionPruning。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the low-rank structure observed in trained linear attention models, which suggests underutilized capacity and can amplify query noise, affecting retrieval error. To address this, the authors propose a hardware-aware structured pruning method that reduces the state size by pruning key and query matrices, introducing a novel rank-revealing QR decomposition-based approach alongside adapted existing strategies. Experimental results across various model sizes and tasks show that the framework can remove 50% of query and key channels with only a marginal increase in perplexity, yielding faster and more memory-efficient models.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究训练后的线性注意力模型为何常呈现低秩状态，这表明其容量未被充分利用。作者通过理论分析表明低有效秩会放大查询噪声并影响检索误差，随后提出一种硬件感知的结构化剪枝方法，通过移除键和查询矩阵通道来减小状态规模，同时保持与现有CUDA内核的兼容性。在不同规模和下游任务上的实验结果表明，该框架能够移除50%的查询和键通道，仅带来边际困惑度增加，从而获得更快且更节省内存的模型。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-04T18:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人机协同贝叶斯优化的个性化图像生成</div>
<div class="mono" style="margin-top:8px">假设Alice心中有一幅特定图像$x^\ast$，例如她童年时代成长街道的景象。为生成该图像，她通过多轮提示引导生成模型，得到图像$x^{p*}$。虽然$x^{p*}$已接近$x^\ast$，但Alice发现难以通过语言提示完全消除差异。本文指出：即使语言描述已达极限，人类仍能判断新图像$x^+$是否比$x^{p*}$更接近$x^\ast$。基于此，我们提出MultiBO（多选择偏好贝叶斯优化）方法：以$x^{p*}$为基准生成$K$幅新图像，获取用户偏好反馈，利用反馈指导扩散模型，最终生成新一轮$K$幅图像。研究表明，在$B$轮用户反馈内，即使生成模型未获得$x^\ast$的直接信息，仍能大幅逼近目标图像。通过30位用户的定性评分，以及与5个基线模型的定量指标对比，实验结果表明人类的多选择反馈能有效助力个性化图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of personalized image generation when language prompts alone are insufficient to capture a user&#x27;s specific mental image. The proposed method, Multi-Choice Preferential Bayesian Optimization (MultiBO), iteratively refines an initial generated image by presenting the user with multiple new image choices, collecting preferential feedback on which is closer to the target, and using this feedback to guide a diffusion model. Experimental results from 30 users and quantitative comparisons against 5 baselines demonstrate that this human-in-the-loop approach can significantly narrow the gap to the target image within a limited number of feedback rounds.</div>
<div class="mono" style="margin-top:8px">本研究针对仅凭语言提示难以精确生成用户心中特定图像的问题，提出了一种解决方案。该方法名为多选择偏好贝叶斯优化（MultiBO），它通过向用户展示K个新生成的候选图像、收集用户关于哪个图像更接近目标图像的偏好反馈，并利用该反馈指导扩散模型，从而迭代优化初始生成结果。来自30名用户的定性评分以及与5个基线方法的定量比较结果表明，这种人在回路的交互方法能够在有限的反馈轮次内，显著缩小生成图像与目标图像之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">OverThink: Slowdown Attacks on Reasoning LLMs</div>
<div class="meta-line">Authors: Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian</div>
<div class="meta-line">First: 2025-02-04T18:12:41+00:00 · Latest: 2026-02-04T18:30:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.02542v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.02542v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most flagship language models generate explicit reasoning chains, enabling inference-time scaling. However, producing these reasoning chains increases token usage (i.e., reasoning tokens), which in turn increases latency and costs. Our OverThink attack increases overhead for applications that rely on reasoning language models (RLMs) and external context by forcing them to spend substantially more reasoning tokens while still producing contextually correct answers. An adversary mounts an attack by injecting decoy reasoning problems into public content that is consumed by RLM at inference time. Because our decoys (e.g., Markov decision processes, Sudokus, etc.) are benign, they evade safety filters. We evaluate OverThink on both closed-source and open-source reasoning models across the FreshQA, SQuAD, and MuSR datasets. We also explore the attack in multi-modal settings by creating images that cause excessive reasoning. We show that the resulting slowdown transfers across models. Finally, we explore both LLM-based and systems-level defenses, and discuss the societal, financial, and energy implications of the OverThink attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>过度思考：针对推理型大语言模型的减速攻击</div>
<div class="mono" style="margin-top:8px">多数主流语言模型会生成显式推理链，支持推理时扩展。然而，生成这些推理链会增加令牌使用量（即推理令牌），进而导致延迟上升与成本增加。我们的&#x27;过度思考&#x27;攻击通过迫使依赖推理语言模型（RLM）及外部上下文的应用消耗显著更多的推理令牌（同时仍生成上下文正确的答案），增加了此类应用的运行开销。攻击者通过向推理时被RLM读取的公共内容中注入伪装推理问题（如马尔可夫决策过程、数独等）发起攻击。由于这些伪装问题本身无害，可规避安全过滤机制。我们在FreshQA、SQuAD和MuSR数据集上对闭源与开源推理模型进行了&#x27;过度思考&#x27;攻击评估，并探索了通过生成引发过度推理的图像实现多模态场景攻击。实验表明，由此产生的减速效应在不同模型间具有迁移性。最后，我们探讨了基于大语言模型和系统层级的防御方案，并分析了&#x27;过度思考&#x27;攻击对社会、财务及能源消耗的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that reasoning language models (RLMs) incur increased latency and costs due to generating explicit reasoning chains, which adversaries could exploit to degrade performance. The proposed OverThink attack forces RLMs to consume substantially more reasoning tokens by injecting benign decoy problems (e.g., Markov decision processes, Sudokus) into public content, evading safety filters while still producing correct answers. Experimental evaluations on closed-source and open-source models across FreshQA, SQuAD, and MuSR datasets demonstrate significant slowdowns that transfer across models, with extensions to multi-modal settings; the study also explores LLM-based and systems-level defenses and discusses broader societal implications.</div>
<div class="mono" style="margin-top:8px">该研究的动机是观察到推理语言模型因生成显式推理链而消耗额外令牌，导致延迟和成本增加。OverThink攻击通过向推理模型处理的公共内容中注入良性的诱饵推理问题（如马尔可夫决策过程或数独），迫使模型消耗大量推理令牌但仍能给出正确答案，从而规避安全过滤器。在FreshQA、SQuAD和MuSR数据集上对闭源和开源模型的实验评估表明，攻击能有效引发减速效果，且这种效果在不同模型间可迁移；研究还探讨了多模态扩展、潜在防御措施以及社会影响。</div>
</details>
</div>
<div class="card">
<div class="title">Grammatical Error Correction for Low-Resource Languages: The Case of Zarma</div>
<div class="meta-line">Authors: Mamadou K. Keita, Adwoa Bremang, Huy Le, Dennis Owusu, Christopher Homan, Marcos Zampieri</div>
<div class="meta-line">First: 2024-10-20T23:51:36+00:00 · Latest: 2026-02-04T18:29:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.15539v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.15539v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Grammatical error correction (GEC) aims to improve text quality and readability. Previous work on the task focused primarily on high-resource languages, while low-resource languages lack robust tools. To address this shortcoming, we present a study on GEC for Zarma, a language spoken by over five million people in West Africa. We compare three approaches: rule-based methods, machine translation (MT) models, and large language models (LLMs). We evaluated GEC models using a dataset of more than 250,000 examples, including synthetic and human-annotated data. Our results showed that the MT-based approach using M2M100 outperforms others, with a detection rate of 95.82% and a suggestion accuracy of 78.90% in automatic evaluations (AE) and an average score of 3.0 out of 5.0 in manual evaluation (ME) from native speakers for grammar and logical corrections. The rule-based method was effective for spelling errors but failed on complex context-level errors. LLMs -- Gemma 2b and MT5-small -- showed moderate performance. Our work supports use of MT models to enhance GEC in low-resource settings, and we validated these results with Bambara, another West African language.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低资源语言的语法纠错：以扎尔马语为例</div>
<div class="mono" style="margin-top:8px">语法纠错旨在提升文本质量与可读性。先前研究主要集中于高资源语言，而低资源语言缺乏可靠工具。为弥补这一不足，本研究针对西非超五百万人使用的扎尔马语开展语法纠错探索，对比了基于规则的方法、机器翻译模型与大语言模型三种方案。我们使用包含合成数据与人工标注数据在内的25万余条样本数据集进行评估。结果显示：基于M2M100的机器翻译方法在自动评估中取得95.82%的检测率与78.90%的修正准确率，在母语者对语法逻辑修正的人工评估中获得3.0/5.0的平均分，表现优于其他方法。基于规则的方法能有效处理拼写错误，但难以应对复杂语境错误。大语言模型（Gemma 2b与MT5-small）表现中等。本研究证实了机器翻译模型在低资源场景下提升语法纠错的有效性，并在另一西非语言班巴拉语中验证了该结论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the lack of grammatical error correction (GEC) tools for low-resource languages by focusing on Zarma, spoken by millions in West Africa. The research compares three approaches: rule-based methods, machine translation (MT) models like M2M100, and large language models (LLMs) such as Gemma 2b and MT5-small, evaluated on a dataset of over 250,000 synthetic and human-annotated examples. Experimental results show that the MT-based approach achieved the best performance with a 95.82% detection rate and 78.90% suggestion accuracy in automatic evaluations, along with an average manual score of 3.0/5.0 from native speakers; rule-based methods were effective only for spelling errors, while LLMs showed moderate performance, and the findings were validated on Bambara, another low-resource language.</div>
<div class="mono" style="margin-top:8px">本研究针对低资源语言缺乏语法纠错工具的问题，以西非数百万人使用的哲尔马语（Zarma）为例展开研究。它比较了三种方法：基于规则的方法、使用M2M100的机器翻译模型以及Gemma 2b和MT5-small等大语言模型，并在超过25万个合成和人工标注的示例数据集上进行了评估。基于机器翻译的方法取得了最佳结果，在自动评估中检测率达到95.82%，建议准确率为78.90%，母语者人工评估的平均得分为3.0/5.0；基于规则的方法仅对拼写错误有效，大语言模型表现中等，这些发现在班巴拉语（Bambara）上得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</div>
<div class="meta-line">Authors: Zhao Tong, Chunlin Gong, Yimeng Gu, Haichao Shi, Qiang Liu, Shu Wu, Xiao-Yu Zhang</div>
<div class="meta-line">First: 2025-10-10T04:39:57+00:00 · Latest: 2026-02-04T18:29:24+00:00</div>
<div class="meta-line">Comments: 10 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09712v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09712v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online fake news profoundly distorts public judgment and erodes trust in social platforms. While existing detectors achieve competitive performance on benchmark datasets, they remain notably vulnerable to malicious comments designed specifically to induce misclassification. This evolving threat landscape necessitates detection systems that simultaneously prioritize predictive accuracy and structural robustness. However, current detectors often fail to generalize across diverse and novel comment attack patterns. To bridge this gap, we propose AdComment, an adaptive adversarial training framework for robustness enhancement against diverse malicious comments. Based on cognitive psychology, we categorize adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and leverage LLMs to synthesize diverse, category-specific perturbations. Central to our framework is an InfoDirichlet Resampling (IDR) mechanism that dynamically adjusts malicious comment proportions during training, thereby steering optimization toward the model&#x27;s most susceptible regions. Experimental results demonstrate that our approach achieves state-of-the-art performance on three benchmark datasets, improving the F1 scores by 17.9%, 14.5% and 9.0%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向恶意评论的鲁棒虚假新闻检测：群体自适应对抗学习</div>
<div class="mono" style="margin-top:8px">网络虚假新闻严重扭曲公众判断并侵蚀对社交平台的信任。现有检测器在基准数据集上虽具竞争力，但面对专门诱导误判的恶意评论仍存在显著脆弱性。这一不断演变的威胁态势要求检测系统必须同时兼顾预测准确性与结构鲁棒性。然而当前检测器往往难以泛化至多样化的新型评论攻击模式。为填补这一空白，我们提出AdComment——一种针对多样化恶意评论的鲁棒性增强自适应对抗训练框架。基于认知心理学，我们将对抗性评论划分为事实扭曲、逻辑混淆和情感操纵三类，并利用大语言模型合成多样化的类别特异性扰动。该框架的核心是信息狄利克雷重采样机制，能在训练过程中动态调整恶意评论比例，从而将优化导向模型最敏感区域。实验结果表明，我们的方法在三个基准数据集上均取得最先进性能，F1分数分别提升17.9%、14.5%和9.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the vulnerability of existing fake news detectors to malicious comments designed to induce misclassification, necessitating systems that balance predictive accuracy with structural robustness. The proposed method, AdComment, is an adaptive adversarial training framework that first categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation based on cognitive psychology, then uses LLMs to synthesize diverse, category-specific perturbations, and employs an InfoDirichlet Resampling mechanism to dynamically adjust malicious comment proportions during training. Key experimental findings show the approach achieves state-of-the-art performance on three benchmark datasets, improving F1 scores by 17.9%, 14.5%, and 9.0% respectively.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有虚假新闻检测器易受诱导错误分类的恶意评论攻击，需要构建兼顾预测准确性和结构鲁棒性的系统。所提出的方法AdComment是一种自适应对抗训练框架，首先基于认知心理学将对抗性评论分类为事实扭曲等类型，利用大语言模型合成特定类别的扰动，并采用信息狄利克雷重采样机制在训练中动态调整恶意评论比例。关键实验结果在三个基准数据集上实现了最先进的性能，F1分数分别提升了17.9%、14.5%和9.0%。</div>
</details>
</div>
<div class="card">
<div class="title">It&#x27;s not a Lottery, it&#x27;s a Race: Understanding How Gradient Descent Adapts the Network&#x27;s Capacity to the Task</div>
<div class="meta-line">Authors: Hannah Pinson</div>
<div class="meta-line">First: 2026-02-04T18:22:40+00:00 · Latest: 2026-02-04T18:22:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04832v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Our theoretical understanding of neural networks is lagging behind their empirical success. One of the important unexplained phenomena is why and how, during the process of training with gradient descent, the theoretical capacity of neural networks is reduced to an effective capacity that fits the task. We here investigate the mechanism by which gradient descent achieves this through analyzing the learning dynamics at the level of individual neurons in single hidden layer ReLU networks. We identify three dynamical principles -- mutual alignment, unlocking and racing -- that together explain why we can often successfully reduce capacity after training through the merging of equivalent neurons or the pruning of low norm weights. We specifically explain the mechanism behind the lottery ticket conjecture, or why the specific, beneficial initial conditions of some neurons lead them to obtain higher weight norms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>这不是彩票，而是竞赛：理解梯度下降如何使网络能力适应任务</div>
<div class="mono" style="margin-top:8px">我们对神经网络的理论理解滞后于其实际成功。一个重要未解现象是：在梯度下降训练过程中，神经网络的理论能力为何及如何被缩减为适应任务的有效能力。本文通过分析单隐藏层ReLU网络中单个神经元的学习动态，探究梯度下降实现这一过程的机制。我们提出三个动态原则——相互对齐、解锁与竞赛——共同解释了为何训练后常能通过合并等效神经元或修剪低范数权重来成功缩减能力。我们特别阐释了彩票假设背后的机制，即为何某些神经元特定的有利初始条件会使其获得更高的权重范数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to explain the gap between the high theoretical capacity of neural networks and their effective, task-appropriate capacity after training with gradient descent. By analyzing learning dynamics in single hidden-layer ReLU networks at the level of individual neurons, the authors identify three key mechanisms: mutual alignment, unlocking, and racing. These principles collectively explain why post-training capacity reduction techniques like neuron merging and weight pruning are effective, and specifically elucidate the mechanism behind the lottery ticket hypothesis, showing how beneficial initial conditions allow certain neurons to develop higher weight norms.</div>
<div class="mono" style="margin-top:8px">本研究旨在解释神经网络的高理论容量与经过梯度下降训练后获得的、适应任务的有效容量之间的差距。通过分析单隐藏层ReLU网络中单个神经元的学习动态，作者识别了三个关键机制：相互对齐、解锁和竞争。实验结果表明，这些动态解释了为何在训练后合并等效神经元或修剪低范数权重等降低容量的技术是有效的，并特别阐明了彩票假设背后的机制，即某些有益的初始条件为何能使神经元获得更高的权重范数。</div>
</details>
</div>
<div class="card">
<div class="title">Guardrailed Uplift Targeting: A Causal Optimization Playbook for Marketing Strategy</div>
<div class="meta-line">Authors: Deepit Sapru</div>
<div class="meta-line">First: 2025-12-22T19:02:09+00:00 · Latest: 2026-02-04T18:18:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19805v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19805v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a marketing decision framework that optimizes customer targeting by integrating heterogeneous treatment effect estimation with explicit business guardrails. The objective is to maximize revenue and retention while adhering to constraints such as budget, revenue protection, and customer experience. The framework first estimates Conditional Average Treatment Effects (CATE) using uplift learners, then solves a constrained allocation problem to decide whom to target and which offer to deploy. It supports decisions in retention messaging, event rewards, and spend-threshold assignment. Validated through offline simulations and online A/B tests, the approach consistently outperforms propensity and static baselines, offering a reusable playbook for causal targeting at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>护栏式提升目标定位：营销策略的因果优化操作手册</div>
<div class="mono" style="margin-top:8px">本文提出一种营销决策框架，通过将异质性处理效应估计与明确的业务护栏相结合，优化客户目标定位。该框架旨在最大化收入与留存率，同时遵守预算、收入保护及客户体验等约束条件。首先利用提升学习器估计条件平均处理效应（CATE），随后通过求解约束分配问题决定目标客户与优惠策略。该框架适用于留存信息推送、活动奖励及消费阈值设定等场景。经离线模拟与在线A/B测试验证，该方法持续优于倾向性评分与静态基线模型，为规模化因果目标定位提供了可复用的操作手册。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for marketing strategies that maximize revenue and retention while respecting practical business constraints like budget and customer experience. The proposed framework first estimates heterogeneous treatment effects using Conditional Average Treatment Effect (CATE) models, then solves a constrained optimization problem to allocate offers and target customers. Experimental validation through offline simulations and online A/B tests demonstrates that this guardrailed uplift targeting approach consistently outperforms traditional propensity-based and static targeting baselines.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决营销策略中如何在预算和客户体验等实际业务约束下最大化收入和用户留存的需求。提出的框架首先使用条件平均处理效应模型估计异质性处理效应，然后通过求解约束优化问题来分配优惠并确定目标客户。通过离线模拟和在线A/B测试的实验验证表明，这种带有防护栏的提升度定向方法持续优于传统的基于倾向性评分和静态定向的基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Domain Generalization Under Posterior Drift</div>
<div class="meta-line">Authors: Yilun Zhu, Naihao Deng, Naichen Shi, Aditya Gangrade, Clayton Scott</div>
<div class="meta-line">First: 2025-10-06T02:17:12+00:00 · Latest: 2026-02-04T18:18:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04441v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04441v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Domain generalization (DG) is the problem of generalizing from several distributions (or domains), for which labeled training data are available, to a new test domain for which no labeled data is available. For the prevailing benchmark datasets in DG, there exists a single classifier that performs well across all domains.
  In this work, we study a fundamentally different regime where the domains satisfy a \emph{posterior drift} assumption, in which the optimal classifier might vary substantially with domain. We establish a decision-theoretic framework for DG under posterior drift, and investigate the practical implications of this framework through experiments on language and vision tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后验漂移下的领域泛化</div>
<div class="mono" style="margin-top:8px">领域泛化（DG）旨在从多个具有标注训练数据的分布（或领域）中学习，以泛化至无标注数据的新测试领域。当前主流DG基准数据集通常存在一个在所有领域均表现良好的单一分类器。本研究探讨了一种本质不同的场景：各领域满足\emph{后验漂移}假设，即最优分类器可能随领域发生显著变化。我们建立了后验漂移下DG的决策理论框架，并通过语言与视觉任务的实验验证了该框架的实际意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses domain generalization (DG) in a regime where the optimal classifier varies substantially across domains, a scenario termed posterior drift, which contrasts with prevailing benchmarks where a single classifier performs well universally. The authors establish a decision-theoretic framework for DG under this assumption and investigate its practical implications through experiments on language and vision tasks, demonstrating the framework&#x27;s relevance in settings with significant inter-domain classifier variation.</div>
<div class="mono" style="margin-top:8px">本研究针对后验漂移下的领域泛化问题，即最优分类器在不同领域间可能显著变化，这与现有基准中单一分类器通常有效的场景不同。作者建立了一个决策理论框架来建模此问题，并通过语言和视觉任务的实验探究其实际影响。实验结果验证了该框架的适用性，并揭示了在后验漂移显著时泛化所面临的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Attention Consistency Regularization for Interpretable Early-Exit Neural Networks</div>
<div class="meta-line">Authors: Yanhua Zhao</div>
<div class="meta-line">First: 2026-01-13T11:19:09+00:00 · Latest: 2026-02-04T18:16:12+00:00</div>
<div class="meta-line">Comments: 2 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08891v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08891v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early-exit neural networks enable adaptive inference by allowing predictions at intermediate layers, reducing computational cost. However, early exits often lack interpretability and may focus on different features than deeper layers, limiting trust and explainability. This paper presents Explanation-Guided Training (EGT), a multi-objective framework that improves interpretability and consistency in early-exit networks through attention-based regularization. EGT introduces an attention consistency loss that aligns early-exit attention maps with the final exit. The framework jointly optimizes classification accuracy and attention consistency through a weighted combination of losses. Experiments on a real-world image classification dataset demonstrate that EGT achieves up to 98.97% overall accuracy (matching baseline performance) with a 1.97x inference speedup through early exits, while improving attention consistency by up to 18.5% compared to baseline models. The proposed method provides more interpretable and consistent explanations across all exit points, making early-exit networks more suitable for explainable AI applications in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向可解释早退神经网络的注意力一致性正则化方法</div>
<div class="mono" style="margin-top:8px">早退神经网络通过在中间层进行预测实现自适应推理，从而降低计算成本。然而，早退出口通常缺乏可解释性，且可能关注与深层不同的特征，限制了可信度与可解释性。本文提出解释引导训练（EGT）——一种通过基于注意力的正则化提升早退网络可解释性与一致性的多目标框架。EGT引入注意力一致性损失函数，使早退注意力图与最终出口对齐。该框架通过加权组合损失函数，联合优化分类准确率与注意力一致性。在真实图像分类数据集上的实验表明，EGT通过早退机制实现1.97倍推理加速的同时，达到98.97%的整体准确率（与基线性能持平），且注意力一致性较基线模型提升达18.5%。该方法在所有出口点提供更具可解释性与一致性的解释，使早退网络更适用于资源受限环境下的可解释人工智能应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Early-exit neural networks reduce computational cost but often lack interpretability and feature consistency across exits, limiting trust. To address this, the paper proposes Explanation-Guided Training (EGT), a multi-objective framework that introduces an attention consistency loss to align early-exit attention maps with the final exit, jointly optimizing classification and consistency. Experiments on an image classification dataset show EGT achieves 98.97% overall accuracy with a 1.97x inference speedup, while improving attention consistency by up to 18.5% compared to baselines, yielding more interpretable explanations across exits.</div>
<div class="mono" style="margin-top:8px">早期退出神经网络可降低计算成本，但其早期退出层通常缺乏可解释性，且与深层关注特征不一致，限制了可信度。为此，本文提出解释引导训练（EGT）框架，通过引入注意力一致性损失来对齐早期退出与最终退出的注意力图，联合优化分类准确性和一致性。在真实图像分类数据集上的实验表明，EGT实现了98.97%的整体准确率和1.97倍的推理加速，同时将注意力一致性较基线模型提升高达18.5%，从而在所有退出点提供了更可解释且一致的预测解释。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Urban Traffic Control via Uncertainty-Aware Conformal Prediction and World-Model Reinforcement Learning</div>
<div class="meta-line">Authors: Joydeep Chandra, Satyam Kumar Navneet, Aleksandr Algazinov, Yong Zhang</div>
<div class="meta-line">First: 2026-02-04T18:10:59+00:00 · Latest: 2026-02-04T18:10:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04821v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Urban traffic management demands systems that simultaneously predict future conditions, detect anomalies, and take safe corrective actions -- all while providing reliability guarantees. We present STREAM-RL, a unified framework that introduces three novel algorithmic contributions: (1) PU-GAT+, an Uncertainty-Guided Adaptive Conformal Forecaster that uses prediction uncertainty to dynamically reweight graph attention via confidence-monotonic attention, achieving distribution-free coverage guarantees; (2) CRFN-BY, a Conformal Residual Flow Network that models uncertainty-normalized residuals via normalizing flows with Benjamini-Yekutieli FDR control under arbitrary dependence; and (3) LyCon-WRL+, an Uncertainty-Guided Safe World-Model RL agent with Lyapunov stability certificates, certified Lipschitz bounds, and uncertainty-propagated imagination rollouts. To our knowledge, this is the first framework to propagate calibrated uncertainty from forecasting through anomaly detection to safe policy learning with end-to-end theoretical guarantees. Experiments on multiple real-world traffic trajectory data demonstrate that STREAM-RL achieves 91.4\% coverage efficiency, controls FDR at 4.1\% under verified dependence, and improves safety rate to 95.2\% compared to 69\% for standard PPO while achieving higher reward, with 23ms end-to-end inference latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于不确定性感知共形预测与世界模型强化学习的城市交通安全控制</div>
<div class="mono" style="margin-top:8px">城市交通管理系统需同时预测未来状况、检测异常并采取安全纠正措施，同时提供可靠性保证。我们提出STREAM-RL统一框架，包含三项创新算法贡献：(1) PU-GAT+：不确定性引导的自适应共形预测器，通过置信度单调注意力动态调整图注意力权重，实现无分布覆盖保证；(2) CRFN-BY：共形残差流网络，通过归一化流建模不确定性归一化残差，在任意依赖关系下实现Benjamini-Yekutieli错误发现率控制；(3) LyCon-WRL+：具备李雅普诺夫稳定性证明、经认证的利普希茨边界及不确定性传播想象推演的引导式安全世界模型强化学习智能体。据我们所知，这是首个将校准不确定性从预测端经异常检测传递至安全策略学习，并具备端到端理论保证的框架。在多个真实交通轨迹数据上的实验表明：STREAM-RL实现91.4%的覆盖效率，在已验证依赖关系下将错误发现率控制在4.1%，安全率提升至95.2%（标准PPO为69%），同时获得更高奖励，端到端推理延迟为23毫秒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Urban traffic management requires systems that can predict future conditions, detect anomalies, and execute safe control actions with reliability assurances. The proposed STREAM-RL framework addresses this by integrating three novel components: PU-GAT+ for uncertainty-guided adaptive conformal forecasting with coverage guarantees, CRFN-BY for anomaly detection using conformal residual flows with false discovery rate control, and LyCon-WRL+ for safe reinforcement learning with world models and stability certificates. Experimental evaluation on real-world traffic data shows the system achieves 91.4% coverage efficiency, maintains a 4.1% false discovery rate, improves the safety rate to 95.2% from a baseline of 69%, and delivers end-to-end inference in 23ms.</div>
<div class="mono" style="margin-top:8px">城市交通控制需要能够可靠预测、检测异常并安全行动且具有形式化保证的系统。为此，作者提出了STREAM-RL这一统一框架，集成了三个新颖组件：PU-GAT+，一种不确定性引导的共形预测器，通过动态图注意力实现无分布覆盖保证；CRFN-BY，一种用于异常检测的共形残差流网络，在依赖下控制FDR；以及LyCon-WRL+，一种具有李雅普诺夫稳定性和不确定性传播推演的安全世界模型强化学习智能体。在真实交通轨迹数据上的实验表明，该框架实现了91.4%的覆盖效率，将FDR控制在4.1%，将安全率从标准PPO的69%提升至95.2%，获得了更高奖励，并保持23毫秒的端到端推理延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization</div>
<div class="meta-line">Authors: Farzia Hossain, Samanta Ghosh, Shahida Begum, B. M. Shahria Alam, Mohammad Tahmid Noor, Md Parvez Mia, Nishat Tasnim Niloy</div>
<div class="meta-line">First: 2026-02-04T18:08:13+00:00 · Latest: 2026-02-04T18:08:13+00:00</div>
<div class="meta-line">Comments: 6 pages, 12 figures. This is the author&#x27;s accepted manuscript of a paper accepted for publication in the Proceedings of the 16th International IEEE Conference on Computing, Communication and Networking Technologies (ICCCNT 2025). The final published version will be available via IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body&#x27;s health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向可靠且可解释的指甲疾病分类：利用对抗训练与Grad-CAM可视化</div>
<div class="mono" style="margin-top:8px">人类指甲疾病在各年龄段逐渐显现，尤其在老年群体中常被忽视直至恶化。早期检测与准确诊断至关重要，因其可能反映身体健康问题，但疾病类型间视觉差异细微，诊断面临挑战。本文基于包含3,835张图像、涵盖六类疾病的公开数据集，提出一种机器学习驱动的自动分类模型。所有图像统一调整为224×224像素以确保一致性。为评估性能，对InceptionV3、DenseNet201、EfficientNetV2和ResNet50四种经典CNN模型进行训练分析，其中InceptionV3以95.57%准确率表现最优，DenseNet201以94.79%次之。通过对抗训练增强模型对复杂或噪声图像的鲁棒性，并采用SHAP方法可视化预测关键特征以提升决策可解释性。该系统有望辅助医生实现更精准、高效的指甲疾病诊断。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for early detection of nail diseases, which can indicate underlying health issues but are challenging to diagnose due to subtle visual differences, this study develops an automated classification system. The method involves training and comparing four CNN architectures (InceptionV3, DenseNet201, EfficientNetV2, ResNet50) on a dataset of 3,835 nail images, employing adversarial training to enhance robustness and SHAP for explainability via feature visualization. Experimentally, InceptionV3 achieved the highest accuracy of 95.57%, followed by DenseNet201 at 94.79%, demonstrating the model&#x27;s potential as a decision-support tool to improve diagnostic accuracy and speed.</div>
<div class="mono" style="margin-top:8px">本研究旨在实现指甲疾病的早期检测，这类疾病常反映潜在健康问题，但因视觉差异细微而诊断困难。方法上，利用包含六类共3,835张图像的公开数据集，将图像统一调整为224x224像素，训练并比较了四种CNN模型（InceptionV3、DenseNet201、EfficientNetV2、ResNet50），采用对抗训练增强模型对噪声图像的鲁棒性，并利用SHAP进行特征可视化以提升可解释性。实验结果表明，InceptionV3模型取得了95.57%的最高准确率，DenseNet201次之为94.79%，验证了该系统作为辅助诊断工具在提升准确性和效率方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas</div>
<div class="meta-line">Authors: Aqsa Sultana, Rayan Afsar, Ahmed Rahu, Surendra P. Singh, Brian Shula, Brandon Combs, Derrick Forchetti, Vijayan K. Asari</div>
<div class="meta-line">First: 2026-02-04T18:07:51+00:00 · Latest: 2026-02-04T18:07:51+00:00</div>
<div class="meta-line">Comments: 13 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XtraLight-MedMamba用于肿瘤性管状腺瘤分类</div>
<div class="mono" style="margin-top:8px">在常规结肠镜筛查中对癌前息肉进行准确的风险分层对于降低结直肠癌（CRC）发病风险至关重要。然而，低级别异型增生的评估仍受限于主观的组织病理学解读。数字病理学与深度学习的进展为识别与恶性进展相关的、人眼难以察觉的细微形态学模式提供了新机遇。本研究提出XtraLight-MedMamba——一种基于状态空间的超轻量深度学习框架，用于从全切片图像（WSI）中分类肿瘤性管状腺瘤。该架构融合了基于ConvNext的浅层特征提取器与并行视觉Mamba模块，能高效建模长程/短程依赖关系并提升图像泛化能力。空间与通道注意力桥接（SCAB）模块增强了多尺度特征提取，而固定非负正交分类器（FNOClassifier）实现了参数大幅缩减与泛化性能提升。模型在经整理的低级别管状腺瘤患者数据集上评估，根据后续CRC发展情况分为病例组与对照组。XtraLight-MedMamba以约32,000参数取得97.18%准确率与0.9767的F1分数，显著优于参数复杂度更高的基于Transformer及传统Mamba架构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate risk stratification of precancerous polyps is crucial for preventing colorectal cancer, but current histopathologic assessment of low-grade dysplasia is subjective. To address this, the authors propose XtraLight-MedMamba, an ultra-lightweight deep learning framework that combines a ConvNext-based shallow feature extractor with a parallel vision Mamba to model long- and short-range dependencies in whole-slide images, enhanced by a Spatial and Channel Attention Bridge module and a Fixed Non-Negative Orthogonal Classifier for parameter efficiency. Evaluated on a curated dataset of low-grade tubular adenomas stratified by subsequent cancer development, the model achieved 97.18% accuracy and a 0.9767 F1-score using only about 32,000 parameters, outperforming more complex transformer and Mamba architectures.</div>
<div class="mono" style="margin-top:8px">为改善结直肠癌风险分层，解决当前低级别异型增生评估受限于主观组织病理学判读的问题，本研究提出了XtraLight-MedMamba，一种超轻量深度学习框架。该方法结合了基于ConvNext的浅层特征提取器与并行视觉Mamba来建模依赖关系，集成空间与通道注意力桥模块以增强多尺度特征提取，并采用固定非负正交分类器实现参数高效性。在根据后续癌症发展情况分层的低级别管状腺瘤患者全切片图像数据集上评估，该模型仅使用约32,000个参数即实现了97.18%的准确率和0.9767的F1分数，性能优于更复杂的Transformer和Mamba架构。</div>
</details>
</div>
<div class="card">
<div class="title">A Generalization Bound for a Family of Implicit Networks</div>
<div class="meta-line">Authors: Samy Wu Fung, Benjamin Berkels</div>
<div class="meta-line">First: 2024-10-09T20:44:15+00:00 · Latest: 2026-02-04T18:05:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.07427v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.07427v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Implicit networks are a class of neural networks whose outputs are defined by the fixed point of a parameterized operator. They have enjoyed success in many applications including natural language processing, image processing, and numerous other applications. While they have found abundant empirical success, theoretical work on its generalization is still under-explored. In this work, we consider a large family of implicit networks defined parameterized contractive fixed point operators. We show a generalization bound for this class based on a covering number argument for the Rademacher complexity of these architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一类隐式网络的泛化界</div>
<div class="mono" style="margin-top:8px">隐式网络是一类通过参数化算子的不动点来定义输出的神经网络，已在自然语言处理、图像处理等众多应用中取得成功。尽管其实证成果丰富，但其泛化性能的理论研究仍显不足。本文针对由参数化压缩不动点算子定义的一大类隐式网络，基于该架构Rademacher复杂度的覆盖数论证，给出了此类网络的泛化界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the empirical success of implicit neural networks, which define outputs via fixed points of parameterized operators, but the limited theoretical understanding of their generalization, this work provides a theoretical generalization bound for a broad family of such networks characterized by contractive fixed point operators. The method employs a covering number argument to bound the Rademacher complexity of these architectures. The main experimental result is the derivation of a formal generalization bound, establishing a theoretical foundation for the learning guarantees of this important class of models.</div>
<div class="mono" style="margin-top:8px">本研究针对隐式神经网络在自然语言处理和图像处理等任务中取得的经验成功及其泛化理论研究的不足，分析了由参数化压缩不动点算子定义的一大类隐式网络。方法上，通过覆盖数论证来约束这些架构的Rademacher复杂度。主要的实验结果是推导出了该类网络的泛化界，为其学习保证提供了理论基础。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Generalizable Heterogeneous Legal Link Prediction</div>
<div class="meta-line">Authors: Lorenz Wendlinger, Simon Alexander Nonn, Abdullah Al Zubaer, Michael Granitzer</div>
<div class="meta-line">First: 2026-02-04T17:59:13+00:00 · Latest: 2026-02-04T17:59:13+00:00</div>
<div class="meta-line">Comments: 9 Pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04812v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04812v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has applied link prediction to large heterogeneous legal citation networks \new{with rich meta-features}. We find that this approach can be improved by including edge dropout and feature concatenation for the learning of more robust representations, which reduces error rates by up to 45%. We also propose an approach based on multilingual node features with an improved asymmetric decoder for compatibility, which allows us to generalize and extend the prediction to more, geographically and linguistically disjoint, data from New Zealand. Our adaptations also improve inductive transferability between these disjoint legal systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒可泛化的异构法律链接预测</div>
<div class="mono" style="margin-top:8px">近期研究将链接预测应用于具有丰富元特征的大型异构法律引文网络。我们发现，通过引入边丢弃和特征拼接来学习更鲁棒的表征，可将错误率降低高达45%。此外，我们提出一种基于多语言节点特征的方法，并采用改进的非对称解码器以增强兼容性，从而能够将预测泛化并扩展至新西兰在地理和语言上更为分散的数据。我们的调整还提升了这些独立法律系统间的归纳迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance link prediction in heterogeneous legal citation networks, which often contain rich meta-features, by developing more robust and generalizable methods. The proposed approach incorporates edge dropout and feature concatenation to learn robust representations, and introduces an improved asymmetric decoder combined with multilingual node features to handle geographically and linguistically disjoint data. Experimental results show that these adaptations reduce error rates by up to 45% and improve inductive transferability between different legal systems, such as extending predictions to New Zealand data.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进具有丰富元特征的异构法律引文网络中的链接预测，以提高其在不同法律体系间的鲁棒性和泛化能力。方法结合了边丢弃和特征拼接以学习更鲁棒的表示，并提出了利用多语言节点特征与改进的非对称解码器来增强兼容性。实验结果表明，这些改进使错误率降低了高达45%，能够有效泛化到地理和语言上不相干的新西兰数据，并提升了不同法律系统间的归纳可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization</div>
<div class="meta-line">Authors: Jiarui Yuan, Tailin Jin, Weize Chen, Zeyuan Liu, Zhiyuan Liu, Maosong Sun</div>
<div class="meta-line">First: 2026-02-04T17:58:32+00:00 · Latest: 2026-02-04T17:58:32+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04811v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04811v1">PDF</a> · <a href="https://github.com/thunlp/SE-Bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new&#x27;&#x27; knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring &quot;Closed-Book Training&quot; to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SE-Bench：基于知识内化的自我进化基准测试</div>
<div class="mono" style="margin-top:8px">真正的自我进化要求智能体作为终身学习者，将新经验内化以解决未来问题。然而，严谨衡量这一基础能力面临两大障碍：先验知识纠缠（即“新”知识可能已存在于预训练数据中）与推理复杂度纠缠（即失败可能源于问题难度而非知识召回能力缺失）。我们提出SE-Bench——通过将NumPy库及其API文档混淆为随机标识符的伪新包，构建诊断环境。智能体需内化该包并在无文档访问条件下完成简单编码任务，形成“掌握新API可轻松解题、基线模型则完全无法解决”的纯净测试场景。研究发现：（1）开卷悖论：依赖参考文档的训练会抑制知识留存，需采用“闭卷训练”强制知识压缩至权重；（2）强化学习鸿沟：标准RL因PPO裁剪与负梯度无法完全内化新知识；（3）自我博弈可行性：模型可通过监督微调从自生成的噪声任务中学习，但强化学习无效。SE-Bench为知识内化的自我进化研究建立了严谨诊断平台。代码与数据集详见https://github.com/thunlp/SE-Bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to benchmark self-evolution in AI agents, specifically their ability to internalize novel knowledge for lifelong learning, addressing challenges like the entanglement of prior knowledge in pre-training data and reasoning complexity that obscures evaluation. The method introduces SE-Bench, a diagnostic environment that obfuscates the NumPy library into a pseudo-novel package with randomized identifiers; agents are trained to internalize this package and then tested on simple coding tasks without documentation, ensuring tasks are trivial with the new API but impossible for base models. Key findings include the Open-Book Paradox, where training with reference docs hinders retention, necessitating Closed-Book Training for knowledge compression; the RL Gap, where standard reinforcement learning fails due to PPO clipping and negative gradients; and the viability of Self-Play with supervised fine-tuning for learning from self-generated tasks, though not with RL.</div>
<div class="mono" style="margin-top:8px">本研究旨在为AI智能体的自我进化能力建立基准，重点关注其内化新知识以实现终身学习的能力，以解决预训练数据中先验知识纠缠和评估中推理复杂性带来的挑战。方法上引入了SE-Bench，这是一个诊断环境，通过随机标识符将NumPy库混淆为伪新包；智能体被训练内化该包，并在无文档访问的情况下测试简单编码任务，确保任务在使用新API时是简单的，而对基础模型则不可能完成。主要实验发现包括：开卷训练悖论，即使用参考文档训练会抑制知识保留，需要闭卷训练来强制知识压缩到权重中；强化学习差距，标准强化学习因PPO裁剪和负梯度而无法完全内化新知识；以及自博弈与监督微调结合可用于从自生成噪声任务中学习，但强化学习无效。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Rewards in Reinforcement Learning for Cyber Defence</div>
<div class="meta-line">Authors: Elizabeth Bates, Chris Hicks, Vasilios Mavroudis</div>
<div class="meta-line">First: 2026-02-04T17:55:23+00:00 · Latest: 2026-02-04T17:55:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have seen an explosion of interest in autonomous cyber defence agents trained to defend computer networks using deep reinforcement learning. These agents are typically trained in cyber gym environments using dense, highly engineered reward functions which combine many penalties and incentives for a range of (un)desirable states and costly actions. Dense rewards help alleviate the challenge of exploring complex environments but risk biasing agents towards suboptimal and potentially riskier solutions, a critical issue in complex cyber environments. We thoroughly evaluate the impact of reward function structure on learning and policy behavioural characteristics using a variety of sparse and dense reward functions, two well-established cyber gyms, a range of network sizes, and both policy gradient and value-based RL algorithms. Our evaluation is enabled by a novel ground truth evaluation approach which allows directly comparing between different reward functions, illuminating the nuanced inter-relationships between rewards, action space and the risks of suboptimal policies in cyber environments. Our results show that sparse rewards, provided they are goal aligned and can be encountered frequently, uniquely offer both enhanced training reliability and more effective cyber defence agents with lower-risk policies. Surprisingly, sparse rewards can also yield policies that are better aligned with cyber defender goals and make sparing use of costly defensive actions without explicit reward-based numerical penalties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越奖励：强化学习在网络安全防御中的应用</div>
<div class="mono" style="margin-top:8px">近年来，利用深度强化学习训练自主网络防御代理以保护计算机网络的研究兴趣激增。这些代理通常在网络训练环境中通过密集且高度工程化的奖励函数进行训练，这些函数结合了对多种（非）理想状态和代价高昂行动的惩罚与激励。密集奖励有助于缓解复杂环境探索的挑战，但可能导致代理偏向次优且风险更高的解决方案，这在复杂的网络环境中尤为关键。我们通过多种稀疏与密集奖励函数、两个成熟的网络训练环境、不同规模的网络以及策略梯度和基于价值的强化学习算法，全面评估了奖励函数结构对学习过程及策略行为特征的影响。评估采用了一种新颖的基准评估方法，可直接比较不同奖励函数的效果，揭示奖励、行动空间与网络环境中次优策略风险之间的微妙关联。结果表明，只要稀疏奖励与目标一致且能频繁触发，不仅能提高训练可靠性，还能产生具有更低风险策略的更有效网络防御代理。令人惊讶的是，稀疏奖励还能生成更符合网络防御目标、且无需显式数值惩罚即可节制使用高成本防御行动的优化策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the risk that dense, engineered reward functions may bias reinforcement learning agents toward suboptimal and risky policies in autonomous cyber defense, this study systematically evaluates the impact of reward function structure. The method employs a novel ground truth evaluation approach to compare sparse and dense reward functions across two established cyber gym environments, various network sizes, and both policy gradient and value-based RL algorithms. Key experimental findings reveal that sparse rewards, when goal-aligned and frequently encountered, uniquely enhance training reliability and produce more effective defense agents with lower-risk policies, even leading to policies that better align with defender goals and use costly actions sparingly without explicit penalties.</div>
<div class="mono" style="margin-top:8px">针对复杂网络防御环境中密集奖励函数可能导致强化学习智能体偏向次优且高风险策略的问题，本研究系统评估了奖励结构的影响。方法采用一种新颖的基准评估方法，在两个成熟的网络训练环境、不同网络规模以及策略梯度和基于价值的强化学习算法中，对稀疏和密集奖励函数进行比较。关键实验结果表明，稀疏且目标对齐的奖励，只要能够被频繁遇到，就能独特地提高训练可靠性，并产生更有效的防御智能体，其策略风险更低、更符合防御者目标，且能节省使用代价高昂的防御动作，而无需依赖明确的数值惩罚。</div>
</details>
</div>
<div class="card">
<div class="title">Evolving Afferent Architectures: Biologically-inspired Models for Damage-Avoidance Learning</div>
<div class="meta-line">Authors: Wolfgang Maass, Sabine Janzen, Prajvi Saxena, Sach Mukherjee</div>
<div class="meta-line">First: 2026-02-04T17:53:28+00:00 · Latest: 2026-02-04T17:53:28+00:00</div>
<div class="meta-line">Comments: 16 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04807v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Afferent Learning, a framework that produces Computational Afferent Traces (CATs) as adaptive, internal risk signals for damage-avoidance learning. Inspired by biological systems, the framework uses a two-level architecture: evolutionary optimization (outer loop) discovers afferent sensing architectures that enable effective policy learning, while reinforcement learning (inner loop) trains damage-avoidance policies using these signals. This formalizes afferent sensing as providing an inductive bias for efficient learning: architectures are selected based on their ability to enable effective learning (rather than directly minimizing damage). We provide theoretical convergence guarantees under smoothness and bounded-noise assumptions. We illustrate the general approach in the challenging context of biomechanical digital twins operating over long time horizons (multiple decades of the life-course). Here, we find that CAT-based evolved architectures achieve significantly higher efficiency and better age-robustness than hand-designed baselines, enabling policies that exhibit age-dependent behavioral adaptation (23% reduction in high-risk actions). Ablation studies validate CAT signals, evolution, and predictive discrepancy as essential. We release code and data for reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>演化传入架构：面向损伤规避学习的仿生模型</div>
<div class="mono" style="margin-top:8px">本文提出传入学习框架，通过生成计算传入痕迹作为损伤规避学习的自适应内部风险信号。该仿生框架采用双层架构：进化优化（外层循环）发现能支持有效策略学习的传入感知架构，而强化学习（内层循环）利用这些信号训练损伤规避策略。该研究将传入感知形式化为高效学习的归纳偏置——架构选择标准是其促进有效学习的能力（而非直接最小化损伤）。我们在平滑性与有界噪声假设下提供了理论收敛保证。通过在生物力学数字孪生长时程（数十年生命周期）场景中的验证表明：基于计算传入痕迹的演化架构相比人工设计基线显著提升效率与年龄鲁棒性，实现具有年龄依赖行为适应的策略（高风险行为减少23%）。消融实验验证了计算传入痕迹信号、进化机制与预测差异的核心作用。我们已开源代码与数据以确保可复现性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research introduces Afferent Learning, a biologically-inspired framework for damage-avoidance learning that generates adaptive internal risk signals called Computational Afferent Traces (CATs). The method employs a two-level architecture: an outer evolutionary loop optimizes afferent sensing architectures, which then provide an inductive bias for an inner reinforcement learning loop to train policies, formalizing the principle that architectures should be selected for their ability to enable effective learning rather than directly minimizing damage. In experiments with biomechanical digital twins operating over long time horizons, CAT-based evolved architectures significantly outperformed hand-designed baselines in efficiency and age-robustness, enabling policies that reduced high-risk actions by 23% and exhibited age-dependent behavioral adaptation, with ablation studies confirming the essential roles of CAT signals, evolution, and predictive discrepancy.</div>
<div class="mono" style="margin-top:8px">本研究提出了传入学习，这是一个受生物系统启发的损伤避免学习框架，它生成称为计算传入痕迹的自适应内部风险信号。该方法采用双层架构：外部进化循环优化传入感知架构，这些架构随后为内部强化学习循环提供归纳偏置以训练策略，形式化了架构应被选择用于促成有效学习而非直接最小化损伤的原则。在运行数十年的生物力学数字孪生实验中，基于计算传入痕迹的进化架构在效率和年龄鲁棒性上显著优于人工设计的基线，使策略将高风险行为减少了23%并展现出年龄依赖的行为适应，消融研究证实了传入信号、进化和预测差异的关键作用。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
