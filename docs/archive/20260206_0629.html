<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 06:29</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0629</div>
    <div class="row"><div class="card">
<div class="title">MIGHTY: Hermite Spline-based Efficient Trajectory Planning</div>
<div class="meta-line">Authors: Kota Kondo, Yuwei Wu, Vijay Kumar, Jonathan P. How</div>
<div class="meta-line">First: 2025-11-13T21:45:54+00:00 · Latest: 2026-02-04T18:59:13+00:00</div>
<div class="meta-line">Comments: 10 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10822v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.10822v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hard-constraint trajectory planners often rely on commercial solvers and demand substantial computational resources. Existing soft-constraint methods achieve faster computation, but either (1) decouple spatial and temporal optimization or (2) restrict the search space. To overcome these limitations, we introduce MIGHTY, a Hermite spline-based planner that performs spatiotemporal optimization while fully leveraging the continuous search space of a spline. In simulation, MIGHTY achieves a 9.3% reduction in computation time and a 13.1% reduction in travel time over state-of-the-art baselines, with a 100% success rate. In hardware, MIGHTY completes multiple high-speed flights up to 6.7 m/s in a cluttered static environment and long-duration flights with dynamically added obstacles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIGHTY：基于埃尔米特样条的高效轨迹规划方法</div>
<div class="mono" style="margin-top:8px">硬约束轨迹规划器通常依赖商业求解器且计算资源消耗大。现有软约束方法虽能加速计算，但存在（1）时空优化解耦或（2）搜索空间受限的局限。为此，我们提出MIGHTY——一种基于埃尔米特样条的规划器，在充分利用样条连续搜索空间的同时实现时空联合优化。仿真结果表明，MIGHTY相比前沿基线方法计算时间减少9.3%，行程时间降低13.1%，且成功率保持100%。硬件实验中，MIGHTY在静态杂乱环境中实现了最高6.7米/秒的多轮高速飞行，并能在动态新增障碍物场景下完成长时飞行任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational demands of hard-constraint planners and the limitations of existing soft-constraint methods—which either decouple spatial and temporal optimization or restrict the search space—this paper introduces MIGHTY, a trajectory planner based on Hermite splines. The method performs joint spatiotemporal optimization by fully leveraging the continuous search space of the spline representation. Experimental results show that in simulation, MIGHTY reduces computation time by 9.3% and travel time by 13.1% compared to state-of-the-art baselines while maintaining a 100% success rate; in hardware tests, it successfully executes high-speed flights up to 6.7 m/s in cluttered static environments and handles long-duration flights with dynamically introduced obstacles.</div>
<div class="mono" style="margin-top:8px">为解决硬约束轨迹规划器计算资源需求大，以及现有软约束方法存在的时空优化解耦或搜索空间受限等问题，本研究提出了MIGHTY，一种基于Hermite样条的轨迹规划器。该方法通过充分利用样条的连续搜索空间，实现了时空联合优化。实验结果表明，在仿真中，MIGHTY相比最先进的基线方法，计算时间减少了9.3%，行程时间减少了13.1%，且成功率达到100%；在硬件测试中，它成功在杂乱静态环境中完成了高达6.7米/秒的高速飞行，并能处理动态添加障碍物的长时间飞行任务。</div>
</details>
</div>
<div class="card">
<div class="title">Capturing Visual Environment Structure Correlates with Control Performance</div>
<div class="meta-line">Authors: Jiahua Dong, Yunze Man, Pavel Tokmakov, Yu-Xiong Wang</div>
<div class="meta-line">First: 2026-02-04T18:59:12+00:00 · Latest: 2026-02-04T18:59:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04880v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04880v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The choice of visual representation is key to scaling generalist robot policies. However, direct evaluation via policy rollouts is expensive, even in simulation. Existing proxy metrics focus on the representation&#x27;s capacity to capture narrow aspects of the visual world, like object shape, limiting generalization across environments. In this paper, we take an analytical perspective: we probe pretrained visual encoders by measuring how well they support decoding of environment state -- including geometry, object structure, and physical attributes -- from images. Leveraging simulation environments with access to ground-truth state, we show that this probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, significantly outperforming prior metrics and enabling efficient representation selection. More broadly, our study provides insight into the representational properties that support generalizable manipulation, suggesting that learning to encode the latent physical state of the environment is a promising objective for control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>捕捉视觉环境结构与控制性能的相关性研究</div>
<div class="mono" style="margin-top:8px">视觉表征的选择是扩展通用机器人策略的关键。然而，即使通过仿真进行策略推演的直接评估成本高昂。现有代理指标主要关注表征捕捉视觉世界局部特征（如物体形状）的能力，限制了跨环境泛化能力。本文采用分析视角：通过测量预训练视觉编码器从图像中解码环境状态（包括几何结构、物体构型和物理属性）的准确度来探究其性能。借助可获取真实状态数据的仿真环境，我们证明这种探测准确度与跨多样化环境和学习场景的下游策略性能呈强相关性，显著优于现有指标，并能实现高效表征选择。更广泛而言，本研究揭示了支持可泛化操作的表示特性，表明学习编码环境的潜在物理状态是实现控制目标的有效途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high cost of evaluating visual representations for robot policies through policy rollouts, this study proposes an analytical probing method that assesses how well pretrained visual encoders can decode environment state—including geometry, object structure, and physical attributes—from images. Using simulation environments with ground-truth state, the method demonstrates that probing accuracy strongly correlates with downstream policy performance across diverse environments and learning settings, outperforming prior metrics focused on narrow aspects like object shape. This correlation enables efficient representation selection and suggests that encoding the latent physical state of the environment is a key objective for generalizable manipulation.</div>
<div class="mono" style="margin-top:8px">为解决通过策略部署评估视觉表示的高成本以及现有代理指标泛化性有限的问题，本研究提出一种分析性探测方法，评估预训练视觉编码器从图像中解码环境状态（包括几何、物体结构和物理属性）的能力。利用具有真实状态信息的仿真环境，该方法表明探测精度与下游策略性能在不同环境和学习设置中强相关，显著优于先前指标，并能实现高效的表示选择。研究结果指出，编码环境的潜在物理状态是支持可泛化操作的关键表示特性。</div>
</details>
</div>
<div class="card">
<div class="title">PDF-HR: Pose Distance Fields for Humanoid Robots</div>
<div class="meta-line">Authors: Yi Gu, Yukang Gao, Yangchen Zhou, Xingyu Chen, Yixiao Feng, Mingle Zhao, Yunyang Mo, Zhaorui Wang, Lixin Xu, Renjing Xu</div>
<div class="meta-line">First: 2026-02-04T18:38:51+00:00 · Latest: 2026-02-04T18:38:51+00:00</div>
<div class="meta-line">Comments: \href{https://gaoyukang33.github.io/PDF-HR/}{Project page}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04851v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://gaoyukang33.github.io/PDF-HR/}{Project">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PDF-HR：人形机器人的姿态距离场</div>
<div class="mono" style="margin-top:8px">姿态与运动先验在人形机器人学中至关重要。尽管此类先验已在人体运动恢复（HMR）领域通过多种模型得到广泛研究，但其在人形机器人中的应用仍较为有限，主要原因是高质量人形机器人运动数据的稀缺。本研究提出人形机器人姿态距离场（PDF-HR），这是一种轻量级先验模型，将机器人姿态分布表示为连续可微的流形。给定任意姿态，PDF-HR可预测其与大规模重定向机器人姿态库的距离，从而生成适用于优化与控制任务的平滑姿态合理性度量。PDF-HR可作为奖励塑形项、正则化器或独立合理性评分器集成至多种流程中。我们在多项人形机器人任务中评估PDF-HR，包括单轨迹运动追踪、通用运动追踪、基于风格的运动模仿及通用运动重定向。实验表明，这一即插即用先验模型能持续显著增强现有基线方法的性能。代码与模型将公开释放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limited adoption of pose and motion priors in humanoid robotics due to a scarcity of high-quality motion data, despite their importance. The method introduces Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that models the robot pose distribution as a continuous, differentiable manifold, predicting the distance of any given pose to a large corpus of retargeted robot poses to provide a smooth measure of plausibility. Experimental results across tasks such as motion tracking, style-based mimicry, and motion retargeting demonstrate that PDF-HR consistently and substantially enhances strong baselines as a plug-and-play component.</div>
<div class="mono" style="margin-top:8px">本研究针对人形机器人领域因高质量运动数据稀缺而难以广泛应用姿态与运动先验的问题，尽管这些先验在人体运动恢复中已被广泛验证。方法提出了人形机器人姿态距离场（PDF-HR），这是一种轻量级先验，将机器人姿态分布建模为连续可微的流形；它通过预测任意给定姿态与大量重定向机器人姿态库之间的距离，提供适用于优化与控制的平滑合理性度量。在单轨迹运动跟踪、通用运动跟踪、基于风格的运动模仿及通用运动重定向等任务上的实验结果表明，PDF-HR作为即插即用的先验，能够持续且显著地增强现有强基线模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond the Control Equations: An Artifact Study of Implementation Quality in Robot Control Software</div>
<div class="meta-line">Authors: Nils Chur, Thorsten Berger, Einar Broch Johnsen, Andrzej Wąsowski</div>
<div class="meta-line">First: 2026-02-04T17:45:59+00:00 · Latest: 2026-02-04T17:45:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04799v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04799v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A controller -- a software module managing hardware behavior -- is a key component of a typical robot system. While control theory gives safety guarantees for standard controller designs, the practical implementation of controllers in software introduces complexities that are often overlooked. Controllers are often designed in continuous space, while the software is executed in discrete space, undermining some of the theoretical guarantees. Despite extensive research on control theory and control modeling, little attention has been paid to the implementations of controllers and how their theoretical guarantees are ensured in real-world software systems. We investigate 184 real-world controller implementations in open-source robot software. We examine their application context, the implementation characteristics, and the testing methods employed to ensure correctness. We find that the implementations often handle discretization in an ad hoc manner, leading to potential issues with real-time reliability. Challenges such as timing inconsistencies, lack of proper error handling, and inadequate consideration of real-time constraints further complicate matters. Testing practices are superficial, no systematic verification of theoretical guarantees is used, leaving possible inconsistencies between expected and actual behavior. Our findings highlight the need for improved implementation guidelines and rigorous verification techniques to ensure the reliability and safety of robotic controllers in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越控制方程：机器人控制软件实现质量的实证研究</div>
<div class="mono" style="margin-top:8px">控制器——管理硬件行为的软件模块——是典型机器人系统的核心组件。尽管控制理论为标准控制器设计提供了安全保障，但控制器在软件中的实际实现引入了常被忽视的复杂性。控制器通常在连续空间设计，而软件在离散空间执行，这削弱了部分理论保证。尽管控制理论和控制建模研究广泛，但控制器实现及其理论保证如何在现实软件系统中得到确保的问题却鲜有关注。我们调查了开源机器人软件中的184个真实控制器实现，考察了其应用场景、实现特征以及用于确保正确性的测试方法。研究发现，实现常以临时方式处理离散化，可能导致实时可靠性问题。时序不一致、缺乏适当错误处理、实时约束考虑不足等挑战进一步加剧了复杂性。测试实践流于表面，未采用系统化的理论保证验证，导致预期行为与实际行为间可能存在不一致。本研究结果凸显了需要改进实现指南并采用严格验证技术，以确保机器人控制器在实际应用中的可靠性与安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the gap between theoretical safety guarantees in robot control designs and their practical software implementations, which often overlook complexities like discretization and real-time constraints. By analyzing 184 open-source robot controller implementations, the research examines their application contexts, implementation characteristics, and testing methods. Key findings reveal that implementations frequently handle discretization ad hoc, suffer from timing inconsistencies and inadequate error handling, and employ superficial testing without systematic verification of theoretical guarantees, highlighting risks to real-world reliability and safety.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人控制设计中的理论安全保证与实际软件实现之间的差距，后者常忽视离散化和实时约束等复杂性。通过分析184个开源机器人控制器实现，作者考察了应用背景、实现特性和测试方法。主要发现表明，实现通常以临时方式处理离散化，存在时序不一致和错误处理不足的问题，且测试实践肤浅，缺乏对理论保证的系统性验证，凸显了实际可靠性和安全性的风险。</div>
</details>
</div>
<div class="card">
<div class="title">PuppetAI: A Customizable Platform for Designing Tactile-Rich Affective Robot Interaction</div>
<div class="meta-line">Authors: Jiaye Li, Tongshun Chen, Siyi Ma, Elizabeth Churchill, Ke Wu</div>
<div class="meta-line">First: 2026-02-04T17:37:31+00:00 · Latest: 2026-02-04T17:37:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04787v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04787v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce PuppetAI, a modular soft robot interaction platform. This platform offers a scalable cable-driven actuation system and a customizable, puppet-inspired robot gesture framework, supporting a multitude of interaction gesture robot design formats. The platform comprises a four-layer decoupled software architecture that includes perceptual processing, affective modeling, motion scheduling, and low-level actuation. We also implemented an affective expression loop that connects human input to the robot platform by producing real-time emotional gestural responses to human vocal input. For our own designs, we have worked with nuanced gestures enacted by &quot;soft robots&quot; with enhanced dexterity and &quot;pleasant-to-touch&quot; plush exteriors. By reducing operational complexity and production costs while enhancing customizability, our work creates an adaptable and accessible foundation for future tactile-based expressive robot research. Our goal is to provide a platform that allows researchers to independently construct or refine highly specific gestures and movements performed by social robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PuppetAI：面向触觉丰富情感机器人交互设计的可定制平台</div>
<div class="mono" style="margin-top:8px">本文介绍PuppetAI——一个模块化软体机器人交互平台。该平台提供可扩展的线驱传动系统及可定制的木偶式机器人手势框架，支持多种交互手势机器人设计模式。平台采用四层解耦软件架构，包含感知处理、情感建模、运动调度与底层驱动。我们还实现了情感表达闭环，通过实时生成对人类语音输入的情感手势响应，将人类输入与机器人平台连接。在自主设计中，我们采用具备增强灵巧性与“宜触感”绒面外表的软体机器人执行精细手势。通过降低操作复杂度与生产成本，同时提升可定制性，本研究为未来基于触觉的表达型机器人研究奠定了适应性强、易于推广的基础。我们的目标是构建一个允许研究者独立构建或优化社交机器人执行高度特定手势与动作的平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To advance tactile-rich affective human-robot interaction, this research introduces PuppetAI, a customizable platform featuring a modular soft robot design with a scalable cable-driven actuation system and a puppet-inspired gesture framework. The method employs a four-layer decoupled software architecture for perceptual processing, affective modeling, motion scheduling, and low-level actuation, alongside an affective expression loop that generates real-time emotional gestural responses to human vocal input. Experimental results demonstrate that the platform, utilizing soft robots with plush exteriors, effectively reduces operational complexity and production costs while enhancing customizability, thereby providing an adaptable foundation for designing nuanced, pleasant-to-touch social robot gestures.</div>
<div class="mono" style="margin-top:8px">为推进基于触觉的丰富情感人机交互，本研究提出了PuppetAI，这是一个可定制平台，采用模块化软体机器人设计，包含可扩展的线驱传动系统和受木偶启发的姿态框架。其方法采用四层解耦软件架构，负责感知处理、情感建模、运动调度和底层驱动，并配备一个情感表达循环，可根据人类语音输入生成实时情感姿态响应。实验结果表明，该平台利用具有柔软外表的软体机器人实现细腻姿态，有效降低了操作复杂性和生产成本，同时增强了可定制性，从而为未来富有表现力的社交机器人研究提供了一个适应性强的技术基础。</div>
</details>
</div>
<div class="card">
<div class="title">Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics</div>
<div class="meta-line">Authors: Nozomi Nakajima, Pedro Reynolds-Cuéllar, Caitrin Lynch, Kate Darling</div>
<div class="meta-line">First: 2026-02-04T16:48:06+00:00 · Latest: 2026-02-04T16:48:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04746v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04746v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics, the concept of &quot;dull, dirty, and dangerous&quot; (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on &quot;dull,&quot; &quot;dirty,&quot; and &quot;dangerous&quot; work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>枯燥、肮脏、危险：理解机器人学关键驱动力的过去、现在与未来</div>
<div class="mono" style="margin-top:8px">在机器人学领域，“枯燥、肮脏、危险”（DDD）工作的概念常被用作机器人适用场景的驱动力。本文通过对1980年至2024年间提及DDD的机器人学文献进行实证分析，发现仅2.7%的文献明确定义了DDD，仅8.7%的文献提供了DDD任务或工作的具体案例。随后，我们回顾了关于“枯燥”“肮脏”“危险”工作的社会科学文献，为机器人学领域界定DDD概念提供了定义与框架指引。最后，我们提出一个帮助机器人学界系统性考量技术应用场景的框架，以促进对机器人技术如何影响人类劳动形成更审慎的认知视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the prevalent but often vague use of the &quot;dull, dirty, and dangerous&quot; (DDD) concept as a motivation for robotics applications. Through an empirical analysis of robotics publications from 1980 to 2024, the authors found that only 2.7% of papers explicitly define DDD and 8.7% provide concrete examples of such tasks. To address this lack of clarity, the research synthesizes definitions from social science literature and proposes a framework to help the robotics community more thoughtfully consider the job contexts where robots are deployed, aiming for a more informed understanding of their impact on human labor.</div>
<div class="mono" style="margin-top:8px">本研究调查了机器人学中普遍但常被模糊使用的&#x27;枯燥、肮脏、危险&#x27;（DDD）动机，通过对1980年至2024年文献的实证分析发现，仅2.7%的文献定义了DDD，8.7%提供了具体任务案例。为弥补这一不足，作者回顾了社会科学文献，为DDD建立了更清晰的定义和概念指导。他们提出了一个考虑工作背景的框架，旨在促进对机器人技术影响人类劳动形成更明智的视角。</div>
</details>
</div>
<div class="card">
<div class="title">Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization</div>
<div class="meta-line">Authors: Aaron Wilhelm, Nils Napp</div>
<div class="meta-line">Venue: ICRA 2025</div>
<div class="meta-line">First: 2025-05-16T18:37:18+00:00 · Latest: 2026-02-04T16:43:10+00:00</div>
<div class="meta-line">Comments: Accepted to ICRA 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.11620v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.11620v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ground texture localization using a downward-facing camera offers a low-cost, high-precision localization solution that is robust to dynamic environments and requires no environmental modification. We present a significantly improved bag-of-words (BoW) image retrieval system for ground texture localization, achieving substantially higher accuracy for global localization and higher precision and recall for loop closure detection in SLAM. Our approach leverages an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits the consistent orientation and constant scale constraints inherent to ground texture localization. Identifying the different needs of global localization vs. loop closure detection for SLAM, we present both high-accuracy and high-speed versions of our algorithm. We test the effect of each of our proposed improvements through an ablation study and demonstrate our method&#x27;s effectiveness for both global localization and loop closure detection. With numerous ground texture localization systems already using BoW, our method can readily replace other generic BoW systems in their pipeline and immediately improve their results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何约束的改进词袋模型地面纹理定位图像检索</div>
<div class="mono" style="margin-top:8px">利用下视相机进行地面纹理定位提供了一种低成本、高精度的定位方案，该方案对环境动态变化具有鲁棒性且无需改造环境。本文提出一种显著改进的词袋模型图像检索系统，用于地面纹理定位，在全局定位中实现了更高的准确率，同时在SLAM闭环检测中获得了更高的精确率与召回率。该方法采用近似K均值词汇表配合软分配策略，并利用地面纹理定位固有的方向一致性与尺度恒定约束。针对SLAM中全局定位与闭环检测的不同需求，我们分别提出了高精度与高速版本算法。通过消融实验验证了各项改进措施的效果，并证明了本方法在全局定位与闭环检测中的有效性。鉴于现有众多地面纹理定位系统已采用词袋模型，本方法可直接替代其流程中的通用词袋系统并即时提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance ground texture localization, a low-cost and robust method using a downward-facing camera, by improving the widely adopted bag-of-words (BoW) image retrieval system to achieve higher accuracy for global localization and better precision and recall for loop closure detection in SLAM. The method introduces an approximate k-means vocabulary with soft assignment and explicitly incorporates the geometric constraints of consistent orientation and constant scale inherent to ground textures, while also developing separate high-accuracy and high-speed algorithm versions tailored for global localization and loop closure detection, respectively. Experimental ablation studies confirm the effectiveness of each proposed improvement, demonstrating that the method significantly outperforms generic BoW systems and can be readily integrated into existing pipelines.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进使用下视相机的地面纹理定位方法，这是一种低成本、高鲁棒性且无需环境改造的解决方案，通过提升广泛采用的词袋模型图像检索系统，以实现更高的全局定位精度以及SLAM中闭环检测的精确率与召回率。该方法采用了带软分配的近似k均值词汇表，并专门利用了地面纹理定位中固有的方向一致性和尺度恒定几何约束。通过消融实验验证了各项改进的有效性，结果表明性能显著提升；系统还提供了高精度与高速度两种版本，以分别满足全局定位与闭环检测的不同需求。</div>
</details>
</div>
<div class="card">
<div class="title">AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation</div>
<div class="meta-line">Authors: Jin-Chuan Shi, Binhong Ye, Tao Liu, Junzhe He, Yangjinhui Xu, Xiaoyang Liu, Zeju Li, Hao Chen, Chunhua Shen</div>
<div class="meta-line">First: 2026-02-04T15:42:58+00:00 · Latest: 2026-02-04T15:42:58+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04672v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AGILE：基于智能体生成从视频重建手-物交互</div>
<div class="mono" style="margin-top:8px">从单目视频重建动态手-物交互对于灵巧操作数据收集以及为机器人与VR创建真实数字孪生至关重要。然而，现有方法面临两大瓶颈：(1) 依赖神经渲染常导致严重遮挡下产生破碎、无法直接用于仿真的几何体；(2) 依赖脆弱的运动恢复结构初始化导致野外视频频繁失败。为突破这些限制，我们提出AGILE框架，将范式从重建转向面向交互学习的智能体生成。首先，采用智能体流程：视觉语言模型引导生成模型合成完整、密闭的高保真纹理物体网格，不受视频遮挡影响。其次，完全绕过脆弱的SfM，提出鲁棒的锚点跟踪策略：通过基础模型在单帧交互起始帧初始化物体位姿，并利用生成资产与视频观测间的强视觉相似性进行时序传播。最后，通过接触感知优化整合语义、几何与交互稳定性约束，确保物理合理性。在HO3D、DexYCB及野外视频上的大量实验表明，AGILE在全局几何精度上超越基线方法，并在现有方法常失效的挑战性序列中展现卓越鲁棒性。通过优先保障物理有效性，本方法产出可直接用于仿真的资产，并已通过机器人应用的实-仿重定向验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges of reconstructing dynamic hand-object interactions from monocular videos, which are critical for robotics and VR, but current methods suffer from fragmented geometries due to neural rendering and failures from brittle SfM initialization. The proposed AGILE framework shifts from reconstruction to agentic generation, using a Vision-Language Model to guide the synthesis of a complete object mesh and a robust anchor-and-track strategy for pose estimation, followed by contact-aware optimization for physical plausibility. Experimental results on HO3D, DexYCB, and in-the-wild videos show that AGILE outperforms baselines in geometric accuracy and robustness, producing simulation-ready assets validated for robotic retargeting.</div>
<div class="mono" style="margin-top:8px">本研究针对从单目视频重建动态手物交互的现有方法存在的局限性，这些方法常因依赖神经渲染和脆弱的运动恢复结构初始化而产生破碎几何体，并在野外视频上频繁失败。提出的AGILE框架将范式转向智能体生成，利用视觉语言模型引导合成完整物体网格，采用稳健的锚定跟踪策略进行姿态估计，并通过接触感知优化确保物理合理性。在HO3D、DexYCB和野外视频上的大量实验表明，AGILE在几何精度上优于基线方法，并在挑战性序列上展现出卓越的鲁棒性，生成了经过机器人应用验证的仿真就绪资产。</div>
</details>
</div>
<div class="card">
<div class="title">Model Reconciliation through Explainability and Collaborative Recovery in Assistive Robotics</div>
<div class="meta-line">Authors: Britt Besch, Tai Mai, Jeremias Thun, Markus Huff, Jörn Vogel, Freek Stulp, Samuel Bustamante</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-01-10T12:38:08+00:00 · Latest: 2026-02-04T15:42:19+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06552v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.06552v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whenever humans and robots work together, it is essential that unexpected robot behavior can be explained to the user. Especially in applications such as shared control the user and the robot must share the same model of the objects in the world, and the actions that can be performed on these objects.
  In this paper, we achieve this with a so-called model reconciliation framework. We leverage a Large Language Model to predict and explain the difference between the robot&#x27;s and the human&#x27;s mental models, without the need of a formal mental model of the user. Furthermore, our framework aims to solve the model divergence after the explanation by allowing the human to correct the robot. We provide an implementation in an assistive robotics domain, where we conduct a set of experiments with a real wheelchair-based mobile manipulator and its digital twin.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>辅助机器人中通过可解释性与协同恢复实现模型调和</div>
<div class="mono" style="margin-top:8px">当人类与机器人协作时，必须能够向用户解释机器人出现的意外行为。尤其在共享控制等应用中，用户与机器人需对世界中的物体及可执行操作持有相同的认知模型。本文通过一种称为模型调和的框架实现这一目标：我们利用大型语言模型预测并解释机器人与人类心智模型间的差异，无需预先构建用户的形式化心智模型。此外，该框架在解释差异后允许人类修正机器人行为，从而解决模型分歧问题。我们在辅助机器人领域实现了该框架，并通过真实轮椅移动机械臂及其数字孪生系统开展了一系列实验验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to explain unexpected robot behavior in human-robot collaboration, particularly in shared control scenarios where a shared understanding of the world model is crucial. The method introduces a model reconciliation framework that employs a Large Language Model to predict and explain differences between the human&#x27;s and robot&#x27;s mental models without requiring a formal user model, and it enables collaborative recovery by allowing the human to correct the robot. Experimental results from an assistive robotics domain, tested with both a real wheelchair-based mobile manipulator and its digital twin, demonstrate the framework&#x27;s effectiveness in achieving model alignment through explanation and correction.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决人机协作中意外机器人行为需要解释的问题，特别是在共享控制场景中，保持对世界模型的共同理解至关重要。所提出的方法引入了一个模型协调框架，利用大语言模型来预测和解释人类与机器人心理模型之间的差异，无需构建正式的用户模型，并通过允许人类纠正机器人来实现协同恢复。在辅助机器人领域的实验中，使用真实的轮椅移动机械臂及其数字孪生系统进行验证，结果表明该框架能够通过解释和纠正有效识别并解决模型分歧。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed-Density Diffuser: Efficient Planning with Non-Uniform Temporal Resolution</div>
<div class="meta-line">Authors: Crimson Stambaugh, Rajesh P. N. Rao</div>
<div class="meta-line">First: 2025-10-27T05:45:59+00:00 · Latest: 2026-02-04T15:33:28+00:00</div>
<div class="meta-line">Comments: European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning (ESANN) (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23026v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.23026v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies demonstrate that diffusion planners benefit from sparse-step planning over single-step planning. Training models to skip steps in their trajectories helps capture long-term dependencies without additional memory or computational cost. However, predicting excessively sparse plans degrades performance. We hypothesize this temporal density threshold is non-uniform across a planning horizon and that certain parts of a predicted trajectory should be more densely generated. We propose Mixed-Density Diffuser (MDD), a diffusion planner where the densities throughout the horizon are tunable hyperparameters. We show that MDD surpasses the SOTA Diffusion Veteran (DV) framework across the Maze2D, Franka Kitchen, and Antmaze Datasets for Deep Data-Driven Reinforcement Learning (D4RL) task domains, achieving a new SOTA on the D4RL benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合密度扩散器：非均匀时间分辨率下的高效规划</div>
<div class="mono" style="margin-top:8px">近期研究表明，扩散规划器通过稀疏步长规划优于单步规划。训练模型跳过轨迹中的步骤有助于捕捉长期依赖关系，且无需额外内存或计算成本。然而，预测过度稀疏的规划会降低性能。我们假设时间密度阈值在规划时域内是非均匀的，且预测轨迹的某些部分应更密集生成。为此，我们提出混合密度扩散器（MDD），一种扩散规划器，其整个时域的密度可作为可调超参数。实验表明，在Maze2D、Franka Kitchen和Antmaze数据集上，MDD在深度数据驱动强化学习（D4RL）任务领域超越了当前最优的扩散规划框架（DV），并在D4RL基准测试中创造了新的最优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the performance degradation caused by excessively sparse step prediction in diffusion planners, this research hypothesizes that the optimal temporal density for planning is non-uniform across the trajectory horizon. The proposed Mixed-Density Diffuser (MDD) introduces tunable hyperparameters to control the generation density at different parts of the predicted trajectory, allowing for denser planning in critical segments. Experimental results demonstrate that MDD outperforms the state-of-the-art Diffusion Veteran framework on the Maze2D, Franka Kitchen, and Antmaze datasets, achieving a new state-of-the-art performance on the D4RL benchmark.</div>
<div class="mono" style="margin-top:8px">该研究的动机是观察到，虽然稀疏步扩散规划能通过捕捉长期依赖关系来提高效率且无需额外成本，但过度稀疏的计划会降低性能，且最佳的时间密度可能在规划时域内非均匀分布。方法提出了混合密度扩散器（MDD），这是一种扩散规划器，允许将非均匀时间分辨率作为可调超参数，以在不同部分生成不同密度的轨迹。实验结果表明，在Maze2D、Franka Kitchen和Antmaze数据集上，MDD优于最先进的Diffusion Veteran框架，并在D4RL基准测试中取得了新的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">From Vision to Assistance: Gaze and Vision-Enabled Adaptive Control for a Back-Support Exoskeleton</div>
<div class="meta-line">Authors: Alessandro Leanza, Paolo Franceschi, Blerina Spahiu, Loris Roveda</div>
<div class="meta-line">First: 2026-02-04T15:23:42+00:00 · Latest: 2026-02-04T15:23:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04648v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04648v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Back-support exoskeletons have been proposed to mitigate spinal loading in industrial handling, yet their effectiveness critically depends on timely and context-aware assistance. Most existing approaches rely either on load-estimation techniques (e.g., EMG, IMU) or on vision systems that do not directly inform control. In this work, we present a vision-gated control framework for an active lumbar occupational exoskeleton that leverages egocentric vision with wearable gaze tracking. The proposed system integrates real-time grasp detection from a first-person YOLO-based perception system, a finite-state machine (FSM) for task progression, and a variable admittance controller to adapt torque delivery to both posture and object state. A user study with 15 participants performing stooping load lifting trials under three conditions (no exoskeleton, exoskeleton without vision, exoskeleton with vision) shows that vision-gated assistance significantly reduces perceived physical demand and improves fluency, trust, and comfort. Quantitative analysis reveals earlier and stronger assistance when vision is enabled, while questionnaire results confirm user preference for the vision-gated mode. These findings highlight the potential of egocentric vision to enhance the responsiveness, ergonomics, safety, and acceptance of back-support exoskeletons.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视觉到辅助：基于注视与视觉的背部支撑外骨骼自适应控制系统</div>
<div class="mono" style="margin-top:8px">背部支撑外骨骼已被提出用于减轻工业搬运中的脊柱负荷，但其有效性关键取决于及时且情境感知的辅助。现有方法大多依赖负荷估计技术（如肌电图、惯性测量单元）或未直接参与控制的视觉系统。本研究提出一种用于主动式腰部职业外骨骼的视觉门控控制框架，该框架结合可穿戴注视追踪的自我中心视觉。所提出的系统整合了基于第一视角YOLO感知系统的实时抓握检测、用于任务推进的有限状态机，以及可变导纳控制器，使扭矩输出能适应姿势和物体状态。一项包含15名参与者的用户研究在三种条件下（无外骨骼、无视觉外骨骼、视觉外骨骼）进行弯腰负重提升试验，结果表明视觉门控辅助显著降低感知体力负荷，并提升操作流畅性、信任度与舒适性。定量分析显示启用视觉时辅助更早且更强，问卷结果证实用户更偏好视觉门控模式。这些发现凸显了自我中心视觉在提升背部支撑外骨骼响应性、人机工程学、安全性与用户接受度方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To improve the timeliness and context-awareness of assistance in back-support exoskeletons, which is crucial for reducing spinal loading during industrial tasks, this work introduces a vision-gated control framework. The method integrates egocentric vision with wearable gaze tracking, using a first-person YOLO-based system for real-time grasp detection, a finite-state machine to track task progression, and a variable admittance controller to adapt torque based on posture and object state. Experimental results from a 15-participant study performing stooping lifts show that vision-gated assistance significantly lowers perceived physical demand and enhances fluency, trust, and comfort compared to no exoskeleton or an exoskeleton without vision, with quantitative analysis confirming earlier and stronger torque delivery and user preference for the vision-enabled mode.</div>
<div class="mono" style="margin-top:8px">为提高背部支撑外骨骼辅助的及时性与情境感知能力，这对减轻脊柱负荷至关重要，本研究提出了一种视觉门控控制框架。该方法整合了基于第一人称YOLO的感知系统进行实时抓握检测，用于任务进程跟踪的有限状态机，以及根据姿势和物体状态调整扭矩的可变导纳控制器。一项15名参与者的实验结果表明，与无外骨骼和无视觉辅助的外骨骼条件相比，视觉启用系统显著降低了感知体力负荷，并提升了操作流畅性、信任度和舒适度，定量分析证实了其能更早、更强地提供辅助。</div>
</details>
</div>
<div class="card">
<div class="title">Relational Scene Graphs for Object Grounding of Natural Language Commands</div>
<div class="meta-line">Authors: Julia Kuhn, Francesco Verdoja, Tsvetomila Mihaylova, Ville Kyrki</div>
<div class="meta-line">First: 2026-02-04T15:05:29+00:00 · Latest: 2026-02-04T15:05:29+00:00</div>
<div class="meta-line">Comments: In review for RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04635v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04635v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robots are finding wider adoption in human environments, increasing the need for natural human-robot interaction. However, understanding a natural language command requires the robot to infer the intended task and how to decompose it into executable actions, and to ground those actions in the robot&#x27;s knowledge of the environment, including relevant objects, agents, and locations. This challenge can be addressed by combining the capabilities of Large language models (LLMs) to understand natural language with 3D scene graphs (3DSGs) for grounding inferred actions in a semantic representation of the environment. However, many 3DSGs lack explicit spatial relations between objects, even though humans often rely on these relations to describe an environment. This paper investigates whether incorporating open- or closed-vocabulary spatial relations into 3DSGs can improve the ability of LLMs to interpret natural language commands. To address this, we propose an LLM-based pipeline for target object grounding from open-vocabulary language commands and a vision language model (VLM)-based pipeline to add open-vocabulary spatial edges to 3DSGs from images captured while mapping. Finally, two LLMs are evaluated in a study assessing their performance on the downstream task of target object grounding. Our study demonstrates that explicit spatial relations improve the ability of LLMs to ground objects. Moreover, open-vocabulary relation generation with VLMs proves feasible from robot-captured images, but their advantage over closed-vocabulary relations is found to be limited.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于关系场景图的自然语言指令对象定位</div>
<div class="mono" style="margin-top:8px">机器人在人类环境中的应用日益广泛，这增强了对自然人机交互的需求。然而，理解自然语言指令要求机器人推断预期任务、将其分解为可执行动作，并将这些动作基于机器人对环境（包括相关对象、智能体及位置）的认知进行落地。这一挑战可通过结合大语言模型（LLMs）理解自然语言的能力与三维场景图（3DSGs）在环境语义表征中落地推断动作来解决。但许多3DSGs缺乏对象间的显式空间关系，而人类常依赖这些关系描述环境。本文研究将开放或封闭词汇的空间关系融入3DSGs是否能提升LLMs解释自然语言指令的能力。为此，我们提出基于LLM的开放词汇语言指令目标对象定位流程，以及基于视觉语言模型（VLM）的流程，用于从建图时捕获的图像中为3DSGs添加开放词汇空间边。最后，通过评估两种LLMs在目标对象定位下游任务中的表现进行研究。结果表明，显式空间关系能提升LLMs的对象定位能力。此外，基于机器人捕获图像通过VLM生成开放词汇关系具有可行性，但其相较于封闭词汇关系的优势有限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance human-robot interaction by improving a robot&#x27;s ability to interpret natural language commands, this research investigates augmenting 3D scene graphs (3DSGs) with explicit spatial relations. The method employs a pipeline that uses a Vision Language Model (VLM) to generate open-vocabulary spatial edges for 3DSGs from robot-captured images and leverages Large Language Models (LLMs) for grounding target objects from open-vocabulary commands. Experimental evaluation on the target object grounding task demonstrates that incorporating explicit spatial relations into 3DSGs improves LLM performance, though the advantage of open-vocabulary relations over closed-vocabulary ones is found to be limited.</div>
<div class="mono" style="margin-top:8px">为提升机器人在人类环境中理解自然语言指令的能力，本研究探讨了在三维场景图（3DSG）中显式加入空间关系是否能够改善对象指代任务。该方法提出了一种流程：利用视觉语言模型（VLM）从机器人采集的图像中生成开放词汇的空间关系以增强3DSG，再通过大语言模型（LLMs）处理开放词汇指令以指代目标对象。实验评估表明，在3DSG中加入显式空间关系确实提高了LLMs的对象指代表现，但通过VLM生成的开放词汇关系相较于封闭词汇关系的优势较为有限。</div>
</details>
</div>
<div class="card">
<div class="title">Radar-Inertial Odometry For Computationally Constrained Aerial Navigation</div>
<div class="meta-line">Authors: Jan Michalczyk</div>
<div class="meta-line">First: 2026-02-04T15:03:26+00:00 · Latest: 2026-02-04T15:03:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04631v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04631v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the progress in the radar sensing technology consisting in the miniaturization of the packages and increase in measuring precision has drawn the interest of the robotics research community. Indeed, a crucial task enabling autonomy in robotics is to precisely determine the pose of the robot in space. To fulfill this task sensor fusion algorithms are often used, in which data from one or several exteroceptive sensors like, for example, LiDAR, camera, laser ranging sensor or GNSS are fused together with the Inertial Measurement Unit (IMU) measurements to obtain an estimate of the navigation states of the robot. Nonetheless, owing to their particular sensing principles, some exteroceptive sensors are often incapacitated in extreme environmental conditions, like extreme illumination or presence of fine particles in the environment like smoke or fog. Radars are largely immune to aforementioned factors thanks to the characteristics of electromagnetic waves they use. In this thesis, we present Radar-Inertial Odometry (RIO) algorithms to fuse the information from IMU and radar in order to estimate the navigation states of a (Uncrewed Aerial Vehicle) UAV capable of running on a portable resource-constrained embedded computer in real-time and making use of inexpensive, consumer-grade sensors. We present novel RIO approaches relying on the multi-state tightly-coupled Extended Kalman Filter (EKF) and Factor Graphs (FG) fusing instantaneous velocities of and distances to 3D points delivered by a lightweight, low-cost, off-the-shelf Frequency Modulated Continuous Wave (FMCW) radar with IMU readings. We also show a novel way to exploit advances in deep learning to retrieve 3D point correspondences in sparse and noisy radar point clouds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向计算受限航空导航的雷达-惯性里程计</div>
<div class="mono" style="margin-top:8px">近年来，雷达传感技术在封装微型化和测量精度提升方面的进展引起了机器人研究界的关注。实现机器人自主性的关键任务在于精确确定其在空间中的位姿。为完成此任务，常采用传感器融合算法，将激光雷达、相机、激光测距传感器或GNSS等外部感知传感器的数据与惯性测量单元（IMU）的测量值融合，以估计机器人的导航状态。然而，由于特定传感原理，某些外部感知传感器在极端环境条件下（如极端光照或环境中存在烟雾等细颗粒物）常无法正常工作。雷达凭借其使用的电磁波特性，对上述因素具有较强抗干扰能力。本论文提出雷达-惯性里程计（RIO）算法，通过融合IMU与雷达数据来估计无人机（UAV）的导航状态，该算法可在便携式资源受限的嵌入式计算机上实时运行，并使用廉价的消费级传感器。我们提出了基于多状态紧耦合扩展卡尔曼滤波器（EKF）和因子图（FG）的新型RIO方法，将轻量级低成本商用调频连续波（FMCW）雷达提供的三维点瞬时速度与距离测量值，与IMU读数进行融合。同时展示了一种利用深度学习进展的新方法，用于在稀疏且含噪声的雷达点云中提取三维点对应关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for robust aerial navigation in computationally constrained and environmentally challenging conditions, where traditional sensors like LiDAR or cameras may fail. The authors propose Radar-Inertial Odometry (RIO) algorithms that fuse data from a low-cost FMCW radar and an IMU using multi-state tightly-coupled Extended Kalman Filters and Factor Graphs, while also incorporating a deep learning method to establish 3D point correspondences in sparse radar point clouds. Experimental results demonstrate that the system can estimate UAV navigation states in real-time on a portable embedded computer, offering resilience to factors such as poor illumination, smoke, or fog.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决无人机在极端环境（如烟雾、强光）中导航时，传统传感器（如激光雷达、相机）可能失效的问题，为此开发了雷达-惯性里程计（RIO）算法，融合惯性测量单元（IMU）和低成本调频连续波（FMCW）雷达的数据。方法上，提出了基于多状态紧耦合扩展卡尔曼滤波和因子图的新颖方法，以整合雷达测量的3D点速度和距离与IMU读数，并利用深度学习技术从稀疏、噪声的雷达点云中获取点对应关系。实验结果表明，该系统能够在资源受限的嵌入式计算机上实时估计无人机导航状态，且仅使用廉价的消费级传感器。</div>
</details>
</div>
<div class="card">
<div class="title">Realistic adversarial scenario generation via human-like pedestrian model for autonomous vehicle control parameter optimisation</div>
<div class="meta-line">Authors: Yueyang Wang, Mehmet Dogar, Russell Darling, Gustav Markkula</div>
<div class="meta-line">First: 2026-01-05T13:10:32+00:00 · Latest: 2026-02-04T14:57:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02082v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02082v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous vehicles (AVs) are rapidly advancing and are expected to play a central role in future mobility. Ensuring their safe deployment requires reliable interaction with other road users, not least pedestrians. Direct testing on public roads is costly and unsafe for rare but critical interactions, making simulation a practical alternative. Within simulation-based testing, adversarial scenarios are widely used to probe safety limits, but many prioritise difficulty over realism, producing exaggerated behaviours which may result in AV controllers that are overly conservative. We propose an alternative method, instead using a cognitively inspired pedestrian model featuring both inter-individual and intra-individual variability to generate behaviourally plausible adversarial scenarios. We provide a proof of concept demonstration of this method&#x27;s potential for AV control optimisation, in closed-loop testing and tuning of an AV controller. Our results show that replacing the rule-based CARLA pedestrian with the human-like model yields more realistic gap acceptance patterns and smoother vehicle decelerations. Unsafe interactions occur only for certain pedestrian individuals and conditions, underscoring the importance of human variability in AV testing. Adversarial scenarios generated by this model can be used to optimise AV control towards safer and more efficient behaviour. Overall, this work illustrates how incorporating human-like road user models into simulation-based adversarial testing can enhance the credibility of AV evaluation and provide a practical basis to behaviourally informed controller optimisation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于类人行人模型的自动驾驶车辆控制参数优化对抗场景生成方法</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆正快速发展，预计将在未来交通中发挥核心作用。确保其安全部署需要与其他道路使用者（尤其是行人）进行可靠交互。在公共道路上直接测试成本高昂，且对罕见但关键的交互场景存在安全隐患，因此仿真成为实用替代方案。在基于仿真的测试中，对抗场景被广泛用于探测安全边界，但许多方法过于追求难度而牺牲真实性，产生夸张行为，可能导致自动驾驶控制器过于保守。本研究提出一种替代方法，采用具有个体间与个体内变异性的认知启发式行人模型，生成行为合理的对抗场景。通过闭环测试与自动驾驶控制器调参，我们提供了该方法在控制优化潜力的概念验证。结果表明：用类人模型替代基于规则的CARLA行人模型，能产生更真实的间隙接受模式和更平顺的车辆减速行为。不安全交互仅出现在特定行人个体与条件下，凸显了人类行为变异性在自动驾驶测试中的重要性。该模型生成的对抗场景可用于优化自动驾驶控制策略，实现更安全高效的行为。总体而言，本研究表明将类人道路使用者模型融入基于仿真的对抗测试，能提升自动驾驶评估的可信度，并为行为导向的控制器优化提供实践基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To ensure autonomous vehicle (AV) safety in interactions with pedestrians without costly real-world testing, this study addresses the lack of realism in simulated adversarial scenarios, which often prioritize extreme difficulty over behavioral plausibility. The authors propose a method that generates adversarial scenarios using a cognitively inspired pedestrian model incorporating both inter-individual and intra-individual variability to produce more human-like behavior. In closed-loop testing for AV control optimization, replacing the rule-based CARLA pedestrian model with this human-like model resulted in more realistic pedestrian gap acceptance patterns and smoother vehicle decelerations, with unsafe interactions occurring only under specific pedestrian traits and conditions, highlighting the role of human variability in effective AV testing.</div>
<div class="mono" style="margin-top:8px">为确保自动驾驶车辆与行人交互的安全性，同时避免高成本的实路测试，本研究针对模拟对抗场景中缺乏真实性的问题，提出了一种认知启发的行人模型，该模型结合了个体间和个体内的变异性，以生成行为上合理的场景。该方法通过在仿真中对自动驾驶控制器进行闭环测试和调参来验证，用该类人模型替代基于规则的行人模型后，产生了更真实的间隙接受模式和更平滑的车辆减速。实验结果表明，不安全交互仅发生在特定行人个体和条件下，这凸显了人类变异性在自动驾驶测试中的重要性，并使得这些对抗场景可用于优化自动驾驶控制，以实现更安全、更高效的行为。</div>
</details>
</div>
<div class="card">
<div class="title">Can We Redesign a Shoulder Exosuit to Enhance Comfort and Usability Without Losing Assistance?</div>
<div class="meta-line">Authors: Roberto Ferroni, Daniele Filippo Mauceri, Jacopo Carpaneto, Alessandra Pedrocchi, Tommaso Proietti</div>
<div class="meta-line">First: 2026-02-04T14:56:25+00:00 · Latest: 2026-02-04T14:56:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04625v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reduced shoulder mobility limits upper-limb function and the performance of activities of daily living across a wide range of conditions. Wearable exosuits have shown promise in assisting arm elevation, reducing muscle effort, and supporting functional movements; however, comfort is rarely prioritized as an explicit design objective, despite it strongly affects real-life, long-term usage. This study presents a redesigned soft shoulder exosuit (Soft Shoulder v2) developed to address comfort-related limitations identified in our previous version, while preserving assistive performance. In parallel, assistance was also improved, shifting from the coronal plane to the sagittal plane to better support functionally relevant hand positioning. A controlled comparison between the previous (v1) and redesigned (v2) modules was conducted in eight healthy participants, who performed static holding, dynamic lifting, and a functional pick and place task. Muscle activity, kinematics, and user-reported outcomes were assessed. Both versions increased endurance time, reduced deltoid activation, and preserved transparency during unpowered shoulder elevation. However, the difference between them emerged most clearly during functional tasks and comfort evaluation. The redesigned module facilitated forward arm positioning and increased transverse plane mobility by up to 30 deg, without increasing muscular demand. User-reported outcomes further indicated a substantial improvement in wearability, with markedly lower perceived pressure and higher ratings in effectiveness, ease of use, and comfort compared to the previous design. Taken together, these findings show that targeted, user-centered design refinements can improve comfort and functional interaction without compromising assistive performance, advancing the development of soft exosuits suitable for prolonged and daily use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>能否重新设计肩部外骨骼服以提升舒适性与可用性，同时保持辅助功能？</div>
<div class="mono" style="margin-top:8px">肩关节活动度受限会广泛影响上肢功能与日常生活活动表现。可穿戴外骨骼服在辅助手臂抬升、降低肌肉负荷及支持功能性运动方面展现出潜力；然而，尽管舒适度显著影响实际长期使用，却鲜少被列为明确的设计目标。本研究提出重新设计的软质肩部外骨骼服（Soft Shoulder v2），旨在解决前代版本中发现的舒适性局限，同时保持辅助性能。辅助功能亦同步改进——从冠状面转向矢状面辅助，以更好地支持功能性手部定位。研究对八名健康参与者进行了前代（v1）与重新设计（v2）模块的对照比较，测试包括静态维持、动态抬举及功能性拾取放置任务。评估指标涵盖肌肉活动、运动学数据及用户反馈。两个版本均延长了耐力时间，降低了三角肌激活水平，并在无动力肩部抬升时保持透明度。但二者差异在功能性任务与舒适度评估中最为显著：重新设计的模块促进了手臂前伸定位，将水平面活动度提升达30度，且未增加肌肉负荷。用户反馈进一步表明穿戴性大幅改善——相较于前代设计，感知压力显著降低，在有效性、易用性与舒适度方面评分明显提升。综上，这些发现表明以用户为中心的针对性设计优化，可在不影响辅助性能的前提下改善舒适度与功能交互，推动适用于长期日常使用的软质外骨骼服发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aimed to redesign a soft shoulder exosuit to improve comfort and usability, which are critical for long-term adoption but often overlooked, while maintaining its assistive performance. The method involved developing a new version (Soft Shoulder v2) with design changes to address comfort limitations from the prior version (v1) and shifting assistance from the coronal to the sagittal plane to better support functional hand positioning. Experimental results from eight healthy participants showed that both versions increased endurance time and reduced deltoid activation, but the v2 design specifically improved forward arm positioning and transverse plane mobility by up to 30 degrees without increasing muscle demand, alongside significantly higher user ratings for comfort, ease of use, and effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究旨在重新设计一款软性肩部外服，在保持辅助性能的同时，改善舒适性和可用性，这对长期使用至关重要但常被忽视。方法包括开发新版本（Soft Shoulder v2），通过设计变更解决先前版本的舒适性问题，并将辅助从冠状面转向矢状面以更好地支持功能性手部定位。对八名健康参与者的实验结果表明，两个版本均提高了耐力和降低了三角肌激活，但重新设计的模块特别改善了前臂定位和横断面活动度达30度，且未增加肌肉需求，同时在舒适性、易用性和有效性方面获得了显著更高的用户评分。</div>
</details>
</div>
<div class="card">
<div class="title">Act, Sense, Act: Learning Non-Markovian Active Perception Strategies from Large-Scale Egocentric Human Data</div>
<div class="meta-line">Authors: Jialiang Li, Yi Qiao, Yunhan Guo, Changwen Chen, Wenzhao Lian</div>
<div class="meta-line">First: 2026-02-04T14:28:14+00:00 · Latest: 2026-02-04T14:28:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04600v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04600v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving generalizable manipulation in unconstrained environments requires the robot to proactively resolve information uncertainty, i.e., the capability of active perception. However, existing methods are often confined in limited types of sensing behaviors, restricting their applicability to complex environments. In this work, we formalize active perception as a non-Markovian process driven by information gain and decision branching, providing a structured categorization of visual active perception paradigms. Building on this perspective, we introduce CoMe-VLA, a cognitive and memory-aware vision-language-action (VLA) framework that leverages large-scale human egocentric data to learn versatile exploration and manipulation priors. Our framework integrates a cognitive auxiliary head for autonomous sub-task transitions and a dual-track memory system to maintain consistent self and environmental awareness by fusing proprioceptive and visual temporal contexts. By aligning human and robot hand-eye coordination behaviors in a unified egocentric action space, we train the model progressively in three stages. Extensive experiments on a wheel-based humanoid have demonstrated strong robustness and adaptability of our proposed method across diverse long-horizon tasks spanning multiple active perception scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行动、感知、再行动：基于大规模第一人称人类数据学习非马尔可夫主动感知策略</div>
<div class="mono" style="margin-top:8px">在非结构化环境中实现泛化操作要求机器人能主动消解信息不确定性，即具备主动感知能力。然而现有方法常受限于有限的感知行为类型，难以适应复杂环境。本研究将主动感知形式化为由信息增益与决策分支驱动的非马尔可夫过程，建立了视觉主动感知范式的结构化分类体系。基于此，我们提出CoMe-VLA——一种认知与记忆增强的视觉-语言-行动框架，通过大规模人类第一视角数据学习通用探索与操作先验。该框架集成认知辅助模块实现自主子任务转换，并采用双通道记忆系统融合本体感知与视觉时序上下文，维持稳定的自我与环境认知。通过将人类与机器人的手眼协调行为对齐到统一的第一人称行动空间，我们分三阶段渐进训练模型。在轮式人形机器人上的大量实验表明，该方法在跨越多类主动感知场景的长时序任务中均展现出卓越的鲁棒性与适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable robots to perform generalizable manipulation in unstructured environments, this work addresses the need for active perception beyond limited sensing behaviors by formalizing it as a non-Markovian process driven by information gain and decision branching. The method introduces CoMe-VLA, a cognitive and memory-aware vision-language-action framework that learns versatile exploration and manipulation priors from large-scale human egocentric data, incorporating a cognitive auxiliary head for sub-task transitions and a dual-track memory system to fuse proprioceptive and visual contexts. Experimental results on a wheel-based humanoid demonstrate strong robustness and adaptability across diverse long-horizon tasks involving multiple active perception scenarios.</div>
<div class="mono" style="margin-top:8px">为使机器人能在非结构化复杂环境中通过主动感知来应对信息不确定性，本研究将主动感知形式化为一个由信息增益和决策分支驱动的非马尔可夫过程。所提出的方法CoMe-VLA是一个认知与记忆感知的视觉-语言-动作框架，它利用大规模人类第一视角数据学习通用的探索与操作先验；该框架集成了一个用于子任务自主转换的认知辅助头和一个双轨记忆系统，以融合本体感觉与视觉时序上下文。在轮式人形机器人上的大量实验表明，该方法在涵盖多种主动感知场景的多样长程任务中表现出强大的鲁棒性和适应性。</div>
</details>
</div>
<div class="card">
<div class="title">TouchGuide: Inference-Time Steering of Visuomotor Policies via Touch Guidance</div>
<div class="meta-line">Authors: Zhemeng Zhang, Jiahua Ma, Xincheng Yang, Xin Wen, Yuzhi Zhang, Boyan Li, Yiran Qin, Jin Liu, Can Zhao, Li Kang, Haoqin Hong, Zhenfei Yin, Philip Torr, Hao Su, Ruimao Zhang, Daolin Ma</div>
<div class="meta-line">First: 2026-01-28T04:22:47+00:00 · Latest: 2026-02-04T13:30:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20239v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20239v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained and contact-rich manipulation remain challenging for robots, largely due to the underutilization of tactile feedback. To address this, we introduce TouchGuide, a novel cross-policy visuo-tactile fusion paradigm that fuses modalities within a low-dimensional action space. Specifically, TouchGuide operates in two stages to guide a pre-trained diffusion or flow-matching visuomotor policy at inference time. First, the policy produces a coarse, visually-plausible action using only visual inputs during early sampling. Second, a task-specific Contact Physical Model (CPM) provides tactile guidance to steer and refine the action, ensuring it aligns with realistic physical contact conditions. Trained through contrastive learning on limited expert demonstrations, the CPM provides a tactile-informed feasibility score to steer the sampling process toward refined actions that satisfy physical contact constraints. Furthermore, to facilitate TouchGuide training with high-quality and cost-effective data, we introduce TacUMI, a data collection system. TacUMI achieves a favorable trade-off between precision and affordability; by leveraging rigid fingertips, it obtains direct tactile feedback, thereby enabling the collection of reliable tactile data. Extensive experiments on five challenging contact-rich tasks, such as shoe lacing and chip handover, show that TouchGuide consistently and significantly outperforms state-of-the-art visuo-tactile policies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TouchGuide：通过触觉引导在推理时操控视觉运动策略</div>
<div class="mono" style="margin-top:8px">精细且接触密集的操作对机器人而言仍具挑战性，主要原因是触觉反馈未得到充分利用。为此，我们提出了TouchGuide，一种新颖的跨策略视觉-触觉融合范式，可在低维动作空间内融合多模态信息。具体而言，TouchGuide在推理时通过两个阶段引导预训练的扩散或流匹配视觉运动策略：首先，策略在早期采样阶段仅使用视觉输入生成粗略但视觉合理的动作；其次，任务特定的接触物理模型（CPM）提供触觉引导以调整和优化该动作，确保其符合真实的物理接触条件。CPM通过在有限专家演示上进行对比学习训练，提供触觉感知的可行性评分，从而引导采样过程生成满足物理接触约束的精细化动作。此外，为利用高质量且成本可控的数据促进TouchGuide训练，我们开发了TacUMI数据采集系统。该系统通过采用刚性指尖获取直接触觉反馈，在精度与成本间实现了良好平衡，从而能够采集可靠的触觉数据。在系鞋带、芯片交接等五项高难度接触密集型任务上的大量实验表明，TouchGuide始终显著优于当前最先进的视觉-触觉策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of fine-grained, contact-rich robotic manipulation by improving the utilization of tactile feedback. The proposed TouchGuide method introduces a two-stage, inference-time steering paradigm that fuses visual and tactile modalities within a low-dimensional action space: a pre-trained diffusion or flow-matching visuomotor policy first generates a coarse action from visual input, which is then refined by a task-specific Contact Physical Model (CPM) that uses tactile guidance to ensure actions satisfy physical contact constraints. The CPM is trained via contrastive learning on limited demonstrations, and a cost-effective data collection system, TacUMI, is introduced to gather reliable tactile data. Experimental results on five challenging tasks, including shoe lacing and chip handover, demonstrate that TouchGuide consistently and significantly outperforms existing state-of-the-art visuo-tactile policies.</div>
<div class="mono" style="margin-top:8px">该研究针对机器人精细、接触丰富的操作任务中触觉反馈利用不足的挑战，提出了一种改进方法。该方法引入了TouchGuide，这是一种两阶段的推理时引导框架：首先使用预训练的扩散或流匹配视觉运动策略从视觉输入生成粗略的、视觉上合理的动作，然后通过接触物理模型（CPM）提供触觉引导来细化动作，确保其符合实际物理接触约束；CPM通过对比学习在有限的专家演示数据上训练。在鞋带系扣和芯片交接等五个接触丰富的任务上的实验结果表明，TouchGuide一致且显著地优于最先进的视觉-触觉策略。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Complementarity-based Approach for Rigid-Body Manipulation and Motion Prediction</div>
<div class="meta-line">Authors: Bingkun Huang, Xin Ma, Nilanjan Chakraborty, Riddhiman Laha</div>
<div class="meta-line">First: 2026-02-04T13:10:57+00:00 · Latest: 2026-02-04T13:10:57+00:00</div>
<div class="meta-line">Comments: 18 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04522v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04522v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation in unstructured environments requires planners to reason jointly about free-space motion and sustained, frictional contact with the environment. Existing (local) planning and simulation frameworks typically separate these regimes or rely on simplified contact representations, particularly when modeling non-convex or distributed contact patches. Such approximations limit the fidelity of contact-mode transitions and hinder the robust execution of contact-rich behaviors in real time. This paper presents a unified discrete-time modeling framework for robotic manipulation that consistently captures both free motion and frictional contact within a single mathematical formalism (Unicomp). Building on complementarity-based rigid-body dynamics, we formulate free-space motion and contact interactions as coupled linear and nonlinear complementarity problems, enabling principled transitions between contact modes without enforcing fixed-contact assumptions. For planar patch contact, we derive a frictional contact model from the maximum power dissipation principle in which the set of admissible contact wrenches is represented by an ellipsoidal limit surface. This representation captures coupled force-moment effects, including torsional friction, while remaining agnostic to the underlying pressure distribution across the contact patch. The resulting formulation yields a discrete-time predictive model that relates generalized velocities and contact wrenches through quadratic constraints and is suitable for real-time optimization-based planning. Experimental results show that the proposed approach enables stable, physically consistent behavior at interactive speeds across tasks, from planar pushing to contact-rich whole-body maneuvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于互补性的刚体操作与运动预测统一方法</div>
<div class="mono" style="margin-top:8px">非结构化环境中的机器人操作要求规划器同时考虑自由空间运动与持续的环境摩擦接触。现有（局部）规划与仿真框架通常分离这两种状态，或依赖简化的接触表征，尤其在建模非凸或分布式接触区域时。此类近似限制了接触模式转换的保真度，阻碍了实时鲁棒执行密集接触行为。本文提出一种统一的离散时间机器人操作建模框架，在单一数学形式体系（Unicomp）内一致描述自由运动与摩擦接触。基于互补性刚体动力学，我们将自由空间运动与接触交互构建为耦合的线性和非线性互补问题，实现接触模式间的原理性转换，无需强制固定接触假设。针对平面区域接触，我们从最大功率耗散原理推导出摩擦接触模型，其中可容许接触力矩集合由椭球极限曲面表征。该表征捕捉了力-力矩耦合效应（包括扭转摩擦），同时独立于接触区域的底层压力分布。最终构建的离散时间预测模型通过二次约束关联广义速度与接触力矩，适用于基于实时优化的规划。实验结果表明，所提方法能在交互速度下实现从平面推送到密集接触全身动作等任务的稳定、物理一致行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for robotic manipulation planners to jointly reason about free-space motion and frictional contact in unstructured environments, as existing methods often separate these regimes or use simplified contact models, limiting fidelity and robustness. The proposed method, Unicomp, introduces a unified discrete-time framework based on complementarity-based rigid-body dynamics, formulating motion and contact as coupled linear and nonlinear complementarity problems to enable principled contact-mode transitions; for planar patch contact, it derives a frictional model from the maximum power dissipation principle using an ellipsoidal limit surface to capture coupled force-moment effects like torsional friction. Experimental results demonstrate that this approach achieves stable, physically consistent behavior at interactive speeds in tasks ranging from planar pushing to contact-rich whole-body maneuvers.</div>
<div class="mono" style="margin-top:8px">该研究针对非结构化环境中机器人操作规划器需同时处理自由空间运动和摩擦接触的需求，现有方法常分离这两种状态或使用简化接触模型，限制了保真度和鲁棒性。方法提出了一种基于互补性刚体动力学的统一离散时间框架（Unicomp），将运动和接触表述为耦合的线性和非线性互补问题，以实现有原则的接触模式转换；对于平面接触面，从最大功率耗散原理推导出摩擦模型，用椭球极限曲面表示允许的接触力矩，以捕捉包括扭转摩擦在内的耦合力-力矩效应。实验结果表明，该方法在从平面推送到接触丰富的全身操作等任务中，能以交互速度实现稳定、物理一致的行为。</div>
</details>
</div>
<div class="card">
<div class="title">S-MUSt3R: Sliding Multi-view 3D Reconstruction</div>
<div class="meta-line">Authors: Leonid Antsfeld, Boris Chidlovskii, Yohann Cabon, Vincent Leroy, Jerome Revaud</div>
<div class="meta-line">First: 2026-02-04T13:07:14+00:00 · Latest: 2026-02-04T13:07:14+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04517v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04517v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S-MUSt3R：滑动多视图三维重建</div>
<div class="mono" style="margin-top:8px">三维视觉领域近期的范式转变催生了基础模型的兴起，这些模型从未标定图像中获取三维感知的能力显著。然而，由于内存限制，将这些模型扩展至大规模RGB流三维重建仍具挑战。本研究提出S-MUSt3R——一种简洁高效的流程，突破了基础模型在单目三维重建中的性能边界。我们通过序列分割、分段对齐与轻量级闭环优化的简单策略，解决了基础模型的可扩展性瓶颈。无需重新训练模型，即可继承MUSt3R模型卓越的三维重建能力，在轨迹与重建效果上达到与传统复杂架构方法相当的水平。我们在TUM、7-Scenes及专有机器人导航数据集上评估S-MUSt3R，证明其能成功处理长RGB序列并生成精确一致的三维重建结果。本研究凸显了利用MUSt3R模型实现可扩展单目三维场景重建在实际应用中的潜力，其重要优势在于可直接在度量空间中进行预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the memory limitations that hinder the application of 3D vision foundation models to large-scale RGB stream reconstruction, this work introduces S-MUSt3R, a pipeline that extends the MUSt3R model without retraining. The method employs a strategy of segmenting long image sequences, aligning the segments, and applying lightweight loop closure optimization to achieve scalability. Experimental evaluation on TUM, 7-Scenes, and robot navigation datasets demonstrates that S-MUSt3R successfully processes long sequences, producing accurate, metric-scale 3D reconstructions with performance comparable to more complex traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将用于3D感知的基础模型扩展到大规模单目RGB视频流重建时所面临的内存限制挑战。提出的S-MUSt3R流程无需重新训练，通过分割长序列、对齐片段并进行轻量级闭环优化的策略，扩展了MUSt3R基础模型的能力。在TUM、7-Scenes和机器人导航数据集上的实验评估表明，该方法能成功处理长序列，产生准确、一致且具有公制尺度的3D重建结果，其性能可与更复杂的传统架构相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">TACO: Temporal Consensus Optimization for Continual Neural Mapping</div>
<div class="meta-line">Authors: Xunlan Zhou, Hongrui Zhao, Negar Mehr</div>
<div class="meta-line">First: 2026-02-04T13:07:08+00:00 · Latest: 2026-02-04T13:07:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04516v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TACO：持续神经建图的时间一致性优化方法</div>
<div class="mono" style="margin-top:8px">神经隐式建图已成为机器人导航与场景理解的重要范式。然而现实机器人部署需要在严格的内存与计算限制下持续适应动态环境，现有建图系统均无法满足这一需求。现有方法多依赖历史观测回放以维持一致性，并假设场景静态，因而难以适应动态机器人场景中的持续学习。为此，我们提出TACO（时间一致性优化）——一种无需回放的持续神经建图框架。我们将建图重构为时间一致性优化问题，将历史模型快照视为时间相邻节点。该方法本质上是模型对自身历史知识的动态参照：通过强制当前地图与历史表征达成加权共识来实现更新，使可靠的历史几何结构约束优化过程，同时允许根据新观测修正不可靠或过时的区域。TACO在不存储或回放历史数据的前提下，实现了内存效率与适应能力的平衡。大量仿真与真实场景实验表明，TACO能稳健适应场景变化，其性能持续优于其他持续学习基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing neural implicit mapping systems, which cannot adapt to changing environments under strict memory and computation constraints in continual robotic deployment. The proposed TACO method reformulates mapping as a temporal consensus optimization problem, treating past model snapshots as temporal neighbors to enforce weighted consensus with historical representations, thereby balancing memory efficiency and adaptability without data replay. Experimental results from simulated and real-world tests demonstrate that TACO robustly adapts to scene changes and consistently outperforms other continual learning baselines.</div>
<div class="mono" style="margin-top:8px">该研究针对现有神经隐式建图系统无法在严格内存和计算限制下适应动态环境的问题，提出了TACO这一无需回放的持续神经建图框架。该方法将建图重新定义为时间一致性优化问题，将过去模型快照视为时间邻居，通过强制与历史表示进行加权一致性来更新当前地图，使得可靠的过去几何约束优化，同时允许不可靠或过时区域根据新观测进行修订。模拟和真实世界实验结果表明，TACO能够稳健适应场景变化，并持续优于其他持续学习基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：基于视觉语言模型将人形机器人任务规划落地为空间感知的自我中心动作</div>
<div class="mono" style="margin-top:8px">在现实场景中部署人形机器人面临根本性挑战，因其需要在部分信息观测和动态变化环境下，紧密整合感知、移动与操控能力，并实现不同类型子任务间的鲁棒切换。为应对这些挑战，我们提出一项新任务——EgoActing，要求将高层指令直接映射为多样化、精确且具有空间感知的人形机器人动作。我们进一步通过引入EgoActor实例化该任务：这是一个统一且可扩展的视觉语言模型，能够预测移动基元（如行走、转向、侧移、高度调整）、头部运动、操控指令及人机交互行为，以实时协调感知与执行。我们利用来自真实世界演示的纯RGB自我中心数据、空间推理问答及仿真环境演示进行广泛监督，使EgoActor在8B和4B参数模型下均能做出鲁棒的上下文感知决策，并在1秒内完成流畅的动作推理。在仿真与真实环境中的大量实验表明，EgoActor有效衔接了抽象任务规划与具体运动执行，并能泛化至多样化任务及未见环境。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deploying humanoid robots in real world requires robust integration of perception, locomotion, and manipulation under partial observations and dynamic conditions. To address this, the authors propose the EgoActing task and introduce EgoActor, a unified vision-language model that directly grounds high-level instructions into spatially-aware actions, predicting locomotion primitives, head movements, and manipulation commands. The model is trained with broad supervision from real-world egocentric demonstrations, spatial QA, and simulated data, enabling real-time inference under 1 second. Evaluations in simulated and real environments show that EgoActor effectively bridges abstract task planning with concrete motor execution and generalizes to diverse, unseen tasks.</div>
<div class="mono" style="margin-top:8px">在现实世界中部署人形机器人具有根本性挑战，因为它需要在部分信息观察和动态变化环境下紧密整合感知、移动和操作能力。为解决这一问题，本研究提出了EgoActing任务，并引入了EgoActor这一统一的视觉-语言模型，能够将高层指令直接转化为具有空间感知的移动基元、头部运动、操作命令等动作。该模型利用来自真实世界演示的以自我为中心RGB数据、空间推理问答以及模拟环境演示进行广泛监督训练，从而能够做出鲁棒的、上下文感知的决策，并以流畅的实时推理速度执行。在模拟和真实环境中的广泛评估表明，EgoActor有效地桥接了抽象任务规划与具体运动执行，并能泛化到多样化的任务和未见过的环境中。</div>
</details>
</div>
<div class="card">
<div class="title">The Supportiveness-Safety Tradeoff in LLM Well-Being Agents</div>
<div class="meta-line">Authors: Himanshi Lalwani, Hanan Salam</div>
<div class="meta-line">First: 2026-02-04T12:15:43+00:00 · Latest: 2026-02-04T12:15:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04487v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04487v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are being integrated into socially assistive robots (SARs) and other conversational agents providing mental health and well-being support. These agents are often designed to sound empathic and supportive in order to maximize user&#x27;s engagement, yet it remains unclear how increasing the level of supportive framing in system prompts influences safety relevant behavior. We evaluated 6 LLMs across 3 system prompts with varying levels of supportiveness on 80 synthetic queries spanning 4 well-being domains (1440 responses). An LLM judge framework, validated against human ratings, assessed safety and care quality. Moderately supportive prompts improved empathy and constructive support while maintaining safety. In contrast, strongly validating prompts significantly degraded safety and, in some cases, care across all domains, with substantial variation across models. We discuss implications for prompt design, model selection, and domain specific safeguards in SARs deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM健康助手的支持度与安全性权衡</div>
<div class="mono" style="margin-top:8px">大型语言模型正被整合到社交辅助机器人及其他提供心理健康与福祉支持的对话助手中。这类助手常被设计为具有同理心和支持性以提升用户参与度，但系统提示中支持性框架的增强如何影响安全相关行为尚不明确。本研究通过涵盖4个福祉领域的80条合成查询，在3种不同支持度的系统提示下评估了6个LLM模型（共1440条回复）。采用经人工评分验证的LLM评判框架评估安全性与关怀质量。适度支持性提示在保持安全性的同时提升了同理心与建设性支持；而强烈认同性提示则显著降低安全性，部分情况下损害所有领域的关怀质量，且不同模型间存在显著差异。本文进一步探讨了提示设计、模型选择及社交辅助机器人部署中领域特定保障措施的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The integration of large language models (LLMs) into well-being support agents raises concerns about balancing supportive engagement with safety, as the impact of increasingly supportive prompts on safety-critical behavior is unknown. To investigate this trade-off, the study evaluated six LLMs using three system prompts of varying supportiveness levels on 80 synthetic queries across four well-being domains, employing an LLM judge framework validated against human ratings to assess safety and care quality. Experimental results showed that moderately supportive prompts enhanced empathy and constructive support without compromising safety, whereas strongly validating prompts significantly degraded safety and, in some cases, care quality across all domains, with notable variation among different models.</div>
<div class="mono" style="margin-top:8px">将大语言模型集成到健康助手中，需要在提供支持性互动与确保安全性之间取得平衡，因为过度强调共情可能无意中损害安全回应。为研究这一权衡，该研究使用三种不同支持程度的系统提示，在四个健康领域对六个大语言模型进行了80个合成查询的评估，并采用经人类评分验证的大语言模型评判框架来评估安全性和关怀质量。实验结果表明，中等支持程度的提示在提升共情和建设性支持的同时保持了安全性，而强烈验证性的提示则显著降低了安全性，有时甚至损害了关怀质量，且不同模型间存在显著性能差异。</div>
</details>
</div>
<div class="card">
<div class="title">RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</div>
<div class="meta-line">Authors: Mingcong Lei, Honghao Cai, Yuyuan Yang, Yimou Wu, Jinke Ren, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Shaohan Jiang, Ge Wang, Junyuan Tan, Zhenglin Wan, Zheng Li, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han</div>
<div class="meta-line">First: 2025-08-02T15:39:42+00:00 · Latest: 2026-02-04T12:10:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01415v6">Abs</a> · <a href="https://arxiv.org/pdf/2508.01415v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied intelligence aims to enable robots to learn, reason, and generalize robustly across complex real-world environments. However, existing approaches often struggle with partial observability, fragmented spatial reasoning, and inefficient integration of heterogeneous memories, limiting their capacity for long-horizon adaptation. To address this, we introduce RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory within a parallelized architecture for efficient long-horizon planning and interactive learning. Its core innovations are a dynamic spatial knowledge graph for scalable, consistent memory updates and a closed-loop planner with a critic module for adaptive decision-making. Extensive experiments on EmbodiedBench show that RoboMemory, instantiated with Qwen2.5-VL-72B-Ins, improves the average success rate by 26.5% over its strong baseline and even surpasses the closed-source SOTA, Claude-3.5-Sonnet. Real-world trials further confirm its capability for cumulative learning, with performance consistently improving over repeated tasks. Our results position RoboMemory as a scalable foundation for memory-augmented embodied agents, bridging insights from cognitive neuroscience with practical robotic autonomy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboMemory：面向物理具身系统交互式环境学习的类脑多记忆智能体框架</div>
<div class="mono" style="margin-top:8px">具身智能旨在使机器人能够在复杂现实环境中稳健地学习、推理和泛化。然而，现有方法常受限于部分可观测性、碎片化空间推理及异构记忆的低效整合，制约了其长期适应能力。为此，我们提出RoboMemory——一个受大脑启发的框架，通过并行化架构统一空间、时序、情景与语义记忆，以实现高效的长时程规划与交互式学习。其核心创新包括：用于可扩展、一致性记忆更新的动态空间知识图谱，以及配备批判模块以实现自适应决策的闭环规划器。在EmbodiedBench上的大量实验表明，基于Qwen2.5-VL-72B-Ins构建的RoboMemory将平均成功率较其强基线提升26.5%，甚至超越闭源SOTA模型Claude-3.5-Sonnet。真实场景测试进一步验证了其持续学习能力，在重复任务中性能持续提升。本研究将RoboMemory确立为记忆增强型具身智能体的可扩展基础框架，为认知神经科学与实用机器人自主系统的融合搭建了桥梁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing embodied intelligence approaches in handling partial observability, fragmented spatial reasoning, and inefficient memory integration for long-horizon adaptation, this research introduces RoboMemory, a brain-inspired framework that unifies Spatial, Temporal, Episodic, and Semantic memory within a parallelized architecture. The method features a dynamic spatial knowledge graph for scalable memory updates and a closed-loop planner with a critic module for adaptive decision-making. Experimental evaluations on EmbodiedBench, using the Qwen2.5-VL-72B-Ins model, demonstrate a 26.5% average improvement in success rate over a strong baseline and performance surpassing the closed-source SOTA, Claude-3.5-Sonnet, with real-world trials confirming its capability for cumulative learning through repeated tasks.</div>
<div class="mono" style="margin-top:8px">为解决现有具身智能系统在部分可观测性、碎片化空间推理以及低效记忆整合方面对长时域适应的限制，本文提出了RoboMemory，这是一个受大脑启发的框架，在并行化架构中统一了空间、时序、情景和语义记忆。该方法的核心创新包括一个用于可扩展、一致性记忆更新的动态空间知识图谱，以及一个带有评判模块的自适应决策闭环规划器。在EmbodiedBench上的大量实验表明，基于Qwen2.5-VL-72B-Ins实现的RoboMemory，其平均成功率比强基线提高了26.5%，甚至超越了闭源SOTA模型Claude-3.5-Sonnet，真实世界试验进一步证实了其在重复任务中持续提升的累积学习能力。</div>
</details>
</div>
<div class="card">
<div class="title">Autonomous Navigation at the Nano-Scale: Algorithms, Architectures, and Constraints</div>
<div class="meta-line">Authors: Mahmud S. Zango, Jianglin Lan</div>
<div class="meta-line">First: 2026-01-19T17:38:15+00:00 · Latest: 2026-02-04T12:08:13+00:00</div>
<div class="meta-line">Comments: 30 pages, 5 figures, 2 table. Review article</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13252v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13252v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs) is governed by extreme Size, Weight, and Power (SWaP) constraints (with the weight &lt; 50 g and sub-100 mW onboard processor), distinguishing it fundamentally from standard robotic paradigms. This review synthesizes the state-of-the-art in sensing, computing, and control architectures designed specifically for these sub- 100mW computational envelopes. We critically analyse the transition from classical geometry-based methods to emerging &quot;Edge AI&quot; paradigms, including quantized deep neural networks deployed on ultra-low-power System-on-Chips (SoCs) and neuromorphic event-based control. Beyond algorithms, we evaluate the hardware-software co-design requisite for autonomy, covering advancements in dense optical flow, optimized Simultaneous Localization and Mapping (SLAM), and learning-based flight control. While significant progress has been observed in visual navigation and relative pose estimation, our analysis reveals persistent gaps in long-term endurance, robust obstacle avoidance in dynamic environments, and the &quot;Sim-to-Real&quot; transfer of reinforcement learning policies. This survey provides a roadmap for bridging these gaps, advocating for hybrid architectures that fuse lightweight classical control with data-driven perception to enable fully autonomous, agile nano-UAVs in GPS-denied environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>纳米尺度自主导航：算法、架构与约束</div>
<div class="mono" style="margin-top:8px">纳米级无人机（nano-UAV）的自主导航受极端的尺寸、重量与功耗（SWaP）约束主导（重量&lt;50克，机载处理器功耗低于100mW），这使其与标准机器人范式存在根本区别。本综述综合了专为低于100mW计算平台设计的传感、计算与控制架构的最新进展。我们批判性分析了从传统几何方法向新兴“边缘人工智能”范式的转变，包括部署在超低功耗片上系统（SoCs）上的量化深度神经网络与基于事件的神经形态控制。除算法外，我们评估了实现自主性所需的软硬件协同设计，涵盖稠密光流、优化的同步定位与建图（SLAM）以及基于学习的飞行控制等方面的进展。尽管视觉导航与相对位姿估计已取得显著进展，但分析表明在长期续航、动态环境中的鲁棒避障以及强化学习策略的“仿真到现实”迁移方面仍存在持续差距。本综述为弥合这些差距提供了路线图，倡导融合轻量级经典控制与数据驱动感知的混合架构，以实现GPS拒止环境下完全自主、敏捷的纳米级无人机。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This review addresses the unique challenges of autonomous navigation for nano-scale unmanned aerial vehicles (nano-UAVs), which operate under extreme Size, Weight, and Power (SWaP) constraints, requiring sub-100 mW onboard processing. It synthesizes state-of-the-art methods by analyzing a transition from classical geometry-based approaches to ultra-low-power Edge AI paradigms, including quantized deep neural networks on specialized SoCs and neuromorphic event-based control, while emphasizing hardware-software co-design for sensing, computing, and control. Key experimental findings indicate significant progress in visual navigation and relative pose estimation, but persistent gaps remain in long-term endurance, robust obstacle avoidance in dynamic settings, and the sim-to-real transfer of learned policies, pointing to a need for hybrid architectures that combine lightweight classical control with data-driven perception.</div>
<div class="mono" style="margin-top:8px">本综述探讨了纳米级无人机自主导航的独特挑战，这些无人机在极端的尺寸、重量和功率约束下运行，需要低于100毫瓦的机载处理器。它通过分析从经典几何方法向超低功耗边缘人工智能范式的转变，综合了最新技术，包括在专用片上系统上部署的量化深度神经网络和基于事件的神经形态控制，同时强调了传感、计算和控制所需的硬件-软件协同设计。主要实验结果表明，在视觉导航和相对位姿估计方面取得了进展，但也指出了在长期续航、动态环境中的鲁棒避障以及强化学习策略的仿真到现实迁移方面存在持续差距，主张采用融合轻量级经典控制与数据驱动感知的混合架构。</div>
</details>
</div>
<div class="card">
<div class="title">LiDAR, GNSS and IMU Sensor Fine Alignment through Dynamic Time Warping to Construct 3D City Maps</div>
<div class="meta-line">Authors: Haitian Wang, Hezam Albaqami, Xinyu Wang, Muhammad Ibrahim, Zainy M. Malakan, Abdullah M. Algamdi, Mohammed H. Alghamdi, Ajmal Mian</div>
<div class="meta-line">First: 2025-07-11T09:06:14+00:00 · Latest: 2026-02-04T11:56:40+00:00</div>
<div class="meta-line">Comments: This paper has been submitted to IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS) and is currently under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.08420v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.08420v3">PDF</a> · <a href="https://github.com/HaitianWang/LiDAR-GNSS-and-IMU-Sensor-Fine-Alignment-through-Dynamic-Time-Warping-to-Construct-3D-City-Maps">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LiDAR-based 3D mapping suffers from cumulative drift causing global misalignment, particularly in GNSS-constrained environments. To address this, we propose a unified framework that fuses LiDAR, GNSS, and IMU data for high-resolution city-scale mapping. The method performs velocity-based temporal alignment using Dynamic Time Warping and refines GNSS and IMU signals via extended Kalman filtering. Local maps are built using Normal Distributions Transform-based registration and pose graph optimization with loop closure detection, while global consistency is enforced using GNSS-constrained anchors followed by fine registration of overlapping segments. We also introduce a large-scale multimodal dataset captured in Perth, Western Australia to facilitate future research in this direction. Our dataset comprises 144,000 frames acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS trajectories, and MEMS-IMU measurements across 21 urban loops. To assess geometric consistency, we evaluated our method using alignment metrics based on road centerlines and intersections to capture both global and local accuracy. The proposed framework reduces the average global alignment error from 3.32m to 1.24m, achieving a 61.4% improvement, and significantly decreases the intersection centroid offset from 13.22m to 2.01m, corresponding to an 84.8% enhancement. The constructed high-fidelity map and raw dataset are publicly available through https://ieee-dataport.org/documents/perth-cbd-high-resolution-lidar-map-gnss-and-imu-calibration, and its visualization can be viewed at https://www.youtube.com/watch?v=-ZUgs1KyMks. The source code is available at https://github.com/HaitianWang/LiDAR-GNSS-and-IMU-Sensor-Fine-Alignment-through-Dynamic-Time-Warping-to-Construct-3D-City-Maps. This dataset and method together establish a new benchmark for evaluating 3D city mapping in GNSS-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于动态时间规整的LiDAR、GNSS与IMU传感器精细对准构建三维城市地图</div>
<div class="mono" style="margin-top:8px">基于LiDAR的三维建图存在累积漂移导致全局错位的问题，尤其在GNSS受限环境中更为突出。为此，我们提出一个融合LiDAR、GNSS和IMU数据的统一框架，用于高分辨率城市级地图构建。该方法采用动态时间规整进行基于速度的时间对准，并通过扩展卡尔曼滤波优化GNSS与IMU信号。局部地图通过基于正态分布变换的配准与闭环检测的位姿图优化构建，同时利用GNSS约束锚点保证全局一致性，并对重叠段进行精细配准。我们还发布了在西澳大利亚珀斯采集的大规模多模态数据集，包含128通道Ouster LiDAR采集的14.4万帧数据、同步RTK-GNSS轨迹及MEMS-IMU测量数据，覆盖21条城市环路。为评估几何一致性，我们采用基于道路中心线与交叉口的对准指标衡量全局与局部精度。该框架将平均全局对准误差从3.32米降至1.24米（提升61.4%），交叉口质心偏移从13.22米降至2.01米（提升84.8%）。高保真地图与原始数据集已通过https://ieee-dataport.org/documents/perth-cbd-high-resolution-lidar-map-gnss-and-imu-calibration公开，可视化结果可访问https://www.youtube.com/watch?v=-ZUgs1KyMks，源代码发布于https://github.com/HaitianWang/LiDAR-GNSS-and-IMU-Sensor-Fine-Alignment-through-Dynamic-Time-Warping-to-Construct-3D-City-Maps。本数据集与方法共同为GNSS受限环境下的三维城市建图评估设立了新基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To mitigate cumulative drift in LiDAR-based 3D city mapping, especially in GNSS-constrained environments, this work introduces a unified framework that fuses LiDAR, GNSS, and IMU data. The method employs Dynamic Time Warping for velocity-based temporal alignment, refines sensor signals with an extended Kalman filter, and constructs maps using Normal Distributions Transform registration and pose graph optimization with loop closure, followed by global GNSS anchoring and fine registration of overlapping segments. Experimental evaluation on a new large-scale dataset from Perth shows the framework reduces average global alignment error from 3.32m to 1.24m (61.4% improvement) and intersection centroid offset from 13.22m to 2.01m (84.8% enhancement), demonstrating significant gains in geometric consistency.</div>
<div class="mono" style="margin-top:8px">为解决激光雷达（LiDAR）三维城市建图中，尤其在GNSS受限环境下存在的累积漂移和全局错位问题，本研究提出了一个融合LiDAR、GNSS和IMU数据的统一框架。该方法采用动态时间规整（Dynamic Time Warping）进行基于速度的时间对齐，通过扩展卡尔曼滤波优化传感器信号，并利用正态分布变换（NDT）配准与带闭环检测的位姿图优化构建局部地图，同时通过GNSS约束锚点和重叠段精细配准来保证全局一致性。在珀斯新采集的大规模数据集上的实验评估表明，该框架将平均全局对齐误差从3.32米降低至1.24米（提升61.4%），并将交叉路口质心偏移从13.22米显著减少至2.01米（提升84.8%），为三维城市建图设立了新的基准。</div>
</details>
</div>
<div class="card">
<div class="title">PhysBrain: Human Egocentric Data as a Bridge from Vision Language Models to Physical Intelligence</div>
<div class="meta-line">Authors: Xiaopeng Lin, Shijie Lian, Bin Yu, Ruoqi Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Yurun Jin, Yukun Shi, Jiyan He, Cong Huang, Bojun Cheng, Kai Chen</div>
<div class="meta-line">First: 2025-12-18T17:27:03+00:00 · Latest: 2026-02-04T11:53:52+00:00</div>
<div class="meta-line">Comments: 21 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16793v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16793v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic generalization relies on physical intelligence: the ability to reason about state changes, contact-rich interactions, and long-horizon planning under egocentric perception and action. Vision Language Models (VLMs) are essential to Vision-Language-Action (VLA) systems, but the reliance on third-person training data creates a viewpoint gap for humanoid robots. Collecting massive robot-centric data is an ideal but impractical solution due to cost and diversity constraints. Conversely, human egocentric videos offer a highly scalable data source with rich interaction context, yet the embodiment mismatch prevents the direct application. To bridge this gap, we propose an Egocentric2Embodiment Translation Pipeline that transforms raw human egocentric videos into multi-level, schema-driven embodiment supervision with enforced evidence grounding and temporal consistency, enabling the construction of the Egocentric2Embodiment dataset (E2E-3M) at scale. An egocentric-aware embodied brain, termed PhysBrain, is obtained by training on the E2E-3M dataset. PhysBrain exhibits substantially improved egocentric understanding, particularly for planning. It provides an egocentric-aware initialization that enables more sample-efficient VLA fine-tuning and higher success rates, demonstrating effective transfer from human egocentric supervision to downstream robot control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysBrain：以人类第一视角数据为桥梁，从视觉语言模型迈向物理智能</div>
<div class="mono" style="margin-top:8px">机器人泛化能力依赖于物理智能：即在第一视角感知与行动下，对状态变化、密集接触交互和长时程规划进行推理的能力。视觉语言模型（VLMs）是视觉-语言-行动（VLA）系统的核心，但依赖第三人称训练数据为人形机器人带来了视角鸿沟。大规模收集机器人中心数据虽理想却因成本与多样性限制难以实现。相反，人类第一视角视频提供了高度可扩展且富含交互情境的数据源，但具身形态差异阻碍了其直接应用。为弥合此鸿沟，我们提出一种第一视角到具身形态转换管道，将原始人类第一视角视频转化为具有强制证据锚定与时间一致性的多层次、模式驱动的具身监督信号，从而规模化构建第一视角到具身形态数据集（E2E-3M）。通过在该数据集上训练，我们获得了具备第一视角感知的具身智能体——PhysBrain。PhysBrain展现出显著增强的第一视角理解能力，尤其在规划方面。它提供了具备第一视角感知的初始化模型，可实现更高效的VLA微调与更高的任务成功率，证明了从人类第一视角监督到下游机器人控制的有效迁移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the viewpoint gap in Vision-Language-Action systems for humanoid robots, where third-person training data limits physical intelligence for egocentric perception and action. The method introduces an Egocentric2Embodiment Translation Pipeline that converts raw human egocentric videos into a multi-level, schema-driven dataset (E2E-3M) with evidence grounding and temporal consistency, which is used to train an egocentric-aware model called PhysBrain. Experimental results show that PhysBrain significantly improves egocentric understanding and planning, providing a more sample-efficient initialization for fine-tuning that leads to higher success rates in downstream robot control tasks.</div>
<div class="mono" style="margin-top:8px">为解决第三人称视觉语言模型与人形机器人所需具身感知之间的视角差异，本研究利用可扩展的人类第一人称视频作为数据源。方法提出了一个从自我中心到具身的转换流程，将原始人类视频处理成具有证据基础和时序一致性的多层次、模式驱动的具身监督数据，从而构建了大规模E2E-3M数据集。在该数据集上训练的自我中心感知模型PhysBrain显著提升了对第一人称视角的理解和规划能力，其提供的初始化权重使得视觉-语言-动作模型的微调更加样本高效，并在下游机器人控制任务中取得了更高的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-Assisted Group Tours for Blind People</div>
<div class="meta-line">Authors: Yaxin Hu, Masaki Kuribayashi, Allan Wang, Seita Kayukawa, Daisuke Sato, Bilge Mutlu, Hironobu Takagi, Chieko Asakawa</div>
<div class="meta-line">First: 2026-02-04T11:42:42+00:00 · Latest: 2026-02-04T11:42:42+00:00</div>
<div class="meta-line">Comments: In Proceedings of ACM CHI 2026 conference on Human Factors in Computing Systems</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Group interactions are essential to social functioning, yet effective engagement relies on the ability to recognize and interpret visual cues, making such engagement a significant challenge for blind people. In this paper, we investigate how a mobile robot can support group interactions for blind people. We used the scenario of a guided tour with mixed-visual groups involving blind and sighted visitors. Based on insights from an interview study with blind people (n=5) and museum experts (n=5), we designed and prototyped a robotic system that supported blind visitors to join group tours. We conducted a field study in a science museum where each blind participant (n=8) joined a group tour with one guide and two sighted participants (n=8). Findings indicated users&#x27; sense of safety from the robot&#x27;s navigational support, concerns in the group participation, and preferences for obtaining environmental information. We present design implications for future robotic systems to support blind people&#x27;s mixed-visual group participation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向视障人士的机器人辅助团体导览</div>
<div class="mono" style="margin-top:8px">群体互动是社会功能的核心，但有效参与依赖于识别和解读视觉线索的能力，这使得视障人士面临显著挑战。本研究探讨移动机器人如何支持视障人士的群体互动。我们以混合视觉群体（含视障与明眼参观者）的导览场景为案例，基于对视障人士（n=5）和博物馆专家（n=5）的访谈研究，设计并原型开发了支持视障游客加入团体导览的机器人系统。在科学博物馆的实地研究中，每位视障参与者（n=8）与一名导游及两名明眼参与者（n=8）共同完成团体导览。研究发现：用户通过机器人导航支持获得安全感，对群体参与存在顾虑，并对环境信息获取方式存在偏好。本文提出未来机器人系统支持视障人士参与混合视觉群体的设计启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge that blind people face in group interactions, which often rely on visual cues, by exploring how a mobile robot can facilitate participation in mixed-visual group tours. The method involved conducting interviews with blind individuals and museum experts to inform the design of a robotic system, followed by a field study in a science museum where blind participants joined guided tours with sighted visitors. Key experimental findings revealed that the robot&#x27;s navigational support enhanced users&#x27; sense of safety, while also highlighting concerns about group participation and preferences for how environmental information is delivered, leading to design implications for future assistive robotic systems.</div>
<div class="mono" style="margin-top:8px">本研究针对盲人在依赖视觉线索的群体互动中面临的挑战，探索移动机器人如何协助盲人参与混合视力群体的导览活动。方法包括对盲人和博物馆专家进行访谈，以指导机器人系统的设计，随后在科学博物馆进行实地研究，让盲人参与者与视力正常的参观者一起加入导览。主要实验结果表明，机器人的导航支持增强了用户的安全感，同时也揭示了他们对群体参与的担忧以及对环境信息获取方式的偏好，从而为未来辅助机器人系统的设计提供了启示。</div>
</details>
</div>
<div class="card">
<div class="title">Gust Estimation and Rejection with a Disturbance Observer for Proprioceptive Underwater Soft Morphing Wings</div>
<div class="meta-line">Authors: Tobias Cook, Leo Micklem, Huazhi Dong, Yunjie Yang, Michael Mistry, Francesco Giorgio Serchi</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-04T11:13:51+00:00 · Latest: 2026-02-04T11:13:51+00:00</div>
<div class="meta-line">Comments: 2026 IEEE International Conference on Robotics &amp; Automation (ICRA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04438v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04438v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unmanned underwater vehicles are increasingly employed for maintenance and surveying tasks at sea, but their operation in shallow waters is often hindered by hydrodynamic disturbances such as waves, currents, and turbulence. These unsteady flows can induce rapid changes in direction and speed, compromising vehicle stability and manoeuvrability. Marine organisms contend with such conditions by combining proprioceptive feedback with flexible fins and tails to reject disturbances. Inspired by this strategy, we propose soft morphing wings endowed with proprioceptive sensing to mitigate environmental perturbations. The wing&#x27;s continuous deformation provides a natural means to infer dynamic disturbances: sudden changes in camber directly reflect variations in the oncoming flow. By interpreting this proprioceptive signal, a disturbance observer can reconstruct flow parameters in real time. To enable this, we develop and experimentally validate a dynamic model of a hydraulically actuated soft wing with controllable camber. We then show that curvature-based sensing allows accurate estimation of disturbances in the angle of attack. Finally, we demonstrate that a controller leveraging these proprioceptive estimates can reject disturbances in the lift response of the soft wing. By combining proprioceptive sensing with a disturbance observer, this technique mirrors biological strategies and provides a pathway for soft underwater vehicles to maintain stability in hazardous environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扰动观测器的本体感知水下软体变形翼阵风估计与抑制</div>
<div class="mono" style="margin-top:8px">无人水下航行器在海上维护与勘测任务中的应用日益广泛，但在浅水区域作业常受波浪、洋流和湍流等水动力扰动阻碍。这些非定常流动会引发航向与速度的急剧变化，影响航行器的稳定性和机动性。海洋生物通过结合本体感知反馈与柔性鳍尾结构来应对此类扰动。受此策略启发，我们提出配备本体感知的软体变形翼以缓解环境扰动。机翼的连续形变为推断动态扰动提供了自然途径：翼型弯度的突变直接反映了来流变化。通过解译该本体感知信号，扰动观测器可实时重构流动参数。为此，我们开发并实验验证了液压驱动的可控弯度软翼动力学模型。进而证明基于曲率的传感能精确估计攻角扰动。最后，我们展示了利用该本体感知估计的控制器可有效抑制软翼升力响应中的扰动。通过融合本体感知与扰动观测器，该技术模拟了生物策略，为软体水下航行器在危险环境中保持稳定性提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve unmanned underwater vehicle stability in shallow waters where hydrodynamic disturbances like waves and currents compromise maneuverability. The method involves developing soft morphing wings with proprioceptive sensing, using a dynamic model of a hydraulically actuated soft wing to interpret curvature changes as flow disturbances via a disturbance observer. Experimental results show that curvature-based sensing accurately estimates angle-of-attack disturbances, and a controller using these estimates effectively rejects disturbances in the wing&#x27;s lift response, enabling stable operation in turbulent environments.</div>
<div class="mono" style="margin-top:8px">为解决波浪和水流等水动力扰动在浅水区损害无人水下航行器稳定性和机动性的问题，本研究受海洋生物启发，提出了具有本体感知的软变形机翼。该方法通过建立液压驱动软机翼的动态模型，并利用基于曲率的本体感知信号，通过扰动观测器实时估计来流扰动，特别是攻角的变化。实验结果验证了该模型，并表明利用这些本体感知估计的控制器能有效抑制扰动，改善软机翼在不稳定流动中的升力响应和稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Integrated Exploration and Sequential Manipulation on Scene Graph with LLM-based Situated Replanning</div>
<div class="meta-line">Authors: Heqing Yang, Ziyuan Jiao, Shu Wang, Yida Niu, Si Liu, Hangxin Liu</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-04T10:52:53+00:00 · Latest: 2026-02-04T10:52:53+00:00</div>
<div class="meta-line">Comments: 8 pages, 7 figures; accepted by ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04419v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In partially known environments, robots must combine exploration to gather information with task planning for efficient execution. To address this challenge, we propose EPoG, an Exploration-based sequential manipulation Planning framework on Scene Graphs. EPoG integrates a graph-based global planner with a Large Language Model (LLM)-based situated local planner, continuously updating a belief graph using observations and LLM predictions to represent known and unknown objects. Action sequences are generated by computing graph edit operations between the goal and belief graphs, ordered by temporal dependencies and movement costs. This approach seamlessly combines exploration and sequential manipulation planning. In ablation studies across 46 realistic household scenes and 5 long-horizon daily object transportation tasks, EPoG achieved a success rate of 91.3%, reducing travel distance by 36.1% on average. Furthermore, a physical mobile manipulator successfully executed complex tasks in unknown and dynamic environments, demonstrating EPoG&#x27;s potential for real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型情境重规划的图场景集成探索与序列化操作</div>
<div class="mono" style="margin-top:8px">在部分已知环境中，机器人需结合探索收集信息与任务规划以实现高效执行。为此，我们提出EPoG——一种基于场景图的探索式序列化操作规划框架。EPoG将基于图的全局规划器与基于大语言模型的情境局部规划器相结合，通过观测数据和LLM预测持续更新信念图以表征已知与未知物体。通过计算目标图与信念图之间的图编辑操作生成动作序列，并依据时序依赖性与移动成本排序。该方法实现了探索与序列化操作规划的无缝融合。在46个真实家居场景与5项长周期日常物品搬运任务的消融实验中，EPoG达成91.3%的成功率，平均减少36.1%的移动距离。此外，实体移动机械臂在未知动态环境中成功执行复杂任务，展现了EPoG在实际应用中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable robots to efficiently perform tasks in partially known environments by integrating exploration and sequential manipulation planning, this work proposes EPoG, a framework that combines a graph-based global planner with an LLM-based situated local planner. The method continuously updates a belief scene graph using observations and LLM predictions, then generates action sequences by computing graph edit operations between the goal and belief graphs, ordered by dependencies and movement costs. Experimental results from 46 household scenes and 5 long-horizon tasks show EPoG achieved a 91.3% success rate and reduced travel distance by 36.1% on average, with successful physical demonstrations in unknown, dynamic environments.</div>
<div class="mono" style="margin-top:8px">为使机器人在部分已知环境中能高效执行任务，本研究提出EPoG框架，将基于图的全局规划器与基于大语言模型的局部情境规划器相结合，以整合探索与顺序操作规划。该方法利用观测和LLM预测持续更新信念场景图，并通过计算目标图与信念图之间的图编辑操作来生成动作序列，同时考虑时序依赖和移动成本。在46个真实家居场景和5个长视野物体搬运任务上的实验表明，EPoG取得了91.3%的成功率，平均减少36.1%的移动距离，并在未知动态环境中通过实体移动机械臂成功完成了复杂任务。</div>
</details>
</div>
<div class="card">
<div class="title">HoRD: Robust Humanoid Control via History-Conditioned Reinforcement Learning and Online Distillation</div>
<div class="meta-line">Authors: Puyue Wang, Jiawei Hu, Yan Gao, Junyan Wang, Yu Zhang, Gillian Dobbie, Tao Gu, Wafa Johal, Ting Dang, Hong Jia</div>
<div class="meta-line">First: 2026-02-04T10:41:23+00:00 · Latest: 2026-02-04T10:41:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04412v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04412v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots can suffer significant performance drops under small changes in dynamics, task specifications, or environment setup. We propose HoRD, a two-stage learning framework for robust humanoid control under domain shift. First, we train a high-performance teacher policy via history-conditioned reinforcement learning, where the policy infers latent dynamics context from recent state--action trajectories to adapt online to diverse randomized dynamics. Second, we perform online distillation to transfer the teacher&#x27;s robust control capabilities into a transformer-based student policy that operates on sparse root-relative 3D joint keypoint trajectories. By combining history-conditioned adaptation with online distillation, HoRD enables a single policy to adapt zero-shot to unseen domains without per-domain retraining. Extensive experiments show HoRD outperforms strong baselines in robustness and transfer, especially under unseen domains and external perturbations. Code and project page are available at \href{https://tonywang-0517.github.io/hord/}{https://tonywang-0517.github.io/hord/}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HoRD：基于历史条件强化学习与在线蒸馏的鲁棒人形机器人控制</div>
<div class="mono" style="margin-top:8px">人形机器人在动力学特性、任务规范或环境设置的微小变化下可能出现性能显著下降。本文提出HoRD——一种面向领域偏移下鲁棒人形控制的两阶段学习框架。第一阶段通过历史条件强化学习训练高性能教师策略，该策略从近期状态-动作轨迹推断潜在动力学上下文，从而在线适应多样化的随机动力学。第二阶段执行在线蒸馏，将教师的鲁棒控制能力迁移至基于Transformer的学生策略，该策略以稀疏的根关节相对三维关键点轨迹作为输入。通过结合历史条件适应与在线蒸馏，HoRD使单一策略能够零样本适应未见领域，无需针对每个领域重新训练。大量实验表明，HoRD在鲁棒性与迁移性上均优于强基线方法，尤其在未见领域和外部扰动场景下表现突出。代码与项目页面详见：https://tonywang-0517.github.io/hord/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the sensitivity of humanoid robots to minor variations in dynamics, tasks, or environments, this study introduces HoRD, a two-stage learning framework for robust control under domain shifts. The method first trains a teacher policy using history-conditioned reinforcement learning, which infers latent dynamics context from recent state-action trajectories to adapt online to randomized dynamics. It then employs online distillation to transfer this robustness to a transformer-based student policy that processes sparse 3D joint keypoint trajectories. Experimental results demonstrate that HoRD surpasses baseline methods in robustness and zero-shot transfer to unseen domains, effectively handling external perturbations without requiring per-domain retraining.</div>
<div class="mono" style="margin-top:8px">人形机器人在面对动力学、任务或环境的微小变化时，性能常会严重下降。为解决领域偏移下的鲁棒性控制问题，本研究提出了HoRD框架，它包含两个阶段：首先通过历史条件强化学习训练一个高性能教师策略，该策略从近期的状态-动作轨迹推断潜在动力学上下文，以在线适应随机化的动力学；随后通过在线蒸馏将教师的鲁棒控制能力迁移至一个基于Transformer的学生策略，该策略处理稀疏的根相对3D关节关键点轨迹。大量实验表明，HoRD在鲁棒性和零样本迁移到未见领域方面优于现有基线，尤其在外部扰动下表现突出，且无需针对每个领域重新训练。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0531.html">20260206_0531</a>
<a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
