<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-04 04:56</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260204_0456</div>
    <div class="row"><div class="card">
<div class="title">ReasonEdit: Editing Vision-Language Models using Human Reasoning</div>
<div class="meta-line">Authors: Jiaxing Qiu, Kaihua Hou, Roxana Daneshjou, Ahmed Alaa, Thomas Hartvigsen</div>
<div class="meta-line">First: 2026-02-02T18:06:14+00:00 · Latest: 2026-02-02T18:06:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02408v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02408v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images.We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReasonEdit：基于人类推理的视觉语言模型编辑方法</div>
<div class="mono" style="margin-top:8px">模型编辑旨在修正大型预训练模型的错误而不影响无关行为。现有视觉语言模型编辑器尚未涉及需要人类与模型对图像进行推理的复杂任务。为此，我们提出首个支持用户在编辑过程中解释推理逻辑的视觉语言模型编辑器ReasonEdit，构建了新颖实用的模型编辑框架。该方法通过受网络科学启发的拓扑平衡多模态嵌入技术，持续将人类推理存储于编码本，并在推理时仅检索相关事实。在四个视觉语言模型及多个基于推理的视觉问答数据集上的实验表明，ReasonEdit实现了最先进的编辑性能，证明编辑过程中引入人类推理能显著提升编辑泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to correct errors in vision-language models (VLMs) without affecting unrelated capabilities, particularly for reasoning-heavy tasks where existing editors fall short. The method introduces ReasonEdit, a novel editor that allows users to incorporate human reasoning during edits by storing reasoning in a codebook and retrieving relevant facts during inference via a topology-balanced multimodal embedding technique inspired by network science. Experimental results on multiple rationale-based visual question answering datasets across four VLMs demonstrate state-of-the-art editing performance, showing that integrating human reasoning significantly enhances edit generalization.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要纠正视觉语言模型在推理密集型任务中的错误，而现有模型编辑方法未能融入人类推理过程。所提出的方法ReasonEdit引入了一种新颖的编辑设置，允许用户提供推理解释，持续将这些信息存储在码本中，并利用受网络科学启发的拓扑平衡多模态嵌入技术在推理时检索相关事实。在四个视觉语言模型和多个基于推理的视觉问答数据集上的实验结果表明，该方法实现了最先进的编辑性能，证明融入人类推理能显著提升编辑的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization</div>
<div class="meta-line">Authors: Zhenpeng Huang, Jiaqi Li, Zihan Jia, Xinhao Li, Desen Meng, Lingxue Song, Xi Chen, Liang Li, Limin Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-02-02T17:03:37+00:00 · Latest: 2026-02-02T17:03:37+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02341v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02341v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model&#x27;s scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model&#x27;s preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LongVPO：从锚定线索到自推理的长视频偏好优化</div>
<div class="mono" style="margin-top:8px">我们提出LongVPO，一种新颖的两阶段直接偏好优化框架，使短上下文视觉语言模型能够无需任何长视频标注即可稳健理解超长视频。第一阶段，我们通过将问题锚定至独立短视频片段、穿插干扰项，并应用视觉相似性与问题特异性过滤来合成偏好三元组，以缓解位置偏差并确保明确的监督。同时，我们仅通过评估锚定片段来近似参考模型在长上下文中的评分，从而降低计算开销。第二阶段，我们在长视频上采用递归描述管道生成场景级元数据，随后利用大语言模型构建多片段推理查询与负向响应，通过多片段推理任务对齐模型偏好。仅使用16K合成样本且无需昂贵人工标注，LongVPO在多个长视频基准测试中超越当前最优开源模型，同时保持强大的短视频性能（如在MVBench上），为高效长视频理解提供了可扩展的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling short-context vision-language models to understand ultra-long videos without requiring expensive long-video annotations. The proposed LongVPO method introduces a two-stage Direct Preference Optimization framework: first, it synthesizes preference triples by anchoring questions to individual short clips and applying filtering to mitigate positional bias, while approximating reference model scoring for efficiency; second, it employs a recursive captioning pipeline and a large language model to generate multi-segment reasoning queries for preference alignment. Experimental results show that with only 16K synthetic examples and no human labels, the model outperforms state-of-the-art open-source models on multiple long-video benchmarks while maintaining strong performance on short-video tasks like MVBench.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决如何让短上下文视觉语言模型理解超长视频，同时避免使用昂贵的长视频标注数据。提出的LongVPO方法采用两阶段直接偏好优化框架：第一阶段通过将问题锚定到单个短视频片段并应用过滤来合成偏好三元组以减轻偏差；第二阶段使用递归描述管道和大语言模型生成多片段推理查询以实现偏好对齐。实验结果表明，仅使用16K个合成样本且无需人工标注，该模型在多个长视频基准测试中超越了最先进的开源模型，同时在MVBench等短视频任务上保持了强劲性能。</div>
</details>
</div>
<div class="card">
<div class="title">U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding</div>
<div class="meta-line">Authors: Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo</div>
<div class="meta-line">First: 2025-05-23T11:48:48+00:00 · Latest: 2026-02-02T13:10:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17779v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.17779v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U2-BENCH：大型视觉语言模型在超声理解领域的基准测试</div>
<div class="mono" style="margin-top:8px">超声是一种广泛使用的成像方式，对全球医疗保健至关重要，但由于其图像质量受操作者、噪声和解剖结构影响而存在差异，解读仍具挑战性。尽管大型视觉语言模型（LVLMs）在自然和医学领域展现出卓越的多模态能力，但其在超声领域的性能尚未得到充分探索。我们推出首个综合性基准测试U2-BENCH，用于评估LVLMs在分类、检测、回归和文本生成等超声理解任务中的表现。该基准整合了涵盖15个解剖区域的7,241例病例，在50个超声应用场景中定义了8项临床启发任务，如诊断、切面识别、病灶定位、临床价值评估和报告生成。我们评估了23个开源与闭源、通用与医学专用的前沿LVLMs。结果显示，模型在图像级分类任务中表现优异，但在空间推理和临床语言生成方面仍存在持续挑战。U2-BENCH为医学超声成像这一独特多模态领域的LVLM研究建立了严谨统一的测试平台，以推动相关研究发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the widespread clinical use of ultrasound and its interpretation challenges due to operator-dependent image quality and noise, coupled with the unexplored performance of large vision-language models (LVLMs) in this domain. The method introduces U2-BENCH, a comprehensive benchmark comprising 7,241 ultrasound cases across 15 anatomical regions and 50 application scenarios, designed to evaluate LVLMs on eight clinically inspired tasks including classification, detection, regression, and text generation. Experimental evaluation of 23 state-of-the-art LVLMs reveals that while these models perform strongly on image-level classification tasks, they exhibit persistent difficulties in spatial reasoning and generating clinically accurate language.</div>
<div class="mono" style="margin-top:8px">本研究动机源于超声在临床中的广泛应用及其因操作者依赖的图像质量和噪声带来的解读挑战，同时大型视觉-语言模型在该领域的性能尚未得到充分探索。方法上提出了U2-BENCH，这是一个包含7,241个超声病例、涵盖15个解剖区域和50个应用场景的综合基准，旨在通过八个临床启发任务（如分类、检测、回归和文本生成）评估模型。通过对23个先进的大型视觉-语言模型进行评估，主要实验结果表明，模型在图像级分类任务上表现强劲，但在空间推理和生成临床准确语言方面仍存在持续困难，从而为推进医学超声领域的模型研究建立了一个严格的测试平台。</div>
</details>
</div>
<div class="card">
<div class="title">SciTextures: Collecting and Connecting Visual Patterns, Models, and Code Across Science and Art</div>
<div class="meta-line">Authors: Sagi Eppel, Alona Strugatski</div>
<div class="meta-line">First: 2025-11-03T18:22:11+00:00 · Latest: 2026-02-02T13:00:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.01817v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.01817v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to connect visual patterns with the processes that form them represents one of the deepest forms of visual understanding. Textures of clouds and waves, the growth of cities and forests, or the formation of materials and landscapes are all examples of patterns emerging from underlying mechanisms. We present the SciTextures dataset, a large-scale collection of textures and visual patterns from all domains of science, tech, and art, along with the models and code that generate these images. Covering over 1,270 different models and 100,000 images of patterns and textures from physics, chemistry, biology, sociology, technology, mathematics, and art, this dataset offers a way to explore the deep connection between the visual patterns that shape our world and the mechanisms that produce them. Built through an agentic AI pipeline that autonomously collects, implements, and standardizes scientific and generative models. This AI pipeline is also used to autonomously invent and implement novel methods for generating visual patterns and textures. SciTextures enables systematic evaluation of vision language models (VLM&#x27;s) ability to link visual patterns to the models and code that generate them, and to identify different patterns that emerge from the same underlying process. We also test VLMs ability to infer and recreate the mechanisms behind visual patterns by providing a natural image of a real-world phenomenon and asking the AI to identify and code a model of the process that formed it, then run this code to generate a simulated image that is compared to the reference image. These benchmarks reveal that VLM&#x27;s can understand and simulate physical systems beyond visual patterns at multiple levels of abstraction. The dataset and code are available at: https://zenodo.org/records/17485502</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SciTextures：跨科学与艺术领域视觉模式、模型与代码的收集与关联</div>
<div class="mono" style="margin-top:8px">将视觉模式与其形成过程相连接的能力，代表了视觉理解中最深刻的层次之一。云层与波浪的纹理、城市与森林的生长、材料与地貌的形成，都是底层机制涌现出模式的例证。我们推出SciTextures数据集，这是一个大规模收集自科学、技术与艺术各领域的纹理与视觉模式库，同时包含生成这些图像的模型与代码。该数据集涵盖物理学、化学、生物学、社会学、技术、数学及艺术领域的1,270余种不同模型和10万张模式与纹理图像，为探索塑造世界的视觉模式与其生成机制之间的深层关联提供了途径。数据集通过自主收集、实现并标准化科学与生成模型的智能AI流程构建而成，该流程亦用于自主发明并实现生成视觉模式与纹理的新方法。SciTextures支持系统评估视觉语言模型（VLM）在以下方面的能力：将视觉模式关联至生成它们的模型与代码，以及识别源自同一底层过程的不同模式。我们还测试VLM通过自然图像推断并重建视觉模式背后机制的能力：给定真实世界现象的自然图像，要求AI识别并编写形成该过程的模型代码，随后运行代码生成模拟图像以与原图对比。这些基准测试表明，VLM能在多个抽象层次上理解并模拟超越视觉模式的物理系统。数据集与代码发布于：https://zenodo.org/records/17485502</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand the deep connection between visual patterns and the underlying generative processes across scientific and artistic domains. The authors introduce the SciTextures dataset, which comprises over 1,270 models and 100,000 images from fields like physics, biology, and art, collected and standardized via an agentic AI pipeline that also autonomously invents new generation methods. Experimental benchmarks demonstrate that vision-language models can effectively link visual patterns to their generating code, identify patterns from shared processes, and even infer and simulate the mechanisms behind real-world phenomena by coding and comparing generated images to reference images.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过连接科学和艺术领域中的视觉模式与其生成过程，以深化视觉理解。作者提出了SciTextures数据集，包含超过1,270个模型和10万张图像，这些数据通过一个智能AI流程自动收集、标准化，并能自主发明新的生成方法。实验基准测试表明，视觉语言模型能够有效将模式与其底层模型和代码关联，从真实世界图像中推断生成机制，并在多个抽象层次上模拟物理系统。</div>
</details>
</div>
<div class="card">
<div class="title">Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models</div>
<div class="meta-line">Authors: Cristian Sbrolli, Matteo Matteucci, Toshihiko Yamasaki</div>
<div class="meta-line">First: 2026-02-02T12:39:39+00:00 · Latest: 2026-02-02T12:39:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02043v1">PDF</a> · <a href="https://huggingface.co/AutoComp">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) exhibit a critical flaw in compositional reasoning, often confusing &quot;a red cube and a blue sphere&quot; with &quot;a blue cube and a red sphere&quot;. Disentangling the visual and linguistic roots of these failures is a fundamental challenge for robust evaluation. To enable fine-grained, controllable analysis, we introduce Auto-Comp, a fully automated and synthetic pipeline for generating scalable benchmarks. Its controllable nature is key to dissecting and isolating different reasoning skills. Auto-Comp generates paired images from Minimal (e.g., &quot;a monitor to the left of a bicycle on a white background&quot;) and LLM-generated Contextual captions (e.g., &quot;In a brightly lit photography studio, a monitor is positioned to the left of a bicycle&quot;), allowing a controlled A/B test to disentangle core binding ability from visio-linguistic complexity. Our evaluation of 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures in both CLIP and SigLIP model families. Crucially, our novel &quot;Confusion Benchmark&quot; reveals a deeper flaw beyond simple attribute swaps: models are highly susceptible to low-entropy distractors (e.g., repeated objects or colors), demonstrating their compositional failures extend beyond known bag-of-words limitations. we uncover a surprising trade-off: visio-linguistic context, which provides global scene cues, aids spatial reasoning but simultaneously hinders local attribute binding by introducing visual clutter. We release the Auto-Comp pipeline to facilitate future benchmark creation, alongside all our generated benchmarks (https://huggingface.co/AutoComp).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Auto-Comp：一种用于可扩展组合性探测对比视觉语言模型的自动化流程</div>
<div class="mono" style="margin-top:8px">现代视觉语言模型在组合推理中存在关键缺陷，常混淆“红色立方体和蓝色球体”与“蓝色立方体和红色球体”。厘清这些失效的视觉与语言根源是鲁棒性评估的基础挑战。为实现细粒度可控分析，我们提出Auto-Comp——一个全自动合成流程，用于生成可扩展基准。其可控特性是解构与隔离不同推理能力的关键。Auto-Comp通过最小化描述（如“白色背景上自行车左侧的显示器”）与LLM生成的上下文描述（如“在明亮摄影棚中，显示器置于自行车左侧”）生成配对图像，通过受控A/B测试分离核心绑定能力与视觉语言复杂性。我们在颜色绑定和空间关系新基准上评估20个VLM，发现CLIP和SigLIP模型系列普遍存在组合性缺陷。关键的是，新颖的“混淆基准”揭示了超越简单属性交换的深层缺陷：模型极易受低熵干扰项（如重复物体或颜色）影响，表明其组合性失效超出已知词袋模型局限。我们揭示了一个惊人权衡：提供全局场景线索的视觉语言上下文虽有助于空间推理，却会因引入视觉干扰而阻碍局部属性绑定。我们开源Auto-Comp流程以促进未来基准构建，并发布所有生成基准（https://huggingface.co/AutoComp）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To systematically evaluate and dissect the compositional reasoning failures in Vision-Language Models (VLMs), which often confuse attributes like color and shape, this work introduces Auto-Comp, an automated synthetic pipeline for generating scalable and controllable benchmarks. The method generates paired images from Minimal and LLM-generated Contextual captions, enabling controlled A/B tests to isolate core binding ability from visio-linguistic complexity. Evaluating 20 VLMs on novel benchmarks for color binding and spatial relations reveals universal compositional failures across CLIP and SigLIP families, with a novel &quot;Confusion Benchmark&quot; showing models are highly susceptible to low-entropy distractors like repeated objects, indicating failures beyond known bag-of-words limitations; the study also uncovers a trade-off where contextual scene cues aid spatial reasoning but hinder local attribute binding due to visual clutter.</div>
<div class="mono" style="margin-top:8px">为系统评估和剖析视觉-语言模型在组合推理上的缺陷（例如经常混淆颜色和形状等属性），本研究提出了Auto-Comp，一个用于生成可扩展、可控基准的自动化合成流程。该方法通过从极简描述和LLM生成的上下文描述生成配对图像，实现受控的A/B测试，以分离核心绑定能力与视觉-语言复杂性。在颜色绑定和空间关系的新基准上评估20个VLM发现，CLIP和SigLIP模型家族普遍存在组合推理失败；新颖的“混淆基准”进一步表明模型极易受到低熵干扰物（如重复物体）的影响，这揭示了缺陷超越了简单的属性替换。一个关键发现是存在一种权衡：上下文场景线索有助于空间推理，但同时会因视觉杂乱而阻碍局部属性绑定。</div>
</details>
</div>
<div class="card">
<div class="title">VL-JEPA: Joint Embedding Predictive Architecture for Vision-language</div>
<div class="meta-line">Authors: Delong Chen, Mustafa Shukor, Theo Moutakanni, Willy Chung, Jade Yu, Tejaswi Kasarla, Yejin Bang, Allen Bolourchi, Yann LeCun, Pascale Fung</div>
<div class="meta-line">First: 2025-12-11T18:59:22+00:00 · Latest: 2026-02-02T12:38:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10942v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10942v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce VL-JEPA, a vision-language model built on a Joint Embedding Predictive Architecture (JEPA). Instead of autoregressively generating tokens as in classical VLMs, VL-JEPA predicts continuous embeddings of the target texts. By learning in an abstract representation space, the model focuses on task-relevant semantics while abstracting away surface-level linguistic variability. In a strictly controlled comparison against standard token-space VLM training with the same vision encoder and training data, VL-JEPA achieves stronger performance while having 50% fewer trainable parameters. At inference time, a lightweight text decoder is invoked only when needed to translate VL-JEPA predicted embeddings into text. We show that VL-JEPA natively supports selective decoding that reduces the number of decoding operations by 2.85x while maintaining similar performance compared to non-adaptive uniform decoding. Beyond generation, the VL-JEPA&#x27;s embedding space naturally supports open-vocabulary classification, text-to-video retrieval, and discriminative VQA without any architecture modification. On eight video classification and eight video retrieval datasets, the average performance VL-JEPA surpasses that of CLIP, SigLIP2, and Perception Encoder. At the same time, the model achieves comparable performance as classical VLMs (InstructBLIP, QwenVL) on four VQA datasets: GQA, TallyQA, POPE and POPEv2, despite only having 1.6B parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VL-JEPA：视觉-语言联合嵌入预测架构</div>
<div class="mono" style="margin-top:8px">本文提出VL-JEPA，一种基于联合嵌入预测架构（JEPA）的视觉-语言模型。与传统视觉语言模型的自回归词元生成方式不同，VL-JEPA直接预测目标文本的连续嵌入表示。通过在抽象表示空间中学习，该模型聚焦于任务相关语义，同时剥离表层语言变异。在严格控制变量（使用相同视觉编码器与训练数据）的对比实验中，VL-JEPA以可训练参数量减少50%的条件实现了更优性能。推理阶段仅需按需调用轻量级文本解码器，将预测嵌入转换为文本。实验表明VL-JEPA原生支持选择性解码机制，在保持性能相当的前提下将解码操作量减少至非自适应均匀解码的1/2.85。除生成任务外，VL-JEPA的嵌入空间无需架构修改即可直接支持开放词汇分类、文本-视频检索及判别式视觉问答任务。在八个视频分类与八个视频检索数据集上，VL-JEPA平均性能超越CLIP、SigLIP2及Perception Encoder；同时在GQA、TallyQA、POPE和POPEv2四个视觉问答数据集上，仅以16亿参数即达到与经典视觉语言模型（InstructBLIP、QwenVL）相当的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for more efficient vision-language models that focus on semantic understanding rather than surface-level token generation, this paper introduces VL-JEPA, a model based on a Joint Embedding Predictive Architecture. The method predicts continuous embeddings of target texts in an abstract representation space, using a shared vision encoder and a lightweight text decoder invoked only when needed for text generation. Key experimental results show that VL-JEPA achieves stronger performance with 50% fewer trainable parameters compared to standard token-space VLM training, supports selective decoding that reduces decoding operations by 2.85x while maintaining similar performance, and surpasses models like CLIP and SigLIP2 on video classification and retrieval datasets while achieving comparable results on VQA benchmarks despite having only 1.6B parameters.</div>
<div class="mono" style="margin-top:8px">为了构建更高效的视觉-语言模型，使其专注于高层语义而非表层标记生成，本文提出了基于联合嵌入预测架构的VL-JEPA模型。该方法在抽象表示空间中预测目标文本的连续嵌入，使用共享的视觉编码器，并仅在需要文本生成时调用轻量级文本解码器。关键实验结果表明，VL-JEPA在可训练参数减少50%的情况下，性能优于标准的标记空间VLM训练；通过选择性解码将解码操作减少了2.85倍；在视频分类和检索任务上超越了CLIP和SigLIP2等模型，同时在VQA基准测试中与更大规模的VLM取得了相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Genomic Modeling Through Optical Character Recognition</div>
<div class="meta-line">Authors: Hongxin Xiang, Pengsen Ma, Yunkang Cao, Di Yu, Haowen Chen, Xinyu Yang, Xiangxiang Zeng</div>
<div class="meta-line">First: 2026-02-02T12:12:00+00:00 · Latest: 2026-02-02T12:12:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent genomic foundation models largely adopt large language model architectures that treat DNA as a one-dimensional token sequence. However, exhaustive sequential reading is structurally misaligned with sparse and discontinuous genomic semantics, leading to wasted computation on low-information background and preventing understanding-driven compression for long contexts. Here, we present OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding. OpticalDNA renders DNA into structured visual layouts and trains an OCR-capable vision--language model with a \emph{visual DNA encoder} and a \emph{document decoder}, where the encoder produces compact, reconstructible visual tokens for high-fidelity compression. Building on this representation, OpticalDNA defines prompt-conditioned objectives over core genomic primitives-reading, region grounding, subsequence retrieval, and masked span completion-thereby learning layout-aware DNA representations that retain fine-grained genomic information under a reduced effective token budget. Across diverse genomic benchmarks, OpticalDNA consistently outperforms recent baselines; on sequences up to 450k bases, it achieves the best overall performance with nearly $20\times$ fewer effective tokens, and surpasses models with up to $985\times$ more activated parameters while tuning only 256k \emph{trainable} parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过光学字符识别重新思考基因组建模</div>
<div class="mono" style="margin-top:8px">当前的基因组基础模型大多采用大型语言模型架构，将DNA视为一维标记序列。然而，详尽的顺序读取在结构上与稀疏且不连续的基因组语义不匹配，导致在低信息背景上浪费计算资源，并阻碍了针对长上下文的基于理解的压缩。本文提出OpticalDNA，一种基于视觉的框架，将基因组建模重新定义为光学字符识别（OCR）式的文档理解。OpticalDNA将DNA渲染为结构化视觉布局，并通过一个\emph{视觉DNA编码器}和一个\emph{文档解码器}训练具备OCR能力的视觉-语言模型，其中编码器生成紧凑、可重构的视觉标记以实现高保真压缩。基于此表示，OpticalDNA针对核心基因组原语——读取、区域定位、子序列检索和掩码片段补全——定义了提示条件目标，从而学习具有布局感知的DNA表示，在减少有效标记预算的同时保留细粒度基因组信息。在多种基因组基准测试中，OpticalDNA始终优于近期基线；在长达45万个碱基的序列上，它以近$20\times$更少的有效标记实现了最佳整体性能，并且超越了激活参数多达$985\times$的模型，而仅需微调256k个\emph{可训练}参数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work arises from the structural misalignment between current genomic foundation models, which treat DNA as a one-dimensional token sequence, and the sparse, discontinuous nature of genomic semantics, leading to computational inefficiency. The method introduces OpticalDNA, a vision-based framework that reframes genomic modeling as Optical Character Recognition (OCR)-style document understanding by rendering DNA into structured visual layouts and training a model with a visual DNA encoder and a document decoder to produce compact, reconstructible visual tokens. Key experimental results show that OpticalDNA consistently outperforms recent baselines across diverse genomic benchmarks; on sequences up to 450k bases, it achieves the best overall performance with nearly 20× fewer effective tokens and surpasses models with up to 985× more activated parameters while tuning only 256k trainable parameters.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于当前将DNA视为一维标记序列的基因组基础模型与基因组语义的稀疏、不连续特性之间的结构错位，这导致了计算效率低下。方法上提出了OpticalDNA，这是一个基于视觉的框架，通过将DNA渲染为结构化视觉布局，并训练一个包含视觉DNA编码器和文档解码器的模型来生成紧凑、可重构的视觉标记，从而将基因组建模重新定义为光学字符识别（OCR）式的文档理解。主要实验结果表明，OpticalDNA在多种基因组基准测试中持续优于近期基线；在长达45万个碱基的序列上，它以近20倍更少的有效标记实现了最佳整体性能，并且以仅25.6万个可训练参数，超越了激活参数多达985倍的模型。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Multi-Image Understanding through Delimiter Token Scaling</div>
<div class="meta-line">Authors: Minyoung Lee, Yeji Park, Dongjun Hwang, Yejin Kim, Seong Joon Oh, Junsuk Choe</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-02T11:38:01+00:00 · Latest: 2026-02-02T11:38:01+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01984v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01984v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) achieve strong performance on single-image tasks, but their performance declines when multiple images are provided as input. One major reason is the cross-image information leakage, where the model struggles to distinguish information across different images. Existing LVLMs already employ delimiter tokens to mark the start and end of each image, yet our analysis reveals that these tokens fail to effectively block cross-image information leakage. To enhance their effectiveness, we propose a method that scales the hidden states of delimiter tokens. This enhances the model&#x27;s ability to preserve image-specific information by reinforcing intra-image interaction and limiting undesired cross-image interactions. Consequently, the model is better able to distinguish between images and reason over them more accurately. Experiments show performance gains on multi-image benchmarks such as Mantis, MuirBench, MIRB, and QBench2. We further evaluate our method on text-only tasks that require clear distinction. The method improves performance on multi-document and multi-table understanding benchmarks, including TQABench, MultiNews, and WCEP-10. Notably, our method requires no additional training or inference cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过分隔符标记缩放增强多图像理解能力</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型在单图像任务上表现优异，但在处理多图像输入时性能下降。主要原因是跨图像信息泄露问题，即模型难以区分不同图像间的信息。现有模型虽已采用分隔符标记来标识各图像起止位置，但分析表明这些标记未能有效阻断跨图像信息泄露。为此，我们提出通过缩放分隔符标记的隐藏状态来增强其有效性。该方法通过强化图像内部交互并限制非预期的跨图像交互，提升模型保留图像特定信息的能力，从而更准确地区分和推理多图像内容。实验表明，该方法在Mantis、MuirBench、MIRB和QBench2等多图像基准测试中取得性能提升。在需要清晰区分的纯文本任务上，该方法同样提升了TQABench、MultiNews和WCEP-10等多文档/多表格理解基准的性能。值得注意的是，本方法无需额外训练或推理成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Vision-Language Models (LVLMs) perform well on single-image tasks but degrade with multiple images due to cross-image information leakage, where models fail to distinguish information across images. To address this, the authors propose scaling the hidden states of the delimiter tokens that mark image boundaries, which reinforces intra-image interactions and suppresses unwanted cross-image interactions without requiring additional training or inference cost. Experiments demonstrate consistent performance improvements on multi-image benchmarks like Mantis, MuirBench, MIRB, and QBench2, as well as on text-only multi-document and multi-table understanding tasks including TQABench, MultiNews, and WCEP-10.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型视觉语言模型在多图像任务中因跨图像信息泄露而导致的性能下降问题，现有分隔符标记未能有效区分不同图像的信息。为此，提出了一种通过缩放分隔符标记的隐藏状态来增强其效果的方法，该方法通过加强图像内部交互并限制不必要的跨图像交互，从而提升模型保留图像特定信息的能力。实验结果表明，该方法在Mantis、MuirBench、MIRB和QBench2等多图像基准测试上取得了性能提升，同时在TQABench、MultiNews和WCEP-10等多文档和多表格理解任务上也表现出改进，且无需额外的训练或推理成本。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Faithful Reasoning in Remote Sensing: A Perceptually-Grounded GeoSpatial Chain-of-Thought for Vision-Language Models</div>
<div class="meta-line">Authors: Jiaqi Liu, Lang Sun, Ronghao Fu, Bo Yang</div>
<div class="meta-line">First: 2025-09-26T11:34:42+00:00 · Latest: 2026-02-02T10:01:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22221v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22221v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) in remote sensing often fail at complex analytical tasks, a limitation stemming from their end-to-end training paradigm that bypasses crucial reasoning steps and leads to unverifiable outputs. To address this limitation, we introduce the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT), a framework that models remote sensing analysis as a verifiable, multi-step process. We instill this analytical process through a two-stage alignment strategy, leveraging Geo-CoT380k, the first large-scale dataset of structured Geo-CoT rationales. This strategy first employs supervised fine-tuning (SFT) to instill the foundational cognitive architecture, then leverages Group Reward Policy Optimization (GRPO) to refine the model&#x27;s reasoning policy towards factual correctness. The resulting model, RSThinker, outputs both a final answer and its justifying, verifiable analytical trace. This capability yields dominant performance, significantly outperforming state-of-the-art models across a comprehensive range of tasks. The public release of our Geo-CoT380k dataset and RSThinker model upon publication serves as a concrete pathway from opaque perception towards structured, verifiable reasoning for Earth Observation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向遥感领域的可信推理：面向视觉语言模型的感知基础地理空间思维链</div>
<div class="mono" style="margin-top:8px">遥感领域的视觉语言模型在复杂分析任务中常存在不足，这源于其端到端训练范式绕过了关键推理步骤，导致输出结果难以验证。为解决这一局限，我们提出了感知基础地理空间思维链框架，将遥感分析建模为可验证的多步骤过程。通过两阶段对齐策略，并借助首个大规模结构化Geo-CoT推理数据集Geo-CoT380k，我们构建了这一分析流程。该策略首先采用监督微调建立基础认知架构，继而通过群体奖励策略优化技术精炼模型的推理策略以提升事实准确性。最终模型RSThinker可同步输出最终答案及其可验证的分析轨迹，在广泛任务中显著超越现有最优模型。我们公开发布的Geo-CoT380k数据集与RSThinker模型，为地球观测从隐式感知迈向结构化可验证推理提供了切实路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models for remote sensing struggle with complex analytical tasks due to their end-to-end training, which skips reasoning steps and produces unverifiable results. To enable verifiable multi-step analysis, this work introduces the Perceptually-Grounded Geospatial Chain-of-Thought (Geo-CoT) framework, implemented via a two-stage alignment strategy using a new large-scale dataset, Geo-CoT380k, for structured rationales. The method involves supervised fine-tuning to establish a cognitive architecture followed by Group Reward Policy Optimization to refine reasoning for factual correctness, resulting in the RSThinker model that outputs both answers and justifications. Experiments show that RSThinker significantly outperforms state-of-the-art models across a comprehensive suite of tasks, demonstrating a concrete advance towards structured and verifiable reasoning in Earth Observation.</div>
<div class="mono" style="margin-top:8px">遥感领域的视觉语言模型由于端到端训练忽略了显式推理步骤，在处理复杂分析任务时存在局限，导致输出结果难以验证。为实现可验证的多步骤分析，本研究提出了感知基础的地理空间思维链框架，通过利用新构建的大规模结构化原理数据集Geo-CoT380k，采用两阶段对齐策略进行实现；该方法首先通过监督微调建立基础认知架构，随后利用群体奖励策略优化来细化推理过程以提高事实准确性。最终得到的RSThinker模型能够同时输出最终答案及其可验证的分析轨迹，在广泛的任务中取得了显著优于现有先进模型的性能优势。</div>
</details>
</div>
<div class="card">
<div class="title">AVERY: Adaptive VLM Split Computing through Embodied Self-Awareness for Efficient Disaster Response Systems</div>
<div class="meta-line">Authors: Rajat Bhattacharjya, Sing-Yao Wu, Hyunwoo Oh, Chaewon Nam, Suyeon Koo, Mohsen Imani, Elaheh Bozorgzadeh, Nikil Dutt</div>
<div class="meta-line">First: 2025-11-22T18:42:04+00:00 · Latest: 2026-02-02T09:53:38+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures. Paper is currently under review. Authors&#x27; version posted for personal use and not for redistribution</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18151v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18151v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unmanned Aerial Vehicles (UAVs) in disaster response require complex, queryable intelligence that on-board CNNs cannot provide. While Vision-Language Models (VLMs) offer this semantic reasoning, their high resource demands make on-device deployment infeasible, and naive cloud offloading fails under the low-bandwidth networks common in disaster zones. We present AVERY, a framework that enables VLM deployment through adaptive split computing. We advance the split computing paradigm beyond traditional depth-wise partitioning by introducing a functional, cognitive-inspired dual-stream split that separates the VLM into a high-frequency, low-resolution &quot;context stream&quot; for real-time awareness and a low-frequency, high-fidelity &quot;insight stream&quot; for deep analysis. A lightweight, self-aware on-board controller manages this architecture, monitoring network conditions and operator intent to dynamically select from pre-trained compression models, navigating the fundamental accuracy-throughput trade-off. Evaluated using the VLM LISA-7B across an edge-cloud scenario under fluctuating network conditions, AVERY consistently outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution, thereby enhancing mission efficiency and enabling real-time, queryable intelligence on resource-constrained platforms in dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AVERY：基于具身自适应的视觉语言模型分载计算框架——面向高效灾害响应系统</div>
<div class="mono" style="margin-top:8px">灾害响应中的无人机需要可查询的复杂智能分析能力，而机载卷积神经网络无法满足这一需求。虽然视觉语言模型具备语义推理能力，但其高资源需求使得设备端部署不可行，而灾害现场常见的低带宽网络又使简单的云端卸载方案失效。本文提出AVERY框架，通过自适应分载计算实现视觉语言模型的部署。我们突破传统按深度划分的分载计算范式，提出受认知启发的双流功能分割方案：将视觉语言模型分离为高频低分辨率的“情境流”用于实时感知，以及低频高保真的“洞察流”用于深度分析。轻量级自适应的机载控制器通过监测网络状态和操作者意图，动态选择预训练的压缩模型，在精度与吞吐量的根本性权衡中实现优化。基于LISA-7B视觉语言模型在边缘-云场景波动网络条件下的评估表明，AVERY始终优于静态配置：相比原始图像压缩方案精度提升11.2%，相比全边缘执行方案能耗降低93.98%，从而在动态环境中为资源受限平台提升任务效率，实现实时可查询的智能分析能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable unmanned aerial vehicles (UAVs) in disaster response to perform complex semantic reasoning under low-bandwidth conditions, this work introduces AVERY, a framework for adaptive split computing of vision-language models (VLMs). The method advances split computing by proposing a cognitive-inspired dual-stream architecture that separates the VLM into a high-frequency, low-resolution context stream for real-time awareness and a low-frequency, high-fidelity insight stream for deep analysis, managed by a lightweight on-board controller that dynamically selects compression models based on network conditions and operator intent. Experimental evaluation using the LISA-7B VLM in an edge-cloud scenario shows that AVERY outperforms static configurations, achieving 11.2% higher accuracy than raw image compression and 93.98% lower energy consumption compared to full-edge execution.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在灾难响应中，将资源密集型的视觉语言模型部署到无人机上的挑战，因为低带宽网络阻碍了云端卸载，而设备端执行又不可行。提出的AVERY框架采用了一种自适应分割计算方法，其核心是一个受认知启发的双流架构，将视觉语言模型分离为实时上下文流和深度分析洞察流，并由一个轻量级机载控制器管理，该控制器根据网络条件和操作员意图动态选择压缩模型。在边缘-云场景中使用LISA-7B模型进行的实验评估表明，AVERY优于静态配置，相比原始图像压缩实现了11.2%的准确率提升，相比全边缘执行降低了93.98%的能耗。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Cross-Country Data Acquisition Strategy for ADAS via Street-View Imagery</div>
<div class="meta-line">Authors: Yin Wu, Daniel Slieter, Carl Esselborn, Ahmed Abouelazm, Tsung Yuan Tseng, J. Marius Zöllner</div>
<div class="meta-line">First: 2026-02-02T09:09:07+00:00 · Latest: 2026-02-02T09:09:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01836v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01836v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying ADAS and ADS across countries remains challenging due to differences in legislation, traffic infrastructure, and visual conventions, which introduce domain shifts that degrade perception performance. Traditional cross-country data collection relies on extensive on-road driving, making it costly and inefficient to identify representative locations. To address this, we propose a street-view-guided data acquisition strategy that leverages publicly available imagery to identify places of interest (POI). Two POI scoring methods are introduced: a KNN-based feature distance approach using a vision foundation model, and a visual-attribution approach using a vision-language model. To enable repeatable evaluation, we adopt a collect-detect protocol and construct a co-located dataset by pairing the Zenseact Open Dataset with Mapillary street-view images. Experiments on traffic sign detection, a task particularly sensitive to cross-country variations in sign appearance, show that our approach achieves performance comparable to random sampling while using only half of the target-domain data. We further provide cost estimations for full-country analysis, demonstrating that large-scale street-view processing remains economically feasible. These results highlight the potential of street-view-guided data acquisition for efficient and cost-effective cross-country model adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于街景图像的高效跨国ADAS数据采集策略</div>
<div class="mono" style="margin-top:8px">由于法规、交通基础设施及视觉习惯的差异导致域偏移并降低感知性能，跨国部署ADAS与ADS仍面临挑战。传统跨国数据采集依赖大规模道路驾驶，成本高昂且难以高效定位代表性场景。为此，本文提出一种街景引导的数据采集策略，利用公开图像识别兴趣点（POI）。引入两种POI评分方法：基于视觉基础模型的KNN特征距离法，以及基于视觉-语言模型的视觉归因法。为支持可重复评估，采用“采集-检测”协议，通过配对Zenseact开放数据集与Mapillary街景图像构建共位数据集。在交通标志检测任务（对跨国标志外观差异尤为敏感）上的实验表明，本方法仅需半数目标域数据即可达到与随机采样相当的性能。进一步提供了全国范围分析的成本估算，证明大规模街景处理仍具经济可行性。这些结果凸显了街景引导数据采集策略在高效、低成本跨国模型适配中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of deploying Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS) across different countries, where variations in traffic infrastructure and visual conventions cause domain shifts that degrade perception performance. The proposed method introduces a street-view-guided data acquisition strategy that uses publicly available imagery to identify places of interest (POI) for targeted data collection, employing two scoring approaches: a KNN-based feature distance method using a vision foundation model and a visual-attribution method using a vision-language model. Experimental evaluation on traffic sign detection, using a co-located dataset pairing the Zenseact Open Dataset with Mapillary street-view images, demonstrates that the strategy achieves performance comparable to random sampling while requiring only 50% of the target-domain data, with cost analysis confirming the economic feasibility of large-scale street-view processing for cross-country model adaptation.</div>
<div class="mono" style="margin-top:8px">由于各国交通法规、基础设施和视觉惯例的差异导致域偏移，从而降低感知性能，使得高级驾驶辅助系统（ADAS）和自动驾驶系统（ADS）的跨国部署面临挑战。为解决传统跨国数据收集依赖大规模道路驾驶、成本高且效率低的问题，本研究提出一种利用公开街景图像识别兴趣点（POI）的引导式数据采集策略。该方法引入了两种POI评分方法：基于视觉基础模型的KNN特征距离方法和基于视觉-语言模型的视觉归因方法。通过在交通标志检测任务上的实验（使用配对构建的协同定位数据集进行评估），该策略仅使用50%的目标域数据即可达到与随机采样相当的性能，且大规模街景处理的成本分析证明了其经济可行性，为高效的跨国模型适应提供了潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Physics-Based Benchmarking Metrics for Multimodal Synthetic Images</div>
<div class="meta-line">Authors: Kishor Datta Gupta, Marufa Kamal, Md. Mahfuzur Rahman, Fahad Rahman, Mohd Ariful Haque, Sunzida Siddique</div>
<div class="meta-line">First: 2025-11-19T07:52:20+00:00 · Latest: 2026-02-02T07:21:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.15204v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.15204v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current state of the art measures like BLEU, CIDEr, VQA score, SigLIP-2 and CLIPScore are often unable to capture semantic or structural accuracy, especially for domain-specific or context-dependent scenarios. For this, this paper proposes a Physics-Constrained Multimodal Data Evaluation (PCMDE) metric combining large language models with reasoning, knowledge based mapping and vision-language models to overcome these limitations. The architecture is comprised of three main stages: (1) feature extraction of spatial and semantic information with multimodal features through object detection and VLMs; (2) Confidence-Weighted Component Fusion for adaptive component-level validation; and (3) physics-guided reasoning using large language models for structural and relational constraints (e.g., alignment, position, consistency) enforcement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物理的多模态合成图像基准评测指标</div>
<div class="mono" style="margin-top:8px">当前先进指标如BLEU、CIDEr、VQA分数、SigLIP-2和CLIPScore常难以捕捉语义或结构准确性，尤其在领域特定或上下文依赖场景中。为此，本文提出一种物理约束多模态数据评估（PCMDE）指标，结合大语言模型与推理、基于知识的映射及视觉语言模型，以克服这些局限。该架构包含三个主要阶段：（1）通过目标检测和视觉语言模型提取空间与语义信息的多模态特征；（2）置信度加权组件融合，用于自适应组件级验证；（3）利用大语言模型进行物理引导推理，以强化结构及关系约束（如对齐、位置、一致性）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing metrics such as BLEU, CIDEr, and CLIPScore often fail to adequately assess the semantic and structural accuracy of multimodal synthetic images, particularly in domain-specific contexts. To address this, the paper introduces the Physics-Constrained Multimodal Data Evaluation (PCMDE) metric, which integrates large language models for reasoning, knowledge-based mapping, and vision-language models through a three-stage architecture: multimodal feature extraction via object detection and VLMs, adaptive confidence-weighted component fusion, and physics-guided reasoning to enforce structural and relational constraints. Experimental results demonstrate that PCMDE effectively captures semantic and structural details, overcoming the limitations of current state-of-the-art evaluation methods.</div>
<div class="mono" style="margin-top:8px">现有指标如BLEU、CIDEr和CLIPScore往往难以充分评估多模态合成图像的语义和结构准确性，尤其在领域特定场景中。为此，本文提出了物理约束多模态数据评估（PCMDE）指标，它通过结合大型语言模型的推理能力、基于知识的映射以及视觉语言模型，采用三阶段架构：通过目标检测和视觉语言模型进行多模态特征提取，自适应置信度加权组件融合，以及利用大型语言模型进行物理引导推理以强制执行结构和关系约束。实验结果表明，PCMDE能有效捕捉语义和结构细节，克服了当前最先进评估方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Simplicity Prevails: The Emergence of Generalizable AIGI Detection in Visual Foundation Models</div>
<div class="meta-line">Authors: Yue Zhou, Xinan He, Kaiqing Lin, Bing Fan, Feng Ding, Bin Li</div>
<div class="meta-line">First: 2026-02-02T07:20:02+00:00 · Latest: 2026-02-02T07:20:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01738v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01738v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While specialized detectors for AI-Generated Images (AIGI) achieve near-perfect accuracy on curated benchmarks, they suffer from a dramatic performance collapse in realistic, in-the-wild scenarios. In this work, we demonstrate that simplicity prevails over complex architectural designs. A simple linear classifier trained on the frozen features of modern Vision Foundation Models , including Perception Encoder, MetaCLIP 2, and DINOv3, establishes a new state-of-the-art. Through a comprehensive evaluation spanning traditional benchmarks, unseen generators, and challenging in-the-wild distributions, we show that this baseline not only matches specialized detectors on standard benchmarks but also decisively outperforms them on in-the-wild datasets, boosting accuracy by striking margins of over 30\%. We posit that this superior capability is an emergent property driven by the massive scale of pre-training data containing synthetic content. We trace the source of this capability to two distinct manifestations of data exposure: Vision-Language Models internalize an explicit semantic concept of forgery, while Self-Supervised Learning models implicitly acquire discriminative forensic features from the pretraining data. However, we also reveal persistent limitations: these models suffer from performance degradation under recapture and transmission, remain blind to VAE reconstruction and localized editing. We conclude by advocating for a paradigm shift in AI forensics, moving from overfitting on static benchmarks to harnessing the evolving world knowledge of foundation models for real-world reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>简约制胜：视觉基础模型中可泛化AIGI检测能力的涌现</div>
<div class="mono" style="margin-top:8px">尽管针对AI生成图像（AIGI）的专用检测器在精选基准测试中达到近乎完美的准确率，但在真实开放场景中却遭遇性能急剧崩溃。本研究表明，简约方法优于复杂架构设计：基于现代视觉基础模型（包括Perception Encoder、MetaCLIP 2和DINOv3）冻结特征训练的简单线性分类器，创造了新的性能标杆。通过涵盖传统基准、未知生成器和复杂开放分布的综合评估，该基线方法不仅在标准基准上媲美专用检测器，更在开放数据集上以超过30%的显著优势全面超越。我们认为这种卓越能力是由包含合成内容的海量预训练数据驱动的涌现特性，其能力来源可追溯至两种数据暴露形式：视觉语言模型内化了伪造的显式语义概念，而自监督学习模型则从预训练数据中隐式习得判别性取证特征。然而研究也揭示了持续存在的局限：这些模型在重捕获与传输场景下会出现性能衰减，且对VAE重建与局部编辑操作仍存在盲区。最后我们倡导AI取证领域的范式转变——从对静态基准的过拟合转向利用基础模型不断演进的世界知识，以实现现实场景的可靠检测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that specialized AI-Generated Image (AIGI) detectors, despite high benchmark performance, fail dramatically in real-world, in-the-wild scenarios. The method demonstrates that a simple linear classifier trained on frozen features from modern Vision Foundation Models (VFMs) like Perception Encoder, MetaCLIP 2, and DINOv3 establishes a new state-of-the-art. Key experimental findings show this baseline matches specialized detectors on standard benchmarks and decisively outperforms them on in-the-wild datasets by over 30% accuracy, with the capability emerging from VFMs&#x27; exposure to massive pre-training data containing synthetic content; however, limitations persist under recapture, transmission, VAE reconstruction, and localized editing.</div>
<div class="mono" style="margin-top:8px">针对AI生成图像（AIGI）的专用检测器在精心构建的基准测试中表现优异，但在真实场景中性能急剧下降。本研究证明，基于现代视觉基础模型（如Perception Encoder、MetaCLIP 2和DINOv3）的冻结特征训练的简单线性分类器，取得了新的最先进性能。综合评估表明，该基线方法在标准基准测试上与专用检测器相当，并在真实世界数据集上以超过30%的准确率优势显著超越它们，这种能力源于模型在包含合成内容的大规模预训练数据中暴露所产生的涌现特性。研究将这种能力归因于视觉语言模型内化的显式伪造语义概念，以及自监督学习模型从预训练数据中隐式获得的判别性取证特征，但也揭示了其在重捕获、传输、VAE重建和局部编辑等情况下性能下降的持续局限性，主张推动AI取证范式转变，利用基础模型不断演进的世界知识以实现现实世界的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs</div>
<div class="meta-line">Authors: Yanlong Chen, Amirhossein Habibian, Luca Benini, Yawei Li</div>
<div class="meta-line">First: 2026-01-30T08:30:52+00:00 · Latest: 2026-02-02T06:39:48+00:00</div>
<div class="meta-line">Comments: This paper is currently under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22709v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.22709v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) achieve strong multimodal performance but are costly to deploy, and post-training quantization often causes significant accuracy loss. Despite its potential, quantization-aware training for VLMs remains underexplored. We propose GRACE, a framework unifying knowledge distillation and QAT under the Information Bottleneck principle: quantization constrains information capacity while distillation guides what to preserve within this budget. Treating the teacher as a proxy for task-relevant information, we introduce confidence-gated decoupled distillation to filter unreliable supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive controller via Lagrangian relaxation to balance fidelity against capacity constraints. Across extensive benchmarks on LLaVA and Qwen families, our INT4 models consistently outperform FP16 baselines (e.g., LLaVA-1.5-7B: 70.1 vs. 66.8 on SQA; Qwen2-VL-2B: 76.9 vs. 72.6 on MMBench), nearly matching teacher performance. Using real INT4 kernel, we achieve 3$\times$ throughput with 54% memory reduction. This principled framework significantly outperforms existing quantization methods, making GRACE a compelling solution for resource-constrained deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于置信度蒸馏的门控关系对齐高效视觉语言模型</div>
<div class="mono" style="margin-top:8px">视觉语言模型在多模态任务中表现出色，但部署成本高昂，且后训练量化常导致精度显著下降。尽管量化感知训练潜力巨大，其在VLM领域的探索仍显不足。我们提出GRACE框架，基于信息瓶颈原理统一知识蒸馏与量化感知训练：量化约束信息容量，蒸馏则指导在此预算下保留关键信息。将教师模型视为任务相关信息的代理，我们引入置信门控解耦蒸馏以过滤不可靠监督、关系中心核对齐以传递视觉标记结构，以及通过拉格朗日松弛实现的自适应控制器来平衡保真度与容量约束。在LLaVA和Qwen系列的广泛基准测试中，我们的INT4模型持续超越FP16基线（例如LLaVA-1.5-7B在SQA上达70.1对66.8；Qwen2-VL-2B在MMBench上达76.9对72.6），几乎匹配教师模型性能。使用真实INT4内核实现3倍吞吐量提升与54%内存缩减。该原则性框架显著优于现有量化方法，使GRACE成为资源受限部署的理想解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high deployment cost and accuracy degradation from post-training quantization of Vision-Language Models (VLMs), this work proposes GRACE, a quantization-aware training framework that unifies knowledge distillation and quantization under the Information Bottleneck principle. The method employs confidence-gated decoupled distillation to filter unreliable teacher supervision, relational centered kernel alignment to transfer visual token structures, and an adaptive Lagrangian controller to balance information fidelity with capacity constraints. Experiments on LLaVA and Qwen model families show that the resulting INT4 models consistently outperform their FP16 baselines on benchmarks like SQA and MMBench, nearly matching the full-precision teacher&#x27;s accuracy while achieving 3x throughput and a 54% memory reduction with real INT4 kernels.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型后训练量化导致的显著精度损失以及量化感知训练研究不足的问题，本研究提出了GRACE框架，该框架基于信息瓶颈原理将知识蒸馏与量化感知训练统一起来。方法引入了基于置信度的门控解耦蒸馏以过滤不可靠的教师监督、关系中心核对齐以传递视觉令牌结构，以及通过拉格朗日松弛的自适应控制器来平衡信息保真度与容量约束。在LLaVA和Qwen系列模型上的实验结果表明，INT4量化模型在SQA和MMBench等基准测试上持续优于FP16基线，几乎达到全精度教师模型的性能，同时在使用真实INT4内核时实现了3倍吞吐量提升和54%的内存减少。</div>
</details>
</div>
<div class="card">
<div class="title">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</div>
<div class="meta-line">Authors: Chen Li, Han Zhang, Zhantao Yang, Fangyi Chen, Zihan Wang, Anudeepsekhar Bolimera, Marios Savvides</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-12T07:27:50+00:00 · Latest: 2026-02-02T05:52:55+00:00</div>
<div class="meta-line">Comments: This paper has been accepted at AAAI 2026. This is the author&#x27;s extended version. The final version will appear in the official proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08688v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.08688v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STELAR-VISION：面向视觉对齐推理的自拓扑感知高效学习框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型在推理任务上已取得显著进展，但在处理复杂多模态任务时仍面临挑战，且常生成冗余输出。其核心局限在于过度依赖链式思维推理，而许多任务实际受益于树状或图状等拓扑结构。为此，我们提出STELAR-Vision——一种拓扑感知推理训练框架。其核心是TopoAug合成数据管道，通过多样化拓扑结构增强训练数据。采用监督微调与强化学习相结合的方法，我们在保持准确率的同时对Qwen2VL模型进行高效后训练。此外，我们提出&#x27;节俭学习&#x27;方法，在精度损失最小化的前提下显著缩短输出长度。在MATH-V和VLM-S2H基准测试中，STELAR-Vision较基础模型准确率提升9.7%，并超越参数量更大的Qwen2VL-72B-Instruct模型7.3%。在五个分布外测试集上，其性能最高超越Phi-4-Multimodal-Instruct达28.4%，超越LLaMA-3.2-11B-Vision-Instruct达13.2%，展现出卓越的泛化能力。与纯链式训练相比，本方法在分布内数据集上整体准确率提升4.3%，并在所有分布外基准测试中保持稳定优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models often struggle with complex multimodal reasoning and generate verbose outputs due to their reliance on chain-of-thought reasoning, which is suboptimal for tasks better suited to alternative structures like trees or graphs. To address this, the authors introduce STELAR-Vision, a training framework that incorporates TopoAug, a synthetic data pipeline for generating diverse topological reasoning structures, and employs supervised fine-tuning and reinforcement learning to enhance both accuracy and efficiency, alongside a Frugal Learning technique to reduce output length. Experimental results show that STELAR-Vision improves accuracy by 9.7% over its base model and outperforms larger models like Qwen2VL-72B-Instruct by 7.3% on in-distribution benchmarks, while also demonstrating strong generalization by surpassing other models by up to 28.4% on out-of-distribution tasks.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在处理复杂多模态推理任务时，常因依赖链式思维推理而表现不佳并产生冗长输出，而许多任务更适合树状或图状等拓扑结构。为此，研究者提出了STELAR-Vision训练框架，其核心是TopoAug合成数据管道，用于生成多样化的拓扑推理结构，并通过监督微调和强化学习来提升准确性和效率，其中还包括旨在减少输出长度的Frugal Learning技术。实验结果表明，在MATH-V和VLM-S2H数据集上，STELAR-Vision相比其基础模型准确率提升了9.7%，并超越了更大的Qwen2VL-72B-Instruct模型7.3%；在五个分布外基准测试中，其性能优于Phi-4-Multimodal-Instruct模型高达28.4%，展现了强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">ReCALL: Recalibrating Capability Degradation for MLLM-based Composed Image Retrieval</div>
<div class="meta-line">Authors: Tianyu Yang, ChenWei He, Xiangzhao Hao, Tianyue Wang, Jiarui Guo, Haiyun Guo, Leigang Qu, Jinqiao Wang, Tat-Seng Chua</div>
<div class="meta-line">First: 2026-02-02T04:52:54+00:00 · Latest: 2026-02-02T04:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01639v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Composed Image Retrieval (CIR) aims to retrieve target images based on a hybrid query comprising a reference image and a modification text. Early dual-tower Vision-Language Models (VLMs) struggle with cross-modality compositional reasoning required for this task. Recently, adapting generative Multimodal Large Language Models (MLLMs) for retrieval offers a promising direction. However, we identify that this adaptation strategy overlooks a fundamental issue: adapting a generative MLLM into a single-embedding discriminative retriever triggers a paradigm conflict, which leads to Capability Degradation - the deterioration of native fine-grained reasoning after retrieval adaptation. To address this challenge, we propose ReCALL (Recalibrating Capability Degradation), a model-agnostic framework that follows a diagnose-generate-refine pipeline: Firstly, we diagnose cognitive blind spots of the retriever via self-guided informative instance mining. Next, we generate corrective instructions and triplets by CoT prompting the foundation MLLM and conduct quality control with VQA-based consistency filtering. Finally, we refine the retriever through continual training on these triplets with a grouped contrastive scheme, thereby internalizing fine-grained visual-semantic distinctions and realigning the discriminative embedding space of retriever with intrinsic compositional reasoning within the MLLM. Extensive experiments on CIRR and FashionIQ show that ReCALL consistently recalibrates degraded capabilities and achieves state-of-the-art performance. Code will be released soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReCALL：基于多模态大语言模型的组合图像检索能力退化再校准</div>
<div class="mono" style="margin-top:8px">组合图像检索（CIR）旨在通过结合参考图像和修改文本的混合查询来检索目标图像。早期的双塔视觉语言模型（VLM）难以应对该任务所需的跨模态组合推理。近期，将生成式多模态大语言模型（MLLM）适配于检索任务展现出潜力，但该策略忽视了一个根本问题：将生成式MLLM转化为单嵌入判别式检索器会引发范式冲突，导致能力退化——即检索适配后原生细粒度推理能力的衰退。为解决此问题，我们提出ReCALL（能力退化再校准），这是一个模型无关的框架，遵循诊断-生成-优化流程：首先通过自引导信息实例挖掘诊断检索器的认知盲点；接着利用思维链提示基座MLLM生成矫正指令与三元组，并采用基于视觉问答的一致性过滤进行质量控制；最后通过分组对比学习持续训练优化检索器，从而内化细粒度视觉语义区分，并将检索器的判别式嵌入空间与MLLM内在的组合推理能力重新对齐。在CIRR和FashionIQ数据集上的大量实验表明，ReCALL能持续校准退化能力并实现最优性能。代码即将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the capability degradation that occurs when adapting generative multimodal large language models (MLLMs) for composed image retrieval (CIR), where a generative model&#x27;s fine-grained reasoning deteriorates upon conversion into a single-embedding discriminative retriever. To mitigate this, the authors propose ReCALL, a model-agnostic framework that diagnoses the retriever&#x27;s cognitive blind spots via self-guided instance mining, generates corrective instructions and triplets using chain-of-thought prompting from the foundation MLLM with VQA-based quality filtering, and refines the retriever through continual training with a grouped contrastive scheme. Experiments on CIRR and FashionIQ benchmarks demonstrate that ReCALL effectively recalibrates the degraded capabilities and achieves state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将生成式多模态大语言模型（MLLM）适配于组合图像检索（CIR）任务时出现的能力退化问题，该任务需要跨模态的组合推理。提出的ReCALL框架采用诊断-生成-优化的流程：首先通过自引导信息实例挖掘诊断检索器的认知盲点，然后利用基础MLLM的思维链提示生成校正指令和三联数据，并辅以基于视觉问答的一致性过滤进行质量控制，最后通过分组对比方案的持续训练来优化检索器。在CIRR和FashionIQ基准上的大量实验表明，ReCALL能有效重新校准退化的细粒度推理能力，并取得了最先进的检索性能。</div>
</details>
</div>
<div class="card">
<div class="title">PISCES: Annotation-free Text-to-Video Post-Training via Optimal Transport-Aligned Rewards</div>
<div class="meta-line">Authors: Minh-Quan Le, Gaurav Mittal, Cheng Zhao, David Gu, Dimitris Samaras, Mei Chen</div>
<div class="meta-line">First: 2026-02-02T04:37:11+00:00 · Latest: 2026-02-02T04:37:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01624v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01624v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-video (T2V) generation aims to synthesize videos with high visual quality and temporal consistency that are semantically aligned with input text. Reward-based post-training has emerged as a promising direction to improve the quality and semantic alignment of generated videos. However, recent methods either rely on large-scale human preference annotations or operate on misaligned embeddings from pre-trained vision-language models, leading to limited scalability or suboptimal supervision. We present $\texttt{PISCES}$, an annotation-free post-training algorithm that addresses these limitations via a novel Dual Optimal Transport (OT)-aligned Rewards module. To align reward signals with human judgment, $\texttt{PISCES}$ uses OT to bridge text and video embeddings at both distributional and discrete token levels, enabling reward supervision to fulfill two objectives: (i) a Distributional OT-aligned Quality Reward that captures overall visual quality and temporal coherence; and (ii) a Discrete Token-level OT-aligned Semantic Reward that enforces semantic, spatio-temporal correspondence between text and video tokens. To our knowledge, $\texttt{PISCES}$ is the first to improve annotation-free reward supervision in generative post-training through the lens of OT. Experiments on both short- and long-video generation show that $\texttt{PISCES}$ outperforms both annotation-based and annotation-free methods on VBench across Quality and Semantic scores, with human preference studies further validating its effectiveness. We show that the Dual OT-aligned Rewards module is compatible with multiple optimization paradigms, including direct backpropagation and reinforcement learning fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PISCES：基于最优传输对齐奖励的无标注文本到视频后训练方法</div>
<div class="mono" style="margin-top:8px">文本到视频（T2V）生成旨在合成具有高视觉质量、时间连贯性且与输入文本语义对齐的视频。基于奖励的后训练已成为提升生成视频质量与语义对齐度的有效方向。然而，现有方法要么依赖大规模人工偏好标注，要么使用预训练视觉-语言模型中未对齐的嵌入表示，导致可扩展性受限或监督效果欠佳。本文提出 $\texttt{PISCES}$，一种无需标注的后训练算法，通过新颖的“双重最优传输对齐奖励”模块解决上述局限。为使奖励信号与人类判断对齐，$\texttt{PISCES}$ 利用最优传输在分布层面和离散词元层面桥接文本与视频嵌入，使奖励监督实现双重目标：（i）分布对齐的质量奖励，捕捉整体视觉质量与时间连贯性；（ii）词元级对齐的语义奖励，强化文本与视频词元间的语义及时空对应关系。据我们所知，$\texttt{PISCES}$ 是首个通过最优传输视角改进生成式后训练中无标注奖励监督的方法。在短视频与长视频生成任务上的实验表明，$\texttt{PISCES}$ 在 VBench 的质量与语义评分上均优于基于标注及无标注方法，人类偏好研究进一步验证了其有效性。双重最优传输对齐奖励模块兼容多种优化范式，包括直接反向传播与强化学习微调。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To improve text-to-video generation without costly human preference annotations, this research introduces PISCES, a post-training algorithm that employs a novel Dual Optimal Transport-aligned Rewards module. The method aligns text and video embeddings at both distributional and discrete token levels using optimal transport, creating a quality reward for visual and temporal coherence and a semantic reward for spatio-temporal correspondence. Experimental results on VBench for short- and long-video generation demonstrate that PISCES surpasses both annotation-based and annotation-free methods in quality and semantic scores, with human preference studies confirming its effectiveness, and the reward module is shown to be compatible with various optimization paradigms.</div>
<div class="mono" style="margin-top:8px">为了在不依赖昂贵人工偏好标注或未对齐的预训练模型嵌入的情况下改进文本到视频生成，本研究提出了PISCES，一种采用新型双重最优传输对齐奖励模块的后训练算法。该方法利用最优传输在分布和离散标记两个层面对齐文本与视频嵌入，从而生成用于视觉和时间一致性的质量奖励，以及用于时空对应关系的语义奖励。在VBench上的实验结果表明，PISCES在质量和语义分数上均优于基于标注和无标注的方法，人类偏好研究进一步验证了其有效性，且该奖励模块证明可与多种优化范式兼容。</div>
</details>
</div>
<div class="card">
<div class="title">OpenWorldSAM: Extending SAM2 for Universal Image Segmentation with Language Prompts</div>
<div class="meta-line">Authors: Shiting Xiao, Rishabh Kabra, Yuhang Li, Donghyun Lee, Joao Carreira, Priyadarshini Panda</div>
<div class="meta-line">First: 2025-07-07T19:16:22+00:00 · Latest: 2026-02-02T04:21:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05427v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.05427v4">PDF</a> · <a href="https://github.com/GinnyXiao/OpenWorldSAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to segment objects based on open-ended language prompts remains a critical challenge, requiring models to ground textual semantics into precise spatial masks while handling diverse and unseen categories. We present OpenWorldSAM, a framework that extends the prompt-driven Segment Anything Model v2 (SAM2) to open-vocabulary scenarios by integrating multi-modal embeddings extracted from a lightweight vision-language model (VLM). Our approach is guided by four key principles: i) Unified prompting: OpenWorldSAM supports a diverse range of prompts, including category-level and sentence-level language descriptions, providing a flexible interface for various segmentation tasks. ii) Efficiency: By freezing the pre-trained components of SAM2 and the VLM, we train only 4.5 million parameters on the COCO-stuff dataset, achieving remarkable resource efficiency. iii) Instance Awareness: We enhance the model&#x27;s spatial understanding through novel positional tie-breaker embeddings and cross-attention layers, enabling effective segmentation of multiple instances. iv) Generalization: OpenWorldSAM exhibits strong zero-shot capabilities, generalizing well on unseen categories and an open vocabulary of concepts without additional training. Extensive experiments demonstrate that OpenWorldSAM achieves state-of-the-art performance in open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks. Code is available at https://github.com/GinnyXiao/OpenWorldSAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenWorldSAM：扩展SAM2实现基于语言提示的通用图像分割</div>
<div class="mono" style="margin-top:8px">基于开放式语言提示分割物体的能力仍是关键挑战，需要模型将文本语义映射为精确的空间掩码，同时处理多样且未见过的类别。我们提出OpenWorldSAM框架，通过集成轻量级视觉语言模型提取的多模态嵌入，将提示驱动的Segment Anything Model v2（SAM2）扩展至开放词汇场景。该框架遵循四项核心原则：一）统一提示：支持类别级和句子级语言描述等多种提示，为不同分割任务提供灵活接口；二）高效性：冻结SAM2与视觉语言模型的预训练组件，仅在COCO-stuff数据集上训练450万个参数，实现显著资源效率；三）实例感知：通过新颖的位置决胜嵌入和交叉注意力层增强模型空间理解能力，实现多实例有效分割；四）泛化性：展现强大的零样本能力，在未见类别和开放概念词汇上无需额外训练即可良好泛化。大量实验表明，OpenWorldSAM在多个基准测试的开放词汇语义分割、实例分割及全景分割中达到最先进性能。代码发布于https://github.com/GinnyXiao/OpenWorldSAM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of segmenting objects from open-ended language prompts, which requires grounding textual semantics into precise spatial masks across diverse and unseen categories, this work introduces OpenWorldSAM. The method extends the Segment Anything Model v2 (SAM2) by integrating multi-modal embeddings from a lightweight vision-language model, freezing pre-trained components to train only 4.5 million parameters, and enhancing spatial understanding with positional tie-breaker embeddings and cross-attention layers for instance awareness. Experimental results show that OpenWorldSAM achieves state-of-the-art performance in zero-shot open-vocabulary semantic, instance, and panoptic segmentation across multiple benchmarks, demonstrating strong generalization without additional training.</div>
<div class="mono" style="margin-top:8px">为解决基于开放语言提示分割物体这一关键挑战，该研究需要将文本语义精确地映射到空间掩码并处理多样且未见过的类别。为此，本文提出了OpenWorldSAM框架，通过集成轻量级视觉语言模型提取的多模态嵌入，将提示驱动的Segment Anything Model v2（SAM2）扩展至开放词汇场景。该方法遵循统一提示、高效性、实例感知和泛化能力四大原则，仅训练450万个参数，并引入了位置决胜嵌入和交叉注意力层以增强空间理解。大量实验表明，该模型在多个基准测试的开放词汇语义、实例和全景分割任务中实现了最先进的零样本性能。</div>
</details>
</div>
<div class="card">
<div class="title">CrossCheck-Bench: Diagnosing Compositional Failures in Multimodal Conflict Resolution</div>
<div class="meta-line">Authors: Baoliang Tian, Yuxuan Si, Jilong Wang, Lingyao Li, Zhongyuan Bao, Zineng Zhou, Tao Wang, Sixu Li, Ziyao Xu, Mingze Wang, Zhouzhuo Zhang, Zhihao Wang, Yike Yun, Ke Tian, Ning Yang, Minghui Qiu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-11-19T12:17:15+00:00 · Latest: 2026-02-02T03:28:04+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21717v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21717v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models are primarily trained and evaluated on aligned image-text pairs, which leaves their ability to detect and resolve real-world inconsistencies largely unexplored. In open-domain applications visual and textual cues often conflict, requiring models to perform structured reasoning beyond surface-level alignment. We introduce CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The benchmark adopts a hierarchical task framework covering three levels of reasoning complexity and defines seven atomic capabilities essential for resolving cross-modal inconsistencies. CrossCheck-Bench includes 15k question-answer pairs sourced from real-world artifacts with synthetically injected contradictions. The dataset is constructed through a multi-stage annotation pipeline involving more than 450 expert hours to ensure semantic validity and calibrated difficulty across perception, integration, and reasoning. We evaluate 13 state-of-the-art vision-language models and observe a consistent performance drop as tasks shift from perceptual matching to logical contradiction detection. Most models perform well on isolated entity recognition but fail when multiple clues must be synthesized for conflict reasoning. Capability-level analysis further reveals uneven skill acquisition, especially in tasks requiring multi-step inference or rule-based validation. Additional probing shows that conventional prompting strategies such as Chain-of-Thought and Set-of-Mark yield only marginal gains. By contrast, methods that interleave symbolic reasoning with grounded visual processing achieve more stable improvements. These results highlight a persistent bottleneck in multimodal reasoning and suggest new directions for building models capable of robust cross-modal verification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CrossCheck-Bench：诊断多模态冲突解决中的组合性故障</div>
<div class="mono" style="margin-top:8px">多模态大语言模型主要在图文对齐的数据上进行训练和评估，这使其检测和解决现实世界不一致性的能力在很大程度上未被探索。在开放域应用中，视觉与文本线索常存在冲突，要求模型进行超越表层对齐的结构化推理。我们提出CrossCheck-Bench——一个用于评估多模态输入中矛盾检测能力的诊断性基准。该基准采用分层任务框架，涵盖三个推理复杂度层级，并定义了解决跨模态不一致所需的七项核心原子能力。CrossCheck-Bench包含1.5万个源自真实世界素材并人工注入矛盾的问答对，通过耗时超450专家小时的多阶段标注流程构建，确保语义有效性及在感知、整合、推理层面的难度校准。我们对13个前沿视觉语言模型进行评估，发现随着任务从感知匹配转向逻辑矛盾检测，模型性能出现系统性下降。多数模型在孤立实体识别上表现良好，但在需要综合多重线索进行冲突推理时失效。能力层级分析进一步揭示了技能习得不均衡现象，尤其在需要多步推理或基于规则验证的任务中。额外实验表明，传统提示策略（如思维链、标记集）仅带来边际增益；而将符号推理与具象视觉处理交织的方法则实现更稳定的改进。这些结果凸显了多模态推理中的持续瓶颈，并为构建具备鲁棒跨模态验证能力的模型指明了新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limited exploration of multimodal models&#x27; ability to handle real-world inconsistencies, this research introduces CrossCheck-Bench, a diagnostic benchmark for evaluating contradiction detection in multimodal inputs. The method constructs a hierarchical benchmark with 15k question-answer pairs from real-world artifacts, featuring synthetically injected contradictions and defined through a multi-stage expert annotation pipeline to assess seven atomic capabilities across three reasoning complexity levels. Key experimental findings from evaluating 13 state-of-the-art vision-language models reveal a consistent performance drop as tasks require deeper logical contradiction detection over simple perceptual matching, with most models struggling to synthesize multiple clues for conflict reasoning; conventional prompting strategies offered marginal gains, while methods integrating symbolic reasoning with visual processing showed more stable improvements, highlighting a bottleneck in robust cross-modal verification.</div>
<div class="mono" style="margin-top:8px">多模态大语言模型通常在对齐的图文对上训练，其处理现实世界不一致性的能力尚未得到充分探索。为此，研究者提出了CrossCheck-Bench这一诊断性基准，它采用分层任务框架，涵盖三个推理复杂度和七种解决跨模态冲突的核心能力；该基准包含来自真实世界素材的1.5万个问答对，其中注入了人工合成的矛盾，并通过多阶段专家标注流程构建。对13个先进视觉语言模型的评估显示，随着任务从感知匹配转向逻辑矛盾检测，模型性能持续下降，它们在综合多重线索进行冲突推理时表现不佳，且在多步推理等能力上发展不均。进一步测试表明，传统提示策略仅带来边际收益，而将符号推理与视觉处理交织的方法则能实现更稳定的改进，这凸显了多模态推理中的一个持续瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Visual Code Mobile World Models</div>
<div class="meta-line">Authors: Woosung Koh, Sungjun Han, Segyu Lee, Se-Young Yun, Jamin Shin</div>
<div class="meta-line">First: 2026-02-02T03:12:16+00:00 · Latest: 2026-02-02T03:12:16+00:00</div>
<div class="meta-line">Comments: Pre-print (technical report)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01576v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01576v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile Graphical User Interface (GUI) World Models (WMs) offer a promising path for improving mobile GUI agent performance at train- and inference-time. However, current approaches face a critical trade-off: text-based WMs sacrifice visual fidelity, while the inability of visual WMs in precise text rendering led to their reliance on slow, complex pipelines dependent on numerous external models. We propose a novel paradigm: visual world modeling via renderable code generation, where a single Vision-Language Model (VLM) predicts the next GUI state as executable web code that renders to pixels, rather than generating pixels directly. This combines the strengths of both approaches: VLMs retain their linguistic priors for precise text rendering while their pre-training on structured web code enables high-fidelity visual generation. We introduce gWorld (8B, 32B), the first open-weight visual mobile GUI WMs built on this paradigm, along with a data generation framework (gWorld) that automatically synthesizes code-based training data. In extensive evaluation across 4 in- and 2 out-of-distribution benchmarks, gWorld sets a new pareto frontier in accuracy versus model size, outperforming 8 frontier open-weight models over 50.25x larger. Further analyses show that (1) scaling training data via gWorld yields meaningful gains, (2) each component of our pipeline improves data quality, and (3) stronger world modeling improves downstream mobile GUI policy performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式视觉代码移动端世界模型</div>
<div class="mono" style="margin-top:8px">移动图形用户界面（GUI）世界模型（WMs）为提升移动GUI智能体在训练和推理阶段的性能提供了可行路径。然而，现有方法面临关键权衡：基于文本的世界模型牺牲视觉保真度，而视觉世界模型因无法精确渲染文本，不得不依赖缓慢复杂、需要大量外部模型的流程。我们提出一种新范式：通过可渲染代码生成进行视觉世界建模，即使用单一视觉语言模型（VLM）将下一GUI状态预测为可执行且能渲染为像素的网页代码，而非直接生成像素。该方法融合了两种范式的优势：VLM保留其语言先验以实现精确文本渲染，同时其对结构化网页代码的预训练支持高保真视觉生成。我们推出基于此范式的首个开放权重视觉移动GUI世界模型gWorld（8B, 32B），并配套开发自动合成代码训练数据的数据生成框架（gWorld）。在4个领域内和2个领域外基准测试中，gWorld在准确率与模型规模的帕累托前沿取得突破，性能超越8个前沿开放权重模型（其总参数量超gWorld 50.25倍）。进一步分析表明：（1）通过gWorld扩展训练数据能带来显著增益；（2）流程各组件均提升数据质量；（3）更强的世界建模能力可改善下游移动GUI策略性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the trade-off in mobile GUI world models between text-based approaches that lack visual fidelity and visual models that struggle with precise text rendering and rely on complex pipelines, this work introduces a novel paradigm of visual world modeling via renderable code generation. The method employs a single vision-language model to predict the next GUI state as executable web code that renders to pixels, leveraging the model&#x27;s linguistic priors for text and its pre-training on structured web code for visual generation. Experimental results on multiple benchmarks show that the proposed gWorld models establish a new pareto frontier in accuracy versus size, outperform larger models, and that improved world modeling enhances downstream GUI policy performance.</div>
<div class="mono" style="margin-top:8px">为了解决移动图形用户界面世界模型中文本方法缺乏视觉保真度与视觉模型难以精确渲染文本且依赖复杂流程之间的权衡，本研究提出了一种通过可渲染代码生成进行视觉世界建模的新范式。该方法使用单一的视觉语言模型，将下一个GUI状态预测为可执行的、能渲染为像素的网页代码，利用模型的语言先验处理文本，并借助其在结构化网页代码上的预训练实现视觉生成。在四个分布内和两个分布外基准上的实验结果表明，所提出的gWorld模型（8B, 32B）在准确性与模型大小之间建立了新的帕累托前沿，性能优于八个更大的开源模型；进一步分析证实，扩展训练数据、改进流程组件以及更强的世界建模能力都能提升下游GUI策略的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SGHA-Attack: Semantic-Guided Hierarchical Alignment for Transferable Targeted Attacks on Vision-Language Models</div>
<div class="meta-line">Authors: Haobo Wang, Weiqi Luo, Xiaojun Jia, Xiaochun Cao</div>
<div class="meta-line">First: 2026-02-02T03:10:41+00:00 · Latest: 2026-02-02T03:10:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01574v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01574v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) are vulnerable to transfer-based adversarial perturbations, enabling attackers to optimize on surrogate models and manipulate black-box VLM outputs. Prior targeted transfer attacks often overfit surrogate-specific embedding space by relying on a single reference and emphasizing final-layer alignment, which underutilizes intermediate semantics and degrades transfer across heterogeneous VLMs. To address this, we propose SGHA-Attack, a Semantic-Guided Hierarchical Alignment framework that adopts multiple target references and enforces intermediate-layer consistency. Concretely, we generate a visually grounded reference pool by sampling a frozen text-to-image model conditioned on the target prompt, and then carefully select the Top-K most semantically relevant anchors under the surrogate to form a weighted mixture for stable optimization guidance. Building on these anchors, SGHA-Attack injects target semantics throughout the feature hierarchy by aligning intermediate visual representations at both global and spatial granularities across multiple depths, and by synchronizing intermediate visual and textual features in a shared latent subspace to provide early cross-modal supervision before the final projection. Extensive experiments on open-source and commercial black-box VLMs show that SGHA-Attack achieves stronger targeted transferability than prior methods and remains robust under preprocessing and purification defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SGHA-Attack：面向视觉语言模型可迁移定向攻击的语义引导分层对齐框架</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLM）易受基于迁移的对抗性扰动影响，攻击者可通过在代理模型上优化来操纵黑盒VLM输出。现有定向迁移攻击常因依赖单一参考且强调最终层对齐，过度拟合代理特定的嵌入空间，未能充分利用中间语义并降低跨异构VLM的迁移效果。为此，我们提出SGHA-Attack——一种语义引导分层对齐框架，采用多目标参考并强化中间层一致性。具体而言，我们通过采样基于目标提示的冻结文生图模型生成视觉锚点池，在代理模型下精选语义最相关的Top-K锚点形成加权混合，为优化提供稳定引导。基于这些锚点，SGHA-Attack通过在多深度层级对齐全局与空间粒度的中间视觉表征，并在共享潜空间同步中间视觉与文本特征以提供最终投影前的早期跨模态监督，将目标语义注入特征层次结构。在开源与商业黑盒VLM上的大量实验表明，SGHA-Attack较现有方法具有更强的定向迁移能力，且在预处理与净化防御下保持鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the vulnerability of large vision-language models (VLMs) to transfer-based adversarial attacks, where existing targeted attacks often overfit to surrogate models by relying on a single reference and focusing on final-layer alignment, limiting transferability across heterogeneous VLMs. To address this, SGHA-Attack introduces a semantic-guided hierarchical alignment framework that utilizes multiple target references sampled from a frozen text-to-image model and selects the top-K most relevant anchors to guide stable optimization; it enforces intermediate-layer consistency by aligning visual representations at multiple depths and synchronizing visual and textual features in a shared latent subspace. Experimental results on open-source and commercial black-box VLMs demonstrate that SGHA-Attack achieves superior targeted transferability compared to prior methods and maintains robustness against preprocessing and purification defenses.</div>
<div class="mono" style="margin-top:8px">该研究针对现有基于迁移的视觉语言模型针对性对抗攻击的局限性，即过度依赖单一代理模型的嵌入空间和最终层对齐，导致攻击难以有效迁移到异构黑盒模型。提出的SGHA-Attack方法采用语义引导的分层对齐框架，利用通过文本到图像模型生成的多目标参考图像，并选择最相关的锚点，通过在多个深度上对齐中间视觉特征的全局和空间粒度，并在共享潜在子空间中同步中间视觉与文本特征，以实现跨层语义一致性。实验结果表明，SGHA-Attack在多种开源和商业黑盒视觉语言模型上实现了更强的针对性迁移能力，并且在预处理和净化防御下保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Semantically Guided Dynamic Visual Prototype Refinement for Compositional Zero-Shot Learning</div>
<div class="meta-line">Authors: Zhong Peng, Yishi Xu, Gerong Wang, Wenchao Chen, Bo Chen, Jing Zhang, Hongwei Liu</div>
<div class="meta-line">First: 2025-01-13T08:04:32+00:00 · Latest: 2026-02-02T03:09:49+00:00</div>
<div class="meta-line">Comments: Accepted for publication in Neurocomputing</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.07114v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.07114v2">PDF</a> · <a href="https://github.com/ISPZ/Duplex-CZSL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compositional Zero-Shot Learning (CZSL) seeks to recognize unseen state-object pairs by recombining primitives learned from seen compositions. Despite recent progress with vision-language models (VLMs), two limitations remain: (i) text-driven semantic prototypes are weakly discriminative in the visual feature space; and (ii) unseen pairs are optimized passively, thereby inducing seen bias. To address these limitations, we present Duplex, a framework that couples dual-prototype learning with dynamic local-graph refinement of visual prototypes. For each composition, Duplex maintains a semantic prototype via prompt learning and a visual prototype for unseen pairs constructed by recombining disentangled state and object primitives from seen images. The visual prototypes are updated dynamically through lightweight aggregation on mini-batch local graphs, which incorporates unseen compositions during training without labels. This design introduces fine-grained visual evidence while preserving semantic structure. It enriches class prototypes, better disambiguates semantically similar yet visually distinct pairs, and mitigates seen bias. Experiments on MIT-States, UT-Zappos, and CGQA in closed-world and open-world settings achieve competitive performance and consistent compositional generalization. Our source code is available at https://github.com/ISPZ/Duplex-CZSL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向组合零样本学习的语义引导动态视觉原型优化</div>
<div class="mono" style="margin-top:8px">组合零样本学习（CZSL）旨在通过重组从已见组合中学到的基元来识别未见的状态-对象组合。尽管视觉语言模型（VLMs）已取得进展，仍存在两个局限：（i）文本驱动的语义原型在视觉特征空间中区分性较弱；（ii）未见组合的优化过程被动，易引发已见组合偏差。为此，我们提出Duplex框架，该框架将双原型学习与视觉原型的动态局部图优化相结合。针对每个组合，Duplex通过提示学习维护语义原型，并通过重组已见图像中解耦的状态与对象基元构建未见组合的视觉原型。视觉原型通过小批量局部图的轻量级聚合动态更新，在训练过程中无需标签即可纳入未见组合。该设计在保持语义结构的同时引入细粒度视觉证据，丰富了类别原型，更好地区分语义相似但视觉相异的组合，并缓解已见组合偏差。在MIT-States、UT-Zappos和CGQA数据集上进行的闭集与开集实验均取得竞争性性能，并展现出一致的组合泛化能力。源代码发布于https://github.com/ISPZ/Duplex-CZSL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses two key limitations in Compositional Zero-Shot Learning (CZSL): the weak discriminative power of text-driven semantic prototypes in visual space and the passive optimization leading to bias towards seen compositions. The proposed Duplex framework introduces a dual-prototype learning approach, coupling a semantic prototype from prompt learning with a visual prototype constructed by recombining disentangled state and object primitives from seen images. These visual prototypes are dynamically refined through lightweight aggregation on mini-batch local graphs, incorporating unseen compositions during training without labels. Experiments on MIT-States, UT-Zappos, and CGQA datasets demonstrate that this method achieves competitive performance and consistent compositional generalization in both closed-world and open-world settings by enriching class prototypes and better disambiguating visually distinct pairs.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决组合零样本学习中的两个关键局限：文本驱动的语义原型在视觉空间中区分能力较弱，以及被动优化导致的已见组合偏差。提出的Duplex框架采用双原型学习方法，将基于提示学习的语义原型与通过重组已见图像中解耦的状态和对象基元构建的视觉原型相结合。该方法通过在迷你批次局部图上进行轻量级聚合来动态优化视觉原型，从而在无标签情况下将未见组合纳入训练。在MIT-States、UT-Zappos和CGQA数据集上的实验表明，该方法在封闭世界和开放世界设置下均取得了有竞争力的性能，并实现了一致的组合泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Preserving Localized Patch Semantics in VLMs</div>
<div class="meta-line">Authors: Parsa Esmaeilkhani, Longin Jan Latecki</div>
<div class="meta-line">First: 2026-02-02T01:48:11+00:00 · Latest: 2026-02-02T01:48:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01530v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01530v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Logit Lens has been proposed for visualizing tokens that contribute most to LLM answers. Recently, Logit Lens was also shown to be applicable in autoregressive Vision-Language Models (VLMs), where it illustrates the conceptual content of image tokens in the form of heatmaps, e.g., which image tokens are likely to depict the concept of cat in a given image. However, the visual content of image tokens often gets diffused to language tokens, and consequently, the locality of visual information gets mostly destroyed, which renders Logit Lens visualization unusable for explainability. To address this issue, we introduce a complementary loss to next-token prediction (NTP) to prevent the visual tokens from losing the visual representation inherited from corresponding image patches. The proposed Logit Lens Loss (LLL) is designed to make visual token embeddings more semantically aligned with the textual concepts that describe their image regions (e.g., patches containing a cat with the word &quot;cat&quot;), without requiring any architectural modification or large-scale training. This way, LLL constrains the mixing of image and text tokens in the self-attention layers in order to prevent image tokens from losing their localized visual information. As our experiments show, LLL not only makes Logit Lens practically relevant by producing meaningful object confidence maps in images, but also improves performance on vision-centric tasks like segmentation without attaching any special heads.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在视觉语言模型中保持局部图像块语义</div>
<div class="mono" style="margin-top:8px">Logit Lens 最初被提出用于可视化对大型语言模型答案贡献最大的词元。近期研究表明，该方法也可应用于自回归视觉语言模型，通过热力图形式展示图像词元的概念内容（例如图像中哪些区域可能包含“猫”的概念）。然而，图像词元的视觉内容常会扩散至语言词元，导致视觉信息的局部性基本丧失，使得 Logit Lens 可视化无法用于模型可解释性分析。为解决此问题，我们提出在下一词元预测损失基础上引入互补损失函数，防止视觉词元丢失从对应图像块继承的视觉表征。所提出的 Logit Lens 损失函数旨在使视觉词元嵌入与描述其对应图像区域的文本概念（如包含猫的图像块与“猫”这个词）实现语义对齐，且无需修改模型架构或进行大规模训练。通过约束自注意力层中图像与文本词元的混合，该损失函数能有效防止图像词元丢失局部视觉信息。实验表明，该方法不仅通过生成有意义的图像对象置信度图使 Logit Lens 具备实际应用价值，还能在无需附加特殊模块的情况下提升分割等视觉中心任务的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Logit Lens in autoregressive Vision-Language Models (VLMs), where visual information from image tokens diffuses into language tokens, destroying locality and making the visualization unusable for explainability. The method introduces a Logit Lens Loss (LLL) as a complementary loss to next-token prediction, designed to keep visual token embeddings semantically aligned with textual concepts describing their corresponding image patches, without architectural changes or large-scale training, thereby constraining the mixing in self-attention layers. Experimental results show that LLL enables meaningful object confidence map visualizations via Logit Lens and improves performance on vision-centric tasks like segmentation.</div>
<div class="mono" style="margin-top:8px">本研究针对自回归视觉语言模型中Logit Lens可视化失效的问题，即图像令牌的视觉信息扩散到语言令牌中破坏了局部性。为保持图像块与其文本描述之间的语义对齐，该方法引入了一种Logit Lens损失作为下一令牌预测的补充目标，通过约束自注意力层中的令牌混合，无需修改模型架构。实验结果表明，该损失函数使得Logit Lens能够生成有意义的对象置信度图，并提升了分割等视觉中心任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Toward a Machine Bertin: Why Visualization Needs Design Principles for Machine Cognition</div>
<div class="meta-line">Authors: Brian Keith-Norambuena</div>
<div class="meta-line">First: 2026-02-02T01:39:33+00:00 · Latest: 2026-02-02T01:39:33+00:00</div>
<div class="meta-line">Comments: Preprint submitted to IEEE TVCG on February 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01527v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01527v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visualization&#x27;s design knowledge-effectiveness rankings, encoding guidelines, color models, preattentive processing rules -- derives from six decades of psychophysical studies of human vision. Yet vision-language models (VLMs) increasingly consume chart images in automated analysis pipelines, and a growing body of benchmark evidence indicates that this human-centered knowledge base does not straightforwardly transfer to machine audiences. Machines exhibit different encoding performance patterns, process images through patch-based tokenization rather than holistic perception, and fail on design patterns that pose no difficulty for humans-while occasionally succeeding where humans struggle. Current approaches address this gap primarily by bypassing vision entirely, converting charts to data tables or structured text. We argue that this response forecloses a more fundamental question: what visual representations would actually serve machine cognition well? This paper makes the case that the visualization field needs to investigate machine-oriented visual design as a distinct research problem. We synthesize evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies to show that the human-machine perceptual divergence is qualitative, not merely quantitative, and critically examine the prevailing bypassing approach. We propose a conceptual distinction between human-oriented and machine-oriented visualization-not as an engineering architecture but as a recognition that different audiences may require fundamentally different design foundations-and outline a research agenda for developing the empirical foundations the field currently lacks: the beginnings of a &quot;machine Bertin&quot; to complement the human-centered knowledge the field already possesses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向机器贝尔坦：为何可视化需要面向机器认知的设计原则</div>
<div class="mono" style="margin-top:8px">可视化设计知识——如有效性排序、编码准则、色彩模型、前注意处理规则——源自六十年来对人类视觉的心理物理学研究。然而，视觉语言模型在自动化分析流程中日益频繁地处理图表图像，大量基准测试证据表明，这套以人为中心的知识体系并不能直接迁移至机器受众。机器表现出不同的编码性能模式，通过基于图像块的标记化而非整体感知来处理图像，在人类毫无困难的设计模式上失败，却偶尔能在人类感到困难的场景中成功。当前方法主要通过完全绕过视觉处理（将图表转换为数据表或结构化文本）来弥合这一差距。我们认为这种应对方式回避了一个更根本的问题：究竟何种视觉表征才能真正服务于机器认知？本文主张可视化领域需要将面向机器的视觉设计作为一个独立的研究课题进行探索。我们综合视觉语言模型基准测试、视觉推理研究和可视化素养研究的证据，表明人机感知差异是质性的而不仅是量化的，并对主流的“绕过式”方法进行批判性审视。我们提出面向人类与面向机器的可视化在概念上的区分——并非作为工程架构，而是承认不同受众可能需要根本不同的设计基础——并勾勒出建立该领域当前缺乏的实证基础的研究议程：作为对现有以人为中心知识体系的补充，构建“机器贝尔坦”的雏形。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that established visualization design principles, derived from decades of human vision studies, do not effectively transfer to machine audiences like vision-language models (VLMs), which process images differently and exhibit distinct performance patterns. The method involves a critical synthesis of evidence from VLM benchmarks, visual reasoning research, and visualization literacy studies to argue that the perceptual divergence is qualitative and to examine the limitations of current approaches that bypass visual perception by converting charts to structured data. Key experimental findings indicate that machines fail on certain human-intuitive design patterns while sometimes succeeding on challenging ones, underscoring the need for a distinct research agenda to develop empirical, machine-oriented visualization design principles.</div>
<div class="mono" style="margin-top:8px">本文指出，基于人类视觉研究得出的可视化设计原则无法有效迁移到视觉语言模型等机器视觉系统中，因为机器通过基于分块的标记化处理图表图像，并表现出不同的性能模式。作者批评了当前将图表转换为结构化数据以规避视觉分析的主流方法，主张该领域必须将面向机器的视觉设计作为一个独立的研究问题来探索。通过综合视觉语言模型基准测试和视觉推理研究的证据，他们论证了人类与机器在感知上存在质的差异，并提出了一个研究议程，旨在为建立“机器Bertin”——即专门服务于机器认知的设计框架——奠定实证基础。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Paradigm Evaluation of Gaze-Based Semantic Object Identification for Intelligent Vehicles</div>
<div class="meta-line">Authors: Penghao Deng, Jidong J. Yang, Jiachen Bian</div>
<div class="meta-line">First: 2026-02-01T21:43:02+00:00 · Latest: 2026-02-01T21:43:02+00:00</div>
<div class="meta-line">Comments: 21 pages, 15 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01452v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01452v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding where drivers direct their visual attention during driving, as characterized by gaze behavior, is critical for developing next-generation advanced driver-assistance systems and improving road safety. This paper tackles this challenge as a semantic identification task from the road scenes captured by a vehicle&#x27;s front-view camera. Specifically, the collocation of gaze points with object semantics is investigated using three distinct vision-based approaches: direct object detection (YOLOv13), segmentation-assisted classification (SAM2 paired with EfficientNetV2 versus YOLOv13), and query-based Vision-Language Models, VLMs (Qwen2.5-VL-7b versus Qwen2.5-VL-32b). The results demonstrate that the direct object detection (YOLOv13) and Qwen2.5-VL-32b significantly outperform other approaches, achieving Macro F1-Scores over 0.84. The large VLM (Qwen2.5-VL-32b), in particular, exhibited superior robustness and performance for identifying small, safety-critical objects such as traffic lights, especially in adverse nighttime conditions. Conversely, the segmentation-assisted paradigm suffers from a &quot;part-versus-whole&quot; semantic gap that led to large failure in recall. The results reveal a fundamental trade-off between the real-time efficiency of traditional detectors and the richer contextual understanding and robustness offered by large VLMs. These findings provide critical insights and practical guidance for the design of future human-aware intelligent driver monitoring systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能车辆中基于注视的语义对象识别的跨范式评估</div>
<div class="mono" style="margin-top:8px">理解驾驶员在驾驶过程中视觉注意力的指向（以注视行为为特征）对于开发下一代高级驾驶辅助系统和提升道路安全至关重要。本文将此挑战视为一项基于车辆前视摄像头捕捉的道路场景的语义识别任务。具体而言，研究采用三种不同的视觉方法探究注视点与对象语义的关联：直接对象检测（YOLOv13）、分割辅助分类（SAM2 配对 EfficientNetV2 与 YOLOv13 对比）以及基于查询的视觉语言模型（Qwen2.5-VL-7b 与 Qwen2.5-VL-32b 对比）。结果表明，直接对象检测（YOLOv13）和 Qwen2.5-VL-32b 显著优于其他方法，宏平均 F1 分数超过 0.84。特别是大型 VLM（Qwen2.5-VL-32b）在识别交通信号灯等小型安全关键对象时表现出卓越的鲁棒性和性能，尤其在恶劣的夜间条件下。相反，分割辅助范式因“局部与整体”的语义鸿沟导致召回率大幅下降。结果揭示了传统检测器的实时效率与大型 VLM 提供的更丰富上下文理解及鲁棒性之间的根本权衡。这些发现为未来具备人类感知能力的智能驾驶员监控系统设计提供了关键见解和实践指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance road safety and develop advanced driver-assistance systems, this research aims to identify what drivers are looking at by semantically linking gaze points to objects in front-view camera scenes. The study evaluates three vision-based methods: direct object detection with YOLOv13, a segmentation-assisted approach combining SAM2 with EfficientNetV2 or YOLOv13, and query-based Vision-Language Models (Qwen2.5-VL-7b and Qwen2.5-VL-32b). Experimental results show that YOLOv13 and the larger Qwen2.5-VL-32b model achieve the highest performance, with Macro F1-Scores exceeding 0.84; Qwen2.5-VL-32b is particularly robust for small, critical objects like traffic lights in nighttime conditions, while the segmentation-assisted method suffers from low recall due to semantic gaps. The findings highlight a trade-off between the real-time efficiency of traditional detectors and the contextual robustness of large VLMs, offering practical guidance for designing driver monitoring systems.</div>
<div class="mono" style="margin-top:8px">为提高道路安全并开发先进驾驶辅助系统，本研究旨在通过将驾驶员的注视点与前视摄像头场景中的物体进行语义关联，来识别驾驶员的视觉关注目标。研究评估了三种基于视觉的方法：使用YOLOv13的直接目标检测、结合SAM2与EfficientNetV2或YOLOv13的分割辅助分类，以及基于查询的视觉语言模型（包括Qwen2.5-VL-7b和Qwen2.5-VL-32b）。实验结果表明，YOLOv13和更大的Qwen2.5-VL-32b模型性能最佳，宏F1分数超过0.84；特别是Qwen2.5-VL-32b在夜间恶劣条件下检测交通灯等小型关键物体时表现出更强的鲁棒性。相比之下，分割辅助方法因物体部分与整体间的语义差距导致召回率大幅下降，这揭示了传统检测器的实时效率与大型视觉语言模型的上下文理解鲁棒性之间的基本权衡。</div>
</details>
</div>
<div class="card">
<div class="title">VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding</div>
<div class="meta-line">Authors: Haorui Yu, Ramon Ruiz-Dolz, Diji Yang, Hang He, Fengrui Zhang, Qiufeng Yi</div>
<div class="meta-line">Venue: ACL 2026</div>
<div class="meta-line">First: 2026-01-12T20:36:30+00:00 · Latest: 2026-02-01T21:03:08+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures, submitted to ACL 2026 Dataset Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07986v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07986v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models&#x27; (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VULCA-Bench：用于评估文化理解的多文化视觉语言基准</div>
<div class="mono" style="margin-top:8px">我们提出了VULCA-Bench，这是一个用于评估视觉语言模型（VLMs）超越表层视觉感知的文化理解能力的多文化艺术评论基准。现有VLM基准主要衡量L1-L2能力（物体识别、场景描述和事实问答），而对高阶文化解读能力的评估不足。VULCA-Bench包含7,410个匹配的图像-评论对，涵盖八种文化传统，并提供中英双语覆盖。我们采用五层框架（L1-L5，从视觉感知到哲学美学）来操作化文化理解，具体化为225个文化特定维度，并辅以专家撰写的双语评论。初步结果表明，高层推理（L3-L5）始终比视觉和技术分析（L1-L2）更具挑战性。数据集、评估脚本和标注工具在补充材料中以CC BY 4.0许可提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of existing vision-language model benchmarks, which primarily evaluate basic visual perception and factual reasoning while neglecting higher-order cultural interpretation. The authors introduce VULCA-Bench, a multicultural art-critique dataset containing 7,410 image-critique pairs spanning eight cultural traditions with Chinese-English bilingual coverage. They operationalize cultural understanding through a five-layer framework (from visual perception to philosophical aesthetics) instantiated as 225 culture-specific dimensions, with expert-written critiques. Experimental results demonstrate that higher-layer reasoning tasks (cultural context, symbolism, and aesthetics) are consistently more challenging for models than lower-level visual and technical analysis.</div>
<div class="mono" style="margin-top:8px">本研究针对现有视觉语言模型评测基准主要评估基础视觉感知和事实推理、而忽视高阶文化理解的问题，提出了VULCA-Bench这一多文化艺术评论数据集。该数据集包含跨越八种文化传统的7,410个图像-评论对，采用中英双语覆盖，并通过从视觉感知到哲学美学的五层框架来系统化衡量文化理解能力。实验结果表明，模型在高层文化语境、象征意义和美学分析等任务上的表现明显低于基础视觉和技术分析任务。</div>
</details>
</div>
<div class="card">
<div class="title">What Does Vision Tool-Use Reinforcement Learning Really Learn? Disentangling Tool-Induced and Intrinsic Effects for Crop-and-Zoom</div>
<div class="meta-line">Authors: Yan Ma, Weiyu Zhang, Tianle Li, Linge Du, Xuyang Shen, Pengfei Liu</div>
<div class="meta-line">First: 2026-02-01T17:00:50+00:00 · Latest: 2026-02-01T17:00:50+00:00</div>
<div class="meta-line">Comments: code: https://github.com/GAIR-NLP/Med</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01334v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01334v1">PDF</a> · <a href="https://github.com/GAIR-NLP/Med">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision tool-use reinforcement learning (RL) can equip vision-language models with visual operators such as crop-and-zoom and achieves strong performance gains, yet it remains unclear whether these gains are driven by improvements in tool use or evolving intrinsic capabilities.We introduce MED (Measure-Explain-Diagnose), a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposes the tool-induced performance difference into gain and harm terms, and probes the mechanisms driving their evolution. Across checkpoint-level analyses on two VLMs with different tool priors and six benchmarks, we find that improvements are dominated by intrinsic learning, while tool-use RL mainly reduces tool-induced harm (e.g., fewer call-induced errors and weaker tool schema interference) and yields limited progress in tool-based correction of intrinsic failures. Overall, current vision tool-use RL learns to coexist safely with tools rather than master them.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉工具使用强化学习究竟学到了什么？——针对裁剪缩放操作中工具效应与内在能力的解耦分析</div>
<div class="mono" style="margin-top:8px">视觉工具使用强化学习（RL）能为视觉语言模型配备裁剪缩放等视觉算子并带来显著性能提升，但尚不清楚这些提升源于工具使用能力的改善还是内在能力的演化。我们提出MED（测量-解释-诊断）框架，通过粗到细的分析解耦内在能力变化与工具效应，将工具引发的性能差异分解为增益项与损害项，并探究其演化机制。基于两种不同工具先验的视觉语言模型在六个基准测试上的检查点分析表明：性能提升主要由内在学习主导，而工具使用RL主要减少工具引发的损害（如降低调用错误、减弱工具模式干扰），在基于工具修正内在错误方面进展有限。总体而言，当前视觉工具使用RL学会的是与工具安全共存而非熟练掌握。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates whether performance gains in vision tool-use reinforcement learning (RL) for crop-and-zoom operations stem from improved tool mastery or from the model&#x27;s intrinsic learning, as current methods lack clarity on this distinction. The authors propose MED, a coarse-to-fine framework that disentangles intrinsic capability changes from tool-induced effects, decomposing tool-induced performance differences into gain and harm components to analyze their evolution. Experimental analyses across two vision-language models and six benchmarks reveal that improvements are primarily driven by intrinsic learning, while tool-use RL mainly reduces tool-induced harm, such as call-induced errors and tool schema interference, with limited progress in correcting intrinsic failures using tools, indicating that current methods learn to coexist safely with tools rather than master them.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究视觉工具使用强化学习在裁剪缩放操作中的性能提升，究竟是源于工具掌握能力的改善，还是模型内在能力的演化。作者提出了MED这一由粗到精的分析框架，用于分离内在能力变化与工具诱导效应，将工具诱导的性能差异分解为增益和损害成分，并探究其背后的机制。通过对两个视觉语言模型和六个基准测试的检查点分析发现，性能提升主要由内在学习驱动，而工具使用强化学习主要减少了工具诱导的损害——例如减少了不必要的工具调用错误并削弱了已有工具模式的干扰——而在利用工具纠正模型内在失败方面进展有限。</div>
</details>
</div>
<div class="card">
<div class="title">Med3D-R1: Incentivizing Clinical Reasoning in 3D Medical Vision-Language Models for Abnormality Diagnosis</div>
<div class="meta-line">Authors: Haoran Lai, Zihang Jiang, Kun Zhang, Qingsong Yao, Rongsheng Wang, Zhiyang He, Xiaodong Tao, Wei Wei, Shaohua Kevin Zhou</div>
<div class="meta-line">First: 2026-02-01T12:43:11+00:00 · Latest: 2026-02-01T12:43:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01200v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01200v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing 3D vision-language models with robust clinical reasoning remains a challenge due to the inherent complexity of volumetric medical imaging, the tendency of models to overfit superficial report patterns, and the lack of interpretability-aware reward designs. In this paper, we propose Med3D-R1, a reinforcement learning framework with a two-stage training process: Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL). During SFT stage, we introduce a residual alignment mechanism to bridge the gap between high-dimensional 3D features and textual embeddings, and an abnormality re-weighting strategy to emphasize clinically informative tokens and reduce structural bias in reports. In RL stage, we redesign the consistency reward to explicitly promote coherent, step-by-step diagnostic reasoning. We evaluate our method on medical multiple-choice visual question answering using two 3D diagnostic benchmarks, CT-RATE and RAD-ChestCT, where our model attains state-of-the-art accuracies of 41.92\% on CT-RATE and 44.99\% on RAD-ChestCT. These results indicate improved abnormality diagnosis and clinical reasoning and outperform prior methods on both benchmarks. Overall, our approach holds promise for enhancing real-world diagnostic workflows by enabling more reliable and transparent 3D medical vision-language systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Med3D-R1：激励三维医学视觉语言模型在异常诊断中的临床推理能力</div>
<div class="mono" style="margin-top:8px">由于三维医学影像的固有复杂性、模型易过拟合报告表层模式，以及缺乏可解释性奖励设计，开发具备稳健临床推理能力的三维视觉语言模型仍面临挑战。本文提出Med3D-R1强化学习框架，采用两阶段训练：监督微调（SFT）与强化学习（RL）。SFT阶段引入残差对齐机制以弥合高维三维特征与文本嵌入的鸿沟，并采用异常重加权策略以突出临床信息标记、减少报告结构偏差。RL阶段重新设计一致性奖励，显式促进连贯的逐步诊断推理。我们在CT-RATE和RAD-ChestCT两个三维诊断基准上通过医学多选视觉问答进行评估，模型分别取得41.92%和44.99%的最优准确率。结果表明该方法提升了异常诊断与临床推理能力，在两项基准上均超越现有方法。总体而言，本方法通过构建更可靠、透明的三维医学视觉语言系统，有望增强真实世界的诊断工作流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of developing 3D vision-language models with robust clinical reasoning for medical diagnosis, which is hindered by the complexity of volumetric imaging, model overfitting to superficial report patterns, and a lack of interpretability-aware training. The proposed Med3D-R1 framework employs a two-stage reinforcement learning process: a Supervised Fine-Tuning stage with a residual alignment mechanism and an abnormality re-weighting strategy to better link 3D visual features with text, followed by a Reinforcement Learning stage with a redesigned consistency reward to explicitly incentivize coherent, step-by-step diagnostic reasoning. Experimental evaluation on the CT-RATE and RAD-ChestCT benchmarks for medical visual question answering shows the model achieves state-of-the-art accuracies of 41.92% and 44.99%, respectively, demonstrating improved abnormality diagnosis and clinical reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决开发具备稳健临床推理能力的3D医学视觉-语言模型所面临的挑战，包括模型容易过拟合报告表层模式以及缺乏可解释性奖励设计等问题。提出的Med3D-R1方法采用两阶段强化学习框架：监督微调阶段引入了残差对齐机制以弥合3D视觉特征与文本嵌入之间的差距，并采用异常重新加权策略以强调临床信息丰富的标记；强化学习阶段则重新设计了连贯性奖励，以明确激励逐步、连贯的诊断推理。在CT-RATE和RAD-ChestCT两个3D诊断基准上的医学视觉问答实验表明，该模型分别达到了41.92%和44.99%的最先进准确率，证明了其在异常诊断和临床推理方面的提升。</div>
</details>
</div>
<div class="card">
<div class="title">NAP-Tuning: Neural Augmented Prompt Tuning for Adversarially Robust Vision-Language Models</div>
<div class="meta-line">Authors: Jiaming Zhang, Xin Wang, Xingjun Ma, Lingyu Qiu, Yu-Gang Jiang, Jitao Sang</div>
<div class="meta-line">First: 2025-06-15T03:34:23+00:00 · Latest: 2026-02-01T12:19:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12706v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12706v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) such as CLIP have demonstrated remarkable capabilities in understanding relationships between visual and textual data through joint embedding spaces. Despite their effectiveness, these models remain vulnerable to adversarial attacks, particularly in the image modality, posing significant security concerns. Building upon our previous work on Adversarial Prompt Tuning (AdvPT), which introduced learnable text prompts to enhance adversarial robustness in VLMs without extensive parameter training, we present a significant extension by introducing the Neural Augmentor framework for Multi-modal Adversarial Prompt Tuning (NAP-Tuning).Our key innovations include: (1) extending AdvPT from text-only to multi-modal prompting across both text and visual modalities, (2) expanding from single-layer to multi-layer prompt architectures, and (3) proposing a novel architecture-level redesign through our Neural Augmentor approach, which implements feature purification to directly address the distortions introduced by adversarial attacks in feature space. Our NAP-Tuning approach incorporates token refiners that learn to reconstruct purified features through residual connections, allowing for modality-specific and layer-specific feature correction.Comprehensive experiments demonstrate that NAP-Tuning significantly outperforms existing methods across various datasets and attack types. Notably, our approach shows significant improvements over the strongest baselines under the challenging AutoAttack benchmark, outperforming them by 33.5% on ViT-B16 and 33.0% on ViT-B32 architectures while maintaining competitive clean accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NAP-Tuning：面向对抗鲁棒视觉语言模型的神经增强提示调优</div>
<div class="mono" style="margin-top:8px">视觉语言模型（如CLIP）通过联合嵌入空间在理解视觉与文本数据关联方面展现出卓越能力，但其图像模态易受对抗攻击，存在严重安全隐患。基于先前通过可学习文本提示提升模型对抗鲁棒性的对抗提示调优（AdvPT）工作，本文提出多模态对抗提示调优的神经增强框架（NAP-Tuning），核心创新包括：（1）将AdvPT从纯文本提示扩展至文本与视觉双模态提示；（2）从单层提示架构升级为多层架构；（3）通过神经增强器实现架构级重构，采用特征净化机制直接修复对抗攻击导致的特征空间失真。NAP-Tuning通过残差连接重构净化特征的令牌优化器，实现模态与层级特定的特征校正。实验表明，该方法在多种数据集与攻击类型中显著优于现有方案，在AutoAttack基准测试中，ViT-B16与ViT-B32架构分别以33.5%和33.0%的优势超越基线模型，且保持优异的标准准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models like CLIP are effective but vulnerable to adversarial image attacks, creating security risks. To address this, the authors extend their prior Adversarial Prompt Tuning (AdvPT) work by proposing NAP-Tuning, a method that introduces multi-modal learnable prompts for both text and vision, employs a multi-layer prompt architecture, and features a novel Neural Augmentor. This augmentor uses token refiners with residual connections to perform feature purification, directly correcting adversarial distortions in the feature space. Experiments show NAP-Tuning substantially outperforms existing methods, achieving improvements of 33.5% and 33.0% over strong baselines on the AutoAttack benchmark for ViT-B16 and ViT-B32 models respectively, while preserving clean accuracy.</div>
<div class="mono" style="margin-top:8px">CLIP等视觉语言模型虽然有效，但容易受到图像对抗攻击，这推动了对更鲁棒方法的研究。本研究扩展了先前的对抗提示调优工作，提出了神经增强提示调优（NAP-Tuning），该方法在文本和视觉模态上使用多模态可学习提示、多层提示架构，以及一个新颖的神经增强器，其通过残差连接进行特征纯化的令牌细化器来校正对抗性失真。实验表明，NAP-Tuning显著优于现有方法，在AutoAttack基准下，相比最强基线在ViT-B16和ViT-B32架构上分别提升了33.5%和33.0%，同时保持了有竞争力的干净准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Koo-Fu CLIP: Closed-Form Adaptation of Vision-Language Models via Fukunaga-Koontz Linear Discriminant Analysis</div>
<div class="meta-line">Authors: Matej Suchanek, Klara Janouskova, Ondrej Vasatko, Jiri Matas</div>
<div class="meta-line">First: 2026-02-01T09:56:25+00:00 · Latest: 2026-02-01T09:56:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01127v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual-language models such as CLIP provide powerful general-purpose representations, but their raw embeddings are not optimized for supervised classification, often exhibiting limited class separation and excessive dimensionality. We propose Koo-Fu CLIP, a supervised CLIP adaptation method based on Fukunaga-Koontz Linear Discriminant Analysis, which operates in a whitened embedding space to suppress within-class variation and enhance between-class discrimination. The resulting closed-form linear projection reshapes the geometry of CLIP embeddings, improving class separability while performing effective dimensionality reduction, and provides a lightweight and efficient adaptation of CLIP representations.
  Across large-scale ImageNet benchmarks, nearest visual prototype classification in the Koo-Fu CLIP space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains persisting as the label space expands to 14K and 21K classes. The method supports substantial compression by up to 10-12x with little or no loss in accuracy, enabling efficient large-scale classification and retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Koo-Fu CLIP：基于福永-库恩茨线性判别分析的视觉语言模型闭式自适应方法</div>
<div class="mono" style="margin-top:8px">CLIP等视觉语言模型虽能提供强大的通用表征，但其原始嵌入向量未针对监督分类任务优化，常存在类间区分度有限与维度冗余的问题。本文提出Koo-Fu CLIP——一种基于福永-库恩茨线性判别分析的监督式CLIP自适应方法，该方法在白化嵌入空间中运作，能有效抑制类内差异并增强类间判别力。通过闭式线性投影重构CLIP嵌入的几何结构，在实现有效降维的同时提升类别可分性，为CLIP表征提供轻量高效的自适应方案。在大型ImageNet基准测试中，Koo-Fu CLIP空间内的最近视觉原型分类将ImageNet-1K的top-1准确率从75.1%提升至79.1%，当标签空间扩展至1.4万及2.1万类别时仍保持稳定增益。该方法支持10-12倍的显著压缩率且精度损失极小，为高效的大规模分类与检索任务提供了可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limited class separation and excessive dimensionality of raw CLIP embeddings for supervised classification tasks. The proposed Koo-Fu CLIP method adapts vision-language models by applying Fukunaga-Koontz Linear Discriminant Analysis in a whitened embedding space, which suppresses within-class variation and enhances between-class discrimination through a closed-form linear projection. Experimental results on ImageNet benchmarks show that nearest prototype classification in the adapted space improves top-1 accuracy from 75.1% to 79.1% on ImageNet-1K, with consistent gains on larger label sets, while supporting 10-12x dimensionality compression with minimal accuracy loss.</div>
<div class="mono" style="margin-top:8px">针对原始CLIP嵌入在监督分类中存在的类间区分度有限和维度冗余问题，本研究提出了Koo-Fu CLIP，一种基于Fukunaga-Koontz线性判别分析的闭式适配方法。该方法在白化嵌入空间中运行，通过线性投影抑制类内差异并增强类间判别力，从而重塑嵌入几何以提升可分性并实现降维。在ImageNet基准上的实验结果表明，在适配空间中进行最近邻原型分类将ImageNet-1K的top-1准确率从75.1%提升至79.1%，且该增益在扩展到14K和21K类别时保持稳定，同时支持10-12倍的压缩而几乎不损失精度。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
