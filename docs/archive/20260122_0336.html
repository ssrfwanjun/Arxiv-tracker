<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 03:36</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0336</div>
    <div class="row"><div class="card">
<div class="title">VideoMaMa: Mask-Guided Video Matting via Generative Prior</div>
<div class="meta-line">Authors: Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim, Joon-Young Lee</div>
<div class="meta-line">First: 2026-01-20T18:59:56+00:00 · Latest: 2026-01-20T18:59:56+00:00</div>
<div class="meta-line">Comments: Project page: https://cvlab-kaist.github.io/VideoMaMa/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14255v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14255v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cvlab-kaist.github.io/VideoMaMa/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoMaMa：基于生成先验的掩码引导视频抠图</div>
<div class="mono" style="margin-top:8px">由于标注数据稀缺，将视频抠图模型泛化至真实世界视频仍面临重大挑战。为此，我们提出视频掩码转蒙版模型（VideoMaMa），通过利用预训练视频扩散模型，将粗粒度分割掩码转换为像素级精确的阿尔法蒙版。尽管仅使用合成数据训练，VideoMaMa在真实世界影像上展现出强大的零样本泛化能力。基于此能力，我们开发了可扩展的大规模视频抠图伪标注流程，并构建了视频通用抠图（MA-V）数据集，该数据集为涵盖多样场景与运动的超过5万条真实世界视频提供了高质量抠图标注。为验证该数据集的有效性，我们在MA-V上对SAM2模型进行微调得到SAM2-Matte，其在真实场景视频的鲁棒性方面优于基于现有抠图数据集训练的同类模型。这些发现凸显了大规模伪标注视频抠图的重要性，并展示了生成先验与易得分割线索如何推动视频抠图研究的可扩展进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generalizing video matting models to real-world videos due to limited labeled data by introducing VideoMaMa, which uses coarse segmentation masks and leverages pretrained video diffusion models to generate accurate alpha mattes. The method demonstrates strong zero-shot generalization on real footage despite being trained only on synthetic data, and it enables a scalable pseudo-labeling pipeline to create the MA-V dataset with high-quality annotations for over 50K diverse real-world videos. Experimental results show that fine-tuning SAM2 on MA-V yields SAM2-Matte, which outperforms models trained on existing datasets in robustness on in-the-wild videos, highlighting the value of large-scale pseudo-labeled data and generative priors for advancing video matting.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决因标注数据稀缺而导致的视频抠图模型难以泛化到真实世界视频的挑战。方法提出了VideoMaMa，通过利用预训练的视频扩散模型，将粗略的分割掩码转换为精确的alpha遮罩，仅使用合成数据训练即可实现强大的零样本泛化能力。关键实验结果包括构建了包含超过5万个伪标注视频的大规模MA-V数据集，并证明在该数据集上微调的SAM2-Matte模型在真实视频的鲁棒性上优于基于现有数据集训练的模型。</div>
</details>
</div>
<div class="card">
<div class="title">APEX-Agents</div>
<div class="meta-line">Authors: Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski</div>
<div class="meta-line">First: 2026-01-20T18:53:44+00:00 · Latest: 2026-01-20T18:53:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APEX-Agents</div>
<div class="mono" style="margin-top:8px">我们推出AI智能体生产力指数（APEX-Agents），这是一个用于评估AI智能体能否执行由投行分析师、管理顾问和企业律师创建的跨应用长周期任务的基准测试。APEX-Agents要求智能体在包含文件与工具的真实工作环境中进行操作。我们采用Pass@1指标对八个智能体进行排行榜测试，Gemini 3 Flash（思考模式=高）以24.0%得分位居榜首，其后依次为GPT-5.2（思考模式=高）、Claude Opus 4.5（思考模式=高）和Gemini 3 Pro（思考模式=高）。我们开源了包含全部提示词、评分标准、标准答案、文件及元数据的APEX-Agents基准测试集（n=480），同时开源了用于智能体执行与评估的基础设施Archipelago。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate AI agents on complex, real-world professional tasks that span multiple applications and require long-term planning, as encountered in fields like investment banking and management consulting. The method introduces the APEX-Agents benchmark, which simulates realistic work environments with files and tools, and employs a Pass@1 metric to test agent performance across 480 tasks. Key experimental results show that Gemini 3 Flash (Thinking=High) achieved the highest score of 24.0%, followed by GPT-5.2, Claude Opus 4.5, and Gemini 3 Pro, all with high thinking settings, while the benchmark and an accompanying infrastructure named Archipelago are open-sourced.</div>
<div class="mono" style="margin-top:8px">该研究的动机是评估AI智能体在投资银行、管理咨询等专业领域中执行跨应用、长周期复杂现实任务的能力。方法上提出了APEX-Agents基准测试，通过模拟包含文件和工具的真实工作环境，使用Pass@1指标对480项任务进行评估。主要实验结果表明，Gemini 3 Flash（Thinking=High）以24.0%的最高得分领先，优于GPT-5.2和Claude Opus 4.5等其他主流模型，同时该基准测试及名为Archipelago的执行评估基础设施已开源。</div>
</details>
</div>
<div class="card">
<div class="title">Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration</div>
<div class="meta-line">Authors: LSST Dark Energy Science Collaboration, Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Raphaël Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra Ćiprijanović, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Juan de Vicente, Seth W. Digel, Steven Dillmann, Mariano Javier de León Dominguez Romero, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Mustapha Ishak, Wassim Kabalan, Arun Kannawadi, François Lanusse, C. Danielle Leonard, Pierre-François Léget, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais Möller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Laurence Perreault-Levasseur, Andrés A. Plazas Malagón, Nesar Ramachandra, Benjamin Remy, Cécile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Ved G. Shah, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Tröster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang, Yuanyuan Zhang</div>
<div class="meta-line">First: 2026-01-20T18:46:42+00:00 · Latest: 2026-01-20T18:46:42+00:00</div>
<div class="meta-line">Comments: 84 pages. This is v1.0 of the DESC&#x27;s white paper on AI/ML, a collaboration document that is being made public but which is not planned for submission to a journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14235v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Vera C. Rubin Observatory&#x27;s Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC&#x27;s primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁宾LSST暗能量科学合作中人工智能/机器学习的机遇</div>
<div class="mono" style="margin-top:8px">薇拉·C·鲁宾天文台的时空遗产巡天（LSST）将产生前所未有的海量异构天文数据（图像、星表和警报），这对传统分析流程构成挑战。LSST暗能量科学合作（DESC）旨在从这些数据中获取对暗能量和暗物质的可靠约束，需要具备统计效力强、可扩展且运行可靠的方法。人工智能和机器学习（AI/ML）已深度融入DESC科学工作流，涵盖光度红移、瞬变天体分类、弱引力透镜推断到宇宙学模拟等领域。然而，其在精密宇宙学中的应用价值取决于可信的不确定性量化、对协变量偏移和模型误设的鲁棒性，以及在科学流程中的可复现集成。本白皮书系统梳理了AI/ML在DESC主要宇宙学探针和交叉分析中的应用现状，揭示相同核心方法论与基础挑战在不同科学案例中反复出现。由于这些交叉挑战的突破将同时惠及多个探针，我们明确了关键方法论研究重点，包括大规模贝叶斯推断、物理信息融合方法、验证框架及用于发现的主动学习。着眼新兴技术，我们还探讨了最新基础模型方法论与LLM驱动的智能体AI系统重塑DESC工作流的潜力，前提是其部署需结合严格评估与治理机制。最后，我们讨论了成功部署这些新方法所需的关键软件、计算、数据基础设施与人才资源，并考量了与外部机构广泛协作的相关风险与机遇。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Rubin Observatory&#x27;s LSST will generate vast, heterogeneous astronomical data, posing challenges for traditional analysis in dark energy and dark matter research. This white paper surveys how AI/ML is currently applied across DESC workflows, such as for photometric redshifts and weak lensing, but emphasizes that precision cosmology requires addressing cross-cutting challenges like trustworthy uncertainty quantification and robustness to data shifts. It identifies key research priorities including scalable Bayesian inference and physics-informed methods, explores the potential of foundation models and agentic AI, and discusses the necessary software, computing, and human capital for successful deployment.</div>
<div class="mono" style="margin-top:8px">鲁宾天文台的LSST将产生海量异构数据，这对暗能量和暗物质研究的传统分析方法构成挑战，需要具备统计效力且可扩展的新方法。本白皮书调研了AI/ML在DESC工作流（如光度红移和弱引力透镜）中的现有应用，但指出其在精密宇宙学中的有效性取决于解决共性挑战，如可靠的不确定性量化和对数据偏移的鲁棒性。报告确定了包括可扩展贝叶斯推断和物理信息方法在内的关键研究重点，探讨了基础模型和智能体AI的潜力，并讨论了成功部署所需的关键软件、计算和人力资源基础设施。</div>
</details>
</div>
<div class="card">
<div class="title">Q-learning with Adjoint Matching</div>
<div class="meta-line">Authors: Qiyang Li, Sergey Levine</div>
<div class="meta-line">First: 2026-01-20T18:45:34+00:00 · Latest: 2026-01-20T18:45:34+00:00</div>
<div class="meta-line">Comments: 32 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14234v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14234v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic&#x27;s action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伴随匹配Q学习</div>
<div class="mono" style="margin-top:8px">我们提出伴随匹配Q学习（QAM），这是一种基于时序差分的新型强化学习算法，解决了连续动作强化学习中长期存在的挑战：针对参数化Q函数高效优化表达能力强的扩散或流匹配策略。有效优化需要利用评论家的一阶信息，但对流或扩散策略而言，通过多步去噪过程进行基于梯度的反向传播优化存在数值不稳定性。现有方法要么仅使用价值函数而丢弃梯度信息，要么依赖近似方法牺牲策略表达能力或引入偏差。QAM通过运用生成建模中最新提出的伴随匹配技术，将评论家的动作梯度转化为逐步目标函数，既避免了不稳定的反向传播，又在最优解处提供无偏且表达能力强的策略。结合评论家学习的时序差分更新，QAM在离线及离线到在线强化学习的困难稀疏奖励任务中持续超越现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of efficiently optimizing expressive diffusion or flow-matching policies in continuous-action reinforcement learning, where direct gradient-based optimization through multi-step denoising is numerically unstable. The proposed method, Q-learning with Adjoint Matching (QAM), leverages adjoint matching to transform the critic&#x27;s action gradient into a step-wise objective, avoiding unstable backpropagation while maintaining an unbiased and expressive policy. Experimental results show that QAM consistently outperforms prior methods on hard, sparse reward tasks in both offline and offline-to-online RL settings.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决连续动作强化学习中高效优化表达性扩散或流匹配策略的挑战，其中通过多步去噪的直接梯度优化存在数值不稳定性。所提出的方法——伴随匹配Q学习（QAM）——利用伴随匹配技术将评论者的动作梯度转换为逐步目标函数，避免了不稳定的反向传播，同时保持了无偏且表达性强的策略。实验结果表明，在离线和离线到在线强化学习的困难稀疏奖励任务上，QAM一致优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning</div>
<div class="meta-line">Authors: Egor Cherepanov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov</div>
<div class="meta-line">First: 2026-01-20T18:44:28+00:00 · Latest: 2026-01-20T18:44:28+00:00</div>
<div class="meta-line">Comments: 38 pages, 44 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://avanturist322.github.io/KAGEBench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAGE-Bench：面向强化学习的快速已知轴视觉泛化评估基准</div>
<div class="mono" style="margin-top:8px">基于像素的强化学习智能体常在纯视觉分布偏移下失效，即使潜在动态与奖励机制未变，但现有基准常混杂多种偏移源而阻碍系统分析。我们提出KAGE-Env——一个基于JAX的2D平台环境，它将观测过程分解为独立可控的视觉轴，同时保持底层控制问题不变。通过结构设计，改变视觉轴仅通过像素策略引发的状态条件动作分布影响性能，为视觉泛化提供清晰抽象框架。基于此环境，我们构建KAGE-Bench基准，包含6个已知轴测试套件共34组训练-评估配置对，可隔离单一视觉偏移。采用标准PPO-CNN基线实验发现：背景与光度偏移常导致任务完全失败，而智能体外貌偏移影响相对较小；部分偏移能维持前进动作却破坏任务完成，表明仅凭回报指标可能掩盖泛化缺陷。全向量化JAX实现支持单GPU每秒3300万环境步长，实现视觉因素的高效可复现扫描。代码：https://avanturist322.github.io/KAGEBench/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge that pixel-based reinforcement learning agents often fail under visual distribution shifts even when underlying dynamics remain unchanged, but existing benchmarks conflate multiple sources of shift, hindering systematic analysis. To isolate individual visual factors, the authors introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the core control problem fixed, and define KAGE-Bench, a benchmark comprising six known-axis suites with 34 train-evaluation pairs. Experimental results using a standard PPO-CNN baseline reveal strong axis-dependent failures, where background and photometric shifts often cause complete collapse in success, while agent-appearance shifts are relatively benign, and several shifts preserve forward motion but break task completion, indicating that return alone can mask generalization failures; the fully vectorized JAX implementation also enables up to 33 million environment steps per second on a single GPU for fast, reproducible sweeps.</div>
<div class="mono" style="margin-top:8px">本研究针对基于像素的强化学习智能体在潜在动态和奖励不变的情况下，常因纯视觉分布变化而失败的问题，但现有基准测试混杂了多种变化来源，阻碍了系统分析。为分离个体视觉变化，作者引入了KAGE-Env，这是一个JAX原生的2D平台游戏环境，它将观察过程分解为独立可控的视觉轴，同时保持底层控制问题不变，并在此基础上定义了KAGE-Bench基准，包含六个已知轴套件，共34个训练-评估配置对。使用标准PPO-CNN基线的实验结果显示，存在强烈的轴依赖性失败：背景和光度变化常导致成功率完全崩溃，而智能体外观变化相对温和；一些变化在保持前进运动的同时破坏了任务完成，表明仅凭回报可能掩盖泛化失败。完全向量化的JAX实现使得在单个GPU上每秒可执行高达3300万环境步数，从而支持对视觉因素的快速、可重复扫描。</div>
</details>
</div>
<div class="card">
<div class="title">MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems</div>
<div class="meta-line">Authors: Yiyang Wang, Yiqiao Jin, Alex Cabral, Josiah Hester</div>
<div class="meta-line">First: 2026-01-20T18:44:04+00:00 · Latest: 2026-01-20T18:44:04+00:00</div>
<div class="meta-line">Comments: 15 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14230v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14230v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MASCOT：迈向多智能体社会协作伴侣系统</div>
<div class="mono" style="margin-top:8px">多智能体系统（MAS）近年来已成为情感与认知支持领域极具前景的社会协作伴侣。然而，这类系统常面临角色崩溃（智能体退化为通用化、同质化的助手行为）与社会迎合（产生冗余、非建设性对话）的问题。本文提出MASCOT——一种可泛化的多视角社会协作伴侣框架。MASCOT引入新颖的双层优化策略以协调个体与集体行为：1）角色感知行为对齐：基于RLAIF的微调流程，确保个体智能体严格遵循角色设定以防止身份丢失；2）协作对话优化：通过群体级奖励引导的元策略，保障对话的多样性与建设性。在心理支持与工作场景的大规模评估表明，MASCOT在角色一致性（提升+14.1）与社会贡献度（提升+10.6）上显著优于现有基线，为构建下一代社会智能多智能体系统提供了实用技术路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-agent systems often suffer from persona collapse and social sycophancy, leading to generic and unproductive interactions. To address this, the authors propose MASCOT, a framework featuring a bi-level optimization strategy with Persona-Aware Behavioral Alignment for individual agent fidelity and Collaborative Dialogue Optimization for productive group discourse. Experimental results in psychological support and workplace scenarios show MASCOT outperforms baselines, improving Persona Consistency by up to +14.1 and Social Contribution by +10.6.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决用于社会协作陪伴的多智能体系统中存在的人格崩溃和社会谄媚问题，这些问题导致智能体行为趋同且对话缺乏建设性。提出的MASCOT框架采用双层优化策略，结合了通过RLAIF实现的人格感知行为对齐以保持个体智能体的人格，以及利用群体级奖励元策略的协作对话优化以促进多样化和建设性的交流。在心理支持和职场场景中的实验结果表明，MASCOT显著优于现有基线方法，在人格一致性上提升了高达14.1分，在社会贡献度上提升了10.6分。</div>
</details>
</div>
<div class="card">
<div class="title">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</div>
<div class="meta-line">Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-19T17:55:48+00:00 · Latest: 2026-01-20T18:25:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 25 figures. The first four authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17853v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17853v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTask：面向仿真到现实策略学习进阶的自动化任务与数据生成框架</div>
<div class="mono" style="margin-top:8px">通用机器人学习仍受数据制约：大规模、多样化且高质量的交互数据在现实世界中采集成本高昂。仿真虽已成为扩展数据收集的有效途径，但相关任务——包括仿真任务设计、任务感知场景生成、专家示范合成及仿真到现实迁移——仍需大量人工投入。本文提出AnyTask，一个结合大规模并行GPU仿真与基础模型的自动化框架，用于设计多样化操作任务并合成机器人数据。我们引入三种AnyTask智能体以生成解决尽可能多任务的专家示范：1) ViPR，一种集成视觉语言模型循环并行优化的新型任务与运动规划智能体；2) ViPR-Eureka，一种结合生成密集奖励与大型语言模型引导接触采样的强化学习智能体；3) ViPR-RL，一种联合规划与学习的混合方法，仅通过稀疏奖励即可生成高质量示范。基于生成数据训练行为克隆策略，在仿真中验证后直接部署于真实机器人硬件。该策略能泛化至新物体位姿，在一系列真实世界抓放、抽屉开启、密集接触推动及长程操作任务中平均成功率可达44%。项目网站：https://anytask.rai-inst.com</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the data bottleneck in generalist robot learning by automating the generation of diverse manipulation tasks and expert demonstrations in simulation to reduce human effort. The method introduces the AnyTask framework, which leverages GPU-accelerated simulation and foundation models to create tasks and synthesize data, featuring three specialized agents: ViPR for task and motion planning with visual language model refinement, ViPR-Eureka for reinforcement learning with dense rewards and contact sampling, and ViPR-RL for hybrid planning with sparse rewards. Experimental results show that behavior cloning policies trained on this generated data achieve a 44% average success rate when deployed directly on real robots, generalizing to novel object poses across pick-and-place, drawer opening, contact-rich pushing, and long-horizon tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对通用机器人学习中的数据瓶颈问题，旨在自动化生成模拟环境中的多样化操作任务和专家演示，这些过程传统上需要大量人工投入。提出的AnyTask框架结合了大规模并行GPU模拟和基础模型，包含三个专用智能体：ViPR利用视觉语言模型进行循环优化以实现任务和运动规划，ViPR-Eureka通过生成密集奖励和大型语言模型引导的接触采样进行强化学习，ViPR-RL则采用稀疏奖励进行混合规划与学习。实验结果表明，基于生成数据训练的行为克隆策略直接部署到真实机器人上时，在拾放、抽屉开启、接触丰富的推动和长时程操作任务中，对新物体姿态实现了44%的平均成功率。</div>
</details>
</div>
<div class="card">
<div class="title">InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</div>
<div class="meta-line">Authors: Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-20T18:15:38+00:00 · Latest: 2026-01-20T18:15:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14209v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InT：自提干预实现大语言模型推理中的信用分配</div>
<div class="mono" style="margin-top:8px">结果奖励强化学习（RL）已被证明能有效提升大语言模型（LLM）的推理能力。然而，标准RL仅对最终答案层面分配信用：当结果错误时惩罚整个推理轨迹，结果正确时则均匀强化所有步骤。这导致失败轨迹中的正确中间步骤可能被抑制，而成功轨迹中的无效步骤反而被强化。我们将此失效模式称为信用分配问题。虽然训练过程奖励模型是自然的解决方案，但精准优化此类模型以识别纠正性推理步骤仍具挑战性。本文提出干预训练（InT），该训练范式使模型通过提出简短、有针对性的修正（引导轨迹获得更高奖励）对自身推理轨迹进行细粒度信用分配。利用数学推理数据集中普遍存在的参考答案，并基于验证模型生成解比从头生成正确解更简单的事实，模型首先识别其推理中的首个错误，提出单步干预以将轨迹导向正确解，随后对错误发生前的策略轨迹（拼接干预步骤）进行监督微调（SFT），从而将错误定位至导致失败的具体步骤。实验表明，所得模型可作为RL训练更优的初始化参数。经过InT及后续RL微调，我们在IMO-AnswerBench上将4B参数基模型的准确率提升近14%，性能超越gpt-oss-20b等更大规模开源模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the credit assignment problem in outcome-reward reinforcement learning for large language models, where entire reasoning traces are penalized or reinforced uniformly based on final answer correctness, potentially discouraging correct intermediate steps or reinforcing spurious ones. The method, Intervention Training (InT), enables the model to perform fine-grained credit assignment by identifying the first error in its own reasoning trace using available reference solutions and proposing a single-step intervention to steer the trajectory toward the correct solution, followed by supervised fine-tuning on the corrected rollout. Experimental results show that InT significantly improves model initialization for subsequent RL training, leading to a nearly 14% accuracy gain over a 4B-parameter base model on IMO-AnswerBench and outperforming larger open-source models.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型在基于结果奖励的强化学习中存在的信用分配问题，即仅根据最终答案的正确性对整个推理轨迹进行统一惩罚或强化，这可能导致正确的中间步骤被抑制或错误的步骤被强化。提出的干预训练方法使模型能够通过识别自身推理轨迹中的首个错误，并提出单步干预以引导轨迹朝向正确解决方案，从而进行细粒度信用分配，随后对纠正后的轨迹进行监督微调。实验结果表明，干预训练为后续强化学习训练提供了更好的初始化，在IMO-AnswerBench上将4B参数基础模型的准确率提升了近14%，并超越了更大的开源模型。</div>
</details>
</div>
<div class="card">
<div class="title">GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization</div>
<div class="meta-line">Authors: Jingxing Li, Yongjae Lee, Deliang Fan</div>
<div class="meta-line">First: 2025-09-27T01:21:38+00:00 · Latest: 2026-01-20T18:07:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23038v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23038v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and state-of-the-art regression accuracy, yet our analysis reveals subtle geometric inconsistencies in its internal representations that prevent reaching the precision ceiling of correspondence-based methods like MASt3R (which require 300ms per pair). In this work, we present GeLoc3r, a novel approach to relative camera pose estimation that enhances pose regression methods through Geometric Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma by training regression networks to produce geometrically consistent poses without inference-time geometric computation. During training, GeLoc3r leverages ground-truth depth to generate dense 3D-2D correspondences, weights them using a FusionTransformer that learns correspondence importance, and computes geometrically-consistent poses via weighted RANSAC. This creates a consistency loss that transfers geometric knowledge into the regression network. Unlike FAR method which requires both regression and geometric solving at inference, GeLoc3r only uses the enhanced regression head at test time, maintaining ReLoc3R&#x27;s fast speed and approaching MASt3R&#x27;s high accuracy. On challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving significant improvements including 40.45% vs. 34.85% AUC@5° on the CO3Dv2 dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5° on RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric consistency during training rather than enforcing it at inference, GeLoc3r represents a paradigm shift in how neural networks learn camera geometry, achieving both the speed of regression and the geometric understanding of correspondence methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeLoc3r：通过几何一致性正则化增强相对相机位姿回归</div>
<div class="mono" style="margin-top:8px">先前ReLoc3R以25ms快速推理和顶尖回归精度取得突破性性能，但我们的分析发现其内部表征存在细微的几何不一致性，阻碍其达到基于对应点方法（如MASt3R需300ms/对）的精度上限。本研究提出GeLoc3r——一种通过几何一致性正则化增强位姿回归方法的相对相机位姿估计新方法。GeLoc3r通过训练回归网络生成几何一致位姿而无需推理时几何计算，从而克服速度-精度权衡。训练阶段，GeLoc3r利用真实深度生成稠密3D-2D对应点，通过FusionTransformer学习对应点重要性进行加权，并基于加权RANSAC计算几何一致位姿，由此构建将几何知识迁移至回归网络的一致性损失。与FAR方法需同时运行回归和几何求解不同，GeLoc3r在测试时仅使用增强的回归头，既保持ReLoc3R的快速推理特性，又逼近MASt3R的高精度水平。在挑战性基准测试中，GeLoc3r全面超越ReLoc3R：CO3Dv2数据集AUC@5°从34.85%提升至40.45%（相对提升16%），RealEstate10K从66.70%提升至68.66%，MegaDepth1500从49.60%提升至50.45%。通过在训练中教授几何一致性而非推理时强制约束，GeLoc3r实现了神经网络学习相机几何的范式转变，兼具回归方法的速度优势与对应点方法的几何理解能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the geometric inconsistencies in fast relative camera pose regression methods like ReLoc3R, which limit their accuracy compared to slower correspondence-based approaches. The proposed GeLoc3r method introduces Geometric Consistency Regularization (GCR) during training, where it uses ground-truth depth to generate dense correspondences, weights them with a learned FusionTransformer, and computes geometrically consistent poses via weighted RANSAC to create a consistency loss that informs the regression network. Experiments show that GeLoc3r significantly outperforms ReLoc3R, achieving a 16% relative improvement in AUC@5° on CO3Dv2 (40.45% vs. 34.85%) and superior results on RealEstate10K and MegaDepth1500, while maintaining fast 25ms inference without geometric computation at test time.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决快速相对相机姿态回归方法（如ReLoc3R）中存在的几何不一致性问题，该问题限制了其精度，使其无法达到基于对应点方法的水平。提出的GeLoc3r方法在训练中引入了几何一致性正则化：利用真实深度生成密集对应点，通过FusionTransformer学习其重要性，并使用加权RANSAC计算几何一致的姿态以构建一致性损失，从而将几何知识迁移到回归网络中。实验结果表明，GeLoc3r显著优于ReLoc3R，在CO3Dv2数据集上的AUC@5°指标实现了16%的相对提升（40.45%对34.85%），在RealEstate10K和MegaDepth1500数据集上也表现更优，同时在测试时无需几何计算，保持了25ms的快速推理速度。</div>
</details>
</div>
<div class="card">
<div class="title">DiffusionAgent: Navigating Expert Models for Agentic Image Generation</div>
<div class="meta-line">Authors: Jie Qin, Jie Wu, Weifeng Chen, Yueming Lyu</div>
<div class="meta-line">First: 2024-01-18T15:30:58+00:00 · Latest: 2026-01-20T18:02:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.10061v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.10061v2">PDF</a> · <a href="https://github.com/DiffusionAgent/DiffusionAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the accelerating era of human-instructed visual content creation, diffusion models have demonstrated remarkable generative potential. Yet their deployment is constrained by a dual bottleneck: semantic ambiguity in diverse prompts and the narrow specialization of individual models. A single diffusion architecture struggles to maintain optimal performance across heterogeneous prompts, while conventional &quot;parse-then-call&quot; pipelines artificially separate semantic understanding from generative execution. To bridge this gap, we introduce DiffusionAgent, a unified, language-model-driven agent that casts the entire &quot;prompt comprehension-expert routing-image synthesis&quot; loop into a agentic framework. Our contributions are three-fold: (1) a tree-of-thought-powered expert navigator that performs fine-grained semantic parsing and zero-shot matching to the most suitable diffusion model via an extensible prior-knowledge tree; (2) an advantage database updated with human-in-the-loop feedback, continually aligning model-selection policy with human aesthetic and semantic preferences; and (3) a fully decoupled agent architecture that activates the optimal generative path for open-domain prompts without retraining or fine-tuning any expert. Extensive experiments show that DiffusionAgent retains high generation quality while significantly broadening prompt coverage, establishing a new performance and generality benchmark for multi-domain image synthesis. The code is available at https://github.com/DiffusionAgent/DiffusionAgent</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionAgent：面向智能图像生成的专家模型导航框架</div>
<div class="mono" style="margin-top:8px">在人类指令驱动的视觉内容创作加速时代，扩散模型已展现出卓越的生成潜力，但其应用受限于双重瓶颈：多样化提示的语义模糊性及单一模型的狭窄专业性。传统扩散架构难以在异构提示中保持最优性能，而常规“解析-调用”流程人为割裂了语义理解与生成执行。为此，我们提出DiffusionAgent——一个基于语言模型的统一智能体，将完整的“提示理解-专家路由-图像合成”流程整合至智能体框架。我们的贡献包括：（1）基于思维树的专家导航器，通过可扩展的先验知识树实现细粒度语义解析与零样本匹配，精准对接最适配的扩散模型；（2）融入人机交互反馈的优势数据库，持续优化模型选择策略以对齐人类审美与语义偏好；（3）完全解耦的智能体架构，无需重训练或微调专家模型即可为开放域提示激活最优生成路径。大量实验表明，DiffusionAgent在显著拓宽提示覆盖范围的同时保持高质量生成效果，为多领域图像合成设立了性能与泛化能力的新基准。代码已开源：https://github.com/DiffusionAgent/DiffusionAgent</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses two key limitations in diffusion-based image generation: semantic ambiguity in diverse prompts and the narrow specialization of individual models, which struggle with heterogeneous inputs. The proposed DiffusionAgent is a unified, language-model-driven framework that integrates prompt comprehension, expert routing, and image synthesis into an agentic loop. Its core method features a tree-of-thought-powered expert navigator for fine-grained semantic parsing and zero-shot matching to suitable diffusion models, an advantage database updated with human feedback to align model selection with preferences, and a decoupled architecture that activates optimal generative paths without retraining experts. Experimental results demonstrate that DiffusionAgent maintains high generation quality while significantly broadening prompt coverage, establishing a new benchmark for performance and generality in multi-domain image synthesis.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决扩散模型图像生成中的两个关键限制：多样化提示的语义模糊性以及单个模型的狭窄专业性，这些问题阻碍了模型在异构输入上的最佳性能。提出的方法DiffusionAgent是一个统一的、由语言模型驱动的智能体，它将提示理解、专家路由和图像合成集成到一个框架中。该方法采用思维树驱动的专家导航器进行细粒度语义解析和零样本匹配以选择合适的扩散模型，利用人类反馈更新的优势数据库来使模型选择策略与人类审美和语义偏好对齐，并采用完全解耦的架构在不重新训练专家模型的情况下激活最优生成路径。大量实验表明，DiffusionAgent在保持高生成质量的同时显著拓宽了提示覆盖范围，为多领域图像合成设立了新的性能和通用性基准。</div>
</details>
</div>
<div class="card">
<div class="title">KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments</div>
<div class="meta-line">Authors: Junyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee, Chris Lott</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-04-21T18:12:46+00:00 · Latest: 2026-01-20T17:55:29+00:00</div>
<div class="meta-line">Comments: 37 pages, 19 figures, NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.15364v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.15364v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We demonstrate that geometrically distinctive keys during LLM inference tend to have high attention scores. Based on the phenomenon we propose KeyDiff, a training-free KV cache eviction method based solely on key similarity. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses. We provide a theoretical basis for KeyDiff by relating key diversity with attention scores. These results imply KeyDiff can efficiently identify the most important tokens to retain. Notably KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. Under a strict memory allowance, we demonstrate the effectiveness of KeyDiff for the Llama and Qwen model families by observing a performance gap of less than 0.04% with 8K cache budget ($\sim$23% KV cache reduction) from the non-evicting baseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near baseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning benchmark and decrease end-to-end inference latency by up to 30% compared to the other token-eviction methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KeyDiff：基于键相似性的KV缓存淘汰方法，用于资源受限环境下的长上下文LLM推理</div>
<div class="mono" style="margin-top:8px">我们证明，在LLM推理过程中几何特征显著的键往往具有较高的注意力分数。基于这一现象，我们提出了KeyDiff，一种仅依赖键相似性的免训练KV缓存淘汰方法。与其他KV缓存淘汰方法不同，KeyDiff能在严格资源限制下处理任意长提示，并高效生成响应。我们通过关联键多样性与注意力分数，为KeyDiff提供了理论基础。这些结果表明KeyDiff能有效识别需保留的关键词元。值得注意的是，KeyDiff不依赖注意力分数，因此可与FlashAttention等优化注意力机制兼容。在严格内存限制下，我们在Llama和Qwen模型系列上验证了KeyDiff的有效性：在LongBench基准测试中，Llama 3.1-8B和Llama 3.2-3B使用8K缓存预算（约减少23% KV缓存）时，与非淘汰基线相比性能差距小于0.04%。在Math500推理基准测试中，Deepseek-R1-Distill-Llama-8B模型性能接近基线，且相比其他词元淘汰方法，端到端推理延迟降低达30%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable long-context LLM inference under strict memory constraints, this work introduces KeyDiff, a training-free KV cache eviction method that relies solely on key similarity. The method is motivated by the observation that geometrically distinctive keys correspond to high attention scores; it identifies and retains the most important tokens by measuring key diversity, without requiring attention scores, thus allowing compatibility with optimized attention kernels like FlashAttention. Experimental results on Llama and Qwen models show that with an 8K cache budget (approximately 23% KV cache reduction), performance drops by less than 0.04% compared to the non-evicting baseline on LongBench, while on the Math500 reasoning benchmark, Deepseek-R1-Distill-Llama-8B maintains near-baseline performance and achieves up to 30% lower inference latency than other eviction methods.</div>
<div class="mono" style="margin-top:8px">为在严格内存限制下实现长上下文大语言模型推理，本研究提出了KeyDiff，一种仅基于键相似性的免训练KV缓存淘汰方法。该方法的动机是观察到几何上独特的键往往对应较高的注意力分数；它通过识别并保留键最不相似的令牌来淘汰缓存，从理论上建立了键多样性与注意力重要性之间的联系。在Llama和Qwen模型系列上的实验结果表明，在8K缓存预算下（约减少23% KV缓存），LongBench上的性能相比不淘汰的基线下降小于0.04%；在Math500推理基准上，性能接近基线，且端到端推理延迟比其他淘汰方法降低了高达30%。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Alignment of Multilingual Knowledge Graphs via Contextualized Vector Projections</div>
<div class="meta-line">Authors: Abhishek Kumar</div>
<div class="meta-line">First: 2025-12-22T11:02:30+00:00 · Latest: 2026-01-20T17:52:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00814v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00814v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The paper presents our work on cross-lingual ontology alignment system which uses embedding based cosine similarity matching. The ontology entities are made contextually richer by creating descriptions using novel techniques. We use a fine-tuned transformer based multilingual model for generating better embeddings. We use cosine similarity to find positive ontology entities pairs and then apply threshold filtering to retain only highly similar entities. We have evaluated our work on OAEI-2022 multifarm track. We achieve 71% F1 score (78% recall and 65% precision) on the evaluation dataset, 16% increase from best baseline score. This suggests that our proposed alignment pipeline is able to capture the subtle cross-lingual similarities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于上下文向量投影的多语言知识图谱语义对齐</div>
<div class="mono" style="margin-top:8px">本文介绍了一种基于嵌入余弦相似度匹配的跨语言本体对齐系统。通过创新技术生成描述，使本体实体在语境上更丰富。我们采用基于微调Transformer的多语言模型以生成更优嵌入，利用余弦相似度寻找正例本体实体对，并通过阈值过滤保留高相似度实体。在OAEI-2022多语言农场赛道评估中，我们的系统在测试集上取得71%的F1值（召回率78%，精确率65%），较最佳基线提升16%，表明所提出的对齐流程能有效捕捉细微的跨语言相似性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve cross-lingual ontology alignment by addressing the challenge of capturing subtle semantic similarities across languages. The method enriches ontology entities with contextually generated descriptions and employs a fine-tuned multilingual transformer model to produce embeddings, followed by cosine similarity matching and threshold filtering to identify aligned entity pairs. Experimental evaluation on the OAEI-2022 multifarm track shows a 71% F1 score, representing a 16% improvement over the best baseline, with 78% recall and 65% precision, demonstrating the pipeline&#x27;s effectiveness in capturing cross-lingual similarities.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过捕捉跨语言的细微语义相似性来改进跨语言本体对齐。该方法利用新颖技术生成描述以丰富本体实体的上下文信息，并采用微调的多语言Transformer模型生成嵌入表示，随后使用余弦相似度进行匹配并通过阈值过滤保留高置信度实体对。在OAEI-2022 multifarm赛道上的评估结果显示，该对齐流程取得了71%的F1分数（召回率78%，精确率65%），相比最佳基线提升了16%，表明其能有效对齐多语言知识图谱。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Efficient Agents: Memory, Tool learning, and Planning</div>
<div class="meta-line">Authors: Xiaofang Yang, Lijun Li, Heng Zhou, Tong Zhu, Xiaoye Qu, Yuchen Fan, Qianshan Wei, Rui Ye, Li Kang, Yiran Qin, Zhiqiang Kou, Daizong Liu, Qi Li, Ning Ding, Siheng Chen, Jing Shao</div>
<div class="meta-line">First: 2026-01-20T17:51:56+00:00 · Latest: 2026-01-20T17:51:56+00:00</div>
<div class="meta-line">Comments: 35 pages, 200 references</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent years have witnessed increasing interest in extending large language models into agentic systems. While the effectiveness of agents has continued to improve, efficiency, which is crucial for real-world deployment, has often been overlooked. This paper therefore investigates efficiency from three core components of agents: memory, tool learning, and planning, considering costs such as latency, tokens, steps, etc. Aimed at conducting comprehensive research addressing the efficiency of the agentic system itself, we review a broad range of recent approaches that differ in implementation yet frequently converge on shared high-level principles including but not limited to bounding context via compression and management, designing reinforcement learning rewards to minimize tool invocation, and employing controlled search mechanisms to enhance efficiency, which we discuss in detail. Accordingly, we characterize efficiency in two complementary ways: comparing effectiveness under a fixed cost budget, and comparing cost at a comparable level of effectiveness. This trade-off can also be viewed through the Pareto frontier between effectiveness and cost. From this perspective, we also examine efficiency oriented benchmarks by summarizing evaluation protocols for these components and consolidating commonly reported efficiency metrics from both benchmark and methodological studies. Moreover, we discuss the key challenges and future directions, with the goal of providing promising insights.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向高效智能体：记忆、工具学习与规划</div>
<div class="mono" style="margin-top:8px">近年来，将大语言模型扩展为智能体系统的研究日益受到关注。尽管智能体的效能持续提升，但对其实际部署至关重要的效率问题却常被忽视。本文从智能体的三个核心组成部分——记忆、工具学习和规划——出发，结合延迟、令牌数、步骤数等成本因素，系统探讨效率问题。为全面研究智能体系统自身的效率，我们综述了近期多种实现各异但常遵循共同高层原则的方法，包括但不限于通过压缩与管理限制上下文、设计强化学习奖励以最小化工具调用、采用受控搜索机制提升效率等，并对此展开详细讨论。我们通过两种互补方式刻画效率：在固定成本预算下比较效能，以及在可比效能水平下比较成本。这种权衡亦可从效能与成本的帕累托前沿视角理解。基于此，我们还通过总结各组件评估方案、整合基准与方法研究中常用效率指标，审视了面向效率的基准测试。此外，本文讨论了关键挑战与未来方向，旨在提供有价值的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve efficiency for real-world deployment of large language model agents, this paper reviews approaches to enhance three core components: memory, tool learning, and planning, considering costs like latency and token usage. The method involves analyzing a broad range of recent techniques that share high-level principles such as context compression, reward design to minimize tool calls, and controlled search mechanisms. Key experimental findings characterize efficiency through a trade-off between effectiveness and cost, often visualized via Pareto frontiers, and the study consolidates evaluation protocols and metrics to benchmark these efficiency-oriented improvements.</div>
<div class="mono" style="margin-top:8px">本文针对基于大语言模型的智能体部署中常被忽视的效率问题，聚焦于记忆、工具学习和规划三个核心组件，并考虑了延迟、令牌使用等成本。研究方法是对近期各类方法进行综合评述，这些方法虽实现各异但共享高级原则，如通过压缩和管理限制上下文、设计强化学习奖励以减少工具调用、以及采用受控搜索机制提升效率。主要实验结果表明，效率可通过固定成本预算下的有效性比较或同等有效性水平下的成本比较来表征，常以帕累托前沿进行分析，研究还整合了相关评估协议和常用效率指标，以基准化这些权衡关系。</div>
</details>
</div>
<div class="card">
<div class="title">The Dog the Cat Chased Stumped the Model: Measuring When Language Models Abandon Structure for Shortcuts</div>
<div class="meta-line">Authors: Sangmitra Madhusudan, Kaige Chen, Ali Emami</div>
<div class="meta-line">First: 2025-10-23T13:30:40+00:00 · Latest: 2026-01-20T17:46:36+00:00</div>
<div class="meta-line">Comments: 9 pages (excluding references), accepted to EACL 2026 Main Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20543v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.20543v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When language models correctly parse &quot;The cat that the dog chased meowed,&quot; are they analyzing syntax or simply familiar with dogs chasing cats? Despite extensive benchmarking, we lack methods to distinguish structural understanding from semantic pattern matching. We introduce CenterBench, a dataset of 9,720 comprehension questions on center-embedded sentences (like &quot;The cat [that the dog chased] meowed&quot;) where relative clauses nest recursively, creating processing demands from simple to deeply nested structures. Each sentence has a syntactically identical but semantically implausible counterpart (e.g., mailmen prescribe medicine, doctors deliver mail) and six comprehension questions testing surface understanding, syntactic dependencies, and causal reasoning. Testing six models reveals that performance gaps between plausible and implausible sentences widen systematically with complexity, with models showing median gaps up to 26.8 percentage points, quantifying when they abandon structural analysis for semantic associations. Notably, semantic plausibility harms performance on questions about resulting actions, where following causal relationships matters more than semantic coherence. Reasoning models improve accuracy but their traces show semantic shortcuts, overthinking, and answer refusal. Unlike models whose plausibility advantage systematically widens with complexity, humans shows variable semantic effects. CenterBench provides the first framework to identify when models shift from structural analysis to pattern matching.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>被猫追逐的狗难倒了模型：测量语言模型何时放弃结构分析转向语义捷径</div>
<div class="mono" style="margin-top:8px">当语言模型正确解析&quot;被狗追逐的猫发出喵叫&quot;时，它们是在分析句法结构还是仅依赖&#x27;狗追猫&#x27;的语义模式？尽管已有大量基准测试，我们仍缺乏区分结构理解与语义模式匹配的方法。本研究提出CenterBench数据集，包含9,720个关于中心嵌套句（如&quot;被[狗追逐的]猫发出喵叫&quot;）的理解问题，其中关系从句递归嵌套，形成从简单到深层嵌套的处理需求。每个句子均配有句法相同但语义不合理的对照版本（如&quot;邮递员开药方，医生送邮件&quot;）及六个测试表层理解、句法依赖和因果推理的问题。对六个模型的测试显示：合理句与不合理句的性能差距随复杂度系统性扩大，模型的中位差距最高达26.8个百分点，量化了其放弃结构分析转向语义关联的临界点。值得注意的是，语义合理性反而损害对动作结果类问题的表现——这类问题中因果关系的推理比语义连贯性更重要。推理模型虽提升准确率，但其思维轨迹仍显示语义捷径、过度思考和答案拒绝现象。与模型随复杂度系统性扩大合理性优势不同，人类受语义影响存在波动。CenterBench首次提供了识别模型从结构分析转向模式匹配的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of distinguishing whether language models genuinely parse syntactic structures or merely rely on semantic pattern matching when processing complex sentences. The authors introduce CenterBench, a dataset of 9,720 comprehension questions based on center-embedded sentences, which range from simple to deeply nested structures; each sentence has a syntactically identical but semantically implausible counterpart, and questions test surface understanding, syntactic dependencies, and causal reasoning. Experiments on six models reveal that performance gaps between plausible and implausible sentences widen systematically with structural complexity, with median gaps reaching up to 26.8 percentage points, indicating that models increasingly abandon structural analysis for semantic shortcuts as complexity increases. Notably, semantic plausibility can harm performance on questions about resulting actions, where causal reasoning is critical, and while reasoning models improve accuracy, their traces still exhibit overthinking and reliance on semantic associations, unlike humans who show variable semantic effects.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决一个关键问题：语言模型在处理复杂句子时，究竟是真正解析句法结构，还是仅仅依赖语义模式匹配。作者引入了CenterBench数据集，包含9,720个基于中心嵌套句的理解问题，这些句子带有递归关系从句，结构从简单到深度嵌套；每个句子都有一个句法相同但语义不合理的对应版本，问题测试表面理解、句法依赖和因果推理。对六个模型的实验表明，合理与不合理句子之间的性能差距随结构复杂性系统性地扩大，中位数差距高达26.8个百分点，这显示模型随着复杂性增加逐渐放弃结构分析而转向语义捷径；值得注意的是，语义合理性可能损害与动作相关问题的性能，因为这类问题需要因果推理，而尽管推理模型提高了准确性，其痕迹显示它们仍依赖捷径、过度思考和拒绝回答，这与人类表现中语义效应更多变的情况形成对比。</div>
</details>
</div>
<div class="card">
<div class="title">AlphaMapleSAT: An MCTS-based Cube-and-Conquer SAT Solver for Hard Combinatorial Problems</div>
<div class="meta-line">Authors: Piyush Jha, Zhengyu Li, Zhengyang Lu, Raymond Zeng, Curtis Bright, Vijay Ganesh</div>
<div class="meta-line">First: 2024-01-24T19:37:10+00:00 · Latest: 2026-01-20T17:44:29+00:00</div>
<div class="meta-line">Comments: Added more experiments</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.13770v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.13770v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces AlphaMapleSAT, a Cube-and-Conquer (CnC) parallel SAT solver that integrates Monte Carlo Tree Search (MCTS) with deductive feedback to efficiently solve challenging combinatorial SAT problems. Traditional lookahead cubing methods, used by solvers such as March, limit their search depth to reduce overhead often resulting in suboptimal partitions. By contrast, AlphaMapleSAT performs a deeper MCTS search guided by deductive rewards from SAT solvers. This approach enables informed exploration of the cubing space while keeping cubing costs low. We demonstrate the efficacy of our technique via extensive evaluations against the widely used and established March cubing solver on three well-known challenging combinatorial benchmarks, including the minimum Kochen-Specker (KS) problem from quantum mechanics, the Murty-Simon Conjecture, and the Ramsey problems from extremal graph theory. We compare AlphaMapleSAT against March using different types of conquering solvers such as SAT Modulo Symmetries (SMS) and SAT+CAS, both built on top of the CaDiCaL SAT solver. We show that in all cases, there is a speedup in elapsed real time (wall clock time) ranging from 1.61x to 7.57x on a 128 core machine for the above-mentioned problems. We also perform cube-level and parallel scaling analysis over 32, 64, and 128 cores, which shows that AlphaMapleSAT outperforms March on all these settings. Our results show that deductively-guided MCTS search technique for cubing in CnC solvers can significantly outperform March on hard combinatorial problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlphaMapleSAT：一种基于蒙特卡洛树搜索的立方体-征服SAT求解器，用于解决困难组合问题</div>
<div class="mono" style="margin-top:8px">本文介绍了AlphaMapleSAT，一种立方体-征服并行SAT求解器，它通过整合蒙特卡洛树搜索与演绎反馈，高效解决具有挑战性的组合SAT问题。传统的前瞻立方体划分方法（如March求解器所采用）为降低开销而限制搜索深度，常导致次优划分。相比之下，AlphaMapleSAT在SAT求解器提供的演绎奖励引导下进行更深层的MCTS搜索，从而在保持低划分成本的同时实现对立方体空间的智能探索。我们通过在三个知名困难组合基准测试（包括量子力学中的最小Kochen-Specker问题、Murty-Simon猜想以及极值图论中的Ramsey问题）上，与广泛使用的March立方体求解器进行广泛评估，证明了该方法的有效性。我们使用基于CaDiCaL SAT求解器构建的SAT模对称性与SAT+CAS等不同类型的征服求解器，将AlphaMapleSAT与March进行对比。结果显示，在128核机器上，针对上述问题，实际运行时间加速比达到1.61倍至7.57倍。我们还进行了32、64和128核的立方体级别与并行扩展分析，表明AlphaMapleSAT在所有配置下均优于March。研究证明，在立方体-征服求解器中采用演绎引导的MCTS立方体划分技术，能在困难组合问题上显著超越March求解器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of traditional lookahead cubing methods in Cube-and-Conquer SAT solvers, which often restrict search depth to manage overhead, leading to suboptimal partitions. The proposed AlphaMapleSAT solver integrates Monte Carlo Tree Search (MCTS) guided by deductive feedback from SAT solvers to perform deeper, more informed exploration of the cubing space while maintaining low cubing costs. Extensive evaluations on hard combinatorial benchmarks, including the minimum Kochen-Specker problem, the Murty-Simon Conjecture, and Ramsey problems, demonstrate that AlphaMapleSAT achieves speedups in wall-clock time ranging from 1.61x to 7.57x on a 128-core machine compared to the March solver, and consistently outperforms March across 32, 64, and 128 cores in cube-level and parallel scaling analyses.</div>
<div class="mono" style="margin-top:8px">针对传统前瞻立方体划分方法在搜索深度和划分质量上的局限，本文提出了AlphaMapleSAT，一种基于蒙特卡洛树搜索（MCTS）与演绎反馈相结合的立方体-征服并行SAT求解器。该方法利用SAT求解器提供的演绎奖励引导MCTS进行更深的立方体空间探索，同时控制划分开销，并与基于CaDiCaL的SAT Modulo Symmetries（SMS）和SAT+CAS等征服求解器结合。在包括量子力学中的最小Kochen-Specker问题、Murty-Simon猜想以及极值图论中的Ramsey问题在内的硬组合基准测试上，实验结果表明，在128核机器上，AlphaMapleSAT相比March立方体求解器在实时运行时间上实现了1.61倍至7.57倍的加速，且在32、64和128核配置下均表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">A model of errors in transformers</div>
<div class="meta-line">Authors: Suvrat Raju, Praneeth Netrapalli</div>
<div class="meta-line">First: 2026-01-20T17:27:03+00:00 · Latest: 2026-01-20T17:27:03+00:00</div>
<div class="meta-line">Comments: 8+17pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14175v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the error rate of LLMs on tasks like arithmetic that require a deterministic output, and repetitive processing of tokens drawn from a small set of alternatives. We argue that incorrect predictions arise when small errors in the attention mechanism accumulate to cross a threshold, and use this insight to derive a quantitative two-parameter relationship between the accuracy and the complexity of the task. The two parameters vary with the prompt and the model; they can be interpreted in terms of an elementary noise rate, and the number of plausible erroneous tokens that can be predicted. Our analysis is inspired by an ``effective field theory&#x27;&#x27; perspective: the LLM&#x27;s many raw parameters can be reorganized into just two parameters that govern the error rate. We perform extensive empirical tests, using Gemini 2.5 Flash, Gemini 2.5 Pro and DeepSeek R1, and find excellent agreement between the predicted and observed accuracy for a variety of tasks, although we also identify deviations in some cases. Our model provides an alternative to suggestions that errors made by LLMs on long repetitive tasks indicate the ``collapse of reasoning&#x27;&#x27;, or an inability to express ``compositional&#x27;&#x27; functions. Finally, we show how to construct prompts to reduce the error rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Transformer模型中的错误模型</div>
<div class="mono" style="margin-top:8px">我们研究大语言模型在需要确定性输出的任务（如算术）以及从小规模备选词集中重复处理词元时的错误率。我们认为，当注意力机制中的微小误差累积超过阈值时会产生错误预测，并基于此推导出任务准确率与复杂度之间的定量双参数关系。这两个参数随提示词和模型变化，可解释为基础噪声率与可能被预测的错误词元数量。我们的分析受“有效场论”视角启发：大语言模型的众多原始参数可重组为仅控制错误率的两个参数。我们使用Gemini 2.5 Flash、Gemini 2.5 Pro和DeepSeek R1进行了大量实证测试，发现多种任务的预测准确率与观测值高度吻合（尽管某些案例存在偏差）。该模型为“大语言模型在长重复任务中的错误表明‘推理崩溃’或无法表达‘组合式’函数”的观点提供了替代解释。最后，我们展示了如何构建提示词以降低错误率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates why large language models (LLMs) make errors on deterministic, repetitive tasks like arithmetic, where outputs are drawn from a small set of tokens. The authors propose that errors stem from the accumulation of small inaccuracies in the attention mechanism, which eventually cross a critical threshold. This insight leads to a quantitative two-parameter model relating task complexity to accuracy, where the parameters represent an elementary noise rate and the number of plausible erroneous tokens. Inspired by an effective field theory perspective, the model condenses many raw parameters into these two governing factors. Extensive empirical tests with models including Gemini 2.5 Flash, Gemini 2.5 Pro, and DeepSeek R1 show excellent agreement between predicted and observed accuracy across various tasks, though some deviations are noted. The findings challenge interpretations that such errors indicate a fundamental &quot;collapse of reasoning&quot; or inability to handle compositionality, and the work demonstrates how prompts can be constructed to reduce error rates.</div>
<div class="mono" style="margin-top:8px">本研究调查了大语言模型在算术等确定性、重复性任务上的错误率，这类任务的输出来自一个小的令牌集合。作者提出，错误源于注意力机制中微小不准确性的积累，最终超过一个临界阈值。这一见解导出了一个定量的双参数模型，将任务复杂度与准确率联系起来，其中参数代表基本噪声率和可能错误令牌的数量。受有效场论视角启发，该模型将许多原始参数浓缩为这两个主导因素。通过对包括Gemini 2.5 Flash、Gemini 2.5 Pro和DeepSeek R1在内的模型进行广泛实证测试，结果显示预测准确率与观测准确率在各种任务上高度吻合，但也发现了一些偏差。这些发现挑战了将错误解释为推理根本性崩溃或无法处理组合函数的观点，并展示了如何通过构建提示来降低错误率。</div>
</details>
</div>
<div class="card">
<div class="title">Human Values in a Single Sentence: Moral Presence, Hierarchies, and Transformer Ensembles on the Schwartz Continuum</div>
<div class="meta-line">Authors: Víctor Yeste, Paolo Rosso</div>
<div class="meta-line">First: 2026-01-20T17:25:33+00:00 · Latest: 2026-01-20T17:25:33+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/VictorMYeste/human-value-detection, 37 pages, 4 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14172v1">PDF</a> · <a href="https://github.com/VictorMYeste/human-value-detection">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study sentence-level identification of the 19 values in the Schwartz motivational continuum as a concrete formulation of human value detection in text. The setting - out-of-context sentences from news and political manifestos - features sparse moral cues and severe class imbalance. This combination makes fine-grained sentence-level value detection intrinsically difficult, even for strong modern neural models. We first operationalize a binary moral presence task (&quot;does any value appear?&quot;) and show that it is learnable from single sentences (positive-class F1 $\approx$ 0.74 with calibrated thresholds). We then compare a presence-gated hierarchy to a direct multi-label classifier under matched compute, both based on DeBERTa-base and augmented with lightweight signals (prior-sentence context, LIWC-22/eMFD/MJD lexica, and topic features). The hierarchy does not outperform direct prediction, indicating that gate recall limits downstream gains. We also benchmark instruction-tuned LLMs - Gemma 2 9B, Llama 3.1 8B, Mistral 8B, and Qwen 2.5 7B - in zero-/few-shot and QLoRA setups and build simple ensembles; a soft-vote supervised ensemble reaches macro-F1 0.332, significantly surpassing the best single supervised model and exceeding prior English-only baselines. Overall, in this scenario, lightweight signals and small ensembles yield the most reliable improvements, while hierarchical gating offers limited benefit. We argue that, under an 8 GB single-GPU constraint and at the 7-9B scale, carefully tuned supervised encoders remain a strong and compute-efficient baseline for structured human value detection, and we outline how richer value structure and sentence-in-document context could further improve performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单句中的普世价值观：施瓦茨连续统上的道德存在性、层级结构与Transformer集成模型</div>
<div class="mono" style="margin-top:8px">本研究以施瓦茨动机连续统中的19种价值观为框架，探索文本中人类价值观的句子级检测任务。实验数据采用新闻和政治宣言中的脱语境单句，存在道德线索稀疏和严重类别不平衡的特点。这种组合使得细粒度句子级价值观检测具有内在难度，即使对现代强神经网络模型亦然。我们首先构建了二元道德存在性任务（“是否存在任何价值观？”），证明其可从单句学习（经阈值校准后正类F1≈0.74）。在同等计算量下，我们比较了基于DeBERTa-base的两种架构：存在性门控层级模型与直接多标签分类器，二者均辅以轻量级信号增强（前序句子上下文、LIWC-22/eMFD/MJD词典特征及主题特征）。层级模型未超越直接预测，表明门控召回率限制了下游增益。同时评估了指令调优大语言模型（Gemma 2 9B、Llama 3.1 8B、Mistral 8B、Qwen 2.5 7B）在零样本/少样本及QLoRA设置下的表现，并构建了简单集成模型：软投票监督集成达到宏观F1值0.332，显著超越最佳单监督模型并刷新了先前英文基准。总体而言，在此场景下，轻量级信号与小规模集成能带来最可靠的性能提升，而层级门控机制收益有限。我们认为，在8GB单GPU限制与7-9B参数规模下，精心调优的监督编码器仍是结构化人类价值观检测的强大且计算高效的基线方案，并指出更丰富的价值结构与文档级上下文可进一步提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of fine-grained human value detection at the sentence level, focusing on the 19 Schwartz values in out-of-context sentences from news and political manifestos, where moral cues are sparse and class imbalance is severe. The method compares a presence-gated hierarchical classifier against a direct multi-label classifier, both based on DeBERTa-base and augmented with lightweight features (prior-sentence context, lexica, and topic features), and benchmarks instruction-tuned LLMs (Gemma 2, Llama 3.1, Mistral, Qwen 2.5) in zero-shot, few-shot, and QLoRA setups. Key experimental results show that binary moral presence detection is learnable (positive-class F1 ≈ 0.74), the hierarchical approach does not outperform direct prediction due to gate recall limitations, and a soft-vote supervised ensemble achieves a macro-F1 of 0.332, significantly surpassing the best single supervised model and prior English-only baselines, with lightweight signals and small ensembles providing the most reliable improvements.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从新闻和政治宣言的单句中检测细粒度人类价值观的挑战，这些文本中道德线索稀疏且类别分布极不平衡。方法上，首先训练一个二元分类器检测道德存在性，然后比较基于DeBERTa-base并增强上下文和词汇特征的存在性门控分层模型与直接多标签分类器，并进一步在零样本、少样本和QLoRA配置下对指令调优的大语言模型进行基准测试。主要实验结果表明，二元存在性任务可学习（F1 ≈ 0.74），分层方法因门控召回限制未优于直接分类，而一个监督软投票集成模型达到了0.332的宏F1分数，超越了单个模型和先前基线，证明了在计算约束下轻量级特征和集成方法能带来可靠提升。</div>
</details>
</div>
<div class="card">
<div class="title">Paper2Rebuttal: A Multi-Agent Framework for Transparent Author Response Assistance</div>
<div class="meta-line">Authors: Qianli Ma, Chang Guo, Zhiheng Tian, Siyu Wang, Jipeng Xiao, Yuanhao Yue, Zhipeng Zhang</div>
<div class="meta-line">First: 2026-01-20T17:23:51+00:00 · Latest: 2026-01-20T17:23:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14171v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Writing effective rebuttals is a high-stakes task that demands more than linguistic fluency, as it requires precise alignment between reviewer intent and manuscript details. Current solutions typically treat this as a direct-to-text generation problem, suffering from hallucination, overlooked critiques, and a lack of verifiable grounding. To address these limitations, we introduce $\textbf{RebuttalAgent}$, the first multi-agents framework that reframes rebuttal generation as an evidence-centric planning task. Our system decomposes complex feedback into atomic concerns and dynamically constructs hybrid contexts by synthesizing compressed summaries with high-fidelity text while integrating an autonomous and on-demand external search module to resolve concerns requiring outside literature. By generating an inspectable response plan before drafting, $\textbf{RebuttalAgent}$ ensures that every argument is explicitly anchored in internal or external evidence. We validate our approach on the proposed $\textbf{RebuttalBench}$ and demonstrate that our pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, offering a transparent and controllable assistant for the peer review process. Code will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Paper2Rebuttal：面向透明化作者回复辅助的多智能体框架</div>
<div class="mono" style="margin-top:8px">撰写有效反驳是一项高风险任务，不仅需要语言流畅性，更要求精准对齐审稿人意图与稿件细节。现有方案通常将其视为直接文本生成问题，存在幻觉、遗漏批评及缺乏可验证依据等缺陷。为突破这些局限，我们提出首个多智能体框架$\textbf{RebuttalAgent}$，将反驳生成重构为以证据为中心的规划任务。该系统将复杂反馈分解为原子化问题，通过融合压缩摘要与高保真文本动态构建混合上下文，同时集成自主按需的外部检索模块以解决需外部文献支撑的问题。通过在起草前生成可检视的回复计划，$\textbf{RebuttalAgent}$确保每个论点均明确锚定于内部或外部证据。我们在提出的$\textbf{RebuttalBench}$上验证方法，证明该流程在覆盖度、忠实度与策略连贯性上优于强基线，为同行评审提供透明可控的辅助工具。代码将开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve rebuttal writing, which requires aligning reviewer critiques with manuscript details, as current direct-to-text generation methods often produce ungrounded or incomplete responses. The method introduces RebuttalAgent, a multi-agent framework that decomposes feedback into atomic concerns, constructs hybrid contexts by synthesizing compressed summaries with original text, and incorporates an autonomous external search module to address literature-dependent issues, generating an inspectable response plan before drafting. Experimental validation on RebuttalBench shows that the pipeline outperforms strong baselines in coverage, faithfulness, and strategic coherence, providing a transparent and controllable assistant for peer review.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决当前直接文本生成方法在撰写审稿回复时的局限性，如幻觉、遗漏批评和缺乏可验证依据。方法上提出了RebuttalAgent，这是一个多智能体框架，将回复生成重新定义为以证据为中心的规划任务，通过将复杂反馈分解为原子问题、从压缩摘要和高保真文本构建混合上下文，并集成自主外部搜索模块来处理文献需求。在RebuttalBench上的实验验证表明，该流程在覆盖度、忠实度和策略连贯性上优于强基线，为同行评审提供了一个透明且可控的辅助工具。</div>
</details>
</div>
<div class="card">
<div class="title">Domain-Adaptation through Synthetic Data: Fine-Tuning Large Language Models for German Law</div>
<div class="meta-line">Authors: Ali Hamza Bashir, Muhammad Rehan Khalid, Kostadin Cvejoski, Jana Birr, Jule Berghaus, Armin Berger, Sandra Halscheidt, Christian Temath, Rafet Sifa, David Berghaus</div>
<div class="meta-line">First: 2026-01-20T17:11:51+00:00 · Latest: 2026-01-20T17:11:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) often struggle in specialized domains such as legal reasoning due to limited expert knowledge, resulting in factually incorrect outputs or hallucinations. This paper presents an effective method for adapting advanced LLMs to German legal question answering through a novel synthetic data generation approach. In contrast to costly human-annotated resources or unreliable synthetic alternatives, our approach systematically produces high-quality, diverse, and legally accurate question-answer pairs directly from authoritative German statutes. Using rigorous automated filtering methods and parameter-efficient fine-tuning techniques, we demonstrate that LLMs adapted with our synthetic dataset significantly outperform their baseline counterparts on German legal question answering tasks. Our results highlight the feasibility of using carefully designed synthetic data as a robust alternative to manual annotation in high-stakes, knowledge-intensive domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于合成数据的领域自适应：面向德国法律的大语言模型微调</div>
<div class="mono" style="margin-top:8px">大语言模型在专业领域（如法律推理）常因专家知识有限而表现不佳，导致事实性错误或幻觉。本文提出一种通过新型合成数据生成方法，将先进大语言模型适配至德国法律问答任务的有效方案。相较于昂贵的人工标注资源或不可靠的合成替代方案，本方法直接从德国权威法规中系统生成高质量、多样化且法律准确的问答对。通过严格的自动化过滤方法与参数高效微调技术，我们证明采用本合成数据集适配的大语言模型在德国法律问答任务上显著优于基线模型。研究结果凸显了在高风险、知识密集型领域，精心设计的合成数据可作为人工标注的可靠替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models often lack the specialized knowledge required for accurate legal reasoning, leading to factual errors. To address this, the authors propose a method for adapting LLMs to German legal question answering by generating high-quality synthetic question-answer pairs directly from authoritative statutes, followed by automated filtering and parameter-efficient fine-tuning. Experimental results show that models fine-tuned with this synthetic data significantly outperform baseline models on German legal QA tasks, demonstrating that carefully designed synthetic data can effectively substitute for costly manual annotation in knowledge-intensive domains.</div>
<div class="mono" style="margin-top:8px">大型语言模型在专业领域（如法律推理）中常因缺乏专业知识而产生错误输出。为此，本研究提出一种方法，通过直接从权威德国法规生成高质量合成训练数据，并结合自动过滤与参数高效微调技术，使大型语言模型适应德国法律问答任务。实验结果表明，使用该合成数据微调的模型在德国法律问答任务上显著优于基线模型，证明精心设计的合成数据可作为知识密集型领域中人工标注的有效替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">ConceptCaps -- a Distilled Concept Dataset for Interpretability in Music Models</div>
<div class="meta-line">Authors: Bruno Sienkiewicz, Łukasz Neumann, Mateusz Modrzejewski</div>
<div class="meta-line">First: 2026-01-20T17:04:08+00:00 · Latest: 2026-01-20T17:04:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14157v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concept-based interpretability methods like TCAV require clean, well-separated positive and negative examples for each concept. Existing music datasets lack this structure: tags are sparse, noisy, or ill-defined. We introduce ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. Our pipeline separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts attribute lists into professional descriptions, and MusicGen synthesizes corresponding audio. This separation improves coherence and controllability over end-to-end approaches. We validate the dataset through audio-text alignment (CLAP), linguistic quality metrics (BERTScore, MAUVE), and TCAV analysis confirming that concept probes recover musically meaningful patterns. Dataset and code are available online.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ConceptCaps——面向音乐模型可解释性的蒸馏概念数据集</div>
<div class="mono" style="margin-top:8px">基于概念的可解释性方法（如TCAV）需要每个概念具备清晰分离的正负样本。现有音乐数据集缺乏这种结构：标签稀疏、含噪或定义模糊。我们推出ConceptCaps数据集，包含2.3万个音乐-文本-音频三元组，标注源自200个属性的分类体系。我们的流程将语义建模与文本生成分离：VAE学习合理的属性共现模式，微调LLM将属性列表转为专业描述，MusicGen合成对应音频。这种分离相比端到端方法提升了连贯性与可控性。我们通过音频-文本对齐（CLAP）、语言质量指标（BERTScore、MAUVE）及TCAV分析验证数据集，证实概念探针能提取具有音乐意义的模式。数据集与代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable concept-based interpretability methods like TCAV in music AI, which require clean concept definitions, this work introduces ConceptCaps, a dataset of 23k music-caption-audio triplets with explicit labels from a 200-attribute taxonomy. The method employs a pipeline that separates semantic modeling from text generation: a VAE learns plausible attribute co-occurrence patterns, a fine-tuned LLM converts these attribute lists into professional descriptions, and MusicGen synthesizes the corresponding audio, improving coherence and controllability over end-to-end approaches. Experimental validation shows strong audio-text alignment via CLAP, high linguistic quality measured by BERTScore and MAUVE, and TCAV analysis confirming that concept probes recover musically meaningful patterns.</div>
<div class="mono" style="margin-top:8px">为支持音乐AI中基于概念的可解释性方法（如TCAV），这些方法需要清晰的概念定义，本研究提出了ConceptCaps数据集，包含23k个音乐-描述-音频三元组，并带有来自200个属性分类法的明确标签。该方法采用了一个将语义建模与文本生成分离的流程：变分自编码器学习合理的属性共现模式，微调的大语言模型将属性列表转换为专业描述，MusicGen则合成对应的音频。实验验证通过音频-文本对齐（CLAP）和语言质量指标（BERTScore、MAUVE）表明，该方法相比端到端方法提升了连贯性，且TCAV分析证实概念探针能够恢复具有音乐意义的模式。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Augmented Intervenable Multimodal Adaptor for Post-operative Complication Prediction in Lung Cancer Surgery</div>
<div class="meta-line">Authors: Shubham Pandey, Bhavin Jawade, Srirangaraj Setlur, Venu Govindaraju, Kenneth Seastedt</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-20T16:58:12+00:00 · Latest: 2026-01-20T16:58:12+00:00</div>
<div class="meta-line">Comments: Accepted to P2P-CV @ WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14154v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14154v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Postoperative complications remain a critical concern in clinical practice, adversely affecting patient outcomes and contributing to rising healthcare costs. We present MIRACLE, a deep learning architecture for prediction of risk of postoperative complications in lung cancer surgery by integrating preoperative clinical and radiological data. MIRACLE employs a hyperspherical embedding space fusion of heterogeneous inputs, enabling the extraction of robust, discriminative features from both structured clinical records and high-dimensional radiological images. To enhance transparency of prediction and clinical utility, we incorporate an interventional deep learning module in MIRACLE, that not only refines predictions but also provides interpretable and actionable insights, allowing domain experts to interactively adjust recommendations based on clinical expertise. We validate our approach on POC-L, a real-world dataset comprising 3,094 lung cancer patients who underwent surgery at Roswell Park Comprehensive Cancer Center. Our results demonstrate that MIRACLE outperforms various traditional machine learning models and contemporary large language models (LLM) variants alone, for personalized and explainable postoperative risk management.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于肺癌术后并发症预测的LLM增强可干预多模态适配器</div>
<div class="mono" style="margin-top:8px">术后并发症仍是临床实践中的关键问题，对患者预后产生不利影响并推高医疗成本。我们提出MIRACLE深度学习架构，通过整合术前临床与影像学数据预测肺癌术后并发症风险。该架构采用超球面嵌入空间融合异构输入，能从结构化临床记录和高维影像中提取鲁棒的判别性特征。为提升预测透明度与临床实用性，我们在MIRACLE中引入干预式深度学习模块，不仅能优化预测结果，还可提供可解释、可操作的临床洞见，使领域专家能基于临床经验交互调整建议。我们在包含罗斯威尔公园综合癌症中心3094例肺癌手术患者的真实世界数据集POC-L上验证了该方法。结果表明，在个性化可解释术后风险管理方面，MIRACLE优于传统机器学习模型及现有大型语言模型变体。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Postoperative complications in lung cancer surgery significantly impact patient outcomes and healthcare costs, motivating the need for accurate and interpretable risk prediction. The proposed MIRACLE architecture integrates preoperative clinical and radiological data by fusing heterogeneous inputs into a hyperspherical embedding space to extract robust features, and incorporates an interventional deep learning module to refine predictions and provide actionable, interpretable insights for clinical adjustment. Experimental validation on a real-world dataset of 3,094 patients shows that MIRACLE outperforms traditional machine learning models and contemporary large language model variants in personalized, explainable risk management.</div>
<div class="mono" style="margin-top:8px">为应对肺癌手术术后风险管理的迫切需求，本研究提出了MIRACLE深度学习架构，该架构整合了术前临床与影像数据。该方法采用超球面嵌入空间融合异构输入，并引入干预模块以提供可解释、可操作的见解，使临床专家能基于专业知识交互式调整预测。在包含3,094名患者的真实世界数据集上的验证表明，MIRACLE在预测术后并发症方面优于多种传统机器学习模型及当代大语言模型变体。</div>
</details>
</div>
<div class="card">
<div class="title">Lost in the Prompt Order: Revealing the Limitations of Causal Attention in Language Models</div>
<div class="meta-line">Authors: Hyunjong Ok, Jaeho Lee</div>
<div class="meta-line">First: 2026-01-20T16:54:22+00:00 · Latest: 2026-01-20T16:54:22+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14152v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14152v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models exhibit surprising sensitivity to the structure of the prompt, but the mechanisms underlying this sensitivity remain poorly understood. In this work, we conduct an in-depth investigation on a striking case: in multiple-choice question answering, placing context before the questions and options (CQO) outperforms the reverse order (QOC) by over 14%p, consistently over a wide range of models and datasets. Through systematic architectural analysis, we identify causal attention as the core mechanism: in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck where context becomes invisible to options.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迷失于提示顺序：揭示语言模型中因果注意力的局限性</div>
<div class="mono" style="margin-top:8px">大型语言模型对提示结构表现出惊人的敏感性，但其内在机制尚不明确。本研究针对一个典型案例展开深入探究：在多项选择题回答任务中，将上下文置于问题和选项之前（CQO）的提示方式，相比反向顺序（QOC）在广泛模型与数据集上持续获得超过14%的性能提升。通过系统性架构分析，我们发现因果注意力是核心机制：在QOC提示中，因果掩码阻止选项词元关注上下文，形成信息瓶颈导致上下文对选项不可见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates why large language models show significant performance variations based on prompt structure, specifically observing that placing context before questions and options (CQO) consistently yields over 14% higher accuracy than the reverse order (QOC) across various models and datasets. Through systematic architectural analysis, the authors identify causal attention as the key mechanism, revealing that in QOC prompts, the causal mask prevents option tokens from attending to context, creating an information bottleneck that renders context invisible to options. Experimental results demonstrate this limitation, highlighting how causal attention constraints can substantially impact model performance in multiple-choice question answering tasks.</div>
<div class="mono" style="margin-top:8px">本研究动机源于观察到大型语言模型的表现对提示结构高度敏感，特别是在多项选择题回答中，将上下文置于问题和选项之前（CQO）的性能显著优于相反顺序（QOC）。研究方法通过系统性的架构分析来确定其内在机制，揭示了因果注意力是核心因素：在QOC提示中，因果掩码阻止了选项词元关注上下文，从而造成了信息瓶颈。关键实验结果表明，在多种模型和数据集上，CQO相比QOC的性能优势持续超过14个百分点，这证明了因果注意力在此场景下带来的关键局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics of Agentic Loops in Large Language Models: A Geometric Theory of Trajectories</div>
<div class="meta-line">Authors: Nicolas Tacheny</div>
<div class="meta-line">First: 2025-12-11T07:06:14+00:00 · Latest: 2026-01-20T16:35:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10350v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10350v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems built on large language models operate through recursive feedback loops, where each output becomes the next input. Yet the geometric behavior of these agentic loops (whether they converge, diverge, or exhibit more complex dynamics) remains poorly understood. This paper introduces a geometric framework for analyzing agentic trajectories in semantic embedding space, treating iterative transformations as discrete dynamical systems. We distinguish the artifact space, where linguistic transformations occur, from the embedding space, where geometric measurements are performed. Because cosine similarity is biased by embedding anisotropy, we introduce an isotonic calibration that eliminates systematic bias and aligns similarities with human semantic judgments while preserving high local stability. This enables rigorous measurement of trajectories, clusters and attractors. Through controlled experiments on singular agentic loops, we identify two fundamental regimes. A contractive rewriting loop converges toward a stable attractor with decreasing dispersion, while an exploratory summarize and negate loop produces unbounded divergence with no cluster formation. These regimes display qualitatively distinct geometric signatures of contraction and expansion. Our results show that prompt design directly governs the dynamical regime of an agentic loop, enabling systematic control of convergence, divergence and trajectory structure in iterative LLM transformations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中能动性循环的动力学：轨迹的几何理论</div>
<div class="mono" style="margin-top:8px">基于大语言模型的能动性系统通过递归反馈循环运作，其中每个输出成为下一个输入。然而，这些能动性循环的几何行为（无论它们是收敛、发散还是表现出更复杂的动力学特性）仍鲜为人知。本文引入了一个几何框架，用于分析语义嵌入空间中的能动性轨迹，将迭代变换视为离散动力系统。我们区分了发生语言变换的产物空间与执行几何测量的嵌入空间。由于余弦相似度受嵌入各向异性影响而产生偏差，我们引入了一种等渗校准方法，该方法能消除系统性偏差，使相似度与人类语义判断保持一致，同时保持较高的局部稳定性。这实现了对轨迹、聚类和吸引子的严格测量。通过对单一能动性循环的受控实验，我们识别出两种基本机制：收缩性改写循环会朝着稳定吸引子收敛且离散度递减，而探索性总结与否定循环则产生无界发散且不形成聚类。这些机制展现出收缩与扩张在性质上截然不同的几何特征。我们的结果表明，提示设计直接决定了能动性循环的动力学机制，从而能够系统性地控制迭代大语言模型变换中的收敛性、发散性和轨迹结构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the lack of understanding of the geometric behavior of agentic loops in large language models, where outputs recursively become inputs, by developing a geometric framework to analyze these trajectories as discrete dynamical systems in semantic embedding space. The method distinguishes between artifact and embedding spaces, introduces an isotonic calibration to correct cosine similarity bias from anisotropy, aligning it with human judgments while maintaining local stability, enabling rigorous measurement of trajectories, clusters, and attractors. Experimental results on singular loops reveal two fundamental regimes: contractive rewriting converges to stable attractors with decreasing dispersion, while exploratory summarization and negation leads to unbounded divergence without cluster formation, demonstrating that prompt design directly controls convergence, divergence, and trajectory structure.</div>
<div class="mono" style="margin-top:8px">本研究针对基于大语言模型的智能体系统中递归反馈循环的几何行为缺乏理解的问题展开，其中每个输出都作为下一个输入。作者提出了一个几何框架，将迭代变换视为离散动力系统，区分了人工产物空间和嵌入空间，并引入了一种等张校准方法来纠正嵌入各向异性导致的余弦相似度偏差，使其与人类语义判断一致同时保持局部稳定性。通过对单一智能体循环的受控实验，发现了两种基本机制：一种是收缩性重写循环，会收敛于一个稳定吸引子且分散度减小；另一种是探索性总结与否定循环，会导致无界发散且不形成聚类，这表明提示词设计直接控制着智能体循环的动力机制，从而能够系统性地管理收敛、发散和轨迹结构。</div>
</details>
</div>
<div class="card">
<div class="title">Style Transfer as Bias Mitigation: Diffusion Models for Synthetic Mental Health Text for Arabic</div>
<div class="meta-line">Authors: Saad Mankarious, Aya Zirikly</div>
<div class="meta-line">First: 2026-01-20T16:21:41+00:00 · Latest: 2026-01-20T16:21:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14124v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Synthetic data offers a promising solution for mitigating data scarcity and demographic bias in mental health analysis, yet existing approaches largely rely on pretrained large language models (LLMs), which may suffer from limited output diversity and propagate biases inherited from their training data. In this work, we propose a pretraining-free diffusion-based approach for synthetic text generation that frames bias mitigation as a style transfer problem. Using the CARMA Arabic mental health corpus, which exhibits a substantial gender imbalance, we focus on male-to-female style transfer to augment underrepresented female-authored content. We construct five datasets capturing varying linguistic and semantic aspects of gender expression in Arabic and train separate diffusion models for each setting. Quantitative evaluations demonstrate consistently high semantic fidelity between source and generated text, alongside meaningful surface-level stylistic divergence, while qualitative analysis confirms linguistically plausible gender transformations. Our results show that diffusion-based style transfer can generate high-entropy, semantically faithful synthetic data without reliance on pretrained LLMs, providing an effective and flexible framework for mitigating gender bias in sensitive, low-resource mental health domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>风格迁移作为偏差缓解：面向阿拉伯语心理健康文本生成的扩散模型</div>
<div class="mono" style="margin-top:8px">合成数据为缓解心理健康分析中的数据稀缺性和人口统计偏差提供了可行方案，但现有方法主要依赖预训练大语言模型（LLMs），其输出多样性有限且可能延续训练数据中的偏差。本研究提出一种无需预训练的扩散模型合成文本生成方法，将偏差缓解构建为风格迁移任务。基于存在显著性别失衡的CARMA阿拉伯语心理健康语料库，我们聚焦于男性至女性风格迁移以扩充代表性不足的女性创作内容。我们构建了五个数据集，分别捕捉阿拉伯语中性别表达的不同语言与语义特征，并为每种设定训练独立的扩散模型。定量评估显示生成文本与源文本间保持较高的语义保真度，同时实现有效的表层风格分化；定性分析证实了语言层面合理的性别转换。结果表明，基于扩散的风格迁移无需依赖预训练LLMs即可生成高熵值、语义忠实的合成数据，为敏感低资源心理健康领域的性别偏差缓解提供了灵活有效的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address data scarcity and demographic bias in mental health analysis, particularly the gender imbalance in Arabic mental health corpora, this work proposes a pretraining-free diffusion-based approach that frames bias mitigation as a style transfer problem. The method involves constructing five datasets capturing different linguistic and semantic aspects of gender expression from the CARMA Arabic mental health corpus and training separate diffusion models to perform male-to-female style transfer, thereby augmenting underrepresented female-authored content. Experimental results show the generated text maintains high semantic fidelity to the source while achieving meaningful stylistic divergence, with qualitative analysis confirming linguistically plausible gender transformations, demonstrating the approach&#x27;s effectiveness in generating diverse, faithful synthetic data without relying on pretrained large language models.</div>
<div class="mono" style="margin-top:8px">为解决心理健康分析中的数据稀缺和人口统计偏差问题，特别是阿拉伯语心理健康文本中的性别失衡，本研究提出了一种无需预训练的扩散模型方法，将偏差缓解构建为风格转换任务。该方法使用CARMA阿拉伯语心理健康语料库，构建了五个捕捉性别表达不同语言层面的数据集，并训练独立的扩散模型来执行男性到女性的风格转换，以增强代表性不足的女性创作内容。实验结果表明，生成的文本在保持与源文本高度语义保真度的同时，实现了有意义的表层风格差异，定性分析确认了语言上合理的性别转换，证明该方法能够在不依赖预训练大语言模型的情况下，生成多样化且语义忠实的合成数据。</div>
</details>
</div>
<div class="card">
<div class="title">Riemannian Liquid Spatio-Temporal Graph Network</div>
<div class="meta-line">Authors: Liangsi Lu, Jingchao Wang, Zhaorong Dai, Hanqian Liu, Yang Shi</div>
<div class="meta-line">Venue: The Web Conference 2026</div>
<div class="meta-line">First: 2026-01-20T16:09:05+00:00 · Latest: 2026-01-20T16:09:05+00:00</div>
<div class="meta-line">Comments: This paper has been accepted to The Web Conference 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14115v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14115v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rlstg.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Liquid Time-Constant networks (LTCs), a type of continuous-time graph neural network, excel at modeling irregularly-sampled dynamics but are fundamentally confined to Euclidean space. This limitation introduces significant geometric distortion when representing real-world graphs with inherent non-Euclidean structures (e.g., hierarchies and cycles), degrading representation quality. To overcome this limitation, we introduce the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), a framework that unifies continuous-time liquid dynamics with the geometric inductive biases of Riemannian manifolds. RLSTG models graph evolution through an Ordinary Differential Equation (ODE) formulated directly on a curved manifold, enabling it to faithfully capture the intrinsic geometry of both structurally static and dynamic spatio-temporal graphs. Moreover, we provide rigorous theoretical guarantees for RLSTG, extending stability theorems of LTCs to the Riemannian domain and quantifying its expressive power via state trajectory analysis. Extensive experiments on real-world benchmarks demonstrate that, by combining advanced temporal dynamics with a Riemannian spatial representation, RLSTG achieves superior performance on graphs with complex structures. Project Page: https://rlstg.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>黎曼流形时空图网络</div>
<div class="mono" style="margin-top:8px">液态时间常数网络（LTCs）作为一种连续时间图神经网络，擅长建模不规则采样的动态系统，但其本质上局限于欧几里得空间。这一限制在表示具有固有非欧结构（如层次结构和循环）的真实世界图时，会引入显著的几何失真，从而降低表示质量。为克服此限制，我们提出了黎曼流形时空图网络（RLSTG），该框架将连续时间液态动态与黎曼流形的几何归纳偏置相统一。RLSTG通过直接在弯曲流形上构建的常微分方程（ODE）来建模图演化，使其能够忠实捕捉结构静态和动态时空图的内在几何特性。此外，我们为RLSTG提供了严格的理论保证，将LTCs的稳定性定理扩展至黎曼域，并通过状态轨迹分析量化其表达能力。在真实世界基准测试上的大量实验表明，通过将先进的时间动态与黎曼空间表示相结合，RLSTG在复杂结构图上实现了卓越的性能。项目页面：https://rlstg.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Liquid Time-Constant networks (LTCs), which operate in Euclidean space and introduce geometric distortion when modeling real-world graphs with inherent non-Euclidean structures like hierarchies and cycles. The proposed method, the Riemannian Liquid Spatio-Temporal Graph Network (RLSTG), unifies continuous-time liquid dynamics with Riemannian manifolds by formulating graph evolution through an Ordinary Differential Equation directly on a curved manifold, enabling faithful capture of intrinsic geometry in both static and dynamic spatio-temporal graphs. Experimental results on real-world benchmarks demonstrate that RLSTG achieves superior performance on graphs with complex structures, supported by theoretical guarantees extending LTC stability theorems to the Riemannian domain and quantifying expressive power via state trajectory analysis.</div>
<div class="mono" style="margin-top:8px">本研究针对液态时间常数网络（LTCs）的局限性，该网络在欧几里得空间中运行，在建模具有固有非欧几里得结构（如层次结构和循环）的真实世界图时会引入几何失真。所提出的方法，即黎曼液态时空图网络（RLSTG），通过直接在弯曲流形上构建描述图演化的常微分方程，将连续时间液态动力学与黎曼流形的几何归纳偏置相统一，从而能够忠实捕捉静态和动态时空图的内在几何结构。在真实世界基准测试上的实验结果表明，RLSTG在具有复杂结构的图上实现了优越的性能，其理论保证将LTC的稳定性定理扩展至黎曼域，并通过状态轨迹分析量化了其表达能力。</div>
</details>
</div>
<div class="card">
<div class="title">Causal feature selection framework for stable soft sensor modeling based on time-delayed cross mapping</div>
<div class="meta-line">Authors: Shi-Shun Chen, Xiao-Yang Li, Enrico Zio</div>
<div class="meta-line">Venue: Advanced Engineering Informatics 2026, 71, 104337</div>
<div class="meta-line">First: 2026-01-20T15:58:51+00:00 · Latest: 2026-01-20T15:58:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14099v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14099v1">PDF</a> · <a href="https://github.com/dirge1/TDPCM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Soft sensor modeling plays a crucial role in process monitoring. Causal feature selection can enhance the performance of soft sensor models in industrial applications. However, existing methods ignore two critical characteristics of industrial processes. Firstly, causal relationships between variables always involve time delays, whereas most causal feature selection methods investigate causal relationships in the same time dimension. Secondly, variables in industrial processes are often interdependent, which contradicts the decorrelation assumption of traditional causal inference methods. Consequently, soft sensor models based on existing causal feature selection approaches often lack sufficient accuracy and stability. To overcome these challenges, this paper proposes a causal feature selection framework based on time-delayed cross mapping. Time-delayed cross mapping employs state space reconstruction to effectively handle interdependent variables in causality analysis, and considers varying causal strength across time delay. Time-delayed convergent cross mapping (TDCCM) is introduced for total causal inference, and time-delayed partial cross mapping (TDPCM) is developed for direct causal inference. Then, in order to achieve automatic feature selection, an objective feature selection strategy is presented. The causal threshold is automatically determined based on the model performance on the validation set, and the causal features are then selected. Two real-world case studies show that TDCCM achieves the highest average performance, while TDPCM improves soft sensor stability and performance in the worst scenario. The code is publicly available at https://github.com/dirge1/TDPCM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于时滞交叉映射的稳定软测量建模因果特征选择框架</div>
<div class="mono" style="margin-top:8px">软测量建模在过程监控中至关重要。因果特征选择能提升工业应用中软测量模型的性能，但现有方法常忽略工业过程的两个关键特性：一是变量间的因果关系总存在时滞，而多数因果特征选择方法仅在同一时间维度分析因果关系；二是工业过程变量常相互依赖，与传统因果推断方法的去相关性假设相悖。这导致基于现有方法的软测量模型常缺乏足够精度与稳定性。为此，本文提出一种基于时滞交叉映射的因果特征选择框架。该框架通过状态空间重构有效处理因果分析中的变量互依问题，并考虑时滞差异下的因果强度变化。研究引入时滞收敛交叉映射（TDCCM）进行整体因果推断，开发时滞偏交叉映射（TDPCM）用于直接因果推断。进而提出客观特征选择策略以实现自动化：基于验证集模型性能自动确定因果阈值并筛选特征。两项实际案例表明，TDCCM实现了最优平均性能，而TDPCM在极端场景下显著提升了软测量的稳定性与性能。代码已公开于https://github.com/dirge1/TDPCM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses limitations in existing causal feature selection methods for soft sensor modeling in industrial processes, which often overlook time-delayed causal relationships and violate the decorrelation assumption for interdependent variables, leading to models with insufficient accuracy and stability. The proposed framework introduces time-delayed cross mapping, utilizing state space reconstruction to handle variable interdependencies and account for varying causal strengths across time delays; specifically, time-delayed convergent cross mapping (TDCCM) is developed for total causal inference, and time-delayed partial cross mapping (TDPCM) for direct causal inference, coupled with an objective feature selection strategy that automatically determines causal thresholds based on validation set performance. Experimental results from two real-world case studies demonstrate that TDCCM achieves the highest average performance, while TDPCM enhances model stability and performance in the worst-case scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对工业过程中软测量建模的现有因果特征选择方法的局限性，这些方法常忽略时间延迟的因果关系，且在变量相互依赖时违背去相关假设。所提出的框架引入了时间延迟交叉映射，利用状态空间重构处理相互依赖的变量并捕捉不同时间延迟下的因果强度；具体而言，开发了时间延迟收敛交叉映射（TDCCM）用于总因果推断，以及时间延迟偏交叉映射（TDPCM）用于直接因果推断，并结合基于验证集性能自动确定因果阈值的特征选择策略。两个实际案例的实验结果表明，TDCCM实现了最高的平均性能，而TDPCM在最差情况下提升了模型的稳定性和性能。</div>
</details>
</div>
<div class="card">
<div class="title">Remapping and navigation of an embedding space via error minimization: a fundamental organizational principle of cognition in natural and artificial systems</div>
<div class="meta-line">Authors: Benedikt Hartl, Léo Pio-Lopez, Chris Fields, Michael Levin</div>
<div class="meta-line">First: 2026-01-20T15:57:36+00:00 · Latest: 2026-01-20T15:57:36+00:00</div>
<div class="meta-line">Comments: 41 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14096v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14096v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emerging field of diverse intelligence seeks an integrated view of problem-solving in agents of very different provenance, composition, and substrates. From subcellular chemical networks to swarms of organisms, and across evolved, engineered, and chimeric systems, it is hypothesized that scale-invariant principles of decision-making can be discovered. We propose that cognition in both natural and synthetic systems can be characterized and understood by the interplay between two equally important invariants: (1) the remapping of embedding spaces, and (2) the navigation within these spaces. Biological collectives, from single cells to entire organisms (and beyond), remap transcriptional, morphological, physiological, or 3D spaces to maintain homeostasis and regenerate structure, while navigating these spaces through distributed error correction. Modern Artificial Intelligence (AI) systems, including transformers, diffusion models, and neural cellular automata enact analogous processes by remapping data into latent embeddings and refining them iteratively through contextualization. We argue that this dual principle - remapping and navigation of embedding spaces via iterative error minimization - constitutes a substrate-independent invariant of cognition. Recognizing this shared mechanism not only illuminates deep parallels between living systems and artificial models, but also provides a unifying framework for engineering adaptive intelligence across scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过误差最小化实现嵌入空间的重映射与导航：自然与人工系统中认知的基本组织原则</div>
<div class="mono" style="margin-top:8px">新兴的多元智能领域致力于为来源、构成和基质迥异的主体构建统一的问题解决框架。从亚细胞化学网络到生物集群，跨越进化、工程和嵌合系统，我们假设可发现尺度不变的决策原则。本文提出，自然与合成系统的认知可通过两个同等重要的不变量的相互作用来表征和理解：(1) 嵌入空间的重映射，(2) 在这些空间内的导航。从单细胞到完整有机体（及更广尺度）的生物集体，通过重映射转录、形态、生理或三维空间以维持稳态并再生结构，同时通过分布式误差修正实现空间导航。现代人工智能系统（包括Transformer、扩散模型和神经细胞自动机）通过将数据重映射至潜在嵌入空间，并基于上下文迭代优化，实现了类似过程。我们认为，这种通过迭代误差最小化实现嵌入空间重映射与导航的双重原则，构成了基质无关的认知不变量。认识这一共享机制不仅能揭示生命系统与人工模型间的深层共性，还为跨尺度构建自适应智能提供了统一框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need for a unified framework to understand problem-solving across diverse intelligent systems, from biological networks to artificial intelligence. The authors propose that cognition in both natural and synthetic systems can be characterized by two core invariants: the remapping of embedding spaces (e.g., transcriptional or latent spaces) and the navigation within these spaces via distributed error correction or iterative refinement. Key experimental findings, supported by analysis across biological collectives and modern AI models like transformers and diffusion models, demonstrate that this dual principle of remapping and error-minimizing navigation operates as a substrate-independent organizational mechanism, revealing deep parallels and providing a unifying framework for adaptive intelligence.</div>
<div class="mono" style="margin-top:8px">本研究旨在为从生物网络到人工智能的多样化智能系统提供统一的问题解决理解框架。其方法提出了认知的双重不变性：通过迭代误差最小化实现嵌入空间的重映射与导航。实验结果表明，从生物群体到现代人工智能模型（如变换器和扩散模型），这一原则支持稳态维持、结构再生和上下文精炼，表明它是一种与底物无关的组织机制。</div>
</details>
</div>
<div class="card">
<div class="title">Quantization Meets Reasoning: Exploring and Mitigating Degradation of Low-Bit LLMs in Mathematical Reasoning</div>
<div class="meta-line">Authors: Zhen Li, Yupeng Su, Songmiao Wang, Runming Yang, Congkai Xie, Aofan Liu, Ming Li, Jiannong Cao, Yuan Xie, Ngai Wong, Hongxia Yang</div>
<div class="meta-line">First: 2025-05-16T12:11:40+00:00 · Latest: 2026-01-20T15:56:29+00:00</div>
<div class="meta-line">Comments: 27pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.11574v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.11574v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-bit post-training quantization (PTQ) is a practical route to deploy reasoning-capable LLMs under tight memory and latency budgets, yet it can markedly impair mathematical reasoning (drops up to 69.81% in our harder settings). We address two deployment-critical questions with process-level precision: Where along a step-structured solution does degradation first arise? How to mitigate it while staying in the low-bit regime? Across widely used PTQ methods (AWQ, GPTQ, SmoothQuant), open-source model families (Qwen, LLaMA; 0.5--7B), and math reasoning benchmarks (GSM8K, MATH, AIME), we perform format-aligned chain-of-thought with step-aligned attribution and uncover two robust regularities: (i) PTQ disproportionately elevates method and execution errors relative to high-level conceptual mistakes; and (ii) failures emerge early, with the first vulnerable step flipping and cascading to the final answer. These regularities suggest a general intervention principle: restore local token-level margins exactly at the earliest failure frontier. We instantiate this principle as a lightweight measure$\rightarrow$locate$\rightarrow$restore loop that operates directly on the quantized model: detect the first faulty step, construct our &quot;Silver Bullet&quot; datasets, and apply small-scale supervised/preference tuning. In our settings, as few as 332 curated examples and 3--5 minutes of compute on a single GPU recover 4-bit weight math reasoning toward the full-precision baseline while preserving PTQ efficiency. Our framework is quantizer- and architecture-agnostic within the evaluated regimes, and turns low-bit degradation from a global accuracy problem into a local, reproducible process intervention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量化与推理相遇：探索并缓解低比特大语言模型在数学推理中的性能退化</div>
<div class="mono" style="margin-top:8px">低比特后训练量化（PTQ）是在严格内存与延迟预算下部署具备推理能力大语言模型的实用途径，但其可能显著损害数学推理性能（在我们更严苛的设置中下降高达69.81%）。我们以过程级精度探讨两个部署关键问题：在结构化分步解答中，性能退化最早出现在哪个环节？如何在保持低比特状态下缓解该问题？通过对广泛使用的PTQ方法（AWQ、GPTQ、SmoothQuant）、开源模型系列（Qwen、LLaMA；0.5-7B参数）及数学推理基准（GSM8K、MATH、AIME）进行格式对齐的思维链与步骤对齐归因分析，我们揭示出两个稳健规律：（i）PTQ会不成比例地增加方法与执行错误，而非高层概念错误；（ii）故障出现较早，首个脆弱步骤的翻转会级联影响最终答案。这些规律指向通用干预原则：在最早失效边界精确恢复局部词元级容错空间。我们将该原则实例化为直接在量化模型上运行的轻量级“测量→定位→恢复”循环：检测首个错误步骤，构建“银弹”数据集，并应用小规模监督/偏好微调。在我们的设置中，仅需332个精选样本和单GPU上3-5分钟计算，即可使4比特权重数学推理能力恢复至全精度基线水平，同时保持PTQ效率。该框架在评估范围内与量化器及架构无关，将低比特退化问题从全局精度挑战转化为局部可复现的过程干预。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Low-bit post-training quantization (PTQ) is essential for deploying large language models under resource constraints but severely degrades mathematical reasoning performance, with accuracy drops up to 69.81%. To address this, the study systematically analyzes where and how degradation occurs by applying step-aligned attribution to chain-of-thought reasoning across various PTQ methods, model families, and benchmarks, revealing that PTQ disproportionately increases method and execution errors and that failures emerge at the first vulnerable step and cascade. Based on these findings, the authors propose a lightweight intervention framework that detects the first faulty step, constructs targeted &#x27;Silver Bullet&#x27; datasets, and applies small-scale tuning, which with as few as 332 examples and 3–5 minutes of GPU compute recovers 4-bit weight math reasoning performance to near full-precision levels while maintaining PTQ efficiency.</div>
<div class="mono" style="margin-top:8px">低比特后训练量化（PTQ）是在资源受限条件下部署大语言模型的关键技术，但会严重损害其数学推理能力。为解决此问题，本研究在GSM8K和MATH等基准上，对Qwen和LLaMA等量化模型进行了步骤对齐归因的过程级分析，发现PTQ会不成比例地增加方法和执行错误，且失败最早出现在第一个脆弱步骤并引发级联效应。基于这些规律，作者提出了一种轻量级干预框架，通过检测最早的错误步骤、构建针对性的“银弹”数据集并进行小规模微调，仅用332个样本和极少的计算量，即可将4比特模型的推理性能恢复至接近全精度基线水平。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot adaptable task planning for autonomous construction robots: a comparative study of lightweight single and multi-AI agent systems</div>
<div class="meta-line">Authors: Hossein Naderi, Alireza Shojaei, Lifu Huang, Philip Agee, Kereshmeh Afsari, Abiola Akanmu</div>
<div class="meta-line">First: 2026-01-20T15:54:33+00:00 · Latest: 2026-01-20T15:54:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14091v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14091v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robots are expected to play a major role in the future construction industry but face challenges due to high costs and difficulty adapting to dynamic tasks. This study explores the potential of foundation models to enhance the adaptability and generalizability of task planning in construction robots. Four models are proposed and implemented using lightweight, open-source large language models (LLMs) and vision language models (VLMs). These models include one single agent and three multi-agent teams that collaborate to create robot action plans. The models are evaluated across three construction roles: Painter, Safety Inspector, and Floor Tiling. Results show that the four-agent team outperforms the state-of-the-art GPT-4o in most metrics while being ten times more cost-effective. Additionally, teams with three and four agents demonstrate the improved generalizability. By discussing how agent behaviors influence outputs, this study enhances the understanding of AI teams and supports future research in diverse unstructured environments beyond construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自主建造机器人的零样本可适应任务规划：轻量级单智能体与多智能体系统的比较研究</div>
<div class="mono" style="margin-top:8px">机器人预计将在未来建造业发挥重要作用，但面临高成本和动态任务适应性的挑战。本研究探索基础模型提升建造机器人任务规划适应性与泛化能力的潜力。采用轻量级开源大语言模型（LLMs）和视觉语言模型（VLMs），提出并实现了四种模型，包括一个单智能体和三个通过协作生成机器人行动方案的多智能体团队。模型在油漆工、安全巡检员和地板铺贴工三种建造角色中进行评估。结果显示，四智能体团队在多数指标上优于当前最先进的GPT-4o模型，且成本效益提升十倍。三智能体与四智能体团队还展现出更强的泛化能力。通过分析智能体行为对输出的影响，本研究深化了对AI团队协作机制的理解，为建造领域之外多样化非结构化环境的未来研究提供支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high costs and limited adaptability of robots in dynamic construction tasks, this study investigates the use of lightweight foundation models for zero-shot adaptable task planning. The method proposes and implements four models based on open-source large language models (LLMs) and vision language models (VLMs), comprising one single-agent and three multi-agent collaborative systems, to generate action plans for roles like Painter, Safety Inspector, and Floor Tiling. Experimental results demonstrate that a four-agent team surpasses the state-of-the-art GPT-4o in most performance metrics while being ten times more cost-effective, with three- and four-agent teams also showing enhanced generalizability, providing insights into AI team behaviors for applications in unstructured environments.</div>
<div class="mono" style="margin-top:8px">为解决建筑机器人成本高、在动态任务中适应性差的问题，本研究探索利用基础模型实现零样本可适应的任务规划。研究提出并实现了四种基于轻量级开源大语言模型和视觉语言模型的系统，包括一个单智能体和三个多智能体团队，这些系统通过协作生成机器人动作计划，并在油漆工、安全巡检员和地板铺贴工三个建筑角色上进行评估。实验结果表明，四智能体团队在多数性能指标上优于最先进的GPT-4o模型，同时成本效益高出十倍，且三智能体和四智能体团队表现出更好的泛化能力；对智能体行为的分析为未来在非结构化环境中应用AI团队提供了见解。</div>
</details>
</div>
<div class="card">
<div class="title">&#x27;1&#x27;-bit Count-based Sorting Unit to Reduce Link Power in DNN Accelerators</div>
<div class="meta-line">Authors: Ruichi Han, Yizhi Chen, Tong Lei, Jordi Altayo Gonzalez, Ahmed Hemani</div>
<div class="meta-line">First: 2026-01-20T15:47:36+00:00 · Latest: 2026-01-20T15:47:36+00:00</div>
<div class="meta-line">Comments: Accepted for oral presentation at the 2026 VLSI Symposium on Technology, Systems and Applications (VLSI TSA) on April 13-17, 2026, at the Ambassador Hotel, Hsinchu, Taiwan</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14087v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14087v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interconnect power consumption remains a bottleneck in Deep Neural Network (DNN) accelerators. While ordering data based on &#x27;1&#x27;-bit counts can mitigate this via reduced switching activity, practical hardware sorting implementations remain underexplored. This work proposes the hardware implementation of a comparison-free sorting unit optimized for Convolutional Neural Networks (CNN). By leveraging approximate computing to group population counts into coarse-grained buckets, our design achieves hardware area reductions while preserving the link power benefits of data reordering. Our approximate sorting unit achieves up to 35.4% area reduction while maintaining 19.50\% BT reduction compared to 20.42% of precise implementation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于&#x27;1&#x27;位计数的排序单元以降低DNN加速器链路功耗</div>
<div class="mono" style="margin-top:8px">互连功耗仍是深度神经网络加速器的瓶颈。虽然基于&#x27;1&#x27;位计数排序数据可通过降低开关活动缓解此问题，但实用的硬件排序实现仍研究不足。本研究提出专为卷积神经网络优化的免比较排序单元硬件实现方案。通过近似计算将总体计数分组至粗粒度桶，该设计在保持数据重排链路功耗优势的同时减少了硬件面积。相较于精确实现方案的20.42%位翻转降低率，本近似排序单元在保持19.50%位翻转降低率的同时，实现了高达35.4%的面积缩减。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the bottleneck of interconnect power consumption in Deep Neural Network (DNN) accelerators, this work proposes a hardware implementation of a comparison-free sorting unit that orders data based on &#x27;1&#x27;-bit counts to reduce switching activity. The method leverages approximate computing by grouping population counts into coarse-grained buckets, thereby optimizing the design for Convolutional Neural Networks (CNNs). Experimental results show the approximate sorting unit achieves up to 35.4% area reduction while maintaining a 19.50% bit toggle (BT) reduction, closely matching the 20.42% BT reduction of a precise implementation.</div>
<div class="mono" style="margin-top:8px">互连功耗是深度神经网络加速器的主要瓶颈。为应对此问题，本文提出一种专为卷积神经网络设计的免比较排序单元硬件实现，通过根据数据的&#x27;1&#x27;比特数量进行排序来降低链路翻转活动。该方法利用近似计算，将总体计数分组到粗粒度桶中，从而简化了硬件。实验结果表明，该近似排序单元在保持19.50%的比特翻转降低率（接近精确实现的20.42%）的同时，实现了高达35.4%的面积缩减。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
