<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-26 06:26</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260126_0626</div>
    <div class="row"><div class="card">
<div class="title">Point Bridge: 3D Representations for Cross Domain Policy Learning</div>
<div class="meta-line">Authors: Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, Yashraj Narang, Ajay Mandlekar</div>
<div class="meta-line">First: 2026-01-22T18:59:24+00:00 · Latest: 2026-01-22T18:59:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pointbridge3d.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Point Bridge：面向跨域策略学习的3D表征方法</div>
<div class="mono" style="margin-top:8px">机器人基础模型正逐步实现通用智能体的愿景，但大规模真实世界操作数据的稀缺仍制约着发展。仿真与合成数据生成提供了可扩展的替代方案，但其有效性受限于仿真与现实的视觉域差异。本研究提出Point Bridge框架，该框架利用统一、领域无关的点云表征，无需显式的视觉或物体级对齐，即可实现零样本仿真到现实的策略迁移。Point Bridge通过视觉语言模型自动提取点云表征，结合基于Transformer的策略学习与高效推理流程，仅使用合成数据即可训练出高效的现实世界操作智能体。在少量真实演示数据上进行协同训练后，Point Bridge性能进一步提升，显著优于现有基于视觉的仿真-现实协同训练方法。在单任务与多任务场景中，其零样本仿真到现实迁移性能最高提升44%，结合有限真实数据后最高提升66%。机器人演示视频详见：https://pointbridge3d.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity of large-scale real-world robot manipulation datasets and the visual domain gap that limits the utility of synthetic data, this work introduces Point Bridge, a framework for cross-domain policy learning. The method employs unified, domain-agnostic point-based representations extracted automatically by Vision-Language Models (VLMs) and learns policies via transformers, enabling zero-shot sim-to-real transfer without explicit visual alignment. Experimental results show the framework achieves up to a 44% improvement in zero-shot transfer performance and up to a 66% gain when co-trained with limited real demonstrations, substantially outperforming prior vision-based methods in both single-task and multitask settings.</div>
<div class="mono" style="margin-top:8px">为解决大规模真实世界机器人操作数据稀缺以及仿真数据因视觉域差异难以利用的问题，本研究提出了用于跨域策略学习的Point Bridge框架。该方法利用视觉语言模型自动提取统一的、领域无关的点云表示，并仅使用合成数据训练基于Transformer的策略，实现了无需显式视觉对齐的零样本仿真到现实迁移。实验结果表明，该框架在零样本迁移性能上最高提升44%，当与少量真实演示数据协同训练时，性能最高提升66%，在单任务和多任务设置中均显著优于先前的基于视觉的方法。</div>
</details>
</div>
<div class="card">
<div class="title">IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance</div>
<div class="meta-line">Authors: Jongwoo Park, Kanchana Ranasinghe, Jinhyeok Jang, Cristina Mata, Yoo Sung Jang, Michael S Ryoo</div>
<div class="meta-line">First: 2026-01-22T18:57:13+00:00 · Latest: 2026-01-22T18:57:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model&#x27;s built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IVRA：基于无训练提示引导的视觉-令牌关系优化提升机器人动作策略</div>
<div class="mono" style="margin-top:8px">多数视觉-语言-动作模型将图像块展平为一维令牌序列，削弱了精确操作所需的二维空间线索。本文提出IVRA，一种轻量级、无需训练的方法，通过利用模型内置视觉编码器中已有的亲和性提示来增强空间理解，无需外部编码器或重新训练。IVRA选择性地将这些亲和性信号注入到包含实例级特征的语言模型层中。这种推理时干预重新校准了视觉-令牌的交互，在保持所有模型参数固定的同时更好地保留几何结构。我们通过在多种VLA架构（LLaRA、OpenVLA和FLOWER）上应用IVRA，跨越涵盖2D和3D操作的模拟基准（VIMA和LIBERO）及多种真实机器人任务，证明了其通用性。在2D VIMA任务中，IVRA在低数据场景下较基线LLaRA平均成功率提升4.2%；在3D LIBERO任务中，对OpenVLA和FLOWER基线均带来稳定增益，包括在基线准确率接近饱和时（96.3%至97.1%）的进一步提升。所有代码与模型将公开发布，可视化结果详见：jongwoopark7978.github.io/IVRA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often flatten image patches into a 1D sequence, which degrades the 2D spatial information crucial for precise robotic manipulation. To address this, the authors propose IVRA, a lightweight and training-free method that enhances spatial understanding by extracting and selectively injecting affinity hints from the model&#x27;s existing vision encoder into a language-model layer containing instance-level features, thereby realigning visual-token interactions without modifying any parameters. Experiments on diverse VLA architectures across 2D and 3D simulated benchmarks and real-robot tasks show consistent improvements, such as a +4.2% average success increase over LLaRA on 2D VIMA and gains from 96.3% to 97.1% on 3D LIBERO where baseline performance was near saturation.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型通常将图像块展平为一维序列，这削弱了精确机器人操作所需的二维空间信息。为此，研究者提出了IVRA，这是一种轻量级且无需训练的方法，它通过将模型自身视觉编码器中的亲和性提示注入到包含实例级特征的特定语言模型层，来增强空间理解能力。这种推理时干预重新对齐了视觉-标记的交互，更好地保留了几何结构，且无需修改任何模型参数。在多种VLA架构上，针对2D和3D模拟基准（VIMA、LIBERO）以及真实机器人任务的实验显示出一致的性能提升，例如在VIMA上相比LLaRA基线平均成功率提高4.2%，并将OpenVLA在LIBERO上的准确率从96.3%提升至97.1%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Compensated Model Predictive Control for Off-road Driving on Unknown Deformable Terrain</div>
<div class="meta-line">Authors: Prakhar Gupta, Jonathon M. Smereka, Yunyi Jia</div>
<div class="meta-line">First: 2024-08-17T16:53:51+00:00 · Latest: 2026-01-22T18:38:26+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Intelligent Vehicles as a Regular Paper; was withdrawn in March 2025. A revised version of this manuscript was submitted to ACC 2025 review as a regular paper in Sep 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.09253v2">Abs</a> · <a href="https://arxiv.org/pdf/2408.09253v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents an Actor-Critic reinforcement learning Compensated Model Predictive Controller (AC2MPC) designed for high-speed, off-road autonomous driving on deformable terrains. Addressing the difficulty of modeling unknown tire-terrain interaction and ensuring real-time control feasibility and performance, this framework integrates deep reinforcement learning with a model predictive controller to manage unmodeled nonlinear dynamics. We evaluate the controller framework over constant and varying velocity profiles using high-fidelity simulator Project Chrono. Our findings demonstrate that our controller statistically outperforms standalone model-based and learning-based controllers over three unknown terrains that represent sandy deformable track, sandy and rocky track and cohesive clay-like deformable soil track. Despite varied and previously unseen terrain characteristics, this framework generalized well enough to track longitudinal reference speeds with the least error. Furthermore, this framework required significantly less training data compared to purely learning based controller, converging in fewer steps while delivering better performance. Even when under-trained, this controller outperformed the standalone controllers, highlighting its potential for safer and more efficient real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向未知可变形地形的越野驾驶：强化学习补偿模型预测控制</div>
<div class="mono" style="margin-top:8px">本研究提出一种基于执行器-评判器强化学习的补偿模型预测控制器（AC2MPC），用于可变形地形上的高速越野自动驾驶。针对未知轮胎-地形交互建模困难及实时控制可行性问题，该框架将深度强化学习与模型预测控制器结合以处理未建模的非线性动力学。通过高保真仿真器Project Chrono，我们在恒定与变化速度场景下评估该控制器。结果表明，在代表沙质可变形路面、沙石混合路面及黏性类黏土可变形土壤的三种未知地形上，该控制器在统计意义上优于纯模型控制器与纯学习控制器。面对多样且未见过的地形特征，该框架能有效跟踪纵向参考速度且误差最小。相较于纯学习控制器，本框架所需训练数据显著减少，收敛步数更少且性能更优。即使在训练不足时，该控制器仍优于独立控制器，突显其在实际部署中具有更高安全性与效率的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of controlling autonomous vehicles at high speeds on unknown deformable off-road terrain, where modeling tire-terrain interactions is difficult and real-time performance is critical. The proposed method, an Actor-Critic reinforcement learning Compensated Model Predictive Controller (AC2MPC), integrates deep reinforcement learning with model predictive control to compensate for unmodeled nonlinear dynamics. Experimental evaluation in the high-fidelity Project Chrono simulator across three unknown terrains (sandy, sandy-rocky, and clay-like) shows that AC2MPC statistically outperforms standalone model-based and learning-based controllers in tracking longitudinal reference speeds with the least error, generalizes well to unseen terrain, requires significantly less training data, and maintains robust performance even when under-trained.</div>
<div class="mono" style="margin-top:8px">本研究针对未知可变形地形上的自主越野驾驶挑战，该场景中轮胎与地形的复杂相互作用难以精确建模，且需要实时控制。所提出的方法称为演员-评论家强化学习补偿模型预测控制器（AC2MPC），它将深度强化学习的演员-评论家架构与模型预测控制器相结合，以补偿未建模的非线性动力学。在高保真仿真器Project Chrono中对三种不同的未知地形（沙地、沙石混合地和类黏土）进行的实验评估表明，AC2MPC在纵向参考速度跟踪上的误差最小，统计性能显著优于独立的基于模型和基于学习的控制器。该框架对未见过的地形特征表现出良好的泛化能力，所需训练数据远少于纯学习型控制器，即使在训练不足时仍保持优越性能，显示出更安全、高效的实际部署潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</div>
<div class="meta-line">Authors: Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</div>
<div class="meta-line">First: 2026-01-22T18:09:30+00:00 · Latest: 2026-01-22T18:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16163v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16163v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#x27;s latent diffusion process, harnessing the model&#x27;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cosmos策略：面向视觉运动控制与规划的视觉模型微调</div>
<div class="mono" style="margin-top:8px">近期视频生成模型展现出捕捉复杂物理交互与时序场景演变的卓越能力。为利用其时空先验知识，机器人研究领域已采用视频模型进行策略学习，但需通过多阶段后训练及新增动作生成架构组件，引入了复杂性。本研究提出Cosmos策略，这是一种通过单阶段后训练将大型预训练视频模型（Cosmos-Predict2）适配为高效机器人策略的简洁方法：仅使用目标平台采集的机器人演示数据进行训练，无需修改模型架构。该策略通过视频模型的潜在扩散过程直接生成编码为潜在帧的机器人动作，利用模型的预训练先验与核心学习算法捕捉复杂动作分布。此外，Cosmos策略还能生成编码为潜在帧的未来状态图像与价值函数（预期累积奖励），支持在测试阶段规划更高成功率的动作轨迹。实验评估显示，Cosmos策略在LIBERO与RoboCasa仿真基准测试中分别达到98.5%与67.1%的平均成功率，在现实世界复杂双手操作任务中获得最高平均分，其表现优于从头训练的扩散策略、基于视频模型的策略，以及经相同机器人演示数据微调的先进视觉-语言-动作模型。进一步地，基于策略推演数据，Cosmos策略可通过经验学习优化其世界模型与价值函数，并借助基于模型的规划在复杂任务中实现更高成功率。相关代码、模型及训练数据已发布于https://research.nvidia.com/labs/dir/cosmos-policy/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To leverage the strong spatiotemporal priors of video generation models for robotics without introducing complex multi-stage training or architectural modifications, this work introduces Cosmos Policy, which fine-tunes a pretrained video model (Cosmos-Predict2) directly on robot demonstration data. The method encodes robot actions, future state images, and value predictions as latent frames within the model&#x27;s existing latent diffusion process, enabling it to capture complex action distributions and perform test-time planning. Experiments show state-of-the-art performance on LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates) and superior results in real-world bimanual manipulation, outperforming diffusion policies trained from scratch and other fine-tuned models, with additional improvements possible through learning from rollout data for model-based planning.</div>
<div class="mono" style="margin-top:8px">为了利用视频生成模型强大的时空先验进行机器人控制，同时避免复杂的多阶段训练或架构修改，本研究提出了Cosmos Policy方法，该方法直接在机器人演示数据上微调预训练的视频模型（Cosmos-Predict2）。该方法将机器人动作、未来状态图像和价值预测编码为模型现有潜在扩散过程中的潜在帧，使其能够捕捉复杂的动作分布并支持测试时规划。实验表明，该方法在LIBERO和RoboCasa仿真基准上取得了最先进的性能（平均成功率分别为98.5%和67.1%），并在真实世界双手操作任务中获得了最高平均分，优于从头训练的扩散策略和其他微调模型，且通过从策略 rollout 数据中学习以改进其世界模型和价值函数，还能实现进一步的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics</div>
<div class="meta-line">Authors: Britton Jordan, Jordan Thompson, Jesse F. d&#x27;Almeida, Hao Li, Nithesh Kumar, Susheela Sharma Stern, James Ferguson, Ipek Oguz, Robert J. Webster, Daniel Brown, Alan Kuntz</div>
<div class="meta-line">First: 2025-12-12T18:36:53+00:00 · Latest: 2026-01-22T17:44:32+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures. Project page: https://brittonjordan.github.io/probe_mde/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11773v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.11773v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://brittonjordan.github.io/probe_mde/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble&#x27;s variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.
  Project page: https://brittonjordan.github.io/probe_mde/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProbeMDE：面向手术机器人单目深度估计的不确定性引导主动本体感知</div>
<div class="mono" style="margin-top:8px">单目深度估计（MDE）为机器人感知提供了有效工具，但其预测在手术场景等挑战性环境中常因纹理缺失表面、镜面反射和遮挡而存在不确定性和不准确性。为此，我们提出ProbeMDE——一种成本感知的主动感知框架，将RGB图像与稀疏本体感知测量相结合用于MDE。该方法利用MDE模型集合，基于RGB图像和通过本体感知获取的稀疏已知深度测量值（机器人在已知构型下接触环境获得）预测稠密深度图。我们通过模型集合方差量化预测不确定性，并计算不确定性相对于候选测量位置的梯度。为避免选择最具信息量的本体感知（接触）位置时出现模式崩溃，我们在梯度图上采用Stein变分梯度下降法（SVGD）。我们在中央气道阻塞手术体模的仿真与物理实验中验证了该方法。结果表明，本方法在标准深度评估指标上均优于基线方法，能以最少的本体感知测量次数实现更高精度。项目页面：https://brittonjordan.github.io/probe_mde/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Monocular depth estimation (MDE) in surgical robotics is challenged by textureless surfaces and specular reflections, leading to uncertain and inaccurate predictions. To address this, ProbeMDE introduces an active sensing framework that fuses RGB images with sparse, actively chosen proprioceptive depth measurements. The method uses an ensemble of MDE models to predict dense depth, quantifies uncertainty via ensemble variance, and selects optimal touch locations by applying Stein Variational Gradient Descent (SVGD) to the gradient of uncertainty with respect to candidate points. Experiments on surgical airway phantoms in simulation and real-world settings show that ProbeMDE outperforms baselines in standard depth metrics, achieving higher accuracy with fewer required proprioceptive measurements.</div>
<div class="mono" style="margin-top:8px">在手术机器人中，单目深度估计常因无纹理表面、镜面反射和遮挡等问题导致预测不确定且不准确。为此，ProbeMDE提出一种成本感知的主动感知框架，将RGB图像与稀疏的本体感知深度测量相结合，利用模型集成在两种数据条件下预测稠密深度图。该方法通过集成方差量化预测不确定性，并基于不确定性梯度图使用Stein变分梯度下降（SVGD）来选择最佳触觉探测位置。在模拟和真实气道阻塞体模上的实验表明，该方法在深度估计指标上优于基线方法，同时最大限度地减少了所需本体感知测量的次数。</div>
</details>
</div>
<div class="card">
<div class="title">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-22T17:01:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayesianVLA：基于潜在动作查询的视觉语言动作模型贝叶斯分解</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但常难以泛化至新指令或复杂多任务场景。我们指出当前训练范式的关键缺陷：目标驱动的数据收集导致数据集偏差，其中仅凭视觉观测即可高度预测语言指令，致使指令与动作间的条件互信息消失，此现象称为“信息坍缩”。因此，模型退化为仅依赖视觉的策略，忽略语言约束并在分布外（OOD）场景中失效。为此，我们提出BayesianVLA，一种通过贝叶斯分解强制遵循指令的新框架。通过引入可学习的潜在动作查询，构建双分支架构以估计仅视觉先验$p(a \mid v)$和语言条件后验$π(a \mid v, \ell)$，进而优化策略以最大化动作与指令间的条件点互信息（PMI）。该目标有效惩罚视觉捷径，并奖励明确解释语言指令的动作。无需新数据，BayesianVLA显著提升泛化能力：在SimplerEnv和RoboCasa上的大量实验显示明显增益，包括在挑战性OOD SimplerEnv基准上提升11.3%，验证了本方法在动作中稳健关联语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions become predictable from visual context alone, leading to an Information Collapse that degrades models into vision-only policies. To counteract this, the proposed BayesianVLA framework introduces learnable Latent Action Queries within a dual-branch architecture, explicitly modeling a vision-only prior and a language-conditioned posterior to maximize the conditional pointwise mutual information between actions and instructions, thereby penalizing visual shortcuts. Experiments on SimplerEnv and RoboCasa benchmarks show the method significantly improves out-of-distribution generalization, achieving an 11.3% performance gain on a challenging OOD benchmark without requiring additional data.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差导致泛化能力不足，其中语言指令可从视觉上下文预测，引发信息坍缩，使模型退化为仅依赖视觉的策略。为解决该问题，BayesianVLA提出一种贝叶斯分解框架，通过可学习的潜在动作查询分别建模仅视觉先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息。在SimplerEnv和RoboCasa上的实验表明，该方法显著提升了分布外泛化能力，在具有挑战性的OOD基准测试中性能提高了11.3%，且无需额外数据。</div>
</details>
</div>
<div class="card">
<div class="title">Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision</div>
<div class="meta-line">Authors: Yashuai Yan, Tobias Egle, Christian Ott, Dongheui Lee</div>
<div class="meta-line">First: 2026-01-22T16:56:52+00:00 · Latest: 2026-01-22T16:56:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16109v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16109v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过模型监督强化学习高效掌握基于扭矩的鲁棒运动控制</div>
<div class="mono" style="margin-top:8px">我们提出一种控制框架，将基于模型的双足运动与残差强化学习相结合，以在现实世界不确定性条件下实现鲁棒且自适应的行走。该方法利用包含发散运动分量轨迹规划器和全身控制器的模型控制器作为可靠基础策略。为应对动力学建模不精确和传感器噪声等不确定因素，我们引入通过领域随机化强化学习训练的残差策略。关键创新在于：训练时采用具备真实动力学特权的模型参考策略，通过新型监督损失指导残差策略。这种监督机制使策略能高效学习补偿未建模效应的校正行为，无需复杂奖励函数设计。实验表明，该方法在多种随机化条件下展现出更强的鲁棒性和泛化能力，为双足运动的仿真到现实迁移提供了可扩展解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance the robustness of torque-based bipedal locomotion against real-world uncertainties like modeling inaccuracies and sensor noise. The method integrates a model-based controller (using a DCM planner and whole-body control) as a base policy, augmented by a residual policy trained with reinforcement learning and domain randomization. A key innovation is the use of a model-based oracle with privileged ground-truth dynamics to supervise the residual policy via a novel loss, enabling efficient learning of corrective behaviors without complex reward engineering. Experimental results show the approach improves robustness and generalization across randomized conditions, facilitating sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">为实现双足机器人在现实不确定性（如不精确动力学和传感器噪声）下的鲁棒行走，本研究将基于模型的控制器与残差强化学习相结合。该方法利用一个具有真实动力学特权的模型预言机，通过一种新颖的监督损失来指导残差强化学习策略，从而无需大量奖励调整即可高效学习补偿未建模效应的校正行为。实验表明，该方法在一系列随机化条件下提升了鲁棒性和泛化能力，为扭矩控制双足行走的仿真到现实迁移提供了可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application</div>
<div class="meta-line">Authors: Jiarui Cui, Maosong Wang, Wenqi Wu, Peiqi Li, Xianfei Pan</div>
<div class="meta-line">First: 2026-01-22T16:21:18+00:00 · Latest: 2026-01-22T16:21:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16078v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升基于SE2(3)群的扩展卡尔曼滤波组合导航自主性：应用研究</div>
<div class="mono" style="margin-top:8px">SE2(3)李群导航建模框架的核心优势之一在于误差传播的自主性。前期论文已从惯性系、地球系与世界系三个坐标系对导航模型的自主性进行了理论分析，并提出了一种改进非惯性导航模型实现完全自主性的SE2(3)群导航模型构建方法。本文作为前期研究的实践对应，通过实际捷联惯性导航系统(SINS)/里程计(ODO)实验与蒙特卡洛仿真，验证了改进型SE2(3)群高精度导航模型的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance the autonomy of navigation models within the SE2(3) Lie group framework, building on prior theoretical analysis of error propagation in inertial, earth, and world frames. The method involves constructing an improved SE2(3) group navigation model to achieve full autonomy, specifically refining non-inertial navigation models. Experimental validation through real-world strapdown inertial navigation system (SINS)/odometer tests and Monte-Carlo simulations demonstrates the performance of these enhanced high-precision navigation models.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升SE2(3)李群框架中导航模型的自主性，基于先前对惯性系、地球系和世界系中误差传播自主性的理论分析。所提出的方法通过构建改进的SE2(3)群导航模型，以完善非惯性导航模型，从而实现完全自主。通过实际的车载惯性导航系统/里程计实验以及蒙特卡洛仿真验证，结果表明改进后的模型能够实现高性能的精密导航。</div>
</details>
</div>
<div class="card">
<div class="title">DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models</div>
<div class="meta-line">Authors: Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan, Yangfan He, Yuchen Li, Jingqun Tang</div>
<div class="meta-line">First: 2026-01-22T16:02:56+00:00 · Latest: 2026-01-22T16:02:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16065v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16065v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as &#x27;distracting tokens&#x27;. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model&#x27;s visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DTP：一种面向视觉语言动作模型的高效干扰令牌剪枝框架</div>
<div class="mono" style="margin-top:8px">视觉语言动作（VLA）模型通过利用视觉语言模型（VLM）强大的感知能力理解环境并直接输出动作，在机器人操作领域取得了显著进展。然而，VLA模型默认可能过度关注任务无关区域的图像令牌（即“干扰令牌”），这种注意力偏差会干扰模型在每一步生成预期动作令牌的过程，从而影响任务成功率。本文提出了一种即插即用的干扰令牌剪枝（DTP）框架，能够动态检测并剪除这些干扰图像令牌。通过修正模型的视觉注意力模式，我们在不改变原始架构或增加额外输入的前提下，旨在提升任务成功率，并探索模型的性能上限。在SIMPLER基准测试（Li等人，2024）上的实验表明，该方法在不同类型的新型VLA模型中均能持续提升任务成功率，显示出对基于Transformer的VLA模型的普适性。进一步分析揭示了所有测试模型的任务成功率与任务无关区域注意力强度呈负相关，这一普遍现象可为未来研究提供指引。代码已开源：https://anonymous.4open.science/r/CBD3。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Action (VLA) models for robotic manipulation can be disrupted by over-attending to irrelevant image regions, termed &#x27;distracting tokens&#x27;, which lowers task success. To address this, the authors propose the Distracting Token Pruning (DTP) framework, a plug-and-play method that dynamically identifies and removes these distracting tokens to correct visual attention without modifying the model architecture. Experiments on the SIMPLER benchmark show that DTP consistently improves task success rates across various transformer-based VLA models, and analysis reveals a negative correlation between success rate and attention on task-irrelevant areas, highlighting a common issue for future research.</div>
<div class="mono" style="margin-top:8px">用于机器人操作的视觉-语言-动作（VLA）模型可能会因关注不相关的图像区域（即“干扰令牌”）而受到误导，从而影响动作生成和任务成功率。为此，研究者提出了一种即插即用的干扰令牌剪枝（DTP）框架，该框架能动态识别并移除这些干扰令牌以纠正视觉注意力，且无需修改基础模型架构。在SIMPLER基准测试上的实验表明，该方法在不同类型的基于Transformer的VLA模型上均能持续提升任务成功率，进一步分析揭示了任务成功率与模型在任务无关区域上的注意力呈负相关。</div>
</details>
</div>
<div class="card">
<div class="title">Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis</div>
<div class="meta-line">Authors: Jiarui Cui, Maosong Wang, Wenqi Wu, Peiqi Li, Xianfei Pan</div>
<div class="meta-line">First: 2026-01-22T15:58:56+00:00 · Latest: 2026-01-22T15:58:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16062v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升基于SE2(3)群的扩展卡尔曼滤波在组合导航中的自主性：理论分析</div>
<div class="mono" style="margin-top:8px">SE2(3)李群框架在导航建模中的核心优势之一在于误差传播的自主性。当前基于李群的扩展卡尔曼滤波研究表明，误差传播自主性在低精度应用中（如不考虑地球自转和惯性器件偏差的微机电系统组合导航）得以保持。然而，在高精度导航状态估计中，当考虑地球自转和惯性器件偏差时，维持自主性极为困难。本文分别针对惯性系、地球系与世界系下的SE2(3)群高精度导航模型自主性进行了理论分析。通过理论分析发现，传统简易SE2(3)群导航建模方法的局限在于非惯性系中速度引入的科里奥利力项的存在。为此，提出了一种SE2(3)群导航模型的构建方法，使导航模型更趋近于完全自主。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates the autonomy of error propagation in SE2(3) Lie group-based extended Kalman filters for high-precision integrated navigation, where maintaining autonomy is challenging when accounting for earth rotation and inertial device biases. Through theoretical analysis of navigation models under inertial, earth, and world frames, the study identifies that the traditional SE2(3) modeling approach is limited by Coriolis force terms arising from velocity in non-inertial frames. To address this, a new construction method for SE2(3) group navigation models is proposed, which enhances autonomy by mitigating these limitations, thereby advancing the framework&#x27;s applicability in high-precision state estimation scenarios.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于SE2(3)李群的扩展卡尔曼滤波器在高精度组合导航中误差传播的自洽性问题，该问题在考虑地球旋转和惯性器件偏差时难以维持。研究方法是对惯性系、地球系和世界系下的SE2(3)群导航模型进行理论分析，发现传统建模方法的局限性在于非惯性系中速度引入的科里奥利力项。主要实验结果表明，所提出的一种新的SE2(3)群导航模型构建方法克服了这一局限，使导航模型比传统方法更接近完全自洽。</div>
</details>
</div>
<div class="card">
<div class="title">DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning</div>
<div class="meta-line">Authors: Junha Lee, Eunha Park, Minsu Cho</div>
<div class="meta-line">First: 2026-01-22T15:23:35+00:00 · Latest: 2026-01-22T15:23:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16046v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16046v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DextER：基于语言驱动的灵巧抓取生成与具身推理</div>
<div class="mono" style="margin-top:8px">语言驱动的灵巧抓取生成要求模型理解任务语义、三维几何及复杂的手-物交互。尽管视觉-语言模型已应用于此问题，现有方法直接将观测映射至抓取参数，缺乏对物理交互的中间推理。本文提出DextER（具身推理的灵巧抓取生成），引入基于接触的具身推理以实现多指操控。核心洞见在于：预测手部哪些连杆接触物体表面的哪些位置，可形成一种具身感知的中间表示，从而连接任务语义与物理约束。DextER通过自回归生成具身接触令牌（指定手指连杆与物体表面的接触位置），再生成编码手部构型的抓取令牌。在DexGYS数据集上，DextER实现67.14%的成功率，较最优方法提升3.83个百分点，意图对齐度提升96.4%。我们还通过部分接触指定实现了可引导的生成，为抓取合成提供细粒度控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating dexterous grasps from language instructions, which requires understanding task semantics, 3D geometry, and complex hand-object interactions. The proposed method, DextER, introduces an embodied reasoning approach that first autoregressively generates contact tokens specifying which finger links contact specific object surfaces, and then produces grasp tokens encoding the full hand configuration. Experimental results on the DexGYS benchmark show that DextER achieves a 67.14% success rate, outperforming prior state-of-the-art by 3.83 percentage points and improving intention alignment by 96.4%, while also enabling steerable generation through partial contact specification.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决根据语言指令生成灵巧抓取姿态的挑战，这需要理解任务语义、三维几何和复杂的手-物体交互。提出的DextER方法引入了一种具身推理方法，首先自回归生成接触令牌，指定哪些手指连杆接触物体表面的特定位置，然后生成编码完整手部配置的抓取令牌。在DexGYS基准测试上的实验结果显示，成功率达到67.14%，比先前的最先进方法高出3.83个百分点，且在意图对齐上实现了96.4%的改进，同时通过部分接触指定实现了可引导的生成控制。</div>
</details>
</div>
<div class="card">
<div class="title">Collision-Free Humanoid Traversal in Cluttered Indoor Scenes</div>
<div class="meta-line">Authors: Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu, Yunrui Lian, Jilong Wang, Qingtao Liu, Xuesong Shi, Li Yi</div>
<div class="meta-line">First: 2026-01-22T15:08:53+00:00 · Latest: 2026-01-22T15:08:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16035v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://axian12138.github.io/CAT/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>杂乱室内场景中的无碰撞人形机器人穿越</div>
<div class="mono" style="margin-top:8px">本研究探讨了人形机器人在杂乱室内场景（如跨越地面散落物体、蹲身通过低矮障碍或挤过狭窄通道）中实现无碰撞穿越的问题。为实现这一目标，机器人需将感知到的多样化空间布局与几何形态的障碍物映射至相应的穿越技能。然而，由于缺乏能有效表征避碰过程中人形与障碍物关系的表达方式，直接学习此类映射较为困难。为此，我们提出人形势场（HumanoidPF），将此类关系编码为无碰撞运动方向，显著促进了基于强化学习的穿越技能学习。我们还发现，HumanoidPF作为一种感知表征，其仿真到现实的迁移差距可忽略不计。为进一步提升穿越技能在多样化高难度杂乱室内场景中的泛化能力，我们提出一种混合场景生成方法，结合真实三维室内场景片段与程序化合成的障碍物。我们成功将策略迁移至现实世界，并开发了遥操作系统，用户仅需点击一次即可指挥人形机器人在杂乱室内场景中穿越。通过大量仿真与实物实验验证了方法的有效性。演示与代码详见项目网站：https://axian12138.github.io/CAT/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling humanoid robots to navigate cluttered indoor environments by hurdling over, crouching under, or squeezing through obstacles. The core method is the Humanoid Potential Field (HumanoidPF), a novel representation that encodes humanoid-obstacle relationships as collision-free motion directions to facilitate reinforcement learning of traversal skills. Experimental results demonstrate that policies learned with HumanoidPF successfully transfer to a real humanoid robot with a negligible sim-to-real gap, and a hybrid scene generation method enables generalization across diverse cluttered scenes, validated through extensive simulation and real-world tests.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决人形机器人在杂乱室内场景（如跨越地面物体、蹲伏通过低矮障碍或挤过狭窄通道）中的无碰撞穿越问题。其核心方法是提出人形势场（HumanoidPF），该表示将人形与障碍物的关系编码为无碰撞运动方向，从而显著促进了基于强化学习的穿越技能学习。实验结果表明，基于HumanoidPF学习的策略能够以极小的仿真到现实差距成功迁移到真实人形机器人上，并且所提出的混合场景生成方法结合了真实场景片段与程序化障碍物，使技能能泛化至多样化的挑战性场景，最终实现了一个仅需单次点击即可指挥机器人穿越的遥操作系统。</div>
</details>
</div>
<div class="card">
<div class="title">Keyframe-Based Feed-Forward Visual Odometry</div>
<div class="meta-line">Authors: Weichen Dai, Wenhan Su, Da Kong, Yuhang Ming, Wanzeng Kong</div>
<div class="meta-line">First: 2026-01-22T14:45:42+00:00 · Latest: 2026-01-22T14:45:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16020v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于关键帧的前馈视觉里程计</div>
<div class="mono" style="margin-top:8px">视觉基础模型的出现革新了视觉里程计（VO）与SLAM，使得姿态估计和密集重建能在单一前馈网络中实现。然而，与利用关键帧方法提升效率和精度的传统流程不同，当前基于基础模型的方法（如VGGT-Long）通常不加区分地处理原始图像序列，导致计算冗余和因帧间视差低（提供有限的上下文立体信息）而性能下降。将传统几何启发式方法融入这些模型并非易事，因其性能依赖于高维潜在表示而非显式几何度量。为弥合这一差距，我们提出了一种新颖的基于关键帧的前馈VO方法。该方法不依赖手工规则，而是通过强化学习以数据驱动方式推导自适应关键帧策略，使选择与底层基础模型的内在特性对齐。我们在TartanAir数据集上训练智能体，并在多个真实世界数据集上进行广泛评估。实验结果表明，所提方法相比最先进的前馈VO方法取得了持续且显著的改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational redundancy and performance degradation in visual foundation model-based visual odometry (VO) methods, which process all frames indiscriminately unlike traditional keyframe-based pipelines. The proposed method introduces a keyframe selection mechanism using reinforcement learning to derive an adaptive policy that aligns with the model&#x27;s latent representations, rather than relying on hand-crafted geometric rules. Experiments on real-world datasets show that this approach yields consistent and substantial improvements over state-of-the-art feed-forward VO methods.</div>
<div class="mono" style="margin-top:8px">本研究针对当前基于视觉基础模型的前馈视觉里程计方法计算冗余且性能下降的问题，这些方法不加区分地处理所有帧，不同于传统的关键帧流程。所提出的方法引入了一种基于关键帧的前馈方法，利用强化学习来学习自适应的关键帧选择策略，使其与模型的潜在表征对齐，而非依赖手工设计的几何规则。在真实世界数据集上的实验表明，该方法相比最先进的前馈视觉里程计技术取得了持续且显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour</div>
<div class="meta-line">Authors: Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu, Zhe Sun, Qiuguo Zhu</div>
<div class="meta-line">First: 2026-01-22T14:16:12+00:00 · Latest: 2026-01-22T14:16:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15995v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15995v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot&#x27;s real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA&#x27;s exceptional agility and robustness in challenging scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PUMA：面向移动增强型四足跑酷的感知驱动统一落脚点先验</div>
<div class="mono" style="margin-top:8px">四足机器人的跑酷任务已成为敏捷运动能力的重要测试基准。人类运动员能通过感知环境特征选择合适落脚点以跨越障碍，但赋予腿式机器人类似的感知推理能力仍面临巨大挑战。现有方法多依赖遵循预计算落脚点的分层控制器，限制了机器人的实时适应性与强化学习的探索潜力。为突破这些局限，我们提出PUMA——一种将视觉感知与落脚点先验整合至单阶段训练过程的端到端学习框架。该方法利用地形特征估计以自我为中心的极坐标落脚点先验（包含相对距离与航向角），引导机器人在跑酷任务中进行主动姿态调整。在仿真与真实环境中针对多种离散复杂地形开展的广泛实验表明，PUMA在挑战性场景中展现出卓越的敏捷性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling quadruped robots to perform agile parkour by autonomously selecting footholds based on environmental perception, a capability that existing hierarchical controllers with pre-computed footholds lack. The proposed method, PUMA, is an end-to-end learning framework that integrates visual perception directly into a unified policy, using terrain features to generate egocentric polar foothold priors (distance and heading) to guide real-time posture adaptation. Experimental results in both simulation and real-world settings across various complex terrains demonstrate that PUMA achieves exceptional agility and robustness in parkour tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决四足机器人执行跑酷任务时，如何像人类运动员一样根据环境感知自主选择落脚点的挑战，现有基于预计算落脚点的分层控制器限制了机器人的实时适应能力。为此，我们提出了PUMA，这是一个端到端的学习框架，它将视觉感知与落脚点先验集成到单阶段训练中，利用地形特征估计以自我为中心的极坐标落脚点先验（相对距离和朝向），以引导机器人进行主动姿态调整。在仿真和真实世界多种离散复杂地形上的大量实验表明，PUMA在具有挑战性的场景中表现出了卓越的敏捷性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems</div>
<div class="meta-line">Authors: Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li, Lihua Xie</div>
<div class="meta-line">First: 2026-01-22T13:28:09+00:00 · Latest: 2026-01-22T13:28:09+00:00</div>
<div class="meta-line">Comments: This article has been accepted for publication in IEEE Robotics and Automation Letters (RA-L). Personal use is permitted. All other uses require IEEE permission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15946v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15946v1">PDF</a> · <a href="https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr">Code1</a> · <a href="https://github.com/zijiechenrobotics/lm_calibr">Code2</a> · <a href="http://github.com/zijiechenrobotics/lm">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \textcolor{blue}{\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr}}. The video is available at \textcolor{blue}{\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>旋转驱动激光雷达系统的精确标定与鲁棒激光雷达惯性里程计</div>
<div class="mono" style="margin-top:8px">精确标定与鲁棒定位是旋转驱动激光雷达应用下游任务的基础。然而，现有方法需根据不同安装构型对外参进行参数化，限制了其泛化能力。此外，旋转驱动激光雷达不可避免地会扫描特征缺失区域，这使扫描覆盖范围与定位鲁棒性之间的平衡复杂化。为解决这些挑战，本文基于Denavit-Hartenberg约定提出无靶标激光雷达-电机标定方法（LM-Calibr）以及环境自适应激光雷达惯性里程计（EVA-LIO）。LM-Calibr支持多种安装构型的激光雷达-电机系统标定。大量实验验证了其在不同场景、安装角度和初始值下的精度与收敛性。此外，EVA-LIO根据空间尺度自适应选择下采样率与地图分辨率。这种自适应性使驱动器能以最高速度运行，从而在确保鲁棒定位的同时提升扫描完整性，即使激光雷达短暂扫描特征缺失区域时亦然。源代码与硬件设计发布于GitHub：https://github.com/zijiechenrobotics/lm_calibr。演示视频可见：https://youtu.be/cZyyrkmeoSk</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for accurate calibration and robust localization in spinning actuated LiDAR systems, where existing methods lack generalizability across different mounting configurations and struggle with featureless regions that compromise robustness. The proposed solution introduces a targetless LiDAR-motor calibration method (LM-Calibr) based on the Denavit-Hartenberg convention to handle various mounting setups, and an environmental adaptive LiDAR-inertial odometry (EVA-LIO) that dynamically adjusts downsample rates and map resolutions based on spatial scale. Experimental results show that LM-Calibr achieves accurate and convergent calibration across diverse scenarios, mounting angles, and initial values, while EVA-LIO enables the actuator to operate at maximum speed, enhancing scanning completeness and maintaining robust localization even during brief scans of featureless areas.</div>
<div class="mono" style="margin-top:8px">本研究针对旋转驱动激光雷达系统，解决了其精确标定和鲁棒定位的挑战，现有方法因缺乏对不同安装配置的普适性且在特征缺失区域易导致定位鲁棒性下降。提出的解决方案包括：基于Denavit-Hartenberg约定的无目标激光雷达-电机标定方法（LM-Calibr），以处理各种安装配置；以及环境自适应激光雷达-惯性里程计（EVA-LIO），可根据空间尺度动态调整下采样率和地图分辨率。实验结果表明，LM-Calibr在不同场景、安装角度和初始值下均能实现精确且收敛的标定，而EVA-LIO使驱动器能以最大速度运行，提高了扫描完整性，并在扫描特征缺失区域时仍能保持鲁棒的定位性能。</div>
</details>
</div>
<div class="card">
<div class="title">TeNet: Text-to-Network for Compact Policy Synthesis</div>
<div class="meta-line">Authors: Ariyan Bighashdel, Kevin Sebastian Luck</div>
<div class="meta-line">First: 2026-01-22T12:42:30+00:00 · Latest: 2026-01-22T12:42:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15912v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15912v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TeNet：面向紧凑策略合成的文本到网络框架</div>
<div class="mono" style="margin-top:8px">遵循自然语言指令的机器人通常采用两种方式：通过人工设计的接口进行高层规划，或依赖难以实时部署的大型端到端模型。本文提出TeNet（文本到网络）框架，可直接从自然语言描述实例化紧凑的任务专用机器人策略。TeNet基于预训练大语言模型生成的文本嵌入条件化超网络，生成完全可执行的策略，该策略仅需在控制高频下处理低维状态输入。通过在策略实例化时仅使用一次语言描述，TeNet继承了预训练大语言模型的通用知识与语义泛化鲁棒性，同时保持轻量高效的运行时特性。为提升泛化能力，我们在训练中通过对齐文本嵌入与演示动作实现语言的行为锚定，而推理阶段无需任何演示。在MuJoCo和Meta-World基准测试中，TeNet生成的策略比基于序列的基线模型小数个数量级，同时在多任务与元学习场景中均表现优异，并支持高频控制。实验表明，文本条件化超网络为资源受限且需实时响应的机器人控制任务提供了一种构建紧凑语言驱动控制器的实用方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To bridge the gap between high-level language planning and efficient real-time robot control, this research introduces TeNet, a framework that synthesizes compact, task-specific policies directly from natural language instructions. The method conditions a hypernetwork on text embeddings from a pretrained large language model to generate an executable policy network, which then runs efficiently on low-dimensional state inputs; training can optionally align text with demonstrated actions for better generalization, without needing demonstrations at inference. Experimental results on MuJoCo and Meta-World benchmarks demonstrate that TeNet produces policies orders of magnitude smaller than sequence-based baselines while achieving strong performance in multi-task and meta-learning settings and supporting high-frequency control.</div>
<div class="mono" style="margin-top:8px">该研究针对机器人执行自然语言指令的部署挑战，现有方法要么依赖手工设计的高层规划器，要么使用难以实时控制的大型端到端模型。提出的TeNet框架利用基于预训练大语言模型文本嵌入调节的超网络，生成紧凑的任务特定策略，直接以高频处理低维状态输入，从而将语言处理与执行解耦。在MuJoCo和Meta-World基准测试中的实验结果表明，TeNet生成的策略比基于序列的基线模型小几个数量级，同时在多任务和元学习设置中表现出色，支持资源受限机器人任务的高频实时控制。</div>
</details>
</div>
<div class="card">
<div class="title">Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach</div>
<div class="meta-line">Authors: Eric Hirsch, Christian Friedrich</div>
<div class="meta-line">First: 2024-12-27T23:10:32+00:00 · Latest: 2026-01-22T11:22:37+00:00</div>
<div class="meta-line">Comments: This work is a preprint and has been submitted for possible publication,14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.19950v5">Abs</a> · <a href="https://arxiv.org/pdf/2412.19950v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate tool wear prediction is essential for maintaining productivity and minimizing costs in machining. However, the complex nature of the tool wear process poses significant challenges to achieving reliable predictions. This study explores data-driven methods, in particular deep learning, for tool wear prediction. Traditional data-driven approaches often focus on a single process, relying on multi-sensor setups and extensive data generation, which limits generalization to new settings. Moreover, multi-sensor integration is often impractical in industrial environments. To address these limitations, this research investigates the transferability of predictive models using minimal training data, validated across two processes. Furthermore, it uses a simple setup with a single acceleration sensor to establish a low-cost data generation approach that facilitates the generalization of models to other processes via transfer learning. The study evaluates several machine learning models, including transformer-inspired convolutional neural networks (CNN), long short-term memory networks (LSTM), support vector machines (SVM), and decision trees, trained on different input formats such as feature vectors and short-time Fourier transform (STFT). The performance of the models is evaluated on two machines and on different amounts of training data, including scenarios with significantly reduced datasets, providing insight into their effectiveness under constrained data conditions. The results demonstrate the potential of specific models and configurations for effective tool wear prediction, contributing to the development of more adaptable and efficient predictive maintenance strategies in machining. Notably, the ConvNeXt model has an exceptional performance, achieving 99.1\% accuracy in identifying tool wear using data from only four milling tools operated until they are worn.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于工艺集成单传感器方法的铣削数据驱动刀具磨损预测</div>
<div class="mono" style="margin-top:8px">精准的刀具磨损预测对维持加工生产率和控制成本至关重要，但磨损过程的复杂性给可靠预测带来显著挑战。本研究探索数据驱动方法（特别是深度学习）用于刀具磨损预测。传统数据驱动方法常局限于单一工艺，依赖多传感器配置和大量数据生成，限制了其在新场景的泛化能力，且多传感器集成在工业环境中往往不切实际。为突破这些局限，本研究通过两个工艺验证，探究了使用最少训练数据的预测模型可迁移性，并采用单加速度传感器的简易配置建立低成本数据生成方案，通过迁移学习促进模型向其他工艺泛化。研究评估了多种机器学习模型（包括Transformer启发的卷积神经网络、长短期记忆网络、支持向量机和决策树），这些模型基于特征向量和短时傅里叶变换等不同输入格式进行训练。通过在两种机床及不同训练数据量（包括大幅缩减的数据集场景）上的性能评估，揭示了模型在受限数据条件下的有效性。结果表明特定模型与配置能实现有效的刀具磨损预测，有助于开发适应性更强、更高效的加工预测性维护策略。值得注意的是，ConvNeXt模型表现卓越，仅使用四把磨损铣刀的数据就实现了99.1%的刀具磨损识别准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of accurate tool wear prediction in milling, which is crucial for productivity and cost control, by exploring data-driven methods that overcome the limitations of traditional multi-sensor setups and extensive data requirements. The study investigates the transferability of predictive models using minimal training data and a low-cost, process-integrated approach with a single acceleration sensor, evaluating various machine learning models including transformer-inspired CNNs, LSTMs, SVMs, and decision trees on different input formats like feature vectors and STFT across two machines. Key experimental findings show that specific models, particularly the ConvNeXt model, achieve high effectiveness under constrained data conditions, with ConvNeXt reaching 99.1% accuracy in tool wear identification using data from only four worn milling tools.</div>
<div class="mono" style="margin-top:8px">精确的刀具磨损预测对于保证加工生产率和控制成本至关重要，但传统的多传感器数据驱动方法通常复杂且泛化能力不足。本研究提出了一种低成本、工艺集成的单加速度传感器方法，并探索迁移学习，以使用最少的训练数据实现模型在不同铣削工艺间的泛化。在两台机床上使用缩减数据集的实验评估表明，深度学习模型，特别是ConvNeXt架构，表现出色，仅使用四个磨损刀具的数据就实现了99.1%的刀具磨损识别准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</div>
<div class="meta-line">Authors: Libo Wang</div>
<div class="meta-line">First: 2025-11-30T08:37:01+00:00 · Latest: 2026-01-22T10:28:40+00:00</div>
<div class="meta-line">Comments: The Sigma model has been open-sourced on Hugging Face. Weights, dataset, some scripts, and logs are all available. The link is: https://huggingface.co/Veltraxor/Sigma</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00783v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.00783v3">PDF</a> · <a href="https://huggingface.co/Veltraxor/Sigma">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To address a fundamental limitation in cognitive systems, namely the absence of a time-updatable mediating thought space between semantics and continuous control, this work constructs and trains a vision-language-action model termed Sigma, deployed on a single RTX 4090. The model is built upon the open-source pi0.5_base backbone, with the svla_so101_pickplace dataset preprocessed into a structured training corpus. An independently designed VLA architecture is introduced to integrate deep semantic understanding with associative reasoning, enabling telepathic-style alignment between perception and action. Training proceeds through iterative optimization of data preprocessing, LoRA-based fine-tuning, and inference-stage adapter design. Evaluation is conducted using offline closed-loop replay, comparing Sigma against the untuned pi0.5_base under identical data conditions. Experimental results indicate a consistent reduction in control MSE across vector-, fragment-, and trajectory-level scales, while preserving the stability of the telepathy norm and semantic-text alignment quality. These findings demonstrate that mind-responsive alignment control can be quantitatively achieved through semantic and associative architectural integration without retraining the base model, providing a reproducible pathway for semantic alignment and intention-driven behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sigma：实现心电感应对齐的视觉-语言-动作模型关键</div>
<div class="mono" style="margin-top:8px">为解决认知系统中语义与连续控制间缺乏时间可更新中介思维空间的核心局限，本研究构建并训练了名为Sigma的视觉-语言-动作模型，部署于单张RTX 4090显卡。该模型基于开源pi0.5_base主干网络，将svla_so101_pickplace数据集预处理为结构化训练语料。通过独立设计的VLA架构融合深度语义理解与关联推理，实现感知与动作间类心电感应的对齐。训练过程通过数据预处理迭代优化、基于LoRA的微调及推理阶段适配器设计逐步推进。采用离线闭环回放评估，在相同数据条件下将Sigma与未调优的pi0.5_base进行对比。实验结果表明：在向量级、片段级和轨迹级尺度上控制均方误差持续降低，同时保持心电感应范数稳定性与语义-文本对齐质量。这些发现证明，通过语义与关联架构整合可定量实现思维响应式对齐控制，无需重训基础模型，为语义对齐与意图驱动行为提供了可复现的技术路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of a time-updatable mediating thought space between high-level semantics and low-level continuous control in cognitive systems. The method constructs a vision-language-action model called Sigma, based on the pi0.5_base backbone, using a preprocessed dataset and a novel architecture designed to integrate deep semantic understanding with associative reasoning for telepathic-style alignment. Experimental evaluation via offline closed-loop replay shows that Sigma, compared to the untuned base model, achieves a consistent reduction in control mean squared error at vector-, fragment-, and trajectory-level scales while maintaining alignment stability and semantic-text quality, demonstrating that mind-responsive control can be achieved through architectural integration without retraining the base model.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决认知系统中缺乏一个在高层语义与低层连续控制之间可随时间更新的中介思维空间这一根本局限。作者基于pi0.5_base骨干网络，利用预处理后的svla_so101_pickplace数据集，构建并训练了一个名为Sigma的视觉-语言-动作模型，其独立设计的架构整合了深度语义理解与关联推理，以实现感知与动作之间的“心灵感应”式对齐。通过数据预处理、基于LoRA的微调和推理阶段适配器设计的迭代优化，该模型得以训练，并采用离线闭环回放进行评估。实验结果表明，与未经调优的基础模型相比，Sigma在向量、片段和轨迹级别上持续降低了控制均方误差，同时保持了“心灵感应”范数的稳定性和语义-文本对齐质量，证明无需重新训练基础模型即可实现思维响应的对齐控制。</div>
</details>
</div>
<div class="card">
<div class="title">A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation</div>
<div class="meta-line">Authors: Alexandre Albore, Humbert Fiorino, Damien Pellier</div>
<div class="meta-line">First: 2026-01-22T09:34:56+00:00 · Latest: 2026-01-22T09:34:56+00:00</div>
<div class="meta-line">Comments: 8 pages. IEEE TechDefense 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15802v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信标的水下无人潜航器GNSS拒止隐蔽导航解决方案</div>
<div class="mono" style="margin-top:8px">自主式水下无人潜航器（UUV）可在不依赖支援舰船或全球导航卫星系统（GNSS）的情况下，于沿海区域执行军民隐蔽任务。当无法通过水面进入且需在保护区或禁入危险区等受限环境中进行隐蔽航行时，此类任务尤为关键。GNSS拒止导航对保持隐蔽性至关重要，因为上浮可能导致UUV暴露。为确保编队精确定位，通过空中或水面无人机部署的信标星座构建合成地标网络，引导UUV编队沿大陆架至岸上目标的最优路径航行。这些沉底或漂浮信标发射声学信号用于UUV定位导航。分层规划器为执行基础动作的无人机生成自适应航线，同时持续监测并在必要时重新规划以保持轨迹精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling stealthy navigation for autonomous Unmanned Underwater Vehicles (UUVs) in GNSS-denied coastal environments, which is critical for covert military and civilian operations where surfacing for a GPS fix would compromise concealment. The proposed method involves deploying a constellation of acoustic beacons via aerial or surface drones to create a synthetic landmark network; a hierarchical planner then generates and continuously adapts optimized routes for the UUV fleet based on these signals. Experimental findings indicate that this beacon-based solution allows UUVs to maintain precise positioning and navigate accurately from the continental shelf to a shore-based goal without relying on GNSS.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自主无人水下航行器在GNSS拒止的沿海环境（如保护区或危险区域）中，需要隐蔽且精确导航的问题，因为上浮获取卫星信号会暴露目标。所提出的方法是通过空中或水面无人机部署声学信标星座，构建一个合成地标网络；随后，一个分层规划器基于这些信号为UUV舰队生成并自适应地重新规划优化路径。实验结果表明，这种基于信标的解决方案能够实现从大陆架到岸上目标的水下精确定位与引导，同时保持行动的隐蔽性。</div>
</details>
</div>
<div class="card">
<div class="title">Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV</div>
<div class="meta-line">Authors: Amir Habel, Ivan Snegirev, Elizaveta Semenyakina, Miguel Altamirano Cabrera, Jeffrin Sam, Fawad Mehboob, Roohan Ahmed Khan, Muhammad Ahsan Mustafa, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-22T09:03:57+00:00 · Latest: 2026-01-22T09:03:57+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15775v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15775v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Glove2UAV：基于可穿戴IMU手套的无人机直观控制系统</div>
<div class="mono" style="margin-top:8px">本文提出Glove2UAV——一种基于惯性测量单元的可穿戴手套接口，通过手部与手指姿态实现无人机的直观控制，并集成振动触觉预警功能以提示超速状态。为提升动态飞行中的安全性与可预测性，该系统设计为轻量化、易部署的可穿戴实时操作界面。Glove2UAV实时传输惯性测量数据，并通过融合中值离群抑制与Madgwick方向估计算法的紧凑处理流程，实时估计手掌与手指朝向。运动估计结果被映射为方向飞行（前进/后退与横向移动）的基础控制指令，并在平台支持时扩展至物体交互指令。当飞行速度超过预设阈值时，系统触发振动触觉反馈以提供额外警示通道。我们通过在仿真与真实飞行中同步手套信号与无人机遥测数据，验证了系统的实时可行性。实验结果表明：该系统能快速执行手势指令，保持手势动态与平台运动的稳定耦合，在测试中正确执行核心指令集，并及时传递振动预警信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enable more intuitive and safer control of unmanned aerial vehicles (UAVs) by developing a wearable glove interface. The method involves a glove equipped with inertial measurement units (IMUs) that streams real-time data, processes it using median-based outlier suppression and Madgwick-based orientation estimation to track hand and finger gestures, and maps these gestures to UAV control primitives for directional flight and object interaction, augmented with vibrotactile warnings for excessive speed. Experimental validation in simulation and real-world flights demonstrated fast gesture command execution, stable coupling between gesture dynamics and UAV motion, correct operation of the core command set, and timely delivery of vibrotactile alerts.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过开发一种可穿戴手套接口，实现更直观、更安全的无人机控制。所提出的Glove2UAV系统使用惯性测量单元捕捉手部和手指手势，通过结合中值滤波和Madgwick方向估计的流程处理数据，并将估计的运动映射为无人机飞行指令，同时提供超速的振动触觉警告。在模拟和真实飞行中的实验验证表明，该系统手势执行快速，手势动态与平台运动耦合稳定，核心指令集运行可靠，且触觉警告能及时有效传递。</div>
</details>
</div>
<div class="card">
<div class="title">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</div>
<div class="meta-line">Authors: Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T17:59:44+00:00 · Latest: 2026-01-22T08:52:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project page: https://faceong.github.io/VIKI-R/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09049v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09049v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://faceong.github.io/VIKI-R/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIKI-R：基于强化学习的具身多智能体协同协调框架</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体仍是人工智能的核心挑战，需要感知驱动的推理与可扩展的协作策略。尽管近期研究已利用大语言模型进行多智能体规划，但基于视觉语言模型的视觉推理探索仍处于起步阶段，且现有方法对多样化具身形态的支持有限。本研究提出首个面向具身多智能体协作的层次化基准VIKI-Bench，包含智能体激活、任务规划与轨迹感知三层结构，涵盖多类机器人形态、多视角视觉观测及结构化监督信号，以评估基于视觉输入的推理能力。为验证该基准的实用性，我们提出两阶段框架VIKI-R：首先利用思维链标注数据微调预训练视觉语言模型，随后在多级奖励信号下进行强化学习。实验表明VIKI-R在所有任务层级上均显著优于基线方法，且强化学习能促使异构智能体形成组合式协作模式。VIKI-Bench与VIKI-R共同为具身AI系统中的多智能体视觉驱动协作提供了统一测试平台与方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments requires both visual reasoning and scalable cooperation strategies, but existing vision-language model (VLM) approaches often lack support for diverse robot embodiments. To address this, the authors introduce VIKI-Bench, a hierarchical benchmark for embodied multi-agent cooperation with structured levels for agent activation, task planning, and trajectory perception, and propose VIKI-R, a two-stage framework that fine-tunes a pretrained VLM with Chain-of-Thought demonstrations and then applies reinforcement learning with multi-level rewards. Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels and enables the emergence of compositional cooperation patterns among heterogeneous agents.</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体需要可扩展的合作策略和感知驱动的推理，但现有的视觉语言模型方法通常缺乏对不同机器人具身形态的支持。为此，研究者提出了VIKI-Bench，这是一个为具身多智能体合作设计的层次化基准测试，包含智能体激活、任务规划和轨迹感知三个结构化层级，并提出了VIKI-R，一个两阶段框架：首先使用思维链标注的演示数据对预训练的视觉语言模型进行微调，然后通过多层级奖励信号进行强化学习。实验结果表明，VIKI-R在所有任务层级上均显著优于基线方法，并能在异构智能体之间涌现出组合式的合作模式。</div>
</details>
</div>
<div class="card">
<div class="title">DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving</div>
<div class="meta-line">Authors: Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma</div>
<div class="meta-line">First: 2026-01-22T07:56:36+00:00 · Latest: 2026-01-22T07:56:36+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DualShield：基于可达性分析的交互式自动驾驶安全模型预测扩散方法</div>
<div class="mono" style="margin-top:8px">扩散模型已成为自动驾驶多模态运动规划的有效方法，但其实际部署常受限于难以严格遵循车辆动力学，且高度依赖对其他智能体的精确预测，导致在不确定交互中易出现安全问题。为应对这些局限，我们提出DualShield——一种利用汉密尔顿-雅可比（HJ）可达性值函数双重能力的规划控制框架。首先，该值函数作为主动引导，将扩散去噪过程导向安全且动力学可行的区域；其次，其通过控制屏障值函数（CBVF）构成反应式安全屏障，修正执行动作以确保安全性。这种双重机制在保留扩散模型丰富探索能力的同时，为不确定甚至对抗性交互场景提供理论安全保障。在无保护U形转弯等挑战性场景的仿真表明，相较于不确定条件下不同规划范式的领先方法，DualShield在安全性与任务效率上均有显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses safety concerns in deploying diffusion models for autonomous driving motion planning, which struggle with enforcing vehicle dynamics and rely heavily on accurate predictions of other agents, leading to risks under uncertain interactions. The proposed DualShield framework employs Hamilton-Jacobi reachability value functions in a dual role: proactively guiding the diffusion denoising process toward safe, dynamically feasible regions and reactively forming a safety shield using control barrier-value functions to adjust executed actions. Experimental simulations in unprotected U-turn scenarios show that DualShield substantially enhances both safety and task efficiency compared to leading methods across different planning paradigms under uncertainty.</div>
<div class="mono" style="margin-top:8px">该研究针对扩散模型在自动驾驶运动规划中部署的安全挑战，这些模型难以强制执行车辆动力学，且严重依赖对其他智能体的准确预测，导致不确定交互下的安全风险。提出的DualShield框架以双重方式利用哈密顿-雅可比可达性值函数：主动引导扩散去噪过程朝向安全且动态可行的区域，以及被动地利用控制障碍值函数形成安全屏障以修改执行动作。在无保护U形转弯场景的仿真实验中，DualShield相比不确定性下不同规划范式的领先方法，显著提高了安全性和任务效率。</div>
</details>
</div>
<div class="card">
<div class="title">D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot</div>
<div class="meta-line">Authors: Qifan Hu, Branko Celler, Weidong Mu, Steven W. Su</div>
<div class="meta-line">First: 2026-01-22T07:20:55+00:00 · Latest: 2026-01-22T07:20:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于D最优性引导的强化学习用于三自由度踝关节康复机器人高效开环标定</div>
<div class="mono" style="margin-top:8px">多自由度康复机器人的精确对准对安全有效的患者训练至关重要。本文针对自主设计的三自由度踝关节康复机器人提出了一种两阶段标定框架。首先，开发了一种基于Kronecker积的开环标定方法，将输入输出对准问题转化为线性参数辨识问题，进而通过生成的信息矩阵定义相关实验设计目标。基于此公式，标定姿态选择被构建为以D最优性准则指导的组合实验设计问题，即选择能最大化信息矩阵行列式的小规模姿态子集。为在约束下实现实际选择，在仿真中训练近端策略优化智能体从50个候选姿态中选取4个信息量最大的姿态。仿真与真实机器人实验表明，学习策略始终比随机选择获得显著更高信息量的姿态组合：PPO所得信息矩阵行列式均值比随机选择高两个数量级以上且方差更低。此外，真实实验结果显示，仅通过四个D最优性引导姿态辨识的参数向量，比从50个无结构姿态集中获得的估计具有更强的跨周期预测一致性。该框架在保持稳健参数估计的同时提升了标定效率，为多自由度康复机器人的高精度对准提供了实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate alignment of multi-degree-of-freedom rehabilitation robots is crucial for safe and effective therapy, motivating the development of an efficient calibration method. The proposed two-stage framework first formulates the calibration of a 3-DOF ankle robot as a linear parameter identification problem using a Kronecker-product-based method, then frames posture selection as a combinatorial design-of-experiments problem optimized via a D-optimality criterion. To solve this, a Proximal Policy Optimization (PPO) agent is trained in simulation to select a small, informative set of four postures from 50 candidates. Experimental results in simulation and on a real robot show that the PPO-selected postures yield an information matrix determinant over two orders of magnitude higher than random selection, with reduced variance, and the resulting parameter estimates provide stronger cross-episode prediction consistency than those from a larger, unstructured set of 50 postures.</div>
<div class="mono" style="margin-top:8px">多自由度康复机器人的精确对准对于安全有效的训练至关重要，这促使本研究为一种3自由度踝关节康复机器人开发高效的标定方法。所提出的两阶段框架首先使用基于Kronecker积的开环标定方法将输入-输出对准转化为线性参数辨识问题，进而通过所得信息矩阵将标定姿态选择构建为以D最优性准则为指导的组合实验设计问题；在仿真中训练近端策略优化（PPO）智能体从50个候选姿态中选择四个最优姿态。仿真和真实机器人实验结果表明，PPO选择的姿态所获得的信息矩阵行列式值比随机选择高两个数量级以上且方差更低，仅用四个D最优姿态辨识的参数向量比使用50个非结构化姿态得到的估计具有更强的跨周期预测一致性，从而在保证参数估计鲁棒性的同时提高了标定效率。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Layered Reasoning from a Single Viewpoint for Learning See-Through Grasping</div>
<div class="meta-line">Authors: Fang Wan, Chaoyang Song</div>
<div class="meta-line">First: 2023-12-15T14:21:14+00:00 · Latest: 2026-01-22T07:14:46+00:00</div>
<div class="meta-line">Comments: 39 pages, 13 figures, 2 tables, for supplementary videos, see https://bionicdl.ancorasir.com/?p=1658, for opensourced codes, see https://github.com/ancorasir/SeeThruFinger</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.09822v5">Abs</a> · <a href="https://arxiv.org/pdf/2312.09822v5">PDF</a> · <a href="https://github.com/ancorasir/SeeThruFinger">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sensory substitution enables biological systems to perceive stimuli that are typically perceived by another organ, which is inspirational for physical agents. Multimodal perception of intrinsic and extrinsic interactions is critical in building an intelligent robot that learns. This study presents a Vision-based See-Through Perception (VBSeeThruP) architecture that simultaneously perceives multiple intrinsic and extrinsic modalities from a single visual input, in a markerless manner, all packed into a soft robotic finger using the Soft Polyhedral Network design. It is generally applicable to miniature vision systems placed beneath deformable networks with a see-through design, capturing real-time images of the network&#x27;s physical interactions induced by contact-based events, overlaid on the visual scene of the external environment, as demonstrated in the ablation study. We present the VBSeeThruP&#x27;s capability for learning reactive grasping without using external cameras or dedicated force and torque sensors on the fingertips. Using the inpainted scene and the deformation mask, we further demonstrate the multimodal performance of the VBSeeThruP architecture to simultaneously achieve various perceptions, including but not limited to scene inpainting, object detection, depth sensing, scene segmentation, masked deformation tracking, 6D force/torque sensing, and contact event detection, all within a single sensory input from the in-finger vision markerlessly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单视角多层推理的学习型透视抓取研究</div>
<div class="mono" style="margin-top:8px">感觉替代使生物系统能感知通常由其他器官接收的刺激，这对物理智能体具有启发意义。内在与外在交互的多模态感知对构建学习型智能机器人至关重要。本研究提出一种基于视觉的透视感知架构，能从单一视觉输入中以无标记方式同步感知多种内在与外在模态，并通过软体多面体网络设计集成于软体机器人手指中。该架构普遍适用于部署在可变形网络下方的微型视觉系统，采用透视设计实时捕捉接触事件引发的网络物理交互图像，并叠加于外部环境视觉场景上（如消融实验所示）。我们展示了该架构无需外部摄像头或指尖专用力/力矩传感器即可实现反应式抓取学习的能力。利用修复场景与形变掩膜，进一步验证了该架构同步实现多模态感知的性能，包括但不限于：场景修复、物体检测、深度感知、场景分割、掩膜形变追踪、六维力/力矩感知及接触事件检测——所有功能均基于单一无标记指内视觉传感器输入。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Inspired by sensory substitution in biological systems, this research aims to enable robots to learn reactive grasping through multimodal perception without relying on external cameras or dedicated force/torque sensors. The method introduces a Vision-based See-Through Perception (VBSeeThruP) architecture, which uses a single visual input from a miniature camera inside a soft robotic finger to simultaneously capture intrinsic deformations and extrinsic environmental scenes in a markerless manner. Experimental results demonstrate that this architecture can achieve various perceptual tasks from one sensory input, including scene inpainting, object detection, depth sensing, and 6D force/torque sensing, and successfully enables learning of reactive grasping.</div>
<div class="mono" style="margin-top:8px">本研究受生物感官替代启发，旨在让智能机器人从单一感官输入中感知多模态的内在与外在交互。方法上提出了视觉透视感知架构，通过在软体机器人手指内集成摄像头，以无标记方式实时捕获内部变形与外部环境的叠加图像。主要实验结果表明，该系统无需外部摄像头或专用力传感器即可学习反应式抓取，并能同时从单一视觉输入实现场景修复、物体检测、深度感知、场景分割、变形跟踪、六维力/力矩传感和接触事件检测。</div>
</details>
</div>
<div class="card">
<div class="title">AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Zichen Yan, Yuchen Hou, Shenao Wang, Yichao Gao, Rui Huang, Lin Zhao</div>
<div class="meta-line">First: 2026-01-22T03:35:34+00:00 · Latest: 2026-01-22T03:35:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15614v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15614v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AION：基于双策略强化学习的空中室内目标导航系统</div>
<div class="mono" style="margin-top:8px">目标导航任务要求智能体在未知环境中自主探索，并导航至语义标签指定的目标物体。现有研究主要关注二维平面移动的零样本目标导航，而将其扩展至具备三维移动能力的空中平台仍待深入探索。空中机器人虽具备卓越的机动性与搜索效率，但也带来了空间感知、动态控制与安全保障的新挑战。本文提出AION系统，实现不依赖外部定位或全局地图的视觉空中目标导航。AION采用端到端双策略强化学习框架，将探索行为与目标抵达行为解耦为两个专用策略。我们在AI2-THOR基准测试中评估AION，并基于高精度无人机模型在IsaacSim中验证其实时性能。实验结果表明，AION在探索能力、导航效率与安全性等综合评估指标上均表现优异。演示视频详见：https://youtu.be/TgsUm6bb7zg。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the underexplored challenge of extending Object-Goal Navigation (ObjectNav) to aerial robots, which offer superior 3D maneuverability but introduce difficulties in perception, control, and safety. The proposed method, AION, is an end-to-end dual-policy reinforcement learning framework that decouples the task into separate specialized policies for exploration and goal-reaching, operating without external localization or global maps using only vision. Experimental evaluation on the AI2-THOR benchmark and in IsaacSim with high-fidelity drone models demonstrates that AION achieves superior performance in exploration efficiency, navigation success, and safety metrics compared to prior approaches.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将目标物体导航任务从二维地面智能体扩展到具有三维运动能力的空中平台这一尚未充分探索的挑战，其动机在于无人机具有更优的机动性和搜索效率，但同时也带来了感知、控制和安全性方面的新难题。所提出的方法AION是一种端到端的双策略强化学习框架，它将任务解耦为专门用于探索和抵达目标的两个策略，仅依赖视觉输入，无需外部定位或全局地图。在AI2-THOR基准测试以及使用高保真无人机模型在IsaacSim中的实时性能评估表明，AION在探索能力、导航效率和安全性等综合指标上均取得了优越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor</div>
<div class="meta-line">Authors: Lenworth Thomas, Tjaden Bridges, Sarah Bergbreiter</div>
<div class="meta-line">First: 2026-01-22T03:13:24+00:00 · Latest: 2026-01-22T03:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15607v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors &lt; 100 g. We use this sensor to implement a modified version of the `Cast and Surge&#x27; algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单气流传感器的小型四旋翼气流源追踪</div>
<div class="mono" style="margin-top:8px">随着环境灾害日益频发且严重，利用羽流追踪技术定位污染物或有害颗粒源的重要性愈发凸显。在小型四旋翼飞行器上实现羽流追踪，可使系统更贴近人类活动区域并适应狭窄空间作业，但受限于小型四旋翼搭载的气体传感器灵敏度低、响应时间长等挑战。本研究提出一种结合定制气流传感器的气流源追踪方法，该传感器可在重量小于100克的小型四旋翼上同步感知气流强度与方向。基于此传感器，我们改进了&#x27;抛射-突进&#x27;算法，利用气流方向感知能力实现流向源的定位与导航。通过系列特性实验验证，该系统能在飞行中检测气流并调整四旋翼朝向气流方向。多组随机起始位置与方向的测试表明，该溯源算法能稳定定位气流源。本研究旨在为未来平台奠定基础，通过融合气流传感器与其他传感器，实现更丰富的羽流追踪数据采集与溯源能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing frequency of environmental disasters and the need for small quadrotors to track pollutant plumes in confined spaces, this work addresses the limitations of gas sensors by introducing an airflow source-seeking method using a custom flow sensor capable of measuring both magnitude and direction on sub-100g quadrotors. The method implements a modified &#x27;Cast and Surge&#x27; algorithm that leverages directional airflow sensing to locate and navigate toward flow sources. Experimental results demonstrate that the system can detect airflow during flight, reorient the quadrotor accordingly, and reliably find a flow source from random starting positions and orientations.</div>
<div class="mono" style="margin-top:8px">本研究针对环境灾害频发、小型四旋翼飞行器需在受限空间追踪污染物羽流的需求，旨在克服气体传感器灵敏度低、响应慢的局限，提出了一种利用定制流量传感器进行气流源搜寻的方法，该传感器可在重量小于100克的飞行器上感知气流大小和方向。该方法采用改进的&#x27;抛射与突进&#x27;算法，利用气流方向感知来定位并导航至气流源。实验表征验证了系统能在飞行中检测气流并使飞行器朝向气流重新定向，从随机起始位置进行的多次试验表明，该源搜寻算法能可靠地找到气流源，为未来结合多种传感器实现更丰富的羽流追踪数据收集和源搜寻奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Natural Language Environment: Understanding Seamless Natural-Language-Based Human-Multi-Robot Interactions</div>
<div class="meta-line">Authors: Ziyi Liu, Xinyi Wang, Shao-Kang Hsia, Chenfei Zhu, Zhengzhe Zhu, Xiyun Hu, Anastasia Kouvaras Ostrowski, Karthik Ramani</div>
<div class="meta-line">First: 2026-01-19T19:22:00+00:00 · Latest: 2026-01-22T02:50:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13338v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13338v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As multiple robots are expected to coexist in future households, natural language is increasingly envisioned as a primary medium for human-robot and robot-robot communication. This paper introduces the concept of a Natural Language Environment (NLE), defined as an interaction space in which humans and multiple heterogeneous robots coordinate primarily through natural language.
  Rather than proposing a deployable system, this work aims to explore the design space of such environments. We first synthesize prior work on language-based human-robot interaction to derive a preliminary design space for NLEs. We then conduct a role-playing study in virtual reality to investigate how people conceptualize, negotiate, and coordinate human-multi-robot interactions within this imagined environment.
  Based on qualitative and quantitative analysis, we refine the preliminary design space and derive design implications that highlight key tensions and opportunities around task coordination dominance, robot autonomy, and robot personality in Natural Language Environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向自然语言环境：理解基于自然语言的无缝人-多机器人交互</div>
<div class="mono" style="margin-top:8px">随着未来家庭中多机器人共存成为趋势，自然语言日益被视为人机及机器人间通信的主要媒介。本文提出自然语言环境的概念，即人类与多个异构机器人主要通过自然语言进行协调的交互空间。本研究并非旨在构建可部署系统，而是探索此类环境的设计空间。我们首先综合基于语言的人机交互先前研究，推导出自然语言环境的初步设计框架；随后通过虚拟现实中的角色扮演实验，探究人们在此设想环境中如何概念化、协商并协调人-多机器人交互。基于定性与定量分析，我们完善了初步设计框架，并提出设计启示，着重探讨自然语言环境中任务协调主导权、机器人自主性与机器人个性之间的核心矛盾与机遇。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the anticipated coexistence of multiple robots in domestic settings, this paper explores the design space for Natural Language Environments (NLEs), where humans and heterogeneous robots coordinate primarily through natural language. The method involves synthesizing prior work to derive a preliminary design space and then conducting a role-playing study in virtual reality to investigate how people conceptualize and negotiate interactions within such an environment. Key experimental findings from qualitative and quantitative analysis refine the design space, highlighting critical tensions and opportunities related to task coordination dominance, robot autonomy, and robot personality.</div>
<div class="mono" style="margin-top:8px">本文针对未来家庭中多机器人系统共存的预期，旨在探索自然语言环境的设计空间，该环境中人类与异构机器人主要通过自然语言进行协调。研究方法包括综合先前工作以推导初步设计空间，并通过虚拟现实中的角色扮演研究来探究人们在此类想象环境中如何概念化、协商和协调人机交互。基于定性与定量分析的关键实验结果，研究细化了初步设计空间，并得出了设计启示，突出了在任务协调主导权、机器人自主性和机器人个性方面的核心张力与机遇。</div>
</details>
</div>
<div class="card">
<div class="title">MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments</div>
<div class="meta-line">Authors: Cyril Shih-Huan Hsu, Xi Li, Lanfranco Zanzi, Zhiheng Yang, Chrysa Papagianni, Xavier Costa Pérez</div>
<div class="meta-line">First: 2026-01-22T01:57:48+00:00 · Latest: 2026-01-22T01:57:48+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at IEEE International Conference on Communications (ICC) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15578v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapViT：面向动态环境中实时无线质量地图预测的双阶段视觉Transformer框架</div>
<div class="mono" style="margin-top:8px">移动与无线网络的最新进展正全面释放机器人自主潜力，使其能够利用超低延迟、高数据吞吐量与泛在连接。然而，为实现无缝、高效、可靠的导航与操作，机器人必须准确感知周边环境与无线信号质量。在高度动态且持续变化的环境中实现这一目标，仍是亟待解决的挑战性难题。本文提出MapViT——一种受大语言模型预训练-微调范式启发的双阶段视觉Transformer框架，旨在同时预测环境变化与预期无线信号质量。我们通过一组代表性机器学习模型评估该框架，分析其在不同场景下的优势与局限。实验结果表明，所提出的双阶段流程可实现实时预测，基于ViT的实现方案在精度与计算效率间取得了良好平衡，使MapViT成为移动机器人等能源与资源受限平台的理想解决方案。此外，通过自监督预训练阶段衍生的几何基础模型提升了数据效率与可迁移性，即使在标注数据有限时也能实现有效的下游预测。总体而言，本研究为下一代数字孪生生态系统奠定基础，并为推动未来6G系统中多模态智能的新型机器学习基础模型开辟了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable seamless robot navigation in dynamic environments, accurate prediction of both environmental changes and radio signal quality is required, but remains challenging. This paper proposes MapViT, a two-stage Vision Transformer framework inspired by the pre-train and fine-tune paradigm of LLMs, designed for real-time prediction. Experimental evaluation shows the ViT-based implementation achieves a strong balance between accuracy and computational efficiency, enabling real-time operation on resource-constrained platforms, while the self-supervised pre-training stage improves data efficiency and transferability for downstream tasks with limited labeled data.</div>
<div class="mono" style="margin-top:8px">为使机器人在动态环境中实现无缝导航，需要准确预测环境变化和无线电信号质量，而这仍是一个未解决的挑战。本文提出了MapViT，一个受大语言模型预训练与微调范式启发的两阶段视觉Transformer框架，用于实时预测。实验结果表明，基于ViT的实现方案在预测精度与计算效率之间取得了良好平衡，支持在资源受限的移动机器人平台上实时运行，同时自监督预训练阶段提升了数据效率和模型可迁移性，使得在有限标注数据下也能进行有效的下游预测。</div>
</details>
</div>
<div class="card">
<div class="title">A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control</div>
<div class="meta-line">Authors: Zhifan Yan, Chang Liu, Yiyang Jiang, Wenxuan Zheng, Xinhao Chen, Axel Krieger</div>
<div class="meta-line">First: 2026-01-22T00:30:11+00:00 · Latest: 2026-01-22T00:30:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15545v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15545v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a &quot;model-calibration bottleneck&quot;, requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习控制的胃肠道导航移动磁操控平台</div>
<div class="mono" style="margin-top:8px">利用磁控机器人在胃肠道内进行靶向给药为全身性治疗提供了有前景的替代方案，但其精确控制仍面临重大挑战。传统固定式磁系统工作空间有限，而移动式系统（如搭载线圈的机械臂）存在“模型校准瓶颈”，需依赖耗时构建且计算成本高昂的预校准物理模型。本文提出一种紧凑型低成本移动磁操控平台，通过深度强化学习克服了这一局限。该系统将紧凑四电磁铁阵列搭载于UR5协作机器人，采用基于柔性执行器-评价器算法的控制策略，通过仿真到实物的训练流程，可在15分钟内实现有效策略部署，大幅缩短配置时间。平台通过操控7毫米磁胶囊沿二维轨迹运动进行验证：基于深度强化学习的控制器在方形路径上实现1.18毫米均方根误差，圆形路径上为1.50毫米，并在临床相关的30厘米×20厘米工作空间内成功完成轨迹跟踪。本研究展示了一种可快速部署的无模型控制框架，能在大型工作空间实现精确磁操控，并通过二维胃肠道模型验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of controlling magnetic robots for gastrointestinal drug delivery, where stationary systems have limited workspace and mobile systems face a model-calibration bottleneck requiring complex pre-calibrated physical models. The authors developed a compact mobile platform with four electromagnets mounted on a UR5 robot arm and implemented a Deep Reinforcement Learning control strategy using Soft Actor-Critic trained through a sim-to-real pipeline, enabling policy deployment within 15 minutes. Experimental results showed the controller achieved root-mean-square errors of 1.18 mm for square paths and 1.50 mm for circular paths while successfully tracking trajectories over a clinically relevant 30 cm × 20 cm workspace using a 2D gastrointestinal phantom.</div>
<div class="mono" style="margin-top:8px">利用磁力机器人进行胃肠道靶向给药前景广阔，但面临控制挑战：固定系统工作空间有限，而移动系统需要复杂、预先校准的物理模型，存在模型校准瓶颈。为此，本研究开发了一个紧凑、低成本的移动平台，将四电磁铁阵列安装在UR5机械臂上，并采用基于深度强化学习的无模型控制策略，具体通过模拟到现实的流程训练软演员-评论家算法，以实现快速部署。使用7毫米磁胶囊的实验验证表明，该控制器在方形和圆形二维轨迹上的均方根误差分别为1.18毫米和1.50毫米，并在胃肠道模型上成功实现了在30厘米乘20厘米临床相关工作空间内的跟踪。</div>
</details>
</div>
<div class="card">
<div class="title">CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Heng Zhang, Wei-Hsing Huang, Qiyi Tong, Gokhan Solak, Puze Liu, Sheng Liu, Jan Peters, Arash Ajoudani</div>
<div class="meta-line">First: 2026-01-21T23:52:40+00:00 · Latest: 2026-01-21T23:52:40+00:00</div>
<div class="meta-line">Comments: under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15541v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/compliantvla">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\% to 17.29\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CompliantVLA-adaptor：基于视觉语言模型引导的可变阻抗动作实现安全接触密集型操作</div>
<div class="mono" style="margin-top:8px">本文提出一种CompliantVLA-adaptor，通过视觉语言模型（VLM）感知任务上下文，为先进视觉-语言-动作（VLA）模型引入可变阻抗控制（VIC），以提升接触密集型机器人操作任务的安全性与有效性。现有VLA系统（如RDT、Pi0、OpenVLA-oft）通常仅输出位置指令，缺乏力感知适应能力，导致涉及接触、顺应性或不确定性的物理任务存在安全隐患或失败。所提出的适配器利用VLM解析图像与自然语言中的任务上下文，动态调整VIC控制器的刚度与阻尼参数，并结合实时力/力矩反馈调节参数，确保交互力维持在安全阈值内。实验表明，该方法在仿真与真实硬件的一系列复杂接触密集型任务中均优于基线VLA系统，成功率显著提升且违规力大幅降低。所有任务整体成功率从9.86%提升至17.29%，为VLA实现安全接触密集型操作提供了可行路径。代码、提示词及力-力矩-阻抗场景上下文数据集已发布于https://sites.google.com/view/compliantvla。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the safety limitations of existing Vision-Language-Action (VLA) models, which output only position commands and lack force adaptation, leading to failures in contact-rich robotic manipulation. The proposed CompliantVLA-adaptor augments VLA models by using a Vision-Language Model (VLM) to interpret task context from images and language, generating variable impedance control parameters (stiffness and damping) that are further regulated in real-time using force/torque feedback to maintain safe interaction forces. Experimental results on complex contact-rich tasks in simulation and on real hardware show the method outperforms VLA baselines, increasing the overall success rate from 9.86% to 17.29% while reducing force violations.</div>
<div class="mono" style="margin-top:8px">本研究针对现有视觉-语言-动作（VLA）模型仅输出位置指令、缺乏力适应能力，导致在接触密集的机器人操作中存在不安全或失败交互的问题。所提出的方法CompliantVLA-adaptor，通过使用视觉-语言模型（VLM）从图像和语言中解读任务上下文，来增强VLA模型；该上下文信息用于指导一个可变阻抗控制器，该控制器的刚度和阻尼参数进一步通过实时力/力矩反馈进行调节，以维持安全的交互力。在仿真和真实硬件上的复杂接触密集任务实验结果表明，该方法优于VLA基线模型，将整体成功率从9.86%提升至17.29%，同时减少了力违规情况。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
