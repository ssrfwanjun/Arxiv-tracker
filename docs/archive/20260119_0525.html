<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-19 05:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260119_0525</div>
    <div class="row"><div class="card">
<div class="title">DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids</div>
<div class="meta-line">Authors: Navami Kairanda, Shanthika Naik, Marc Habermann, Avinash Sharma, Christian Theobalt, Vladislav Golyanik</div>
<div class="meta-line">First: 2026-01-15T18:59:57+00:00 · Latest: 2026-01-15T18:59:57+00:00</div>
<div class="meta-line">Comments: 25 pages; 16 figures; project page: https://4dqv.mpi-inf.mpg.de/DInf-Grid/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10715v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10715v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DInf-Grid：一种具备可微分特征网格的神经微分方程求解器</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的基于可微分网格的表示方法，用于高效求解微分方程。现有神经求解器广泛采用的架构（如正弦神经网络）是基于坐标的多层感知机，其计算密集且训练缓慢。虽然基于网格的隐式表示替代方案（如Instant-NGP和K-Planes）通过利用信号结构实现了更快训练，但其对线性插值的依赖限制了高阶导数的计算能力，使其不适用于求解微分方程。我们的方法通过将特征网格的效率与无限可微的径向基函数插值相结合，克服了这些限制。为有效捕捉高频解并实现稳定快速的全局梯度计算，我们引入了共位网格的多分辨率分解。所提出的DInf-Grid表示以微分方程作为损失函数进行隐式训练，能够精确建模物理场。我们在多项任务中验证了DInf-Grid，包括图像重建的泊松方程、波场求解的亥姆霍兹方程，以及布料模拟的基尔霍夫-洛夫边值问题。实验结果表明，相较于基于坐标的MLP方法，本方法实现了5-20倍的加速，能在数秒至数分钟内求解微分方程，同时保持相当的精度与紧凑性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of coordinate-based MLPs like sinusoidal neural networks for solving differential equations (DEs), which are computationally intensive and slow to train. The method introduces DInf-Grid, a differentiable grid-based representation that combines multi-resolution feature grids with radial basis function interpolation to enable efficient, higher-order derivative computation and capture high-frequency solutions. Experimental validation on tasks including the Poisson equation, Helmholtz equation, and Kirchhoff-Love boundary value problem shows that DInf-Grid achieves a 5-20x speed-up over coordinate-based MLP methods while maintaining comparable accuracy and compactness.</div>
<div class="mono" style="margin-top:8px">该研究针对基于坐标的多层感知机（如正弦神经网络）在求解微分方程时计算量大、训练慢的问题，提出了一种新方法DInf-Grid。该方法结合了基于网格表示的训练速度和径向基函数插值，以支持高阶导数计算，这对于微分方程求解至关重要，并引入了共置网格的多分辨率分解来捕捉高频解并稳定梯度计算。在泊松方程、亥姆霍兹方程和基尔霍夫-洛夫边界值问题等任务上的实验验证表明，相比基于多层感知机的方法，速度提升了5-20倍，同时保持了相当的精度和紧凑性。</div>
</details>
</div>
<div class="card">
<div class="title">High-accuracy and dimension-free sampling with diffusions</div>
<div class="meta-line">Authors: Khashayar Gatmiry, Sitan Chen, Adil Salim</div>
<div class="meta-line">First: 2026-01-15T18:58:50+00:00 · Latest: 2026-01-15T18:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10708v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy&#x27;&#x27; guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的高精度无维度依赖采样方法</div>
<div class="mono" style="margin-top:8px">扩散模型在从复杂多模态分布中采样方面展现出卓越的实证效果。其推断依赖于数值求解特定微分方程，该方程无法获得闭式解，通常需要通过离散化方法进行大量小步长迭代才能生成高质量样本。现有研究表明，扩散模型离散化方法的迭代复杂度随环境维度和逆精度1/ε呈多项式增长。本文提出一种基于低阶近似与配置法精妙结合的新型求解器，证明其迭代复杂度在1/ε上呈多对数增长，首次为仅需（近似）访问数据分布分数函数的扩散采样器提供了高精度保证。此外，该复杂度界限不显式依赖于环境维度，维度仅通过目标分布支撑集的有效半径影响求解器复杂度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high computational cost of existing diffusion model samplers, which require many small discretization steps to achieve high-quality samples, this work introduces a new solver that combines low-degree approximation with the collocation method. The method leverages approximate access to the scores of the data distribution to achieve polylogarithmic iteration complexity in inverse accuracy, independent of explicit ambient dimension dependence, scaling instead with the effective radius of the target distribution&#x27;s support. Experimental findings demonstrate that this approach provides the first high-accuracy guarantee for diffusion-based samplers, significantly reducing the number of iterations needed compared to prior polynomial-scaling methods.</div>
<div class="mono" style="margin-top:8px">针对现有扩散模型采样器计算成本高、需要大量离散化步骤才能生成高质量样本的问题，本研究提出了一种结合低阶近似和配置方法的新求解器。该方法利用数据分布得分函数的近似访问，实现了在逆精度上的多对数迭代复杂度，相比之前的多项式缩放有显著改进，且其复杂度仅依赖于目标分布的有效支撑半径，而非显式地依赖于环境维度。</div>
</details>
</div>
<div class="card">
<div class="title">See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection</div>
<div class="meta-line">Authors: Amir Mallak, Erfan Aasi, Shiva Sreeram, Tsun-Hsuan Wang, Daniela Rus, Alaa Maalouf</div>
<div class="meta-line">First: 2026-01-15T18:58:33+00:00 · Latest: 2026-01-15T18:58:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少看多开：基于基础模型随机补丁选择的端到端自动驾驶泛化方法</div>
<div class="mono" style="margin-top:8px">端到端自动驾驶的最新进展表明，基于基础模型提取的补丁对齐特征训练的策略在分布外泛化方面表现更优。我们假设，由于自注意力机制，每个补丁特征都以不同方式和强度隐式嵌入了所有其他补丁的信息，导致这些描述符高度冗余。我们通过主成分分析和跨补丁相似性量化了此类（BLIP2）特征的冗余度：90%的方差可由17/64个主成分捕获，且强跨令牌相关性普遍存在。基于这种重叠信息训练会导致策略过拟合虚假相关性，损害分布外鲁棒性。我们提出随机补丁选择方法——一种简单有效的策略学习框架，能提升模型的鲁棒性、泛化能力和效率。SPS在每帧随机掩蔽部分补丁描述符（不输入策略模型），同时保持剩余补丁的空间布局。策略因此获得同一场景的不同随机但完整的视图：每个随机补丁子集都构成对世界的不同但合理的连贯投影。策略决策将基于对特定令牌存留具有不变性的特征。大量实验证实，在所有分布外场景中，我们的方法均优于现有最优技术：平均提升6.2%，闭环仿真最高提升20.4%，同时提速2.4倍。我们通过9个系统的消融实验验证了掩蔽率和补丁特征重组的影响，其中8个系统超越先前最优水平。最终证明，学习到的策略无需调整即可迁移至物理世界的真实车辆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the issue of feature redundancy in foundation model-based autonomous driving, which leads to overfitting and poor out-of-distribution (OOD) generalization, this paper introduces Stochastic Patch Selection (SPS). The method randomly masks a fraction of patch descriptors at each frame while preserving spatial layout, forcing the policy to learn from varied, incomplete views and focus on invariant features. Experiments demonstrate that SPS outperforms state-of-the-art methods across OOD scenarios, achieving an average 6.2% improvement and up to 20.4% in closed-loop simulations, while being 2.4 times faster, and the policy successfully transfers to a real-world car without tuning.</div>
<div class="mono" style="margin-top:8px">本研究针对基于基础模型补丁对齐特征的端到端自动驾驶策略中存在的过拟合问题，这些特征冗余度高，导致分布外泛化性能差。提出的随机补丁选择方法在训练时对每帧随机掩码一部分补丁描述符，迫使策略从变化但连贯的场景投影中学习不变特征。实验结果表明，该方法在所有分布外场景中均优于现有技术，平均性能提升6.2%，闭环仿真中最高提升20.4%，且速度加快2.4倍，所学策略无需调整即可成功迁移到真实物理车辆上。</div>
</details>
</div>
<div class="card">
<div class="title">Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication</div>
<div class="meta-line">Authors: Keval Jain, Anant Raj, Saurav Prakash, Girish Varma</div>
<div class="meta-line">First: 2026-01-15T18:56:54+00:00 · Latest: 2026-01-15T18:56:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10705v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>有界陈旧性、部分参与及噪声通信下的分布式感知机</div>
<div class="mono" style="margin-top:8px">我们研究一种通过迭代参数混合（IPM式平均）训练的半异步客户端-服务器感知机：客户端运行本地感知机更新，服务器通过聚合每轮通信中到达的更新形成全局模型。该设定捕捉了联邦与分布式部署中的三种系统效应：（i）因模型交付延迟和客户端计算应用延迟导致的陈旧更新（双向版本滞后），（ii）部分参与（客户端间歇可用性），（iii）下行与上行链路上的非理想通信，建模为具有有界二阶矩的有效零均值加性噪声。我们提出一种称为带填充的陈旧性分桶聚合的服务器端聚合规则，该规则能在不假设任何延迟或参与的随机模型的情况下，确定性地在更新年龄上强制执行预设的陈旧性分布。在间隔可分性和数据半径有界的条件下，我们证明了给定服务器轮数内感知机错误累积加权数的有限时域期望界：延迟的影响仅通过强制平均陈旧性体现，而通信噪声则贡献一个额外项，该项以时域平方根的量级随总噪声能量增长。在无噪声情形下，我们展示了在温和的新鲜参与条件下，有限的期望错误预算如何产生显式的有限轮次稳定界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of training a distributed perceptron in realistic federated learning settings characterized by two-sided version lag (staleness), intermittent client availability (partial participation), and noisy communication channels. The method employs an iterative parameter mixing framework with a novel server-side staleness-bucket aggregation rule with padding, which deterministically controls update ages without relying on stochastic delay models. Key experimental findings, derived from theoretical analysis under margin separability and bounded data assumptions, show that the cumulative weighted mistakes are bounded, with staleness impact confined to its mean and communication noise adding a term scaling with the square root of the training horizon; in noiseless scenarios, a finite mistake budget leads to explicit finite-round stabilization given sufficient fresh client participation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在具有双向版本滞后（陈旧性）、客户端间歇性可用（部分参与）以及噪声通信信道等现实联邦学习场景中训练分布式感知机的挑战。方法采用迭代参数混合框架，并引入一种新颖的服务端陈旧性分桶聚合与填充规则，该规则能够确定性控制更新延迟，无需依赖随机延迟模型。在数据满足间隔可分性和有界半径的假设下，主要实验结果包括一个有限轮次的错误数上界，其中陈旧性的影响仅取决于其均值，而通信噪声则贡献一个与轮次平方根相关的附加项；在无噪声情况下，若满足客户端新鲜参与条件，可推导出明确的有限轮次稳定界。</div>
</details>
</div>
<div class="card">
<div class="title">Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis</div>
<div class="meta-line">Authors: Chun Hei Michael Shiu, Chih Wei Ling</div>
<div class="meta-line">First: 2026-01-15T18:55:00+00:00 · Latest: 2026-01-15T18:55:00+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 figures. This work is submitted in part to the 2026 IEEE International Symposium on Information Theory (ISIT). arXiv admin note: substantial text overlap with arXiv:2501.12046</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10701v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM&#x27;s utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通信高效与隐私自适应机制——一种具有收敛性分析的联邦学习方案</div>
<div class="mono" style="margin-top:8px">联邦学习允许多方在不共享底层数据的情况下联合训练学习模型，为数据治理约束下的隐私保护协作提供了可行路径。持续研究联邦学习对解决其关键挑战至关重要，包括通信效率和参与方间的隐私保护。近期研究提出了一种名为通信高效与隐私自适应机制（CEPAM）的新方法，可同时实现这两个目标。CEPAM采用拒绝采样通用量化器（RSUQ），这是一种随机向量量化器，其量化误差等同于预设噪声，可通过调整该噪声为参与方定制隐私保护级别。本文从理论上分析了CEPAM的隐私保障与收敛特性，并通过实验评估验证其性能表现，包括与其他基线的收敛曲线对比，以及不同参与方间的精度-隐私权衡关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Federated learning faces challenges in communication efficiency and privacy protection when multiple parties collaborate without sharing raw data. To address these issues, this work proposes the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which employs a rejection-sampled universal quantizer (RSUQ) that converts quantization error into tunable noise for customizable privacy protection. Theoretical analysis confirms CEPAM&#x27;s privacy guarantees and convergence properties, while experiments demonstrate its competitive convergence profiles against baselines and its ability to balance accuracy and privacy trade-offs among participating parties.</div>
<div class="mono" style="margin-top:8px">联邦学习支持多方在不共享数据的情况下协同训练模型，但面临通信效率和隐私保护的双重挑战。本研究分析了通信高效且隐私可调机制（CEPAM），该机制采用拒绝采样通用量化器，通过量化误差等效于预设噪声的方式，同时降低通信开销并提供可调节的隐私保护。理论分析验证了CEPAM的隐私保障和收敛特性，实验结果表明其收敛性能优于基线方法，并揭示了不同参与方之间的准确率-隐私权衡关系。</div>
</details>
</div>
<div class="card">
<div class="title">Data-driven stochastic reduced-order modeling of parametrized dynamical systems</div>
<div class="meta-line">Authors: Andrew F. Ilersich, Kevin Course, Prasanth B. Nair</div>
<div class="meta-line">First: 2026-01-15T18:50:18+00:00 · Latest: 2026-01-15T18:50:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>参数化动力系统的数据驱动随机降阶建模</div>
<div class="mono" style="margin-top:8px">在变化条件下建模复杂动力系统计算成本高昂，高保真仿真往往难以实现。尽管降阶模型提供了一种有前景的解决方案，但现有方法常难以处理随机动力学且无法量化预测不确定性，限制了其在鲁棒决策场景中的应用。为应对这些挑战，我们提出一种数据驱动框架，用于学习跨参数空间和激励条件泛化的连续时间随机降阶模型。该方法基于摊销随机变分推断，利用马尔可夫高斯过程的重参数化技巧，在训练过程中无需计算昂贵的前向求解器。这使得我们能够以独立于数据集规模和系统刚度的计算成本，联合学习概率自编码器与支配潜在动力学的随机微分方程。此外，该方法可灵活融入已知的物理先验信息。通过对三个挑战性测试问题的数值研究，我们展示了该方法对未见参数组合与激励的优异泛化能力，以及与现有方法相比显著的效率提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modeling complex dynamical systems under varying conditions is computationally intensive, and existing reduced-order models often lack the ability to handle stochastic dynamics and quantify prediction uncertainty. To address this, the authors propose a data-driven framework that uses amortized stochastic variational inference and a reparametrization trick for Markov Gaussian processes to learn continuous-time stochastic reduced-order models without requiring expensive forward solvers during training. This method jointly learns a probabilistic autoencoder and latent stochastic differential equations, achieving computational efficiency independent of dataset size and system stiffness. In numerical tests on three challenging problems, the framework demonstrated excellent generalization to unseen parameters and forcings, along with significant efficiency gains over existing approaches.</div>
<div class="mono" style="margin-top:8px">在变化条件下对复杂动力系统进行建模计算成本高昂，而现有的降阶模型往往难以处理随机动力学并量化预测不确定性。为解决这一问题，作者提出了一种数据驱动框架，利用摊销随机变分推断和马尔可夫高斯过程的重参数化技巧，在训练过程中无需昂贵的正向求解器即可学习连续时间随机降阶模型。该方法联合学习概率自编码器和潜在随机微分方程，实现了与数据集规模和系统刚度无关的计算效率。在三个具有挑战性的测试问题的数值研究中，该框架对未见过的参数组合和激励表现出优异的泛化能力，并且相比现有方法取得了显著的效率提升。</div>
</details>
</div>
<div class="card">
<div class="title">On the origin of neural scaling laws: from random graphs to natural language</div>
<div class="meta-line">Authors: Maissam Barkeshli, Alberto Alfarano, Andrey Gromov</div>
<div class="meta-line">First: 2026-01-15T18:46:09+00:00 · Latest: 2026-01-15T18:46:09+00:00</div>
<div class="meta-line">Comments: 33 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10684v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论神经缩放定律的起源：从随机图到自然语言</div>
<div class="mono" style="margin-top:8px">缩放定律在现代人工智能革命中发挥了重要作用，为从业者提供了模型性能如何随数据量、计算量和模型参数数量增加而提升的预测能力。这激发了人们对神经缩放定律起源的浓厚兴趣，一种普遍观点认为它们源于数据中已存在的幂律结构。本文研究了在具有可调复杂度的图上训练用于预测随机游走（二元语法）的Transformer的缩放定律。我们证明，即使在数据相关性中不存在幂律结构的情况下，这种简化设置也能产生神经缩放定律。我们进一步通过训练从逐步简化的生成语言模型（从4层、2层、1层Transformer语言模型直至语言二元语法）采样的序列，系统地降低自然语言的复杂度，揭示了缩放指数的单调演化规律。我们的结果还包括在从Erdös-Renyi和无标度Barabási-Albert集合中抽取的随机图上进行随机游走训练获得的缩放定律。最后，我们重新审视了语言建模的传统缩放定律，证明使用上下文长度为50的2层Transformer即可复现多项关键结果；对先前文献中使用的多种拟合方法进行了批判性分析；展示了与当前文献实践相比获取计算最优曲线的替代方法；并提供了初步证据表明最大更新参数化可能比标准参数化具有更高的参数效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the origins of neural scaling laws, which are crucial for predicting model performance improvements with increased data, compute, and parameters, challenging the common assumption that these laws arise solely from power-law structures in data. The method involves training transformers on random walks (bigrams) on graphs with tunable complexity, including Erdös-Renyi and scale-free Barabási-Albert ensembles, and systematically simplifying natural language by training on sequences from progressively simpler generative language models down to bigrams. Key experimental findings reveal that neural scaling laws emerge even without power-law data correlations, with scaling exponents evolving monotonically as language complexity is reduced; the work also reproduces essential language modeling scaling laws using minimal 2-layer transformers, critiques prior fitting methods, proposes an alternative for compute-optimal curves, and provides preliminary evidence for the parameter efficiency of maximal update parameterization.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究神经缩放定律的起源，该定律对于预测模型性能随规模提升至关重要，现有观点常将其归因于数据中的幂律结构。方法包括在具有可调复杂度的图上训练变换器进行随机游走，以及在逐步简化的生成语言模型序列上进行训练，并与来自Erdös-Renyi和Barabási-Albert集合的随机图进行比较。关键实验结果表明，即使在没有幂律数据相关性的情况下，神经缩放定律也会出现；缩放指数随语言复杂性降低而单调演变；且最大更新参数化可能比标准参数化具有更高的参数效率。</div>
</details>
</div>
<div class="card">
<div class="title">Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models</div>
<div class="meta-line">Authors: Zirui Ren, Ziming Liu</div>
<div class="meta-line">First: 2026-01-15T18:42:50+00:00 · Latest: 2026-01-15T18:42:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10679v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10679v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) &quot;Grokking&quot; dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM &quot;guesses&quot; the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be &quot;guessing&quot; instead of &quot;reasoning&quot;. Leveraging this &quot;guessing&quot; picture, we propose three strategies to scale HRM&#x27;s guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models &quot;reason&quot;.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的推理模型是在推理还是在猜测？——对分层推理模型的机制分析</div>
<div class="mono" style="margin-top:8px">分层推理模型（HRM）在多种推理任务中表现卓越，显著优于基于大语言模型的推理器。为理解HRM的优势与潜在失效模式，我们对其推理模式展开机制研究，发现三个意外现象：（a）在极简谜题（如仅含一个未知单元格的谜题）上失效，这归因于HRM违背了其基本假设——不动点性质；（b）推理步骤中的“顿悟”动态，即答案并非均匀改进，而是在某个关键推理步骤突然变得正确；（c）存在多个不动点，HRM会“猜测”首个不动点（可能错误）并陷入其中。这些现象表明HRM更似“猜测”而非“推理”。基于此，我们提出三种扩展HRM猜测的策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测数量）和模型自举（利用训练随机性增加猜测数量）。实践中，通过整合所有方法，我们开发出增强型HRM，将数独极限任务的准确率从54.5%提升至96.9%。科学层面，本研究为理解推理模型的“推理”机制提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reasoning mechanisms of hierarchical reasoning models (HRMs), which excel at complex tasks but exhibit puzzling failures on simple puzzles, suggesting they may be guessing rather than reasoning. Through mechanistic analysis, the authors identify key failure modes: violation of the fixed-point property on trivial puzzles, &#x27;grokking&#x27; dynamics where correctness emerges suddenly at a specific step, and entrapment in incorrect fixed points. To address these issues, they propose three scaling strategies—data augmentation, input perturbation, and model bootstrapping—which collectively form Augmented HRM, boosting Sudoku-Extreme accuracy from 54.5% to 96.9% and offering new insights into model reasoning processes.</div>
<div class="mono" style="margin-top:8px">本研究探讨了层次推理模型（HRM）的推理机制，这些模型在复杂任务上表现出色，但在简单谜题上却出现令人困惑的失败。通过机制分析，作者发现了三个关键问题：HRM可能因违反不动点性质而在琐碎谜题上失败，表现出‘顿悟’动态即正确性在关键步骤突然出现，并且常常收敛到错误的不动点，这表明它们更依赖于‘猜测’而非系统推理。基于这一见解，他们提出了三种扩展策略——数据增强、输入扰动和模型自举——以提高猜测的质量和多样性，最终形成了增强型HRM，将Sudoku-Extreme的准确率从54.5%提升至96.9%。</div>
</details>
</div>
<div class="card">
<div class="title">Single-Stage Huffman Encoder for ML Compression</div>
<div class="meta-line">Authors: Aditya Agrawal, Albert Magyar, Hiteshwar Eswaraiah, Patrick Sheridan, Pradeep Janedula, Ravi Krishnan Venkatesan, Krishna Nair, Ravi Iyer</div>
<div class="meta-line">First: 2026-01-15T18:37:56+00:00 · Latest: 2026-01-15T18:37:56+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10673v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10673v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向机器学习压缩的单阶段霍夫曼编码器</div>
<div class="mono" style="margin-top:8px">大语言模型的训练与部署需将数据分布至多个加速器，此时集体操作常受网络带宽制约。采用霍夫曼编码的无损压缩能有效缓解此问题，但其传统三阶段设计（需实时频率分析、码本生成及码本与数据同步传输）会带来计算、延迟与数据开销，对芯片间通信等延迟敏感场景构成障碍。本文提出一种单阶段霍夫曼编码器，通过采用基于历史数据批次平均概率分布的固定码本，消除了上述开销。基于Gemma 2B模型的分析表明，张量在不同层级与分片间具有高度统计相似性。该方法在压缩率上达到分片级霍夫曼编码的0.5%误差范围内，且接近理想香农压缩极限的1%误差范围，实现了高效的实时压缩。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To reduce network bandwidth bottlenecks in distributed LLM training and serving, this work addresses the latency and overhead issues of traditional three-stage Huffman coding by proposing a single-stage encoder that uses fixed codebooks derived from average probability distributions of prior data batches. The method leverages the observed high statistical similarity of tensors across layers and shards in models like Gemma 2B. Experimental results show the approach achieves compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon limit, enabling efficient low-latency compression suitable for scenarios like die-to-die communication.</div>
<div class="mono" style="margin-top:8px">为缓解分布式大语言模型训练与推理中的网络带宽瓶颈，本研究针对传统三阶段哈夫曼编码的延迟和开销问题，提出了一种单阶段编码器，该编码器使用基于先前数据批次平均概率分布的固定码本。该方法利用了在Gemma 2B等模型中观察到的张量在层和分片间的高度统计相似性。实验结果表明，该方法的压缩率与按分片哈夫曼编码的差距在0.5%以内，与理想香农压缩极限的差距在1%以内，从而实现了适用于芯片间通信的高效低延迟压缩。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Homophily in Large Language Models</div>
<div class="meta-line">Authors: Utkarsh Sahu, Zhisheng Qi, Mahantesh Halappanavar, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt, Yu Zhang, Yao Ma, Yu Wang</div>
<div class="meta-line">First: 2025-09-28T09:40:27+00:00 · Latest: 2026-01-15T18:26:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23773v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23773v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中的知识同质性</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）作为支持问答和事实核查等知识密集型应用的神经知识库，正日益受到研究关注。然而，其知识的结构化组织方式尚未得到充分探索。受认知神经科学发现（如语义聚类和启动效应，即知晓一个事实会增加回忆起相关事实的可能性）的启发，本研究探讨了LLMs中类似的知识同质性模式。为此，我们通过在三元组和实体层面进行知识检查，将LLM知识映射为图表示。随后，分析实体与其邻居之间的知识掌握程度关系，发现LLMs倾向于对图中位置相近的实体具有相似的知识水平。基于这一同质性原理，我们提出一种图神经网络（GNN）回归模型，通过利用邻域评分来估计三元组的实体级知识掌握度得分。预测的知识掌握度使我们能够优先检查较不为人知的三元组，从而在相同标注预算下最大化知识覆盖范围。这不仅提高了为LLMs注入知识的主动标注微调效率，还增强了推理密集型问答中的多跳路径检索能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the structural organization of knowledge in Large Language Models (LLMs), motivated by the need to understand them as neural knowledge bases for applications like question answering. The method involves mapping LLM knowledge into a graph and analyzing entity relationships, revealing a knowledge homophily pattern where entities closer in the graph share similar knowledgeability levels. Based on this, a Graph Neural Network regression model is proposed to predict entity-level knowledgeability scores, which experimental results show can prioritize checking less-known triplets to improve knowledge coverage under a fixed labeling budget, thereby enhancing active labeling efficiency for fine-tuning and boosting multi-hop path retrieval in reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究大型语言模型（LLMs）内部的知识结构组织，以更好地将其作为神经知识库应用于问答等任务。方法上，通过将LLM的知识映射为图结构进行分析，发现了知识同质性模式，即图中位置相近的实体具有相似的知识掌握程度；基于此原则，研究训练了一个图神经网络（GNN）回归模型来预测实体层面的知识掌握分数。主要实验结果表明，利用这些预测分数优先检查知识掌握较薄弱的事实，能够在固定标注预算下，提高知识注入的主动标注效率，并增强问答任务中的多跳推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution</div>
<div class="meta-line">Authors: Minghao Yan, Bo Peng, Benjamin Coleman, Ziqi Chen, Zhouhang Xie, Zhankui He, Noveen Sachdeva, Isabella Ye, Weili Wang, Chi Wang, Ed H. Chi, Wang-Cheng Kang, Derek Zhiyuan Cheng, Beidou Wang</div>
<div class="meta-line">First: 2026-01-15T18:25:23+00:00 · Latest: 2026-01-15T18:25:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10657v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10657v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent&#x27;s context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PACEvolve：实现长视野进度感知一致性演化</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已成为进化搜索的强大操作者，但高效搜索框架的设计仍缺乏系统性。当前基于LLM的循环系统虽具潜力，却缺少管理演化过程的系统化方法。我们识别出三种典型失效模式：上下文污染（实验历史偏差影响后续候选生成）、模式坍塌（因探索-利用失衡导致智能体陷入局部最优）以及弱协作（僵化的交叉策略无法有效利用并行搜索轨迹）。为此，我们提出进度感知一致性演化框架PACEvolve，通过分层上下文管理结合剪枝机制应对上下文污染；基于动量的回溯策略逃离局部最优；以及自适应采样策略统一回溯与交叉操作，实现动态搜索协调，使智能体能平衡内部优化与跨轨迹协作。实验表明，PACEvolve为持续长视野自我改进提供了系统化路径，在LLM-SR和KernelBench基准上取得最优结果，并在Modded NanoGPT任务中发现超越现有记录的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses systematic failures in LLM-driven evolutionary search, specifically context pollution, mode collapse, and weak collaboration, which hinder long-horizon progress. The proposed PACEvolve framework employs hierarchical context management with pruning, momentum-based backtracking, and a self-adaptive sampling policy for dynamic search coordination to govern context and search dynamics robustly. Experimental results show that PACEvolve achieves state-of-the-art performance on LLM-SR and KernelBench and discovers solutions surpassing the record on Modded NanoGPT, demonstrating consistent long-horizon self-improvement.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型驱动的进化搜索中存在的系统性失效问题，即上下文污染、模式崩溃和弱协作，这些问题阻碍了长期进展。提出的PACEvolve框架采用分层上下文管理与剪枝、基于动量的回溯以及自适应采样策略来实现动态搜索协调，从而稳健地管理智能体上下文和搜索动态。实验结果表明，PACEvolve在LLM-SR和KernelBench上取得了最先进的性能，并在Modded NanoGPT上发现了超越记录的解决方案，展现了持续的长期自我改进能力。</div>
</details>
</div>
<div class="card">
<div class="title">PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus</div>
<div class="meta-line">Authors: Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss</div>
<div class="meta-line">First: 2025-05-23T18:01:09+00:00 · Latest: 2026-01-15T18:18:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20323v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20323v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical narratives encode temporal dynamics essential for modeling patient trajectories, yet large-scale temporally annotated resources are scarce. We introduce PMOA-TTS, a corpus of 124,699 single-patient PubMed Open Access case reports converted into structured textual timelines of (event, time) pairs using a scalable large-language-model pipeline (Llama 3.3 70B and DeepSeek-R1). The corpus comprises over 5.6 million timestamped events, alongside extracted demographics and diagnoses. Technical validation uses a clinician-curated gold set and three measures: semantic event matching, temporal concordance (c-index), and alignment error summarized with Area Under the Log-Time CDF (AULTC). We benchmark alternative prompting and model choices and provide documentation to support reproduction. PMOA-TTS enables research on timeline extraction, temporal reasoning, survival modeling and event forecasting from narrative text, and offers broad diagnostic and demographic coverage. Data and code are openly available in public repositories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMOA-TTS：PubMed开放获取文本时间序列语料库介绍</div>
<div class="mono" style="margin-top:8px">临床叙述编码了对建模患者病程至关重要的时间动态，但大规模时间标注资源稀缺。我们推出PMOA-TTS，这是一个包含124,699份单患者PubMed开放获取病例报告的语料库，通过可扩展的大语言模型流程（Llama 3.3 70B与DeepSeek-R1）将其转化为结构化的（事件，时间）对文本时间线。该语料库涵盖超过560万个带时间戳的事件，并提取了人口统计学与诊断信息。技术验证采用临床专家标注的金标准集及三项指标：语义事件匹配、时间一致性（c指数）以及通过对数时间累积分布函数下面积（AULTC）汇总的对齐误差。我们对比了不同提示策略与模型选择，并提供支持复现的文档。PMOA-TTS支持从叙述文本中进行时间线提取、时序推理、生存建模与事件预测的研究，并提供广泛的诊断与人口统计学覆盖。数据与代码已在公共仓库开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity of large-scale temporally annotated clinical narrative resources essential for modeling patient trajectories, this work introduces PMOA-TTS, a corpus of 124,699 single-patient case reports converted into structured textual timelines. The method employs a scalable large-language-model pipeline using Llama 3.3 70B and DeepSeek-R1 to extract over 5.6 million timestamped (event, time) pairs, alongside demographics and diagnoses. Experimental validation on a clinician-curated gold set demonstrates the corpus&#x27;s quality through semantic event matching, temporal concordance (c-index), and low alignment error measured by Area Under the Log-Time CDF, benchmarking alternative prompting and model choices.</div>
<div class="mono" style="margin-top:8px">为解决临床叙事中大规模时序标注数据稀缺、阻碍患者轨迹建模的问题，本研究提出了PMOA-TTS语料库，将124,699份单患者病例报告转化为结构化文本时间线。该方法采用基于Llama 3.3 70B和DeepSeek-R1的可扩展大语言模型流水线，提取了超过560万个带时间戳的（事件，时间）对以及人口统计学和诊断信息。在临床专家标注的金标准集上的技术验证显示，该方法在语义事件匹配、时序一致性（c-index）和时间对齐误差（通过对数时间CDF下面积衡量）方面表现优异，为从叙事文本中提取时间线建立了基准。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Nonparametric Contextual Bandits</div>
<div class="meta-line">Authors: Hamish Flynn, Julia Olkhovskaya, Paul Rognon-Vael</div>
<div class="meta-line">First: 2025-03-20T17:44:56+00:00 · Latest: 2026-01-15T18:10:26+00:00</div>
<div class="meta-line">Comments: 44 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16382v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.16382v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the benefits of sparsity in nonparametric contextual bandit problems, in which the set of candidate features is countably or uncountably infinite. Our contribution is two-fold. First, using a novel reduction to sequences of multi-armed bandit problems, we provide lower bounds on the minimax regret, which show that polynomial dependence on the number of actions is generally unavoidable in this setting. Second, we show that a variant of the Feel-Good Thompson Sampling algorithm enjoys regret bounds that match our lower bounds up to logarithmic factors of the horizon, and have logarithmic dependence on the effective number of candidate features. When we apply our results to kernelised and neural contextual bandits, we find that sparsity enables better regret bounds whenever the horizon is large enough relative to the sparsity and the number of actions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏非参数上下文赌博机</div>
<div class="mono" style="margin-top:8px">我们研究了在非参数上下文赌博机问题中稀疏性的优势，其中候选特征集为可数或不可数无限。我们的贡献包括两方面：首先，通过一种新颖的转化为多臂赌博机序列的方法，我们给出了极小极大遗憾的下界，表明在此设置下对动作数量的多项式依赖通常是不可避免的；其次，我们证明了一种改进的Feel-Good Thompson Sampling算法能够实现与下界匹配的遗憾界（至多相差时间跨度的对数因子），且对有效候选特征数量具有对数依赖。将结果应用于核化与神经上下文赌博机时，我们发现当时间跨度相对于稀疏性和动作数量足够大时，稀疏性能带来更优的遗憾界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the role of sparsity in nonparametric contextual bandits with an infinite or large feature space, motivated by the need to understand fundamental performance limits and design efficient algorithms. The method involves a novel reduction to sequences of multi-armed bandit problems to derive minimax lower bounds, and proposes a variant of the Feel-Good Thompson Sampling algorithm. Key experimental findings show that polynomial dependence on the number of actions is generally unavoidable, but the proposed algorithm achieves regret bounds matching these lower bounds up to logarithmic factors, with logarithmic dependence on the effective number of features; in kernel and neural bandit settings, sparsity yields improved bounds when the horizon is sufficiently large relative to sparsity and action count.</div>
<div class="mono" style="margin-top:8px">本文研究了特征集可能无限的非参数上下文赌博机中的稀疏性作用，旨在理解其基本性能极限并设计高效算法。方法包括通过新颖的归约到多臂赌博机序列来推导极小极大遗憾下界，并分析一种改进的Feel-Good Thompson Sampling算法。主要实验结果表明，对动作数量的多项式依赖通常是不可避免的，但所提算法实现了接近最优的遗憾上界，且对有效特征数量仅有对数依赖，从而在时间范围相对于稀疏性和动作数量足够大时，为核化及神经网络上下文赌博机提供了更好的理论保证。</div>
</details>
</div>
<div class="card">
<div class="title">RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors</div>
<div class="meta-line">Authors: Miaomiao Cai, Zhijie Zhang, Junfeng Fang, Zhiyong Cheng, Xiang Wang, Meng Wang</div>
<div class="meta-line">First: 2026-01-13T16:34:17+00:00 · Latest: 2026-01-15T18:06:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08705v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08705v2">PDF</a> · <a href="https://github.com/miaomiao-cai2/RMBRec/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.
  In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users&#x27; auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RMBRec：面向目标行为的鲁棒多行为推荐</div>
<div class="mono" style="margin-top:8px">多行为推荐在实践中面临关键挑战：辅助行为（如点击、加购）常存在噪声、弱相关性或与目标行为（如购买）语义不对齐，导致偏好学习偏差与次优性能。现有方法虽尝试融合异构信号，但本质上缺乏确保行为不一致鲁棒性的理论机制。本研究提出面向目标行为的鲁棒多行为推荐框架RMBRec，其基于信息论鲁棒性原理，将鲁棒性阐释为最大化预测信息同时最小化其在异构行为环境中方差的联合过程。在此视角下，表征鲁棒性模块通过最大化用户辅助行为与目标行为表征间的互信息增强局部语义一致性；优化鲁棒性模块通过最小化跨行为预测风险方差（即不变风险最小化的高效近似）确保全局稳定性。这种局部-全局协作以理论自洽的方式桥接了表征纯化与优化不变性。在三个真实数据集上的大量实验表明，RMBRec不仅在准确性上优于现有最优方法，且在多种噪声扰动下保持显著稳定性。代码已开源：https://github.com/miaomiao-cai2/RMBRec/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-behavior recommendation systems often suffer from noise and semantic misalignment between auxiliary behaviors (e.g., clicks) and the target behavior (e.g., purchases), leading to biased learning. To address this, the authors propose RMBRec, a framework grounded in information theory that jointly maximizes predictive information and minimizes its variance across behavioral environments. It employs a Representation Robustness Module to enhance local semantic consistency via mutual information maximization between auxiliary and target representations, and an Optimization Robustness Module to enforce global stability by minimizing predictive risk variance across behaviors. Experiments on three real-world datasets show that RMBRec outperforms state-of-the-art methods in accuracy and maintains strong stability under various noise perturbations.</div>
<div class="mono" style="margin-top:8px">多行为推荐系统常因辅助行为（如点击）与目标行为（如购买）之间的噪声和语义错位而导致学习偏差。为解决该问题，作者提出了RMBRec，这是一个基于信息论的框架，旨在最大化预测信息的同时最小化其在各行为环境中的方差。该框架通过表示鲁棒性模块，利用辅助行为与目标行为表示间的互信息最大化来增强局部语义一致性；并通过优化鲁棒性模块，通过最小化跨行为的预测风险方差来确保全局稳定性。在三个真实数据集上的实验表明，RMBRec在准确性上优于现有先进方法，并在多种噪声干扰下保持了出色的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</div>
<div class="meta-line">Authors: Matteo Fasulo, Giusy Spacone, Thorir Mar Ingolfsson, Yawei Li, Luca Benini, Andrea Cossettini</div>
<div class="meta-line">First: 2025-12-05T17:36:57+00:00 · Latest: 2026-01-15T18:05:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15729v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15729v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Objective: Surface electromyography (EMG) is a non-invasive sensing modality widely used in biomechanics, rehabilitation, prosthetic control, and human-machine interfaces. Despite decades of use, achieving robust generalization across subjects, recording systems, and acquisition protocols remains challenging. While foundation models (FMs) are gaining traction for EMG, existing approaches remain limited to single downstream tasks and lack deployability on embedded platforms. This work addresses these limitations. Methods: We present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner using masked reconstruction on publicly available datasets. With only 3.6M parameters, TinyMyo is designed to support multiple downstream tasks through minimal task-specific head adaptations. Results: We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and speech recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 (89.4%), UCI-EMG (97.56%), and EPN-612 (96.74%) datasets. We demonstrate the first-time deployment of an EMG FM on an ultra-low power microcontroller (GAP9), with an inference time of 0.785 s, energy of 44.91 mJ and power envelope of 57.18 mW. Conclusion: TinyMyo demonstrates that compact, self-supervised EMG FM can guarantee strong generalization across multiple downstream tasks while remaining compatible with low-power edge devices. Significance: TinyMyo is the first EMG FM for ultra-low power edge devices, enabling scalable and energy-efficient sensing for motor intent decoding, neuromuscular assessment, and biosignal driven human-machine interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TinyMyo：面向边缘灵活肌电信号处理的微型基础模型</div>
<div class="mono" style="margin-top:8px">目标：表面肌电图（EMG）是一种广泛应用于生物力学、康复、假肢控制和人机交互的非侵入式传感技术。尽管已应用数十年，但在不同受试者、记录系统和采集协议间实现稳健泛化仍具挑战。虽然基础模型（FM）在EMG领域逐渐受到关注，现有方法仍局限于单一下游任务且难以在嵌入式平台部署。本研究旨在解决这些局限。方法：我们提出TinyMyo——一种基于Transformer编码器架构的轻量级FM。该模型通过掩码重建在公开数据集上进行自监督预训练。仅含360万参数的TinyMyo可通过最小化的任务适配头支持多下游任务。结果：我们在手势分类、手部运动回归、语音生成与识别任务中展示了泛化能力，其性能达到或超越当前最优水平（SoA），且模型参数量低于500万。在NinaPro DB5（89.4%）、UCI-EMG（97.56%）和EPN-612（96.74%）数据集上取得了优于既往FM方法的SoA结果。我们首次在超低功耗微控制器（GAP9）上部署了EMG基础模型，推理时间0.785秒，能耗44.91毫焦，功率57.18毫瓦。结论：TinyMyo证明紧凑型自监督EMG基础模型能在保持低功耗边缘设备兼容性的同时，实现多下游任务的强泛化能力。意义：TinyMyo是首个面向超低功耗边缘设备的EMG基础模型，为运动意图解码、神经肌肉评估及生物信号驱动的人机交互提供了可扩展、高能效的传感解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenges of achieving robust generalization across subjects, recording systems, and acquisition protocols in surface electromyography (EMG) processing, and to overcome the limitations of existing foundation models that are restricted to single tasks and lack deployability on embedded platforms, this work introduces TinyMyo, a lightweight foundation model based on a Transformer encoder architecture. The method involves pre-training the model in a self-supervised manner using masked reconstruction on public datasets, resulting in a compact model with only 3.6M parameters designed to support multiple downstream tasks through minimal task-specific head adaptations. Key experimental results show that TinyMyo generalizes effectively across hand gesture classification, hand kinematic regression, speech production, and speech recognition, achieving state-of-the-art performance on datasets such as NinaPro DB5 (89.4%), UCI-EMG (97.56%), and EPN-612 (96.74%), and it is successfully deployed on an ultra-low power microcontroller (GAP9) with an inference time of 0.785 seconds, energy consumption of 44.91 mJ, and a power envelope of 57.18 mW.</div>
<div class="mono" style="margin-top:8px">针对表面肌电信号处理中跨受试者、记录系统和采集协议泛化能力不足的挑战，以及现有基础模型局限于单一任务且难以在嵌入式平台部署的问题，本研究提出了TinyMyo，一种基于Transformer编码器的轻量级基础模型，仅含360万参数，并通过掩码重建进行自监督预训练。该方法通过微小的任务特定头部适配支持多种下游任务。实验结果表明，TinyMyo在手势分类、手部运动回归及语音相关任务上取得了与先进水平相当或更优的性能，并在超低功耗微控制器上成功部署，实现了高效的推理时间和能耗。</div>
</details>
</div>
<div class="card">
<div class="title">Adjusted Similarity Measures and a Violation of Expectations</div>
<div class="meta-line">Authors: William L. Lippitt, Edward J. Bedrick, Nichole E. Carlson</div>
<div class="meta-line">First: 2026-01-15T18:01:26+00:00 · Latest: 2026-01-15T18:01:26+00:00</div>
<div class="meta-line">Comments: 12 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10641v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adjusted similarity measures, such as Cohen&#x27;s kappa for inter-rater reliability and the adjusted Rand index used to compare clustering algorithms, are a vital tool for comparing discrete labellings. These measures are intended to have the property of 0 expectation under a null distribution and maximum value 1 under maximal similarity to aid in interpretation. Measures are frequently adjusted with respect to the permutation distribution for historic and analytic reasons. There is currently renewed interest in considering other null models more appropriate for context, such as clustering ensembles permitting a random number of identified clusters. The purpose of this work is two -- fold: (1) to generalize the study of the adjustment operator to general null models and to a more general procedure which includes statistical standardization as a special case and (2) to identify sufficient conditions for the adjustment operator to produce the intended properties, where sufficient conditions are related to whether and how observed data are incorporated into null distributions. We demonstrate how violations of the sufficient conditions may lead to substantial breakdown, such as by producing a non-positive measure under traditional adjustment rather than one with mean 0, or by producing a measure which is deterministically 0 under statistical standardization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>调整相似性度量与期望性质违背</div>
<div class="mono" style="margin-top:8px">调整相似性度量（如用于评估者间一致性的Cohen&#x27;s kappa系数和用于比较聚类算法的调整兰德指数）是比对离散标注结果的重要工具。这类度量设计上应具备零期望性质（在零分布下期望值为0）和最大值1（在完全相似时），以提升结果可解释性。基于历史与分析考量，此类度量常针对置换分布进行调整。当前学界重新关注更贴合实际场景的其他零模型，例如允许随机聚类数量的集成聚类模型。本研究目的有二：（1）将调整算子的研究推广至通用零模型及更广义的流程（其中统计标准化作为特例）；（2）明确调整算子产生预期性质的充分条件，这些条件与观测数据是否及如何纳入零分布密切相关。我们论证了违反充分条件可能导致严重失效：传统调整可能产生非正定度量（而非零均值度量），统计标准化则可能产生恒为零的确定性度量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to ensure that adjusted similarity measures, like Cohen&#x27;s kappa and the adjusted Rand index, reliably achieve their intended properties of zero expectation under a null model and a maximum value of one. The authors generalize the adjustment procedure to accommodate various null models, including those used in clustering ensembles, and identify sufficient conditions for the adjustment to work correctly, focusing on how observed data is incorporated into the null distribution. Their key experimental findings reveal that violating these conditions can cause significant failures, such as producing non-positive measures instead of zero-mean ones or deterministically zero values under statistical standardization.</div>
<div class="mono" style="margin-top:8px">本研究探讨了调整后相似性度量（如Cohen&#x27;s kappa和调整兰德指数）的基本性质，这些度量旨在在零模型下期望值为零，在完全一致时最大值为一。作者将调整过程推广到更广泛的零模型类别，包括聚类集成中使用的模型，并确立了这些度量达到预期统计特性的充分条件。关键的实验结果表明，违反这些条件可能导致严重问题，例如在传统调整下产生非正值度量，或在统计标准化下产生确定为零的值。</div>
</details>
</div>
<div class="card">
<div class="title">STEM: Scaling Transformers with Embedding Modules</div>
<div class="meta-line">Authors: Ranajoy Sadhukhan, Sheng Cao, Harry Dong, Changsheng Zhao, Attiano Purpura-Pontoniere, Yuandong Tian, Zechun Liu, Beidi Chen</div>
<div class="meta-line">First: 2026-01-15T18:00:27+00:00 · Latest: 2026-01-15T18:00:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10639v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from both per-token FLOPs and cross-device communication. Empirically, STEM trains stably despite extreme sparsity. It improves downstream performance over dense baselines while reducing per-token FLOPs and parameter accesses (eliminating roughly one-third of FFN parameters). STEM learns embedding spaces with large angular spread which enhances its knowledge storage capacity. More interestingly, this enhanced knowledge capacity comes with better interpretability. The token-indexed nature of STEM embeddings allows simple ways to perform knowledge editing and knowledge injection in an interpretable manner without any intervention in the input text or additional computation. In addition, STEM strengthens long-context performance: as sequence length grows, more distinct parameters are activated, yielding practical test-time capacity scaling. Across 350M and 1B model scales, STEM delivers up to ~3--4% accuracy improvements overall, with notable gains on knowledge and reasoning-heavy benchmarks (ARC-Challenge, OpenBookQA, GSM8K, MMLU). Overall, STEM is an effective way of scaling parametric memory while providing better interpretability, better training stability and improved efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STEM：基于嵌入模块的Transformer扩展方法</div>
<div class="mono" style="margin-top:8px">细粒度稀疏性可在不增加按令牌计算量的前提下提升参数容量，但常面临训练不稳定、负载均衡与通信开销等问题。本文提出STEM（基于嵌入模块的Transformer扩展方法），采用静态的令牌索引策略，将前馈网络的上投影层替换为层局部嵌入查找，同时保持门控机制和下投影层的稠密性。该方法消除了运行时路由，支持通过异步预取实现CPU卸载，并将容量与单令牌浮点运算量及跨设备通信解耦。实验表明，STEM在极端稀疏条件下仍能稳定训练，在降低单令牌浮点运算量和参数访问量（约减少三分之一前馈网络参数）的同时，其下游性能优于稠密基线模型。STEM通过构建具有大角度分布特性的嵌入空间增强知识存储容量，更有趣的是，这种增强的知识容量伴随着更好的可解释性。STEM基于令牌索引的嵌入特性，无需干预输入文本或增加计算开销，即可通过简单方式实现可解释的知识编辑与知识注入。此外，STEM能增强长上下文性能：随着序列长度增加，更多差异化参数被激活，实现实际测试时的容量扩展。在3.5亿和10亿参数规模的模型中，STEM整体准确率提升约3-4%，在知识密集与推理密集型基准测试（ARC-Challenge、OpenBookQA、GSM8K、MMLU）上表现尤为突出。总体而言，STEM是一种扩展参数记忆的有效方法，兼具更优的可解释性、训练稳定性与运行效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of fine-grained sparsity in transformers, such as training instability and communication overhead, by proposing STEM, a static token-indexed method that replaces the FFN up-projection with a layer-local embedding lookup while keeping gate and down-projection dense. This approach eliminates runtime routing, enables CPU offload with asynchronous prefetch, and decouples capacity from per-token FLOPs and cross-device communication. Experimental results show that STEM trains stably despite extreme sparsity, improves downstream performance over dense baselines by up to ~3–4% accuracy, reduces per-token FLOPs and parameter accesses, and enhances knowledge storage capacity, interpretability, and long-context performance, with notable gains on benchmarks like ARC-Challenge and GSM8K.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型中细粒度稀疏性带来的训练不稳定和通信开销等挑战，提出了STEM方法，这是一种静态的令牌索引方法，用层局部嵌入查找替换FFN的上投影，同时保持门和下投影的稠密性。该方法消除了运行时路由，支持通过异步预取进行CPU卸载，并将容量与每令牌FLOPs和跨设备通信解耦。实验结果表明，STEM在极端稀疏性下仍能稳定训练，相比稠密基线提高了下游性能，同时减少了每令牌FLOPs和参数访问，通过嵌入的大角度扩展增强了知识存储容量，并加强了长上下文性能，在ARC-Challenge和MMLU等基准测试上实现了高达约3–4%的准确率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Riesz Representer Fitting under Bregman Divergence: A Unified Framework for Debiased Machine Learning</div>
<div class="meta-line">Authors: Masahiro Kato</div>
<div class="meta-line">First: 2026-01-12T17:36:33+00:00 · Latest: 2026-01-15T17:55:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07752v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07752v2">PDF</a> · <a href="https://github.com/MasaKat0/grr">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating the Riesz representer is central to debiased machine learning for causal and structural parameter estimation. We propose generalized Riesz regression, a unified framework that estimates the Riesz representer by fitting a representer model via Bregman divergence minimization. This framework includes the squared loss and the Kullback--Leibler (KL) divergence as special cases: the former recovers Riesz regression, while the latter recovers tailored loss minimization. Under suitable model specifications, the dual problems correspond to covariate balancing, which we call automatic covariate balancing. Moreover, under the same specifications, outcome averages weighted by the estimated Riesz representer satisfy Neyman orthogonality even without estimating the regression function, a property we call automatic Neyman orthogonalization. This property not only reduces the estimation error of Neyman orthogonal scores but also clarifies a key distinction between debiased machine learning and targeted maximum likelihood estimation. Our framework can also be viewed as a generalization of density ratio fitting under Bregman divergences to Riesz representer estimation, and it applies beyond density ratio estimation. We provide convergence analyses for both reproducing kernel Hilbert space (RKHS) and neural network model classes. A Python package for generalized Riesz regression is available at https://github.com/MasaKat0/grr.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于布雷格曼散度的Riesz表示元拟合：去偏机器学习的统一框架</div>
<div class="mono" style="margin-top:8px">估计Riesz表示元是因果与结构参数估计中去偏机器学习的核心问题。本文提出广义Riesz回归，这是一个通过最小化布雷格曼散度拟合表示元模型的统一框架，用于估计Riesz表示元。该框架将平方损失和Kullback-Leibler（KL）散度作为特例包含其中：前者可还原为标准Riesz回归，后者则对应定制化损失最小化。在适当的模型设定下，其对偶问题等价于协变量平衡，我们称之为自动协变量平衡。此外，在同一设定下，由估计的Riesz表示元加权的结果均值即使在不估计回归函数时也满足Neyman正交性，这一性质我们称为自动Neyman正交化。该性质不仅降低了Neyman正交得分的估计误差，也澄清了去偏机器学习与目标最大似然估计的关键区别。本框架亦可视为将布雷格曼散度下的密度比拟合推广至Riesz表示元估计，其应用范围超越密度比估计。我们为再生核希尔伯特空间（RKHS）和神经网络模型类提供了收敛性分析。广义Riesz回归的Python软件包发布于https://github.com/MasaKat0/grr。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to estimate the Riesz representer, a crucial component for debiased machine learning in causal and structural parameter estimation. The authors propose generalized Riesz regression, a unified framework that fits a representer model by minimizing Bregman divergence, encompassing squared loss and KL divergence as special cases to recover existing methods like Riesz regression and tailored loss minimization. Under appropriate model specifications, the dual problems yield automatic covariate balancing, and the estimated Riesz representer enables automatic Neyman orthogonalization, reducing estimation error and clarifying distinctions from targeted maximum likelihood estimation. Experimental analyses demonstrate convergence for RKHS and neural network models, supported by a publicly available Python package.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进用于因果和结构参数估计的去偏机器学习，为此提供了一个统一的Riesz表示子估计方法，该表示子是关键组件。所提出的广义Riesz回归方法通过最小化Bregman散度来拟合表示子模型，这推广了现有方法如Riesz回归（使用平方损失）和定制损失最小化（使用KL散度）。主要实验结果表明，在合适的模型设定下，其对偶问题对应于自动协变量平衡，且使用估计的表示子加权的结果平均值满足自动Neyman正交性，从而降低了估计误差并澄清了与目标最大似然估计等其他方法的区别；研究还提供了在再生核希尔伯特空间和神经网络模型类上的收敛性分析。</div>
</details>
</div>
<div class="card">
<div class="title">Relative Information Gain and Gaussian Process Regression</div>
<div class="meta-line">Authors: Hamish Flynn</div>
<div class="meta-line">First: 2025-10-05T16:35:51+00:00 · Latest: 2026-01-15T17:55:28+00:00</div>
<div class="meta-line">Comments: 30 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04277v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04277v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The sample complexity of estimating or maximising an unknown function in a reproducing kernel Hilbert space is known to be linked to both the effective dimension and the information gain associated with the kernel. While the information gain has an attractive information-theoretic interpretation, the effective dimension typically results in better rates. We introduce a new quantity called the relative information gain, which measures the sensitivity of the information gain with respect to the observation noise. We show that the relative information gain smoothly interpolates between the effective dimension and the information gain, and that the relative information gain has the same growth rate as the effective dimension. In the second half of the paper, we prove a new PAC-Bayesian excess risk bound for Gaussian process regression. The relative information gain arises naturally from the complexity term in this PAC-Bayesian bound. We prove bounds on the relative information gain that depend on the spectral properties of the kernel. When these upper bounds are combined with our excess risk bound, we obtain minimax-optimal rates of convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>相对信息增益与高斯过程回归</div>
<div class="mono" style="margin-top:8px">在再生核希尔伯特空间中估计或最大化未知函数的样本复杂度，已知与核的有效维度和信息增益相关。信息增益具有吸引人的信息论解释，而有效维度通常能带来更好的收敛速率。本文引入了一个称为相对信息增益的新量，用于衡量信息增益对观测噪声的敏感性。我们证明相对信息增益在有效维度和信息增益之间平滑插值，且其增长率与有效维度相同。在论文后半部分，我们为高斯过程回归证明了一个新的PAC贝叶斯超额风险界，相对信息增益自然产生于该界的复杂度项。我们证明了依赖于核谱性质的相对信息增益上界，当这些上界与超额风险界结合时，我们获得了极小极大最优收敛速率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the sample complexity trade-offs between effective dimension and information gain in kernel-based learning, aiming to unify these concepts for improved theoretical understanding. The authors introduce a new quantity called relative information gain, which measures the sensitivity of information gain to observation noise and is shown to interpolate between effective dimension and information gain while matching the growth rate of the former. Experimental analysis demonstrates that combining PAC-Bayesian excess risk bounds with spectral-based upper bounds on relative information gain leads to minimax-optimal convergence rates for Gaussian process regression.</div>
<div class="mono" style="margin-top:8px">本研究针对核基学习中样本复杂度分析的两个关键量——信息增益与有效维数之间的差异，引入了相对信息增益这一新量，用于衡量信息增益对观测噪声的敏感性。方法表明，该量在有效维数和信息增益之间平滑插值，且与有效维数具有相同的增长率，并自然产生于高斯过程回归的一个新PAC-贝叶斯超额风险界中。关键实验结果通过基于核谱性质对相对信息增益进行界定，并将其融入风险界，证明了该方法能够达到极小极大最优收敛速率。</div>
</details>
</div>
<div class="card">
<div class="title">Mathematical theory of deep learning</div>
<div class="meta-line">Authors: Philipp Petersen, Jakob Zech</div>
<div class="meta-line">First: 2024-07-25T20:37:12+00:00 · Latest: 2026-01-15T17:54:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.18384v4">Abs</a> · <a href="https://arxiv.org/pdf/2407.18384v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This book provides an introduction to the mathematical analysis of deep learning. It covers fundamental results in approximation theory, optimization theory, and statistical learning theory, which are the three main pillars of deep neural network theory. Serving as a guide for students and researchers in mathematics and related fields, the book aims to equip readers with foundational knowledge on the topic. It prioritizes simplicity over generality, and presents rigorous yet accessible results to help build an understanding of the essential mathematical concepts underpinning deep learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度学习数学理论</div>
<div class="mono" style="margin-top:8px">本书系统介绍深度学习的数学分析基础，涵盖逼近论、优化理论和统计学习理论三大理论支柱的核心成果。面向数学及相关领域的研究者与学习者，本书以简明性优先于普适性为原则，通过严谨而易于理解的论述，帮助读者掌握支撑深度学习的关键数学概念与理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The book is motivated by the need to establish a rigorous mathematical foundation for deep learning, addressing the gap between empirical success and theoretical understanding. It systematically reviews core results from approximation theory, optimization theory, and statistical learning theory, which form the essential theoretical pillars for analyzing deep neural networks. The presentation prioritizes clarity and accessibility, offering a structured guide that equips students and researchers with the fundamental knowledge required to comprehend the mathematical principles underlying deep learning models.</div>
<div class="mono" style="margin-top:8px">本书的动机是为深度学习建立严谨的数学基础，以弥合其经验性成功与理论理解之间的差距。其方法系统地综合了逼近论、优化理论和统计学习理论的核心成果，将它们呈现为深度神经网络理论的三大支柱。主要贡献在于提供了一个结构清晰、易于理解的指南，优先考虑简洁性与可读性，旨在使学生和研究人员掌握分析深度学习模型所必需的数学概念。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Uncertainty Guided Policy Learning for Multimodal Reasoning</div>
<div class="meta-line">Authors: Rui Liu, Dian Yu, Tong Zheng, Runpeng Dai, Zongxia Li, Wenhao Yu, Zhenwen Liang, Linfeng Song, Haitao Mi, Pratap Tokekar, Dong Yu</div>
<div class="meta-line">First: 2025-10-01T20:32:08+00:00 · Latest: 2026-01-15T17:51:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01444v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced reasoning capabilities in multimodal large language models. However, existing methods typically treat visual inputs as deterministic, overlooking the perceptual ambiguity inherent to the visual modality. Consequently, they fail to distinguish whether a model&#x27;s uncertainty stems from complex reasoning or ambiguous perception, preventing the targeted allocation of exploration or learning signals. To address this gap, we introduce DUPL, a dual-uncertainty guided policy learning approach for multimodal RLVR that quantifies and leverages both perceptual uncertainty (via symmetric KL divergence) and output uncertainty (via policy entropy) to guide policy updates. By establishing an uncertainty-driven feedback loop and employing a dynamic branch prioritization mechanism, DUPL recalibrates the policy advantage to focus learning on states with high perceptual or decisional ambiguity, enabling effective targeted exploration beyond passive data augmentation. Implemented on top of GRPO and evaluated on six multimodal mathematical and general-domain reasoning benchmarks, DUPL improves Qwen2.5-VL 3B and 7B models, achieving accuracy gains of up to 11.2% on visual math tasks and up to 7.1% on general-domain reasoning tasks, while consistently outperforming GRPO. These results demonstrate that dual-uncertainty guided policy learning is an effective and generalizable approach for multimodal RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双不确定性引导的多模态推理策略学习</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）提升了多模态大语言模型的推理能力。然而，现有方法通常将视觉输入视为确定性信息，忽略了视觉模态固有的感知模糊性。这导致模型无法区分其不确定性是源于复杂推理还是模糊感知，从而难以针对性地分配探索或学习信号。为解决这一问题，我们提出了DUPL——一种双不确定性引导的多模态RLVR策略学习方法，通过量化并利用感知不确定性（基于对称KL散度）与输出不确定性（基于策略熵）来指导策略更新。通过建立不确定性驱动的反馈循环并采用动态分支优先级机制，DUPL重新校准策略优势，将学习聚焦于具有高感知或决策模糊性的状态，实现了超越被动数据增强的有效定向探索。在GRPO框架上实现并在六个多模态数学及通用领域推理基准上评估，DUPL提升了Qwen2.5-VL 3B和7B模型的性能，在视觉数学任务上准确率最高提升11.2%，在通用领域推理任务上最高提升7.1%，且持续优于GRPO。这些结果表明，双不确定性引导的策略学习是多模态RLVR中一种有效且可泛化的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation in existing multimodal reinforcement learning with verifiable rewards (RLVR) methods, which treat visual inputs as deterministic and thus cannot differentiate whether model uncertainty arises from complex reasoning or ambiguous perception, hindering targeted learning. The proposed method, DUPL, introduces a dual-uncertainty guided policy learning approach that quantifies perceptual uncertainty via symmetric KL divergence and output uncertainty via policy entropy, using them to recalibrate policy advantages through an uncertainty-driven feedback loop and dynamic branch prioritization for focused exploration. Experimental evaluations on six multimodal reasoning benchmarks show that DUPL, implemented on GRPO, improves Qwen2.5-VL models by up to 11.2% on visual math tasks and up to 7.1% on general-domain reasoning tasks, consistently outperforming the baseline.</div>
<div class="mono" style="margin-top:8px">本研究针对现有多模态可验证奖励强化学习方法将视觉输入视为确定性、无法区分模型不确定性源于复杂推理还是感知模糊性，从而阻碍针对性探索的局限性。提出的DUPL方法采用双重不确定性引导的策略学习，通过对称KL散度量化和觉不确定性，通过策略熵量化输出不确定性，并利用不确定性驱动的反馈循环和动态分支优先级机制重新校准策略优势。在六个多模态推理基准上的实验表明，基于GRPO实现的DUPL提升了Qwen2.5-VL模型性能，在视觉数学任务上准确率最高提升11.2%，在通用领域推理任务上最高提升7.1%，均持续优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Driven Dynamic Factor Modeling via Manifold Learning</div>
<div class="meta-line">Authors: Graeme Baker, Agostino Capponi, J. Antonio Sidaoui</div>
<div class="meta-line">First: 2025-06-24T18:40:40+00:00 · Latest: 2026-01-15T17:50:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.19945v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.19945v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a data-driven dynamic factor framework for modeling the joint evolution of high-dimensional covariates and responses without parametric assumptions. Standard factor models applied to covariates alone often lose explanatory power for responses. Our approach uses anisotropic diffusion maps, a manifold learning technique, to learn low-dimensional embeddings that preserve both the intrinsic geometry of the covariates and the predictive relationship with responses. For time series arising from Langevin diffusions in Euclidean space, we show that the associated graph Laplacian converges to the generator of the underlying diffusion. We further establish a bound on the approximation error between the diffusion map coordinates and linear diffusion processes, and we show that ergodic averages in the embedding space converge under standard spectral assumptions. These results justify using Kalman filtering in diffusion-map coordinates for predicting joint covariate-response evolution. We apply this methodology to equity-portfolio stress testing using macroeconomic and financial variables from Federal Reserve supervisory scenarios, achieving mean absolute error improvements of up to 55% over classical scenario analysis and 39% over principal component analysis benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流形学习的数据驱动动态因子建模</div>
<div class="mono" style="margin-top:8px">本文提出一种数据驱动的动态因子框架，用于建模高维协变量与响应变量的联合演化过程，无需参数化假设。传统仅应用于协变量的因子模型常丧失对响应变量的解释力。本方法采用各向异性扩散映射（一种流形学习技术）来学习低维嵌入，该嵌入既保留协变量的内在几何结构，又维持与响应变量的预测关系。针对欧氏空间中朗之万扩散产生的时间序列，我们证明相关图拉普拉斯算子收敛于底层扩散的生成元。进一步建立了扩散映射坐标与线性扩散过程间近似误差的界限，并证明在标准谱假设下嵌入空间的遍历平均值具有收敛性。这些结果为在扩散映射坐标中使用卡尔曼滤波预测协变量-响应联合演化提供了理论依据。我们将该方法应用于基于美联储监管情景的宏观经济与金融变量进行股票组合压力测试，相比经典情景分析平均绝对误差降低达55%，较主成分分析基准提升39%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of standard factor models, which often lose explanatory power for responses when applied only to covariates, by developing a data-driven dynamic factor framework for modeling high-dimensional covariate-response evolution without parametric assumptions. The method employs anisotropic diffusion maps, a manifold learning technique, to learn low-dimensional embeddings that preserve both the intrinsic geometry of the covariates and their predictive relationship with the responses; theoretical analysis shows the graph Laplacian converges to the generator of an underlying Langevin diffusion, and error bounds justify using Kalman filtering in the embedding space for prediction. Experimental application to equity-portfolio stress testing with Federal Reserve supervisory data demonstrates the approach, achieving mean absolute error improvements of up to 55% over classical scenario analysis and 39% over principal component analysis benchmarks.</div>
<div class="mono" style="margin-top:8px">该研究针对标准因子模型仅应用于协变量时对响应变量的解释力不足的问题，开发了一种数据驱动的动态因子框架，用于对高维协变量与响应变量的联合演化进行非参数建模。该方法采用各向异性扩散映射这一流形学习技术，学习低维嵌入以同时保留协变量的内在几何结构及其与响应变量的预测关系；理论分析证明了图拉普拉斯算子收敛于底层朗之万扩散的生成元，并建立了逼近误差界，从而为在嵌入空间中使用卡尔曼滤波进行预测提供了依据。在基于美联储监管情景的宏观经济和金融变量进行股票投资组合压力测试的应用中，该方法相比经典情景分析的平均绝对误差降低了高达55%，相比主成分分析基准降低了39%。</div>
</details>
</div>
<div class="card">
<div class="title">Classification Imbalance as Transfer Learning</div>
<div class="meta-line">Authors: Eric Xia, Jason M. Klusowski</div>
<div class="meta-line">First: 2026-01-15T17:49:36+00:00 · Latest: 2026-01-15T17:49:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10630v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10630v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Classification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generating synthetic samples from an estimated minority-class distribution to roughly balance the classes, among which the celebrated SMOTE algorithm is a canonical example. We show that the excess risk decomposes into the rate achievable under balanced training (as if the data had been drawn from the balanced target distribution) and an additional term, the cost of transfer, which quantifies the discrepancy between the estimated and true minority-class distributions. In particular, we show that the cost of transfer for SMOTE dominates that of bootstrapping (random oversampling) in moderately high dimensions, suggesting that we should expect bootstrapping to have better performance than SMOTE in general. We corroborate these findings with experimental evidence. More broadly, our results provide guidance for choosing among augmentation strategies for imbalanced classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分类不平衡作为迁移学习</div>
<div class="mono" style="margin-top:8px">分类不平衡指某一类别远少于其他类别。本文将这一场景构建为标签（先验）偏移下的迁移学习问题，其中观测数据诱导的不平衡源分布与评估性能所用的平衡目标分布之间存在偏移。在此框架下，我们研究了一类过采样方法，通过从估计的少数类分布生成合成样本以大致平衡类别，其中著名的SMOTE算法是典型代表。我们证明，超额风险可分解为平衡训练下可达到的速率（即数据仿佛来自平衡目标分布）与一个额外项——迁移成本，该成本量化了估计与真实少数类分布间的差异。特别地，我们发现SMOTE的迁移成本在中等高维情况下主导了自助法（随机过采样），表明通常应预期自助法比SMOTE性能更优。实验证据支持了这些结论。更广泛而言，本研究为不平衡分类中增强策略的选择提供了指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses classification imbalance by reframing it as a transfer learning problem under label shift, where the imbalanced observed data serves as a source distribution and a balanced distribution is the target for evaluation. The method analyzes a family of oversampling procedures, including SMOTE and bootstrapping, which generate synthetic minority-class samples to balance training data. Key experimental findings reveal that the excess risk decomposes into a balanced training rate and a transfer cost term; this cost is shown to dominate for SMOTE compared to bootstrapping in moderately high dimensions, indicating that bootstrapping generally outperforms SMOTE, a conclusion supported by experimental evidence.</div>
<div class="mono" style="margin-top:8px">本文通过将分类不平衡问题重新定义为标签偏移下的迁移学习问题，其中不平衡的观测数据作为源分布，而平衡分布作为评估目标。该方法分析了一系列过采样程序，包括SMOTE和自助法，这些程序通过生成合成少数类样本来平衡训练数据。关键实验结果表明，超额风险可分解为平衡训练速率和“迁移成本”项；特别地，在中等高维情况下，SMOTE的迁移成本高于自助法，这表明自助法通常表现更优，实验证据也支持了这一结论。</div>
</details>
</div>
<div class="card">
<div class="title">Parametric RDT approach to computational gap of symmetric binary perceptron</div>
<div class="meta-line">Authors: Mihailo Stojnic</div>
<div class="meta-line">First: 2026-01-15T17:48:58+00:00 · Latest: 2026-01-15T17:48:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10628v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10628v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \emph{satisfiability} ($α_c$) -- \emph{algorithmic} ($α_a$) constraints density threshold change thereby suggesting a potential existence of a nonzero computational gap $SCG=α_c-α_a$. The second level estimate is shown to match the theoretical $α_c$ whereas the $r\rightarrow \infty$ level one is proposed to correspond to $α_a$. For example, for the canonical SBP ($κ=1$ margin) we obtain $α_c\approx 1.8159$ on the second and $α_a\approx 1.6021$ (with converging tendency towards $\sim 1.59$ range) on the seventh level. Our propositions remarkably well concur with recent literature: (i) in [20] local entropy replica approach predicts $α_{LE}\approx 1.58$ as the onset of clustering defragmentation (presumed driving force behind locally improving algorithms failures); (ii) in $α\rightarrow 0$ regime we obtain on the third lifting level $κ\approx 1.2385\sqrt{\frac{α_a}{-\log\left ( α_a \right ) }}$ which qualitatively matches overlap gap property (OGP) based predictions of [43] and identically matches local entropy based predictions of [24]; (iii) $c$-sequence ordering change phenomenology mirrors the one observed in asymmetric binary perceptron (ABP) in [98] and the negative Hopfield model in [100]; and (iv) as in [98,100], we here design a CLuP based algorithm whose practical performance closely matches proposed theoretical predictions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对称二元感知器计算间隙的参数化RDT方法研究</div>
<div class="mono" style="margin-top:8px">我们通过参数化运用全提升随机对偶理论（fl-RDT）[96]，研究了对称二元感知器（SBP）中统计-计算间隙（SCG）的潜在存在性。在第二提升层级上观察到c序列（fl-RDT的关键参数组件）从递减排序到任意排序的结构性变化，该变化与可满足性阈值（α_c）到算法阈值（α_a）的约束密度转变相关联，从而暗示了非零计算间隙SCG=α_c-α_a的潜在存在。第二层级的估计值与理论α_c相符，而r→∞层级的估计值被提议对应于α_a。例如，对于典型SBP（κ=1边界），我们在第二层级得到α_c≈1.8159，在第七层级得到α_a≈1.6021（具有向∼1.59范围收敛的趋势）。我们的命题与近期文献高度吻合：（i）文献[20]中局部熵复本方法预测α_LE≈1.58为聚类碎片化起始点（推测为局部改进算法失效的驱动因素）；（ii）在α→0区域，我们在第三提升层级得到κ≈1.2385√(α_a/(-log(α_a)))，该结果在定性上与文献[43]基于重叠间隙特性（OGP）的预测一致，并与文献[24]基于局部熵的预测完全相同；（iii）c序列排序变化的现象学与文献[98]中非对称二元感知器（ABP）及文献[100]中负Hopfield模型观察到的现象相呼应；（iv）如文献[98,100]所示，我们在此设计了一种基于CLuP的算法，其实际性能与提出的理论预测高度匹配。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates potential statistical-computational gaps in the symmetric binary perceptron model by applying a parametric version of fully lifted random duality theory. The method involves analyzing structural changes in a key parametric component, the c-sequence, across different lifting levels, linking the second level to the satisfiability threshold and higher levels to the algorithmic threshold. Key experimental findings include estimating the satisfiability threshold α_c ≈ 1.8159 and the algorithmic threshold α_a ≈ 1.6021, which aligns with predictions from local entropy and overlap gap property analyses in recent literature, and the design of an algorithm whose performance matches these theoretical predictions.</div>
<div class="mono" style="margin-top:8px">本研究通过参数化应用完全提升随机对偶理论，探讨对称二元感知器中可能存在的统计-计算间隙。该方法涉及分析关键参数分量c序列在不同提升层级上的结构变化，将第二层级上观察到的特定排序转变与可满足性阈值相关联，并提出更高提升层级对应于算法阈值。主要实验结果包括估计出可满足性阈值α_c ≈ 1.8159和算法阈值α_a ≈ 1.6021，这与近期文献中基于局部熵和重叠间隙属性的预测结果一致，并且所设计的算法其实际性能与这些理论预测密切吻合。</div>
</details>
</div>
<div class="card">
<div class="title">Differential Privacy as a Perk: Federated Learning over Multiple-Access Fading Channels with a Multi-Antenna Base Station</div>
<div class="meta-line">Authors: Hao Liang, Haifeng Wen, Kaishun Wu, Hong Xing</div>
<div class="meta-line">First: 2025-10-27T16:01:15+00:00 · Latest: 2026-01-15T17:38:48+00:00</div>
<div class="meta-line">Comments: 13 pages, 6 figures, submitted for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23463v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23463v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Learning (FL) is a distributed learning paradigm that preserves privacy by eliminating the need to exchange raw data during training. In its prototypical edge instantiation with underlying wireless transmissions enabled by analog over-the-air computing (AirComp), referred to as \emph{over-the-air FL (AirFL)}, the inherent channel noise plays a unique role of \emph{frenemy} in the sense that it degrades training due to noisy global aggregation while providing a natural source of randomness for privacy-preserving mechanisms, formally quantified by \emph{differential privacy (DP)}. It remains, nevertheless, challenging to effectively harness such channel impairments, as prior arts, under assumptions of either simple channel models or restricted types of loss functions, mostly considering (local) DP enhancement with a single-round or non-convergent bound on privacy loss. In this paper, we study AirFL over multiple-access fading channels with a multi-antenna base station (BS) subject to user-level DP requirements. Despite a recent study, which claimed in similar settings that artificial noise (AN) must be injected to ensure DP in general, we demonstrate, on the contrary, that DP can be gained as a \emph{perk} even \emph{without} employing any AN. Specifically, we derive a novel bound on DP that converges under general bounded-domain assumptions on model parameters, along with a convergence bound with general smooth and non-convex loss functions. Next, we optimize over receive beamforming and power allocations to characterize the optimal convergence-privacy trade-offs, which also reveal explicit conditions in which DP is achievable without compromising training. Finally, our theoretical findings are validated by extensive numerical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>差分隐私作为额外收益：多天线基站下多址衰落信道上的联邦学习</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）是一种分布式学习范式，通过在训练过程中避免交换原始数据来保护隐私。在其典型的边缘实例中，通过模拟空中计算（AirComp）实现底层无线传输，称为空中联邦学习（AirFL），固有信道噪声扮演着“亦敌亦友”的角色：一方面因噪声全局聚合而降低训练效果，另一方面为隐私保护机制提供了天然的随机性来源，可通过差分隐私（DP）进行形式化量化。然而，有效利用此类信道损伤仍具挑战性，因为现有研究多在简单信道模型或受限损失函数假设下，主要关注单轮或非收敛隐私界的（局部）DP增强。本文研究了多天线基站（BS）下多址衰落信道上的AirFL，并满足用户级DP要求。尽管近期有研究在类似设定中声称通常需注入人工噪声（AN）以确保DP，但我们证明，即使不采用任何AN，DP仍可作为额外收益获得。具体而言，我们在模型参数有界域的一般假设下推导出收敛的新型DP界，并结合一般光滑非凸损失函数的收敛界。随后，通过优化接收波束成形与功率分配，刻画了最优的收敛-隐私权衡关系，并揭示了在不影响训练前提下实现DP的显式条件。最后，通过大量数值实验验证了理论发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of leveraging inherent channel noise in over-the-air federated learning (AirFL) for differential privacy (DP) without compromising model convergence, as prior work often relied on simplified channel models or required artificial noise injection. The method studies AirFL over multiple-access fading channels with a multi-antenna base station, deriving a novel, convergent bound on user-level DP under general bounded-domain model parameters and smooth, non-convex loss functions, and optimizes receive beamforming and power allocation to characterize the optimal trade-off between convergence and privacy. Experimental results validate that DP can be achieved as a &#x27;perk&#x27; without artificial noise under certain conditions, without sacrificing training performance.</div>
<div class="mono" style="margin-top:8px">本研究针对无线联邦学习中信道噪声既是干扰源又是隐私保护机制的双重角色，探究了在多天线系统中是否必须注入人工噪声以实现差分隐私。方法上，在一般有界域模型假设和平滑非凸损失函数下，推导了新颖的、收敛的用户级差分隐私界限，并优化接收波束成形和功率分配以刻画收敛与隐私的权衡关系。主要实验结果表明，差分隐私可以作为‘额外收益’在不注入人工噪声的情况下实现，优化过程揭示了隐私不损害训练的明确条件，并通过大量数值结果进行了验证。</div>
</details>
</div>
<div class="card">
<div class="title">An Improved Privacy and Utility Analysis of Differentially Private SGD with Bounded Domain and Smooth Losses</div>
<div class="meta-line">Authors: Hao Liang, Wanrong Zhang, Xinlei He, Kaishun Wu, Hong Xing</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-02-25T02:05:41+00:00 · Latest: 2026-01-15T17:33:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 figures, accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17772v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.17772v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to protect sensitive data during the training of machine learning models, but its privacy guarantee often comes at a large cost of model performance due to the lack of tight theoretical bounds quantifying privacy loss. While recent efforts have achieved more accurate privacy guarantees, they still impose some assumptions prohibited from practical applications, such as convexity and complex parameter requirements, and rarely investigate in-depth the impact of privacy mechanisms on the model&#x27;s utility. In this paper, we provide a rigorous privacy characterization for DPSGD with general L-smooth and non-convex loss functions, revealing converged privacy loss with iteration in bounded-domain cases. Specifically, we track the privacy loss over multiple iterations, leveraging the noisy smooth-reduction property, and further establish comprehensive convergence analysis in different scenarios. In particular, we show that for DPSGD with a bounded domain, (i) the privacy loss can still converge without the convexity assumption, (ii) a smaller bounded diameter can improve both privacy and utility simultaneously under certain conditions, and (iii) the attainable big-O order of the privacy utility trade-off for DPSGD with gradient clipping (DPSGD-GC) and for DPSGD-GC with bounded domain (DPSGD-DC) and mu-strongly convex population risk function, respectively. Experiments via membership inference attack (MIA) in a practical setting validate insights gained from the theoretical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>有界域与光滑损失下差分隐私随机梯度下降的隐私性与效用改进分析</div>
<div class="mono" style="margin-top:8px">差分隐私随机梯度下降（DPSGD）被广泛用于机器学习模型训练中的敏感数据保护，但由于缺乏量化隐私损失的严格理论界，其隐私保障常以模型性能大幅下降为代价。尽管近期研究实现了更精确的隐私保障，但仍存在凸性假设、复杂参数要求等不切实际的限制，且鲜有深入探究隐私机制对模型效用的影响。本文针对一般L光滑非凸损失函数，为DPSGD建立了严格的隐私刻画，揭示了有界域情形下隐私损失随迭代收敛的特性。具体而言，我们通过噪声光滑缩减特性追踪多轮迭代的隐私损失，并在不同场景下建立完整的收敛性分析。特别证明：对于有界域DPSGD，（1）无需凸性假设仍可实现隐私损失收敛；（2）在特定条件下更小的有界直径可同步提升隐私性与效用；（3）分别给出梯度裁剪DPSGD（DPSGD-GC）与有界域DPSGD（DPSGD-DC）在μ强凸总体风险函数下可达到的隐私-效用权衡的大O阶数。通过实际场景中的成员推理攻击实验验证了理论结论的洞察。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to improve the theoretical understanding of Differentially Private Stochastic Gradient Descent (DPSGD), as its privacy guarantees often lead to significant model performance degradation due to a lack of tight bounds. The method provides a rigorous privacy analysis for DPSGD with general L-smooth and non-convex loss functions on bounded domains, tracking privacy loss across iterations using a noisy smooth-reduction property and establishing comprehensive convergence analysis. Key experimental findings from membership inference attacks validate the theoretical insights, showing that privacy loss can converge without convexity, a smaller bounded domain can simultaneously enhance privacy and utility, and specific big-O orders characterize the privacy-utility trade-off for variants of DPSGD.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进对差分隐私随机梯度下降（DPSGD）的理论理解，因为其隐私保证常因缺乏紧致边界而导致模型性能显著下降。该方法对有界域上的一般L-光滑非凸损失函数的DPSGD进行了严格的隐私分析，利用噪声光滑约减性质追踪迭代中的隐私损失，并建立了全面的收敛性分析。通过成员推理攻击的实验结果验证了理论见解，表明隐私损失可在非凸假设下收敛，更小的有界域能在特定条件下同时提升隐私性和效用，并且对于DPSGD-GC和DPSGD-DC等变体，其隐私-效用权衡具有特定的大O阶表征。</div>
</details>
</div>
<div class="card">
<div class="title">Rewarding the Rare: Uniqueness-Aware RL for Creative Problem Solving in LLMs</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yucheng Wang, Yufei He, Jiaying Wu, Yilun Zhao, See-Kiong Ng, Cynthia Breazeal, Anh Tuan Luu, Hae Won Park, Bryan Hooi</div>
<div class="meta-line">First: 2026-01-13T17:48:43+00:00 · Latest: 2026-01-15T17:24:46+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08763v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08763v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a central paradigm for post-training large language models (LLMs), particularly for complex reasoning tasks, yet it often suffers from exploration collapse: policies prematurely concentrate on a small set of dominant reasoning patterns, improving pass@1 while limiting rollout-level diversity and gains in pass@k. We argue that this failure stems from regularizing local token behavior rather than diversity over sets of solutions. To address this, we propose Uniqueness-Aware Reinforcement Learning, a rollout-level objective that explicitly rewards correct solutions that exhibit rare high-level strategies. Our method uses an LLM-based judge to cluster rollouts for the same problem according to their high-level solution strategies, ignoring superficial variations, and reweights policy advantages inversely with cluster size. As a result, correct but novel strategies receive higher rewards than redundant ones. Across mathematics, physics, and medical reasoning benchmarks, our approach consistently improves pass@$k$ across large sampling budgets and increases the area under the pass@$k$ curve (AUC@$K$) without sacrificing pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励罕见策略：面向大语言模型创造性问题求解的独特性感知强化学习</div>
<div class="mono" style="margin-top:8px">强化学习已成为大语言模型后训练的核心范式，尤其在复杂推理任务中，但其常面临探索坍缩问题：策略过早集中于少数主导推理模式，虽提升pass@1指标，却限制了轨迹级多样性及pass@k增益。本文指出该问题源于对局部词元行为的正则化而非解决方案集的多样性调控。为此，我们提出独特性感知强化学习——一种轨迹级优化目标，显式奖励采用罕见高层策略的正确解法。该方法基于大语言模型的评判器，将同一问题的求解轨迹按高层策略聚类（忽略表面差异），并依据聚类规模对策略优势进行逆向加权。由此，正确且新颖的策略将比冗余策略获得更高奖励。在数学、物理和医学推理基准测试中，本方法在大规模采样预算下持续提升pass@$k$指标，在保持pass@1性能的同时显著提高pass@$k$曲线下面积，并能维持探索性，系统性地发掘更多样化的求解策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement learning for post-training large language models often leads to exploration collapse, where policies converge prematurely on a few dominant reasoning patterns, limiting solution diversity and gains in pass@k metrics. To address this, the authors propose Uniqueness-Aware Reinforcement Learning, which explicitly rewards correct solutions that employ rare high-level strategies by using an LLM-based judge to cluster rollouts by strategy and reweight policy advantages inversely with cluster size. Experiments on mathematics, physics, and medical reasoning benchmarks show the method consistently improves pass@k and the area under the pass@k curve without harming pass@1, while sustaining exploration and uncovering more diverse solution strategies at scale.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型（LLM）在复杂推理任务上的强化学习后训练常出现探索崩溃问题，即策略过早集中于少数主导推理模式，虽提升pass@1但限制了解决方案的多样性和pass@k的增益。为此，本研究提出独特性感知强化学习，该方法通过基于LLM的评判器根据高层策略对解决方案进行聚类，并依据聚类大小反比调整策略优势，从而显式奖励采用罕见高层策略的正确解决方案。在数学、物理和医学推理基准测试上的实验表明，该方法在不牺牲pass@1的前提下，持续提升了pass@k和pass@k曲线下面积，同时维持了探索性并大规模发现了更多样化的解决策略。</div>
</details>
</div>
<div class="card">
<div class="title">Procedural Fairness in Multi-Agent Bandits</div>
<div class="meta-line">Authors: Joshua Caiata, Carter Blair, Kate Larson</div>
<div class="meta-line">First: 2026-01-15T17:11:51+00:00 · Latest: 2026-01-15T17:11:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10600v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10600v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体老虎机中的程序公平性</div>
<div class="mono" style="margin-top:8px">在多智能体多臂老虎机（MA-MAB）背景下，公平性常被简化为结果导向：最大化福利、减少不平等或平衡效用。然而，心理学、经济学及罗尔斯理论的研究表明，公平性同样关乎决策过程与参与权。本文提出一种新的公平性目标——程序公平性，其赋予所有智能体平等的决策权，处于核心地位，并能实现结果的比例性。实证结果表明，基于结果优化的公平性概念牺牲了平等话语权与代表性，而在程序公平策略下，结果导向的公平目标（如平等主义与功利主义）的牺牲极小。我们进一步证明，不同的公平性概念本质上优先考虑不同且互斥的价值观，这凸显了公平性需要明确的规范性选择。本文主张程序合法性应作为公平性目标得到更多关注，并提供了实践程序公平性的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that fairness in multi-agent multi-armed bandits is typically defined by outcomes (e.g., welfare, equality), this work argues for incorporating procedural fairness, which emphasizes equal decision-making power and representation for all agents. The method introduces a new procedural fairness objective that ensures agents have a voice in decisions, lies in the core of the cooperative game, and yields proportional outcomes. Experimental results show that outcome-optimizing fairness notions sacrifice procedural equity, whereas procedurally fair policies incur only minimal losses in outcome-based metrics like equality and utilitarianism, and theoretical analysis confirms that different fairness notions represent incompatible normative choices.</div>
<div class="mono" style="margin-top:8px">本文的动机源于观察到多智能体多臂老虎机中的公平性通常由结果（如福利、平等）定义，因此主张纳入程序公平性，即强调所有智能体在决策中拥有平等的权力和代表性。方法上，研究引入了一种新的程序公平性目标，确保智能体在决策中有发言权，位于合作博弈的核心，并产生比例性的结果。实验结果表明，基于结果优化的公平性概念牺牲了程序公平，而程序公平策略在结果性指标（如平等和功利主义）上的损失极小，证明了其实用可行性，并凸显了在公平性定义中需要明确的规范性选择。</div>
</details>
</div>
<div class="card">
<div class="title">ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition</div>
<div class="meta-line">Authors: Arundeep Chinta, Lucas Vinh Tran, Jay Katukuri</div>
<div class="meta-line">Venue: AAAI 2026 oral presentation</div>
<div class="meta-line">First: 2026-01-15T17:02:06+00:00 · Latest: 2026-01-15T17:02:06+00:00</div>
<div class="meta-line">Comments: Accepted for oral presentation at the AI Meets Quantitative Finance Workshop at ICAIF 2025. An enhanced version was accepted for oral presentation at the AI for Time Series Analysis Workshop at AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10591v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10591v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student&#x27;s t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student&#x27;s-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER&#x27;s effectiveness in financial applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProbFM：具备不确定性分解能力的概率时间序列基础模型</div>
<div class="mono" style="margin-top:8px">时间序列基础模型已成为零样本金融预测领域的前沿方法，展现出卓越的迁移能力和数据效率优势。然而，其在金融应用中的推广受限于不确定性量化的根本性缺陷：现有方法或依赖强分布假设，或混淆不同不确定性来源，或缺乏理论校准机制。尽管近期研究采用混合模型、学生t分布、保形预测等复杂技术，但均未能解决理论支撑的不确定性分解这一核心挑战。本文首次提出基于Transformer的概率框架ProbFM，通过深度证据回归实现理论完备的不确定性量化，并提供显式的认知-偶然不确定性分解。相较于预设分布形式或依赖采样推断的现有方法，ProbFM通过高阶证据学习优化不确定性表征，同时保持单次前向计算效率。为在排除架构复杂性的干扰下严谨评估深度证据回归的核心量化能力，我们采用统一LSTM架构对五种概率方法进行对照实验：深度证据回归、高斯负对数似然、学生t分布负对数似然、分位数损失和保形预测。加密货币收益率预测实验表明，深度证据回归在保持预测精度的同时，实现了显式的认知-偶然不确定性分解。本研究既为基础模型提供了可扩展的理论化不确定性量化框架，也为深度证据回归在金融领域的有效性提供了实证依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address the limitations of existing Time Series Foundation Models (TSFMs) in providing principled uncertainty quantification for financial forecasting, as current methods often rely on restrictive assumptions or conflate uncertainty sources. The proposed method, ProbFM, is a novel transformer-based probabilistic framework that leverages Deep Evidential Regression (DER) to achieve explicit epistemic-aleatoric uncertainty decomposition through higher-order evidence learning, maintaining single-pass computational efficiency. In controlled experiments using an LSTM architecture across five probabilistic methods for cryptocurrency return forecasting, DER demonstrated competitive forecasting accuracy while successfully providing the desired uncertainty decomposition, establishing its empirical effectiveness.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有时间序列基础模型在金融预测中缺乏原则性不确定性量化的问题，因为当前方法通常依赖限制性假设或混淆了不同的不确定性来源。所提出的方法ProbFM是一种新颖的基于Transformer的概率框架，它利用深度证据回归通过高阶证据学习实现显式的认知-偶然不确定性分解，并保持单次计算效率。在使用LSTM架构对加密货币回报预测进行的对照实验中，深度证据回归在保持竞争力的预测准确性的同时，成功提供了显式的不确定性分解，从而确立了其作为基础模型中不确定性量化框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SSFL: Discovering Sparse Unified Subnetworks at Initialization for Efficient Federated Learning</div>
<div class="meta-line">Authors: Riyasat Ohib, Bishal Thapaliya, Gintare Karolina Dziugaite, Jingyu Liu, Vince Calhoun, Sergey Plis</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2026</div>
<div class="meta-line">First: 2024-05-15T02:13:51+00:00 · Latest: 2026-01-15T17:01:07+00:00</div>
<div class="meta-line">Comments: Published in Transactions on Machine Learning Research (TMLR), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.09037v2">Abs</a> · <a href="https://arxiv.org/pdf/2405.09037v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we propose Salient Sparse Federated Learning (SSFL), a streamlined approach for sparse federated learning with efficient communication. SSFL identifies a sparse subnetwork prior to training, leveraging parameter saliency scores computed separately on local client data in non-IID scenarios, and then aggregated, to determine a global mask. Only the sparse model weights are trained and communicated each round between the clients and the server. On standard benchmarks including CIFAR-10, CIFAR-100, and Tiny-ImageNet, SSFL consistently improves the accuracy sparsity trade off, achieving more than 20\% relative error reduction on CIFAR-10 compared to the strongest sparse baseline, while reducing communication costs by $2 \times$ relative to dense FL. Finally, in a real-world federated learning deployment, SSFL delivers over $2.3 \times$ faster communication time, underscoring its practical efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSFL：初始化阶段发现稀疏统一子网络以实现高效联邦学习</div>
<div class="mono" style="margin-top:8px">本研究提出显著稀疏联邦学习（SSFL），一种面向高效通信的稀疏联邦学习精简方法。SSFL在训练前识别稀疏子网络，通过计算非独立同分布场景下各客户端本地数据的参数显著度分数并聚合，以确定全局掩码。每轮训练和通信仅涉及稀疏模型权重。在CIFAR-10、CIFAR-100和Tiny-ImageNet等基准测试中，SSFL持续优化准确率与稀疏度的平衡，相比最强稀疏基线在CIFAR-10上实现超过20%的相对误差降低，同时通信成本较稠密联邦学习降低2倍。在实际联邦学习部署中，SSFL实现超过2.3倍的通信加速，凸显其实用效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance communication efficiency in federated learning, particularly under non-IID data distributions, this work introduces Salient Sparse Federated Learning (SSFL). The method identifies a sparse, trainable subnetwork before training begins by computing parameter saliency scores on local client data, aggregating them to form a global mask, and subsequently communicating and updating only the masked weights. Experimental results on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that SSFL improves the accuracy-sparsity trade-off, achieving over 20% relative error reduction on CIFAR-10 compared to strong sparse baselines while halving communication costs versus dense FL; a real-world deployment further demonstrated a 2.3× acceleration in communication time.</div>
<div class="mono" style="margin-top:8px">为提升联邦学习中的通信效率，特别是在非独立同分布数据场景下，本研究提出了显著稀疏联邦学习（SSFL）。该方法在训练开始前，通过在本地客户端数据上计算参数显著度分数并聚合以生成全局掩码，从而识别出一个稀疏的可训练子网络，随后每轮仅通信和更新被掩码的权重。在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准基准上的实验结果表明，SSFL改善了准确率与稀疏性的权衡，在CIFAR-10上相比最强的稀疏基线实现了超过20%的相对误差降低，同时将通信成本相较于稠密联邦学习减少了一半；实际部署进一步验证了其通信时间可加速2.3倍。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
