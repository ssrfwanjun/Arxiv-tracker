<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-21 06:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260121_0625</div>
    <div class="row"><div class="card">
<div class="title">Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training</div>
<div class="meta-line">Authors: Shuo Cheng, Liqian Ma, Zhenyang Chen, Ajay Mandlekar, Caelan Garrett, Danfei Xu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-23T04:32:53+00:00 · Latest: 2026-01-16T18:05:09+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18631v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.18631v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ot-sim2real.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to achieve up to a 30% improvement in the real-world success rate and even generalize to scenarios seen only in simulation. Project webpage: https://ot-sim2real.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可泛化的领域自适应：仿真与真实策略协同训练</div>
<div class="mono" style="margin-top:8px">行为克隆在机器人操作任务中展现出潜力，但大规模获取真实世界演示数据成本高昂。仿真数据虽能提供可扩展的替代方案（尤其在自动演示生成技术进步的背景下），但策略向真实世界的迁移仍受多种仿真与真实领域差异的阻碍。本研究提出一个统一的仿真-真实协同训练框架，用于学习可泛化的操作策略，该框架主要利用仿真数据，仅需少量真实世界演示。方法的核心在于学习一个领域不变且与任务相关的特征空间。我们的关键见解是：对齐跨领域观测值及其对应动作的联合分布，比仅对齐观测值（边缘分布）能提供更丰富的信号。为此，我们在协同训练框架中嵌入了一种受最优传输理论启发的损失函数，并将其扩展至非平衡最优传输框架，以处理丰富的仿真数据与有限的真实样本之间的不平衡问题。我们在具有挑战性的操作任务上验证了该方法，结果表明其能利用大量仿真数据将真实世界成功率提升高达30%，甚至能泛化至仅在仿真中见过的场景。项目网页：https://ot-sim2real.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of transferring robot manipulation policies from simulation to reality, where real-world demonstrations are costly to obtain. The proposed method introduces a unified sim-and-real co-training framework that learns a domain-invariant feature space by aligning the joint distributions of observations and actions across domains, using an Optimal Transport-inspired loss extended to an Unbalanced OT formulation to handle data imbalance. Experimental validation on manipulation tasks demonstrates that the method leverages abundant simulation data to achieve up to a 30% improvement in real-world success rates and enables generalization to scenarios only seen in simulation.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人操作策略从仿真到现实迁移的挑战，其中真实世界演示数据稀缺而仿真数据丰富。该方法提出了一个仿真与真实协同训练框架，通过使用最优传输（OT）启发的损失函数对齐跨领域的观测与动作联合分布来学习领域不变的特征空间，并扩展为不平衡OT框架以处理数据不平衡问题。在操作任务上的实验验证表明，该方法可将真实世界成功率提升高达30%，并能泛化到仅在仿真中见过的场景。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations</div>
<div class="meta-line">Authors: Franziska Herbert, Vignesh Prasad, Han Liu, Dorothea Koert, Georgia Chalvatzaki</div>
<div class="meta-line">First: 2026-01-16T17:35:00+00:00 · Latest: 2026-01-16T17:35:00+00:00</div>
<div class="meta-line">Comments: 9 pages, 7 figures, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11460v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11460v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic-geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic-geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人类演示的语义-几何任务图表示学习</div>
<div class="mono" style="margin-top:8px">从人类演示中学习结构化任务表示对于理解长时程操作行为至关重要，尤其在双手操作场景中，动作顺序、对象参与和交互几何关系可能存在显著差异。核心挑战在于以支持任务进程推理的形式，同时捕捉任务的离散语义结构和以对象为中心的几何关系的时间演化。本研究提出一种语义-几何任务图表示方法，从人类演示中编码对象身份、对象间关系及其时间几何演化。基于此框架，我们设计了一个结合消息传递神经网络编码器与基于Transformer解码器的学习框架，将场景表示学习与基于动作条件的任务进程推理解耦。编码器仅使用时序场景图学习结构化表示，解码器则基于动作上下文预测未来动作序列、关联对象及长时程对象运动。通过对人类演示数据集的广泛评估，我们证明语义-几何任务图表示对动作和对象高度可变的任务尤为有效，而传统基于序列的模型难以捕捉此类任务进程。最后，我们验证了任务图表示可迁移至实体双手机器人并用于在线动作选择，凸显其作为可复用任务抽象在操作系统下游决策中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand complex long-horizon bimanual manipulation from human demonstrations, this research addresses the challenge of jointly capturing discrete semantic task structure and the temporal evolution of object-centric geometric relations. The method introduces a semantic-geometric task graph representation and a learning framework that combines a Message Passing Neural Network (MPNN) encoder to learn from temporal scene graphs with a Transformer-based decoder that uses action context to predict future action sequences, associated objects, and object motions. Experimental results on human demonstration datasets show that this graph-based representation outperforms simpler sequence models, especially in tasks with high action and object variability, and the learned representations are successfully transferred to a physical bimanual robot for online action selection.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从人类演示中学习结构化任务表示的挑战，特别是在动作顺序、物体参与和交互几何变化较大的长时程双手操作场景中。该方法提出了一种语义几何任务图表示，以及一个结合了消息传递神经网络编码器和基于Transformer解码器的学习框架：编码器从时序场景图中学习结构化表示，而解码器则基于动作上下文推理任务进展，预测未来的动作、相关物体和运动。在人类演示数据集上的实验结果表明，该表示在动作和物体高度可变的任务中优于简单的序列模型，并能成功迁移到实体双手机器人上实现在线动作选择，证明了其作为可重用任务抽象在操作系统中进行下游决策的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Probabilistic Mission Design for Neuro-Symbolic Unmanned Aircraft Systems</div>
<div class="meta-line">Authors: Simon Kohaut, Benedict Flade, Daniel Ochs, Devendra Singh Dhami, Julian Eggert, Kristian Kersting</div>
<div class="meta-line">First: 2024-12-25T11:04:00+00:00 · Latest: 2026-01-16T17:27:13+00:00</div>
<div class="meta-line">Comments: arXiv admin note: text overlap with arXiv:2406.03454</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.01439v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.01439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advanced Air Mobility (AAM) is a growing field that demands accurate and trustworthy models of legal concepts and restrictions for navigating Unmanned Aircraft Systems (UAS). In addition, any implementation of AAM needs to face the challenges posed by inherently dynamic and uncertain human-inhabited spaces robustly. Nevertheless, the employment of UAS beyond visual line of sight (BVLOS) is an endearing task that promises to significantly enhance today&#x27;s logistics and emergency response capabilities. Hence, we propose Probabilistic Mission Design (ProMis), a novel neuro-symbolic approach to navigating UAS within legal frameworks. ProMis is an interpretable and adaptable system architecture that links uncertain geospatial data and noisy perception with declarative, Hybrid Probabilistic Logic Programs (HPLP) to reason over the agent&#x27;s state space and its legality. To inform planning with legal restrictions and uncertainty in mind, ProMis yields Probabilistic Mission Landscapes (PML). These scalar fields quantify the belief that the HPLP is satisfied across the agent&#x27;s state space. Extending prior work on ProMis&#x27; reasoning capabilities and computational characteristics, we show its integration with potent machine learning models such as Large Language Models (LLM) and Transformer-based vision models. Hence, our experiments underpin the application of ProMis with multi-modal input data and how our method applies to many AAM scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经符号无人航空系统概率任务设计</div>
<div class="mono" style="margin-top:8px">先进空中交通（AAM）是一个快速发展的领域，需要精确可靠的法律概念与限制模型来引导无人航空系统（UAS）。此外，任何AAM的实施都必须稳健应对人类活动空间固有的动态性与不确定性带来的挑战。然而，超视距（BVLOS）UAS的应用是一项极具价值的任务，有望显著提升当前物流与应急响应能力。为此，我们提出概率任务设计（ProMis），一种在法规框架内导航UAS的新型神经符号方法。ProMis是一种可解释、可适应的系统架构，通过声明式混合概率逻辑程序（HPLP）将不确定的地理空间数据与含噪声感知相连接，以推理智能体的状态空间及其合法性。为在规划中兼顾法律限制与不确定性，ProMis生成概率任务态势图（PML）。这些标量场量化了HPLP在智能体状态空间内被满足的置信度。通过扩展ProMis推理能力与计算特性的前期研究，我们展示了其与大型语言模型（LLM）和基于Transformer的视觉模型等强大机器学习模型的集成。实验验证了ProMis在多模态输入数据中的应用，以及该方法对多种AAM场景的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for trustworthy and legally compliant navigation of Unmanned Aircraft Systems (UAS) in Advanced Air Mobility (AAM), particularly for operations beyond visual line of sight in dynamic, uncertain environments. The method introduces Probabilistic Mission Design (ProMis), a neuro-symbolic architecture that integrates uncertain geospatial and perceptual data with declarative Hybrid Probabilistic Logic Programs (HPLP) to reason about an agent&#x27;s state and its legality, generating Probabilistic Mission Landscapes (PML) to inform planning. Key experimental findings demonstrate the integration of ProMis with large language models and transformer-based vision models, validating its application with multi-modal input data across various AAM scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决先进空中交通领域中无人航空系统在动态、不确定环境中进行超视距运行时所面临的合法、可信导航需求。方法上提出了概率任务设计，这是一种神经符号架构，它将不确定的地理空间和感知数据与声明性的混合概率逻辑程序相结合，以推理智能体的状态及其合法性，并生成概率任务场景来指导规划。主要实验结果展示了该方法与大型语言模型和基于Transformer的视觉模型的集成，验证了其利用多模态输入数据在多种先进空中交通场景中的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-Based Shrinking Disturbance-Invariant Tubes for State- and Input-Dependent Uncertainty</div>
<div class="meta-line">Authors: Abdelrahman Ramadan, Sidney Givigi</div>
<div class="meta-line">Venue: IEEE Control Systems Letters, vol. 9, pp. 2699-2704, Dec. 2025</div>
<div class="meta-line">First: 2026-01-16T16:47:04+00:00 · Latest: 2026-01-16T16:47:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11426v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11426v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a learning-based framework for constructing shrinking disturbance-invariant tubes under state- and input-dependent uncertainty, intended as a building block for tube Model Predictive Control (MPC), and certify safety via a lifted, isotone (order-preserving) fixed-point map. Gaussian Process (GP) posteriors become $(1-α)$ credible ellipsoids, then polytopic outer sets for deterministic set operations. A two-time-scale scheme separates learning epochs, where these polytopes are frozen, from an inner, outside-in iteration that converges to a compact fixed point $Z^\star\!\subseteq\!\mathcal G$; its state projection is RPI for the plant. As data accumulate, disturbance polytopes tighten, and the associated tubes nest monotonically, resolving the circular dependence between the set to be verified and the disturbance model while preserving hard constraints. A double-integrator study illustrates shrinking tube cross-sections in data-rich regions while maintaining invariance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的状态与输入依赖不确定性下的收缩扰动不变管构建</div>
<div class="mono" style="margin-top:8px">我们开发了一种基于学习的框架，用于在状态与输入依赖的不确定性下构建收缩扰动不变管，旨在作为管式模型预测控制（MPC）的基础模块，并通过一个提升的、保序的固定点映射来确保安全性。高斯过程（GP）后验被转化为$(1-α)$可信椭球，进而通过确定性集合操作得到多面体外集。采用双时间尺度方案，将学习阶段（其中多面体保持固定）与一个由外向内迭代的内层过程分离，后者收敛至一个紧致固定点$Z^\star\!\subseteq\!\mathcal G$；其状态投影对受控对象是鲁棒正不变的。随着数据积累，扰动多面体逐渐收紧，相关管序列单调嵌套，从而解决了待验证集合与扰动模型之间的循环依赖问题，同时保持硬约束。通过双积分器案例研究，展示了在数据丰富区域管截面收缩的同时保持不变性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of constructing robust disturbance-invariant tubes for tube-based Model Predictive Control (MPC) under state- and input-dependent uncertainty, where a circular dependence exists between the set to be verified and the disturbance model. The method employs a learning-based framework that uses Gaussian Process (GP) regression to model uncertainty, converting its posterior into credible ellipsoids and then into polytopic outer approximations for deterministic set operations. A two-time-scale scheme separates learning epochs from an inner fixed-point iteration, which converges to a compact, robust positively invariant (RPI) set. Experimental results on a double-integrator system demonstrate that as data accumulate, the disturbance polytopes tighten, leading to monotonically nested and shrinking tubes that maintain safety and hard constraints, particularly in data-rich regions.</div>
<div class="mono" style="margin-top:8px">本研究针对状态和输入相关不确定性下，基于管的模型预测控制（MPC）中构建鲁棒扰动不变管的挑战，该问题中存在待验证集合与扰动模型之间的循环依赖。方法采用一个基于学习的框架，利用高斯过程（GP）回归对不确定性进行建模，将后验可信椭球转换为多面体外近似以进行确定性集合运算，并通过一个提升的、保序的不动点映射来认证安全性。一个双时间尺度方案将学习周期与内部迭代分离，后者收敛到一个紧致的鲁棒正不变（RPI）集。在双积分器系统上的实验结果表明，随着数据积累，扰动多面体收紧，导致在数据丰富区域中管集合单调嵌套并收缩，同时保持了不变性和硬约束满足。</div>
</details>
</div>
<div class="card">
<div class="title">The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents</div>
<div class="meta-line">Authors: Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Qingbo Hao, Hongliang Lu, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Ye, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Kecheng Zheng, Qian Zhu, Ran Cheng, Yong-Lu Li</div>
<div class="meta-line">First: 2026-01-16T16:42:05+00:00 · Latest: 2026-01-16T16:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11421v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11421v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rhos.ai/research/gm-100">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伟大征程100：面向具身智能体评估的100项精细化任务</div>
<div class="mono" style="margin-top:8px">近年来，随着机器人学习与模仿学习的快速发展，涌现出大量数据集与方法。然而，这些数据集及其任务设计往往缺乏系统性考量与原则性框架，引发重要思考：现有数据集与任务设计是否真正推动了机器人智能体能力的发展？基于少量通用任务的评估能否准确反映不同团队在不同任务上提出的各类方法的差异化性能？为此，我们推出“伟大征程100”（GM-100）作为迈向机器人学习奥林匹克的第一步。GM-100包含100项精心设计的任务，涵盖广泛的交互场景与长尾行为，旨在通过多样化、高挑战性的任务集合全面评估机器人智能体能力，并促进机器人数据集任务设计的多样性与复杂性。这些任务通过对现有任务设计的系统性分析与拓展，结合人-物交互基元与物体功能属性的洞见构建而成。我们在不同机器人平台上采集了大量轨迹数据，并对多种基线模型进行评估。实验结果表明：GM-100任务具有1）可执行性，2）足够挑战性，能有效区分当前视觉语言动作模型的性能差异。数据与代码已开源：https://rhos.ai/research/gm-100。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by concerns that current robot learning datasets and task designs lack systematic principles, potentially failing to accurately advance and differentiate agent capabilities. The method introduces the Great March 100 (GM-100) benchmark, comprising 100 detail-oriented tasks developed through systematic analysis of existing designs, human-object interaction primitives, and object affordances, with trajectory data collected on multiple robotic platforms. Key experimental findings show that the tasks are both executable and sufficiently challenging to effectively discriminate the performance of current vision-language-action models.</div>
<div class="mono" style="margin-top:8px">本研究源于对当前机器人学习数据集和任务设计缺乏系统性原则的担忧，这可能无法有效提升和区分智能体的能力。方法上提出了Great March 100（GM-100）基准，包含100个细节导向的任务，这些任务通过对现有设计、人-物交互基元及物体可供性的系统分析开发而成，并在多个机器人平台上收集了轨迹数据。关键实验结果表明，这些任务既具备可执行性，又具有足够的挑战性，能够有效区分当前视觉-语言-行动模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Conditioned Variational Bayesian Last Layer Dynamics Models</div>
<div class="meta-line">Authors: Paul Brunzema, Thomas Lew, Ray Zhang, Takeru Shirasawa, John Subosits, Marcus Greiff</div>
<div class="meta-line">First: 2026-01-14T05:25:18+00:00 · Latest: 2026-01-16T16:32:59+00:00</div>
<div class="meta-line">Comments: 9 pages, 7 figures, currently under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09178v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09178v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉条件化变分贝叶斯末层动力学模型</div>
<div class="mono" style="margin-top:8px">机器人系统的敏捷控制常需预判环境对系统行为的影响。例如，驾驶员必须感知前方路况以预判可用摩擦力并相应规划动作。在自主框架内实现此类前瞻性适应仍是挑战，尤其在快速变化条件下。传统建模方法难以捕捉系统行为的突变，而自适应方法本质是反应式的，可能因适应过迟而无法保证安全。我们提出一种视觉条件化变分贝叶斯末层动力学模型，利用视觉上下文预判环境变化。该模型先学习标称车辆动力学，再通过潜在特征的逐特征仿射变换进行微调，实现情境感知的动力学预测。最终模型被集成至车辆竞速的最优控制器中。我们在雷克萨斯LC500涉水竞速场景中验证了方法有效性。借助视觉条件化，系统在不同条件下完成了全部12圈尝试；而所有无视觉上下文的基线模型均持续失控，这证明了前瞻性动力学适应在高性能应用中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling autonomous systems to proactively adapt to rapidly changing environmental conditions, such as a vehicle anticipating road friction from visual cues, which is difficult for traditional or reactive adaptive models. The proposed method is a vision-conditioned variational Bayesian last-layer dynamics model that first learns nominal vehicle dynamics and then fine-tunes them using feature-wise affine transformations of latent features based on visual context, allowing for anticipatory dynamics prediction. Experimental validation on a Lexus LC500 racing through water puddles showed that the vision-conditioned system successfully completed all 12 laps, whereas all baseline methods without visual context consistently lost control, underscoring the critical role of proactive adaptation for safety and performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决机器人系统如何主动适应快速变化的环境条件（如车辆根据视觉线索预判路面摩擦）这一挑战，传统或反应式自适应模型在此方面存在困难。所提出的方法是一种视觉条件变分贝叶斯最后一层动力学模型，它首先学习标称车辆动力学，然后基于视觉上下文对潜在特征进行逐特征仿射变换以进行微调，从而实现上下文感知的预测。在一辆雷克萨斯LC500赛车通过积水坑道的实验验证中，视觉条件系统成功完成了全部12圈测试，而所有不具备视觉上下文的基线方法均持续失控，这凸显了主动适应对于安全与高性能应用的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Linqing Zhong, Yi Liu, Yifei Wei, Ziyu Xiong, Maoqing Yao, Si Liu, Guanghui Ren</div>
<div class="meta-line">First: 2026-01-16T16:17:06+00:00 · Latest: 2026-01-16T16:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11404v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11404v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ACoT-VLA：面向视觉-语言-动作模型的动作思维链</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型已成为多样化操作任务中重要的通用机器人策略，传统方法依赖通过视觉语言模型（VLM）嵌入将多模态输入直接转换为动作。近期研究引入了显式中间推理（如子任务预测（语言）或目标图像合成（视觉））来指导动作生成，但这些中间推理往往间接且固有地限制了对精确动作执行所需完整细粒度信息的传递。我们提出最有效的推理形式应直接在动作空间中进行推演，并引入动作思维链（ACoT）范式——将推理过程构建为引导最终策略的粗粒度动作意图结构化序列。本文提出实现ACoT范式的新型架构ACoT-VLA，具体包含两个互补组件：显式动作推理器（EAR）与隐式动作推理器（IAR）。前者通过粗粒度参考轨迹提供显式动作级推理步骤，后者从多模态输入的内部表征中提取潜在动作先验，共同构成ACoT以调节下游动作头，实现具身策略学习。在真实世界与仿真环境中的大量实验表明，该方法在LIBERO、LIBERO-Plus和VLABench基准上分别达到98.5%、84.1%和47.4%的性能，验证了其优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of current Vision-Language-Action (VLA) models, which often use indirect reasoning like language sub-tasks or goal images that fail to convey granular information for precise robot action execution. The proposed method, ACoT-VLA, introduces an Action Chain-of-Thought paradigm that reasons directly in the action space through two components: an Explicit Action Reasoner that generates coarse reference trajectories and an Implicit Action Reasoner that extracts latent action priors from multimodal inputs, together conditioning a downstream action head for grounded policy learning. Experimental results demonstrate the method&#x27;s superiority, achieving success rates of 98.5% on LIBERO, 84.1% on LIBERO-Plus, and 47.4% on VLABench.</div>
<div class="mono" style="margin-top:8px">本研究动机源于现有视觉-语言-动作（VLA）模型的局限性，这些模型通常依赖语言子任务或目标图像等间接中间推理，无法传递精确机器人动作执行所需的细粒度信息。为解决此问题，该方法提出了动作思维链（ACoT）范式，直接在动作空间中进行推理，并设计了ACoT-VLA架构，包含用于生成粗略参考轨迹的显式动作推理器和用于提取潜在动作先验的隐式动作推理器，以共同指导最终策略学习。关键实验结果表明该方法具有优越性，在LIBERO、LIBERO-Plus和VLABench基准测试中分别达到了98.5%、84.1%和47.4%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning</div>
<div class="meta-line">Authors: Henrik Hose, Paul Brunzema, Devdutt Subhasish, Sebastian Trimpe</div>
<div class="meta-line">First: 2026-01-16T16:06:32+00:00 · Latest: 2026-01-16T16:06:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11394v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11394v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of robust learning-based control algorithms for unstable systems requires high-quality, real-world data, yet access to specialized robotic hardware remains a significant barrier for many researchers. This paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source, quasi-symmetric balancing reaction wheel unicycle. The dataset provides 1 kHz synchronized data encompassing all onboard sensor readings, state estimates, ground-truth poses from a motion capture system, and third-person video logs. To ensure data diversity, we include experiments across multiple hardware instances and surfaces using various control paradigms, including pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents. We include several example applications in dynamics model learning, state estimation, and time-series classification to illustrate common robotics algorithms that can be benchmarked on our dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微型轮式机器人数据集：面向机器人学习的高保真数据</div>
<div class="mono" style="margin-top:8px">为不稳定系统开发鲁棒的基于学习的控制算法需要高质量的真实世界数据，然而获取专用机器人硬件仍是许多研究者的主要障碍。本文介绍了微型轮式机器人的综合动力学数据集，该机器人是一种开源、准对称的平衡反应轮单轮车。数据集提供1kHz同步数据，涵盖所有板载传感器读数、状态估计、来自运动捕捉系统的真实位姿，以及第三人称视频记录。为确保数据多样性，我们纳入了跨多个硬件实例和不同表面的实验，采用多种控制范式，包括伪随机二进制激励、非线性模型预测控制和强化学习智能体。我们提供了在动力学模型学习、状态估计和时间序列分类中的若干应用示例，以展示可在本数据集上基准测试的常见机器人算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity of high-quality real-world data for developing learning-based control algorithms on unstable robotic systems, this paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source balancing reaction wheel unicycle. The method involves collecting 1 kHz synchronized data from multiple hardware instances across different surfaces, using various control paradigms such as pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents, encompassing onboard sensors, state estimates, ground-truth motion capture poses, and third-person video. Key experimental findings demonstrate the dataset&#x27;s utility through example applications in dynamics model learning, state estimation, and time-series classification, providing a benchmark for common robotics algorithms.</div>
<div class="mono" style="margin-top:8px">针对不稳定机器人系统开发基于学习的控制算法时缺乏高质量真实世界数据的问题，本文为开源平衡反应轮独轮车Mini Wheelbot引入了一个全面的动力学数据集。该数据集以1 kHz频率采集，包含同步的板载传感器数据、状态估计、运动捕捉系统的真实位姿以及第三方视频日志，通过伪随机二进制激励、非线性模型预测控制和强化学习等多种控制范式，在多个硬件实例和不同表面上收集以确保数据多样性。示例应用展示了该数据集在动力学模型学习、状态估计和时间序列分类等常见机器人算法基准测试中的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization</div>
<div class="meta-line">Authors: Henrik Hose, Paul Brunzema, Alexander von Rohr, Alexander Gräfe, Angela P. Schoellig, Sebastian Trimpe</div>
<div class="meta-line">First: 2025-12-16T12:24:08+00:00 · Latest: 2026-01-16T14:52:20+00:00</div>
<div class="meta-line">Comments: Presented at the 13th International Conference on Robot Intelligence Technology and Applications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14350v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.14350v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Approximate model-predictive control (AMPC) aims to imitate an MPC&#x27;s behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC&#x27;s optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯优化的神经网络近似模型预测控制免重训练微调方法</div>
<div class="mono" style="margin-top:8px">近似模型预测控制（AMPC）旨在通过神经网络模拟MPC的行为，从而避免在运行时求解昂贵的优化问题。然而，在部署过程中，通常需要对底层MPC的参数进行微调，这往往导致AMPC不实用，因为它需要反复生成新数据集并重新训练神经网络。近期研究通过利用MPC优化问题的近似灵敏度实现AMPC的免重训练自适应，但目前该方法需手动操作，对于高维系统既费时又不够直观。为解决此问题，我们提出基于实验数据使用贝叶斯优化来调整AMPC策略参数。通过将基于模型的控制与直接局部学习相结合，我们的方法在硬件上以最少实验量实现了优于传统AMPC的性能。该方法能够自动且数据高效地使AMPC适配新系统实例，并针对难以直接嵌入MPC的成本函数进行微调。我们在倒立摆起摆控制和欠驱动平衡独轮机器人偏航控制这两个具有挑战性的控制问题上，通过硬件实验验证了所提方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the impracticality of fine-tuning approximate model-predictive control (AMPC) policies, which typically require costly dataset regeneration and neural network retraining when underlying MPC parameters change during deployment. The proposed method employs Bayesian optimization to automatically tune AMPC parameters directly from experimental data, combining model-based control with local learning to adapt policies without retraining. In hardware experiments, including an inverted cartpole swing-up and yaw control of an underactuated balancing unicycle robot, the approach demonstrated superior performance to nominal AMPC, enabling data-efficient adaptation to new system instances and cost functions difficult to implement directly in MPC.</div>
<div class="mono" style="margin-top:8px">本研究针对近似模型预测控制（AMPC）策略在部署中因底层MPC参数变化而需要重新生成数据集和训练神经网络，导致调优不切实际的问题。所提出的方法采用贝叶斯优化，直接从实验数据中自动调整AMPC参数，结合基于模型的控制与直接局部学习，实现了无需重新训练的策略自适应。在倒立摆起摆和欠驱动平衡独轮机器人偏航控制等硬件实验中，该方法表现出优于标准AMPC的性能，能够以较少的数据高效适应新系统实例及难以在MPC中直接实现的成本函数。</div>
</details>
</div>
<div class="card">
<div class="title">Distributed Control Barrier Functions for Safe Multi-Vehicle Navigation in Heterogeneous USV Fleets</div>
<div class="meta-line">Authors: Tyler Paine, Brendan Long, Jeremy Wenger, Michael DeFilippo, James Usevitch, Michael Benjamin</div>
<div class="meta-line">First: 2026-01-16T14:30:57+00:00 · Latest: 2026-01-16T14:30:57+00:00</div>
<div class="meta-line">Comments: 8 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11335v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11335v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Collision avoidance in heterogeneous fleets of uncrewed vessels is challenging because the decision-making processes and controllers often differ between platforms, and it is further complicated by the limitations on sharing trajectories and control values in real-time. This paper presents a pragmatic approach that addresses these issues by adding a control filter on each autonomous vehicle that assumes worst-case behavior from other contacts, including crewed vessels. This distributed safety control filter is developed using control barrier function (CBF) theory and the application is clearly described to ensure explainability of these safety-critical methods. This work compares the worst-case CBF approach with a Collision Regulations (COLREGS) behavior-based approach in simulated encounters. Real-world experiments with three different uncrewed vessels and a human operated vessel were performed to confirm the approach is effective across a range of platforms and is robust to uncooperative behavior from human operators. Results show that combining both CBF methods and COLREGS behaviors achieves the best safety and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异构无人艇编队安全多车导航的分布式控制屏障函数方法</div>
<div class="mono" style="margin-top:8px">异构无人艇编队的避碰具有挑战性，因为不同平台的决策过程与控制器存在差异，且实时共享轨迹与控制值的限制进一步增加了复杂度。本文提出一种实用方法，通过在每艘自主船舶上添加控制滤波器，假设其他接触目标（包括有人驾驶船舶）均采取最差行为。该分布式安全控制滤波器基于控制屏障函数理论开发，并通过清晰的应用描述确保这类安全关键方法的可解释性。研究在模拟遭遇场景中对比了最差情况CBF方法与基于《国际海上避碰规则》行为的方法。通过三艘不同无人艇与一艘人工操作船舶的实际实验，验证了该方法在多种平台上的有效性及对人类操作者不合作行为的鲁棒性。结果表明，结合CBF方法与COLREGS行为能实现最佳安全性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of safe navigation in heterogeneous uncrewed surface vehicle (USV) fleets, where differing decision-making processes, controllers, and real-time communication limitations complicate collision avoidance. The proposed method employs a distributed safety control filter based on control barrier function (CBF) theory, which is applied to each autonomous vehicle and assumes worst-case behavior from other contacts, including crewed vessels, to ensure safety. Experimental results from simulations and real-world tests with three different USVs and a human-operated vessel demonstrate that the worst-case CBF approach is effective across platforms and robust to uncooperative behavior, and that combining it with Collision Regulations (COLREGS) behavior-based methods yields the best performance in both safety and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决异构无人水面艇（USV）集群中安全导航的挑战，其中不同的决策过程、控制器以及实时通信限制使得避碰问题复杂化。所提出的方法采用基于控制屏障函数（CBF）理论的分布式安全控制滤波器，该滤波器应用于每艘自主船只，并假设其他接触目标（包括有人驾驶船只）采取最坏情况行为，从而确保安全。在模拟和真实世界（使用三艘不同的无人艇和一艘有人驾驶船只）进行的实验结果表明，这种最坏情况CBF方法在不同平台上均有效，且对人类操作员的不合作行为具有鲁棒性；同时，将其与《国际海上避碰规则》（COLREGS）行为策略相结合，能在安全性和导航效率方面实现最佳性能。</div>
</details>
</div>
<div class="card">
<div class="title">Skill-Aware Diffusion for Generalizable Robotic Manipulation</div>
<div class="meta-line">Authors: Aoshen Huang, Jiaming Chen, Jiyu Cheng, Ran Song, Wei Pan, Wei Zhang</div>
<div class="meta-line">First: 2026-01-16T13:14:40+00:00 · Latest: 2026-01-16T13:14:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11266v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11266v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/sa-diff">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>技能感知扩散模型在机器人操作泛化中的应用</div>
<div class="mono" style="margin-top:8px">机器人操作的鲁棒泛化能力对其灵活适应多样化环境至关重要。现有方法通常通过扩展数据和网络规模来提升泛化性能，但独立建模任务并忽略了技能层级信息。我们观察到相同技能下的任务具有相似的运动模式，因此提出技能感知扩散模型（SADiff），显式融合技能层级信息以增强泛化能力。SADiff通过可学习技能令牌的技能感知编码模块学习技能特定表征，并约束技能条件扩散模型生成以物体为中心的运动流。技能检索转换策略进一步利用技能特定的轨迹先验，优化从二维运动流到可执行三维动作的映射。此外，我们构建了IsaacSkill高保真数据集，包含基础机器人技能用于全面评估与仿真到现实的迁移。仿真与真实环境实验表明，SADiff在多种操作任务中均表现出优异的性能与泛化能力。代码、数据及视频详见：https://sites.google.com/view/sa-diff。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance robust generalization in robotic manipulation beyond methods that scale data and networks but treat tasks independently, this research proposes Skill-Aware Diffusion (SADiff). The method explicitly incorporates skill-level information by learning skill-specific representations via a skill-aware encoding module with learnable tokens and conditions a skill-constrained diffusion model to generate object-centric motion flow; a skill-retrieval strategy further refines the mapping to 3D actions. Evaluated on the introduced IsaacSkill dataset and in real-world settings, SADiff demonstrates strong performance and generalization across diverse manipulation tasks.</div>
<div class="mono" style="margin-top:8px">为了提升机器人操作的鲁棒泛化能力，避免仅依赖数据和模型规模的扩展，本研究针对现有方法忽视任务间共享技能级运动模式的问题，提出了技能感知扩散模型（SADiff）。该方法通过可学习的技能令牌学习技能特定表示，并以此条件约束扩散模型生成以物体为中心的运动流，再通过技能检索转换策略将其细化为可执行的3D动作。基于所构建的高保真IsaacSkill数据集的实验表明，SADiff在仿真和真实世界的多种操作任务中均实现了良好的性能和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">VLAgents: A Policy Server for Efficient VLA Inference</div>
<div class="meta-line">Authors: Tobias Jülg, Khaled Gamal, Nisarga Nilavadi, Pierre Krack, Seongjin Bien, Michael Krawez, Florian Walter, Wolfram Burgard</div>
<div class="meta-line">First: 2026-01-16T12:58:59+00:00 · Latest: 2026-01-16T12:58:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11250v1">PDF</a> · <a href="https://github.com/RobotControlStack/vlagents">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLAgents：高效视觉语言动作模型推理的策略服务器</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型的快速兴起对机器人领域产生了显著影响。然而，由于分布式部署中接口碎片化及固有的通信延迟，其实际应用仍面临复杂性。为此，我们提出了VLAgents——一个模块化策略服务器，通过统一的Gymnasium风格协议抽象VLA推理过程。其通信层核心创新在于能根据场景透明适配：既支持零拷贝共享内存以实现高速仿真，也支持压缩流传输以适配远程硬件。本研究详细阐述了VLAgents的架构，并通过集成七种策略（包括OpenVLA与Pi Zero）进行验证。在本地与远程通信的基准测试中，该系统进一步展现出优于OpenVLA、OpenPi及LeRobot默认策略服务器的性能。VLAgents已在https://github.com/RobotControlStack/vlagents开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The deployment of Vision-Language-Action models (VLAs) in robotics is hindered by fragmented interfaces and communication latency in distributed systems. To address this, the authors introduce VLAgents, a modular policy server that provides a unified Gymnasium-style API for VLA inference and features a communication layer that adaptively uses zero-copy shared memory for local simulation or compressed streaming for remote hardware. Experimental integration with seven policies, including OpenVLA and Pi Zero, demonstrates that VLAgents outperforms the default policy servers of OpenVLA, OpenPi, and LeRobot in benchmarks involving both local and remote communication.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型在机器人领域的部署因分布式系统中接口碎片化和通信延迟而受阻。为此，研究者提出了VLAgents，这是一个模块化的策略服务器，为VLA推理提供了统一的Gymnasium风格接口，其通信层能自适应地使用零拷贝共享内存进行本地高速模拟，或使用压缩流进行远程硬件通信。通过集成包括OpenVLA和Pi Zero在内的七种策略进行实验验证，结果表明在本地与远程通信的基准测试中，VLAgents的性能优于OpenVLA、OpenPi和LeRobot提供的默认策略服务器。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</div>
<div class="meta-line">Authors: Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, Younggyo Seo</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-29T16:41:12+00:00 · Latest: 2026-01-16T12:54:08+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00070v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00070v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robot-R1：基于强化学习的机器人具身推理增强框架</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）近期通过结合具身推理与机器人控制，在推动机器人技术发展方面展现出巨大潜力。当前主流方法采用监督微调（SFT）对机器人控制相关的具身推理任务进行训练，但SFT数据集常依赖启发式构建，未针对机器人控制优化，且易导致灾难性遗忘与泛化性能下降。为突破这些局限，我们提出Robot-R1——一种利用强化学习专门增强机器人控制具身推理能力的新型框架。该框架基于专家示范的当前场景图像与环境元数据，学习预测完成任务所需的下一个关键点状态。受DeepSeek-R1学习方法启发，Robot-R1对基于推理的响应进行采样，并强化那些能产生更准确预测的响应。为系统评估Robot-R1，我们同时构建了需要多样化具身推理能力的新基准测试。实验表明，采用Robot-R1训练的模型在具身推理任务上全面超越SFT方法。尽管仅拥有70亿参数，Robot-R1在空间与运动推理等底层动作控制相关任务上甚至优于GPT-4o。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in current approaches that use Supervised Fine-Tuning (SFT) for training Large Vision-Language Models on embodied reasoning tasks for robot control, noting that SFT datasets are often heuristically constructed and can lead to catastrophic forgetting and reduced generalization. The proposed method, Robot-R1, introduces a reinforcement learning framework that learns to predict the next keypoint state for task completion, conditioned on the current scene image and environment metadata from expert demonstrations; it samples reasoning-based responses and reinforces those leading to more accurate predictions, inspired by the DeepSeek-R1 approach. Experimental results on a new benchmark requiring diverse embodied reasoning capabilities show that Robot-R1 outperforms SFT methods, and despite having only 7B parameters, it surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div>
<div class="mono" style="margin-top:8px">本研究针对使用监督微调（SFT）训练大型视觉语言模型（LVLM）进行机器人具身推理任务时存在的局限性，即SFT数据集通常是启发式构建的，且可能导致灾难性遗忘和泛化性能下降。提出的Robot-R1框架采用强化学习来增强具身推理能力，该方法基于专家演示的当前场景图像和环境元数据，学习预测完成任务所需的下一关键点状态；它采样基于推理的响应，并强化那些能带来更准确预测的响应。在一个需要多样化具身推理能力的新基准上的实验结果表明，Robot-R1的性能优于SFT方法，并且尽管仅有70亿参数，它在空间和运动推理等低层动作控制推理任务上甚至超越了GPT-4o。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities</div>
<div class="meta-line">Authors: Yiyun Zhou, Mingjing Xu, Jingwei Shi, Quanjiang Li, Jingyuan Chen</div>
<div class="meta-line">First: 2025-11-14T17:34:20+00:00 · Latest: 2026-01-16T12:49:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11512v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.11512v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉、语言与视觉模态对齐的协同表征学习方法</div>
<div class="mono" style="margin-top:8px">触觉感知为视觉和语言提供了丰富且互补的信息，使机器人能够感知细粒度的物体属性。然而，现有触觉传感器缺乏标准化，导致冗余特征阻碍跨传感器泛化。此外，现有方法未能充分整合触觉、语言与视觉模态间的中间交互。为此，我们提出TLV-CoRe，一种基于CLIP的触觉-语言-视觉协同表征学习方法。TLV-CoRe引入传感器感知调制器以统一不同传感器的触觉特征，并采用触觉无关解耦学习来分离无关触觉特征。同时，引入统一桥接适配器以增强共享表征空间内的三模态交互。为公平评估触觉模型性能，我们进一步提出RSS评估框架，聚焦于不同方法的鲁棒性、协同性与稳定性。实验结果表明，TLV-CoRe显著提升了传感器无关表征学习与跨模态对齐能力，为多模态触觉表征提供了新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to integrate tactile sensing with vision and language for robotic perception, addressing issues of sensor non-standardization and insufficient cross-modal interaction. The method introduces TLV-CoRe, a CLIP-based framework that employs a Sensor-Aware Modulator to unify tactile features across sensors, tactile-irrelevant decoupled learning to remove irrelevant features, and a Unified Bridging Adapter to enhance tri-modal interaction in a shared space. Experiments using the proposed RSS evaluation framework show that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, demonstrating robustness, synergy, and stability.</div>
<div class="mono" style="margin-top:8px">为解决触觉传感器缺乏标准化以及触觉、语言和视觉模态间交互不足的问题，本文提出了TLV-CoRe，一种基于CLIP的协作表征学习方法。该方法采用传感器感知调制器来统一不同传感器的触觉特征，利用触觉无关解耦学习过滤无关特征，并引入统一桥接适配器以增强三模态在共享表征空间中的交互。在提出的关注鲁棒性、协同性和稳定性的RSS评估框架下，实验结果表明该方法显著提升了传感器无关的表征学习和跨模态对齐性能。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Monitoring of Stochastic Fire Front Processes via Information-seeking Predictive Control</div>
<div class="meta-line">Authors: Savvas Papaioannou, Panayiotis Kolios, Christos G. Panayiotou, Marios M. Polycarpou</div>
<div class="meta-line">First: 2026-01-16T12:21:27+00:00 · Latest: 2026-01-16T12:21:27+00:00</div>
<div class="meta-line">Comments: 2025 IEEE 64th Conference on Decision and Control (CDC)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11231v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11231v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of adaptively monitoring a wildfire front using a mobile agent (e.g., a drone), whose trajectory determines where sensor data is collected and thus influences the accuracy of fire propagation estimation. This is a challenging problem, as the stochastic nature of wildfire evolution requires the seamless integration of sensing, estimation, and control, often treated separately in existing methods. State-of-the-art methods either impose linear-Gaussian assumptions to establish optimality or rely on approximations and heuristics, often without providing explicit performance guarantees. To address these limitations, we formulate the fire front monitoring task as a stochastic optimal control problem that integrates sensing, estimation, and control. We derive an optimal recursive Bayesian estimator for a class of stochastic nonlinear elliptical-growth fire front models. Subsequently, we transform the resulting nonlinear stochastic control problem into a finite-horizon Markov decision process and design an information-seeking predictive control law obtained via a lower confidence bound-based adaptive search algorithm with asymptotic convergence to the optimal policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信息寻求预测控制的随机火锋过程自适应监测</div>
<div class="mono" style="margin-top:8px">本文研究利用移动智能体（如无人机）自适应监测野火锋线的问题，其飞行轨迹决定传感器数据采集位置，进而影响火势传播估计精度。由于野火演化的随机性需要将感知、估计与控制无缝集成——现有方法常将三者割裂处理，该问题极具挑战性。当前先进方法或依赖线性高斯假设以建立最优性，或采用近似启发式方法，通常缺乏明确的性能保证。为突破这些局限，我们将火锋监测任务构建为集成感知、估计与控制的随机最优控制问题。针对一类随机非线性椭圆扩展火锋模型，推导出最优递归贝叶斯估计器。随后将所得非线性随机控制问题转化为有限时域马尔可夫决策过程，并通过基于置信下界的自适应搜索算法设计信息寻求预测控制律，该算法具有渐近收敛至最优策略的特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adaptively monitoring stochastic wildfire fronts using mobile agents, where existing methods often treat sensing, estimation, and control separately or rely on restrictive assumptions without performance guarantees. The authors formulate the problem as a stochastic optimal control framework, deriving an optimal recursive Bayesian estimator for nonlinear elliptical-growth fire models and transforming it into a finite-horizon Markov decision process. Their solution implements an information-seeking predictive control law via a lower confidence bound-based adaptive search algorithm, which is proven to converge asymptotically to the optimal policy.</div>
<div class="mono" style="margin-top:8px">本研究旨在利用无人机等移动代理增强对随机野火火线的自适应监测，解决现有方法通常将感知、估计和控制分开处理、依赖限制性假设或启发式方法且缺乏性能保证的挑战。该方法将监测任务表述为随机最优控制问题，针对一类随机非线性椭圆增长火线模型推导了最优递归贝叶斯估计器，并将控制问题转化为有限时域马尔可夫决策过程，通过基于置信下界的信息寻求预测控制律求解，该算法具有渐近收敛至最优策略的特性。</div>
</details>
</div>
<div class="card">
<div class="title">SceneFoundry: Generating Interactive Infinite 3D Worlds</div>
<div class="meta-line">Authors: ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang</div>
<div class="meta-line">First: 2026-01-09T14:33:10+00:00 · Latest: 2026-01-16T11:20:40+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05810v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05810v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://anc891203.github.io/SceneFoundry-Demo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research. project page: https://anc891203.github.io/SceneFoundry-Demo/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SceneFoundry：生成交互式无限三维世界</div>
<div class="mono" style="margin-top:8px">自动生成大规模、交互式且物理真实的三维环境能力对推进机器人学习与具身智能至关重要。然而，现有生成方法常难以捕捉真实室内环境的功能复杂性，尤其缺乏对操控与导航至关重要的含可动部件的关节式物体的建模。本文提出SceneFoundry——一种语言引导的扩散框架，可为机器人训练生成公寓级三维世界，其中包含功能可动的家具与语义多样的布局。通过自然语言指令，大语言模型模块控制平面布局生成，而基于扩散的后验采样则高效地从大规模三维资源库中选取关节化资产填充场景。为确保物理可用性，SceneFoundry采用可微分引导函数来调控物体数量、防止关节碰撞，并为机器人导航维持充足的可通行空间。大量实验表明，该框架能在多样场景类型与条件下生成结构有效、语义连贯且功能可交互的环境，为可扩展的具身人工智能研究提供支持。项目页面：https://anc891203.github.io/SceneFoundry-Demo/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for automatically generating large-scale, interactive, and physically realistic 3D environments to advance robotic learning, as existing methods often fail to capture the functional complexity of interiors with articulated objects. The method, SceneFoundry, is a language-guided diffusion framework that uses an LLM module to control floor layout from natural language prompts and diffusion-based posterior sampling to populate scenes with articulated assets from 3D repositories, employing differentiable guidance functions to ensure physical usability by regulating object quantity, preventing collisions, and maintaining walkable space. Experimental results demonstrate that the framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types, enabling scalable embodied AI research.</div>
<div class="mono" style="margin-top:8px">该研究的动机是需要自动生成大规模、交互式且物理逼真的3D环境，以推动机器人学习和具身智能的发展，解决现有方法在捕捉功能复杂性（特别是涉及可动部件物体）方面的不足。所提出的方法SceneFoundry是一个语言引导的扩散框架，它利用大语言模型模块根据自然语言提示控制平面布局生成，并通过基于扩散的后验采样从大规模3D资源库中填充带有可动部件的资产，同时采用可微引导函数来确保物理可用性，包括调控物体数量、防止关节碰撞以及保持足够的机器人可通行空间。实验结果表明，该框架能够生成结构有效、语义连贯且功能交互的环境，适用于多种场景类型和条件，为可扩展的具身人工智能研究提供了支持。</div>
</details>
</div>
<div class="card">
<div class="title">LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</div>
<div class="meta-line">Authors: Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro</div>
<div class="meta-line">First: 2025-12-22T17:00:25+00:00 · Latest: 2026-01-16T10:19:54+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication. 55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository under https://github.com/kdjebko/lelar-in-orbit-data</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19576v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.19576v3">PDF</a> · <a href="https://github.com/kdjebko/lelar-in-orbit-data">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LeLaR：首个基于人工智能的卫星姿态控制器在轨验证</div>
<div class="mono" style="margin-top:8px">姿态控制对多数卫星任务至关重要。然而，传统控制器设计耗时，且对模型不确定性与运行边界条件变化敏感。深度强化学习通过与仿真环境自主交互学习自适应控制策略，提供了有前景的替代方案。克服仿真到现实的差距——即将仿真训练的智能体部署至真实物理卫星——仍是重大挑战。本研究首次成功展示了基于人工智能的姿态控制器在惯性指向机动中的在轨验证。该控制器完全在仿真环境中训练，并部署至由维尔茨堡大学与柏林工业大学合作研制、于2025年1月发射的InnoCube 3U纳卫星。我们介绍了人工智能智能体设计、训练流程方法、仿真与真实卫星观测行为间的差异，以及基于人工智能的姿态控制器与InnoCube传统PD控制器的对比。稳态指标证实了基于人工智能的控制器在重复在轨机动中的鲁棒性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of classical satellite attitude controllers, which are labor-intensive to design and vulnerable to model inaccuracies and changing operational conditions, by developing an adaptive AI-based alternative using Deep Reinforcement Learning (DRL). The method involves training a DRL agent entirely in simulation and then deploying it directly onto the InnoCube 3U nanosatellite for in-orbit testing, tackling the significant Sim2Real transfer challenge. Experimental results from the first successful in-orbit demonstration show that the AI controller achieved robust performance during inertial pointing maneuvers, with steady-state metrics confirming its effectiveness in comparison to the satellite&#x27;s classical PD controller.</div>
<div class="mono" style="margin-top:8px">本研究针对经典卫星姿态控制器设计耗时、且对模型不确定性和运行条件变化敏感的问题，提出了一种基于深度强化学习的解决方案。该方法通过在仿真环境中完全训练自适应控制器，并直接部署到真实卫星上，以应对仿真到现实的迁移挑战。该AI控制器在InnoCube 3U纳卫星上成功完成了首次在轨演示，实验结果表明，在重复的惯性指向机动中，基于AI的控制器表现出稳健的性能，其稳态指标得到确认，并与卫星原有的经典PD控制器进行了对比。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model</div>
<div class="meta-line">Authors: Minho Lee, Hyeonseok Kim, Jin Tak Kim, Sangshin Park, Jeong Hyun Lee, Jungsan Cho, Jemin Hwangbo</div>
<div class="meta-line">Venue: IEEE Robotics and Automation Letters (Volume: 10, Issue: 12, December 2025)</div>
<div class="meta-line">First: 2026-01-16T10:01:09+00:00 · Latest: 2026-01-16T10:01:09+00:00</div>
<div class="meta-line">Comments: 9 pages, Accepted to IEEE Robotics and Automation Letters (RA-L) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The simulation-to-reality (sim-to-real) transfer of large-scale hydraulic robots presents a significant challenge in robotics because of the inherent slow control response and complex fluid dynamics. The complex dynamics result from the multiple interconnected cylinder structure and the difference in fluid rates of the cylinders. These characteristics complicate detailed simulation for all joints, making it unsuitable for reinforcement learning (RL) applications. In this work, we propose an analytical actuator model driven by hydraulic dynamics to represent the complicated actuators. The model predicts joint torques for all 12 actuators in under 1 microsecond, allowing rapid processing in RL environments. We compare our model with neural network-based actuator models and demonstrate the advantages of our model in data-limited scenarios. The locomotion policy trained in RL with our model is deployed on a hydraulic quadruped robot, which is over 300 kg. This work is the first demonstration of a successful transfer of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot, demonstrating advanced sim-to-real transferability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于执行器模型的重型液压机器人四足运动学习</div>
<div class="mono" style="margin-top:8px">大型液压机器人的仿真到现实迁移因固有的控制响应迟缓和复杂流体动力学而成为机器人学中的重大挑战。这种复杂性源于多缸互联结构及各缸流体速率的差异，使得对所有关节进行精细仿真变得困难，不适用于强化学习应用。本研究提出一种基于液压动力学的解析执行器模型来表征复杂执行器。该模型能在1微秒内预测全部12个执行器的关节扭矩，满足强化学习环境的快速处理需求。与基于神经网络的执行器模型对比，本模型在数据有限场景中展现出优势。采用该模型训练的强化学习运动策略已部署于自重超300公斤的液压四足机器人。本研究首次在重型液压四足机器人上实现了基于强化学习的稳定鲁棒指令跟踪运动迁移，展现了先进的仿真到现实迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of simulation-to-reality transfer for large, heavy hydraulic quadruped robots, where complex fluid dynamics and slow control responses hinder accurate simulation and reinforcement learning. The method introduces an analytical actuator model based on hydraulic dynamics to efficiently predict joint torques for all 12 actuators in under a microsecond, enabling rapid policy training. Experimental results show this model outperforms neural network alternatives in data-limited settings, and the trained locomotion policy was successfully deployed on a 300 kg hydraulic robot, marking the first demonstration of stable, command-tracking RL locomotion on such a heavy platform.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将强化学习应用于大型液压机器人的挑战，其复杂的流体动力学和缓慢的控制响应阻碍了精确仿真和仿真到现实的迁移。作者提出了一种基于液压动力学的解析执行器模型，能在微秒内高效预测所有12个执行器的关节扭矩，从而为强化学习训练提供快速仿真。实验结果表明，该模型在数据有限的情况下优于基于神经网络的替代方案，且由此训练出的运动策略成功部署在一台300公斤以上的液压四足机器人上，首次实现了在此类重型机器人上稳定、可跟踪指令的强化学习运动控制。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments</div>
<div class="meta-line">Authors: Jiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi</div>
<div class="meta-line">First: 2026-01-16T08:24:23+00:00 · Latest: 2026-01-16T08:24:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11078v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. We present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity. Using onboard camera sensors (RGB for marker detection and depth for obstacle avoidance), we benchmark two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness. Results underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多样化城市环境中自主无人机着陆的视觉标记搜索</div>
<div class="mono" style="margin-top:8px">基于标记的着陆因其简单可靠，在无人机配送与返航系统中广泛应用。然而，多数方法假设理想化的着陆点可见性与传感器性能，限制了其在复杂城市环境中的鲁棒性。我们在AirSim平台上构建了一套基于仿真的评估系统，通过系统化调整城市布局、光照与天气条件，以模拟真实场景的多样性。利用机载摄像头传感器（RGB用于标记检测，深度用于避障），我们对比了两种启发式覆盖模式与一个基于强化学习的智能体，分析了探索策略与场景复杂度对成功率、路径效率及鲁棒性的影响。结果强调，需在多样化且与传感器相关的条件下评估基于标记的自主着陆，以指导可靠空中导航系统的开发。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing marker-based drone landing systems, which often assume ideal conditions and thus lack robustness in complex urban environments. The authors develop a simulation-based evaluation suite using the AirSim platform to systematically vary urban layouts, lighting, and weather, and they benchmark two heuristic coverage patterns and a reinforcement learning agent using onboard RGB and depth cameras for marker detection and obstacle avoidance. Experimental results highlight how exploration strategy and scene complexity affect success rate and path efficiency, demonstrating the necessity of testing under diverse, sensor-relevant conditions to develop reliable autonomous landing systems.</div>
<div class="mono" style="margin-top:8px">本研究针对现有基于标记的无人机着陆系统在理想化假设下缺乏复杂城市环境鲁棒性的问题，提出了一种基于AirSim平台的仿真评估套件，通过系统变化城市布局、光照和天气来模拟真实操作多样性，并利用机载RGB和深度传感器进行标记检测与避障，对比了两种启发式覆盖模式和一种基于强化学习的智能体。实验结果揭示了探索策略和场景复杂度对成功率、路径效率和鲁棒性的影响，强调了在多样化、与传感器相关的条件下评估自主着陆性能对于开发可靠空中导航系统的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation</div>
<div class="meta-line">Authors: Jiaqi Liang, Yue Chen, Qize Yu, Yan Shen, Haipeng Zhang, Hao Dong, Ruihai Wu</div>
<div class="meta-line">First: 2026-01-16T08:21:42+00:00 · Latest: 2026-01-16T08:21:42+00:00</div>
<div class="meta-line">Comments: AAAI2026 oral</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11076v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11076v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries. We propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions. We establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A3D：基于双臂操作的适应性功能组装框架</div>
<div class="mono" style="margin-top:8px">家具组装对机器人而言是关键且具挑战性的任务，需要精确的双臂协调：一臂操作部件，另一臂提供协作支撑与稳定。为实现更高效的组装，机器人需在长时序组装过程中主动调整支撑策略，并适应不同部件几何形态。本文提出A3D框架，通过学习适应性功能表征以识别家具部件上的最优支撑与稳定位置。该方法采用密集点云级几何表征建模部件交互模式，实现跨几何形态的泛化能力。为应对动态组装状态，我们引入自适应模块，利用交互反馈基于历史操作动态调整组装过程中的支撑策略。我们构建了包含8类家具、50种异质部件的仿真环境，专为双臂协作评估设计。实验表明，该框架在仿真与真实场景中均能有效泛化至不同部件几何形态与家具类别。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of furniture assembly for robots, which requires precise dual-arm coordination where one arm manipulates parts while the other provides adaptive support and stabilization. The proposed A3D framework learns adaptive affordances to identify optimal support locations on parts, using dense point-level geometric representations to model interaction patterns for generalization across varied geometries, and incorporates an adaptive module that dynamically adjusts support strategies based on interaction feedback during the assembly process. Experimental evaluation in a simulation environment with 50 diverse parts across 8 furniture types demonstrates effective generalization to diverse part geometries and furniture categories, with successful transfer to real-world settings.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人执行家具组装任务时面临的挑战，该任务需要精确的双臂协调，其中一只手臂操作部件，另一只手臂提供自适应支撑和稳定。提出的A3D框架通过学习自适应可供性，利用密集点级几何表示来识别家具部件上的最佳支撑位置，从而能够泛化到不同的部件几何形状，并包含一个自适应模块，可根据交互反馈在组装过程中动态调整支撑策略。在包含8种家具类型、50个不同部件的仿真环境中进行的实验，以及在真实世界中的验证表明，该框架能有效泛化到不同的部件几何形状和家具类别。</div>
</details>
</div>
<div class="card">
<div class="title">H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning</div>
<div class="meta-line">Authors: Haishan Zeng, Peng Li</div>
<div class="meta-line">First: 2026-01-16T07:59:50+00:00 · Latest: 2026-01-16T07:59:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11063v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In embodied artificial intelligence, enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions remains a critical challenge. While large language models (LLMs) show promise in instruction parsing and preliminary planning, they exhibit limitations in long-term reasoning and dynamic multi-robot coordination. We propose Hierarchical Autonomous Intelligent Multi-Robot Planning(H-AIM), a novel embodied multi-robot task planning framework that addresses these issues through a three-stage cascaded architecture: 1) It leverages an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, thereby transforming commands into formal planning problems; 2) It combines the semantic reasoning of LLMs with the search capabilities of a classical planner to produce optimized action sequences; 3) It compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization. To validate our approach, we introduce the MACE-THOR benchmark dataset, comprising 42 complex tasks across 8 distinct household layouts. Experimental results demonstrate that H-AIM achieves a remarkable performance improvement, elevating the task success rate from 12% to 55% and boosting the goal condition recall from 32% to 72% against the strongest baseline, LaMMA-P.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>H-AIM：基于大语言模型、PDDL与行为树的分层多机器人规划框架</div>
<div class="mono" style="margin-top:8px">在具身人工智能领域，如何使异构机器人团队根据高层指令执行长时程任务仍是关键挑战。大语言模型在指令解析与初步规划中展现出潜力，但在长期推理与动态多机器人协同方面存在局限。本文提出分层自主智能多机器人规划框架H-AIM，通过三级级联架构解决上述问题：1）利用大语言模型解析指令并生成规划领域定义语言问题描述，将自然语言指令转化为形式化规划问题；2）结合大语言模型的语义推理能力与经典规划器的搜索能力，生成优化行动序列；3）将规划结果编译为行为树以实现反应式控制。该框架通过共享黑板机制支持动态规模的异构机器人团队通信与状态同步。为验证方法有效性，我们构建了MACE-THOR基准数据集，涵盖8种不同家居布局中的42项复杂任务。实验结果表明，相较于最强基线LaMMA-P，H-AIM将任务成功率从12%提升至55%，目标条件召回率从32%提高至72%，实现了显著性能突破。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions in embodied AI, where large language models (LLMs) alone struggle with long-term reasoning and multi-robot coordination. The proposed H-AIM framework employs a three-stage cascaded architecture: it first uses an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, then combines LLM semantic reasoning with a classical planner to produce optimized action sequences, and finally compiles plans into behavior trees for reactive control, with a shared blackboard for team communication. Evaluated on the MACE-THOR benchmark of 42 complex household tasks, H-AIM significantly outperforms the strongest baseline, LaMMA-P, by increasing the task success rate from 12% to 55% and goal condition recall from 32% to 72%.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身人工智能中异构机器人团队执行高层指令的长期任务规划难题，其中大语言模型（LLM）在长期推理和多机器人协调方面存在局限。提出的H-AIM框架采用三级级联架构：首先利用LLM解析指令并生成规划领域定义语言（PDDL）问题描述；然后结合LLM语义推理与经典规划器生成优化动作序列；最后将计划编译为行为树以实现反应式控制，并通过共享黑板机制支持动态异构团队。在新构建的MACE-THOR基准数据集（包含8种不同家庭布局中的42项复杂任务）上的实验表明，H-AIM相比最强基线LaMMA-P显著提升了性能，将任务成功率从12%提高到55%，目标条件召回率从32%提升至72%。</div>
</details>
</div>
<div class="card">
<div class="title">Haptic Light-Emitting Diodes: Miniature, Luminous Tactile Actuators</div>
<div class="meta-line">Authors: Max Linnander, Yon Visell</div>
<div class="meta-line">First: 2026-01-16T07:18:18+00:00 · Latest: 2026-01-16T07:18:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11043v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Haptic Light-Emitting Diodes (HLEDs), luminous thermopneumatic actuators that directly convert pulsed light into mechanical forces and displacements. Each device packages a miniature surface-mount LED in a gas-filled cavity that contains a low-inertia graphite photoabsorber. The cavity is sealed by an elastic membrane, which functions as a working diaphragm. Brief optical pulses heat the photoabsorber, which heats the gas. The resulting rapid pressure increases generate forces and displacements at the working diaphragm. Millimeter-scale HLEDs produce forces exceeding 0.4 N and displacements of 1 mm at low voltages, with 5 to 100 ms response times, making them attractive as actuators providing tactile feedback in human-machine interfaces. Perceptual testing revealed that the strength of tactile feedback increased linearly with optical power. HLEDs devices are mechanically simple and efficient to fabricate. Unusually, these actuators are also light-emitting, as a fraction of optical energy is transmitted through the membrane. These opto-mechanical actuators have many potential applications in tactile displays, human interface engineering, wearable computing, and other areas.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉发光二极管：微型发光触觉致动器</div>
<div class="mono" style="margin-top:8px">我们提出触觉发光二极管（HLEDs），这是一种发光热致动器，能将脉冲光直接转换为机械力和位移。每个器件将微型表面贴装LED封装在充满气体的腔体内，腔体包含低惯性石墨光吸收体，并由弹性膜密封作为工作膜片。短暂的光脉冲加热光吸收体，进而加热气体，产生的快速压力升高在工作膜片上形成力和位移。毫米级HLEDs在低电压下可产生超过0.4 N的力和1 mm位移，响应时间为5至100毫秒，使其成为人机界面中提供触觉反馈的理想致动器。感知测试表明触觉反馈强度随光功率线性增加。HLEDs结构简单、制造高效，且兼具发光特性——部分光能可透过膜片传输。这类光机械致动器在触觉显示器、人机界面工程、可穿戴计算等领域具有广泛应用前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research introduces Haptic Light-Emitting Diodes (HLEDs), aiming to create compact, luminous actuators for tactile feedback in human-machine interfaces. The method involves a miniature surface-mount LED enclosed in a gas-filled cavity with a graphite photoabsorber and sealed by an elastic diaphragm; pulsed light heats the absorber, rapidly increasing gas pressure to drive the diaphragm. Experimental results show that millimeter-scale HLEDs generate forces over 0.4 N and displacements of 1 mm with 5 to 100 ms response times at low voltages, and perceptual tests indicate tactile feedback strength increases linearly with optical power, while the devices also emit light through the membrane.</div>
<div class="mono" style="margin-top:8px">本研究提出触觉发光二极管（HLEDs），旨在为人类-机器界面开发紧凑、发光的触觉致动器。其方法是将微型LED封装在充满气体的腔体中，腔内含有石墨光吸收剂和弹性膜；光脉冲加热吸收剂，使气体压力迅速升高从而驱动膜片运动。主要实验结果表明，毫米级HLEDs在低电压下可产生超过0.4 N的力和1 mm的位移，响应时间为5至100毫秒，感知测试显示触觉反馈强度随光功率线性增加。</div>
</details>
</div>
<div class="card">
<div class="title">Crane Lowering Guidance Using a Attachable Camera Module for Driver Vision Support</div>
<div class="meta-line">Authors: HyoJae Kang, SunWoo Ahn, InGyu Choi, GeonYeong Go, KunWoo Son, Min-Sung Kang</div>
<div class="meta-line">First: 2026-01-16T06:44:17+00:00 · Latest: 2026-01-16T06:44:17+00:00</div>
<div class="meta-line">Comments: Presented at ICCR 2025(International COnference on Control and Robotics 2025). Submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11026v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11026v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cranes have long been essential equipment for lifting and placing heavy loads in construction projects. This study focuses on the lowering phase of crane operation, the stage in which the load is moved to the desired location. During this phase, a constant challenge exists: the load obstructs the operator&#x27;s view of the landing point. As a result, operators traditionally have to rely on verbal or gestural instructions from ground personnel, which significantly impacts site safety. To alleviate this constraint, the proposed system incorporates a attachable camera module designed to be attached directly to the load via a suction cup. This module houses a single-board computer, battery, and compact camera. After installation, it streams and processes images of the ground directly below the load in real time to generate installation guidance. Simultaneously, this guidance is transmitted to and monitored by a host computer. Preliminary experiments were conducted by attaching this module to a test object, confirming the feasibility of real-time image acquisition and transmission. This approach has the potential to significantly improve safety on construction sites by providing crane operators with an instant visual reference of hidden landing zones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>采用可附加摄像头模块的起重机下降引导系统以支持驾驶员视野</div>
<div class="mono" style="margin-top:8px">起重机长期以来是建筑项目中吊装和放置重物的关键设备。本研究聚焦于起重机操作的下降阶段，即载荷移至目标位置的过程。在此阶段，操作员常面临载荷遮挡着陆点视野的持续挑战，传统上需依赖地面人员的口头或手势指令，严重影响工地安全。为缓解此限制，本研究提出一种可附加摄像头模块系统，通过吸盘直接安装在载荷上。该模块集成单板计算机、电池及紧凑型摄像头，安装后可实时采集并处理载荷正下方地面图像以生成安装引导信息，同时将引导数据传输至主机进行监控。初步实验通过将模块附加至测试物体，验证了实时图像采集与传输的可行性。该方法有望通过为起重机操作员提供隐蔽着陆区的即时视觉参考，显著提升建筑工地的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the safety challenge in crane operations where the load obstructs the operator&#x27;s view of the landing point during the lowering phase, forcing reliance on ground personnel. The proposed method uses an attachable camera module with a suction cup to mount on the load; it contains a single-board computer, battery, and camera to stream and process real-time images of the ground below, generating guidance displayed on a host computer. Preliminary experiments with the module attached to a test object confirmed the feasibility of real-time image acquisition and transmission, indicating potential to improve site safety by providing operators a direct visual reference of hidden landing zones.</div>
<div class="mono" style="margin-top:8px">本研究针对起重机操作中，负载下降阶段遮挡操作员视线、传统上需依赖地面人员指挥的安全挑战。提出的方法采用一个带吸盘的可附着摄像头模块，安装在负载上，利用紧凑型摄像头和单板计算机实时流式传输并处理下方地面的图像以生成安装引导，并在主机上显示。初步实验将该模块附着于测试物体，证实了实时图像采集与传输的可行性，表明其有潜力通过为操作员提供隐藏着陆区的视觉参考来提升工地安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Off Policy Lyapunov Stability in Reinforcement Learning</div>
<div class="meta-line">Authors: Sarvan Gill, Daniela Constantinescu</div>
<div class="meta-line">Venue: CORL</div>
<div class="meta-line">First: 2025-09-11T21:34:08+00:00 · Latest: 2026-01-16T02:02:30+00:00</div>
<div class="meta-line">Comments: Conference on Robot Learning (CORL) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09863v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.09863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional reinforcement learning lacks the ability to provide stability guarantees. More recent algorithms learn Lyapunov functions alongside the control policies to ensure stable learning. However, the current self-learned Lyapunov functions are sample inefficient due to their on-policy nature. This paper introduces a method for learning Lyapunov functions off-policy and incorporates the proposed off-policy Lyapunov function into the Soft Actor Critic and Proximal Policy Optimization algorithms to provide them with a data efficient stability certificate. Simulations of an inverted pendulum and a quadrotor illustrate the improved performance of the two algorithms when endowed with the proposed off-policy Lyapunov function.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中的离策略李雅普诺夫稳定性</div>
<div class="mono" style="margin-top:8px">传统强化学习缺乏稳定性保证能力。较新的算法在学习控制策略的同时学习李雅普诺夫函数以确保稳定学习，但当前自学习的李雅普诺夫函数因其同策略特性而样本效率低下。本文提出一种离策略学习李雅普诺夫函数的方法，并将所提出的离策略李雅普诺夫函数融入软演员评论家算法与近端策略优化算法，为二者提供数据高效的稳定性证明。通过倒立摆和四旋翼飞行器的仿真实验表明，两种算法在引入所提出的离策略李雅普诺夫函数后性能均得到提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of stability guarantees in traditional reinforcement learning and the sample inefficiency of on-policy methods for learning Lyapunov functions, this paper proposes an off-policy method for learning Lyapunov functions. The approach integrates the learned off-policy Lyapunov function into the Soft Actor-Critic and Proximal Policy Optimization algorithms to provide a data-efficient stability certificate. Experimental simulations on an inverted pendulum and a quadrotor demonstrate that the enhanced algorithms achieve improved performance when equipped with the proposed stability mechanism.</div>
<div class="mono" style="margin-top:8px">针对传统强化学习缺乏稳定性保证，以及现有基于策略的李雅普诺夫函数学习方法样本效率低的问题，本文提出了一种离策略的李雅普诺夫函数学习方法。该方法将学习到的李雅普诺夫函数集成到Soft Actor-Critic和Proximal Policy Optimization算法中，以提供数据高效的稳定性证明。在倒立摆和四旋翼飞行器上的仿真实验表明，采用所提出的离策略李雅普诺夫函数后，这两种算法的性能均得到了提升。</div>
</details>
</div>
<div class="card">
<div class="title">EqVIO: An Equivariant Filter for Visual Inertial Odometry</div>
<div class="meta-line">Authors: Pieter van Goor, Robert Mahony</div>
<div class="meta-line">Venue: IEEE Transactions on Robotics, vol. 39, no. 5, pp. 3567-3585, Oct. 2023</div>
<div class="meta-line">First: 2022-05-04T10:14:54+00:00 · Latest: 2026-01-16T01:38:06+00:00</div>
<div class="meta-line">Comments: 28 pages, 17 figures, published in IEEE TRO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2205.01980v3">Abs</a> · <a href="https://arxiv.org/pdf/2205.01980v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual-Inertial Odometry (VIO) is the problem of estimating a robot&#x27;s trajectory by combining information from an inertial measurement unit (IMU) and a camera, and is of great interest to the robotics community. This paper develops a novel Lie group symmetry for the VIO problem and applies the recently proposed equivariant filter. The proposed symmetry is compatible with the invariance of the VIO reference frame, leading to improved filter consistency. The bias-free IMU dynamics are group-affine, ensuring that filter linearisation errors depend only on the bias estimation error and measurement noise. Furthermore, visual measurements are equivariant with respect to the symmetry, enabling the application of the higher-order equivariant output approximation to reduce approximation error in the filter update equation. As a result, the equivariant filter (EqF) based on this Lie group is a consistent estimator for VIO with lower linearisation error in the propagation of state dynamics and a higher order equivariant output approximation than standard formulations. Experimental results on the popular EuRoC and UZH FPV datasets demonstrate that the proposed system outperforms other state-of-the-art VIO algorithms in terms of both speed and accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EqVIO：一种用于视觉惯性里程计的等变滤波器</div>
<div class="mono" style="margin-top:8px">视觉惯性里程计（VIO）是通过结合惯性测量单元（IMU）和摄像机的信息来估计机器人轨迹的问题，对机器人学界具有重要意义。本文为VIO问题提出了一种新颖的李群对称性，并应用了近期提出的等变滤波器。所提出的对称性与VIO参考系的不变性兼容，从而提升了滤波器的一致性。无偏置IMU动力学具有群仿射特性，确保滤波器线性化误差仅取决于偏置估计误差和测量噪声。此外，视觉测量相对于该对称性具有等变性，使得能够应用高阶等变输出近似来减少滤波器更新方程中的近似误差。因此，基于此李群的等变滤波器（EqF）是VIO的一致估计器，在状态动力学传播中具有更低的线性化误差，且比标准公式具有更高阶的等变输出近似。在流行的EuRoC和UZH FPV数据集上的实验结果表明，所提出的系统在速度和精度方面均优于其他最先进的VIO算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for more consistent and accurate Visual-Inertial Odometry (VIO) by developing a novel Lie group symmetry specifically for the VIO problem and applying the equivariant filter framework. The method constructs a symmetry compatible with VIO reference frame invariance, where the bias-free IMU dynamics are group-affine and visual measurements are equivariant, enabling the use of a higher-order equivariant output approximation to reduce linearization errors. Experimental evaluation on the EuRoC and UZH FPV datasets demonstrates that the proposed EqVIO filter outperforms other state-of-the-art VIO algorithms in both computational speed and estimation accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉惯性里程计（VIO）中一致性与精度提升的需求，通过为VIO问题设计一种新颖的李群对称性来改进性能。该方法应用了等变滤波器（EqF），利用无偏置IMU动力学的群仿射特性以及视觉测量的等变性，从而减少了状态传播过程中的线性化误差，并在更新时实现了更高阶的输出近似。在EuRoC和UZH FPV数据集上的实验结果表明，所提出的EqVIO系统在速度和精度方面均优于其他先进的VIO算法。</div>
</details>
</div>
<div class="card">
<div class="title">Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation</div>
<div class="meta-line">Authors: Zhixian Xie, Yu Xiang, Michael Posa, Wanxin Jin</div>
<div class="meta-line">Venue: RSS</div>
<div class="meta-line">First: 2026-01-16T01:20:15+00:00 · Latest: 2026-01-16T01:20:15+00:00</div>
<div class="meta-line">Comments: 13 Pages, Plan to submit RSS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10930v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10930v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何处触碰，如何接触：面向几何感知长时程灵巧操作的层次化RL-MPC框架</div>
<div class="mono" style="margin-top:8px">接触密集型灵巧操作的一个关键挑战在于需同时处理几何、运动学约束及复杂非光滑接触动力学。端到端视觉运动策略虽绕开此结构，但通常需要大量数据、仿真到现实的迁移性差，且跨任务/具身的泛化能力弱。我们基于一个简单洞见应对这些局限：灵巧操作本质上是层次化的——高层决策涉及触碰位置（几何）与物体移动（运动学）；底层则通过接触动力学实现该计划。基于此，我们提出一种层次化RL-MPC框架：高层强化学习（RL）策略预测接触意图，这是一种以物体为中心的新型接口，指定（i）物体表面接触位置及（ii）接触后物体层级子目标位姿；底层则基于该接触意图，通过接触隐式模型预测控制（MPC）优化局部接触模式，并利用接触动力学进行重规划，生成稳健驱动物体朝向各子目标的机器人动作。我们在非抓取任务（包括几何泛化推动与物体三维重定向）上评估该框架，其以显著减少的数据量（较端到端基线少10倍）实现近100%成功率，具备高度稳健性能，并完成零样本仿真到现实迁移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of dexterous manipulation by jointly reasoning over geometry, kinematics, and contact dynamics, where end-to-end visuomotor policies often suffer from high data requirements, poor sim-to-real transfer, and weak generalization. The proposed method is a hierarchical RL-MPC framework: a high-level RL policy predicts a contact intention specifying an object-surface contact location and a post-contact subgoal pose, while a low-level contact-implicit model predictive control optimizes local contact modes and replans with dynamics to generate robust robot actions. Experimental results on non-prehensile tasks like pushing and 3D reorientation show near-100% success rates with 10x less data than end-to-end baselines, along with high robustness and zero-shot sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">该研究针对灵巧操作中需联合推理几何、运动学和接触动力学的挑战，其中端到端的视觉运动策略通常存在数据需求大、仿真到现实迁移差和泛化能力弱的问题。提出的方法是一个分层RL-MPC框架：高层强化学习策略预测接触意图，指定物体表面接触位置和接触后子目标位姿；底层接触隐式模型预测控制则优化局部接触模式并基于动力学重新规划以生成动作。在非抓取推动和物体三维重定向任务上的实验结果表明，该方法实现了接近100%的成功率，所需数据比端到端基线少10倍，性能鲁棒，并成功实现了零样本仿真到现实迁移。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
