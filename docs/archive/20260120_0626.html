<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-20 06:26</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260120_0626</div>
    <div class="row"><div class="card">
<div class="title">Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training</div>
<div class="meta-line">Authors: Shuo Cheng, Liqian Ma, Zhenyang Chen, Ajay Mandlekar, Caelan Garrett, Danfei Xu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-23T04:32:53+00:00 · Latest: 2026-01-16T18:05:09+00:00</div>
<div class="meta-line">Comments: Accepted to NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18631v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.18631v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ot-sim2real.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to achieve up to a 30% improvement in the real-world success rate and even generalize to scenarios seen only in simulation. Project webpage: https://ot-sim2real.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可泛化的领域自适应：仿真与真实策略协同训练</div>
<div class="mono" style="margin-top:8px">行为克隆在机器人操作任务中展现出潜力，但大规模获取真实世界演示数据成本高昂。虽然仿真数据提供了可扩展的替代方案（尤其在自动演示生成技术进步的背景下），但策略向真实世界的迁移仍受多种仿真与真实领域差异的阻碍。本研究提出一个统一的仿真-真实协同训练框架，用于学习可泛化的操作策略，该框架主要利用仿真数据，仅需少量真实世界演示。我们方法的核心是学习一个领域不变且与任务相关的特征空间。关键洞见在于：跨领域对齐观测数据及其对应动作的联合分布，比仅对齐观测数据（边缘分布）能提供更丰富的信号。我们通过在协同训练框架中嵌入受最优传输理论启发的损失函数来实现这一点，并将其扩展至非平衡最优传输框架，以处理丰富的仿真数据与有限的真实样本之间的不平衡问题。我们在具有挑战性的操作任务上验证了该方法，结果表明其能利用大量仿真数据将真实世界成功率提升高达30%，甚至能泛化至仅在仿真中见过的场景。项目网页：https://ot-sim2real.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of transferring robot manipulation policies from simulation to reality, where real-world demonstrations are scarce but simulation data is abundant. The method introduces a sim-and-real co-training framework that learns a domain-invariant feature space by aligning the joint distributions of observations and actions across domains using an Optimal Transport (OT)-inspired loss, extended to an Unbalanced OT formulation to handle data imbalance. Experimental validation on manipulation tasks demonstrates that the approach improves real-world success rates by up to 30% and enables generalization to scenarios only seen in simulation.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人操作策略从仿真迁移到现实时面临的挑战，即真实世界演示数据稀缺而仿真数据丰富。该方法提出了一个仿真与现实协同训练框架，通过使用最优传输损失及其处理数据不平衡的非平衡扩展版本，对齐跨领域的观测与动作联合分布，从而学习领域不变的特征。在操作任务上的实验验证表明，该方法能将现实世界成功率提升高达30%，并能泛化到仅在仿真中见过的场景。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Semantic-Geometric Task Graph-Representations from Human Demonstrations</div>
<div class="meta-line">Authors: Franziska Herbert, Vignesh Prasad, Han Liu, Dorothea Koert, Georgia Chalvatzaki</div>
<div class="meta-line">First: 2026-01-16T17:35:00+00:00 · Latest: 2026-01-16T17:35:00+00:00</div>
<div class="meta-line">Comments: 9 pages, 7 figures, preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11460v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11460v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning structured task representations from human demonstrations is essential for understanding long-horizon manipulation behaviors, particularly in bimanual settings where action ordering, object involvement, and interaction geometry can vary significantly. A key challenge lies in jointly capturing the discrete semantic structure of tasks and the temporal evolution of object-centric geometric relations in a form that supports reasoning over task progression. In this work, we introduce a semantic-geometric task graph-representation that encodes object identities, inter-object relations, and their temporal geometric evolution from human demonstrations. Building on this formulation, we propose a learning framework that combines a Message Passing Neural Network (MPNN) encoder with a Transformer-based decoder, decoupling scene representation learning from action-conditioned reasoning about task progression. The encoder operates solely on temporal scene graphs to learn structured representations, while the decoder conditions on action-context to predict future action sequences, associated objects, and object motions over extended time horizons. Through extensive evaluation on human demonstration datasets, we show that semantic-geometric task graph-representations are particularly beneficial for tasks with high action and object variability, where simpler sequence-based models struggle to capture task progression. Finally, we demonstrate that task graph representations can be transferred to a physical bimanual robot and used for online action selection, highlighting their potential as reusable task abstractions for downstream decision-making in manipulation systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人类演示的语义-几何任务图表示学习</div>
<div class="mono" style="margin-top:8px">从人类演示中学习结构化任务表示对于理解长时程操作行为至关重要，尤其在双手操作场景中，动作顺序、物体参与和交互几何关系可能存在显著差异。核心挑战在于以支持任务进程推理的形式，同时捕捉任务的离散语义结构和以物体为中心的几何关系的时间演化。本研究提出一种语义-几何任务图表示方法，从人类演示中编码物体身份、物体间关系及其时间几何演化。基于此框架，我们提出结合消息传递神经网络编码器与基于Transformer解码器的学习框架，将场景表示学习与基于动作条件的任务进程推理解耦。编码器仅使用时序场景图学习结构化表示，解码器则基于动作上下文预测未来动作序列、关联物体及长时程物体运动。通过对人类演示数据集的广泛评估，我们证明语义-几何任务图表示对动作和物体高度可变的任务特别有效，而传统序列模型难以捕捉此类任务进程。最后，我们验证了任务图表示可迁移至实体双手机器人并用于在线动作选择，凸显其作为可复用任务抽象在操作系统下游决策中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To understand long-horizon bimanual manipulation tasks where action order, object roles, and geometric interactions vary, this research aims to jointly capture discrete semantic structure and the temporal evolution of object-centric geometric relations. The method introduces a semantic-geometric task graph representation and a learning framework that combines a Message Passing Neural Network encoder to learn from temporal scene graphs with a Transformer-based decoder that uses action context to predict future action sequences, associated objects, and object motions. Experimental results on human demonstration datasets show that this graph-based representation outperforms simpler sequence models in tasks with high action and object variability, and the learned representations can be transferred to a physical bimanual robot for online action selection, demonstrating their utility as reusable abstractions for downstream decision-making.</div>
<div class="mono" style="margin-top:8px">为理解动作顺序、物体角色和几何交互多变的长时程双手操作任务，本研究旨在从人类演示中共同捕获离散的语义结构和以物体为中心的几何关系的时间演化。该方法引入了语义几何任务图表示和一个学习框架，将场景表示与动作推理解耦：消息传递神经网络（MPNN）编码器从时序场景图中学习结构化表示，而基于Transformer的解码器在动作上下文条件下预测未来的动作序列、相关物体及物体运动。在人类演示数据集上的实验评估表明，这种基于图的表示在动作和物体变异性高的任务中优于简单的序列模型，并且该表示已成功迁移到物理双手机器人上用于在线动作选择，证明了其作为可重用抽象用于下游决策的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Probabilistic Mission Design for Neuro-Symbolic Unmanned Aircraft Systems</div>
<div class="meta-line">Authors: Simon Kohaut, Benedict Flade, Daniel Ochs, Devendra Singh Dhami, Julian Eggert, Kristian Kersting</div>
<div class="meta-line">First: 2024-12-25T11:04:00+00:00 · Latest: 2026-01-16T17:27:13+00:00</div>
<div class="meta-line">Comments: arXiv admin note: text overlap with arXiv:2406.03454</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.01439v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.01439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advanced Air Mobility (AAM) is a growing field that demands accurate and trustworthy models of legal concepts and restrictions for navigating Unmanned Aircraft Systems (UAS). In addition, any implementation of AAM needs to face the challenges posed by inherently dynamic and uncertain human-inhabited spaces robustly. Nevertheless, the employment of UAS beyond visual line of sight (BVLOS) is an endearing task that promises to significantly enhance today&#x27;s logistics and emergency response capabilities. Hence, we propose Probabilistic Mission Design (ProMis), a novel neuro-symbolic approach to navigating UAS within legal frameworks. ProMis is an interpretable and adaptable system architecture that links uncertain geospatial data and noisy perception with declarative, Hybrid Probabilistic Logic Programs (HPLP) to reason over the agent&#x27;s state space and its legality. To inform planning with legal restrictions and uncertainty in mind, ProMis yields Probabilistic Mission Landscapes (PML). These scalar fields quantify the belief that the HPLP is satisfied across the agent&#x27;s state space. Extending prior work on ProMis&#x27; reasoning capabilities and computational characteristics, we show its integration with potent machine learning models such as Large Language Models (LLM) and Transformer-based vision models. Hence, our experiments underpin the application of ProMis with multi-modal input data and how our method applies to many AAM scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经符号无人航空系统的概率任务设计</div>
<div class="mono" style="margin-top:8px">先进空中交通（AAM）是一个不断发展的领域，需要精确可靠的法律概念与限制模型来引导无人航空系统（UAS）。此外，任何AAM的实施都必须稳健应对人类活动空间固有的动态性与不确定性带来的挑战。然而，超视距（BVLOS）运行UAS是一项极具价值的任务，有望显著提升当今物流与应急响应能力。为此，我们提出概率任务设计（ProMis），一种在法规框架内导航UAS的新型神经符号方法。ProMis是一个可解释且适应性强的系统架构，通过声明式混合概率逻辑程序（HPLP）将不确定的地理空间数据与含噪声感知相连接，以推理智能体的状态空间及其合法性。为在规划中兼顾法律限制与不确定性，ProMis生成概率任务态势图（PML）。这些标量场量化了HPLP在智能体状态空间内被满足的置信度。通过扩展ProMis推理能力与计算特性的前期研究，我们展示了其与大型语言模型（LLM）及基于Transformer的视觉模型等强大机器学习模型的集成。实验验证了ProMis在多模态输入数据中的应用，以及该方法对多种AAM场景的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for trustworthy and legally compliant navigation of Unmanned Aircraft Systems (UAS) in dynamic, uncertain Advanced Air Mobility (AAM) environments, particularly for beyond visual line of sight (BVLOS) operations. The method introduces Probabilistic Mission Design (ProMis), a neuro-symbolic architecture that integrates uncertain geospatial and perceptual data with declarative Hybrid Probabilistic Logic Programs (HPLP) to reason about legal constraints, generating Probabilistic Mission Landscapes (PML) to quantify belief in constraint satisfaction across the state space. Key experimental findings demonstrate the integration of ProMis with large language models and Transformer-based vision models, validating its application with multi-modal input data across various AAM scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在为高级空中交通（AAM）中动态、不确定环境下的无人航空系统（UAS）提供可靠且合规的导航方案，特别是针对超视距运行。所提出的方法——概率任务设计（ProMis）——是一种神经符号架构，它将不确定的地理空间和感知数据与声明性的混合概率逻辑程序（HPLP）相结合，以推理合法状态空间，并生成概率任务景观（PML）来指导规划。实验结果展示了ProMis与大型语言模型和基于Transformer的视觉模型的集成，验证了其在多模态数据下对多种AAM场景的应用能力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-Based Shrinking Disturbance-Invariant Tubes for State- and Input-Dependent Uncertainty</div>
<div class="meta-line">Authors: Abdelrahman Ramadan, Sidney Givigi</div>
<div class="meta-line">Venue: IEEE Control Systems Letters, vol. 9, pp. 2699-2704, Dec. 2025</div>
<div class="meta-line">First: 2026-01-16T16:47:04+00:00 · Latest: 2026-01-16T16:47:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11426v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11426v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a learning-based framework for constructing shrinking disturbance-invariant tubes under state- and input-dependent uncertainty, intended as a building block for tube Model Predictive Control (MPC), and certify safety via a lifted, isotone (order-preserving) fixed-point map. Gaussian Process (GP) posteriors become $(1-α)$ credible ellipsoids, then polytopic outer sets for deterministic set operations. A two-time-scale scheme separates learning epochs, where these polytopes are frozen, from an inner, outside-in iteration that converges to a compact fixed point $Z^\star\!\subseteq\!\mathcal G$; its state projection is RPI for the plant. As data accumulate, disturbance polytopes tighten, and the associated tubes nest monotonically, resolving the circular dependence between the set to be verified and the disturbance model while preserving hard constraints. A double-integrator study illustrates shrinking tube cross-sections in data-rich regions while maintaining invariance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的状态与输入依赖不确定性下的收缩扰动不变管构建</div>
<div class="mono" style="margin-top:8px">我们开发了一种基于学习的框架，用于在状态与输入依赖的不确定性下构建收缩扰动不变管，旨在作为管式模型预测控制（MPC）的基础模块，并通过一个提升的、保序的固定点映射来确保安全性。高斯过程（GP）后验被转化为$(1-α)$可信椭球，进而通过确定性集合操作得到多面体外集。采用双时间尺度方案，将学习阶段（其中多面体保持固定）与一个由外向内迭代的内部过程分离，后者收敛至一个紧致固定点$Z^\star\!\subseteq\!\mathcal G$；其状态投影对受控对象是鲁棒正不变的。随着数据积累，扰动多面体逐渐收紧，相关管序列单调嵌套，从而解决了待验证集合与扰动模型之间的循环依赖问题，同时保持硬约束。通过双积分器案例研究，展示了在数据丰富区域管截面收缩的同时保持不变性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of constructing robust disturbance-invariant tubes for tube-based Model Predictive Control (MPC) under state- and input-dependent uncertainty, where a circular dependence exists between the set to be verified and the disturbance model. The proposed method employs Gaussian Process (GP) regression to model the uncertainty, transforming its posterior into credible ellipsoids and then into polytopic outer approximations for deterministic set operations. A two-time-scale scheme separates learning epochs from an inner fixed-point iteration that converges to a robust positively invariant (RPI) set. Experimental results on a double-integrator system demonstrate that as data accumulate, the disturbance polytopes tighten, leading to monotonically nested and shrinking tubes in data-rich regions while maintaining invariance and hard constraint satisfaction.</div>
<div class="mono" style="margin-top:8px">本研究针对状态和输入相关不确定性下的管型模型预测控制（MPC）中的安全保证挑战，该挑战源于扰动模型与待验证的鲁棒不变集之间的循环依赖关系。所提出的方法采用一个基于学习的框架，利用高斯过程回归对不确定性进行建模，并将其后验转换为可信椭球，进而转化为用于确定性集合操作的多面体外近似。一个双时间尺度方案将学习阶段与内部不动点迭代分离，该迭代收敛到一个紧致的鲁棒正不变集。在双积分器系统上的实验结果表明，随着数据积累，扰动多面体收紧，导致管型集合单调嵌套并收缩，同时保持不变性并满足硬约束。</div>
</details>
</div>
<div class="card">
<div class="title">The Great March 100: 100 Detail-oriented Tasks for Evaluating Embodied AI Agents</div>
<div class="meta-line">Authors: Ziyu Wang, Chenyuan Liu, Yushun Xiang, Runhao Zhang, Qingbo Hao, Hongliang Lu, Houyu Chen, Zhizhong Feng, Kaiyue Zheng, Dehao Ye, Xianchao Zeng, Xinyu Zhou, Boran Wen, Jiaxin Li, Mingyu Zhang, Kecheng Zheng, Qian Zhu, Ran Cheng, Yong-Lu Li</div>
<div class="meta-line">First: 2026-01-16T16:42:05+00:00 · Latest: 2026-01-16T16:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11421v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11421v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rhos.ai/research/gm-100">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, with the rapid development of robot learning and imitation learning, numerous datasets and methods have emerged. However, these datasets and their task designs often lack systematic consideration and principles. This raises important questions: Do the current datasets and task designs truly advance the capabilities of robotic agents? Do evaluations on a few common tasks accurately reflect the differentiated performance of various methods proposed by different teams and evaluated on different tasks? To address these issues, we introduce the Great March 100 (\textbf{GM-100}) as the first step towards a robot learning Olympics. GM-100 consists of 100 carefully designed tasks that cover a wide range of interactions and long-tail behaviors, aiming to provide a diverse and challenging set of tasks to comprehensively evaluate the capabilities of robotic agents and promote diversity and complexity in robot dataset task designs. These tasks are developed through systematic analysis and expansion of existing task designs, combined with insights from human-object interaction primitives and object affordances. We collect a large amount of trajectory data on different robotic platforms and evaluate several baseline models. Experimental results demonstrate that the GM-100 tasks are 1) feasible to execute and 2) sufficiently challenging to effectively differentiate the performance of current VLA models. Our data and code are available at https://rhos.ai/research/gm-100.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伟大征程100：面向具身智能体评估的100项精细化任务</div>
<div class="mono" style="margin-top:8px">近年来，随着机器人学习与模仿学习的快速发展，涌现出大量数据集与方法。然而，这些数据集及其任务设计往往缺乏系统性考量与原则。这引发了两个重要问题：现有数据集与任务设计是否真正推动了机器人智能体能力的发展？基于少量通用任务的评估能否准确反映不同团队在不同任务上提出的各类方法的差异化性能？为解决这些问题，我们推出“伟大征程100”（GM-100）作为迈向机器人学习奥林匹克的第一步。GM-100包含100项精心设计的任务，涵盖广泛的交互场景与长尾行为，旨在通过多样化、高挑战性的任务集全面评估机器人智能体能力，并促进机器人数据集任务设计的多样性与复杂性。这些任务通过对现有任务设计的系统性分析与扩展，结合人-物交互基元与物体可供性洞见开发而成。我们在不同机器人平台上采集了大量轨迹数据，并评估了若干基线模型。实验结果表明GM-100任务具有两大特性：1）具备可执行性；2）具有足够挑战性，能有效区分当前视觉语言动作模型的性能差异。我们的数据与代码已开源：https://rhos.ai/research/gm-100。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by concerns that existing robot learning datasets lack systematic design principles and may not accurately differentiate method performance, this paper introduces the Great March 100 (GM-100), a benchmark of 100 detail-oriented tasks for embodied AI agents. The method involves systematically analyzing and expanding existing task designs, incorporating insights from human-object interaction primitives and object affordances to create a diverse set covering a wide range of interactions and long-tail behaviors. Experimental results on collected trajectory data show the tasks are both feasible to execute and sufficiently challenging to effectively differentiate the performance of current Vision-Language-Action (VLA) models.</div>
<div class="mono" style="margin-top:8px">本研究针对现有机器人学习数据集缺乏系统性设计原则、可能无法准确区分方法性能的问题，提出了Great March 100（GM-100）基准，该基准包含100个注重细节的任务，其设计基于对现有任务、人-物交互基元及物体可供性的系统分析。方法上，通过构建覆盖广泛交互和长尾行为的多样化任务集，并在多个机器人平台上收集大量轨迹数据以评估基线模型。主要实验结果表明，GM-100任务既具备可执行性，又具有足够的挑战性，能够有效区分当前视觉-语言-动作（VLA）模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Conditioned Variational Bayesian Last Layer Dynamics Models</div>
<div class="meta-line">Authors: Paul Brunzema, Thomas Lew, Ray Zhang, Takeru Shirasawa, John Subosits, Marcus Greiff</div>
<div class="meta-line">First: 2026-01-14T05:25:18+00:00 · Latest: 2026-01-16T16:32:59+00:00</div>
<div class="meta-line">Comments: 9 pages, 7 figures, currently under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09178v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09178v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agile control of robotic systems often requires anticipating how the environment affects system behavior. For example, a driver must perceive the road ahead to anticipate available friction and plan actions accordingly. Achieving such proactive adaptation within autonomous frameworks remains a challenge, particularly under rapidly changing conditions. Traditional modeling approaches often struggle to capture abrupt variations in system behavior, while adaptive methods are inherently reactive and may adapt too late to ensure safety. We propose a vision-conditioned variational Bayesian last-layer dynamics model that leverages visual context to anticipate changes in the environment. The model first learns nominal vehicle dynamics and is then fine-tuned with feature-wise affine transformations of latent features, enabling context-aware dynamics prediction. The resulting model is integrated into an optimal controller for vehicle racing. We validate our method on a Lexus LC500 racing through water puddles. With vision-conditioning, the system completed all 12 attempted laps under varying conditions. In contrast, all baselines without visual context consistently lost control, demonstrating the importance of proactive dynamics adaptation in high-performance applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉条件化变分贝叶斯末层动力学模型</div>
<div class="mono" style="margin-top:8px">机器人系统的敏捷控制常需预判环境对系统行为的影响。例如，驾驶员必须感知前方路况以预判可用摩擦力并相应规划动作。在自主框架内实现此类前瞻性适应仍具挑战性，尤其在快速变化条件下。传统建模方法难以捕捉系统行为的突变，而自适应方法本质是反应式的，可能因适应过迟而无法保证安全。我们提出一种视觉条件化变分贝叶斯末层动力学模型，利用视觉上下文预判环境变化。该模型先学习标称车辆动力学，再通过潜在特征的逐特征仿射变换进行微调，实现上下文感知的动力学预测。最终模型被集成至车辆竞速的最优控制器中。我们在雷克萨斯LC500涉水竞速场景中验证了方法有效性。借助视觉条件化，系统在不同条件下完成了全部12圈尝试。相比之下，所有无视觉上下文的基线模型均持续失控，这证明了前瞻性动力学适应在高性能应用中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling robotic systems to proactively adapt to rapidly changing environmental conditions, such as a vehicle anticipating road friction from visual cues, which traditional or reactive adaptive models often fail to handle safely. The method introduces a vision-conditioned variational Bayesian last-layer dynamics model that first learns nominal vehicle dynamics and then fine-tunes them using feature-wise affine transformations of latent features based on visual context, allowing for context-aware predictions. Experimental validation on a Lexus LC500 racing through water puddles showed that the vision-conditioned system successfully completed all 12 laps under varying conditions, whereas all baseline methods without visual context consistently lost control, highlighting the critical role of proactive adaptation for high-performance applications.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决机器人系统如何主动适应快速变化的环境条件（例如车辆根据视觉线索预判路面摩擦）这一挑战，传统或反应式自适应模型在此方面存在困难。所提出的方法是一种视觉条件变分贝叶斯最后一层动力学模型，它首先学习标称车辆动力学，然后基于视觉上下文对潜在特征进行逐特征仿射变换来微调模型，从而实现前瞻性的动力学预测。在一辆雷克萨斯LC500赛车通过积水坑道的实验验证中，视觉条件系统在变化条件下成功完成了全部12圈测试，而所有没有视觉上下文的基线模型均持续失控，这凸显了主动适应对于安全和高性能应用的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Linqing Zhong, Yi Liu, Yifei Wei, Ziyu Xiong, Maoqing Yao, Si Liu, Guanghui Ren</div>
<div class="meta-line">First: 2026-01-16T16:17:06+00:00 · Latest: 2026-01-16T16:17:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11404v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11404v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ACoT-VLA：面向视觉-语言-动作模型的动作思维链</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型已成为多样化操作任务中重要的通用机器人策略，传统方法依赖通过视觉语言模型（VLM）嵌入将多模态输入直接转换为动作。近期研究引入了显式中间推理（如子任务预测（语言）或目标图像合成（视觉））来指导动作生成，但这些中间推理往往间接且固有地受限于传达精确动作执行所需的完整细粒度信息。我们认为最有效的推理形式应直接在动作空间中进行推演。本文提出动作思维链（ACoT）范式，将推理过程构建为引导最终策略的粗粒度动作意图结构化序列。我们提出ACoT-VLA这一实现ACoT范式的新颖架构，具体包含两个互补组件：显式动作推理器（EAR）和隐式动作推理器（IAR）。前者提出粗粒度参考轨迹作为显式动作级推理步骤，后者从多模态输入的内部表征中提取潜在动作先验，共同构成ACoT以调节下游动作头，实现具身策略学习。在真实世界和仿真环境中的大量实验表明，该方法在LIBERO、LIBERO-Plus和VLABench基准上分别达到98.5%、84.1%和47.4%的性能，验证了其优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of current Vision-Language-Action (VLA) models, which often use indirect reasoning like language sub-tasks or goal images that fail to convey granular information for precise robot action execution. The proposed method, ACoT-VLA, introduces an Action Chain-of-Thought paradigm that reasons directly in the action space through two components: an Explicit Action Reasoner that generates coarse reference trajectories and an Implicit Action Reasoner that extracts latent action priors from multimodal inputs, together conditioning a downstream action head for grounded policy learning. Experimental results demonstrate the method&#x27;s superiority, achieving success rates of 98.5% on LIBERO, 84.1% on LIBERO-Plus, and 47.4% on VLABench.</div>
<div class="mono" style="margin-top:8px">本研究针对当前视觉-语言-动作（VLA）模型的局限性，即子任务预测或目标图像合成等中间推理步骤较为间接，缺乏精确机器人动作执行所需的细粒度信息。提出的方法ACoT-VLA引入了动作思维链（ACoT）范式，直接在动作空间中进行推理，通过显式动作推理器（EAR）提出粗略参考轨迹和隐式动作推理器（IAR）从多模态输入中提取潜在动作先验，共同为最终策略学习提供条件。实验结果表明该方法具有优越性，在LIBERO、LIBERO-Plus和VLABench基准上分别达到了98.5%、84.1%和47.4%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">The Mini Wheelbot Dataset: High-Fidelity Data for Robot Learning</div>
<div class="meta-line">Authors: Henrik Hose, Paul Brunzema, Devdutt Subhasish, Sebastian Trimpe</div>
<div class="meta-line">First: 2026-01-16T16:06:32+00:00 · Latest: 2026-01-16T16:06:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11394v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11394v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of robust learning-based control algorithms for unstable systems requires high-quality, real-world data, yet access to specialized robotic hardware remains a significant barrier for many researchers. This paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source, quasi-symmetric balancing reaction wheel unicycle. The dataset provides 1 kHz synchronized data encompassing all onboard sensor readings, state estimates, ground-truth poses from a motion capture system, and third-person video logs. To ensure data diversity, we include experiments across multiple hardware instances and surfaces using various control paradigms, including pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents. We include several example applications in dynamics model learning, state estimation, and time-series classification to illustrate common robotics algorithms that can be benchmarked on our dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>微型轮式机器人数据集：面向机器人学习的高保真数据</div>
<div class="mono" style="margin-top:8px">为不稳定系统开发鲁棒的基于学习的控制算法需要高质量的真实世界数据，然而专用机器人硬件的获取仍是许多研究者的主要障碍。本文介绍了微型轮式机器人的综合动力学数据集，该机器人是一种开源、准对称的平衡反应轮单轮车。数据集提供1kHz同步数据，涵盖所有板载传感器读数、状态估计、运动捕捉系统的真实位姿以及第三人称视频记录。为确保数据多样性，我们纳入了跨多个硬件实例和不同表面的实验，采用包括伪随机二进制激励、非线性模型预测控制和强化学习智能体在内的多种控制范式。我们还提供了动力学模型学习、状态估计和时间序列分类的若干示例应用，以展示可在本数据集上基准测试的常见机器人算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity of high-quality real-world data for developing learning-based control algorithms on unstable robotic systems, this paper introduces a comprehensive dynamics dataset for the Mini Wheelbot, an open-source balancing reaction wheel unicycle. The method involves collecting 1 kHz synchronized data from multiple hardware instances across different surfaces, using various control paradigms such as pseudo-random binary excitation, nonlinear model predictive control, and reinforcement learning agents. The key experimental results demonstrate the dataset&#x27;s utility through example applications in dynamics model learning, state estimation, and time-series classification, providing a benchmark for robotics algorithm development.</div>
<div class="mono" style="margin-top:8px">针对不稳定机器人系统开发基于学习的控制算法时缺乏高质量真实世界数据的问题，本文为开源平衡反应轮单轮机器人Mini Wheelbot引入了一个全面的动力学数据集。该方法通过使用伪随机二进制激励、非线性模型预测控制和强化学习等多种控制范式，在多个硬件实例和表面上收集1 kHz的同步数据，以确保数据多样性。主要实验结果展示了该数据集在基准测试常见机器人算法方面的实用性，并通过动力学模型学习、状态估计和时间序列分类等示例应用进行了说明。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-Tuning of Neural Network Approximate MPC without Retraining via Bayesian Optimization</div>
<div class="meta-line">Authors: Henrik Hose, Paul Brunzema, Alexander von Rohr, Alexander Gräfe, Angela P. Schoellig, Sebastian Trimpe</div>
<div class="meta-line">First: 2025-12-16T12:24:08+00:00 · Latest: 2026-01-16T14:52:20+00:00</div>
<div class="meta-line">Comments: Presented at the 13th International Conference on Robot Intelligence Technology and Applications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14350v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.14350v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Approximate model-predictive control (AMPC) aims to imitate an MPC&#x27;s behavior with a neural network, removing the need to solve an expensive optimization problem at runtime. However, during deployment, the parameters of the underlying MPC must usually be fine-tuned. This often renders AMPC impractical as it requires repeatedly generating a new dataset and retraining the neural network. Recent work addresses this problem by adapting AMPC without retraining using approximated sensitivities of the MPC&#x27;s optimization problem. Currently, this adaption must be done by hand, which is labor-intensive and can be unintuitive for high-dimensional systems. To solve this issue, we propose using Bayesian optimization to tune the parameters of AMPC policies based on experimental data. By combining model-based control with direct and local learning, our approach achieves superior performance to nominal AMPC on hardware, with minimal experimentation. This allows automatic and data-efficient adaptation of AMPC to new system instances and fine-tuning to cost functions that are difficult to directly implement in MPC. We demonstrate the proposed method in hardware experiments for the swing-up maneuver on an inverted cartpole and yaw control of an under-actuated balancing unicycle robot, a challenging control problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯优化的免重训练神经网络近似模型预测控制微调方法</div>
<div class="mono" style="margin-top:8px">近似模型预测控制（AMPC）旨在通过神经网络模拟MPC的行为，避免运行时求解昂贵的优化问题。然而，部署过程中通常需要对底层MPC参数进行微调，这往往导致AMPC不实用，因为需要反复生成新数据集并重训练神经网络。近期研究通过利用MPC优化问题的近似灵敏度实现免重训练自适应，但目前仍需人工操作，对高维系统而言既费力又缺乏直观性。为解决该问题，我们提出基于实验数据使用贝叶斯优化调整AMPC策略参数。通过将基于模型的控制与直接局部学习相结合，本方法在硬件上以最少实验量实现了优于传统AMPC的性能，能够自动且数据高效地使AMPC适配新系统实例，并微调难以直接嵌入MPC的成本函数。我们在倒立摆起摆控制和欠驱动独轮平衡机器人偏航控制这两个具有挑战性的硬件实验中验证了所提方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the practical limitation of approximate model-predictive control (AMPC), where fine-tuning the underlying MPC parameters after deployment typically requires costly dataset regeneration and neural network retraining. The proposed method automates this adaptation by employing Bayesian optimization to directly tune the AMPC policy parameters based on experimental data, eliminating the need for manual sensitivity analysis. In hardware experiments, including an inverted cartpole swing-up and yaw control of a balancing unicycle robot, this approach demonstrated superior performance to nominal AMPC, enabling data-efficient automatic adaptation to new system instances and cost functions difficult to implement directly in MPC.</div>
<div class="mono" style="margin-top:8px">本研究针对近似模型预测控制（AMPC）在实际应用中的局限性：部署后调整底层MPC参数通常需要重新生成数据集和训练神经网络，成本高昂。所提出的方法通过采用贝叶斯优化，直接利用实验数据调整AMPC策略参数，从而实现了自适应调优的自动化，避免了手动灵敏度分析或重新训练。在倒立摆起摆和欠驱动平衡独轮机器人偏航控制等硬件实验中，该方法表现出优于标准AMPC的性能，能够高效、自动地针对新系统实例和复杂成本函数进行微调。</div>
</details>
</div>
<div class="card">
<div class="title">Distributed Control Barrier Functions for Safe Multi-Vehicle Navigation in Heterogeneous USV Fleets</div>
<div class="meta-line">Authors: Tyler Paine, Brendan Long, Jeremy Wenger, Michael DeFilippo, James Usevitch, Michael Benjamin</div>
<div class="meta-line">First: 2026-01-16T14:30:57+00:00 · Latest: 2026-01-16T14:30:57+00:00</div>
<div class="meta-line">Comments: 8 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11335v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11335v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Collision avoidance in heterogeneous fleets of uncrewed vessels is challenging because the decision-making processes and controllers often differ between platforms, and it is further complicated by the limitations on sharing trajectories and control values in real-time. This paper presents a pragmatic approach that addresses these issues by adding a control filter on each autonomous vehicle that assumes worst-case behavior from other contacts, including crewed vessels. This distributed safety control filter is developed using control barrier function (CBF) theory and the application is clearly described to ensure explainability of these safety-critical methods. This work compares the worst-case CBF approach with a Collision Regulations (COLREGS) behavior-based approach in simulated encounters. Real-world experiments with three different uncrewed vessels and a human operated vessel were performed to confirm the approach is effective across a range of platforms and is robust to uncooperative behavior from human operators. Results show that combining both CBF methods and COLREGS behaviors achieves the best safety and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异构无人艇编队安全多车导航的分布式控制屏障函数方法</div>
<div class="mono" style="margin-top:8px">异构无人艇编队的避碰具有挑战性，因为不同平台的决策过程与控制器存在差异，且实时共享轨迹与控制值的限制进一步增加了复杂度。本文提出一种实用方法，通过在每辆自主车辆上添加控制滤波器，假设其他接触目标（包括有人驾驶船只）均采取最差行为模式。该分布式安全控制滤波器基于控制屏障函数理论开发，并通过清晰的应用描述确保这类安全关键方法的可解释性。研究在模拟遭遇场景中对比了最差情况CBF方法与基于《国际海上避碰规则》行为模式的方法。通过三艘不同无人艇与一艘人工操作船只的实际实验，验证了该方法在多平台间的有效性及对人类操作者非合作行为的鲁棒性。结果表明，结合CBF方法与COLREGS行为模式能实现最佳安全性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of safe navigation in heterogeneous uncrewed surface vehicle (USV) fleets, where differing decision-making processes, controllers, and real-time communication limitations complicate collision avoidance. The proposed method employs a distributed safety control filter based on control barrier function (CBF) theory, which is applied to each autonomous vehicle and assumes worst-case behavior from other contacts, including human-operated vessels. Experimental results from simulations and real-world tests with three different USVs and a crewed vessel demonstrate that the worst-case CBF approach is effective across platforms and robust to uncooperative behavior, and that combining it with Collision Regulations (COLREGS) behavior-based methods yields the best performance in both safety and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决异构无人水面艇（USV）集群中的安全导航挑战，其中不同的决策过程、控制器以及实时通信限制使得碰撞避免变得复杂。所提出的方法采用基于控制屏障函数（CBF）理论的分布式安全控制滤波器，该滤波器应用于每艘自主船只，并假设其他接触目标（包括有人驾驶船只）采取最坏情况行为，从而确保安全。通过仿真以及使用三艘不同USV和一艘人工操作船只进行的真实世界实验结果表明，最坏情况CBF方法在不同平台上均有效，且对人类操作员的不合作行为具有鲁棒性，同时将其与《国际海上避碰规则》（COLREGS）行为策略相结合，能在安全性和操作效率上实现最佳性能。</div>
</details>
</div>
<div class="card">
<div class="title">Skill-Aware Diffusion for Generalizable Robotic Manipulation</div>
<div class="meta-line">Authors: Aoshen Huang, Jiaming Chen, Jiyu Cheng, Ran Song, Wei Pan, Wei Zhang</div>
<div class="meta-line">First: 2026-01-16T13:14:40+00:00 · Latest: 2026-01-16T13:14:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11266v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11266v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/sa-diff">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust generalization in robotic manipulation is crucial for robots to adapt flexibly to diverse environments. Existing methods usually improve generalization by scaling data and networks, but model tasks independently and overlook skill-level information. Observing that tasks within the same skill share similar motion patterns, we propose Skill-Aware Diffusion (SADiff), which explicitly incorporates skill-level information to improve generalization. SADiff learns skill-specific representations through a skill-aware encoding module with learnable skill tokens, and conditions a skill-constrained diffusion model to generate object-centric motion flow. A skill-retrieval transformation strategy further exploits skill-specific trajectory priors to refine the mapping from 2D motion flow to executable 3D actions. Furthermore, we introduce IsaacSkill, a high-fidelity dataset containing fundamental robotic skills for comprehensive evaluation and sim-to-real transfer. Experiments in simulation and real-world settings show that SADiff achieves good performance and generalization across various manipulation tasks. Code, data, and videos are available at https://sites.google.com/view/sa-diff.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>技能感知扩散模型在机器人操作泛化中的应用</div>
<div class="mono" style="margin-top:8px">机器人操作的鲁棒泛化能力对其灵活适应多样化环境至关重要。现有方法通常通过扩大数据和网络规模来提升泛化性，但独立建模任务并忽略了技能层级信息。观察到同一技能下的任务具有相似的运动模式，我们提出技能感知扩散模型（SADiff），显式融合技能层级信息以增强泛化能力。SADiff通过可学习技能令牌的技能感知编码模块学习技能特定表征，并约束技能条件扩散模型生成以物体为中心的运动流。技能检索转换策略进一步利用技能特定的轨迹先验，优化从二维运动流到可执行三维动作的映射。此外，我们构建了IsaacSkill高保真数据集，包含基础机器人技能用于全面评估与仿真到现实的迁移。仿真与真实环境实验表明，SADiff在多种操作任务中均表现出优异的性能与泛化能力。代码、数据及视频详见https://sites.google.com/view/sa-diff。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance robust generalization in robotic manipulation beyond methods that scale data and networks while overlooking skill-level information, this work proposes Skill-Aware Diffusion (SADiff). The method explicitly incorporates skill-level information by learning skill-specific representations via a skill-aware encoding module with learnable tokens and conditions a skill-constrained diffusion model to generate object-centric motion flow; a skill-retrieval transformation strategy further refines the mapping from 2D flow to 3D actions. Experiments conducted using the introduced IsaacSkill dataset demonstrate that SADiff achieves strong performance and generalization across diverse manipulation tasks in both simulation and real-world settings.</div>
<div class="mono" style="margin-top:8px">为提升机器人操作的鲁棒泛化能力，避免仅依赖数据和网络规模的扩展，本研究针对现有方法忽视任务间共享技能级运动模式的问题，提出了技能感知扩散模型（SADiff）。该方法通过可学习技能令牌编码技能特定表示，并约束扩散模型生成以物体为中心的运动流，进而利用技能检索转换策略将二维运动流细化为可执行的三维动作。基于新构建的高保真IsaacSkill数据集在仿真和真实环境中的实验表明，SADiff在多种操作任务上实现了良好的性能和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">VLAgents: A Policy Server for Efficient VLA Inference</div>
<div class="meta-line">Authors: Tobias Jülg, Khaled Gamal, Nisarga Nilavadi, Pierre Krack, Seongjin Bien, Michael Krawez, Florian Walter, Wolfram Burgard</div>
<div class="meta-line">First: 2026-01-16T12:58:59+00:00 · Latest: 2026-01-16T12:58:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11250v1">PDF</a> · <a href="https://github.com/RobotControlStack/vlagents">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid emergence of Vision-Language-Action models (VLAs) has a significant impact on robotics. However, their deployment remains complex due to the fragmented interfaces and the inherent communication latency in distributed setups. To address this, we introduce VLAgents, a modular policy server that abstracts VLA inferencing behind a unified Gymnasium-style protocol. Crucially, its communication layer transparently adapts to the context by supporting both zero-copy shared memory for high-speed simulation and compressed streaming for remote hardware. In this work, we present the architecture of VLAgents and validate it by integrating seven policies -- including OpenVLA and Pi Zero. In a benchmark with both local and remote communication, we further demonstrate how it outperforms the default policy servers provided by OpenVLA, OpenPi, and LeRobot. VLAgents is available at https://github.com/RobotControlStack/vlagents</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLAgents：高效视觉语言动作模型推理的策略服务器</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型的快速兴起对机器人领域产生了显著影响。然而，由于分布式部署中接口碎片化及固有的通信延迟，其实际应用仍面临复杂性。为此，我们提出了VLAgents——一个模块化策略服务器，通过统一的Gymnasium风格协议抽象VLA推理过程。其通信层核心创新在于能根据场景透明适配：既支持零拷贝共享内存以实现高速仿真，也支持压缩流传输以适配远程硬件。本研究详细阐述了VLAgents的架构，并通过集成七种策略（包括OpenVLA与Pi Zero）进行验证。在本地与远程通信的基准测试中，该系统进一步展现出优于OpenVLA、OpenPi及LeRobot默认策略服务器的性能。VLAgents已在https://github.com/RobotControlStack/vlagents开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The deployment of Vision-Language-Action models in robotics is hindered by fragmented interfaces and communication latency in distributed systems. To address this, the authors introduce VLAgents, a modular policy server that provides a unified Gymnasium-style API for VLA inference and features a communication layer that adaptively uses zero-copy shared memory for local simulation or compressed streaming for remote hardware. Experimental integration with seven policies, including OpenVLA and Pi Zero, demonstrates that VLAgents outperforms the default policy servers of OpenVLA, OpenPi, and LeRobot in benchmarks involving both local and remote communication.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型在机器人领域的部署因分布式系统中接口碎片化和通信延迟而受阻。为此，研究者提出了VLAgents，这是一个模块化的策略服务器，为VLA推理提供了统一的Gymnasium风格接口，其通信层能自适应地使用零拷贝共享内存进行本地高速仿真，并采用压缩流进行远程硬件通信。通过集成包括OpenVLA和Pi Zero在内的七种策略进行实验验证，结果表明，在包含本地和远程通信的基准测试中，VLAgents的性能优于OpenVLA、OpenPi和LeRobot提供的默认策略服务器。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</div>
<div class="meta-line">Authors: Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, Younggyo Seo</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-29T16:41:12+00:00 · Latest: 2026-01-16T12:54:08+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00070v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00070v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robot-R1：基于强化学习的机器人具身推理增强框架</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）近期通过结合具身推理与机器人控制，在推动机器人技术发展方面展现出巨大潜力。常见方法采用监督微调（SFT）对机器人控制相关的具身推理任务进行训练。然而，SFT数据集通常基于启发式构建，未针对提升机器人控制能力进行显式优化，且易导致灾难性遗忘与泛化性能下降等问题。为突破这些局限，我们提出Robot-R1——一种利用强化学习专门增强机器人控制具身推理能力的新型框架。该框架基于专家示范生成的当前场景图像与环境元数据，学习预测完成任务所需的下一个关键点状态。受DeepSeek-R1学习方法启发，Robot-R1对基于推理的响应进行采样，并强化那些能产生更准确预测的响应。为系统评估Robot-R1，我们还构建了需要多样化具身推理能力的新基准测试。实验表明，采用Robot-R1训练的模型在具身推理任务上优于SFT方法。尽管仅拥有70亿参数，Robot-R1在空间与运动推理等底层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses limitations in Supervised Fine-Tuning (SFT) for training Large Vision-Language Models (LVLMs) on embodied reasoning tasks for robot control, where SFT datasets are heuristically constructed and can cause catastrophic forgetting and reduced generalization. The proposed Robot-R1 framework employs reinforcement learning to enhance embodied reasoning by learning to predict the next keypoint state for task completion, conditioned on the current scene image and environment metadata from expert demonstrations; it samples reasoning-based responses and reinforces those leading to more accurate predictions, inspired by the DeepSeek-R1 approach. Experimental results on a new benchmark requiring diverse embodied reasoning capabilities demonstrate that Robot-R1 outperforms SFT methods, and despite having only 7B parameters, it surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div>
<div class="mono" style="margin-top:8px">本研究针对使用监督微调训练大型视觉语言模型进行机器人具身推理任务时存在的局限性，即监督微调数据集未针对控制任务优化，且可能导致灾难性遗忘和泛化性能下降。提出的Robot-R1框架采用强化学习来增强具身推理能力，该方法基于专家演示的当前场景图像和环境元数据，学习预测完成任务所需的下一关键点状态；它采样基于推理的响应，并强化那些能带来更准确预测的响应，其思路受DeepSeek-R1方法启发。在一个需要多样化具身推理能力的新基准上的实验结果表明，使用Robot-R1训练的模型在具身推理任务上优于监督微调方法，并且尽管仅有70亿参数，Robot-R1在空间和运动推理等低层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Representation Learning for Alignment of Tactile, Language, and Vision Modalities</div>
<div class="meta-line">Authors: Yiyun Zhou, Mingjing Xu, Jingwei Shi, Quanjiang Li, Jingyuan Chen</div>
<div class="meta-line">First: 2025-11-14T17:34:20+00:00 · Latest: 2026-01-16T12:49:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11512v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.11512v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tactile sensing offers rich and complementary information to vision and language, enabling robots to perceive fine-grained object properties. However, existing tactile sensors lack standardization, leading to redundant features that hinder cross-sensor generalization. Moreover, existing methods fail to fully integrate the intermediate communication among tactile, language, and vision modalities. To address this, we propose TLV-CoRe, a CLIP-based Tactile-Language-Vision Collaborative Representation learning method. TLV-CoRe introduces a Sensor-Aware Modulator to unify tactile features across different sensors and employs tactile-irrelevant decoupled learning to disentangle irrelevant tactile features. Additionally, a Unified Bridging Adapter is introduced to enhance tri-modal interaction within the shared representation space. To fairly evaluate the effectiveness of tactile models, we further propose the RSS evaluation framework, focusing on Robustness, Synergy, and Stability across different methods. Experimental results demonstrate that TLV-CoRe significantly improves sensor-agnostic representation learning and cross-modal alignment, offering a new direction for multimodal tactile representation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉、语言与视觉模态对齐的协同表征学习方法</div>
<div class="mono" style="margin-top:8px">触觉感知为视觉和语言提供了丰富且互补的信息，使机器人能够感知细粒度的物体属性。然而，现有触觉传感器缺乏标准化，导致冗余特征阻碍跨传感器泛化。此外，现有方法未能充分整合触觉、语言与视觉模态间的中间交互。为此，我们提出TLV-CoRe，一种基于CLIP的触觉-语言-视觉协同表征学习方法。TLV-CoRe引入传感器感知调制器以统一不同传感器的触觉特征，并采用触觉无关解耦学习来分离无关触觉特征。同时，引入统一桥接适配器以增强共享表征空间内的三模态交互。为公平评估触觉模型性能，我们进一步提出RSS评估框架，关注不同方法间的鲁棒性、协同性与稳定性。实验结果表明，TLV-CoRe显著提升了传感器无关表征学习与跨模态对齐能力，为多模态触觉表征提供了新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of standardization in tactile sensors and the insufficient integration of tactile, language, and vision modalities, this paper proposes TLV-CoRe, a CLIP-based collaborative representation learning method. It employs a Sensor-Aware Modulator to unify tactile features across different sensors, uses tactile-irrelevant decoupled learning to filter out irrelevant features, and introduces a Unified Bridging Adapter to enhance tri-modal interaction. Evaluated under a new RSS framework focusing on Robustness, Synergy, and Stability, the method demonstrates significant improvements in sensor-agnostic representation learning and cross-modal alignment.</div>
<div class="mono" style="margin-top:8px">为解决触觉传感器缺乏标准化以及触觉、语言和视觉模态间整合不足的问题，本文提出了TLV-CoRe，一种基于CLIP的协作表征学习方法。该方法引入传感器感知调制器来统一不同传感器的触觉特征，并利用触觉无关解耦学习过滤无关信息，同时通过统一桥接适配器增强共享空间中的三模态交互。在专注于鲁棒性、协同性和稳定性的新RSS评估框架下进行测试，该方法在传感器无关表征学习和跨模态对齐方面表现出显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Monitoring of Stochastic Fire Front Processes via Information-seeking Predictive Control</div>
<div class="meta-line">Authors: Savvas Papaioannou, Panayiotis Kolios, Christos G. Panayiotou, Marios M. Polycarpou</div>
<div class="meta-line">First: 2026-01-16T12:21:27+00:00 · Latest: 2026-01-16T12:21:27+00:00</div>
<div class="meta-line">Comments: 2025 IEEE 64th Conference on Decision and Control (CDC)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11231v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11231v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of adaptively monitoring a wildfire front using a mobile agent (e.g., a drone), whose trajectory determines where sensor data is collected and thus influences the accuracy of fire propagation estimation. This is a challenging problem, as the stochastic nature of wildfire evolution requires the seamless integration of sensing, estimation, and control, often treated separately in existing methods. State-of-the-art methods either impose linear-Gaussian assumptions to establish optimality or rely on approximations and heuristics, often without providing explicit performance guarantees. To address these limitations, we formulate the fire front monitoring task as a stochastic optimal control problem that integrates sensing, estimation, and control. We derive an optimal recursive Bayesian estimator for a class of stochastic nonlinear elliptical-growth fire front models. Subsequently, we transform the resulting nonlinear stochastic control problem into a finite-horizon Markov decision process and design an information-seeking predictive control law obtained via a lower confidence bound-based adaptive search algorithm with asymptotic convergence to the optimal policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信息寻求预测控制的随机火锋过程自适应监测</div>
<div class="mono" style="margin-top:8px">本文研究利用移动智能体（如无人机）自适应监测野火锋线的问题，其飞行轨迹决定传感器数据采集位置，进而影响火势传播估计精度。由于野火演化的随机性需要将感知、估计与控制无缝集成——现有方法常将其割裂处理，该问题极具挑战性。当前先进方法或依赖线性高斯假设以建立最优性，或采用近似启发式方案，通常缺乏明确的性能保证。为突破这些局限，我们将火锋监测任务构建为集成感知、估计与控制的随机最优控制问题。针对一类随机非线性椭圆扩张火锋模型，推导出最优递归贝叶斯估计器。进而将所得非线性随机控制问题转化为有限时域马尔可夫决策过程，并通过基于置信下界的自适应搜索算法设计信息寻求预测控制律，该算法具有渐近收敛至最优策略的特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of adaptively monitoring a stochastically evolving wildfire front using a mobile agent like a drone, where existing methods often treat sensing, estimation, and control separately or rely on restrictive assumptions. The proposed method formulates the task as a stochastic optimal control problem, derives an optimal recursive Bayesian estimator for a class of nonlinear elliptical-growth fire models, and transforms the control problem into a finite-horizon Markov decision process. The key experimental result is the design of an information-seeking predictive control law using a lower confidence bound-based adaptive search algorithm, which is proven to converge asymptotically to the optimal policy, providing explicit performance guarantees absent in prior heuristic approaches.</div>
<div class="mono" style="margin-top:8px">本研究旨在利用无人机等移动代理增强对随机野火火线的自适应监测，解决在火灾演化的非线性和随机性下，感知、估计和控制通常被分开处理的整合难题。方法将监测任务表述为一个随机最优控制问题，推导了一类随机非线性椭圆增长火灾模型的最优递归贝叶斯估计器，并将其转化为有限时域马尔可夫决策过程，通过基于置信下界的信息寻求预测控制律求解，该算法具有渐近收敛到最优策略的保证。关键实验结果表明，该方法提供了明确的性能保证，克服了先前依赖线性高斯假设或启发式方法而缺乏此类保证的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">SceneFoundry: Generating Interactive Infinite 3D Worlds</div>
<div class="meta-line">Authors: ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang</div>
<div class="meta-line">First: 2026-01-09T14:33:10+00:00 · Latest: 2026-01-16T11:20:40+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05810v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05810v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://anc891203.github.io/SceneFoundry-Demo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research. project page: https://anc891203.github.io/SceneFoundry-Demo/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SceneFoundry：生成交互式无限3D世界</div>
<div class="mono" style="margin-top:8px">自动生成大规模、交互式且物理真实的3D环境的能力，对于推动机器人学习和具身智能至关重要。然而，现有生成方法往往难以捕捉真实室内环境的功能复杂性，尤其是那些包含对操作和导航至关重要的可动部件的铰接物体。本文提出SceneFoundry，一个语言引导的扩散框架，可为机器人训练生成具有功能化铰接家具和语义多样化布局的公寓级3D世界。通过自然语言提示，LLM模块控制楼层布局生成，而基于扩散的后验采样则高效地从大规模3D资源库中选取铰接资产填充场景。为确保物理可用性，SceneFoundry采用可微分引导函数来调控物体数量、防止铰接碰撞，并为机器人导航维持足够的可通行空间。大量实验表明，我们的框架能在多样场景类型和条件下生成结构有效、语义连贯且功能交互的环境，从而支持可扩展的具身AI研究。项目页面：https://anc891203.github.io/SceneFoundry-Demo/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for automatically generating large-scale, interactive, and physically realistic 3D environments to advance robotic learning and embodied intelligence, addressing the limitations of existing methods in capturing functional complexity, especially with articulated objects. The method, SceneFoundry, is a language-guided diffusion framework that uses an LLM module to control floor layout generation from natural language prompts and employs diffusion-based posterior sampling to populate scenes with articulated assets from 3D repositories, enhanced by differentiable guidance functions to ensure physical usability by regulating object quantity, preventing articulation collisions, and maintaining walkable space. Experimental results show that the framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, facilitating scalable embodied AI research.</div>
<div class="mono" style="margin-top:8px">该研究的动机是，为推进机器人学习和具身智能，需要自动生成大规模、交互式且物理真实的3D环境，以解决现有方法在捕捉功能复杂性（特别是针对可动部件物体）方面的不足。所提出的方法SceneFoundry是一个语言引导的扩散框架，它利用大语言模型模块根据自然语言提示控制平面布局生成，并采用基于扩散的后验采样从大规模3D资源库中为场景填充可动资产，同时通过可微引导函数来调控物体数量、防止关节碰撞并保持足够的机器人可通行空间。实验结果表明，该框架能够生成结构有效、语义连贯且功能交互的环境，适用于多种场景类型和条件，从而支持可扩展的具身人工智能研究。</div>
</details>
</div>
<div class="card">
<div class="title">LeLaR: The First In-Orbit Demonstration of an AI-Based Satellite Attitude Controller</div>
<div class="meta-line">Authors: Kirill Djebko, Tom Baumann, Erik Dilger, Frank Puppe, Sergio Montenegro</div>
<div class="meta-line">First: 2025-12-22T17:00:25+00:00 · Latest: 2026-01-16T10:19:54+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication. 55 pages, 27 figures, 29 tables. The maneuver telemetry datasets generated and analyzed during this work are available in the GitHub repository under https://github.com/kdjebko/lelar-in-orbit-data</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19576v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.19576v3">PDF</a> · <a href="https://github.com/kdjebko/lelar-in-orbit-data">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Attitude control is essential for many satellite missions. Classical controllers, however, are time-consuming to design and sensitive to model uncertainties and variations in operational boundary conditions. Deep Reinforcement Learning (DRL) offers a promising alternative by learning adaptive control strategies through autonomous interaction with a simulation environment. Overcoming the Sim2Real gap, which involves deploying an agent trained in simulation onto the real physical satellite, remains a significant challenge. In this work, we present the first successful in-orbit demonstration of an AI-based attitude controller for inertial pointing maneuvers. The controller was trained entirely in simulation and deployed to the InnoCube 3U nanosatellite, which was developed by the Julius-Maximilians-Universität Würzburg in cooperation with the Technische Universität Berlin, and launched in January 2025. We present the AI agent design, the methodology of the training procedure, the discrepancies between the simulation and the observed behavior of the real satellite, and a comparison of the AI-based attitude controller with the classical PD controller of InnoCube. Steady-state metrics confirm the robust performance of the AI-based controller during repeated in-orbit maneuvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LeLaR：首个基于人工智能的卫星姿态控制器在轨验证</div>
<div class="mono" style="margin-top:8px">姿态控制对多数卫星任务至关重要。然而，传统控制器设计耗时，且对模型不确定性与运行边界条件变化敏感。深度强化学习通过模拟环境自主交互学习自适应控制策略，提供了有前景的替代方案。克服仿真到现实的鸿沟——即将仿真训练的智能体部署至真实物理卫星——仍是重大挑战。本研究首次成功演示了基于人工智能的姿态控制器在惯性指向机动中的在轨验证。该控制器完全在仿真环境中训练，并部署于由维尔茨堡大学与柏林工业大学合作研发、于2025年1月发射的InnoCube 3U纳卫星。我们介绍了人工智能智能体设计、训练流程方法、仿真与真实卫星观测行为的差异，以及基于人工智能的姿态控制器与InnoCube传统PD控制器的对比。稳态指标证实了基于人工智能的控制器在重复在轨机动中的鲁棒性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the limitations of classical satellite attitude controllers, which are labor-intensive to design and vulnerable to model uncertainties and changing operational conditions, by proposing an AI-based alternative using Deep Reinforcement Learning (DRL). The method involves training a DRL agent entirely in simulation to learn adaptive control strategies and then deploying it directly onto the InnoCube 3U nanosatellite, overcoming the Sim2Real transfer challenge. Experimental results from the first in-orbit demonstration show that the AI-based controller achieved robust performance during inertial pointing maneuvers, with steady-state metrics confirming its effectiveness in comparison to the satellite&#x27;s classical PD controller.</div>
<div class="mono" style="margin-top:8px">本研究针对传统卫星姿态控制器设计耗时、对模型不确定性和操作条件变化敏感的问题，提出了一种基于深度强化学习的AI控制器。该方法通过在仿真环境中完全训练控制器，并首次在轨部署于InnoCube 3U纳卫星上进行了验证。实验结果表明，在重复的惯性指向机动中，AI控制器表现出稳健的性能，其稳态指标证实了相较于传统PD控制器的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Quadrupedal Locomotion for a Heavy Hydraulic Robot Using an Actuator Model</div>
<div class="meta-line">Authors: Minho Lee, Hyeonseok Kim, Jin Tak Kim, Sangshin Park, Jeong Hyun Lee, Jungsan Cho, Jemin Hwangbo</div>
<div class="meta-line">Venue: IEEE Robotics and Automation Letters (Volume: 10, Issue: 12, December 2025)</div>
<div class="meta-line">First: 2026-01-16T10:01:09+00:00 · Latest: 2026-01-16T10:01:09+00:00</div>
<div class="meta-line">Comments: 9 pages, Accepted to IEEE Robotics and Automation Letters (RA-L) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The simulation-to-reality (sim-to-real) transfer of large-scale hydraulic robots presents a significant challenge in robotics because of the inherent slow control response and complex fluid dynamics. The complex dynamics result from the multiple interconnected cylinder structure and the difference in fluid rates of the cylinders. These characteristics complicate detailed simulation for all joints, making it unsuitable for reinforcement learning (RL) applications. In this work, we propose an analytical actuator model driven by hydraulic dynamics to represent the complicated actuators. The model predicts joint torques for all 12 actuators in under 1 microsecond, allowing rapid processing in RL environments. We compare our model with neural network-based actuator models and demonstrate the advantages of our model in data-limited scenarios. The locomotion policy trained in RL with our model is deployed on a hydraulic quadruped robot, which is over 300 kg. This work is the first demonstration of a successful transfer of stable and robust command-tracking locomotion with RL on a heavy hydraulic quadruped robot, demonstrating advanced sim-to-real transferability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于执行器模型的重型液压机器人四足运动学习</div>
<div class="mono" style="margin-top:8px">大型液压机器人的仿真到现实迁移因固有的控制响应迟缓和复杂流体动力学而成为机器人学的重要挑战。这种复杂性源于多缸互联结构及各缸流体速率的差异，使得对所有关节进行精细仿真变得困难，不适用于强化学习应用。本研究提出一种基于液压动力学的解析执行器模型来表征复杂执行器。该模型能在1微秒内预测全部12个执行器的关节扭矩，满足强化学习环境的快速处理需求。通过与基于神经网络的执行器模型对比，验证了本模型在数据受限场景下的优势。基于该模型训练的强化学习运动策略已部署于自重超300公斤的液压四足机器人。本工作首次在重型液压四足机器人上实现了基于强化学习的稳定鲁棒指令跟踪运动策略的成功迁移，展现了先进的仿真到现实迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of simulation-to-reality transfer for large, heavy hydraulic quadruped robots, where complex fluid dynamics and slow control responses hinder accurate simulation and reinforcement learning. The method introduces an analytical actuator model based on hydraulic dynamics to efficiently predict joint torques for all 12 actuators in under a microsecond, enabling rapid policy training. Experimental results show this model outperforms neural network alternatives in data-limited settings, and the trained locomotion policy was successfully deployed on a 300 kg hydraulic robot, marking the first demonstration of stable, command-tracking locomotion via reinforcement learning on such a platform.</div>
<div class="mono" style="margin-top:8px">本研究针对大型液压机器人应用强化学习所面临的挑战，即复杂的流体动力学和缓慢的控制响应阻碍了精确仿真和仿真到现实的迁移。作者提出了一种基于液压动力学的解析执行器模型，能在微秒内高效预测所有12个执行器的关节扭矩，从而为强化学习训练提供快速仿真。实验结果表明，该模型在数据有限的情况下优于基于神经网络的替代方案，并且由此训练出的运动策略成功部署在一台300公斤重的液压四足机器人上，首次实现了在此类重型平台上稳定、可追踪指令的强化学习运动控制。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Marker Search for Autonomous Drone Landing in Diverse Urban Environments</div>
<div class="meta-line">Authors: Jiaohong Yao, Linfeng Liang, Yao Deng, Xi Zheng, Richard Han, Yuankai Qi</div>
<div class="meta-line">First: 2026-01-16T08:24:23+00:00 · Latest: 2026-01-16T08:24:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11078v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Marker-based landing is widely used in drone delivery and return-to-base systems for its simplicity and reliability. However, most approaches assume idealized landing site visibility and sensor performance, limiting robustness in complex urban settings. We present a simulation-based evaluation suite on the AirSim platform with systematically varied urban layouts, lighting, and weather to replicate realistic operational diversity. Using onboard camera sensors (RGB for marker detection and depth for obstacle avoidance), we benchmark two heuristic coverage patterns and a reinforcement learning-based agent, analyzing how exploration strategy and scene complexity affect success rate, path efficiency, and robustness. Results underscore the need to evaluate marker-based autonomous landing under diverse, sensor-relevant conditions to guide the development of reliable aerial navigation systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多样化城市环境中基于视觉标记的无人机自主着陆搜索</div>
<div class="mono" style="margin-top:8px">基于标记的着陆因其简单可靠，广泛应用于无人机配送与返航系统。然而，多数方法假设着陆点可见性与传感器性能处于理想状态，限制了其在复杂城市环境中的鲁棒性。本研究基于AirSim平台构建了模拟评估套件，通过系统调整城市布局、光照与天气条件，复现真实场景的多样性。利用机载摄像头传感器（RGB用于标记检测，深度用于避障），我们对比了两种启发式覆盖模式与一个基于强化学习的智能体，分析探索策略和场景复杂度对成功率、路径效率及鲁棒性的影响。结果强调，需在多样化且与传感器相关的条件下评估基于标记的自主着陆，以指导可靠空中导航系统的开发。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing marker-based drone landing systems, which often assume ideal conditions and thus lack robustness in complex urban environments. The method employs a simulation-based evaluation suite on the AirSim platform, systematically varying urban layouts, lighting, and weather to replicate realistic diversity, and benchmarks two heuristic coverage patterns alongside a reinforcement learning agent using onboard RGB and depth sensors for marker detection and obstacle avoidance. Key experimental findings reveal that exploration strategy and scene complexity significantly impact success rate, path efficiency, and robustness, highlighting the necessity of evaluating autonomous landing under diverse, sensor-relevant conditions to inform reliable aerial navigation development.</div>
<div class="mono" style="margin-top:8px">本研究针对当前基于标记的无人机着陆系统在理想化假设下缺乏复杂城市环境鲁棒性的问题，旨在提升其在多样化场景下的可靠性。方法上，研究基于AirSim平台开发了模拟评估套件，系统性地改变城市布局、光照和天气条件以模拟真实操作多样性，并利用机载RGB和深度相机分别进行标记检测与避障。实验通过比较两种启发式覆盖模式和一种强化学习智能体，分析了探索策略与场景复杂度对成功率、路径效率和鲁棒性的影响，结果表明必须在多样化的传感器相关条件下评估标记着陆系统，以指导可靠空中导航系统的开发。</div>
</details>
</div>
<div class="card">
<div class="title">A3D: Adaptive Affordance Assembly with Dual-Arm Manipulation</div>
<div class="meta-line">Authors: Jiaqi Liang, Yue Chen, Qize Yu, Yan Shen, Haipeng Zhang, Hao Dong, Ruihai Wu</div>
<div class="meta-line">First: 2026-01-16T08:21:42+00:00 · Latest: 2026-01-16T08:21:42+00:00</div>
<div class="meta-line">Comments: AAAI2026 oral</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11076v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11076v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Furniture assembly is a crucial yet challenging task for robots, requiring precise dual-arm coordination where one arm manipulates parts while the other provides collaborative support and stabilization. To accomplish this task more effectively, robots need to actively adapt support strategies throughout the long-horizon assembly process, while also generalizing across diverse part geometries. We propose A3D, a framework which learns adaptive affordances to identify optimal support and stabilization locations on furniture parts. The method employs dense point-level geometric representations to model part interaction patterns, enabling generalization across varied geometries. To handle evolving assembly states, we introduce an adaptive module that uses interaction feedback to dynamically adjust support strategies during assembly based on previous interactions. We establish a simulation environment featuring 50 diverse parts across 8 furniture types, designed for dual-arm collaboration evaluation. Experiments demonstrate that our framework generalizes effectively to diverse part geometries and furniture categories in both simulation and real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A3D：基于双臂操作的适应性可供性装配框架</div>
<div class="mono" style="margin-top:8px">家具装配对机器人而言是一项关键且具有挑战性的任务，需要精确的双臂协调——一只手臂操作零件，另一只手臂提供协作支撑与稳定。为更高效完成该任务，机器人需在长时程装配过程中主动调整支撑策略，并适应不同零件的几何形态。本文提出A3D框架，通过学习适应性可供性来识别家具零件上的最优支撑与稳定位置。该方法采用稠密点级几何表征建模零件交互模式，实现对多样化几何结构的泛化。为应对动态变化的装配状态，我们引入自适应模块，利用交互反馈在装配过程中基于历史交互动态调整支撑策略。我们构建了包含8类家具、50种异质零件的仿真环境，专为双臂协作评估设计。实验表明，该框架在仿真与真实场景中均能有效泛化至不同几何形态的零件及家具类别。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of furniture assembly for robots, which requires precise dual-arm coordination where one arm manipulates parts while the other provides adaptive support and stabilization. The proposed A3D framework learns adaptive affordances to identify optimal support locations on furniture parts using dense point-level geometric representations, enabling generalization across varied part geometries, and incorporates an adaptive module that dynamically adjusts support strategies during assembly based on interaction feedback. Experimental evaluation in a simulation environment with 50 diverse parts across 8 furniture types demonstrates that the framework effectively generalizes to diverse part geometries and furniture categories in both simulation and real-world settings.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人执行家具组装任务时面临的挑战，该任务需要精确的双臂协调，其中一只手臂操作部件，另一只手臂提供自适应支撑和稳定。提出的A3D框架通过学习自适应功能可供性，利用密集点级几何表示来识别家具部件上的最佳支撑位置，从而能够泛化到不同的部件几何形状，并包含一个自适应模块，可根据组装过程中的实时交互反馈动态调整支撑策略。在包含8种家具类型、50个多样化部件的仿真环境以及真实世界测试中的实验评估表明，该方法能有效泛化到不同的部件几何形状和家具类别。</div>
</details>
</div>
<div class="card">
<div class="title">H-AIM: Orchestrating LLMs, PDDL, and Behavior Trees for Hierarchical Multi-Robot Planning</div>
<div class="meta-line">Authors: Haishan Zeng, Peng Li</div>
<div class="meta-line">First: 2026-01-16T07:59:50+00:00 · Latest: 2026-01-16T07:59:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11063v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In embodied artificial intelligence, enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions remains a critical challenge. While large language models (LLMs) show promise in instruction parsing and preliminary planning, they exhibit limitations in long-term reasoning and dynamic multi-robot coordination. We propose Hierarchical Autonomous Intelligent Multi-Robot Planning(H-AIM), a novel embodied multi-robot task planning framework that addresses these issues through a three-stage cascaded architecture: 1) It leverages an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, thereby transforming commands into formal planning problems; 2) It combines the semantic reasoning of LLMs with the search capabilities of a classical planner to produce optimized action sequences; 3) It compiles the resulting plan into behavior trees for reactive control. The framework supports dynamically sized heterogeneous robot teams via a shared blackboard mechanism for communication and state synchronization. To validate our approach, we introduce the MACE-THOR benchmark dataset, comprising 42 complex tasks across 8 distinct household layouts. Experimental results demonstrate that H-AIM achieves a remarkable performance improvement, elevating the task success rate from 12% to 55% and boosting the goal condition recall from 32% to 72% against the strongest baseline, LaMMA-P.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>H-AIM：融合大语言模型、PDDL与行为树的分层多机器人规划框架</div>
<div class="mono" style="margin-top:8px">在具身人工智能领域，如何使异构机器人团队根据高层指令执行长时程任务仍是关键挑战。大语言模型在指令解析和初步规划方面展现出潜力，但在长期推理和动态多机器人协调方面存在局限。本文提出分层自主智能多机器人规划框架H-AIM，通过三级级联架构解决这些问题：1）利用大语言模型解析指令并生成规划领域定义语言问题描述，将命令转化为形式化规划问题；2）结合大语言模型的语义推理能力与经典规划器的搜索能力，生成优化行动序列；3）将规划结果编译为行为树以实现反应式控制。该框架通过共享黑板机制支持动态规模的异构机器人团队通信与状态同步。为验证方法，我们构建了MACE-THOR基准数据集，涵盖8种不同家居布局中的42项复杂任务。实验结果表明，相较于最强基线LaMMA-P，H-AIM实现了显著性能提升：任务成功率从12%提升至55%，目标条件召回率从32%提升至72%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling heterogeneous robot teams to execute long-horizon tasks from high-level instructions in embodied AI, where large language models (LLMs) alone struggle with long-term reasoning and multi-robot coordination. The proposed H-AIM framework employs a three-stage cascaded architecture: it first uses an LLM to parse instructions and generate Planning Domain Definition Language (PDDL) problem descriptions, then combines LLM semantic reasoning with a classical planner to produce optimized action sequences, and finally compiles the plan into behavior trees for reactive control, supporting dynamic teams via a shared blackboard. Experiments on the MACE-THOR benchmark dataset of 42 complex household tasks show H-AIM significantly outperforms the strongest baseline, LaMMA-P, raising the task success rate from 12% to 55% and goal condition recall from 32% to 72%.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身人工智能中异构机器人团队执行高层指令的长期任务规划难题，其中大语言模型（LLM）在长期推理和多机器人协调方面存在局限。提出的H-AIM框架采用三级级联架构：首先利用LLM解析指令并生成规划域定义语言（PDDL）问题描述，将命令转化为形式化规划问题；然后结合LLM的语义推理与经典规划器的搜索能力，生成优化后的动作序列；最后将规划结果编译为行为树以实现反应式控制，并通过共享黑板机制支持动态异构机器人团队的通信与状态同步。在新构建的MACE-THOR基准数据集（包含8种不同家庭布局中的42项复杂任务）上的实验结果表明，H-AIM相比最强基线LaMMA-P实现了显著性能提升，任务成功率从12%提高到55%，目标条件召回率从32%提升至72%。</div>
</details>
</div>
<div class="card">
<div class="title">Haptic Light-Emitting Diodes: Miniature, Luminous Tactile Actuators</div>
<div class="meta-line">Authors: Max Linnander, Yon Visell</div>
<div class="meta-line">First: 2026-01-16T07:18:18+00:00 · Latest: 2026-01-16T07:18:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11043v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Haptic Light-Emitting Diodes (HLEDs), luminous thermopneumatic actuators that directly convert pulsed light into mechanical forces and displacements. Each device packages a miniature surface-mount LED in a gas-filled cavity that contains a low-inertia graphite photoabsorber. The cavity is sealed by an elastic membrane, which functions as a working diaphragm. Brief optical pulses heat the photoabsorber, which heats the gas. The resulting rapid pressure increases generate forces and displacements at the working diaphragm. Millimeter-scale HLEDs produce forces exceeding 0.4 N and displacements of 1 mm at low voltages, with 5 to 100 ms response times, making them attractive as actuators providing tactile feedback in human-machine interfaces. Perceptual testing revealed that the strength of tactile feedback increased linearly with optical power. HLEDs devices are mechanically simple and efficient to fabricate. Unusually, these actuators are also light-emitting, as a fraction of optical energy is transmitted through the membrane. These opto-mechanical actuators have many potential applications in tactile displays, human interface engineering, wearable computing, and other areas.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉发光二极管：微型、发光的触觉致动器</div>
<div class="mono" style="margin-top:8px">我们提出触觉发光二极管（HLEDs），这是一种发光的温控气动致动器，能将脉冲光直接转换为机械力和位移。每个器件将微型表面贴装LED封装在充满气体的腔体内，腔体包含低惯量石墨光吸收体，并由弹性膜密封作为工作隔膜。短暂的光脉冲加热光吸收体，进而加热气体，产生的快速压力升高在工作隔膜处生成力和位移。毫米级HLEDs在低电压下可产生超过0.4 N的力和1 mm的位移，响应时间为5至100 ms，使其成为人机界面中提供触觉反馈的理想致动器。感知测试表明触觉反馈强度随光功率线性增加。HLEDs结构简单且易于制造。独特的是，这些致动器同时具有发光特性，部分光能可通过隔膜透射。这类光机械致动器在触觉显示器、人机界面工程、可穿戴计算等领域具有广泛的应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research introduces Haptic Light-Emitting Diodes (HLEDs) to create compact, luminous actuators for tactile feedback in human-machine interfaces. The method employs a miniature LED sealed in a gas-filled cavity with a graphite photoabsorber and an elastic membrane; pulsed light heats the absorber, rapidly increasing gas pressure to drive the membrane. Key experimental results show that millimeter-scale HLEDs generate forces over 0.4 N and displacements of 1 mm with 5–100 ms response times at low voltages, and perceptual tests confirm tactile feedback strength increases linearly with optical power, while the devices remain mechanically simple to fabricate and inherently light-emitting.</div>
<div class="mono" style="margin-top:8px">本研究介绍了触觉发光二极管（HLEDs），旨在为人类-机器界面开发紧凑、发光的触觉反馈执行器。其方法是将微型表面贴装LED封装在含有石墨光吸收剂和弹性膜的气体填充腔内；脉冲光加热吸收剂，使气体压力迅速升高从而驱动膜片运动。关键实验结果表明，毫米级HLEDs在低电压下可产生超过0.4 N的力和1 mm的位移，响应时间为5至100毫秒，感知测试显示触觉反馈强度随光功率线性增加，同时该器件还能通过膜片透射部分光能而发光。</div>
</details>
</div>
<div class="card">
<div class="title">Crane Lowering Guidance Using a Attachable Camera Module for Driver Vision Support</div>
<div class="meta-line">Authors: HyoJae Kang, SunWoo Ahn, InGyu Choi, GeonYeong Go, KunWoo Son, Min-Sung Kang</div>
<div class="meta-line">First: 2026-01-16T06:44:17+00:00 · Latest: 2026-01-16T06:44:17+00:00</div>
<div class="meta-line">Comments: Presented at ICCR 2025(International COnference on Control and Robotics 2025). Submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11026v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11026v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cranes have long been essential equipment for lifting and placing heavy loads in construction projects. This study focuses on the lowering phase of crane operation, the stage in which the load is moved to the desired location. During this phase, a constant challenge exists: the load obstructs the operator&#x27;s view of the landing point. As a result, operators traditionally have to rely on verbal or gestural instructions from ground personnel, which significantly impacts site safety. To alleviate this constraint, the proposed system incorporates a attachable camera module designed to be attached directly to the load via a suction cup. This module houses a single-board computer, battery, and compact camera. After installation, it streams and processes images of the ground directly below the load in real time to generate installation guidance. Simultaneously, this guidance is transmitted to and monitored by a host computer. Preliminary experiments were conducted by attaching this module to a test object, confirming the feasibility of real-time image acquisition and transmission. This approach has the potential to significantly improve safety on construction sites by providing crane operators with an instant visual reference of hidden landing zones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>采用可附加摄像头模块的起重机下降引导系统以支持驾驶员视野</div>
<div class="mono" style="margin-top:8px">起重机长期以来是建筑项目中吊装和放置重物的关键设备。本研究聚焦于起重机作业的下降阶段，即载荷移至目标位置的过程。在此阶段，操作员常面临载荷遮挡着陆点视野的持续挑战，传统上需依赖地面人员的口头或手势指令，严重影响工地安全。为缓解此限制，本研究提出一种可通过吸盘直接附着于载荷的可附加摄像头模块系统。该模块集成单板计算机、电池及紧凑型摄像头，安装后可实时采集并处理载荷正下方地面图像以生成安装引导信息，同时将引导数据传输至主机进行监控。初步实验通过将模块附着于测试物体，验证了实时图像采集与传输的可行性。该方法有望通过为起重机操作员提供隐蔽着陆区的即时视觉参考，显著提升建筑工地的安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the safety challenge in crane operations where the load obstructs the operator&#x27;s view of the landing point during the lowering phase, forcing reliance on ground personnel. The proposed method uses an attachable camera module with a suction cup to mount on the load; it contains a single-board computer, battery, and camera to stream and process real-time images of the ground below for generating installation guidance, which is monitored on a host computer. Preliminary experiments with the module attached to a test object confirmed the feasibility of real-time image acquisition and transmission, indicating potential for improved site safety by providing operators with an instant visual reference to hidden landing zones.</div>
<div class="mono" style="margin-top:8px">本研究针对起重机在吊装下降阶段，负载遮挡操作员视线、迫使其依赖地面人员指挥的安全隐患。提出的方法采用一个带吸盘的可附着摄像头模块，安装在负载上；该模块包含单板计算机、电池和摄像头，用于实时采集和处理负载正下方的地面图像，生成引导信息并显示在主机上。初步实验将模块附着于测试物体，证实了实时图像采集与传输的可行性，表明该系统有潜力通过为操作员提供隐藏着陆区的即时视觉参考，显著提升工地安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Off Policy Lyapunov Stability in Reinforcement Learning</div>
<div class="meta-line">Authors: Sarvan Gill, Daniela Constantinescu</div>
<div class="meta-line">Venue: CORL</div>
<div class="meta-line">First: 2025-09-11T21:34:08+00:00 · Latest: 2026-01-16T02:02:30+00:00</div>
<div class="meta-line">Comments: Conference on Robot Learning (CORL) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09863v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.09863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional reinforcement learning lacks the ability to provide stability guarantees. More recent algorithms learn Lyapunov functions alongside the control policies to ensure stable learning. However, the current self-learned Lyapunov functions are sample inefficient due to their on-policy nature. This paper introduces a method for learning Lyapunov functions off-policy and incorporates the proposed off-policy Lyapunov function into the Soft Actor Critic and Proximal Policy Optimization algorithms to provide them with a data efficient stability certificate. Simulations of an inverted pendulum and a quadrotor illustrate the improved performance of the two algorithms when endowed with the proposed off-policy Lyapunov function.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化学习中的离策略李雅普诺夫稳定性</div>
<div class="mono" style="margin-top:8px">传统强化学习缺乏稳定性保证能力。较新的算法在学习控制策略的同时学习李雅普诺夫函数以确保稳定学习。然而，当前自学习的李雅普诺夫函数因其同策略特性而样本效率低下。本文提出一种离策略学习李雅普诺夫函数的方法，并将所提出的离策略李雅普诺夫函数融入软演员评论家算法和近端策略优化算法，为二者提供数据高效的稳定性证明。通过倒立摆和四旋翼飞行器的仿真实验表明，在配备所提出的离策略李雅普诺夫函数后，两种算法的性能均得到提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the lack of stability guarantees in traditional reinforcement learning and the sample inefficiency of on-policy methods for learning Lyapunov functions, this paper proposes an off-policy method for learning Lyapunov functions. The method integrates the learned Lyapunov function into the Soft Actor-Critic and Proximal Policy Optimization algorithms to provide a data-efficient stability certificate. Experimental simulations on an inverted pendulum and a quadrotor demonstrate that both algorithms achieve improved performance when equipped with the proposed off-policy Lyapunov function.</div>
<div class="mono" style="margin-top:8px">针对传统强化学习缺乏稳定性保证，以及现有基于策略的李雅普诺夫函数学习方法样本效率低的问题，本文提出了一种离策略学习李雅普诺夫函数的方法，以提供数据高效的稳定性证书。该方法将所学的离策略李雅普诺夫函数集成到Soft Actor-Critic和Proximal Policy Optimization算法中。在倒立摆和四旋翼飞行器上的仿真实验表明，增强后的算法在获得稳定性保证的同时，性能得到了提升。</div>
</details>
</div>
<div class="card">
<div class="title">EqVIO: An Equivariant Filter for Visual Inertial Odometry</div>
<div class="meta-line">Authors: Pieter van Goor, Robert Mahony</div>
<div class="meta-line">Venue: IEEE Transactions on Robotics, vol. 39, no. 5, pp. 3567-3585, Oct. 2023</div>
<div class="meta-line">First: 2022-05-04T10:14:54+00:00 · Latest: 2026-01-16T01:38:06+00:00</div>
<div class="meta-line">Comments: 28 pages, 17 figures, published in IEEE TRO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2205.01980v3">Abs</a> · <a href="https://arxiv.org/pdf/2205.01980v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual-Inertial Odometry (VIO) is the problem of estimating a robot&#x27;s trajectory by combining information from an inertial measurement unit (IMU) and a camera, and is of great interest to the robotics community. This paper develops a novel Lie group symmetry for the VIO problem and applies the recently proposed equivariant filter. The proposed symmetry is compatible with the invariance of the VIO reference frame, leading to improved filter consistency. The bias-free IMU dynamics are group-affine, ensuring that filter linearisation errors depend only on the bias estimation error and measurement noise. Furthermore, visual measurements are equivariant with respect to the symmetry, enabling the application of the higher-order equivariant output approximation to reduce approximation error in the filter update equation. As a result, the equivariant filter (EqF) based on this Lie group is a consistent estimator for VIO with lower linearisation error in the propagation of state dynamics and a higher order equivariant output approximation than standard formulations. Experimental results on the popular EuRoC and UZH FPV datasets demonstrate that the proposed system outperforms other state-of-the-art VIO algorithms in terms of both speed and accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EqVIO：一种用于视觉惯性里程计的等变滤波器</div>
<div class="mono" style="margin-top:8px">视觉惯性里程计（VIO）是通过结合惯性测量单元（IMU）和摄像机的信息来估计机器人轨迹的问题，对机器人学界具有重要意义。本文为VIO问题提出了一种新颖的李群对称性，并应用了近期提出的等变滤波器。所提出的对称性与VIO参考系的不变性兼容，从而提升了滤波器的一致性。无偏置IMU动力学具有群仿射特性，确保滤波器线性化误差仅取决于偏置估计误差和测量噪声。此外，视觉测量相对于该对称性具有等变性，使得能够应用高阶等变输出近似来减少滤波器更新方程中的近似误差。因此，基于此李群的等变滤波器（EqF）是VIO的一致估计器，在状态动力学传播中具有更低的线性化误差，且其等变输出近似阶数高于标准方法。在主流EuRoC和UZH FPV数据集上的实验结果表明，所提出系统在速度和精度方面均优于其他最先进的VIO算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for improved consistency and accuracy in Visual-Inertial Odometry (VIO) by developing a novel Lie group symmetry compatible with VIO reference frame invariance. The method applies an equivariant filter (EqF) where bias-free IMU dynamics are group-affine, limiting linearization errors to bias estimation and measurement noise, while visual measurements&#x27; equivariance enables higher-order output approximations. Experimental evaluation on EuRoC and UZH FPV datasets demonstrates that this approach outperforms state-of-the-art VIO algorithms in both speed and accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉惯性里程计（VIO）中一致性和精度提升的需求，通过设计一种适用于VIO问题的新型李群对称性来改进性能。该方法应用了基于该对称性的等变滤波器（EqF），其中无偏置的IMU动力学具有群仿射特性，且视觉测量具有等变性，从而在状态传播阶段降低了线性化误差，并在更新阶段实现了更高阶的输出近似。在EuRoC和UZH FPV数据集上的实验结果表明，所提出的EqVIO系统在速度和精度方面均优于其他先进的VIO算法。</div>
</details>
</div>
<div class="card">
<div class="title">Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation</div>
<div class="meta-line">Authors: Zhixian Xie, Yu Xiang, Michael Posa, Wanxin Jin</div>
<div class="meta-line">Venue: RSS</div>
<div class="meta-line">First: 2026-01-16T01:20:15+00:00 · Latest: 2026-01-16T01:20:15+00:00</div>
<div class="meta-line">Comments: 13 Pages, Plan to submit RSS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10930v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10930v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何处触碰，如何接触：面向几何感知长时程灵巧操作的层次化RL-MPC框架</div>
<div class="mono" style="margin-top:8px">接触密集型灵巧操作的一个核心挑战在于需同时处理几何结构、运动学约束及复杂非光滑接触动力学。端到端视觉运动策略虽绕过了这一结构，但通常需要大量数据、仿真到现实的迁移性差，且跨任务/具身的泛化能力弱。我们基于一个简单洞见应对这些局限：灵巧操作本质上是层次化的——高层决策涉及触碰位置（几何）与物体移动（运动学）；底层则通过接触动力学实现该计划。基于此，我们提出一种层次化RL-MPC框架：高层强化学习（RL）策略预测接触意图，这是一种以物体为中心的新型接口，指定（i）物体表面接触位置及（ii）接触后物体层级子目标位姿；底层则基于该接触意图，通过接触隐式模型预测控制（MPC）优化局部接触模式，并利用接触动力学进行重规划，生成稳健驱动物体朝向各子目标的机器人动作。我们在非抓取任务（包括几何泛化推动与物体三维重定向）上评估该框架，其以显著减少的数据量（较端到端基线少10倍）实现近100%成功率，具备高度鲁棒性，并能零样本仿真到现实迁移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of dexterous manipulation, which requires reasoning about geometry, kinematics, and contact dynamics, as end-to-end visuomotor policies often suffer from high data needs, poor sim-to-real transfer, and weak generalization. The method introduces a hierarchical RL-MPC framework: a high-level RL policy predicts a contact intention specifying an object-surface contact location and a post-contact subgoal pose, while a low-level contact-implicit MPC optimizes local contact modes and replans with dynamics to generate robust actions. Experimental results on non-prehensile tasks like pushing and 3D reorientation show near-100% success, a 10x reduction in data compared to end-to-end baselines, high robustness, and zero-shot sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">该研究针对灵巧操作中需联合推理几何、运动学和接触动力学的挑战，其中端到端的视觉运动策略通常存在数据需求大、仿真到现实迁移差和泛化能力弱的问题。提出的方法是一个分层RL-MPC框架：高层强化学习策略预测接触意图，指定物体表面接触位置和接触后子目标位姿；底层接触隐式模型预测控制优化局部接触模式并基于动力学重新规划，以生成鲁棒的机器人动作。在非抓取任务（如推动和三维重定向）上的实验结果表明，该方法实现了接近100%的成功率，所需数据比端到端基线少10倍，并具有鲁棒性能和零样本仿真到现实迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Is open robotics innovation a threat to international peace and security?</div>
<div class="meta-line">Authors: Ludovic Righetti, Vincent Boulanin</div>
<div class="meta-line">Venue: IEEE Robotics &amp; Automation Magazine, vol. 32, no. 4, pp. 42-50, Dec. 2025</div>
<div class="meta-line">First: 2026-01-15T22:07:28+00:00 · Latest: 2026-01-15T22:07:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10877v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open access to publication, software and hardware is central to robotics: it lowers barriers to entry, supports reproducible science and accelerates reliable system development. However, openness also exacerbates the inherent dual-use risks associated with research and innovation in robotics. It lowers barriers for states and non-state actors to develop and deploy robotics systems for military use and harmful purposes. Compared to other fields of engineering where dual-use risks are present - e.g., those that underlie the development of weapons of mass destruction (chemical, biological, radiological, and nuclear weapons) and even the field of AI, robotics offers no specific regulation and little guidance as to how research and innovation may be conducted and disseminated responsibly. While other fields can be used for guidance, robotics has its own needs and specificities which have to be taken into account. The robotics community should therefore work toward its own set of sector-specific guidance and possibly regulation. To that end, we propose a roadmap focusing on four practices: a) education in responsible robotics; b) incentivizing risk assessment; c) moderating the diffusion of high-risk material; and d) developing red lines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放式机器人技术创新是否威胁国际和平与安全？</div>
<div class="mono" style="margin-top:8px">开放获取出版物、软件和硬件是机器人技术的核心：它降低了准入门槛，支持可重复的科学实践，并加速了可靠系统的开发。然而，开放性也加剧了机器人研究与创新固有的双重用途风险。它降低了国家和非国家行为体为军事用途和有害目的开发与部署机器人系统的门槛。相较于存在双重用途风险的其他工程领域（例如大规模杀伤性武器——化学、生物、放射性和核武器——的研发基础领域，甚至人工智能领域），机器人技术缺乏具体的监管措施，也鲜有关于如何负责任地进行研究和创新传播的指导。尽管可借鉴其他领域的经验，但机器人技术有其自身需求和特殊性，必须予以考虑。因此，机器人学界应致力于制定一套针对本领域的专门指导方针，并可能推动相关监管。为此，我们提出一个路线图，聚焦四项实践：a) 开展负责任机器人技术教育；b) 激励风险评估；c) 管控高风险材料的传播；d) 设定红线标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the dual-use risks in robotics, where open innovation accelerates development but also lowers barriers for military or harmful applications by state and non-state actors, unlike fields like AI or weapons of mass destruction that have specific regulations. The method involves proposing a roadmap for the robotics community to develop sector-specific guidance and regulation, focusing on four practices: education in responsible robotics, incentivizing risk assessment, moderating diffusion of high-risk material, and developing red lines. The key finding is that robotics lacks tailored regulatory frameworks, necessitating community-driven efforts to address these risks through structured practices to ensure responsible research and innovation dissemination.</div>
<div class="mono" style="margin-top:8px">本研究动机源于机器人技术的双重用途风险：开放获取出版物、软件和硬件虽能促进创新，但也降低了国家和非国家行为者将其用于军事或有害目的的门槛，与人工智能或大规模杀伤性武器等领域相比存在监管空白。研究方法是为机器人学界制定具体路线图，聚焦四项实践：负责任机器人技术教育、激励风险评估、管控高风险材料传播、以及设定红线界限。核心结论是，尽管其他领域的现有框架可提供参考，但机器人技术具有独特性，需要学界自主建立责任规范，以平衡开放创新与安全关切。</div>
</details>
</div>
<div class="card">
<div class="title">IMU-based Real-Time Crutch Gait Phase and Step Detections in Lower-Limb Exoskeletons</div>
<div class="meta-line">Authors: Anis R. Shakkour, David Hexner, Yehuda Bitton, Avishai Sintov</div>
<div class="meta-line">First: 2026-01-15T20:07:54+00:00 · Latest: 2026-01-15T20:07:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10832v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lower limb exoskeletons and prostheses require precise, real time gait phase and step detections to ensure synchronized motion and user safety. Conventional methods often rely on complex force sensing hardware that introduces control latency. This paper presents a minimalist framework utilizing a single, low cost Inertial-Measurement Unit (IMU) integrated into the crutch hand grip, eliminating the need for mechanical modifications. We propose a five phase classification system, including standard gait phases and a non locomotor auxiliary state, to prevent undesired motion. Three deep learning architectures were benchmarked on both a PC and an embedded system. To improve performance under data constrained conditions, models were augmented with a Finite State Machine (FSM) to enforce biomechanical consistency. The Temporal Convolutional Network (TCN) emerged as the superior architecture, yielding the highest success rates and lowest latency. Notably, the model generalized to a paralyzed user despite being trained exclusively on healthy participants. Achieving a 94% success rate in detecting crutch steps, this system provides a high performance, cost effective solution for real time exoskeleton control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于IMU的下肢外骨骼实时拐杖步态相位与步态检测</div>
<div class="mono" style="margin-top:8px">下肢外骨骼与假肢需要精确的实时步态相位与步态检测，以确保运动同步和用户安全。传统方法常依赖复杂的力传感硬件，这会引入控制延迟。本文提出一种极简框架，利用集成于拐杖手柄的单个低成本惯性测量单元，无需机械改造。我们提出包含标准步态相位和非运动辅助状态在内的五相位分类系统，以防止非预期运动。在PC和嵌入式系统上对三种深度学习架构进行了基准测试。为提升数据受限条件下的性能，模型通过有限状态机增强以保持生物力学一致性。时序卷积网络表现出最优性能，实现了最高成功率和最低延迟。值得注意的是，该模型虽仅基于健康受试者训练，却能泛化至瘫痪用户。系统在检测拐杖步态时达到94%的成功率，为实时外骨骼控制提供了高性能、低成本的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for precise, real-time gait phase and step detection in lower-limb exoskeletons and prostheses, aiming to overcome the latency and complexity of conventional force-sensing methods. The proposed method integrates a single low-cost IMU into a crutch handgrip and employs a five-phase classification system, including a non-locomotor state, with deep learning models refined by a Finite State Machine for biomechanical consistency. Experimental benchmarking on PC and embedded systems showed that a Temporal Convolutional Network achieved the best performance, with a 94% success rate in crutch step detection and effective generalization to a paralyzed user despite training only on healthy participants.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决下肢外骨骼和假肢对精确、实时步态相位与步态检测的需求，传统力传感方法可能带来控制延迟和复杂性。所提出的方法使用安装在拐杖手柄上的单个低成本惯性测量单元，并引入了一个包含非运动辅助状态在内的五相位分类系统以防止意外运动；在数据受限条件下，对三种深度学习模型进行了基准测试，并使用有限状态机增强其生物力学一致性。实验结果表明，时序卷积网络（TCN）性能最佳，在拐杖步态检测中达到94%的成功率且延迟最低，并且尽管仅使用健康参与者的数据进行训练，该模型仍能泛化至瘫痪用户。</div>
</details>
</div>
<div class="card">
<div class="title">Approximately Optimal Global Planning for Contact-Rich SE(2) Manipulation on a Graph of Reachable Sets</div>
<div class="meta-line">Authors: Simin Liu, Tong Zhao, Bernhard Paus Graesdal, Peter Werner, Jiuguang Wang, John Dolan, Changliu Liu, Tao Pang</div>
<div class="meta-line">First: 2026-01-15T20:00:30+00:00 · Latest: 2026-01-15T20:00:30+00:00</div>
<div class="meta-line">Comments: 17 pages, 14 figures; under submission to IEEE Transactions on Robotics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10827v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">If we consider human manipulation, it is clear that contact-rich manipulation (CRM)-the ability to use any surface of the manipulator to make contact with objects-can be far more efficient and natural than relying solely on end-effectors (i.e., fingertips). However, state-of-the-art model-based planners for CRM are still focused on feasibility rather than optimality, limiting their ability to fully exploit CRM&#x27;s advantages. We introduce a new paradigm that computes approximately optimal manipulator plans. This approach has two phases. Offline, we construct a graph of mutual reachable sets, where each set contains all object orientations reachable from a starting object orientation and grasp. Online, we plan over this graph, effectively computing and sequencing local plans for globally optimized motion. On a challenging, representative contact-rich task, our approach outperforms a leading planner, reducing task cost by 61%. It also achieves a 91% success rate across 250 queries and maintains sub-minute query times, ultimately demonstrating that globally optimized contact-rich manipulation is now practical for real-world tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可达集合图的接触丰富SE(2)操作近似最优全局规划</div>
<div class="mono" style="margin-top:8px">若考察人类操作行为，显然接触丰富操作（CRM）——即利用操作器任意表面与物体接触的能力——远比仅依赖末端执行器（如指尖）更高效自然。然而，当前基于模型的CRM规划器仍侧重于可行性而非最优性，限制了其充分发挥CRM优势的能力。我们提出一种计算近似最优操作器规划的新范式，该方法包含两个阶段：离线阶段构建互达集合图，每个集合包含从起始物体姿态和抓取状态可达的所有物体朝向；在线阶段在该图上进行规划，通过计算并编排局部规划实现全局优化运动。在具有挑战性的典型接触丰富任务中，本方法优于主流规划器，任务成本降低61%。在250次查询中达成91%成功率，并保持亚分钟级查询时间，最终证明全局优化的接触丰富操作已具备实际应用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to move beyond feasibility-focused planning in contact-rich manipulation (CRM) to achieve greater efficiency akin to human dexterity. The method introduces a two-phase paradigm: offline construction of a graph of mutual reachable sets capturing object orientations from given grasps, followed by online planning over this graph to sequence locally optimal motions into a globally optimized plan. Experimental results on a challenging CRM task show the approach reduces task cost by 61% compared to a leading planner, achieves a 91% success rate across 250 queries, and maintains sub-minute query times, demonstrating practical global optimization for real-world CRM.</div>
<div class="mono" style="margin-top:8px">受人类灵巧操作中接触丰富操作（CRM）相比仅使用指尖更高效自然的启发，本研究针对现有基于模型的CRM规划器缺乏最优性的问题，提出了一种新范式。该方法分为两个阶段：离线构建一个互达集合图，其中每个集合包含从起始物体姿态和抓握可达的所有物体朝向；在线则在该图上进行规划，通过编排局部最优运动实现全局优化。在具有挑战性的CRM任务实验中，该方法相比领先规划器将任务成本降低了61%，在250次查询中成功率达到91%，且查询时间保持在分钟以内，证明了全局优化的接触丰富操作在实际任务中的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">SurfSLAM: Sim-to-Real Underwater Stereo Reconstruction For Real-Time SLAM</div>
<div class="meta-line">Authors: Onur Bagoren, Seth Isaacson, Sacchin Sundar, Yung-Ching Sun, Anja Sheppard, Haoyu Ma, Abrar Shariff, Ram Vasudevan, Katherine A. Skinner</div>
<div class="meta-line">First: 2026-01-15T19:25:33+00:00 · Latest: 2026-01-15T19:25:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10814v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Localization and mapping are core perceptual capabilities for underwater robots. Stereo cameras provide a low-cost means of directly estimating metric depth to support these tasks. However, despite recent advances in stereo depth estimation on land, computing depth from image pairs in underwater scenes remains challenging. In underwater environments, images are degraded by light attenuation, visual artifacts, and dynamic lighting conditions. Furthermore, real-world underwater scenes frequently lack rich texture useful for stereo depth estimation and 3D reconstruction. As a result, stereo estimation networks trained on in-air data cannot transfer directly to the underwater domain. In addition, there is a lack of real-world underwater stereo datasets for supervised training of neural networks. Poor underwater depth estimation is compounded in stereo-based Simultaneous Localization and Mapping (SLAM) algorithms, making it a fundamental challenge for underwater robot perception. To address these challenges, we propose a novel framework that enables sim-to-real training of underwater stereo disparity estimation networks using simulated data and self-supervised finetuning. We leverage our learned depth predictions to develop \algname, a novel framework for real-time underwater SLAM that fuses stereo cameras with IMU, barometric, and Doppler Velocity Log (DVL) measurements. Lastly, we collect a challenging real-world dataset of shipwreck surveys using an underwater robot. Our dataset features over 24,000 stereo pairs, along with high-quality, dense photogrammetry models and reference trajectories for evaluation. Through extensive experiments, we demonstrate the advantages of the proposed training approach on real-world data for improving stereo estimation in the underwater domain and for enabling accurate trajectory estimation and 3D reconstruction of complex shipwreck sites.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SurfSLAM：面向实时SLAM的水下立体重建仿真到现实方法</div>
<div class="mono" style="margin-top:8px">定位与建图是水下机器人的核心感知能力。立体相机为直接估计度量深度提供了低成本手段，以支持这些任务。然而，尽管陆地立体深度估计技术近期取得进展，在水下场景中从图像对计算深度仍具挑战性。水下环境中，图像会因光衰减、视觉伪影和动态光照条件而退化。此外，真实水下场景常缺乏对立体深度估计和三维重建有用的丰富纹理。因此，基于陆地数据训练的立体估计网络无法直接迁移至水下领域。同时，缺乏用于神经网络监督训练的真实水下立体数据集。基于立体的即时定位与建图（SLAM）算法中，水下深度估计的不足进一步加剧，这成为水下机器人感知的根本性挑战。为解决这些问题，我们提出一种新颖框架，利用模拟数据和自监督微调实现水下立体视差估计网络的仿真到现实训练。基于习得的深度预测，我们开发了\algname——一种融合立体相机、IMU、气压计和多普勒速度计（DVL）测量的实时水下SLAM新框架。最后，我们通过水下机器人收集了具有挑战性的真实沉船勘测数据集，包含超过24,000组立体图像对，以及用于评估的高质量密集摄影测量模型与参考轨迹。通过大量实验，我们验证了所提训练方法在真实数据上的优势，能有效提升水下立体估计精度，并实现复杂沉船遗址的精确轨迹估计与三维重建。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of accurate stereo depth estimation and SLAM for underwater robots, where image degradation, lack of texture, and absence of real-world training data hinder performance. The method introduces a sim-to-real training framework for disparity estimation using simulated data and self-supervised finetuning, and integrates these learned depths into a real-time SLAM system fusing stereo, IMU, barometer, and DVL data. Experimental results on a new real-world dataset of shipwreck surveys, comprising over 24,000 stereo pairs, demonstrate improved stereo estimation and enable accurate trajectory estimation and 3D reconstruction of complex underwater sites.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决水下机器人立体深度估计与SLAM的准确性问题，其中图像退化、纹理缺乏和真实世界训练数据缺失阻碍了现有方法。所提出的SurfSLAM方法采用模拟到真实的训练框架，利用模拟数据和自监督微调进行视差估计，并将深度预测与IMU、气压计和多普勒测速仪测量融合，实现实时SLAM。在收集的包含超过24,000个立体图像对的沉船调查数据集上的实验结果表明，该方法改善了水下立体估计，并实现了对复杂水下遗址的准确轨迹估计和三维重建。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
