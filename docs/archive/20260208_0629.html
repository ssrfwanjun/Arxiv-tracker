<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-08 06:29</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260208_0629</div>
    <div class="row"><div class="card">
<div class="title">CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</div>
<div class="meta-line">Authors: Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:45+00:00 · Latest: 2026-02-05T18:59:45+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://comm-cp.github.io/">Project1</a> · <a href="https://comm-cp.github.io">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CommCP：基于大语言模型与保形预测的高效多智能体协同通信框架</div>
<div class="mono" style="margin-top:8px">为完成人类以自然语言下达的任务，机器人需解析指令、生成并回答相关问题以理解场景，并操控目标物体。实际部署常需多个具备不同操控能力的异构机器人协同处理不同任务。除专业操控技能外，有效的信息收集对任务完成至关重要。为此，我们将完全协作环境中的信息收集过程形式化为一个尚未充分探索的多智能体多任务具身问答问题，这是经典具身问答任务的新扩展，其中高效通信对避免冗余协作至关重要。针对该问题，我们提出CommCP——一个专为MM-EQA设计的基于大语言模型的去中心化通信框架。该框架采用保形预测技术校准生成信息，从而最小化接收方干扰并提升通信可靠性。为评估框架性能，我们构建了包含多样化照片级真实家庭场景与具身问题的MM-EQA基准测试。实验结果表明，CommCP较基线方法显著提升了任务成功率和探索效率。实验视频、代码及数据集详见项目网站：https://comm-cp.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of coordinating multiple heterogeneous robots to complete natural language assignments in household environments, where effective information gathering and communication are crucial. The authors formalize this as a multi-agent multi-task Embodied Question Answering (MM-EQA) problem and propose CommCP, a decentralized communication framework that uses large language models (LLMs) to generate messages and employs conformal prediction to calibrate them, reducing receiver distractions and improving reliability. Experiments on a new photo-realistic MM-EQA benchmark show that CommCP significantly improves task success rates and exploration efficiency compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多台异构机器人在家庭环境中协作完成自然语言指令的协调问题，其中有效的信息收集与通信至关重要。作者将此形式化为多智能体多任务具身问答问题，并提出了CommCP框架：一种基于大语言模型的去中心化通信方法，利用共形预测校准生成的消息，以减少接收方干扰并提升通信可靠性。在新建的真实感多智能体多任务具身问答基准测试上的实验表明，与基线方法相比，CommCP显著提高了任务成功率和探索效率。</div>
</details>
</div>
<div class="card">
<div class="title">InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</div>
<div class="meta-line">Authors: Sirui Xu, Samuel Schulter, Morteza Ziyadi, Xialin He, Xiaohan Fei, Yu-Xiong Wang, Liangyan Gui</div>
<div class="meta-line">First: 2026-02-05T18:59:27+00:00 · Latest: 2026-02-05T18:59:27+00:00</div>
<div class="meta-line">Comments: Webpage: https://sirui-xu.github.io/InterPrior/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06035v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sirui-xu.github.io/InterPrior/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InterPrior：面向物理人体-物体交互的可扩展生成控制框架</div>
<div class="mono" style="margin-top:8px">人类很少在显式全身运动层面规划与物体的全身交互。高层意图（如可供性）定义目标，而协调的平衡、接触和操作能力可从底层物理与运动先验中自然涌现。扩展此类先验是实现人形机器人在多样化场景中组合并泛化移动操作技能、同时保持物理连贯全身协调的关键。为此，我们提出InterPrior——一个通过大规模模仿预训练与强化学习后训练来学习统一生成控制器的可扩展框架。该框架首先将全参考模仿专家提炼为基于目标条件的变分策略，能够从多模态观测与高层意图重建运动。虽然提炼策略能复现训练行为，但由于大规模人-物交互的广阔配置空间，其泛化能力有限。为此，我们采用物理扰动的数据增强，并通过强化学习微调提升对未见目标与初始状态的适应能力。这些步骤共同将重建的潜在技能整合至有效流形，形成能超越训练数据泛化的运动先验（例如整合与未见物体的交互行为）。我们进一步验证了其在用户交互控制中的有效性及在真实机器人部署中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for humanoid robots to plan whole-body interactions with objects not through explicit movement sequences but via high-level intentions like affordance, relying on underlying physical and motor priors to emerge coordinated balance, contact, and manipulation. The method introduces InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining to distill a goal-conditioned variational policy from multimodal observations and intent, followed by reinforcement learning finetuning with data augmentation via physical perturbations to consolidate latent skills into a valid manifold. Key experimental findings show that this approach enables the motion prior to generalize beyond training data, incorporating interactions with unseen objects, and demonstrates effectiveness for user-interactive control and potential for real robot deployment.</div>
<div class="mono" style="margin-top:8px">该研究的动机是让人形机器人能够在多样化场景中泛化全身移动操作技能并保持物理协调性，因为人类依赖高层意图和内在物理先验，而非显式的全身运动规划。方法提出了InterPrior这一可扩展框架，首先通过大规模模仿预训练学习一个目标条件变分策略，从多模态观察和意图重建运动，然后通过物理扰动的数据增强和强化学习微调来提升泛化能力。主要实验结果表明，该方法将重建的潜在技能整合到一个有效流形中，使得运动先验能够泛化到训练数据之外，例如与未见物体交互，并展示了其在用户交互控制和真实机器人部署中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Event-Based Shooter Models from Virtual Reality Experiments</div>
<div class="meta-line">Authors: Christopher A. McClurg, Alan R. Wagner</div>
<div class="meta-line">First: 2026-02-05T18:56:49+00:00 · Latest: 2026-02-05T18:56:49+00:00</div>
<div class="meta-line">Comments: Preprint under review for conference publication. 9 pages, 4 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06023v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于虚拟现实实验的事件型枪手模型学习</div>
<div class="mono" style="margin-top:8px">虚拟现实（VR）已成为评估高风险场景（如校园枪击事件）中学校安全措施的有力工具，提供实验控制和高行为保真度。然而，在VR中评估新干预措施需要为每种条件招募新的参与者群体，使得大规模或迭代评估变得困难。这些限制在学习有效干预策略时尤为突出，因为此类策略通常需要大量训练样本。为解决这一挑战，我们开发了一种数据驱动的离散事件模拟器（DES），该模拟器将枪手移动和区域内行动建模为从VR研究中参与者行为学习得到的随机过程。我们利用该模拟器检验基于机器人的枪手干预策略效果。一旦证明DES能复现关键经验模式，它便支持对无法直接通过人类受试者训练的干预策略进行可扩展的评估与学习。总体而言，本研究展示了一种从中高保真度模拟到实际应用的工作流程，为开发和评估自主校园安全干预措施提供了可扩展的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of evaluating school security interventions in virtual reality (VR), where recruiting new human participants for each experimental condition is costly and limits scalability. The authors develop a data-driven discrete-event simulator (DES) that models a shooter&#x27;s movement and actions as stochastic processes, learned directly from participant behavior in prior VR experiments. The simulator, validated by reproducing key empirical patterns, was used to scalably evaluate a robot-based intervention strategy, demonstrating a workflow for developing security measures that are infeasible to test extensively with human subjects.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在虚拟现实（VR）中评估校园安全干预措施的挑战，该方法虽能提供高行为保真度，但资源消耗大，因为每个测试条件都需要招募新的人类参与者。为实现可扩展和迭代的评估，作者开发了一种数据驱动的离散事件模拟器（DES），该模型将射击者的移动和动作建模为随机过程，并直接从VR实验的参与者行为中学习。该模拟器通过复现关键经验模式得到验证，并用于评估基于机器人的干预策略，证明了其作为开发自主安全措施的可扩展替代方案的实用性，而这类措施难以直接通过大量人类受试者进行训练。</div>
</details>
</div>
<div class="card">
<div class="title">Visuo-Tactile World Models</div>
<div class="meta-line">Authors: Carolina Higuera, Sergio Arnaud, Byron Boots, Mustafa Mukadam, Francois Robert Hogan, Franziska Meier</div>
<div class="meta-line">First: 2026-02-05T18:46:33+00:00 · Latest: 2026-02-05T18:46:33+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06001v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06001v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce multi-task Visuo-Tactile World Models (VT-WM), which capture the physics of contact through touch reasoning. By complementing vision with tactile sensing, VT-WM better understands robot-object interactions in contact-rich tasks, avoiding common failure modes of vision-only models under occlusion or ambiguous contact states, such as objects disappearing, teleporting, or moving in ways that violate basic physics. Trained across a set of contact-rich manipulation tasks, VT-WM improves physical fidelity in imagination, achieving 33% better performance at maintaining object permanence and 29% better compliance with the laws of motion in autoregressive rollouts. Moreover, experiments show that grounding in contact dynamics also translates to planning. In zero-shot real-robot experiments, VT-WM achieves up to 35% higher success rates, with the largest gains in multi-step, contact-rich tasks. Finally, VT-WM demonstrates significant downstream versatility, effectively adapting its learned contact dynamics to a novel task and achieving reliable planning success with only a limited set of demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-触觉世界模型</div>
<div class="mono" style="margin-top:8px">我们提出了多任务视觉-触觉世界模型（VT-WM），该模型通过触觉推理捕捉接触物理特性。通过将视觉与触觉感知相结合，VT-WM能更好地理解机器人-物体在密集接触任务中的交互，避免纯视觉模型在遮挡或接触状态模糊时常见的失效模式，例如物体消失、瞬移或以违反基础物理规律的方式运动。在密集接触操作任务集上训练后，VT-WM提升了物理模拟的真实性，在自回归推演中维持物体持久性的性能提升33%，运动规律符合度提升29%。此外，实验表明基于接触动态的建模也提升了规划能力。在零样本真实机器人实验中，VT-WM实现了高达35%的成功率提升，其中多步骤密集接触任务的增益最为显著。最后，VT-WM展现出强大的下游泛化能力，能将其习得的接触动态有效迁移至新任务，仅需少量演示即可实现可靠的规划成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of vision-only models in contact-rich robotic manipulation, which often fail under occlusion or ambiguous contact states, this research introduces Visuo-Tactile World Models (VT-WM) that integrate tactile sensing with vision to better capture contact physics. The method involves training these multi-task world models across various manipulation tasks to improve physical reasoning in model-based imagination and planning. Key experimental results show that VT-WM achieves a 33% improvement in maintaining object permanence and a 29% better compliance with motion laws in autoregressive rollouts, while zero-shot real-robot experiments report up to 35% higher success rates, particularly in multi-step, contact-rich tasks, and the model demonstrates versatile adaptation to novel tasks with limited demonstrations.</div>
<div class="mono" style="margin-top:8px">为解决仅依赖视觉的模型在接触密集的机器人操作任务中，因遮挡或接触状态模糊而常出现的物体消失、瞬移或违反物理规律等失效问题，本研究提出了视觉-触觉世界模型（VT-WM），通过结合触觉感知与视觉来更好地捕捉接触物理特性。该方法在多种操作任务上训练多任务世界模型，以提升基于模型的想象中对物理规律的推理能力。实验结果表明，VT-WM在自回归推演中维持物体恒常性的性能提高了33%，对运动定律的符合度提升了29%；在零样本真实机器人实验中，成功率最高提升35%，尤其在多步骤、接触密集的任务中增益显著，并且该模型能够有效适应新任务，仅需少量演示即可实现可靠的规划成功。</div>
</details>
</div>
<div class="card">
<div class="title">Location-Aware Dispersion on Anonymous Graphs</div>
<div class="meta-line">Authors: Himani, Supantha Pandit, Gokarna Sharma</div>
<div class="meta-line">First: 2026-02-05T18:02:24+00:00 · Latest: 2026-02-05T18:02:24+00:00</div>
<div class="meta-line">Comments: 3 tables, 2 figures, 6 pseudo-codes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05948v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The well-studied DISPERSION problem is a fundamental coordination problem in distributed robotics, where a set of mobile robots must relocate so that each occupies a distinct node of a network. DISPERSION assumes that a robot can settle at any node as long as no other robot settles on that node. In this work, we introduce LOCATION-AWARE DISPERSION, a novel generalization of DISPERSION that incorporates location awareness: Let $G = (V, E)$ be an anonymous, connected, undirected graph with $n = |V|$ nodes, each labeled with a color $\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$. A set $R = \{r_1, \dots, r_k\}$ of $k \leq n$ mobile robots is given, where each robot $r_i$ has an associated color $\mathsf{col}(r_i) \in C$. Initially placed arbitrarily on the graph, the goal is to relocate the robots so that each occupies a distinct node of the same color. When $|C|=1$, LOCATION-AWARE DISPERSION reduces to DISPERSION. There is a solution to DISPERSION in graphs with any $k\leq n$ without knowing $k,n$.
  Like DISPERSION, the goal is to solve LOCATION-AWARE DISPERSION minimizing both time and memory requirement at each agent. We develop several deterministic algorithms with guaranteed bounds on both time and memory requirement. We also give an impossibility and a lower bound for any deterministic algorithm for LOCATION-AWARE DISPERSION. To the best of our knowledge, the presented results collectively establish the algorithmic feasibility of LOCATION-AWARE DISPERSION in anonymous networks and also highlight the challenges on getting an efficient solution compared to the solutions for DISPERSION.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>匿名图上的位置感知分散问题</div>
<div class="mono" style="margin-top:8px">经过深入研究的DISPERSION问题是分布式机器人学中的一个基础协调问题，要求一组移动机器人重新部署，使每个机器人占据网络中的不同节点。DISPERSION假设机器人可以停留在任意节点，只要该节点未被其他机器人占据。本文提出LOCATION-AWARE DISPERSION，这是DISPERSION的一种新颖泛化形式，引入了位置感知机制：设$G = (V, E)$为具有$n = |V|$个节点的匿名连通无向图，每个节点带有颜色标签$\sf{col}(v) \in C = \{c_1, \dots, c_t\}, t\leq n$。给定$k \leq n$个移动机器人的集合$R = \{r_1, \dots, r_k\}$，每个机器人$r_i$具有关联颜色$\mathsf{col}(r_i) \in C$。机器人初始位置任意分布，目标是通过重新部署使每个机器人占据与其颜色相同的不同节点。当$|C|=1$时，LOCATION-AWARE DISPERSION退化为DISPERSION。DISPERSION问题在任意$k\leq n$的图中存在解决方案，且无需预知$k,n$值。
与DISPERSION类似，LOCATION-AWARE DISPERSION的目标是在最小化时间与单智能体内存需求的前提下求解问题。我们开发了多种确定性算法，并给出了时间和内存复杂度的理论界。同时证明了LOCATION-AWARE DISPERSION问题对任意确定性算法的不可能性结果与下界。据我们所知，这些成果共同确立了匿名网络中LOCATION-AWARE DISPERSION问题的算法可行性，并揭示了相较于DISPERSION解决方案，获得高效解所面临的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces Location-Aware Dispersion, a generalization of the classic DISPERSION problem in distributed robotics, where robots must relocate to occupy distinct nodes of an anonymous graph. The key motivation is to incorporate location awareness by requiring each robot to settle on a distinct node that matches its own assigned color, a constraint that reduces to standard DISPERSION when only one color exists. The method involves developing several deterministic algorithms for anonymous, connected, undirected graphs, with a focus on minimizing both time and memory requirements per robot. Experimental findings include guaranteed bounds on time and memory for the proposed algorithms, alongside an impossibility result and a lower bound that highlight the increased complexity compared to the original DISPERSION problem, establishing the algorithmic feasibility but also the challenges of efficient solutions.</div>
<div class="mono" style="margin-top:8px">本文提出了位置感知分散问题，这是分布式机器人学中经典分散问题的一个推广，其动机是引入位置感知，要求每个机器人最终占据一个与其自身颜色匹配的互异节点，当仅有一种颜色时该问题即退化为标准分散问题。研究方法是为匿名、连通、无向图设计多种确定性算法，并着重最小化每个机器人的时间和内存开销。主要实验结果包括给出了算法时间和内存的保证界、一个不可能性结果以及针对任何确定性算法的下界，这些结果共同证明了该问题在匿名网络中的算法可行性，同时也凸显了其相较于基础分散问题更高的求解复杂度。</div>
</details>
</div>
<div class="card">
<div class="title">Physical Human-Robot Interaction: A Critical Review of Safety Constraints</div>
<div class="meta-line">Authors: Riccardo Zanella, Federico Califano, Stefano Stramigioli</div>
<div class="meta-line">First: 2026-01-27T10:45:50+00:00 · Latest: 2026-02-05T17:48:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19462v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.19462v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper aims to provide a clear and rigorous understanding of commonly recognized safety constraints in physical human-robot interaction, particularly regarding ISO/TS 15066. We investigate the derivation of these constraints, critically examine the underlying assumptions, and evaluate their practical implications for system-level safety and performance in industrially relevant scenarios. Key design parameters within safety-critical control architectures are identified, and numerical examples are provided to quantify performance degradation arising from typical approximations and design decisions in manufacturing environments. Within this analysis, the fundamental role of energy in safety assessment is emphasized, providing focused insights into energy-based safety methodologies for collaborative industrial robot systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物理人机交互：安全约束的批判性评述</div>
<div class="mono" style="margin-top:8px">本文旨在清晰严谨地阐述物理人机交互中普遍认可的安全约束，特别是关于ISO/TS 15066标准的内容。我们研究了这些约束的推导过程，批判性地审视其基本假设，并评估其在工业相关场景中对系统级安全与性能的实际影响。文中识别了安全关键控制架构中的关键设计参数，并通过数值算例量化了制造环境中典型近似处理与设计决策导致的性能衰减。分析着重强调了能量在安全评估中的核心作用，为协作型工业机器人系统提供了基于能量的安全方法的深入见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for a rigorous understanding of safety in collaborative robotics, this paper critically reviews established safety constraints, particularly those from ISO/TS 15066. The method involves investigating the derivation of these constraints, examining their underlying assumptions, and evaluating their system-level implications by identifying key safety-critical control parameters and providing numerical examples. The main experimental findings quantify the performance degradation in manufacturing environments resulting from typical design approximations and highlight the fundamental role of energy-based methodologies in safety assessment.</div>
<div class="mono" style="margin-top:8px">本文旨在对物理人机交互中公认的安全约束，特别是ISO/TS 15066相关约束，进行清晰而严格的审视，其动机源于需要深入理解这些约束的推导过程、基本假设及其对工业场景中系统级安全与性能的实际影响。研究方法包括分析安全关键控制架构中的关键设计参数，并通过数值算例量化制造环境中典型近似和设计决策导致的性能下降。主要实验结果表明，安全约束会带来显著的性能折衷，并强调了能量作为协作工业机器人系统安全评估基础指标的核心作用。</div>
</details>
</div>
<div class="card">
<div class="title">From Bench to Flight: Translating Drone Impact Tests into Operational Safety Limits</div>
<div class="meta-line">Authors: Aziz Mohamed Mili, Louis Catar, Paul Gérard, Ilyass Tabiai, David St-Onge</div>
<div class="meta-line">First: 2026-02-05T17:34:49+00:00 · Latest: 2026-02-05T17:34:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05922v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05922v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Indoor micro-aerial vehicles (MAVs) are increasingly used for tasks that require close proximity to people, yet practitioners lack practical methods to tune motion limits based on measured impact risk. We present an end-to-end, open toolchain that converts benchtop impact tests into deployable safety governors for drones. First, we describe a compact and replicable impact rig and protocol for capturing force-time profiles across drone classes and contact surfaces. Second, we provide data-driven models that map pre-impact speed to impulse and contact duration, enabling direct computation of speed bounds for a target force limit. Third, we release scripts and a ROS2 node that enforce these bounds online and log compliance, with support for facility-specific policies. We validate the workflow on multiple commercial off-the-shelf quadrotors and representative indoor assets, demonstrating that the derived governors preserve task throughput while meeting force constraints specified by safety stakeholders. Our contribution is a practical bridge from measured impacts to runtime limits, with shareable datasets, code, and a repeatable process that teams can adopt to certify indoor MAV operations near humans.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从实验台到飞行：将无人机碰撞测试转化为运行安全限制</div>
<div class="mono" style="margin-top:8px">室内微型飞行器（MAV）日益广泛地应用于需要接近人员的任务，但从业者缺乏基于实测碰撞风险调整运动限制的实用方法。我们提出一套端到端的开源工具链，可将实验台碰撞测试转化为可部署的无人机安全控制器。首先，我们设计了一套紧凑可复现的碰撞测试装置与协议，用于采集不同无人机类别和接触表面的力-时间曲线。其次，我们建立了数据驱动模型，将碰撞前速度映射为冲量和接触时长，从而可直接计算目标力限值对应的速度边界。第三，我们发布了在线执行这些边界并记录合规性的脚本及ROS2节点，支持设施特定的策略配置。我们在多款商用现成四旋翼无人机及典型室内场景中验证了该工作流程，证明所推导的控制器能在满足安全利益相关方设定的力约束前提下保持任务执行效率。本研究的贡献在于构建了从实测碰撞到运行时限制的实用桥梁，提供了可共享的数据集、代码及可复现的流程，可供团队用于认证近人环境下的室内MAV作业。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the lack of practical methods for setting drone motion limits based on measured impact risk, particularly for indoor micro-aerial vehicles operating near people. The method involves an end-to-end toolchain that includes a replicable impact rig and protocol to capture force-time profiles, data-driven models mapping pre-impact speed to impulse and contact duration for computing speed bounds, and deployable safety governors implemented via scripts and a ROS2 node. Experimental validation on commercial quadrotors and indoor assets shows that the derived governors maintain task throughput while adhering to specified force constraints, providing a practical bridge from benchtop tests to operational safety limits.</div>
<div class="mono" style="margin-top:8px">针对室内无人机在人员附近安全运行的需求，本研究开发了一套端到端工具链，将物理碰撞测量结果转化为可执行的飞行速度限制。方法包括一个标准化的台架碰撞测试装置用于采集力曲线、基于数据驱动的从速度预测冲量的模型，以及一个运行时软件监管器来强制执行推导出的速度边界。在商用四旋翼无人机上的实验验证表明，该监管器能在保持任务执行效率的同时，有效将碰撞力控制在指定的安全阈值以下。</div>
</details>
</div>
<div class="card">
<div class="title">Residual Reinforcement Learning for Waste-Container Lifting Using Large-Scale Cranes with Underactuated Tools</div>
<div class="meta-line">Authors: Qi Li, Karsten Berns</div>
<div class="meta-line">First: 2026-02-05T17:14:06+00:00 · Latest: 2026-02-05T17:14:06+00:00</div>
<div class="meta-line">Comments: 12 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05895v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper studies the container lifting phase of a waste-container recycling task in urban environments, performed by a hydraulic loader crane equipped with an underactuated discharge unit, and proposes a residual reinforcement learning (RRL) approach that combines a nominal Cartesian controller with a learned residual policy. All experiments are conducted in simulation, where the task is characterized by tight geometric tolerances between the discharge-unit hooks and the container rings relative to the overall crane scale, making precise trajectory tracking and swing suppression essential. The nominal controller uses admittance control for trajectory tracking and pendulum-aware swing damping, followed by damped least-squares inverse kinematics with a nullspace posture term to generate joint velocity commands. A PPO-trained residual policy in Isaac Lab compensates for unmodeled dynamics and parameter variations, improving precision and robustness without requiring end-to-end learning from scratch. We further employ randomized episode initialization and domain randomization over payload properties, actuator gains, and passive joint parameters to enhance generalization. Simulation results demonstrate improved tracking accuracy, reduced oscillations, and higher lifting success rates compared to the nominal controller alone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于残差强化学习的欠驱动工具大型起重机垃圾容器吊装控制</div>
<div class="mono" style="margin-top:8px">本文研究城市环境中垃圾容器回收任务的吊装阶段，该任务由配备欠驱动卸料单元的液压装载起重机执行。论文提出一种残差强化学习方法，将标称笛卡尔控制器与学习得到的残差策略相结合。所有实验均在仿真环境中进行，任务特点是卸料单元吊钩与容器吊环之间的几何公差相对于起重机整体尺度极为严格，因此精确轨迹跟踪与摆动抑制至关重要。标称控制器采用导纳控制实现轨迹跟踪和摆锤感知的摆动阻尼，随后通过带零空间姿态项的阻尼最小二乘逆运动学生成关节速度指令。在Isaac Lab中通过PPO训练的残差策略可补偿未建模动力学和参数变化，在无需从头端到端学习的情况下提升精度与鲁棒性。我们进一步采用随机化回合初始化及对负载特性、执行器增益、被动关节参数的领域随机化以增强泛化能力。仿真结果表明，相较于单独使用标称控制器，该方法能提升跟踪精度、减少振荡并提高吊装成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of precisely lifting waste containers with large-scale cranes using underactuated tools, where tight geometric tolerances demand high accuracy in trajectory tracking and swing suppression. The proposed method combines a nominal Cartesian controller—which uses admittance control for tracking and pendulum-aware damping, followed by inverse kinematics—with a residual policy trained via PPO in simulation to compensate for unmodeled dynamics and parameter variations. Experimental results in simulation show that this residual reinforcement learning approach improves tracking accuracy, reduces oscillations, and achieves higher lifting success rates compared to using the nominal controller alone, with enhanced generalization from domain randomization.</div>
<div class="mono" style="margin-top:8px">本研究针对使用欠驱动工具的大型液压起重机进行垃圾箱精准吊装的挑战，其中紧密的几何公差要求精确的轨迹跟踪和摆动抑制。该方法将用于轨迹跟踪和摆动阻尼的名义导纳控制器，与通过近端策略优化（PPO）在仿真中训练的残差策略相结合，以补偿未建模动力学和参数变化，并通过域随机化增强泛化能力。仿真实验结果表明，与单独使用名义控制器相比，这种残差强化学习方法实现了更高的跟踪精度、更低的振荡以及更高的吊装成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Group Relative Policy Optimization</div>
<div class="meta-line">Authors: Roger Girgis, Rodrigue de Schaetzen, Luke Rowe, Azalée Robitaille, Christopher Pal, Liam Paull</div>
<div class="meta-line">First: 2026-02-05T16:44:23+00:00 · Latest: 2026-02-05T16:44:23+00:00</div>
<div class="meta-line">Comments: 16 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05863v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>约束型群体相对策略优化</div>
<div class="mono" style="margin-top:8px">尽管群体相对策略优化（GRPO）已成为一种可扩展的无评论者策略学习框架，但将其拓展至具有显式行为约束的场景仍研究不足。本文提出约束型GRPO，这是一种基于拉格朗日方法的GRPO扩展框架，用于约束策略优化。约束通过指示器成本函数定义，借助拉格朗日松弛实现对违规率的直接优化。研究发现，优势估计中简单的多分量处理会破坏约束学习：分量间标准差失配将扭曲不同目标项的相对重要性，进而破坏拉格朗日信号并阻碍有效约束实施。我们通过理论推导揭示该现象，并提出标量化优势构建方法以保持奖励项与约束项间的预期权衡。在网格世界玩具环境中的实验验证了预测的优化病理现象，并证明标量化优势能恢复稳定的约束控制。此外，我们在机器人任务中评估约束型GRPO，其在提升任务成功率的同时改善了约束满足度，为日益依赖大规模多模态基础模型的具身AI领域提供了一种简洁有效的约束策略优化方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to extend the scalable, critic-free Group Relative Policy Optimization (GRPO) framework to settings with explicit behavioral constraints, which remains underexplored. The method introduces Constrained GRPO, a Lagrangian-based extension that specifies constraints via indicator cost functions and optimizes violation rates through Lagrangian relaxation; it identifies and addresses a pathology where a naive multi-component advantage estimation distorts term importance and corrupts the Lagrangian signal, proposing a scalarized advantage construction to preserve the intended trade-off. Experimental results in a toy gridworld confirm the optimization pathology and show that scalarizing advantages restores stable constraint control, while evaluations on robotics tasks demonstrate improved constraint satisfaction and increased task success, offering a simple effective recipe for constrained policy optimization in embodied AI domains.</div>
<div class="mono" style="margin-top:8px">本研究针对可扩展、无评论家的组相对策略优化（GRPO）框架缺乏显式行为约束处理的问题。方法上提出了约束GRPO，采用基于拉格朗日松弛和指示器成本函数来直接优化约束违反率；研究识别并修正了朴素多分量优势估计会扭曲不同目标项重要性、破坏拉格朗日信号的优化病理，提出了保持奖励与约束项间权衡的标量化优势构建。在玩具网格世界的实验结果证实了该优化问题，并表明标量化优势能恢复稳定的约束控制，而在机器人任务上的评估则显示出约束满足度和任务成功率的提升，为具身人工智能领域的约束策略优化提供了一个简单有效的方案。</div>
</details>
</div>
<div class="card">
<div class="title">A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion</div>
<div class="meta-line">Authors: Dennis Bank, Joost Cordes, Thomas Seel, Simon F. G. Ehlers</div>
<div class="meta-line">First: 2026-02-05T16:38:42+00:00 · Latest: 2026-02-05T16:38:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05855v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05855v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable terrain perception is a critical prerequisite for the deployment of humanoid robots in unstructured, human-centric environments. While traditional systems often rely on manually engineered, single-sensor pipelines, this paper presents a learning-based framework that uses an intermediate, robot-centric heightmap representation. A hybrid Encoder-Decoder Structure (EDS) is introduced, utilizing a Convolutional Neural Network (CNN) for spatial feature extraction fused with a Gated Recurrent Unit (GRU) core for temporal consistency. The architecture integrates multimodal data from an Intel RealSense depth camera, a LIVOX MID-360 LiDAR processed via efficient spherical projection, and an onboard IMU. Quantitative results demonstrate that multimodal fusion improves reconstruction accuracy by 7.2% over depth-only and 9.9% over LiDAR-only configurations. Furthermore, the integration of a 3.2 s temporal context reduces mapping drift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于融合激光雷达与深度数据的混合自编码器：面向人形机器人步态的鲁棒高度图生成</div>
<div class="mono" style="margin-top:8px">可靠的 terrain perception 是人形机器人在非结构化、以人为中心的环境中部署的关键前提。传统系统通常依赖人工设计的单传感器流水线，而本文提出了一种基于学习的框架，采用以机器人为中心的中间高度图表示。该框架引入了混合编码器-解码器结构，利用卷积神经网络提取空间特征，并结合门控循环单元核心确保时序一致性。该架构融合了来自 Intel RealSense 深度相机、通过高效球面投影处理的 LIVOX MID-360 激光雷达以及板载 IMU 的多模态数据。定量结果表明，多模态融合相比纯深度配置重建精度提升 7.2%，相比纯激光雷达配置提升 9.9%。此外，集成 3.2 秒时序上下文有效减少了建图漂移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable reliable humanoid robot locomotion in unstructured environments, this research addresses the limitations of traditional single-sensor terrain perception by proposing a learning-based framework for generating robot-centric heightmaps. The method employs a hybrid encoder-decoder structure that fuses a CNN for spatial feature extraction with a GRU core for temporal modeling, integrating data from a depth camera, a LiDAR sensor processed via spherical projection, and an onboard IMU. Experimental results show that multimodal fusion improves reconstruction accuracy by 7.2% over using depth data alone and 9.9% over LiDAR alone, while incorporating a 3.2-second temporal context reduces mapping drift.</div>
<div class="mono" style="margin-top:8px">为使双足机器人能在非结构化环境中可靠运动，本研究开发了一种基于学习的鲁棒地形感知框架。该方法引入了一种混合编码器-解码器结构，将卷积神经网络提取的空间特征与门控循环单元处理的时间序列相融合，整合了深度相机、通过球面投影处理的激光雷达以及惯性测量单元的多模态数据，生成以机器人为中心的高度图。实验结果表明，多模态融合将重建精度相比仅使用深度数据提高了7.2%，相比仅使用激光雷达提高了9.9%，同时结合3.2秒的时间上下文有效减少了建图漂移。</div>
</details>
</div>
<div class="card">
<div class="title">Bench-NPIN: Benchmarking Non-prehensile Interactive Navigation</div>
<div class="meta-line">Authors: Ninghan Zhong, Steven Caro, Avraiem Iskandar, Megnath Ramesh, Stephen L. Smith</div>
<div class="meta-line">First: 2025-05-17T16:54:18+00:00 · Latest: 2026-02-05T16:31:05+00:00</div>
<div class="meta-line">Comments: This paper has been withdrawn by the authors. This paper has been superseded by arXiv:2512.11736</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.12084v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.12084v2">PDF</a> · <a href="https://github.com/IvanIZ/BenchNPIN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile robots are increasingly deployed in unstructured environments where obstacles and objects are movable. Navigation in such environments is known as interactive navigation, where task completion requires not only avoiding obstacles but also strategic interactions with movable objects. Non-prehensile interactive navigation focuses on non-grasping interaction strategies, such as pushing, rather than relying on prehensile manipulation. Despite a growing body of research in this field, most solutions are evaluated using case-specific setups, limiting reproducibility and cross-comparison. In this paper, we present Bench-NPIN, the first comprehensive benchmark for non-prehensile interactive navigation. Bench-NPIN includes multiple components: 1) a comprehensive range of simulated environments for non-prehensile interactive navigation tasks, including navigating a maze with movable obstacles, autonomous ship navigation in icy waters, box delivery, and area clearing, each with varying levels of complexity; 2) a set of evaluation metrics that capture unique aspects of interactive navigation, such as efficiency, interaction effort, and partial task completion; and 3) demonstrations using Bench-NPIN to evaluate example implementations of established baselines across environments. Bench-NPIN is an open-source Python library with a modular design. The code, documentation, and trained models can be found at https://github.com/IvanIZ/BenchNPIN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bench-NPIN：非抓取式交互导航基准测试</div>
<div class="mono" style="margin-top:8px">移动机器人在非结构化环境中的部署日益增多，这类环境中的障碍物和物体往往可移动。在此类环境中的导航被称为交互式导航，其任务完成不仅需要避开障碍物，还需对可移动物体进行策略性交互。非抓取式交互导航侧重于非抓握的交互策略（如推动），而非依赖抓取式操作。尽管该领域研究不断增多，但多数解决方案采用特定案例设置进行评估，限制了可复现性与交叉比较。本文提出首个非抓取式交互导航综合基准测试框架Bench-NPIN，包含三大组件：1）涵盖多种复杂度的非抓取式交互导航任务仿真环境，包括含可移动障碍物的迷宫导航、冰域自主船舶导航、箱体运送及区域清理；2）一套针对交互导航独特维度的评估指标，如效率、交互代价和部分任务完成度；3）基于Bench-NPIN对现有基线方法在不同环境中的示例实现进行评估的演示。Bench-NPIN采用模块化设计的开源Python库，代码、文档及训练模型可通过https://github.com/IvanIZ/BenchNPIN获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for reproducible and comparable evaluations in non-prehensile interactive navigation, where mobile robots must strategically push or move objects without grasping to complete tasks in unstructured environments. The method introduces Bench-NPIN, a comprehensive benchmark comprising a suite of simulated environments (e.g., mazes with movable obstacles, icy ship navigation), a set of evaluation metrics for efficiency and interaction effort, and demonstrations for baseline implementations, all packaged as an open-source Python library. Key experimental findings from applying the benchmark to established baselines highlight its utility in assessing performance across diverse task complexities, though the original paper has been withdrawn and superseded by a subsequent work.</div>
<div class="mono" style="margin-top:8px">本研究针对非抓取式交互导航领域缺乏标准化评估的问题，该领域要求移动机器人在非结构化环境中通过推挤等非抓取方式策略性地移动物体以完成导航。作者提出了Bench-NPIN这一综合性基准，包含多样化的模拟任务（如迷宫导航、冰面船舶导航）、一套评估效率与交互代价的指标以及基线实现。利用该基准进行的实验评估表明，它能有效评估和比较不同方法在各种任务复杂度下的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Video Generation Propels Real-World Beyond-the-View Vision-Language Navigation</div>
<div class="meta-line">Authors: Hai Zhang, Siqi Liang, Li Chen, Yuxian Li, Yukuan Xu, Yichao Zhong, Fu Zhang, Hongyang Li</div>
<div class="meta-line">First: 2026-02-05T16:16:13+00:00 · Latest: 2026-02-05T16:16:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Why must vision-language navigation be bound to detailed and verbose language instructions? While such details ease decision-making, they fundamentally contradict the goal for navigation in the real-world. Ideally, agents should possess the autonomy to navigate in unknown environments guided solely by simple and high-level intents. Realizing this ambition introduces a formidable challenge: Beyond-the-View Navigation (BVN), where agents must locate distant, unseen targets without dense and step-by-step guidance. Existing large language model (LLM)-based methods, though adept at following dense instructions, often suffer from short-sighted behaviors due to their reliance on short-horimzon supervision. Simply extending the supervision horizon, however, destabilizes LLM training. In this work, we identify that video generation models inherently benefit from long-horizon supervision to align with language instructions, rendering them uniquely suitable for BVN tasks. Capitalizing on this insight, we propose introducing the video generation model into this field for the first time. Yet, the prohibitive latency for generating videos spanning tens of seconds makes real-world deployment impractical. To bridge this gap, we propose SparseVideoNav, achieving sub-second trajectory inference guided by a generated sparse future spanning a 20-second horizon. This yields a remarkable 27x speed-up compared to the unoptimized counterpart. Extensive real-world zero-shot experiments demonstrate that SparseVideoNav achieves 2.5x the success rate of state-of-the-art LLM baselines on BVN tasks and marks the first realization of such capability in challenging night scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏视频生成推动现实世界超视距视觉语言导航</div>
<div class="mono" style="margin-top:8px">为何视觉语言导航必须依赖详尽冗长的语言指令？虽然细节有助于决策，但这与真实世界导航的根本目标相悖。理想情况下，智能体应具备仅凭简单高层意图在未知环境中自主导航的能力。实现这一愿景带来了严峻挑战：超视距导航（BVN），即智能体需在没有密集逐步指引的情况下定位远处不可见的目标。现有基于大语言模型（LLM）的方法虽擅长遵循密集指令，却因依赖短视域监督常表现出短视行为。然而，单纯扩展监督视域会破坏LLM训练的稳定性。本研究指出，视频生成模型天生受益于长视域监督以实现与语言指令的对齐，使其特别适用于BVN任务。基于此洞见，我们首次将视频生成模型引入该领域。但生成数十秒视频的过高延迟使其难以实际部署。为此，我们提出SparseVideoNav，通过生成跨越20秒视域的稀疏未来轨迹，实现亚秒级轨迹推断，相比未优化版本获得27倍的加速。大量现实世界零样本实验表明，SparseVideoNav在BVN任务上的成功率达到顶尖LLM基线的2.5倍，并首次在极具挑战性的夜间场景中实现了此类能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to move beyond dense, step-by-step language instructions in vision-language navigation, which contradicts the goal of autonomous real-world navigation where agents should follow simple, high-level intents. The method introduces a video generation model to address the Beyond-the-View Navigation (BVN) challenge, proposing SparseVideoNav to generate sparse future trajectories over a 20-second horizon, enabling sub-second inference and a 27x speed-up over unoptimized video generation. Experimental results from real-world zero-shot tests show that SparseVideoNav achieves a 2.5x higher success rate than state-of-the-art LLM baselines on BVN tasks and is the first to demonstrate such capability in challenging night scenes.</div>
<div class="mono" style="margin-top:8px">本研究针对现实世界中密集语言指令导航不切实际的问题，提出了超越视野导航（BVN），要求智能体仅凭高层意图定位未可见目标。该方法首次引入视频生成模型进行长视野规划，并优化为SparseVideoNav，通过生成稀疏未来轨迹实现高效推理。实验结果表明，在零样本真实世界测试中，该方法推理速度提升27倍，成功率比最先进的LLM基线高2.5倍，并首次实现了在夜间场景中的成功导航。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable and General Whole-Body Control for Cross-Humanoid Locomotion</div>
<div class="meta-line">Authors: Yufei Xue, YunFeng Lin, Wentao Dong, Yang Tang, Jingbo Wang, Jiangmiao Pang, Ming Zhou, Minghuan Liu, Weinan Zhang</div>
<div class="meta-line">First: 2026-02-05T15:48:15+00:00 · Latest: 2026-02-05T15:48:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05791v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning-based whole-body controllers have become a key driver for humanoid robots, yet most existing approaches require robot-specific training. In this paper, we study the problem of cross-embodiment humanoid control and show that a single policy can robustly generalize across a wide range of humanoid robot designs with one-time training. We introduce XHugWBC, a novel cross-embodiment training framework that enables generalist humanoid control through: (1) physics-consistent morphological randomization, (2) semantically aligned observation and action spaces across diverse humanoid robots, and (3) effective policy architectures modeling morphological and dynamical properties. XHugWBC is not tied to any specific robot. Instead, it internalizes a broad distribution of morphological and dynamical characteristics during training. By learning motion priors from diverse randomized embodiments, the policy acquires a strong structural bias that supports zero-shot transfer to previously unseen robots. Experiments on twelve simulated humanoids and seven real-world robots demonstrate the strong generalization and robustness of the resulting universal controller.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可扩展且通用的跨人形机器人全身控制方法</div>
<div class="mono" style="margin-top:8px">基于学习的全身控制器已成为人形机器人的关键驱动技术，但现有方法大多需要针对特定机器人进行训练。本文研究跨具身人形控制问题，证明单一策略通过一次性训练即可在广泛的人形机器人设计中实现鲁棒泛化。我们提出XHugWBC——一种创新的跨具身训练框架，通过以下机制实现通用人形控制：(1) 物理一致的形态随机化，(2) 跨异构人形机器人的语义对齐观测与动作空间，(3) 建模形态与动力学特性的高效策略架构。XHugWBC不依赖特定机器人，而是在训练过程中内化广泛的形态与动力学特征分布。通过从多样化随机具身中学习运动先验，该策略获得支持零样本迁移到未见机器人的强结构偏置。在12个仿真人形机器人与7个真实机器人上的实验验证了该通用控制器卓越的泛化能力与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitation of robot-specific training in learning-based whole-body control for humanoids, this research introduces XHugWBC, a cross-embodiment training framework. The method employs physics-consistent morphological randomization, semantically aligned observation and action spaces, and policy architectures that model morphological and dynamical properties to train a single, generalist policy. Experimental results on twelve simulated and seven real-world humanoid robots demonstrate that the policy achieves robust zero-shot transfer and strong generalization to previously unseen robot designs.</div>
<div class="mono" style="margin-top:8px">针对现有基于学习的全身控制器大多需要针对特定机器人进行训练的局限性，本研究开发了一个跨具身控制框架，使单一策略能够泛化到不同的人形机器人设计上。所提出的XHugWBC方法采用物理一致的形态随机化、语义对齐的观察/动作空间以及专门的政策架构来建模多样的形态和动力学特性。在十二个模拟和七个真实人形机器人上的实验结果表明，该控制器实现了对先前未见机器人具身的鲁棒零样本迁移和强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Enriching physical-virtual interaction in AR gaming by tracking identical objects via an egocentric partial observation frame</div>
<div class="meta-line">Authors: Liuchuan Yu, Ching-I Huang, Hsueh-Cheng Wang, Lap-Fai Yu</div>
<div class="meta-line">Venue: Virtual Reality 30 (2026) 51</div>
<div class="meta-line">First: 2025-02-24T18:28:22+00:00 · Latest: 2026-02-05T15:39:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17399v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.17399v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Augmented reality (AR) games, particularly those designed for head-mounted displays, have grown increasingly prevalent. However, most existing systems depend on pre-scanned, static environments and rely heavily on continuous tracking or marker-based solutions, which limit adaptability in dynamic physical spaces. This is particularly problematic for AR headsets and glasses, which typically follow the user&#x27;s head movement and cannot maintain a fixed, stationary view of the scene. Moreover, continuous scene observation is neither power-efficient nor practical for wearable devices, given their limited battery and processing capabilities. A persistent challenge arises when multiple identical objects are present in the environment-standard object tracking pipelines often fail to maintain consistent identities without uninterrupted observation or external sensors. These limitations hinder fluid physical-virtual interactions, especially in dynamic or occluded scenes where continuous tracking is infeasible. To address this, we introduce a novel optimization-based framework for re-identifying identical objects in AR scenes using only one partial egocentric observation frame captured by a headset. We formulate the problem as a label assignment task solved via integer programming, augmented with a Voronoi diagram-based pruning strategy to improve computational efficiency. This method reduces computation time by 50% while preserving 91% accuracy in simulated experiments. Moreover, we evaluated our approach in quantitative synthetic and quantitative real-world experiments. We also conducted three qualitative real-world experiments to demonstrate the practical utility and generalizability for enabling dynamic, markerless object interaction in AR environments. Our video demo is available at https://youtu.be/RwptEfLtW1U.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于第一人称局部观测帧的相同物体追踪增强AR游戏虚实交互</div>
<div class="mono" style="margin-top:8px">增强现实（AR）游戏，尤其是头戴式显示设备专用游戏日益普及。然而，现有系统多依赖预扫描静态环境，并严重依赖持续追踪或基于标记的解决方案，限制了在动态物理空间中的适应性。这对AR头显设备尤为突出——这类设备通常随用户头部运动而移动，无法维持固定的场景视角。此外，持续场景观测对电池和处理能力有限的可穿戴设备既不节能也不实用。当环境中存在多个相同物体时，标准物体追踪流程往往因缺乏持续观测或外部传感器而无法维持身份一致性，这阻碍了流畅的虚实交互，尤其在持续追踪不可行的动态或遮挡场景中。为此，我们提出一种基于优化的新型框架，仅通过头戴设备捕获的单帧第一人称局部观测实现AR场景中相同物体的重识别。我们将该问题建模为通过整数规划求解的标签分配任务，并引入基于Voronoi图的剪枝策略以提升计算效率。该方法在模拟实验中保持91%准确率的同时减少50%计算耗时。我们通过定量合成实验与定量真实场景实验评估方案性能，并开展三项定性真实场景实验，验证其在AR环境中实现动态无标记物体交互的实用性与泛化能力。演示视频详见：https://youtu.be/RwptEfLtW1U。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of tracking multiple identical objects in dynamic augmented reality (AR) gaming environments, where conventional methods relying on continuous observation or markers are impractical due to headset mobility and power constraints. The proposed method introduces an optimization-based framework that re-identifies identical objects using only a single partial egocentric frame, formulating it as a label assignment problem solved via integer programming and enhancing efficiency with a Voronoi diagram-based pruning strategy. Experimental results show the approach reduces computation time by 50% while maintaining 91% accuracy in simulations, with further validation through quantitative synthetic and real-world tests, as well as qualitative demonstrations confirming its utility for enabling dynamic, markerless object interactions in AR.</div>
<div class="mono" style="margin-top:8px">本研究针对当前增强现实（AR）游戏系统的局限性，这些系统通常依赖预扫描环境或连续跟踪，在动态场景中存在多个相同物体时效率低下且不实用，尤其对于资源受限的头戴式设备。所提出的方法引入了一个基于优化的框架，仅使用单个部分自我中心观察帧来重新识别相同物体，将该任务表述为整数规划问题，并采用基于Voronoi图的剪枝策略以提高效率。实验结果表明，该方法在模拟中将计算时间减少了50%，同时保持了91%的准确率，并通过定量合成与真实世界测试以及定性演示进一步验证了其在实现AR中动态、无标记物体交互方面的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Task-Oriented Robot-Human Handovers on Legged Manipulators</div>
<div class="meta-line">Authors: Andreea Tulbure, Carmen Scheidemann, Elias Steiner, Marco Hutter</div>
<div class="meta-line">First: 2026-02-05T15:28:04+00:00 · Latest: 2026-02-05T15:28:04+00:00</div>
<div class="meta-line">Comments: Accepted to 21st ACM/IEEE International Conference on Human-Robot Interaction (HRI) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05760v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05760v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Task-oriented handovers (TOH) are fundamental to effective human-robot collaboration, requiring robots to present objects in a way that supports the human&#x27;s intended post-handover use. Existing approaches are typically based on object- or task-specific affordances, but their ability to generalize to novel scenarios is limited. To address this gap, we present AFT-Handover, a framework that integrates large language model (LLM)-driven affordance reasoning with efficient texture-based affordance transfer to achieve zero-shot, generalizable TOH. Given a novel object-task pair, the method retrieves a proxy exemplar from a database, establishes part-level correspondences via LLM reasoning, and texturizes affordances for feature-based point cloud transfer. We evaluate AFT-Handover across diverse task-object pairs, showing improved handover success rates and stronger generalization compared to baselines. In a comparative user study, our framework is significantly preferred over the current state-of-the-art, effectively reducing human regrasping before tool use. Finally, we demonstrate TOH on legged manipulators, highlighting the potential of our framework for real-world robot-human handovers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向任务的腿式操作器人机交接研究</div>
<div class="mono" style="margin-top:8px">面向任务的交接（TOH）是实现高效人机协作的基础，要求机器人以支持人类交接后预期使用的方式呈现物体。现有方法通常基于物体或任务特定的可供性，但其泛化至新场景的能力有限。为填补这一空白，我们提出了AFT-Handover框架，该框架整合了大型语言模型（LLM）驱动的可供性推理与高效的基于纹理的可供性迁移，以实现零样本、可泛化的TOH。针对新的物体-任务组合，该方法从数据库中检索代理范例，通过LLM推理建立部件级对应关系，并纹理化可供性以实现基于特征的点云迁移。我们在多种任务-物体组合上评估AFT-Handover，结果显示其相比基线方法提高了交接成功率并展现出更强的泛化能力。在对比用户研究中，本框架显著优于当前最优方法，有效减少了人类在使用工具前的重新抓取。最后，我们在腿式操作器上演示了TOH，凸显了该框架在实际人机交接场景中的应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable robots to hand over objects in a manner that supports the human&#x27;s subsequent task, which existing affordance-based methods struggle to generalize for novel scenarios, this work introduces AFT-Handover. The method uses a large language model to reason about object affordances and performs texture-based affordance transfer from a proxy exemplar to a novel object, establishing part-level correspondences for zero-shot generalization. Experimental results show the framework improves handover success rates across diverse object-task pairs, is significantly preferred by users in a comparative study, reduces the need for human regrasping, and is successfully demonstrated on legged manipulators.</div>
<div class="mono" style="margin-top:8px">本研究针对现有基于物体或任务特定可供性、泛化能力有限的面向任务机器人-人物体交接方法，提出了AFT-Handover框架以实现零样本泛化。该方法将大语言模型驱动的可供性推理与基于纹理的可供性迁移相结合：对于新的物体-任务对，它检索一个代理示例，利用大语言模型建立部件级对应关系，并通过基于特征的点云纹理化迁移可供性。在不同任务-物体对上的实验评估表明，该方法相比基线提高了交接成功率和泛化能力；一项比较性用户研究显示，其因减少了人类在使用工具前的重新抓取而显著优于当前最优方法，在腿式操纵器上的演示也证实了其在现实世界中应用的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Dull, Dirty, Dangerous: Understanding the Past, Present, and Future of a Key Motivation for Robotics</div>
<div class="meta-line">Authors: Nozomi Nakajima, Pedro Reynolds-Cuéllar, Caitrin Lynch, Kate Darling</div>
<div class="meta-line">First: 2026-02-04T16:48:06+00:00 · Latest: 2026-02-05T15:26:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04746v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04746v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In robotics, the concept of &quot;dull, dirty, and dangerous&quot; (DDD) work has been used to motivate where robots might be useful. In this paper, we conduct an empirical analysis of robotics publications between 1980 and 2024 that mention DDD, and find that only 2.7% of publications define DDD and 8.7% of publications provide concrete examples of tasks or jobs that are DDD. We then review the social science literature on &quot;dull,&quot; &quot;dirty,&quot; and &quot;dangerous&quot; work to provide definitions and guidance on how to conceptualize DDD for robotics. Finally, we propose a framework that helps the robotics community consider the job context for our technology, encouraging a more informed perspective on how robotics may impact human labor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>枯燥、肮脏、危险：解析机器人学关键驱动理念的过去、现在与未来</div>
<div class="mono" style="margin-top:8px">机器人学领域长期以&#x27;枯燥、肮脏、危险&#x27;（DDD）工作作为技术应用的核心驱动力。本文通过对1980至2024年间提及DDD的机器人学文献进行实证分析，发现仅2.7%的文献明确定义了DDD概念，8.7%的文献提供了具体任务案例。研究进一步整合社会科学领域关于三类工作的理论，为机器人学中的DDD概念化提供定义框架与操作指引。最终提出系统性分析框架，引导学界在技术研发中充分考虑工作场景特性，从而建立更审慎的机器人技术对人类劳动影响的评估体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the prevalent but often loosely used &#x27;dull, dirty, and dangerous&#x27; (DDD) motivation in robotics, finding through an empirical analysis of publications from 1980 to 2024 that only 2.7% define DDD and 8.7% provide concrete task examples. To address this gap, the authors review social science literature to establish clearer definitions and conceptual guidance for DDD. They then propose a framework for considering job context, aiming to foster a more informed perspective on robotics&#x27; impact on human labor.</div>
<div class="mono" style="margin-top:8px">本研究调查了机器人学中普遍但常被模糊使用的&#x27;枯燥、肮脏、危险&#x27;（DDD）动机，通过对1980年至2024年文献的实证分析发现，仅2.7%的文献定义了DDD，8.7%提供了具体任务或工作示例。为弥补这一不足，作者回顾了社会科学文献，为DDD工作建立了更清晰的定义和概念指导。随后，他们提出了一个框架，旨在帮助机器人学界更审慎地考虑工作背景，从而形成关于机器人技术对人类劳动影响的更明智视角。</div>
</details>
</div>
<div class="card">
<div class="title">Improved Bag-of-Words Image Retrieval with Geometric Constraints for Ground Texture Localization</div>
<div class="meta-line">Authors: Aaron Wilhelm, Nils Napp</div>
<div class="meta-line">Venue: ICRA 2025</div>
<div class="meta-line">First: 2025-05-16T18:37:18+00:00 · Latest: 2026-02-05T14:41:21+00:00</div>
<div class="meta-line">Comments: Accepted to ICRA 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.11620v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.11620v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ground texture localization using a downward-facing camera offers a low-cost, high-precision localization solution that is robust to dynamic environments and requires no environmental modification. We present a significantly improved bag-of-words (BoW) image retrieval system for ground texture localization, achieving substantially higher accuracy for global localization and higher precision and recall for loop closure detection in SLAM. Our approach leverages an approximate $k$-means (AKM) vocabulary with soft assignment, and exploits the consistent orientation and constant scale constraints inherent to ground texture localization. Identifying the different needs of global localization vs. loop closure detection for SLAM, we present both high-accuracy and high-speed versions of our algorithm. We test the effect of each of our proposed improvements through an ablation study and demonstrate our method&#x27;s effectiveness for both global localization and loop closure detection. With numerous ground texture localization systems already using BoW, our method can readily replace other generic BoW systems in their pipeline and immediately improve their results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何约束改进词袋模型的地面纹理定位图像检索方法</div>
<div class="mono" style="margin-top:8px">利用下视相机进行地面纹理定位提供了一种低成本、高精度的定位方案，该方案对环境动态变化具有鲁棒性且无需改造环境。本文提出一种显著改进的词袋模型图像检索系统，用于地面纹理定位，在SLAM中实现了全局定位精度的大幅提升，以及闭环检测的更高精确率与召回率。该方法采用近似K均值词汇表配合软分配策略，并利用地面纹理定位固有的方向一致性与尺度恒定约束。针对SLAM中全局定位与闭环检测的不同需求，我们分别提出了高精度与高速版本的算法。通过消融实验验证了各项改进措施的效果，并证明了本方法在全局定位与闭环检测中的有效性。鉴于现有众多地面纹理定位系统已采用词袋模型，本方法可直接替代其流程中的通用词袋系统并即时提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance ground texture localization, a low-cost and robust method using a downward-facing camera, by improving the standard bag-of-words (BoW) image retrieval system for higher accuracy in global localization and better precision/recall in SLAM loop closure detection. The method introduces an approximate k-means vocabulary with soft assignment and incorporates the inherent geometric constraints of consistent orientation and constant scale in ground-based imaging. Experimental ablation studies validate the effectiveness of each proposed improvement, showing that the algorithm, offered in both high-accuracy and high-speed variants, can readily replace existing generic BoW systems to deliver immediate performance gains.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进使用下视相机的地面纹理定位方法，这是一种低成本、高鲁棒性且无需环境改造的技术，通过提升广泛采用的词袋模型图像检索系统的性能以获得更高的准确率和召回率。该方法采用了带软分配的近似k均值词汇表，并专门利用了地面纹理定位中固有的方向一致和尺度恒定几何约束，针对SLAM中的全局定位与回环检测任务分别定制了高精度与高速度的算法版本。实验消融研究证实，所提出的每一项改进都显著提升了全局定位的准确率以及回环检测的精确率和召回率，为现有使用通用词袋模型的系统提供了直接的升级方案。</div>
</details>
</div>
<div class="card">
<div class="title">From Vision to Decision: Neuromorphic Control for Autonomous Navigation and Tracking</div>
<div class="meta-line">Authors: Chuwei Wang, Eduardo Sebastián, Amanda Prorok, Anastasia Bizyaeva</div>
<div class="meta-line">First: 2026-02-05T14:09:09+00:00 · Latest: 2026-02-05T14:09:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05683v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05683v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic navigation has historically struggled to reconcile reactive, sensor-based control with the decisive capabilities of model-based planners. This duality becomes critical when the absence of a predominant option among goals leads to indecision, challenging reactive systems to break symmetries without computationally-intense planners. We propose a parsimonious neuromorphic control framework that bridges this gap for vision-guided navigation and tracking. Image pixels from an onboard camera are encoded as inputs to dynamic neuronal populations that directly transform visual target excitation into egocentric motion commands. A dynamic bifurcation mechanism resolves indecision by delaying commitment until a critical point induced by the environmental geometry. Inspired by recently proposed mechanistic models of animal cognition and opinion dynamics, the neuromorphic controller provides real-time autonomy with a minimal computational burden, a small number of interpretable parameters, and can be seamlessly integrated with application-specific image processing pipelines. We validate our approach in simulation environments as well as on an experimental quadrotor platform.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视觉到决策：面向自主导航与追踪的神经形态控制</div>
<div class="mono" style="margin-top:8px">机器人导航长期面临反应式传感器控制与模型化规划器决策能力难以协调的挑战。当目标间缺乏主导选项导致系统犹豫不决时，这种二元性尤为关键——反应式系统需在不依赖高计算负荷规划器的前提下打破对称性。本文提出一种简约的神经形态控制框架，为视觉引导导航与追踪任务架设桥梁。通过将机载相机采集的图像像素编码为动态神经元群体的输入，该框架直接将视觉目标激励转化为以自我为中心的运动指令。其动态分岔机制通过延迟决策至环境几何诱发的临界点，有效化解犹豫状态。受近期动物认知机制模型与观点动力学研究启发，该神经形态控制器以极低计算负荷、少量可解释参数实现实时自主控制，并能与应用特定的图像处理流程无缝集成。我们在仿真环境及实验性四旋翼平台上验证了该方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge in robotic navigation of combining reactive sensor-based control with decisive model-based planning, particularly when ambiguous goal options cause indecision. The authors propose a neuromorphic control framework that encodes onboard camera pixels as inputs to dynamic neuronal populations, directly converting visual target excitation into egocentric motion commands; a dynamic bifurcation mechanism resolves indecision by delaying commitment until environmental geometry induces a critical point. Experimental validation in simulations and on a quadrotor platform demonstrates that the controller achieves real-time autonomy with minimal computational burden and interpretable parameters.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决机器人导航中反应式传感器控制与基于模型规划难以融合的挑战，特别是在目标选项模糊导致决策困难时。作者提出了一种神经形态控制框架，将机载摄像头的图像像素编码为动态神经元群输入，直接将视觉目标激励转化为以自我为中心的运动指令。通过动态分岔机制延迟决策，直至环境几何诱导出临界点，从而解决犹豫不决的问题。在仿真和四旋翼平台上的实验验证表明，该控制器能以最小的计算负担和可解释的参数实现实时自主导航。</div>
</details>
</div>
<div class="card">
<div class="title">Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation</div>
<div class="meta-line">Authors: Ojasva Mishra, Xiaolong Wu, Min Xu</div>
<div class="meta-line">First: 2026-01-26T16:11:05+00:00 · Latest: 2026-02-05T13:10:26+00:00</div>
<div class="meta-line">Comments: Pending IEEE Transactions on Robotics Publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18639v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.18639v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>考虑执行器饱和约束的机器人关节控制离散时间PID增益优化</div>
<div class="mono" style="margin-top:8px">旋转驱动的精确调节是自主机器人技术的基础，然而实际PID控制回路因离散时间执行、执行器饱和以及微小延迟和测量误差而偏离连续时间理论。本文提出一种面向饱和离散时间关节控制的实现感知分析与整定流程：首先，利用朱里判据推导欧拉法和精确零阶保持离散化下的PI稳定性区域；其次，评估饱和主导工况下的离散反向计算抗饱和实现方案；最后，提出一种混合认证贝叶斯优化流程，在优化具有超调与饱和占空比软惩罚的鲁棒IAE指标时，同步筛选解析不稳定候选参数和行为不安全暂态过程。基准参数扫描（τ=1.0秒，Δt=0.01秒，u∈[-10,10]）量化了P/PI/PID控制的上升/稳定趋势。在模拟不确定性、延迟、噪声、量化及更严格饱和的随机模型族中，鲁棒导向整定将IAE中位数从0.843降至0.430，同时保持超调中位数低于2%。在纯仿真整定中，认证筛选机制可在完整鲁棒评估前剔除边界内11.6%的随机采样增益，显著提升采样效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the practical challenges in robotic joint control where discrete-time execution, actuator saturation, and small delays cause PID controllers to deviate from ideal continuous-time theory. The authors develop an implementation-aware tuning workflow that analytically derives PI stability regions using Jury criterion for different discretization methods, evaluates a discrete anti-windup scheme, and proposes a hybrid-certified Bayesian optimization method that screens unstable candidates while optimizing a robust integral absolute error objective with penalties on overshoot and saturation. Experimental results show that robustness-oriented tuning improves median IAE from 0.843 to 0.430 while maintaining median overshoot below 2%, with the certification screen rejecting 11.6% of randomly sampled gains to improve optimization efficiency.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人关节控制中离散时间执行、执行器饱和及微小延迟导致PID控制偏离理想连续时间理论的实践挑战，提出了一种约束感知的调参流程。该方法通过Jury判据解析推导不同离散化下的PI稳定区域，评估离散抗饱和方案，并采用混合认证的贝叶斯优化，在优化鲁棒积分绝对误差目标的同时筛选不稳定候选参数和不安全瞬态。实验结果表明，在包含延迟、噪声和更严格饱和的随机模型不确定性下，面向鲁棒性的调参将中位积分绝对误差从0.843改善至0.430，同时将中位超调保持在2%以下，且认证筛选在完整鲁棒评估前拒绝了11.6%的随机采样增益，提高了采样效率。</div>
</details>
</div>
<div class="card">
<div class="title">HiCrowd: Hierarchical Crowd Flow Alignment for Dense Human Environments</div>
<div class="meta-line">Authors: Yufei Zhu, Shih-Min Yang, Martin Magnusson, Allan Wang</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-05T12:46:37+00:00 · Latest: 2026-02-05T12:46:37+00:00</div>
<div class="meta-line">Comments: Accepted to the 2026 IEEE International Conference on Robotics and Automation (ICRA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Navigating through dense human crowds remains a significant challenge for mobile robots. A key issue is the freezing robot problem, where the robot struggles to find safe motions and becomes stuck within the crowd. To address this, we propose HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). HiCrowd leverages surrounding pedestrian motion as guidance, enabling the robot to align with compatible crowd flows. A high-level RL policy generates a follow point to align the robot with a suitable pedestrian group, while a low-level MPC safely tracks this guidance with short horizon planning. The method combines long-term crowd aware decision making with safe short-term execution. We evaluate HiCrowd against reactive and learning-based baselines in offline setting (replaying recorded human trajectories) and online setting (human trajectories are updated to react to the robot in simulation). Experiments on a real-world dataset and a synthetic crowd dataset show that our method outperforms in navigation efficiency and safety, while reducing freezing behaviors. Our results suggest that leveraging human motion as guidance, rather than treating humans solely as dynamic obstacles, provides a powerful principle for safe and efficient robot navigation in crowds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiCrowd：面向密集人群环境的层次化人流对齐方法</div>
<div class="mono" style="margin-top:8px">在密集人群中导航对移动机器人仍是一项重大挑战，核心难题是机器人因无法找到安全运动路径而陷入停滞的“冻结”问题。为此，我们提出HiCrowd——一种融合强化学习与模型预测控制的层次化框架。该方法以周围行人运动为引导，使机器人能够融入相容的人流动态。高层强化学习策略生成跟随点以引导机器人适配行人群体，底层模型预测控制则通过短时域规划安全追踪该引导。该框架结合了长期人群感知决策与安全的短期执行能力。我们在离线场景（重放记录的人类轨迹）和在线场景（仿真中人类轨迹根据机器人动态更新）中，将HiCrowd与反应式及基于学习的基线方法进行比较。在真实世界数据集和合成人群数据集上的实验表明，本方法在导航效率与安全性方面均表现更优，同时显著减少了冻结行为。研究结果揭示：将人类运动视为引导而非单纯动态障碍，为人群中的机器人安全高效导航提供了有效范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the freezing robot problem in dense human crowds, where robots become stuck due to difficulty finding safe motions, this work introduces HiCrowd, a hierarchical framework that integrates reinforcement learning (RL) with model predictive control (MPC). The method leverages surrounding pedestrian motion as guidance, using a high-level RL policy to generate a follow point that aligns the robot with a compatible crowd flow, while a low-level MPC safely tracks this guidance with short-horizon planning. Experimental evaluations on real-world and synthetic crowd datasets, in both offline and online simulation settings, demonstrate that HiCrowd outperforms reactive and learning-based baselines in navigation efficiency and safety, effectively reducing freezing behaviors.</div>
<div class="mono" style="margin-top:8px">为解决机器人在密集人群中的冻结问题（即因难以找到安全运动而卡住），本文提出了HiCrowd，一个将强化学习（RL）与模型预测控制（MPC）相结合的分层框架。该方法利用周围行人运动作为引导，通过高层RL策略生成跟随点以对齐兼容的人流，并由低层MPC进行安全的短时域跟踪。在真实世界和合成人群数据集上的实验评估，包括离线回放和在线交互模拟，表明HiCrowd在导航效率和安全性上优于反应式和基于学习的基线方法，同时显著减少了冻结行为。</div>
</details>
</div>
<div class="card">
<div class="title">TOLEBI: Learning Fault-Tolerant Bipedal Locomotion via Online Status Estimation and Fallibility Rewards</div>
<div class="meta-line">Authors: Hokyun Lee, Woo-Jeong Baek, Junhyeok Cha, Jaeheung Park</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-05T12:30:49+00:00 · Latest: 2026-02-05T12:30:49+00:00</div>
<div class="meta-line">Comments: Accepted for Publication at IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05596v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05596v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the growing employment of learning algorithms in robotic applications, research on reinforcement learning for bipedal locomotion has become a central topic for humanoid robotics. While recently published contributions achieve high success rates in locomotion tasks, scarce attention has been devoted to the development of methods that enable to handle hardware faults that may occur during the locomotion process. However, in real-world settings, environmental disturbances or sudden occurrences of hardware faults might yield severe consequences. To address these issues, this paper presents TOLEBI (A faulT-tOlerant Learning framEwork for Bipedal locomotIon) that handles faults on the robot during operation. Specifically, joint locking, power loss and external disturbances are injected in simulation to learn fault-tolerant locomotion strategies. In addition to transferring the learned policy to the real robot via sim-to-real transfer, an online joint status module incorporated. This module enables to classify joint conditions by referring to the actual observations at runtime under real-world conditions. The validation experiments conducted both in real-world and simulation with the humanoid robot TOCABI highlight the applicability of the proposed approach. To our knowledge, this manuscript provides the first learning-based fault-tolerant framework for bipedal locomotion, thereby fostering the development of efficient learning methods in this field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TOLEBI：通过在线状态估计与容错奖励学习双足容错步态</div>
<div class="mono" style="margin-top:8px">随着学习算法在机器人应用中的日益普及，针对双足步态的强化学习研究已成为仿人机器人领域的核心议题。尽管近期成果在步态任务中实现了较高的成功率，但鲜有研究关注如何开发能够处理步态过程中可能发生的硬件故障的方法。然而在现实场景中，环境干扰或硬件突发故障可能导致严重后果。为此，本文提出TOLEBI（双足步态容错学习框架），用于处理机器人运行期间的故障。具体通过在仿真中注入关节锁定、动力丧失和外部干扰，学习容错步态策略。除通过仿真到现实的迁移将习得策略转移至实体机器人外，还引入了在线关节状态模块。该模块能够根据实际运行时的观测数据，在现实条件下对关节状态进行分类。基于仿人机器人TOCABI开展的仿真与实体验证实验，突显了所提方法的适用性。据我们所知，本文首次提出了基于学习的双足步态容错框架，从而推动该领域高效学习方法的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for fault-tolerant bipedal locomotion in real-world settings, where hardware faults and environmental disturbances can cause severe failures. The proposed TOLEBI framework learns robust policies in simulation by injecting joint locking, power loss, and external disturbances, and incorporates an online joint status estimation module to classify joint conditions during real-world operation for sim-to-real transfer. Experimental validation on the TOCABI humanoid robot demonstrates that the approach successfully enables fault-tolerant locomotion under various injected faults, representing a first learning-based framework for this problem.</div>
<div class="mono" style="margin-top:8px">本研究针对现实世界中硬件故障和环境干扰可能导致严重故障的问题，提出了用于双足行走容错学习的需求。TOLEBI框架通过在仿真中注入关节锁定、电源丢失和外部干扰来学习鲁棒策略，并集成了在线关节状态估计模块，以在真实世界运行时根据实际观测分类关节状态，实现有效的仿真到现实迁移。在TOCABI人形机器人上进行的实验验证表明，该方法成功实现了容错行走，是该领域首个基于学习的容错框架。</div>
</details>
</div>
<div class="card">
<div class="title">PIRATR: Parametric Object Inference for Robotic Applications with Transformers in 3D Point Clouds</div>
<div class="meta-line">Authors: Michael Schwingshackl, Fabio F. Oberweger, Mario Niedermeyer, Huemer Johannes, Markus Murschitz</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-05T11:29:09+00:00 · Latest: 2026-02-05T11:29:09+00:00</div>
<div class="meta-line">Comments: 8 Pages, 11 Figures, Accepted at 2026 IEEE International Conference on Robotics &amp; Automation (ICRA) Vienna</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05557v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05557v1">PDF</a> · <a href="https://github.com/swingaxe/piratr">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present PIRATR, an end-to-end 3D object detection framework for robotic use cases in point clouds. Extending PI3DETR, our method streamlines parametric 3D object detection by jointly estimating multi-class 6-DoF poses and class-specific parametric attributes directly from occlusion-affected point cloud data. This formulation enables not only geometric localization but also the estimation of task-relevant properties for parametric objects, such as a gripper&#x27;s opening, where the 3D model is adjusted according to simple, predefined rules. The architecture employs modular, class-specific heads, making it straightforward to extend to novel object types without re-designing the pipeline. We validate PIRATR on an automated forklift platform, focusing on three structurally and functionally diverse categories: crane grippers, loading platforms, and pallets. Trained entirely in a synthetic environment, PIRATR generalizes effectively to real outdoor LiDAR scans, achieving a detection mAP of 0.919 without additional fine-tuning. PIRATR establishes a new paradigm of pose-aware, parameterized perception. This bridges the gap between low-level geometric reasoning and actionable world models, paving the way for scalable, simulation-trained perception systems that can be deployed in dynamic robotic environments. Code available at https://github.com/swingaxe/piratr.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PIRATR：基于Transformer的3D点云参数化物体推理机器人应用框架</div>
<div class="mono" style="margin-top:8px">本文提出PIRATR——一种面向机器人点云应用场景的端到端三维物体检测框架。该方法在PI3DETR基础上进行扩展，通过直接从受遮挡点云数据中联合估计多类别六自由度位姿与类别专属参数化属性，实现了参数化三维物体检测流程的优化。该框架不仅能完成几何定位，还可估计参数化物体的任务相关属性（如夹爪开合度），并依据预定义规则调整三维模型。架构采用模块化的类别专属检测头，无需重构流程即可扩展至新型物体类别。我们在自动叉车平台上验证了PIRATR，聚焦于结构功能各异的三大类别：起重机夹爪、装载平台与托盘。完全在合成环境训练的PIRATR能有效泛化至真实室外激光雷达扫描数据，在未经微调的情况下达到0.919的检测平均精度。PIRATR建立了位姿感知参数化感知的新范式，弥合了底层几何推理与可操作世界模型之间的鸿沟，为可扩展的仿真训练感知系统在动态机器人环境中的部署铺平道路。代码发布于https://github.com/swingaxe/piratr。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for robotic systems to perceive not only object locations but also task-relevant parametric attributes from occlusion-affected 3D point clouds. The method, PIRATR, is an end-to-end detection framework that extends PI3DETR; it uses a transformer-based architecture with modular, class-specific heads to jointly estimate multi-class 6-DoF poses and parametric attributes directly from point clouds, enabling the adjustment of 3D models via predefined rules. Experimental validation on an automated forklift platform with crane grippers, loading platforms, and pallets shows that PIRATR, trained solely on synthetic data, generalizes to real outdoor LiDAR scans without fine-tuning, achieving a mean average precision (mAP) of 0.919.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决机器人系统从受遮挡的3D点云中感知物体位置及任务相关参数化属性的需求。所提出的PIRATR方法扩展了PI3DETR，构建了一个端到端框架，利用Transformer直接从点云中联合估计多类别6自由度位姿和类别特定的参数化属性，并采用模块化头部以便轻松扩展到新物体类型。在自动化叉车平台上对起重机抓具、装载平台和托盘进行的实验验证表明，该模型仅使用合成数据训练，无需微调即可泛化到真实室外激光雷达扫描，实现了0.919的平均精度均值（mAP）。</div>
</details>
</div>
<div class="card">
<div class="title">IndustryShapes: An RGB-D Benchmark dataset for 6D object pose estimation of industrial assembly components and tools</div>
<div class="meta-line">Authors: Panagiotis Sapoutzoglou, Orestis Vaggelis, Athina Zacharia, Evangelos Sartinas, Maria Pateraki</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-05T11:28:57+00:00 · Latest: 2026-02-05T11:28:57+00:00</div>
<div class="meta-line">Comments: To appear in ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05555v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05555v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pose-lab.github.io/IndustryShapes">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce IndustryShapes, a new RGB-D benchmark dataset of industrial tools and components, designed for both instance-level and novel object 6D pose estimation approaches. The dataset provides a realistic and application-relevant testbed for benchmarking these methods in the context of industrial robotics bridging the gap between lab-based research and deployment in real-world manufacturing scenarios. Unlike many previous datasets that focus on household or consumer products or use synthetic, clean tabletop datasets, or objects captured solely in controlled lab environments, IndustryShapes introduces five new object types with challenging properties, also captured in realistic industrial assembly settings. The dataset has diverse complexity, from simple to more challenging scenes, with single and multiple objects, including scenes with multiple instances of the same object and it is organized in two parts: the classic set and the extended set. The classic set includes a total of 4,6k images and 6k annotated poses. The extended set introduces additional data modalities to support the evaluation of model-free and sequence-based approaches. To the best of our knowledge, IndustryShapes is the first dataset to offer RGB-D static onboarding sequences. We further evaluate the dataset on a representative set of state-of-the art methods for instance-based and novel object 6D pose estimation, including also object detection, segmentation, showing that there is room for improvement in this domain. The dataset page can be found in https://pose-lab.github.io/IndustryShapes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IndustryShapes：面向工业装配零件与工具的六维位姿估计RGB-D基准数据集</div>
<div class="mono" style="margin-top:8px">本文推出IndustryShapes——一个面向工业工具与零部件的新型RGB-D基准数据集，专为实例级及新物体六维位姿估计方法设计。该数据集为工业机器人领域的算法评估提供了真实且具应用价值的测试平台，弥合了实验室研究与实际制造场景部署之间的鸿沟。与以往聚焦家用消费品、采用合成化洁净桌面数据集或仅在受控实验室环境采集的数据集不同，IndustryShapes引入五种具有挑战性特性的新型物体类别，并在真实工业装配场景中完成采集。数据集涵盖从简单到复杂的多样化场景，包含单物体与多物体配置，其中涉及同一物体的多实例场景，整体分为经典集与扩展集两部分：经典集包含4.6千幅图像与6千个标注位姿；扩展集新增数据模态以支持无模型与序列化方法的评估。据我们所知，IndustryShapes是首个提供RGB-D静态上料序列的数据集。我们进一步采用代表性前沿方法对数据集进行评估，涵盖实例级与新物体六维位姿估计（含物体检测与分割任务），结果表明该领域仍有提升空间。数据集页面详见：https://pose-lab.github.io/IndustryShapes。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To bridge the gap between lab research and real-world industrial robotics applications, this work introduces IndustryShapes, an RGB-D benchmark dataset for 6D object pose estimation. The method involves capturing five new types of industrial tools and components in realistic assembly settings, organizing the data into a classic set with thousands of annotated images and an extended set with additional modalities like static onboarding sequences. Experimental evaluation with state-of-the-art methods for instance-based and novel object pose estimation, including detection and segmentation, demonstrates that the dataset presents significant challenges, indicating substantial room for improvement in this domain.</div>
<div class="mono" style="margin-top:8px">为弥合实验室研究与现实工业机器人应用之间的差距，本研究提出了IndustryShapes，一个用于6D物体姿态估计的RGB-D基准数据集。该数据集在真实的装配场景中捕获了五种具有挑战性的工业工具和组件，提供了经典的标注图像集和包含静态上架序列等新型数据模态的扩展集。在基于实例和新物体姿态估计的先进方法上进行实验评估，结果表明现有方法仍有很大改进空间，凸显了该数据集作为一个具有挑战性且贴合实际应用的测试平台的价值。</div>
</details>
</div>
<div class="card">
<div class="title">VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator</div>
<div class="meta-line">Authors: Bessie Dominguez-Dager, Sergio Suescun-Ferrandiz, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla</div>
<div class="meta-line">First: 2026-02-05T11:23:11+00:00 · Latest: 2026-02-05T11:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05552v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05552v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLN-Pilot：大型视觉语言模型作为自主室内无人机操作员</div>
<div class="mono" style="margin-top:8px">本文提出VLN-Pilot，一种创新框架，其中大型视觉语言模型（VLLM）扮演室内无人机导航的人类飞行员角色。该框架利用VLLM的多模态推理能力，解析自由形式的自然语言指令，并将其与视觉观测相结合，在无GPS的室内环境中规划并执行无人机轨迹。与传统基于规则或几何路径规划方法不同，本框架将语言驱动的语义理解与视觉感知相融合，实现了具备情境感知能力的高层飞行行为，且无需大量任务特定工程。VLN-Pilot通过推理空间关系、避障及对突发事件的动态响应，支持无人机完全自主的指令跟随。我们在定制的高真实感室内仿真基准上验证了该框架，并证明VLLM驱动的智能体能在复杂指令跟随任务（包括含多语义目标的远距离导航）中实现高成功率。实验结果凸显了用语言引导自主智能体替代远程无人机飞行员的潜力，为室内无人机在巡检、搜救、设施监控等任务中实现可扩展、人性化控制开辟了新途径。研究表明，基于VLLM的飞行员可显著降低操作负荷，同时在受限室内环境中提升安全性与任务灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to replace human remote pilots for indoor drone operations by developing an autonomous agent that can understand high-level natural language commands. The proposed VLN-Pilot framework employs a large Vision-and-Language Model (VLLM) to interpret free-form instructions, ground them in visual observations from the drone, and directly plan and execute flight trajectories in GPS-denied environments, integrating semantic understanding with visual perception. Experimental validation in a custom photorealistic indoor simulator demonstrates that the VLLM-driven agent achieves high success rates on complex, long-horizon navigation tasks with multiple semantic targets, showing promise for reducing operator workload and improving safety in applications like inspection and search-and-rescue.</div>
<div class="mono" style="margin-top:8px">本研究旨在用自主智能体取代室内无人机导航中的人类操作员，以实现检查、监控等任务的可扩展且人性化的控制。提出的VLN-Pilot框架采用大型视觉语言模型，通过解释自由形式的自然语言指令，将其与视觉观察相结合，在无GPS的室内环境中自主规划和执行飞行轨迹，实现了语义理解与视觉感知的融合。在定制的逼真室内模拟基准上的实验表明，该智能体在复杂、长距离的指令跟随任务中取得了高成功率，证明了其降低操作员工作量、提升安全性和任务灵活性的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Virtual-Tube-Based Cooperative Transport Control for Multi-UAV Systems in Constrained Environments</div>
<div class="meta-line">Authors: Runxiao Liu, Pengda Mao, Xiangli Le, Shuang Gu, Yapeng Chen, Quan Quan</div>
<div class="meta-line">First: 2026-02-05T10:16:05+00:00 · Latest: 2026-02-05T10:16:05+00:00</div>
<div class="meta-line">Comments: 10 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05516v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05516v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a novel control framework for cooperative transportation of cable-suspended loads by multiple unmanned aerial vehicles (UAVs) operating in constrained environments. Leveraging virtual tube theory and principles from dissipative systems theory, the framework facilitates efficient multi-UAV collaboration for navigating obstacle-rich areas. The proposed framework offers several key advantages. (1) It achieves tension distribution and coordinated transportation within the UAV-cable-load system with low computational overhead, dynamically adapting UAV configurations based on obstacle layouts to facilitate efficient navigation. (2) By integrating dissipative systems theory, the framework ensures high stability and robustness, essential for complex multi-UAV operations. The effectiveness of the proposed approach is validated through extensive simulations, demonstrating its scalability for large-scale multi-UAV systems. Furthermore, the method is experimentally validated in outdoor scenarios, showcasing its practical feasibility and robustness under real-world conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>受限环境下多无人机系统基于虚拟管道的协同运输控制</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的控制框架，用于在受限环境中由多架无人机协同运输缆绳悬挂载荷。该框架结合虚拟管道理论与耗散系统理论原理，促进多无人机在障碍密集区域的高效协作。所提框架具有多项关键优势：(1) 在无人机-缆绳-载荷系统中以低计算开销实现张力分配与协同运输，并能根据障碍物布局动态调整无人机构型以优化导航效率；(2) 通过融合耗散系统理论，确保系统具备高稳定性与鲁棒性，这对复杂多无人机操作至关重要。通过大量仿真验证了该方法的有效性，证明其适用于大规模多无人机系统；同时，户外场景的实验验证进一步展示了该方法在实际条件下的可行性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of cooperative transportation of cable-suspended loads by multiple UAVs in obstacle-rich, constrained environments. The proposed method develops a control framework that integrates virtual tube theory for path planning and dissipative systems theory to ensure stability, enabling dynamic UAV formation adaptation and efficient tension distribution with low computational cost. Experimental validation through simulations and outdoor tests confirms the framework&#x27;s scalability, robustness, and practical feasibility for large-scale multi-UAV operations.</div>
<div class="mono" style="margin-top:8px">本研究针对多无人机在障碍物密集的受限环境中协同运输缆绳悬挂负载的挑战，需实现高效协作与鲁棒导航。该方法提出了一种融合虚拟管道理论进行路径规划和耗散系统理论以确保稳定性的控制框架，能够以低计算成本动态调整无人机编队并分配张力。通过大量仿真和户外实验验证，该框架被证实具有大规模系统的可扩展性、实际可行性以及在真实环境中的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">DECO: Decoupled Multimodal Diffusion Transformer for Bimanual Dexterous Manipulation with a Plugin Tactile Adapter</div>
<div class="meta-line">Authors: Xukun Li, Yu Sun, Lei Zhang, Bosheng Huang, Yibo Peng, Yuan Meng, Haojun Jiang, Shaoxuan Xie, Guacai Yao, Alois Knoll, Zhenshan Bing, Xinlong Wang, Zhenguo Sun</div>
<div class="meta-line">First: 2026-02-05T10:13:34+00:00 · Latest: 2026-02-05T10:13:34+00:00</div>
<div class="meta-line">Comments: 17 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05513v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05513v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Overview of the Proposed DECO Framework.} DECO is a DiT-based policy that decouples multimodal conditioning. Image and action tokens interact via joint self attention, while proprioceptive states and optional conditions are injected through adaptive layer normalization. Tactile signals are injected via cross attention, while a lightweight LoRA-based adapter is used to efficiently fine-tune the pretrained policy. DECO is also accompanied by DECO-50, a bimanual dexterous manipulation dataset with tactile sensing, consisting of 4 scenarios and 28 sub-tasks, covering more than 50 hours of data, approximately 5 million frames, and 8,000 successful trajectories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DECO：用于双手机器人灵巧操作的解耦多模态扩散Transformer及插件式触觉适配器</div>
<div class="mono" style="margin-top:8px">DECO框架概述：DECO是一种基于扩散Transformer的策略，实现多模态条件解耦。图像与动作令牌通过联合自注意力交互，本体感知状态与可选条件通过自适应层归一化注入。触觉信号通过交叉注意力注入，并采用基于LoRA的轻量级适配器对预训练策略进行高效微调。同时推出DECO-50数据集，包含触觉感知的双手机器人灵巧操作数据，涵盖4种场景、28项子任务，数据总量超50小时（约500万帧、8000条成功轨迹）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To advance bimanual dexterous manipulation with multimodal sensory inputs, this research introduces DECO, a decoupled multimodal diffusion transformer policy. The method processes image and action tokens through joint self-attention, injects proprioceptive states via adaptive layer normalization, and incorporates tactile signals via cross-attention, enhanced by a lightweight LoRA-based adapter for efficient fine-tuning. Key experimental findings are supported by DECO-50, a novel dataset containing over 50 hours of data across 4 scenarios and 28 sub-tasks, which includes approximately 5 million frames and 8,000 successful trajectories, demonstrating the framework&#x27;s capability in handling complex manipulation tasks with tactile sensing.</div>
<div class="mono" style="margin-top:8px">本研究针对双手灵巧操作中多模态感知融合的挑战，提出了DECO，一种解耦的多模态扩散变换器策略。该方法通过联合自注意力处理图像和动作令牌，利用自适应层归一化注入本体感知状态，并通过交叉注意力整合触觉信号，同时采用轻量级的LoRA适配器进行高效策略微调。关键实验结果基于DECO-50数据集，该数据集包含4个场景和28个子任务，覆盖超过50小时的数据，证明了该框架在处理带触觉反馈的复杂操作任务中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">FilMBot: A High-Speed Soft Parallel Robotic Micromanipulator</div>
<div class="meta-line">Authors: Jiangkun Yu, Houari Bettahar, Hakan Kandemir, Quan Zhou</div>
<div class="meta-line">Venue: IEEE Transactions on Robotics, 2026</div>
<div class="meta-line">First: 2024-10-30T14:33:22+00:00 · Latest: 2026-02-05T09:54:13+00:00</div>
<div class="meta-line">Comments: 13 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.23059v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.23059v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Soft robotic manipulators are generally slow despite their great adaptability, resilience, and compliance. This limitation also extends to current soft robotic micromanipulators. Here, we introduce FilMBot, a 3-DOF film-based, electromagnetically actuated, soft kinematic robotic micromanipulator achieving speeds up to 2117 °/s and 2456 °/s in α and \{beta} angular motions, with corresponding linear velocities of 1.61 m/s and 1.92 m/s using a 4-cm needle end-effector, 0.54 m/s along the Z axis, and 1.57 m/s during Z-axis morph switching. The robot can reach ~1.50 m/s in path-following tasks, with an operational bandwidth below ~30 Hz, and remains responsive at 50 Hz. It demonstrates high precision (~6.3 μm, or ~0.05% of its workspace) in path-following tasks, with precision remaining largely stable across frequencies. The novel combination of the low-stiffness soft kinematic film structure and strong electromagnetic actuation in FilMBot opens new avenues for soft robotics. Furthermore, its simple construction and inexpensive, readily accessible components could broaden the application of micromanipulators beyond current academic and professional users.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FilMBot：一种高速软体并联机器人微操作器</div>
<div class="mono" style="margin-top:8px">软体机器人操作器虽具备出色的适应性、恢复性与柔顺性，但普遍存在速度缓慢的问题，当前软体机器人微操作器亦受此限制。本文提出FilMBot——一种基于薄膜结构、电磁驱动的三自由度软体运动学机器人微操作器，其α与β角运动速度分别达2117°/s与2456°/s（使用4厘米针状末端执行器时对应线速度分别为1.61 m/s与1.92 m/s），Z轴方向速度达0.54 m/s，Z轴形态切换时速度达1.57 m/s。该机器人在路径跟踪任务中速度可达约1.50 m/s，工作带宽低于约30 Hz，且在50 Hz下仍保持响应能力。路径跟踪任务中展现出高精度（约6.3 μm，约占工作空间的0.05%），且精度在不同频率下基本保持稳定。FilMBot通过将低刚度软体运动学薄膜结构与强电磁驱动创新结合，为软体机器人领域开辟了新途径。此外，其结构简单、组件成本低廉且易于获取的特点，有望将微操作器的应用范围拓展至当前学术与专业用户之外的更广领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Soft robotic micromanipulators are limited by low operational speeds despite their advantages in adaptability and compliance. To address this, the authors present FilMBot, a high-speed, three-degree-of-freedom soft robotic micromanipulator that uses a film-based kinematic structure and electromagnetic actuation. Experimental results show it achieves angular speeds exceeding 2100 °/s, linear velocities up to 1.92 m/s, an operational bandwidth below 30 Hz, and a path-following precision of approximately 6.3 micrometers, demonstrating that its performance remains stable across different frequencies.</div>
<div class="mono" style="margin-top:8px">尽管软体机器人微操作器具有适应性强、顺应性好的优点，但其普遍存在运行速度慢的局限。为此，本研究提出了FilMBot，一种高速、三自由度的软体微操作器，其采用基于薄膜的低刚度软体运动学结构与强电磁驱动相结合的方法。实验结果表明，该系统角速度超过2100 °/s，线速度高达1.92 m/s，工作带宽低于30 Hz，路径跟踪精度约为6.3微米，验证了在结构简单、成本低廉的软体机器人平台上实现高速高精度操作的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Do Robots Really Need Anthropomorphic Hands? -- A Comparison of Human and Robotic Hands</div>
<div class="meta-line">Authors: Alexander Fabisch, Wadhah Zai El Amri, Chandandeep Singh, Nicolás Navarro-Guerrero</div>
<div class="meta-line">First: 2025-08-07T14:07:51+00:00 · Latest: 2026-02-05T09:49:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05415v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.05415v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human manipulation skills represent a pinnacle of their voluntary motor functions, requiring the coordination of many degrees of freedom and processing of high-dimensional sensor input to achieve such a high level of dexterity. Thus, we attempt to answer whether the human hand, with its associated biomechanical properties, sensors, and control mechanisms, is an ideal that we should strive for in robotics-do we really need anthropomorphic robotic hands? This survey can help practitioners to make the trade-off between hand complexity and potential manipulation skills. We provide an overview of the human hand, a comparison of commercially available robotic and prosthetic hands, and a systematic review of hand mechanisms and skills that they are capable of. This leads to follow-up questions. What is the minimum requirement for mechanisms and sensors to implement most skills that a robot needs? What is missing to reach human-level dexterity? Can we improve upon human dexterity? Although complex five-fingered hands are often used as the ultimate goal for robotic manipulators, they are not necessary for all tasks. We found that wrist flexibility and finger abduction/adduction are often more important for manipulation capabilities. Increasing the number of fingers, actuators, or degrees of freedom is not always necessary. Three fingers often are a good compromise between simplicity and dexterity. Non-anthropomorphic hand designs with two opposing pairs of fingers or human hands with six fingers can further increase dexterity, suggesting that the human hand is not the optimum. Consequently, we argue for function-based rather than form-based biomimicry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人真的需要拟人化手吗？——人类与机器人手部对比研究</div>
<div class="mono" style="margin-top:8px">人类的手部操作技能代表了其自主运动功能的顶峰，需要协调多个自由度并处理高维传感器输入才能实现如此高超的灵巧性。因此，本文试图探讨：具有相应生物力学特性、传感器和控制机制的人类手部，是否应成为机器人领域的理想追求目标——我们真的需要拟人化机器人手吗？本综述可帮助从业者权衡手部复杂性与潜在操作能力。我们系统梳理了人类手部结构、市售机器人手与假肢手的对比研究，以及现有手部机构与功能的系统性分析。由此引申出后续问题：实现机器人所需多数技能对机构和传感器的最低要求是什么？达到人类级灵巧性还欠缺什么？我们能否超越人类的灵巧水平？尽管复杂的五指手常被视为机器人操作器的终极目标，但并非所有任务都需要。研究发现，腕部灵活性和手指外展/内收能力对操作性能往往更为关键。增加手指数量、驱动器或自由度并非总是必要，三指设计常能在简洁性与灵巧性间取得良好平衡。采用两对对立手指的非拟人化设计，或设想具有六指的人类手部，都可能进一步提升灵巧性，这表明人类手部并非最优解。因此，我们主张应基于功能而非形态进行仿生设计。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether anthropomorphic robotic hands are necessary for achieving high dexterity, motivated by the question of whether the human hand is the ideal model for robotics. The method involves a comparative survey of the human hand, commercially available robotic and prosthetic hands, and a systematic review of hand mechanisms and their manipulation skills. Key experimental findings indicate that complex five-fingered designs are not essential for all tasks; instead, wrist flexibility and finger abduction/adduction are often more critical, with three-finger designs offering a good balance between simplicity and dexterity, and non-anthropomorphic or six-fingered hands potentially surpassing human capabilities, advocating for function-based biomimicry.</div>
<div class="mono" style="margin-top:8px">本综述旨在探究拟人化机器人手是否为实现人类灵巧性所必需，其动机源于理解手部复杂性与操作能力之间权衡的需求。研究方法包括系统比较人类手的生物力学和控制机制与商用机器人手及假肢手，并回顾其结构和技能。主要实验结果表明，复杂的五指设计并非普遍必要；相反，手腕灵活性和手指的外展/内收通常更为关键，而三指或非拟人化设计（如具有两对对立手指）能在简单性和灵巧性之间取得更好平衡，有时甚至超越人手性能，因此主张基于功能而非形式的仿生学。</div>
</details>
</div>
<div class="card">
<div class="title">RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation</div>
<div class="meta-line">Authors: Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-12-30T13:25:22+00:00 · Latest: 2026-02-05T09:33:50+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24212v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24212v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RANGER：基于上下文适应的单目零样本语义导航框架</div>
<div class="mono" style="margin-top:8px">在复杂环境中高效寻找目标是现实世界具身应用的基础。尽管多模态基础模型的最新进展已实现零样本目标导航，使机器人无需微调即可搜索任意物体，但现有方法存在两个关键局限：（1）过度依赖模拟器提供的精确深度与位姿信息，限制了实际场景的适用性；（2）缺乏上下文学习能力，难以快速适应新环境（如利用短视频）。为应对这些挑战，我们提出RANGER——一种仅使用单目相机的新型零样本开放词汇语义导航框架。该框架借助强大的3D基础模型，在消除对深度与位姿依赖的同时展现出强大的上下文学习能力。仅通过观察新环境的短视频，系统即可显著提升任务效率，无需架构修改或微调。框架集成多个关键组件：基于关键帧的3D重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应路径点选择与底层动作执行。在HM3D基准测试和真实环境中的实验表明，RANGER在导航成功率和探索效率方面达到竞争性性能，同时展现出卓越的上下文学习适应能力，且无需预先构建环境3D地图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in zero-shot object goal navigation, where existing methods heavily rely on precise depth and pose data from simulators and lack in-context learning (ICL) for quick adaptation to new environments. The proposed RANGER framework uses only a monocular camera, leveraging 3D foundation models for keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model-driven exploration value estimation, adaptive waypoint selection, and low-level action execution to eliminate dependency on depth and pose while enabling ICL. Experimental results on the HM3D benchmark and real-world environments show that RANGER achieves competitive navigation success rates and exploration efficiency, with superior ICL adaptability that improves task efficiency after observing short environmental videos, without requiring prior 3D mapping.</div>
<div class="mono" style="margin-top:8px">该研究针对零样本目标导航中的局限性，现有方法严重依赖模拟器提供的精确深度和姿态数据，且缺乏快速适应新环境的上下文学习能力。提出的RANGER框架仅使用单目摄像头，利用3D基础模型进行关键帧3D重建、语义点云生成、视觉语言模型驱动的探索价值估计、自适应路径点选择和底层动作执行，从而消除对深度和姿态的依赖，并通过观察短视频实现上下文学习。在HM3D基准测试和真实环境中的实验结果表明，RANGER在导航成功率和探索效率方面具有竞争力，并展现出卓越的上下文学习适应性，无需预先进行3D环境建图。</div>
</details>
</div>
<div class="card">
<div class="title">TACO: Temporal Consensus Optimization for Continual Neural Mapping</div>
<div class="meta-line">Authors: Xunlan Zhou, Hongrui Zhao, Negar Mehr</div>
<div class="meta-line">First: 2026-02-04T13:07:08+00:00 · Latest: 2026-02-05T09:31:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04516v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04516v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural implicit mapping has emerged as a powerful paradigm for robotic navigation and scene understanding. However, real-world robotic deployment requires continual adaptation to changing environments under strict memory and computation constraints, which existing mapping systems fail to support. Most prior methods rely on replaying historical observations to preserve consistency and assume static scenes. As a result, they cannot adapt to continual learning in dynamic robotic settings. To address these challenges, we propose TACO (TemporAl Consensus Optimization), a replay-free framework for continual neural mapping. We reformulate mapping as a temporal consensus optimization problem, where we treat past model snapshots as temporal neighbors. Intuitively, our approach resembles a model consulting its own past knowledge. We update the current map by enforcing weighted consensus with historical representations. Our method allows reliable past geometry to constrain optimization while permitting unreliable or outdated regions to be revised in response to new observations. TACO achieves a balance between memory efficiency and adaptability without storing or replaying previous data. Through extensive simulated and real-world experiments, we show that TACO robustly adapts to scene changes, and consistently outperforms other continual learning baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TACO：持续神经地图构建的时间一致性优化</div>
<div class="mono" style="margin-top:8px">神经隐式地图构建已成为机器人导航与场景理解的重要范式。然而，现实中的机器人部署需要在严格的内存与计算限制下持续适应动态环境，现有地图系统难以满足这一需求。多数现有方法依赖回放历史观测以保持一致性，并假设场景静态，因而无法适应动态机器人场景中的持续学习。为解决这些挑战，我们提出TACO（时间一致性优化），一种无需回放的持续神经地图构建框架。我们将地图构建重新定义为时间一致性优化问题，将历史模型快照视为时间邻域。直观上，该方法类似于模型咨询自身的历史知识。通过强制当前地图与历史表征达成加权一致性来实现更新，使可靠的过往几何结构约束优化，同时允许根据新观测修正不可靠或过时区域。TACO在无需存储或回放历史数据的前提下，实现了内存效率与适应性的平衡。通过大量仿真与真实场景实验，我们证明TACO能稳健适应场景变化，并持续优于其他持续学习基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing neural implicit mapping systems, which cannot adapt to changing environments under memory and computation constraints, by proposing TACO, a replay-free framework for continual neural mapping. The method reformulates mapping as a temporal consensus optimization problem, treating past model snapshots as temporal neighbors to enforce weighted consensus with historical representations, allowing reliable past geometry to constrain updates while revising outdated regions. Experimental results from simulated and real-world tests demonstrate that TACO robustly adapts to scene changes and consistently outperforms other continual learning baselines.</div>
<div class="mono" style="margin-top:8px">本研究针对现有神经隐式建图系统在内存和计算约束下难以持续适应动态环境、且常依赖重放静态场景数据的局限性。提出的TACO方法采用一种无需重放的框架，将建图重新定义为时间一致性优化问题，将过去模型快照视为时间邻居，使当前模型能够参考并与历史表示执行加权一致性约束，从而在更新可靠几何结构的同时修订过时区域。模拟和真实世界实验结果表明，TACO能稳健适应场景变化，且在不存储或重放先前数据的情况下，持续优于其他持续学习基线方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260208_0530.html">20260208_0530</a>
<a href="archive/20260208_0444.html">20260208_0444</a>
<a href="archive/20260208_0331.html">20260208_0331</a>
<a href="archive/20260207_0630.html">20260207_0630</a>
<a href="archive/20260207_0534.html">20260207_0534</a>
<a href="archive/20260207_0451.html">20260207_0451</a>
<a href="archive/20260207_0345.html">20260207_0345</a>
<a href="archive/20260206_0629.html">20260206_0629</a>
<a href="archive/20260206_0531.html">20260206_0531</a>
<a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
