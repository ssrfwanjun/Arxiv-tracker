<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-31 06:28</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260131_0628</div>
    <div class="row"><div class="card">
<div class="title">DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</div>
<div class="meta-line">Authors: Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-29T18:59:51+00:00 · Latest: 2026-01-29T18:59:51+00:00</div>
<div class="meta-line">Comments: Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22153v1">PDF</a> · <a href="https://github.com/hzxie/DynamicVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://www.infinitescript.com/project/dynamic-vla/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynamicVLA：面向动态物体操作的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">动态物体操作仍是视觉-语言-动作模型面临的开放挑战。尽管在静态操作中表现出强大的泛化能力，现有模型在需要快速感知、时序预测和持续控制的动态场景中仍存在局限。本文提出DynamicVLA动态物体操作框架，通过三项核心设计整合时序推理与闭环适应能力：1）采用卷积视觉编码器的紧凑型0.4B参数VLA模型，实现空间高效且结构保真的编码，支持快速多模态推理；2）连续推理机制，通过重叠式推理与执行降低延迟，实时适应物体运动；3）潜在感知动作流，通过时序对齐的动作执行弥合感知与执行的间隙。为填补动态操作数据空白，我们构建了动态物体操作基准数据集，通过自动化数据采集流程从零创建了涵盖2.8K场景、206类物体的20万条合成交互序列，并高效采集了2K条无需遥操作的真实世界交互数据。大量实验表明，该框架在响应速度、感知能力和泛化性能上取得显著提升，确立了DynamicVLA作为跨具身形态的通用动态物体操作统一框架的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Manipulating dynamic objects is a significant challenge for Vision-Language-Action models, which excel in static environments but lack the rapid perception and continuous control needed for moving targets. To address this, the authors propose DynamicVLA, a framework featuring a compact 0.4B parameter model with a convolutional vision encoder for efficient multimodal inference, a Continuous Inference mechanism for overlapping reasoning and execution to reduce latency, and Latent-aware Action Streaming to align perception with action temporally. They also introduce the Dynamic Object Manipulation benchmark, comprising 200K synthetic and 2K real-world episodes collected via an automated pipeline. Experimental results show that DynamicVLA achieves substantial improvements in response speed, perception accuracy, and generalization, establishing it as a unified solution for dynamic manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉-语言-动作模型在动态物体操控中的挑战，这类模型在静态场景中泛化能力强，但在处理需要快速感知、时序预测和连续控制的运动物体时存在困难。提出的DynamicVLA框架通过三个关键设计整合了时序推理和闭环适应：一个使用卷积视觉编码器的紧凑0.4B参数模型以实现高效编码；连续推理机制允许推理与执行重叠以降低延迟；以及潜在感知动作流技术来确保感知与执行的时间对齐。为支持训练与评估，作者构建了动态物体操控基准数据集，通过自动化流程收集了20万合成片段和2000个真实世界片段。实验结果表明，DynamicVLA在响应速度、感知精度和泛化能力上均取得显著提升，成为一个适用于动态操控任务的统一框架。</div>
</details>
</div>
<div class="card">
<div class="title">ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection</div>
<div class="meta-line">Authors: Runsheng Wang, Katelyn Lee, Xinyue Zhu, Lauren Winterbottom, Dawn M. Nilsen, Joel Stein, Matei Ciocarlie</div>
<div class="meta-line">First: 2026-01-29T18:26:51+00:00 · Latest: 2026-01-29T18:26:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22090v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReactEMG Stroke：基于表面肌电图的意图检测从健康到中风的少样本适应方法</div>
<div class="mono" style="margin-top:8px">表面肌电图（sEMG）是中风后手部按需辅助康复中一种有前景的控制信号，但从瘫痪肌肉中检测意图通常需要冗长的个体化校准，且对变异性敏感。我们提出一种从健康到中风的适应流程：首先利用在大规模健康人群sEMG数据上预训练的模型初始化意图检测器，随后仅使用少量个体数据对每位中风参与者进行微调。基于新收集的三名慢性中风患者数据集，我们比较了多种适应策略（仅调整头部层、参数高效的LoRA适配器、端到端全微调），并在包含真实分布偏移（如会话内漂移、姿势变化、臂带重新定位）的保留测试集上评估。在所有实验条件下，相较于相同数据量下的零样本迁移和仅使用中风数据的训练，基于健康数据预训练的适应方法持续提升了中风意图检测性能；最佳适应方法将平均转移准确率从0.42提升至0.61，原始准确率从0.69提升至0.78。结果表明，迁移可复用的健康域肌电表征能降低校准负担，同时增强实时中风后意图检测的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Surface electromyography (sEMG) is a promising control signal for assistive hand rehabilitation after stroke, but intent detection from paretic muscles typically requires extensive, subject-specific calibration and is sensitive to variability. To address this, the authors propose a healthy-to-stroke adaptation pipeline that initializes an intent detector with a model pretrained on large-scale able-bodied sEMG data and then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Evaluating on a new dataset from three chronic stroke individuals under realistic distribution shifts, they find that adaptation strategies like LoRA and fine-tuning consistently outperform zero-shot transfer and stroke-only training; the best methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78, demonstrating that transferring a healthy-domain representation can reduce calibration burden and enhance robustness for post-stroke intent detection.</div>
<div class="mono" style="margin-top:8px">表面肌电图（sEMG）是中风后手部康复的一种有前景的控制信号，但从麻痹肌肉中检测运动意图通常需要大量针对特定受试者的校准，且对变异性敏感。为此，研究者提出了一种从健康人到中风患者的适应流程：首先使用在大规模健康人sEMG数据上预训练的模型初始化意图检测器，然后仅用少量特定中风受试者的数据进行微调。在一个新收集的三名慢性中风患者数据集上，并在包含会话内漂移、姿势变化和臂带重新定位等现实分布偏移的测试集上评估，结果表明，与零样本迁移和同等数据量下的纯中风训练相比，采用LoRA等适应策略能持续提升性能，将平均过渡准确率从0.42提高到0.61，原始准确率从0.69提升至0.78，这表明迁移健康领域的肌电表征可以降低校准负担并增强鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning</div>
<div class="meta-line">Authors: Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath, Pieter Abbeel</div>
<div class="meta-line">First: 2026-01-29T18:11:26+00:00 · Latest: 2026-01-29T18:11:26+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/mujocolab/mjlab</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22074v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22074v1">PDF</a> · <a href="https://github.com/mujocolab/mjlab">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mjlab：面向GPU加速机器人学习的轻量级框架</div>
<div class="mono" style="margin-top:8px">本文介绍mjlab——一个轻量级开源的机器人学习框架，集成了GPU加速仿真、可组合环境模块与极简部署流程。该框架采用Isaac Lab提出的管理器式API架构，用户可通过模块化组件构建观测、奖励与事件系统，并耦合MuJoCo Warp实现GPU加速物理计算。其成果是仅需单行命令即可完成安装、依赖极简、且能直接访问原生MuJoCo数据结构的框架。mjlab同步提供了速度跟踪、运动模仿与操作任务的标准实现方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the need for a streamlined and efficient framework for robot learning that minimizes setup complexity while leveraging GPU acceleration. The method introduces mjlab, a lightweight open-source framework that integrates a manager-based API for composing modular environment components with MuJoCo Warp for GPU-accelerated physics simulation, enabling single-command installation and direct access to native MuJoCo data structures. Key experimental findings include the successful deployment of reference implementations for tasks such as velocity tracking, motion imitation, and manipulation, demonstrating the framework&#x27;s practical utility.</div>
<div class="mono" style="margin-top:8px">为降低机器人学习的配置复杂度并提升计算效率，本研究提出了mjlab这一轻量级开源框架。该方法结合了用于组合模块化环境组件的管理器API与MuJoCo Warp，以实现GPU加速的物理仿真，提供单命令安装和原生MuJoCo数据结构的直接访问。实验验证包括参考实现，展示了该框架在速度跟踪、运动模仿和操作任务上的能力。</div>
</details>
</div>
<div class="card">
<div class="title">AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation</div>
<div class="meta-line">Authors: Deepak Singh, Shreyas Khobragade, Nitin J. Sanket</div>
<div class="meta-line">First: 2026-01-24T18:41:40+00:00 · Latest: 2026-01-29T18:09:09+00:00</div>
<div class="meta-line">Comments: 8 pages, 10 figures, Published in IEEE Robotics And Automation Letters</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17550v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17550v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AsterNav：基于被动计算的全黑暗环境下自主空中机器人导航系统</div>
<div class="mono" style="margin-top:8px">全黑暗环境下的自主空中导航对灾后搜救至关重要，此类场景常因灾区断电陷入黑暗。然而受资源限制，本适用于此类任务的微型空中机器人无法在黑暗中安全导航寻找幸存者。本文提出一种结合大孔径编码镜头红外单目相机与结构光的自主空中机器人黑暗导航方案，无需GPS或动作捕捉等外部基础设施。该方法通过获取与深度相关的离焦线索（每个结构光点呈现为深度依赖的图案），为AsterNet深度估计模型提供强先验。模型通过简易光学模型生成的仿真数据训练，无需微调即可直接迁移至现实场景。AsterNet在NVIDIA Jetson Orin™ Nano上以20Hz频率机载运行。该网络对结构光图案变化及图案发射器与红外相机的相对位置具有鲁棒性，可实现简易低成本构建。我们通过纯机载传感与计算，在包含暗色哑光障碍物和细绳（直径6.25mm）的多组现实实验中验证了AsterNav深度导航方案，在未知物体形状、位置与材质的条件下取得95.5%的整体成功率。据我们所知，这是首个基于单目结构光的全黑暗四旋翼导航研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling tiny aerial robots to navigate autonomously in complete darkness for post-disaster search and rescue, where power outages are common. The method, AsterNav, combines an IR monocular camera with a large-aperture coded lens and structured light, using depth-dependent defocus cues from the structured light points as a prior for the AsterNet deep depth estimation model, which is trained solely in simulation and deployed without fine-tuning. Experimental results show the system runs at 20 Hz onboard, is robust to hardware variations, and achieves a 95.5% success rate in real-world tests navigating dark obstacles including thin ropes, using only onboard sensing and computation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决微型空中机器人在完全黑暗环境（如灾后断电的搜救场景）中自主导航的难题。提出的AsterNav方法将红外单目相机与大孔径编码透镜及结构光相结合，利用结构光点的深度相关散焦模式作为先验，通过名为AsterNet的深度学习模型进行深度估计。该模型仅使用光学模型在仿真中训练，无需微调即可直接迁移到现实世界，并在机载硬件上以20赫兹运行。实验结果表明，该方法在黑暗中能成功导航通过包括暗哑光物体和细绳在内的多种障碍物，仅使用机载传感与计算实现了95.5%的整体成功率。</div>
</details>
</div>
<div class="card">
<div class="title">EROAM: Event-based Camera Rotational Odometry and Mapping in Real-time</div>
<div class="meta-line">Authors: Wanli Xing, Shijie Lin, Linhan Yang, Zeqing Zhang, Yanjun Du, Maolin Lei, Yipeng Pan, Chen Wang, Jia Pan</div>
<div class="meta-line">First: 2024-11-17T08:50:47+00:00 · Latest: 2026-01-29T17:25:30+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Transactions on Robotics (T-RO), 2026. Project page: https://wlxing1901.github.io/eroam/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.11004v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.11004v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://wlxing1901.github.io/eroam/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents EROAM, a novel event-based rotational odometry and mapping system that achieves real-time, accurate camera rotation estimation. Unlike existing approaches that rely on event generation models or contrast maximization, EROAM employs a spherical event representation by projecting events onto a unit sphere and introduces Event Spherical Iterative Closest Point (ES-ICP), a novel geometric optimization framework designed specifically for event camera data. The spherical representation simplifies rotational motion formulation while operating in a continuous spherical domain, enabling enhanced spatial resolution. Our system features an efficient map management approach using incremental k-d tree structures and intelligent regional density control, ensuring optimal computational performance during long-term operation. Combined with parallel point-to-line optimization, EROAM achieves efficient computation without compromising accuracy. Extensive experiments on both synthetic and real-world datasets show that EROAM significantly outperforms state-of-the-art methods in terms of accuracy, robustness, and computational efficiency. Our method maintains consistent performance under challenging conditions, including high angular velocities and extended sequences, where other methods often fail or show significant drift. Additionally, EROAM produces high-quality panoramic reconstructions with preserved fine structural details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EROAM：基于事件相机的实时旋转里程计与建图系统</div>
<div class="mono" style="margin-top:8px">本文提出EROAM，一种创新的基于事件的旋转里程计与建图系统，能够实现实时、精确的相机旋转估计。与依赖事件生成模型或对比度最大化的现有方法不同，EROAM通过将事件投影至单位球面构建球面事件表示，并提出了专为事件相机数据设计的几何优化框架——事件球面迭代最近点（ES-ICP）。球面表示在连续球域中简化了旋转运动建模，同时提升了空间分辨率。系统采用基于增量k-d树的高效地图管理方法与智能区域密度控制策略，确保长期运行时的最优计算性能。结合并行点线优化，EROAM在不损失精度的前提下实现了高效计算。在合成与真实数据集上的大量实验表明，EROAM在精度、鲁棒性和计算效率方面显著优于现有先进方法。本方法在高角速度、长序列等挑战性场景下保持稳定性能，而其他方法常出现失效或显著漂移。此外，EROAM能生成保留精细结构细节的高质量全景重建结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for real-time, accurate rotational odometry and mapping using event cameras, which often struggle with high-speed motion and long-term drift. The proposed EROAM system projects events onto a unit sphere for a simplified rotational formulation and introduces Event Spherical Iterative Closest Point (ES-ICP), a geometric optimization framework, alongside efficient map management with incremental k-d trees and parallel point-to-line optimization. Experimental results on synthetic and real-world datasets demonstrate that EROAM surpasses state-of-the-art methods in accuracy, robustness, and computational efficiency, maintaining consistent performance under high angular velocities and producing high-quality panoramic reconstructions.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决事件相机在高速运动和长期运行中实现实时、准确的旋转里程计与建图的需求。提出的EROAM系统将事件投影到单位球面上以简化旋转运动表述，并引入了专为事件数据设计的几何优化框架——事件球面迭代最近点（ES-ICP），同时通过增量k-d树和区域密度控制实现高效的地图管理。在合成和真实数据集上的实验结果表明，EROAM在精度、鲁棒性和计算效率上均优于现有先进方法，能在高角速度等挑战性条件下保持稳定性能，并生成细节丰富的全景重建。</div>
</details>
</div>
<div class="card">
<div class="title">PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy</div>
<div class="meta-line">Authors: Jinhao Zhang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Wenlong Xia, Haoming Song, Youmin Gong, Jie Me</div>
<div class="meta-line">First: 2026-01-29T17:23:25+00:00 · Latest: 2026-01-29T17:23:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22018v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22018v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PocketDP3：高效的袖珍级三维视觉运动策略</div>
<div class="mono" style="margin-top:8px">近年来，基于三维视觉的扩散策略在学习复杂机器人操作技能方面展现出强大能力。然而，这些模型普遍存在架构不匹配问题：通常将微小而高效的点云编码器与庞大的解码器配对。考虑到紧凑的场景表示，我们认为这可能导致解码器参数的大量浪费。基于此观察，我们提出PocketDP3——一种袖珍级三维扩散策略，它用基于MLP-Mixer模块构建的轻量级扩散混合器替代了先前方法中使用的重型条件U-Net解码器。该架构能高效融合时间和通道维度，显著减小模型规模。值得注意的是，无需任何额外的连贯性蒸馏技术，我们的方法支持两步推理且不牺牲性能，提升了实时部署的实用性。在RoboTwin2.0、Adroit和MetaWorld三个仿真基准测试中，PocketDP3以少于先前方法1%的参数实现了最先进的性能，同时加速了推理。真实世界实验进一步验证了该方法在实际场景中的实用性和可迁移性。代码将开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the architectural inefficiency in existing 3D vision-based diffusion policies, where a compact point-cloud encoder is paired with a massive decoder, leading to substantial parameter waste. To address this, the method introduces PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks, enabling efficient fusion across temporal and channel dimensions and significantly reducing model size. Key experimental results show that across three simulation benchmarks—RoboTwin2.0, Adroit, and MetaWorld—PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, accelerates inference, and supports two-step inference without performance loss, with real-world experiments further confirming its practicality and transferability.</div>
<div class="mono" style="margin-top:8px">针对现有基于3D视觉的扩散策略中紧凑点云编码器与庞大解码器架构不匹配导致的参数效率低下问题，本研究提出了PocketDP3，一种轻量级3D视觉运动策略。该方法用基于MLP-Mixer模块构建的轻量级扩散混合器（DiM）取代了笨重的条件U-Net解码器，实现了跨时间和通道维度的高效融合，从而大幅减小模型规模。在RoboTwin2.0、Adroit和MetaWorld三个仿真基准上的实验结果表明，PocketDP3以少于先前方法1%的参数实现了最先进的性能，同时加速了推理过程，并支持无需一致性蒸馏的高效两步推理，真实世界实验进一步验证了其实用性和可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">Causal World Modeling for Robot Control</div>
<div class="meta-line">Authors: Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu</div>
<div class="meta-line">First: 2026-01-29T17:07:43+00:00 · Latest: 2026-01-29T17:07:43+00:00</div>
<div class="meta-line">Comments: Project page: https://technology.robbyant.com/lingbot-va Code: https://github.com/robbyant/lingbot-va</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21998v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21998v1">PDF</a> · <a href="https://github.com/robbyant/lingbot-va">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向机器人控制的因果世界建模</div>
<div class="mono" style="margin-top:8px">本研究指出，视频世界建模与视觉语言预训练相结合，为机器人学习建立了全新且独立的基础。直观而言，视频世界模型通过理解动作与视觉动态之间的因果关系，提供了预测近期未来的能力。受此启发，我们提出了LingBot-VA——一种同时学习帧预测与策略执行的自回归扩散框架。该模型具备三项精心设计：(1) 基于混合Transformer架构的共享潜空间，融合视觉与动作标记；(2) 闭环推演机制，支持通过真实观测持续获取环境反馈；(3) 异步推理流水线，并行执行动作预测与运动控制以实现高效操作。我们在仿真基准测试和实际场景中评估模型，结果显示其在长周期操作、训练后数据效率以及对新配置的强泛化能力方面均展现出显著潜力。代码与模型已开源以促进社区研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the potential of video world modeling and vision-language pre-training to serve as a new foundation for robot learning, as they enable the imagination of future states by understanding action-visual causality. The method introduces LingBot-VA, an autoregressive diffusion framework that jointly learns frame prediction and policy execution through a shared latent space with a Mixture-of-Transformers architecture, a closed-loop rollout mechanism for environmental feedback, and an asynchronous inference pipeline for efficient control. Experimental results on simulation benchmarks and real-world scenarios demonstrate the model&#x27;s significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalization to novel configurations.</div>
<div class="mono" style="margin-top:8px">本研究基于视频世界建模与视觉语言预训练为机器人学习提供新基础的潜力，因其通过理解动作与视觉动态的因果关系，能够预测未来状态。方法上提出了LingBot-VA，一种自回归扩散框架，通过混合Transformer架构的共享潜在空间、用于持续环境反馈的闭环推演机制，以及支持高效控制的异步推理管道，同时学习帧预测与策略执行。在仿真基准和真实场景中的实验结果表明，该模型在长时程操作、训练后数据效率以及对新配置的泛化能力方面均表现出显著优势。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized Information Gathering Under Dynamics Uncertainty</div>
<div class="meta-line">Authors: Fernando Palafox, Jingqi Li, Jesse Milzman, David Fridovich-Keil</div>
<div class="meta-line">First: 2026-01-29T17:00:35+00:00 · Latest: 2026-01-29T17:00:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21988v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey&#x27;s directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态不确定性下的广义信息采集</div>
<div class="mono" style="margin-top:8px">在未知动态系统中运行的智能体需通过观测学习系统动态。主动信息采集能加速这一学习过程，但现有方法需针对特定建模选择（动态模型、信念更新流程、观测模型与规划器）设计专用代价函数。本文提出一个统一框架，通过显式揭示参数、信念与控制间的因果依赖关系，使信息采集代价与这些建模选择解耦。基于该框架，我们推导出基于马西定向信息的通用信息采集代价函数，该函数仅假设具有加性噪声的马尔可夫动态，而对其他建模选择保持中立。我们证明现有文献中使用的互信息代价是本代价函数的特例。进而，利用该框架在线性化贝叶斯估计中建立互信息代价与信息增益的显式关联，从而为基于互信息的主动学习方法提供理论依据。最后，通过线性、非线性及多智能体系统的实验验证了本框架的实际效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of active information gathering in unknown dynamical systems, where existing methods are tailored to specific modeling assumptions, this work introduces a unifying framework that decouples the information-gathering cost from choices of dynamics models, belief updates, observation models, and planners. The method explicitly models causal dependencies between parameters, beliefs, and controls, deriving a general cost based on Massey&#x27;s directed information that requires only Markov dynamics with additive noise. Key experimental findings demonstrate the framework&#x27;s practical utility across linear, nonlinear, and multi-agent systems, while theoretical analysis shows that the mutual information cost used in prior work is a special case and establishes its connection to information gain in linearized Bayesian estimation.</div>
<div class="mono" style="margin-top:8px">针对未知动态系统中主动信息收集的挑战——现有方法通常针对特定建模假设定制，本研究提出了一个统一框架，将信息收集成本与动力学模型、信念更新、观测模型和规划器的选择解耦。该方法通过显式建模参数、信念和控制之间的因果依赖关系，基于马西的有向信息推导出一个通用成本函数，适用于具有加性噪声的马尔可夫动力学，且对其他建模细节保持不可知。在线性、非线性和多智能体系统上的实验证明了该框架的实用性，同时理论分析表明，广泛使用的互信息成本是所提成本的一个特例，并建立了其在线性化贝叶斯估计中与信息增益的明确联系。</div>
</details>
</div>
<div class="card">
<div class="title">Macro-Scale Electrostatic Origami Motor</div>
<div class="meta-line">Authors: Alex S. Miller, Leo McElroy, Jeffrey H. Lang</div>
<div class="meta-line">First: 2026-01-29T16:53:24+00:00 · Latest: 2026-01-29T16:53:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21976v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21976v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>宏观尺度静电折纸电机</div>
<div class="mono" style="margin-top:8px">可折叠机器人因其高体积质量比、易打包性和形状适应性，一直是机器人研究的热点领域。在运动方式上，先前开发的可折叠机器人要么在结构中嵌入线性驱动器，要么附加非折叠旋转电机。此外，那些直接嵌入折叠介质结构的驱动器均用于线性或折叠运动，而非连续旋转运动。在宏观尺度上，尚未出现可折叠的连续旋转驱动器。本文详细介绍了首款可折叠至平面状态、展开后即可运行的宏观尺度折纸旋转电机的开发与测试。该原型电机利用电晕放电产生扭矩，实现了2.5:1的展开比，在-29 kV驱动下最高转速达1440 rpm，最大输出扭矩超过0.15 mN·m，有效部件扭矩密度为0.04 Nm/kg。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of a macro-scale, foldable continuous rotary actuator for origami robots, which typically rely on linear actuators or non-folding motors. The method involves creating an electrostatic origami motor that uses corona discharge to generate torque, enabling the entire structure to be folded flat and then deployed for operation. Experimental results show the prototype achieved a 2.5:1 expansion ratio, a top speed of 1440 rpm at -29 kV, and a maximum output torque exceeding 0.15 mN·m with an active component torque density of 0.04 Nm/kg.</div>
<div class="mono" style="margin-top:8px">本研究针对可折叠机器人缺乏可平折的宏观连续旋转驱动器的问题，这类机器人通常使用线性驱动器或附加的非折叠电机。方法是通过开发一种利用电晕放电产生扭矩的折纸旋转电机，使其能够折叠后展开运行。实验结果表明，原型机实现了2.5:1的扩展比，在-29 kV驱动下最高转速达1440 rpm，最大输出扭矩超过0.15 mN·m，主动部件扭矩密度为0.04 Nm/kg。</div>
</details>
</div>
<div class="card">
<div class="title">MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts</div>
<div class="meta-line">Authors: Lorenzo Mazza, Ariel Rodriguez, Rayan Younis, Martin Lelis, Ortrun Hellig, Chenpan Li, Sebastian Bodenstedt, Martin Wagner, Stefanie Speidel</div>
<div class="meta-line">First: 2026-01-29T16:50:14+00:00 · Latest: 2026-01-29T16:50:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21971v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21971v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoE-ACT：通过监督混合专家模型提升手术模仿学习策略</div>
<div class="mono" style="margin-top:8px">模仿学习在机器人操作领域已取得显著成功，但其在手术机器人中的应用仍面临数据稀缺、工作空间受限以及对极高安全性和可预测性要求的挑战。本文提出一种专为分阶段结构化手术操作任务设计的监督混合专家架构，可集成于任何自主策略之上。与先前依赖多摄像头设置或数千次演示的手术机器人学习方法不同，我们证明：当配备本架构时，轻量级动作解码器策略（如动作分块变换器）仅需不到150次演示，即可从立体内窥镜图像中学习复杂的长时程操作。我们在肠道抓持与牵拉协作手术任务中评估本方法，该任务要求机器人助手解读外科医生的视觉线索、对可变形组织执行精准抓持并进行持续牵拉。我们将本方法与前沿的视觉-语言-动作模型及标准ACT基线进行对比。结果表明：通用型VLA模型即使在标准分布内条件下也完全无法掌握该任务；而标准ACT在分布内仅取得中等成功率，采用监督MoE架构则显著提升其性能——在分布内获得更高成功率，并在分布外场景（包括新抓持位置、照明减弱及部分遮挡）中展现出更优鲁棒性。值得注意的是，该方法能泛化至未见过的测试视角，并无需额外训练即可零样本迁移至离体猪组织，为体内部署提供了可行路径。为此，我们展示了在活体猪手术中策略执行的定性初步结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges of applying imitation learning to surgical robotics, such as data scarcity and stringent safety requirements, by proposing a supervised Mixture-of-Experts (MoE) architecture for phase-structured tasks. The method enhances the Action Chunking Transformer (ACT) policy, enabling it to learn complex, long-horizon manipulation from fewer than 150 demonstrations using only stereo endoscopic images. Experimental evaluation on bowel grasping and retraction shows that while generalist Vision-Language-Action models fail and standard ACT achieves moderate in-distribution success, the MoE-ACT significantly improves performance, demonstrating higher success rates, robustness to out-of-distribution conditions like novel grasps and occlusions, and zero-shot transfer to ex vivo porcine tissue, with preliminary in vivo results.</div>
<div class="mono" style="margin-top:8px">本研究针对模仿学习在手术机器人应用中面临的数据稀缺、工作空间受限和安全性要求高等挑战。方法上，提出了一种用于阶段化手术操作任务的监督混合专家架构，可增强仅使用立体内窥镜图像、基于不足150次演示训练的轻量级动作分块Transformer策略。在肠道抓取与牵拉任务上的实验表明，通用视觉-语言-动作模型完全失败，标准动作分块Transformer策略在分布内取得中等成功率，而采用混合专家架构后，分布内成功率显著提升，并对分布外场景（如新的抓取位置和遮挡）表现出更强的鲁棒性，同时能零样本迁移至离体猪组织并进行了初步的体内验证。</div>
</details>
</div>
<div class="card">
<div class="title">Information Filtering via Variational Regularization for Robot Manipulation</div>
<div class="meta-line">Authors: Jinhao Zhang, Wenlong Xia, Yaojia Wang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Haoming Song, Youmin Gong, Jie Me</div>
<div class="meta-line">First: 2026-01-29T16:17:42+00:00 · Latest: 2026-01-29T16:17:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21926v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21926v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于变分正则化的信息过滤在机器人操作中的应用</div>
<div class="mono" style="margin-top:8px">基于三维视觉表征的扩散式视觉运动策略在学习复杂机器人技能方面已取得显著成效。然而，现有方法大多采用过大的去噪解码器。虽然增加模型容量能提升去噪效果，但实证研究表明这也会在中间特征块中引入冗余和噪声。关键发现是：在推理阶段随机掩码主干特征（无需改变训练过程）可提升性能，这证实了中间特征中存在任务无关噪声。为此，我们提出变分正则化——一种轻量级模块，通过对主干特征施加时间步条件高斯分布并应用KL散度正则化器，构建自适应信息瓶颈。在三个仿真基准（RoboTwin2.0、Adroit和MetaWorld）上的大量实验表明：相较于基线DP3，本方法在RoboTwin2.0上成功率提升6.1%，在Adroit和MetaWorld上提升4.1%，达到新的最优性能。真实场景实验进一步验证了该方法在实际部署中的有效性。代码即将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the problem of redundancy and noise in intermediate features of diffusion-based visuomotor policies for robot manipulation, which arise from using oversized denoising decoders. The proposed solution is Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian distribution over backbone features and applies a KL-divergence regularizer to create an adaptive information bottleneck, thereby filtering out task-irrelevant information. Experimental results on simulation benchmarks RoboTwin2.0, Adroit, and MetaWorld show that VR improves success rates by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld compared to the DP3 baseline, achieving state-of-the-art performance, with real-world experiments confirming practical effectiveness.</div>
<div class="mono" style="margin-top:8px">基于扩散的视觉运动策略利用3D视觉表示在机器人操作中表现良好，但现有方法通常采用过大的去噪解码器，这会在中间特征块中引入冗余和噪声，从而影响性能。为此，本研究提出了变分正则化（VR），这是一个轻量级模块，通过对骨干网络特征施加时间步条件的高斯分布并应用KL散度正则化器，形成一个自适应信息瓶颈，以过滤掉与任务无关的噪声。在三个仿真基准（RoboTwin2.0、Adroit和MetaWorld）上的大量实验表明，与基线DP3相比，该方法在RoboTwin2.0上的成功率提高了6.1%，在Adroit和MetaWorld上提高了4.1%，达到了新的最先进水平，真实世界实验进一步验证了其在实际部署中的良好表现。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation</div>
<div class="meta-line">Authors: Pratik Ingle, Jørn Lambertsen, Kasper Støy, Andres Faina</div>
<div class="meta-line">First: 2026-01-29T15:46:50+00:00 · Latest: 2026-01-29T15:46:50+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21884v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模块MANTA-RAY：用于分布式多物体操控的模块化软表面平台</div>
<div class="mono" style="margin-top:8px">操控表面通过主动变形而非直接抓取来控制物体。密集致动器阵列虽能产生复杂形变，但也引入了高自由度（DOF），增加了系统复杂性并限制了可扩展性。MANTA-RAY（低致动密度自适应非刚性织物操控）平台通过采用低致动密度的柔性织物表面来操控脆弱及异质物体，以应对这些挑战。先前研究集中于由四个致动器支撑的单模块实现，而可扩展的多模块配置的可行性与优势尚未探索。本研究提出一种分布式、模块化、可扩展的MANTA-RAY平台变体，在降低致动密度的同时保持操控性能。该多模块MANTA-RAY平台及控制策略采用模块间物体传递和几何变换驱动的PID控制器，直接将倾斜角控制输出映射至致动器指令，无需大量数据驱动或黑箱训练。我们通过仿真评估了不同模块数量（3x3和4x4）的表面配置性能，并通过2x2物理硬件原型实验验证了可行性。系统成功操控了包括鸡蛋、苹果等脆弱物品在内的多种几何形状、质量及纹理的物体，并实现了并行操控。结果表明，多模块MANTA-RAY提升了可扩展性，支持更大范围内的多物体协同操控，凸显了其在实际应用中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high complexity and limited scalability of dense actuator arrays in manipulation surfaces, this work introduces a modular and scalable variant of the MANTA-RAY platform for distributed multi-object manipulation. The method employs a multi-module soft fabric surface with reduced actuator density, using object passing between modules and a geometric transformation driven PID controller that directly maps tilt commands to actuator inputs, avoiding data-intensive training. Experimental results from simulation and a physical 2x2 prototype show successful manipulation of diverse, fragile objects like eggs and apples, as well as parallel manipulation, demonstrating improved scalability and coordinated control over larger areas.</div>
<div class="mono" style="margin-top:8px">本研究针对密集致动器阵列在操控表面中可扩展性不足的问题，提出了一个模块化软表面平台——多模块MANTA-RAY。该方法采用分布式模块化设计，其控制策略支持模块间物体传递，并利用基于几何变换的PID控制器将倾斜角控制输出直接映射为致动器指令，从而避免了数据驱动的训练。在仿真和2x2物理原型上的实验结果表明，该系统能成功并行操控包括鸡蛋和苹果在内的多种脆弱物体，证明了其在更大范围内实现协调多物体操控的能力，提升了系统的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Driven Scenario-Aware Planning for Autonomous Driving</div>
<div class="meta-line">Authors: He Li, Zhaowei Chen, Rui Gao, Guoliang Li, Qi Hao, Shuai Wang, Chengzhong Xu</div>
<div class="meta-line">First: 2026-01-29T15:42:13+00:00 · Latest: 2026-01-29T15:42:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21876v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21876v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的场景感知自动驾驶规划方法</div>
<div class="mono" style="margin-top:8px">自动驾驶混合规划器切换框架需兼顾高速行驶效率与密集车流中的安全操控。现有方法因启发式场景识别与低频控制更新，常在拥堵环境中难以实现可靠模式切换或保持高效行驶。为此，本文提出LAP——一种基于大语言模型的自适应规划方法，通过在低复杂度场景采用高速驾驶模式、高复杂度场景采用精确驾驶模式的动态切换，实现穿越狭窄通道的高质量轨迹生成。该方法利用LLM进行场景理解，并将其推理融入模式配置与运动规划的联合优化中，通过树搜索模型预测控制与交替最小化求解。我们在机器人操作系统（ROS）中以Python实现LAP，高保真仿真结果表明，该方法在行驶时间与成功率方面均优于现有基准模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing hybrid planner switching frameworks (HPSF) in autonomous driving, which often fail to make reliable mode transitions or sustain efficient driving in dense traffic due to heuristic scene recognition and low-frequency control updates, this paper proposes LAP, an LLM-driven adaptive planning method. The method leverages a large language model for scene understanding and integrates its inference into a joint optimization of mode configuration and motion planning, solved using tree-search model predictive control and alternating minimization, to switch between high-speed and precise driving modes. High-fidelity simulations demonstrate that LAP outperforms benchmark methods in both driving time and success rate.</div>
<div class="mono" style="margin-top:8px">针对现有自动驾驶混合规划器切换框架在密集交通中因启发式场景识别和低频控制更新而难以实现可靠模式转换和高效驾驶的问题，本文提出了LAP，一种基于大语言模型的自适应规划方法。该方法利用大语言模型进行场景理解，并将其推理结果集成到模式配置与运动规划的联合优化中，通过树搜索模型预测控制和交替最小化求解，从而在低复杂度场景下实现高速驾驶，在高复杂度场景下进行精确驾驶。高保真仿真实验结果表明，LAP在驾驶时间和成功率方面均优于其他基准方法。</div>
</details>
</div>
<div class="card">
<div class="title">GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration</div>
<div class="meta-line">Authors: Bsher Karbouj, Baha Eddin Gaaloul, Jorg Kruger</div>
<div class="meta-line">First: 2026-01-29T15:12:58+00:00 · Latest: 2026-01-29T15:12:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAZELOAD：面向工业人机协作中脑力负荷评估的多模态眼动追踪数据集</div>
<div class="mono" style="margin-top:8px">本文介绍了GAZELOAD——一个用于工业人机协作场景中脑力负荷评估的多模态数据集。数据采集于实验室装配测试平台，26名参与者佩戴Meta ARIA智能眼镜与两台协作机器人（UR5和Franka Emika Panda）进行交互。该数据集在任务难度与环境条件的受控调控下，实现了眼动信号（瞳孔直径、注视点、扫视、视线方向、注视转移熵、注视分散指数）与环境实时连续测量数据（照度）及任务与机器人上下文信息（工作台、任务模块、诱发故障）的时间同步。针对每位参与者及按负荷分级的任务模块，我们提供了以250毫秒窗口聚合的眼动指标CSV文件、环境日志、基于1-10李克特量表的自报告脑力负荷评分，并按参与者独立文件夹形式与文档一并归档。这些数据可用于开发与验证真实工业人机协作场景中的脑力负荷估计算法、特征提取与时序建模方法，并探究照明等环境因素对眼动负荷指标的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To support the development of mental workload estimation algorithms in realistic industrial human-robot collaboration (HRC) settings, this work introduces the GAZELOAD dataset. The method involved collecting synchronized multimodal data from 26 participants performing assembly tasks with two collaborative robots while wearing smart glasses, under controlled variations in task difficulty and ambient lighting. Key experimental results include a comprehensive dataset featuring ocular metrics aggregated in 250 ms windows, environmental logs, and self-reported workload ratings, enabling research into feature extraction and the impact of environmental factors like illuminance on eye-based workload markers.</div>
<div class="mono" style="margin-top:8px">为支持在现实的工业人机协作场景中开发心理负荷估计算法，本研究引入了GAZELOAD数据集。其方法是在26名参与者使用两台协作机器人执行装配任务时，通过Meta ARIA智能眼镜收集同步的多模态数据；该数据整合了眼动指标（如瞳孔直径、注视点）、环境照度以及任务上下文，并在受控的任务难度和光照条件下进行采集。主要的实验结果提供了一个全面的、时间同步的数据集，包含以250毫秒窗口聚合的眼动指标、环境日志和自报告负荷评分，可用于负荷估计算法的基准测试，并分析照明等因素对基于眼动的负荷指标的影响。</div>
</details>
</div>
<div class="card">
<div class="title">SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction</div>
<div class="meta-line">Authors: Linyong Gan, Zimo Li, Wenxin Xu, Xingjian Li, Jianhua Z. Huang, Enmei Tu, Shuhang Chen</div>
<div class="meta-line">First: 2026-01-26T14:42:31+00:00 · Latest: 2026-01-29T15:03:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18537v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18537v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SKETCH：基于语义关键点条件建模的长时程船舶轨迹预测</div>
<div class="mono" style="margin-top:8px">由于复杂航行行为与环境因素叠加的不确定性，准确的长时程船舶轨迹预测仍具挑战性。现有方法常难以保持全局航向一致性，导致长时外推时出现轨迹漂移或失准。为此，我们提出一种语义关键点条件轨迹建模框架，通过以捕捉航行意图的高层‘下一关键点’为条件预测未来轨迹。该框架将长时程预测分解为全局语义决策与局部运动建模，有效将未来轨迹的可行域约束在语义合理的子集中。为从历史观测中高效估计下一关键点先验，我们采用预训练-微调策略。基于真实AIS数据的大量实验表明，所提方法在长航时、航向精度及细粒度轨迹预测方面均持续优于现有先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve long-horizon vessel trajectory prediction, which is difficult due to complex navigation behaviors and environmental uncertainties that cause existing methods to produce globally inconsistent or implausible paths. The proposed method, SKETCH, introduces a semantic key-point conditioning framework that predicts future trajectories by conditioning on a high-level Next Key Point (NKP) representing navigational intent, thereby decomposing the problem into global semantic decision-making and local motion modeling to restrict predictions to semantically feasible subsets. Experimental results on real-world AIS data show that this approach consistently outperforms state-of-the-art methods, especially in long-duration prediction, directional accuracy, and fine-grained trajectory details.</div>
<div class="mono" style="margin-top:8px">由于复杂的航行行为和环境不确定性，准确的长时域船舶轨迹预测具有挑战性，现有方法在长时间外推时常常出现方向不一致和轨迹不合理的问题。为解决此问题，研究者提出了一种语义关键点条件化框架，通过预测一个表征航行意图的高层下一关键点来分解问题，然后基于该关键点生成详细轨迹，从而将预测限制在语义可行的子集内；采用预训练-微调策略来高效估计下一关键点先验。在真实AIS数据上的大量实验表明，该方法在长时预测、方向准确性和细粒度轨迹细节方面持续优于现有先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Flocking behavior for dynamic and complex swarm structures</div>
<div class="meta-line">Authors: Carmen D. R. Pita-Romero, Pedro Arias-Perez, Miguel Fernandez-Cortizas, Rafael Perez-Segui, Pascual Campoy</div>
<div class="meta-line">Venue: 2025 International Conference on Unmanned Aircraft Systems (ICUAS), pages 1011-1018</div>
<div class="meta-line">First: 2026-01-29T14:22:53+00:00 · Latest: 2026-01-29T14:22:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21772v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21772v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态复杂集群结构中的群体聚集行为</div>
<div class="mono" style="margin-top:8px">维持多无人机复杂结构编队并实现复杂轨迹仍是一大挑战。本研究提出一种基于虚拟质心概念的无人机群体聚集行为算法，以简化集群结构的构建。该方法在经典虚拟行为基础上，提供了动态控制智能体数量与结构编队的理论框架，并通过仿真与真实实验验证了其在复杂编队与轨迹下的简易性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of maintaining formation and executing complex trajectories with multiple UAVs in dynamic swarm structures. The method introduces a flocking algorithm based on a Virtual Centroid concept, which extends classical virtual-structure approaches to provide a theoretical framework for dynamically controlling both the number of agents and the formation shape. Simulation and real-world experiments demonstrated the algorithm&#x27;s simplicity and effectiveness in managing complex formations and trajectories.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多无人机在动态复杂集群结构中保持编队和执行复杂轨迹的挑战。该方法提出了一种基于虚拟质心概念的集群算法，扩展了经典的虚拟结构方法，为动态控制智能体数量和编队形状提供了一个理论框架。仿真和真实世界实验表明，该算法在管理复杂编队和轨迹方面具有简单性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">OMP: One-step Meanflow Policy with Directional Alignment</div>
<div class="meta-line">Authors: Han Fang, Yize Huang, Yuheng Zhao, Paul Weng, Xiao Li, Yutong Ban</div>
<div class="meta-line">First: 2025-12-22T12:45:35+00:00 · Latest: 2026-01-29T14:02:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19347v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19347v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot manipulation has increasingly adopted data-driven generative policy frameworks, yet the field faces a persistent trade-off: diffusion models suffer from high inference latency, while flow-based methods often require complex architectural constraints. Although in image generation domain, the MeanFlow paradigm offers a path to single-step inference, its direct application to robotics is impeded by critical theoretical pathologies, specifically spectral bias and gradient starvation in low-velocity regimes. To overcome these limitations, we propose the One-step MeanFlow Policy (OMP), a novel framework designed for high-fidelity, real-time manipulation. We introduce a lightweight directional alignment mechanism to explicitly synchronize predicted velocities with true mean velocities. Furthermore, we implement a Differential Derivation Equation (DDE) to approximate the Jacobian-Vector Product (JVP) operator, which decouples forward and backward passes to significantly reduce memory complexity. Extensive experiments on the Adroit and Meta-World benchmarks demonstrate that OMP outperforms state-of-the-art methods in success rate and trajectory accuracy, particularly in high-precision tasks, while retaining the efficiency of single-step generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OMP：具有方向对齐的单步均值流策略</div>
<div class="mono" style="margin-top:8px">机器人操作领域日益采用数据驱动的生成式策略框架，但该领域始终面临一个权衡难题：扩散模型存在高推理延迟，而基于流的方法通常需要复杂的架构约束。虽然在图像生成领域，均值流范式提供了单步推理的路径，但其直接应用于机器人技术受到关键理论缺陷的阻碍，特别是低频速度区域中的谱偏差和梯度匮乏问题。为克服这些限制，我们提出单步均值流策略（OMP）——一个专为高保真实时操作设计的新型框架。我们引入轻量级方向对齐机制，显式同步预测速度与真实平均速度。此外，我们实现微分推导方程（DDE）来近似雅可比-向量积（JVP）算子，通过解耦前向与反向传播显著降低内存复杂度。在Adroit和Meta-World基准上的大量实验表明，OMP在成功率和轨迹精度上均优于现有最优方法，尤其在高精度任务中表现突出，同时保持了单步生成的高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the trade-off in robot manipulation between the high inference latency of diffusion models and the architectural complexity of flow-based methods, this paper proposes the One-step Meanflow Policy (OMP) for real-time, high-fidelity control. The method introduces a directional alignment mechanism to synchronize predicted and true mean velocities and employs a Differential Derivation Equation (DDE) to approximate the Jacobian-Vector Product, thereby decoupling forward and backward passes to reduce memory overhead. Experiments on the Adroit and Meta-World benchmarks show that OMP achieves higher success rates and trajectory accuracy than state-of-the-art methods, especially in high-precision tasks, while maintaining efficient single-step inference.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人操作中扩散模型推理延迟高与流式方法架构复杂之间的权衡问题，指出现有MeanFlow方法在低速状态下存在谱偏差和梯度匮乏的缺陷。提出的单步MeanFlow策略（OMP）引入了方向对齐机制来同步预测速度与真实平均速度，并采用微分推导方程近似雅可比向量积以降低内存复杂度。在Adroit和Meta-World基准测试上的实验表明，OMP在成功率和轨迹准确性上优于现有先进方法，尤其在高精度任务中表现突出，同时保持了单步生成的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations</div>
<div class="meta-line">Authors: Donatien Delehelle, Fei Chen, Darwin Caldwell</div>
<div class="meta-line">First: 2026-01-29T13:41:35+00:00 · Latest: 2026-01-29T13:41:35+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦感知与推理以提升无演示学习的布料操作数据效率</div>
<div class="mono" style="margin-top:8px">布料操作是日常生活中的常见任务，但对机器人而言仍是一个开放挑战。开发布料操作策略的困难源于布料的高维状态空间、复杂动力学特性及易自遮挡的特点。由于分析方法未能提供鲁棒且通用的操作策略，强化学习被视为解决这些问题的有前景途径。然而，为应对庞大的状态空间和复杂动力学，基于数据的方法通常依赖大型模型和长训练时间，其计算成本严重阻碍了这些方法的发展与应用。此外，由于鲁棒状态估计的挑战，衣物操作策略常采用以工作空间图像为输入的端到端学习方法。虽然该方法通过现实世界微调实现了概念上直接的仿真到现实迁移，但训练智能体使用高度有损的环境状态表示也带来了显著计算成本。本文通过探索一种高效模块化的布料操作强化学习方法，对这一常见设计选择提出质疑。我们证明，通过精心设计，在仿真中学习时可显著减小模型规模与训练时间。进一步，我们展示了如何将仿真训练的模型迁移至现实世界。我们在SoftGym基准上评估本方法，在任务中相比现有基线取得显著性能提升，同时所用模型规模大幅减小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the data inefficiency and high computational costs of reinforcement learning (RL) for cloth manipulation, which is challenging due to the fabric&#x27;s high-dimensional state space and complex dynamics. The proposed method disentangles perception from reasoning, moving away from end-to-end learning on raw images to a more modular RL approach that uses a carefully designed, efficient state representation learned in simulation. Experimental evaluation on the SoftGym benchmark shows that this approach achieves significant performance improvements over baselines while using a substantially smaller model and enabling successful sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">本研究针对布料操作中强化学习方法数据效率低、计算成本高的问题，该任务因布料的高维状态空间和复杂动力学而充满挑战。方法质疑了常见的基于图像输入的端到端学习范式，提出了一种将感知与推理解耦的模块化设计，以降低模型复杂度。在SoftGym基准上的实验评估表明，该方法在使用更小模型的情况下显著超越了现有基线性能，并实现了有效的仿真到现实迁移。</div>
</details>
</div>
<div class="card">
<div class="title">CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation</div>
<div class="meta-line">Authors: Xuanran Zhai, Binkai Ou, Yemin Wang, Hui Yi Leong, Qiaojun Yu, Ce Hao, Yaohua Liu</div>
<div class="meta-line">First: 2026-01-29T13:40:46+00:00 · Latest: 2026-01-29T13:40:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21712v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoFreeVLA：基于视觉-语言-动作模型与风险估计的无碰撞双臂协同操作</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型支持指令驱动的机械臂操作，但双臂协同部署仍因手臂与抓持物体间的自碰撞建模不足而存在安全隐患。本文提出CoFreeVLA，通过短时域自碰撞风险估计器增强端到端VLA模型，该估计器基于本体感知、视觉嵌入与规划动作预测碰撞概率。系统可拦截高风险指令，通过风险引导调整恢复至安全状态，并优化策略实现更安全的操作轨迹。该模型采用基于碰撞标签的预训练与真实机器人轨迹的校准后训练。在PiPER机械臂的五项双手任务中，相较于RDT与APEX方法，CoFreeVLA显著降低自碰撞率并提升任务成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety challenge in dual-arm manipulation with Vision-Language-Action (VLA) models, where self-collisions between arms and grasped objects are often under-modeled. The proposed method, CoFreeVLA, augments an end-to-end VLA model with a short-horizon self-collision risk estimator that uses proprioception, visual embeddings, and planned actions to predict collision likelihood; this estimator gates risky commands, enables risk-guided recovery to safe states, and shapes policy refinement. Experimental results on five bimanual tasks with the PiPER robot arm demonstrate that CoFreeVLA reduces self-collisions and improves success rates compared to baseline methods RDT and APEX.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉-语言-动作模型在双臂操作中因对臂间及抓持物体间自碰撞建模不足而存在的安全隐患。提出的CoFreeVLA方法在端到端VLA模型基础上，增加了一个基于本体感知、视觉嵌入和规划动作的短时域自碰撞风险估计器，用于预测碰撞可能性；该估计器可拦截风险指令、引导系统恢复至安全状态，并优化策略以提升安全性。在PiPER机器人上进行的五项双手任务实验表明，与RDT和APEX等基线方法相比，CoFreeVLA有效减少了自碰撞并提高了任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning</div>
<div class="meta-line">Authors: Dongcheng Cao, Jin Zhou, Xian Wang, Shuo Li</div>
<div class="meta-line">First: 2025-08-13T13:27:32+00:00 · Latest: 2026-01-29T13:09:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09797v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.09797v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agile flight for the quadrotor cable-suspended payload system is a formidable challenge due to its underactuated, highly nonlinear, and hybrid dynamics. Traditional optimization-based methods often struggle with high computational costs and the complexities of cable mode transitions, limiting their real-time applicability and maneuverability exploitation. In this letter, we present FLARE, a reinforcement learning (RL) framework that directly learns agile navigation policy from high-fidelity simulation. Our method is validated across three designed challenging scenarios, notably outperforming a state-of-the-art optimization-based approach by a 3x speedup during gate traversal maneuvers. Furthermore, the learned policies achieve successful zero-shot sim-to-real transfer, demonstrating remarkable agility and safety in real-world experiments, running in real time on an onboard computer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLARE：基于强化学习的四旋翼缆索悬挂载荷系统敏捷飞行控制</div>
<div class="mono" style="margin-top:8px">四旋翼缆索悬挂载荷系统的敏捷飞行控制因其欠驱动、高度非线性和混合动力学特性而极具挑战。传统基于优化的方法常受限于高计算成本与缆索模式转换的复杂性，难以实现实时应用与机动性开发。本文提出FLARE——一种通过高保真仿真直接学习敏捷导航策略的强化学习框架。该方法在三种设计的挑战性场景中得到验证，在穿越门框机动中较当前最先进的优化方法提速3倍。此外，学习到的策略成功实现了零样本仿真到现实的迁移，在搭载机载计算机的实时运行中，展现出卓越的敏捷性与安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of achieving agile flight for quadrotor cable-suspended payload systems, which have complex underactuated and hybrid dynamics that hinder traditional optimization-based methods due to high computational cost and difficulty handling cable mode transitions. The proposed method, FLARE, employs a reinforcement learning framework to learn agile navigation policies directly from high-fidelity simulation. Experimental results show that the learned policies outperform a state-of-the-art optimization-based method with a 3x speedup in gate traversal and successfully transfer zero-shot from simulation to real-world flights, demonstrating real-time agility and safety on an onboard computer.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决四旋翼吊挂负载系统实现敏捷飞行的挑战，该系统具有欠驱动、高度非线性的混合动力学特性，传统基于优化的方法因计算成本高且难以处理缆绳模式切换而受限。所提出的FLARE方法采用强化学习框架，直接从高保真仿真中学习敏捷导航策略。实验结果表明，学习到的策略在穿越门框任务中比先进的优化方法快3倍，并成功实现了从仿真到真实环境的零样本迁移，在机载计算机上实时运行，展现了卓越的敏捷性和安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Localization via Semantic Structures in Autonomous Photovoltaic Power Plant Inspection</div>
<div class="meta-line">Authors: Viktor Kozák, Karel Košnar, Jan Chudoba, Miroslav Kulich, Libor Přeučil</div>
<div class="meta-line">First: 2025-01-24T15:48:41+00:00 · Latest: 2026-01-29T13:02:39+00:00</div>
<div class="meta-line">Comments: 50 pages, 23 figures. Submitted for review to Array</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.14587v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.14587v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspection systems utilizing unmanned aerial vehicles (UAVs) equipped with thermal cameras are increasingly popular for the maintenance of photovoltaic (PV) power plants. However, automation of the inspection task is a challenging problem as it requires precise navigation to capture images from optimal distances and viewing angles. This paper presents a novel localization pipeline that directly integrates PV module detection with UAV navigation, allowing precise positioning during inspection. The detections are used to identify the power plant structures in the image. These are associated with the power plant model and used to infer the UAV position relative to the inspected PV installation. We define visually recognizable anchor points for the initial association and use object tracking to discern global associations. Additionally, we present three different methods for visual segmentation of PV modules and evaluate their performance in relation to the proposed localization pipeline. The presented methods were verified and evaluated using custom aerial inspection data sets, demonstrating their robustness and applicability for real-time navigation. Additionally, we evaluate the influence of the power plant model precision on the localization methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语义结构视觉定位的光伏电站自主巡检方法</div>
<div class="mono" style="margin-top:8px">搭载热成像相机的无人机巡检系统在光伏电站维护中日益普及，但巡检自动化因需精准导航以获取最佳距离与视角图像而面临挑战。本文提出一种将光伏组件检测与无人机导航直接融合的新型定位流程，实现巡检过程中的精确定位。通过检测识别图像中的电站结构，关联电站模型并推算无人机相对于被检光伏装置的位置。定义视觉可识别的锚点进行初始关联，并采用目标跟踪实现全局关联。此外，提出三种光伏组件视觉分割方法，评估其与定位流程的适配性能。基于定制航检数据集的验证表明，该方法具有鲁棒性并适用于实时导航。同时评估了电站模型精度对定位方法的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable precise UAV navigation for automated photovoltaic power plant inspection, this study introduces a localization pipeline that integrates PV module detection with UAV positioning. The method uses visual detection to identify power plant structures, associates them with a plant model via anchor points and object tracking, and infers the UAV&#x27;s relative position. Experimental evaluation on custom aerial datasets shows the robustness of the approach for real-time navigation and assesses the impact of model precision on localization accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对无人机光伏电站自动化巡检中需要精确定位以获取最佳拍摄视角的挑战，提出了一种将光伏组件检测与无人机导航直接集成的定位流程。该方法通过将图像中检测到的电站结构与电站模型关联，推断无人机相对于被检光伏装置的位置，利用视觉可识别的锚点进行初始关联，并采用目标跟踪实现全局关联。在自定义航拍数据集上的验证表明，该方法具有鲁棒性和实时应用性，同时评估了电站模型精度对定位方法的影响。</div>
</details>
</div>
<div class="card">
<div class="title">From Instruction to Event: Sound-Triggered Mobile Manipulation</div>
<div class="meta-line">Authors: Hao Ju, Shaofei Huang, Hongyu Li, Zihan Ding, Si Liu, Meng Wang, Zhedong Zheng</div>
<div class="meta-line">First: 2026-01-29T13:02:10+00:00 · Latest: 2026-01-29T13:02:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21667v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21667v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从指令到事件：声音触发的移动操控</div>
<div class="mono" style="margin-top:8px">当前移动操控研究主要遵循指令驱动范式，智能体依赖预定义的文本命令执行任务。然而，这种设定将智能体限制在被动角色，制约了其自主性和对环境动态事件的响应能力。为突破这些局限，我们提出声音触发的移动操控，使智能体无需显式动作指令即可主动感知并操作发声物体。为支持此类任务，我们开发了集成声学渲染与物理交互的数据平台Habitat-Echo。进一步提出由高层任务规划器与底层策略模型构成的基线系统以完成任务。大量实验表明，所提基线使智能体能主动检测并响应听觉事件，无需逐例指令。值得注意的是，在具有挑战性的双声源场景中，智能体成功从重叠的声学干扰中分离主声源执行首次交互，继而操作次要物体，验证了基线的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current mobile manipulation agents are largely passive, relying on explicit textual instructions, which limits their autonomy in dynamic environments. To address this, the authors introduce sound-triggered mobile manipulation, where agents must perceive and interact with sound-emitting objects without step-by-step commands. They develop the Habitat-Echo platform for acoustic-physical simulation and propose a baseline model with a high-level planner and low-level policies. Experiments demonstrate that the agent can actively detect and respond to auditory events, successfully isolating and manipulating a primary sound source even amidst overlapping acoustic interference, confirming the method&#x27;s robustness.</div>
<div class="mono" style="margin-top:8px">本研究针对指令驱动移动操作中智能体被动执行预设命令的局限性，提出了声音触发任务，要求智能体主动感知环境中的听觉事件。方法上开发了集成声学渲染与物理交互的Habitat-Echo数据平台，并构建了包含高层任务规划器与底层策略模型的基线系统来完成这些任务。实验结果表明，该智能体能够在没有逐步指令的情况下自主检测并响应声音，成功在重叠声学干扰中隔离并操作主要声源，随后处理次要对象，验证了系统的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation</div>
<div class="meta-line">Authors: Jianli Sun, Bin Tian, Qiyao Zhang, Chengxiang Li, Zihan Song, Zhiyong Cui, Yisheng Lv, Yonglin Tian</div>
<div class="meta-line">First: 2026-01-29T12:09:00+00:00 · Latest: 2026-01-29T12:09:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object &amp; spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AIR-VLA：面向空中操作的视觉-语言-动作系统</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作模型在地面具身智能领域已取得显著成功，但其在空中操作系统中的应用仍属未充分探索的前沿。空中操作系统固有的浮动基座动力学、无人机与机械臂的强耦合性，以及操作任务的多步骤长时域特性，对现有针对静态或二维移动基座设计的VLA范式构成严峻挑战。为填补这一空白，我们提出首个专为空中操作定制的VLA基准AIR-VLA。我们构建了基于物理的仿真环境，并发布了包含3000条人工遥操作演示的高质量多模态数据集，涵盖基座操控、物体与空间理解、语义推理及长时域规划。依托该平台，我们系统评估了主流VLA模型与前沿视觉语言模型。实验不仅验证了VLA范式向空中系统迁移的可行性，更通过针对空中任务定制的多维度量指标，揭示了当前模型在无人机机动性、机械臂控制及高层规划方面的能力与局限。AIR-VLA为通用空中机器人研究建立了标准化测试平台与数据基础，相关资源可通过https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of Vision-Language-Action (VLA) models for Aerial Manipulation Systems (AMS), which face unique challenges like floating-base dynamics and complex, long-horizon tasks. The method introduces AIR-VLA, a benchmark comprising a physics-based simulation and a multimodal dataset of 3000 teleoperated demonstrations for tasks such as base manipulation and semantic reasoning. Experimental results systematically evaluate existing VLA and VLM models, confirming the feasibility of transferring VLA to aerial systems while delineating their current capabilities and limitations in UAV mobility and high-level planning.</div>
<div class="mono" style="margin-top:8px">本研究动机在于视觉-语言-动作（VLA）模型在航空操控系统（AMS）中的应用尚属空白，这类系统面临浮动基座动力学和复杂长时程任务等独特挑战。方法上提出了首个航空操控专用VLA基准AIR-VLA，构建了基于物理的仿真环境并发布了包含3000次手动遥操作演示的多模态数据集，涵盖基础操控、物体与空间理解、语义推理和长时程规划等任务，进而系统评估了主流VLA和最先进VLM模型。主要实验结果验证了VLA范式迁移到航空系统的可行性，并通过针对航空任务的多维指标揭示了当前模型在无人机机动性、机械臂控制和高层规划方面的能力与边界。</div>
</details>
</div>
<div class="card">
<div class="title">Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</div>
<div class="meta-line">Authors: Sangoh Lee, Sangwoo Mo, Wook-Shin Han</div>
<div class="meta-line">First: 2025-12-23T03:13:39+00:00 · Latest: 2026-01-29T12:02:16+00:00</div>
<div class="meta-line">Comments: Project page with videos and code: https://vap-project.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20014v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20014v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vap-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as &quot;bring my cup,&quot; where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递我的杯子！基于视觉注意力提示的视觉-语言-动作模型个性化方法</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作模型能较好泛化至通用指令，但在处理如&#x27;递我的杯子&#x27;这类个性化指令时存在困难——机器人需在视觉相似物体中识别并操作特定实例。本研究聚焦个性化物体操控场景：VLA模型需仅凭少量参考图像识别并操控训练时未见的用户专属物体。为此，我们提出视觉注意力提示——一种无需训练的高效感知适配器，为冻结的VLA模型注入自上而下的选择性注意力机制。该方法将参考图像视为非参数化视觉记忆，通过开放词汇检测与嵌入匹配在场景中定位目标物体，进而通过高亮目标与指令重写实现视觉提示注入。我们构建了Personalized-SIMPLER与Personalized-VLABench两个仿真基准及真实桌面操作基准，用于评估跨机器人多任务的个性化操控性能。实验表明，VAP在成功率与正确物体操作率上均优于通用策略与词元学习基线，有效弥合语义理解与实例级控制之间的鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Vision-Language-Action (VLA) models in executing personalized commands like &quot;bring my cup,&quot; which require identifying and manipulating a specific user object unseen during training from among visually similar alternatives. The proposed method, Visual Attentive Prompting (VAP), is a training-free perceptual adapter that equips frozen VLAs with selective attention by using a few reference images as a visual memory; it grounds the personal object through open-vocabulary detection and embedding matching, then injects this grounding by highlighting the object and rewriting the instruction. Experimental evaluations on simulation benchmarks (Personalized-SIMPLER and Personalized-VLABench) and a real-world tabletop benchmark demonstrate that VAP consistently outperforms generic policies and token-learning baselines in success rate and correct-object manipulation, effectively bridging the gap between semantic understanding and instance-level control.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉-语言-动作模型在执行个性化指令（如“拿我的杯子”）时的局限性，这类指令需要识别和操控训练中未见的特定用户对象。所提出的方法——视觉注意力提示，是一种无需训练、为冻结模型添加选择性注意力的感知适配器，它将少量参考图像作为视觉记忆，通过开放词汇检测和基于嵌入的匹配来定位个人物品，然后通过高亮对象和重写指令注入此定位信息。在仿真基准（Personalized-SIMPLER和Personalized-VLABench）和真实世界桌面基准上的实验评估表明，该方法在成功率和正确对象操控上持续优于通用策略和基于令牌学习的基线，有效缩小了语义理解与实例级控制之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Virtual Reflections on a Dynamic 2D Eye Model Improve Spatial Reference Identification</div>
<div class="meta-line">Authors: Matti Krüger, Yutaka Oshima, Yu Fang</div>
<div class="meta-line">First: 2024-12-10T09:37:25+00:00 · Latest: 2026-01-29T11:34:37+00:00</div>
<div class="meta-line">Comments: This article has been accepted for publication in IEEE Transactions on Human-Machine Systems. Citation information: DOI 10.1109/THMS.2026.3651818</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.07344v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.07344v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The visible orientation of human eyes creates some transparency about people&#x27;s spatial attention and other mental states. This leads to a dual role of the eyes as a means of sensing and communication. Accordingly, artificial eye models are being explored as communication media in human-machine interaction scenarios. One challenge in the use of eye models for communication consists of resolving spatial reference ambiguities, especially for screen-based models. To address this challenge, we introduce an approach that incorporates reflection-like features that are contingent on the movements of artificial eyes. We conducted a user study with 30 participants in which participants had to use spatial references provided by dynamic eye models to advance in a fast-paced group interaction task. Compared to a non-reflective eye model and a pure reflection mode, the superimposition of screen-based eyes with gaze-contingent virtual reflections resulted in a higher identification accuracy and user experience, suggesting a synergistic benefit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态二维眼球模型的虚拟反射特征提升空间参照识别精度</div>
<div class="mono" style="margin-top:8px">人眼可见的朝向能部分反映个体的空间注意力及其他心理状态，使眼睛兼具感知与交流的双重功能。因此，人工眼球模型正被探索作为人机交互场景中的沟通媒介。使用眼球模型进行交流的挑战之一在于消除空间参照歧义，尤其对于基于屏幕的模型。为解决该问题，我们提出一种融合类反射特征的方法，该特征随人工眼球的运动而动态变化。我们开展了一项30名参与者用户研究，要求参与者利用动态眼球模型提供的空间参照完成快节奏群体交互任务。与无反射眼球模型及纯反射模式相比，基于屏幕的眼球模型叠加注视关联虚拟反射后，识别准确率与用户体验均显著提升，显示出协同增益效应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address spatial reference ambiguities when using artificial eye models for communication in human-machine interaction, this research introduces a method that adds gaze-contingent virtual reflections to a dynamic 2D eye model on a screen. The approach was evaluated in a user study where 30 participants performed a fast-paced group interaction task using spatial references from the eye models. Experimental results showed that the model with gaze-contingent virtual reflections achieved higher identification accuracy and better user experience compared to a non-reflective model and a model with pure reflections, indicating a synergistic benefit.</div>
<div class="mono" style="margin-top:8px">为解决用于人机交互的屏幕人工眼模型存在的空间指代模糊性问题，本研究提出了一种方法，将视线关联的虚拟反射叠加到动态二维眼模型上。该方法通过一项用户研究进行评估，30名参与者在快速团队交互任务中使用眼模型提供的空间参考。实验结果表明，与无反射模型和纯反射模型相比，视线关联的反射模型获得了更高的识别准确率和更好的用户体验，表明组合特征具有协同效益。</div>
</details>
</div>
<div class="card">
<div class="title">EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots</div>
<div class="meta-line">Authors: Zixing Lei, Genjia Liu, Yuanshuo Zhang, Qipeng Liu, Chuan Wen, Shanghang Zhang, Wenzhao Lian, Siheng Chen</div>
<div class="meta-line">First: 2026-01-29T11:33:49+00:00 · Latest: 2026-01-29T11:33:49+00:00</div>
<div class="meta-line">Comments: 37 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21570v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21570v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs&#x27; success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EmboCoach-Bench：基于具身机器人开发的AI智能体基准测试</div>
<div class="mono" style="margin-top:8px">在高保真仿真与大规模数据采集的推动下，具身人工智能领域正朝着通用机器人系统方向快速发展。然而，从复杂的奖励塑形到跨异构后端的超参数调优，当前规模化能力仍严重受限于劳动密集型的人工监督。受大语言模型在软件自动化与科学发现领域成功的启发，我们提出\textsc{EmboCoach-Bench}基准，用于评估大语言模型智能体自主设计具身策略的能力。该框架涵盖32项专家精选的强化学习与模仿学习任务，将可执行代码确立为通用接口。我们超越静态生成，评估动态闭环工作流——智能体利用环境反馈迭代式地起草、调试与优化解决方案，改进范围涵盖从基于物理的奖励设计到扩散策略等策略架构。大量评估得出三个关键结论：（1）自主智能体在平均成功率上可定性超越人工设计基线26.5%；（2）结合环境反馈的智能工作流能有效强化策略开发，显著缩小开源模型与专有模型间的性能差距；（3）智能体对异常工程案例展现出自我修正能力，通过迭代式仿真闭环调试，成功将接近完全失败的任务性能恢复。本研究为自进化具身智能奠定基础，加速具身AI领域从劳动密集型人工调优向可扩展自主工程范式的转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the bottleneck of labor-intensive manual oversight in scaling embodied AI systems, motivated by the need for autonomous engineering of robotic policies. The method introduces EmboCoach-Bench, a benchmark that evaluates LLM agents through a dynamic closed-loop workflow where they iteratively draft, debug, and optimize executable code for 32 RL and IL tasks using environment feedback. Key experimental findings show that autonomous agents outperform human-engineered baselines by 26.5% in average success rate, reduce the performance gap between open-source and proprietary models, and demonstrate self-correction capabilities to recover from near-total failures.</div>
<div class="mono" style="margin-top:8px">具身智能向通用机器人系统的快速发展受到依赖人工密集监督（如奖励塑造和超参数调优）的严重制约。为此，研究者提出了EmboCoach-Bench基准，通过将可执行代码作为通用接口，评估大语言模型智能体在32个强化学习和模仿学习任务中，利用环境反馈进行迭代起草、调试和优化的动态闭环工作流，以自主开发机器人策略。主要实验结果表明，自主智能体的平均成功率比人工设计的基线高出26.5%；基于反馈的智能体工作流有效缩小了开源与专有模型之间的性能差距；并且智能体能够通过迭代调试，从近乎完全失败的任务中实现自我纠正并恢复性能。</div>
</details>
</div>
<div class="card">
<div class="title">Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning</div>
<div class="meta-line">Authors: Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava, Giacomo Indiveri, Chiara De Luca, Chiara Bartolozzi</div>
<div class="meta-line">First: 2026-01-29T11:05:23+00:00 · Latest: 2026-01-29T11:05:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21548v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#x27;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用脉冲强化学习训练慢速硅神经元控制极速机器人</div>
<div class="mono" style="margin-top:8px">空气曲棍球需要在高速冰球运动中实现瞬间决策，我们通过运行在混合信号模拟/数字神经形态处理器上的紧凑脉冲神经元网络应对这一挑战。通过硬件与学习算法的协同设计，我们训练该系统在极少量尝试中通过强化学习实现成功的冰球交互。该网络利用固定随机连接捕捉任务的时间结构，并在读出层采用局部e-prop学习规则，以利用事件驱动活动实现快速高效的学习。最终构建了包含计算机与神经形态芯片的实时闭环学习系统，为机器人自主系统提供了实用的脉冲神经网络训练方案。这项工作将神经科学启发的硬件与现实世界的机器人控制相连接，表明类脑方法能够处理快速交互任务，并支持智能机器的持续在线学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to control high-speed robots, such as an air hockey playing system, which requires extremely fast reactions to a fast-moving puck. The method employs a spiking neural network implemented on a mixed-signal neuromorphic processor and trains it using a reinforcement learning approach called e-prop in the readout layer, leveraging fixed random connectivity to handle temporal dynamics. Key experimental results demonstrate that the system achieves successful real-time puck interactions after a remarkably small number of training trials, enabling efficient, event-driven learning directly on the hardware for robotic control.</div>
<div class="mono" style="margin-top:8px">本研究旨在控制快节奏的机器人任务——空气曲棍球，该任务需要在高速冰球运动中进行毫秒级决策。方法采用硬件与学习算法的协同设计，在神经形态处理器上运行一个紧凑的脉冲神经网络，并在读出层使用局部e-prop强化学习规则来利用事件驱动活动进行训练。实验结果表明，系统在极少数试验中即成功实现了实时冰球交互学习，支持了实用的在环训练，从而将神经形态硬件与现实世界的机器人控制连接起来。</div>
</details>
</div>
<div class="card">
<div class="title">Industrial Internet Robot Collaboration System and Edge Computing Optimization</div>
<div class="meta-line">Authors: Haopeng Zhao, Dajun Tao, Tian Qi, Jingyuan Xu, Zijie Zhou, Lipeng Liu</div>
<div class="meta-line">First: 2025-04-03T11:15:10+00:00 · Latest: 2026-01-29T10:39:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.02492v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.02492v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In industrial Internet environments, mobile robots must generate collision-free global routes under stochastic obstacle layouts and random perturbations in commanded linear and angular velocities. This paper models a differential-drive robot with nonholonomic constraints, then decomposes motion into obstacle avoidance, target turning, and target approaching behaviors to parameterize the control variables. Global path planning is formulated as a constrained optimization problem and converted into a weighted energy function that balances path length and collision penalties. A three-layer neural network represents the planning model, while simulated annealing searches for near-global minima and mitigates local traps. During execution, a fuzzy controller uses heading and lateral-offset errors to output wheel-speed differentials for rapid correction; edge-side computation is discussed to reduce robot-server traffic and latency. Matlab 2024 simulations report deviation within +-5 cm, convergence within 10 ms, and shorter paths than two baseline methods. The approach improves robustness of global navigation in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>工业互联网机器人协同系统与边缘计算优化</div>
<div class="mono" style="margin-top:8px">在工业互联网环境中，移动机器人需在随机障碍布局及线速度与角速度指令受随机扰动的情况下生成无碰撞全局路径。本文对具有非完整约束的差速驱动机器人进行建模，将运动分解为避障、转向目标与接近目标三种行为以参数化控制变量。全局路径规划被构建为约束优化问题，并转化为平衡路径长度与碰撞惩罚的加权能量函数。采用三层神经网络表示规划模型，同时利用模拟退火算法搜索近全局最小值并规避局部陷阱。执行阶段，模糊控制器通过航向角与横向偏移误差输出轮速差以实现快速校正；文中探讨了边缘侧计算以减少机器人与服务器间的通信流量与延迟。基于Matlab 2024的仿真显示：路径偏差控制在±5厘米内，收敛时间小于10毫秒，且所得路径较两种基准方法更短。该方法在实践中提升了全局导航的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of generating collision-free global routes for mobile robots in stochastic industrial Internet environments with nonholonomic constraints, this paper models a differential-drive robot and decomposes its motion into obstacle avoidance, target turning, and target approaching behaviors to parameterize control variables. The method formulates global path planning as a constrained optimization problem, converting it into a weighted energy function that balances path length and collision penalties, and employs a three-layer neural network for the planning model alongside simulated annealing to search for near-global minima and avoid local traps. Experimental simulations in Matlab 2024 demonstrate that the approach achieves path deviation within ±5 cm, convergence within 10 ms, and generates shorter paths compared to two baseline methods, thereby improving the robustness of global navigation in practice.</div>
<div class="mono" style="margin-top:8px">为解决工业互联网随机环境中具有非完整约束的移动机器人生成无碰撞全局路径的挑战，本文对差速驱动机器人进行建模，并将其运动分解为避障、目标转向和目标接近行为以参数化控制变量。该方法将全局路径规划构建为一个约束优化问题，并将其转化为平衡路径长度与碰撞惩罚的加权能量函数，同时采用三层神经网络作为规划模型，并结合模拟退火算法来寻找近全局最优解并避免局部陷阱。在Matlab 2024中的仿真实验表明，该方法路径偏差在±5厘米内，收敛时间在10毫秒内，且相比两种基线方法生成了更短的路径，从而在实践中增强了全局导航的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation</div>
<div class="meta-line">Authors: Joonhee Lee, Hyunseung Shin, Jeonggil Ko</div>
<div class="meta-line">First: 2026-01-29T10:25:14+00:00 · Latest: 2026-01-29T10:25:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21506v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21506v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IROS：基于实时视觉语言模型室内导航的双过程架构</div>
<div class="mono" style="margin-top:8px">室内移动机器人导航需兼具快速响应与鲁棒语义理解能力，现有方法难以同时满足。传统几何方法（如SLAM）虽能实现可靠定位，但依赖精细地图且无法解析室内推理必需的人为标识（如指示牌、房间号）。视觉-语言-动作模型引入了语义基础，但严格遵循反应式决策，仅依据可见帧进行判断，无法预判未见的交叉路口或推理远距离文本线索。视觉语言模型虽能提供更丰富的上下文推理，但存在高计算延迟问题，不适用于嵌入式平台的实时操作。本研究提出IROS实时导航框架，在低成本设备硬件上融合VLM级上下文推理与轻量感知模块的效率。受双过程理论启发，IROS将快速反射决策（系统一）与慢速审慎推理（系统二）分离，仅在必要时调用VLM。通过为紧凑型VLM增强空间与文本线索，IROS以最小延迟实现类人鲁棒导航。在五座真实建筑中的实验表明，相较于持续基于VLM的导航方案，IROS将决策准确率提升66%，并显著降低延迟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of achieving both real-time responsiveness and robust semantic understanding in indoor robot navigation, where classical geometric methods lack semantic interpretation and vision-language models (VLMs) suffer from high latency. The proposed IROS framework introduces a dual-process architecture inspired by cognitive theory, separating fast, reflexive decisions using lightweight perceptual modules from slower, deliberative reasoning with a VLM that is invoked only when necessary; the VLM is also augmented with spatial and textual cues for better contextual inference. Experimental results from five real-world buildings demonstrate that IROS significantly improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.</div>
<div class="mono" style="margin-top:8px">本研究针对现有室内导航方法在语义理解不足或计算延迟高方面的局限，提出了IROS，一种双过程架构，仅在必要时整合快速反射决策与视觉语言模型的慢速审慎推理。该方法将导航分离为用于实时操作的轻量感知模块和用于上下文推理的增强型紧凑视觉语言模型，通过融入空间和文本线索提高鲁棒性。在五个真实建筑中的实验结果表明，与连续基于视觉语言模型的导航相比，IROS提高了决策准确性并降低了66%的延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Don&#x27;t double it: Efficient Agent Prediction in Occlusions</div>
<div class="meta-line">Authors: Anna Rothenhäusler, Markus Mazzola, Andreas Look, Raghu Rajan, Joschka Bödecker</div>
<div class="meta-line">First: 2026-01-29T10:22:38+00:00 · Latest: 2026-01-29T10:22:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21504v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21504v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent&#x27;s heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>避免重复预测：遮挡场景下的高效智能体预测</div>
<div class="mono" style="margin-top:8px">遮挡交通参与者对自动驾驶车辆构成重大挑战，隐藏的行人或车辆可能突然出现，但该问题研究仍不充分。现有基于学习的方法虽能推断隐藏智能体的存在，却常产生冗余的占据预测——单个智能体被重复识别。这一问题使下游规划复杂化并增加计算负荷。为此，我们提出MatchInformer，一种基于先进SceneInformer架构的新型Transformer方法。该方法通过将目标检测领域先进的匈牙利匹配算法整合至训练过程，强制实现预测与真实值的一一对应，从而减少冗余。我们进一步通过解耦智能体航向与运动来优化轨迹预测，该策略提升了预测路径的准确性与可解释性。为更好处理类别不平衡问题，我们提出使用马修斯相关系数评估占据预测。MCC通过综合考虑混淆矩阵所有条目，即使在稀疏或不平衡场景下也能提供稳健度量。在Waymo开放运动数据集上的实验表明，本方法提升了对遮挡区域的推理能力，并较先前方法产生了更精确的轨迹预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of predicting occluded traffic agents for autonomous vehicles, where existing learning-based methods often generate redundant occupancy predictions that complicate planning and increase computational load. The proposed MatchInformer method enhances the SceneInformer architecture by integrating Hungarian Matching during training to enforce a one-to-one correspondence between predictions and ground truth, reducing redundancy, and decouples agent heading from motion to refine trajectory forecasts. Experiments on the Waymo Open Motion Dataset show that this approach improves reasoning in occluded regions and yields more accurate trajectory predictions compared to prior methods.</div>
<div class="mono" style="margin-top:8px">本研究针对自动驾驶中预测被遮挡交通参与者的挑战，现有基于学习的方法常产生冗余的占用预测，从而增加规划复杂性和计算负担。提出的MatchInformer方法在SceneInformer架构基础上，通过在训练中集成匈牙利匹配算法来强制预测与真实值之间的一一对应，以减少冗余，并解耦智能体的航向与运动以优化轨迹预测。在Waymo开放运动数据集上的实验表明，该方法能提升对遮挡区域的推理能力，并产生比先前方法更准确的轨迹预测。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
