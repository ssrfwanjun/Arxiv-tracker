<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-24 03:34</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260124_0334</div>
    <div class="row"><div class="card">
<div class="title">Why Can&#x27;t I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition</div>
<div class="meta-line">Authors: Geo Ahn, Inwoong Lee, Taeoh Kim, Minho Shim, Dongyoon Wee, Jinwoo Choi</div>
<div class="meta-line">First: 2026-01-22T18:59:13+00:00 · Latest: 2026-01-22T18:59:13+00:00</div>
<div class="meta-line">Comments: The code is available at https://github.com/KHU-VLL/RCORE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16211v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16211v1">PDF</a> · <a href="https://github.com/KHU-VLL/RCORE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为何打不开抽屉？缓解零样本组合动作识别中的物体驱动捷径</div>
<div class="mono" style="margin-top:8px">本研究聚焦组合视频理解（CVU），要求模型识别动词与物体并将其组合以泛化至未见组合。我们发现现有零样本组合动作识别（ZS-CAR）模型失效的主要原因是未被重视的失效模式：物体驱动的动词捷径。通过系统分析，我们揭示该行为源于两个交织因素：组合监督的严重稀疏性与偏态分布，以及动词与物体间不对称的学习难度。随着训练进行，现有ZS-CAR模型逐渐忽略视觉证据并过度拟合共现统计，导致无法从未见动宾组合中获得组合识别的优势。为此，我们提出RCORE框架，通过强制时序锚定的动词学习来应对此问题。RCORE包含：（1）组合感知增强技术，在不破坏运动线索的前提下多样化动宾组合；（2）时序顺序正则化损失，通过显式建模时序结构惩罚捷径行为。在Sth-com与新构建的EK100-com两个基准测试中，RCORE显著提升未见组合准确率，降低对共现偏置的依赖，并实现持续正向的组合性能增益。本研究揭示物体驱动捷径是ZS-CAR的关键限制因素，证明解决该问题对实现鲁棒的组合视频理解至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the failure of existing Zero-Shot Compositional Action Recognition (ZS-CAR) models, attributing it to object-driven verb shortcuts where models overfit to object-verb co-occurrences and ignore visual motion evidence. To mitigate this, the authors propose RCORE, a framework that employs composition-aware data augmentation to diversify verb-object pairs and a temporal order regularization loss to penalize shortcut learning by enforcing grounded verb understanding. Experiments on the Sth-com and a newly constructed EK100-com benchmarks show that RCORE significantly improves accuracy on unseen compositions, reduces co-occurrence bias, and achieves positive compositional gaps, demonstrating the importance of addressing object-driven shortcuts for robust compositional video understanding.</div>
<div class="mono" style="margin-top:8px">本研究探讨了现有零样本组合动作识别模型失败的原因，将其归因于物体驱动的动词捷径，即模型忽略视觉运动线索并过度拟合训练数据中虚假的动词-物体共现关系。为缓解此问题，作者提出了RCORE框架，该框架通过组合感知的视频增强来多样化组合，并利用时序顺序正则化损失来惩罚捷径行为，从而强制进行基于时序的动词学习。在Sth-com和新构建的EK100-com基准测试上的实验表明，RCORE显著提升了未见动词-物体组合的识别准确率，降低了对共现偏差的依赖，并实现了持续为正的组合泛化差距，证明了解决物体驱动捷径对于实现鲁棒的组合视频理解至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation</div>
<div class="meta-line">Authors: Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet A. Nguyen, Dong-Hwan Jang, Inderjit S Dhillon, Ismini Lourentzou</div>
<div class="meta-line">First: 2026-01-22T18:58:55+00:00 · Latest: 2026-01-22T18:58:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16210v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PyraTok：面向视频理解与生成的语言对齐金字塔型分词器</div>
<div class="mono" style="margin-top:8px">离散视频变分自编码器是现代文生视频与视频理解系统的核心基础，但现有分词器通常仅在单一尺度下学习视觉码本，其词汇量有限且语言监督浅层，导致跨模态对齐与零样本迁移性能不佳。本文提出PyraTok——一种语言对齐的金字塔型分词器，可在多时空分辨率下学习语义结构化的离散隐变量。PyraTok基于预训练视频VAE构建，其核心创新模块为语言对齐金字塔量化器：该模块通过共享的大型二进制码本，在编码器多个深度层级对特征进行离散化，生成紧凑而富有表现力的视频令牌序列。为实现视觉令牌与语言的紧密耦合，PyraTok联合优化多尺度文本引导量化与令牌层级结构的全局自回归目标。在十项基准测试中，PyraTok实现了最先进的视频重建效果，持续提升文生视频质量，并在视频分割、时序动作定位与视频理解任务中创下零样本性能新纪录，且可稳健扩展至4K/8K分辨率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in existing discrete video VAEs, which often rely on single-scale visual codebooks with limited vocabularies and weak language supervision, resulting in poor cross-modal alignment and zero-shot transfer. The method introduces PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions by building on a pretrained video VAE and a novel Language-aligned Pyramidal Quantization (LaPQ) module; this module discretizes encoder features at several depths using a shared large binary codebook, and the model is jointly optimized with multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Key experimental findings show that PyraTok achieves state-of-the-art video reconstruction, consistently improves text-to-video generation quality, and sets new state-of-the-art zero-shot performance on video segmentation, temporal action localization, and video understanding tasks across ten benchmarks, scaling robustly to up to 4K/8K resolutions.</div>
<div class="mono" style="margin-top:8px">现有的离散视频变分自编码器通常依赖于词汇量有限、语言监督薄弱的单尺度视觉码本，这阻碍了跨模态对齐和零样本迁移能力。为解决此问题，研究者提出了PyraTok，一种语言对齐的金字塔形分词器，它基于预训练的视频变分自编码器，并引入了语言对齐的金字塔量化模块，通过共享的大型二进制码本在多个时空分辨率深度上离散化编码器特征，从而生成紧凑且富有表现力的视频令牌序列。在十个基准测试上的实验结果表明，PyraTok实现了最先进的视频重建效果，持续提升了文本到视频的生成质量，并在视频分割、时序动作定位和视频理解等任务上取得了新的零样本性能最优结果，同时能有效扩展至高达4K/8K的分辨率。</div>
</details>
</div>
<div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：面向文档的具身视觉语言前端系统</div>
<div class="mono" style="margin-top:8px">GutenOCR是通过微调Qwen2.5-VL-3B和Qwen2.5-VL-7B获得的一系列具身OCR前端模型。所得的单检查点视觉语言模型通过统一的提示驱动接口，实现文本读取、检测与定位功能。基于商业文档、科学文献及合成定位数据训练，该模型支持整页与局部读取，提供行级与段落级边界框，并能响应“X位于何处？”的条件查询。我们提出了具身OCR评估框架，实验表明在1.05万份保留的商业与科学文档上，GutenOCR-7B的复合具身OCR分数较其骨干网络Qwen2.5-VL-7B提升超一倍（0.40至0.82）。在Fox与OmniDocBench v1.5基准测试中，本方法显著提升了区域/行级OCR性能及文本检测召回率，但在页面级线性化、色彩引导OCR及公式密集版式处理方面存在权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for unified document understanding models that combine reading, detection, and grounding capabilities. The method develops GutenOCR by fine-tuning Qwen2.5-VL vision-language models (3B and 7B parameters) on business documents, scientific articles, and synthetic grounding data, creating a single-checkpoint model with a prompt-based interface for full-page/localized reading and spatial queries. Experimental results show that GutenOCR-7B more than doubles the composite grounded OCR score of its backbone (0.40 to 0.82) on 10.5K held-out pages, substantially improves region/line-level OCR and text-detection recall on Fox and OmniDocBench benchmarks, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发更强大、统一的文档理解系统，为此提出了GutenOCR系列模型，这是一种基于视觉语言模型的OCR前端。该方法通过对Qwen2.5-VL-3B和Qwen2.5-VL-7B模型在商业文档、科学文章和合成数据上进行微调，创建了一个通过提示接口统一文本识别、检测和定位功能的单一检查点模型。主要实验结果表明，GutenOCR-7B在10.5K份保留的商业和科学文档上，其综合定位OCR分数相比其骨干模型提高了一倍以上，从0.40提升至0.82，并在Fox和OmniDocBench v1.5基准测试中显著提升了区域级、行级OCR性能以及文本检测召回率，但也在页面级线性化、颜色引导OCR和处理公式密集布局方面显示出一定的性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-in-Sandbox Elicits General Agentic Intelligence</div>
<div class="meta-line">Authors: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</div>
<div class="meta-line">First: 2026-01-22T18:57:09+00:00 · Latest: 2026-01-22T18:57:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://llm-in-sandbox.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16206v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16206v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://llm-in-sandbox.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#x27;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM-in-Sandbox激发通用智能体智能</div>
<div class="mono" style="margin-top:8px">我们提出LLM-in-Sandbox方法，使大语言模型能在代码沙盒（即虚拟计算机）内进行探索，从而激发非代码领域的通用智能。我们首先证明，未经额外训练的强大大语言模型展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能自发访问外部资源获取新知识、利用文件系统处理长上下文、执行脚本满足格式要求。我们进一步表明，通过LLM-in-Sandbox强化学习（仅使用非智能体数据训练模型进行沙盒探索），可增强这些智能体能力。实验表明，LLM-in-Sandbox在免训练和训练后两种设置下，均实现了跨越数学、物理、化学、生物医学、长上下文理解及指令遵循的稳健泛化。最后，我们从计算和系统角度分析其效率，并将其开源为Python包以促进实际部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to elicit general agentic intelligence from large language models (LLMs) by enabling them to explore and act within a code sandbox environment, which functions as a virtual computer. The method involves allowing LLMs to use the sandbox for tasks like accessing external resources, managing files for long contexts, and executing scripts, without additional training; this training-free capability is further enhanced through a reinforcement learning approach (LLM-in-Sandbox-RL) that uses non-agentic data to train models for sandbox exploration. Key experimental results show that LLM-in-Sandbox achieves robust generalization across diverse domains including mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following, with efficiency analyses provided from computational and system perspectives.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过让大型语言模型（LLMs）在代码沙盒环境中探索，从而在非代码领域激发其通用的智能体能力。方法涉及允许LLMs在虚拟计算机沙盒中交互，使其能够自发访问外部资源、管理文件以处理长上下文并执行脚本，无需额外训练；这种免训练方法通过LLM-in-Sandbox强化学习（LLM-in-Sandbox-RL）进一步强化，该学习使用非智能体数据来训练模型进行沙盒探索。实验结果表明，LLM-in-Sandbox在数学、物理、化学、生物医学、长上下文理解和指令遵循等多个领域实现了稳健的泛化能力，并从计算和系统角度分析了其效率。</div>
</details>
</div>
<div class="card">
<div class="title">Counterfactual Training: Teaching Models Plausible and Actionable Explanations</div>
<div class="meta-line">Authors: Patrick Altmeyer, Aleksander Buszydlik, Arie van Deursen, Cynthia C. S. Liem</div>
<div class="meta-line">First: 2026-01-22T18:56:14+00:00 · Latest: 2026-01-22T18:56:14+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the 2026 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反事实训练：为模型提供合理且可操作的因果解释</div>
<div class="mono" style="margin-top:8px">本文提出一种称为反事实训练的新型训练机制，利用反事实解释增强模型的可解释性。反事实解释已成为不透明机器学习模型的常用事后解释方法：它们揭示事实输入需如何改变才能使模型产生期望输出。为在实际决策系统中发挥作用，反事实应基于底层数据具有合理性，并符合特征可变性约束的可操作性。现有研究多聚焦开发满足这些要求的事后生成方法。本研究则直接要求模型对最终目标负责：反事实训练在训练阶段运用反事实，以最小化学习表征与合理可操作解释之间的差异。我们通过实证与理论证明，该方法能训练出具有内在理想反事实解释能力的模型，并提升对抗鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for machine learning models to provide plausible and actionable counterfactual explanations, which are crucial for real-world decision-making but are typically generated post-hoc. The method introduces counterfactual training, a novel regime that integrates counterfactual explanations directly into the training phase to align learned representations with these desirable explanation properties. Experimental results show that this approach not only produces models capable of inherently generating better counterfactuals but also enhances their adversarial robustness.</div>
<div class="mono" style="margin-top:8px">为提升机器学习模型的可解释性，本研究提出了一种新颖的反事实训练方法，在模型训练阶段直接利用反事实解释，使学习到的表征与合理且可操作的解释保持一致。该方法最小化学习表征与理想反事实属性之间的差异，超越了事后生成技术。实验结果表明，通过此方法训练的模型能内在地产生更理想的反事实解释，并展现出更强的对抗鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Geospatial Place Representation Learning from Large-Scale Point-of-Interest Graph Data</div>
<div class="meta-line">Authors: Mohammad Hashemi, Hossein Amiri, Andreas Zufle</div>
<div class="meta-line">First: 2025-06-25T15:10:31+00:00 · Latest: 2026-01-22T18:46:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02921v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.02921v3">PDF</a> · <a href="https://github.com/mohammadhashemii/PlaceRep">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning effective representations of urban environments requires capturing spatial structure beyond fixed administrative boundaries. Existing geospatial representation learning approaches typically aggregate Points of Interest(POI) into pre-defined administrative regions such as census units or ZIP code areas, assigning a single embedding to each region. However, POIs often form semantically meaningful groups that extend across, within, or beyond these boundaries, defining places that better reflect human activity and urban function. To address this limitation, we propose PlaceRep, a training-free geospatial representation learning method that constructs place-level representations by clustering spatially and semantically related POIs. PlaceRep summarizes large-scale POI graphs from U.S. Foursquare data to produce general-purpose urban region embeddings while automatically identifying places across multiple spatial scales. By eliminating model pre-training, PlaceRep provides a scalable and efficient solution for multi-granular geospatial analysis. Experiments using the tasks of population density estimation and housing price prediction as downstream tasks show that PlaceRep outperforms most state-of-the-art graph-based geospatial representation learning methods and achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation of PlaceRep is available at https://github.com/mohammadhashemii/PlaceRep.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大规模兴趣点图数据的免训练地理空间场所表征学习</div>
<div class="mono" style="margin-top:8px">学习有效的城市环境表征需捕捉超越固定行政边界的空间结构。现有地理空间表征学习方法通常将兴趣点聚合至人口普查单元或邮政编码区域等预定义行政区域，并为每个区域分配单一嵌入向量。然而，兴趣点常形成跨越、穿透或超出这些边界的语义化群组，从而定义出更能反映人类活动与城市功能的场所。为突破此局限，我们提出PlaceRep——一种免训练的地理空间表征学习方法，通过聚类空间与语义相关的兴趣点构建场所级表征。该方法基于美国Foursquare数据的大规模兴趣点图进行归纳，在自动识别多空间尺度场所的同时生成通用型城市区域嵌入向量。通过消除模型预训练环节，PlaceRep为多粒度地理空间分析提供了可扩展的高效解决方案。在人口密度估算与房价预测下游任务中的实验表明，PlaceRep优于多数基于图结构的先进地理空间表征学习方法，并在大规模兴趣点图上实现区域级表征生成速度提升高达100倍。项目代码已开源：https://github.com/mohammadhashemii/PlaceRep。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitation of existing geospatial representation methods that rely on fixed administrative boundaries and aggregate Points of Interest (POI) into pre-defined regions, this research introduces PlaceRep, a training-free method for learning place-level representations. The approach constructs these representations by clustering spatially and semantically related POIs from large-scale graph data, such as U.S. Foursquare data, to automatically identify places across multiple spatial scales without requiring model pre-training. Experimental results on downstream tasks including population density estimation and housing price prediction demonstrate that PlaceRep outperforms most state-of-the-art graph-based geospatial methods and achieves up to a 100x speedup in generating region-level embeddings on large-scale POI graphs.</div>
<div class="mono" style="margin-top:8px">为克服现有地理空间表征方法依赖固定行政边界、将兴趣点聚合到预定义区域的局限，本研究提出了PlaceRep，一种无需训练即可学习地点级表征的方法。该方法从大规模图数据（如美国Foursquare数据）中聚类空间和语义相关的兴趣点，自动识别多尺度地点，并在无需模型预训练的情况下生成通用城市区域嵌入。在人口密度估计和房价预测等下游任务上的实验评估表明，PlaceRep的性能优于大多数最先进的基于图的地理空间方法，并在大规模兴趣点图上生成区域级表征时实现了高达100倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks</div>
<div class="meta-line">Authors: Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Charles McGrady, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, Yixin Liu, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, Arman Cohan</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-07-01T17:51:59+00:00 · Latest: 2026-01-22T18:32:06+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Datasets &amp; Benchmarks Track (Spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01001v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.01001v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature-grounded tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 47 foundation models and has collected over 20,000 votes from human researchers across diverse scientific domains. Our analysis of the data collected so far confirms its high quality. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on collected preference data. It measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark&#x27;s challenges and emphasize the need for more reliable automated evaluation methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SciArena：面向非可验证科学文献基础任务的开放评估平台</div>
<div class="mono" style="margin-top:8px">我们推出SciArena，一个开放协作的平台，用于评估基础模型在科学文献基础任务上的表现。与传统科学文献理解与综合基准不同，SciArena借鉴Chatbot Arena的社区投票评估模式，直接邀请研究社区参与模型比较投票。通过汇聚集体智慧，该平台以社区驱动的方式评估模型在需要基于文献的长篇开放式科学任务中的性能。目前平台支持47个基础模型，并已收集来自不同科学领域研究人员的超过20,000次投票。我们对现有数据的分析证实了其高质量。基于模型排名榜单，我们讨论了相关结果与洞见。为促进文献任务自动化模型评估系统的研究，我们发布了基于收集偏好数据的元评估基准SciArena-Eval，通过对比模型成对评估与人类投票来衡量其答案质量判断的准确性。实验揭示了该基准的挑战性，并凸显了对更可靠自动化评估方法的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for robust evaluation of foundation models on open-ended scientific literature-grounded tasks, where traditional benchmarks often lack verifiable ground truth. The method introduces SciArena, an open platform that adapts the Chatbot Arena approach by collecting community votes on pairwise model comparisons for long-form, literature-based responses. Experimental results from over 20,000 human votes across 47 models confirm high data quality and yield a public leaderboard; the derived SciArena-Eval benchmark further reveals significant challenges for automated evaluation methods in matching human preference judgments.</div>
<div class="mono" style="margin-top:8px">SciArena的研发动机源于现有基准难以评估大模型在开放、基于文献的科学任务上的表现，这些任务通常需要长篇回答且难以验证。该方法借鉴了Chatbot Arena的社区投票模式，构建了一个开放平台，让研究社区直接比较和投票选择模型生成的回答，从而利用集体智慧进行评估。主要实验结果基于对47个模型收集的超过20,000张人类投票，证实了数据的高质量，并据此生成了模型排名榜单；同时，发布的SciArena-Eval元评估基准表明，现有自动化评估方法在匹配人类判断方面仍面临巨大挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Paramanu: Compact and Competitive Monolingual Language Models for Low-Resource Morphologically Rich Indian Languages</div>
<div class="meta-line">Authors: Mitodru Niyogi, Eric Gaussier, Arnab Bhattacharya</div>
<div class="meta-line">First: 2024-01-31T17:58:10+00:00 · Latest: 2026-01-22T18:28:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.18034v3">Abs</a> · <a href="https://arxiv.org/pdf/2401.18034v3">PDF</a> · <a href="https://huggingface.co/collections/mitodru/paramanu">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multilingual large language models (LLMs) are expensive to pretrain and often suffer from imbalances across languages and datasets, English-centric bias, tokenizer oversegmentation for morphologically rich low-resource languages, and the curse of multilinguality. We introduce PARAMANU, the first family of Indian-only autoregressive language models trained from scratch on open-source language-specific data for the five most spoken Indian languages: Bengali, Hindi, Marathi, Tamil, and Telugu. All models are designed for affordability and are trained on a single GPU with a budget under $1,000, allowing under-resourced researchers to build competitive language models. To address low-resource challenges, we develop morphology-aligned, low-fertility tokenizers, propose an interpolation-based method for token position indices in RoPE based scaling to train longer sequences efficiently. We also create instruction-tuning datasets in Bangla that are translated to the other four languages. Despite their small size (108M-367M parameters), Paramanu achieves a strong performance-efficiency tradeoff and outperforms most larger multilingual models across all five languages. Our collection is available at https://huggingface.co/collections/mitodru/paramanu.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Paramanu：面向低资源形态丰富印度语言的紧凑型高性能单语模型</div>
<div class="mono" style="margin-top:8px">多语种大语言模型（LLM）预训练成本高昂，常面临语言与数据集不平衡、英语中心偏见、对形态丰富的低资源语言分词过度，以及多语性诅咒等问题。我们推出PARAMANU系列——首个完全基于开源单语数据从头训练的印度语言自回归模型家族，涵盖五种使用最广泛的印度语言：孟加拉语、印地语、马拉地语、泰米尔语和泰卢固语。所有模型均注重成本效益，可在单GPU上以低于1000美元的预算完成训练，使资源有限的研究者也能构建有竞争力的语言模型。针对低资源挑战，我们开发了形态对齐的低生育率分词器，提出基于旋转位置编码（RoPE）缩放中令牌位置索引的插值方法以高效训练长序列，并创建了孟加拉语指令微调数据集（同步翻译至其余四种语言）。尽管模型规模较小（1.08亿-3.67亿参数），Paramanu在性能与效率间取得优异平衡，在全部五种语言上超越多数更大规模的多语模型。模型集已发布于https://huggingface.co/collections/mitodru/paramanu。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high cost and imbalances of multilingual large language models, which often exhibit English-centric bias and tokenizer oversegmentation for morphologically rich low-resource languages, this work introduces PARAMANU, a family of monolingual autoregressive models trained from scratch for five major Indian languages: Bengali, Hindi, Marathi, Tamil, and Telugu. The method focuses on affordability, training on a single GPU under $1,000, and employs morphology-aligned tokenizers and an interpolation-based technique for RoPE scaling to handle longer sequences efficiently. Key experimental results show that despite their compact size (108M-367M parameters), these models achieve a strong performance-efficiency tradeoff and outperform most larger multilingual models across all five languages.</div>
<div class="mono" style="margin-top:8px">为解决多语言大语言模型成本高昂、语言间不平衡的问题，特别是针对资源匮乏、形态丰富的印度语言，本研究提出了Paramanu，一个为孟加拉语、印地语、马拉地语、泰米尔语和泰卢固语从头开始训练的单语自回归模型系列。该方法注重可负担性，在单个GPU上以低于1000美元的预算进行训练，并采用形态对齐的分词器以及基于RoPE缩放的插值技术来高效处理长序列。实验结果表明，这些紧凑模型（1.08亿至3.67亿参数）在所有五种语言上的表现均优于大多数更大的多语言模型，展现了优异的性能效率权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-01-22T18:24:00+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v1">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题发现新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时执行强化学习，使大型语言模型能够持续训练，但此时训练经验专门针对测试问题。这种持续学习形式非常特殊，其目标是产生一个卓越解决方案而非多个平均良好的方案，并专注于解决当前问题而非泛化至其他问题。因此，我们的学习目标和搜索子程序被设计为优先考虑最有潜力的解决方案。我们将此方法称为“测试时训练发现法”。遵循先前研究，我们聚焦于具有连续奖励的问题。我们报告了在数学、GPU内核工程、算法设计和生物学领域尝试的所有问题的结果：TTT-Discover在几乎所有问题上都创造了新的最优解：（i）埃尔德什最小重叠问题与自相关不等式；（ii）GPUMode内核竞赛（比现有技术快达2倍）；（iii）过往AtCoder算法竞赛；（iv）单细胞分析中的去噪问题。我们的解决方案均经过专家或组织方评审。所有结果均使用开源模型OpenAI gpt-oss-120b实现，并可通过我们公开的代码复现，而先前的最佳结果需依赖闭源前沿模型。测试时训练通过Thinking Machines的Tinker API运行，每个问题成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance AI&#x27;s capability to discover novel state-of-the-art solutions for scientific problems, moving beyond prior test-time scaling methods that rely on prompting frozen LLMs. The proposed method, Test-Time Training to Discover (TTT-Discover), employs reinforcement learning at test time, allowing the LLM to continuously train with experience specific to the target problem; it is designed with a learning objective and search subroutine that prioritize the most promising solutions to achieve one great outcome for the given problem. Experimental results across mathematics, GPU kernel engineering, algorithm design, and biology demonstrate that TTT-Discover sets new state-of-the-art performance in nearly all attempted problems, including Erdős&#x27; minimum overlap problem, a GPUMode kernel competition (achieving up to 2× speedup), past AtCoder algorithm competitions, and a denoising task in single-cell analysis, with solutions verified by experts and achieved using an open model at a cost of a few hundred dollars per problem.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升人工智能在科学问题中发现新颖最优解的能力，超越先前依赖提示冻结大语言模型的测试时扩展方法。所提出的方法——测试时训练发现，在测试时采用强化学习，使大语言模型能够针对特定测试问题持续训练；其学习目标和搜索子程序旨在优先考虑最有希望的解决方案，以针对该问题获得一个优秀结果。在数学、GPU内核工程、算法设计和生物学等多个领域的实验结果表明，该方法在几乎所有尝试的问题上都取得了新的最优性能，包括埃尔德什最小重叠问题、GPUMode内核竞赛、过去的AtCoder算法竞赛以及单细胞分析中的去噪任务，所有解决方案均经过专家验证，并使用开源模型和公开代码实现。</div>
</details>
</div>
<div class="card">
<div class="title">Structured Hints for Sample-Efficient Lean Theorem Proving</div>
<div class="meta-line">Authors: Zachary Burton</div>
<div class="meta-line">First: 2026-01-22T18:16:46+00:00 · Latest: 2026-01-22T18:16:46+00:00</div>
<div class="meta-line">Comments: 9 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构化提示在样本高效Lean定理证明中的应用</div>
<div class="mono" style="margin-top:8px">当前最先进的神经定理证明器（如DeepSeek-Prover-V1.5）将大语言模型与强化学习相结合，通过复杂训练取得了显著成果。我们提出：这些经过高度训练的模型在推理阶段是否仍能从简单的结构化引导中获益？我们在miniF2F基准上评估了一种轻量级干预方法——基于15种常见策略骨架的固定提示调度方案。该简单方法在相同样本数（k=16）和相同最大生成长度（1024个标记）条件下，实现了21.7%的pass@16成功率，优于同模型标准采样的15.2%，相对提升达43%。结果表明，即使经过强化学习训练的证明器也未能充分利用策略语言中可用的结构化先验知识，而简单的推理阶段引导仍能提供低成本、互补性的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates whether state-of-the-art, reinforcement learning-trained neural theorem provers like DeepSeek-Prover-V1.5 can benefit from simple structural guidance during inference, as they may underutilize available structural priors. The method introduces a lightweight intervention using a fixed prompt schedule over 15 common tactic skeletons to guide the model&#x27;s proof search. On the miniF2F benchmark, this approach increased the pass@16 rate from 15.2% with standard sampling to 21.7%, a 43% relative improvement using the same computational budget, demonstrating that simple inference-time guidance provides a cheap and complementary performance boost.</div>
<div class="mono" style="margin-top:8px">本研究探讨了经过强化学习训练的先进神经定理证明器（如DeepSeek-Prover-V1.5）在推理时是否能从简单的结构引导中受益，因为它们可能未充分利用策略语言中固有的先验知识。该方法引入了一种轻量级干预，使用一个包含15个常见策略骨架的固定提示调度来引导模型在miniF2F基准测试中的证明搜索。主要实验结果表明，该方法实现了21.7%的pass@16成功率，而标准采样仅为15.2%，在使用相同计算资源的情况下实现了43%的相对提升，这证明简单的推理时引导能提供一种廉价且互补的性能增强。</div>
</details>
</div>
<div class="card">
<div class="title">Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</div>
<div class="meta-line">Authors: Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</div>
<div class="meta-line">First: 2026-01-22T18:09:30+00:00 · Latest: 2026-01-22T18:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16163v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16163v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#x27;s latent diffusion process, harnessing the model&#x27;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cosmos策略：面向视觉运动控制与规划的视觉模型微调</div>
<div class="mono" style="margin-top:8px">近期视频生成模型展现出捕捉复杂物理交互与时序场景演变的卓越能力。为利用其时空先验知识，机器人研究领域已采用视频模型进行策略学习，但需通过多阶段后训练及新增动作生成架构组件，引入了复杂性。本研究提出Cosmos策略——一种通过单阶段后训练将大型预训练视频模型（Cosmos-Predict2）适配为高效机器人策略的简洁方法，无需修改架构，仅需在目标平台采集的机器人演示数据上进行训练。该策略通过视频模型的潜在扩散过程直接生成编码为潜在帧的机器人动作，利用模型的预训练先验与核心学习算法捕捉复杂动作分布。此外，Cosmos策略能同步生成编码为潜在帧的未来状态图像与价值函数（预期累积奖励），从而在测试阶段规划更高成功率的动作轨迹。实验评估显示，Cosmos策略在LIBERO与RoboCasa仿真基准测试中分别达到98.5%与67.1%的平均成功率，在现实世界复杂双手操作任务中获得最高平均分，其性能超越从头训练的扩散策略、基于视频模型的策略，以及相同演示数据微调的先进视觉-语言-动作模型。进一步地，基于策略推演数据，Cosmos策略可通过经验学习优化其世界模型与价值函数，并借助基于模型的规划在挑战性任务中实现更高成功率。相关代码、模型与训练数据已发布于https://research.nvidia.com/labs/dir/cosmos-policy/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To leverage the strong spatiotemporal priors of video generation models for robotics without introducing complex multi-stage training or architectural modifications, this work introduces Cosmos Policy, which fine-tunes a pretrained video model (Cosmos-Predict2) directly on robot demonstration data. The method encodes robot actions, future state images, and value predictions as latent frames within the model&#x27;s existing latent diffusion process, enabling it to capture complex action distributions and support test-time planning. Experiments show state-of-the-art performance on LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates) and superior results in real-world bimanual manipulation, outperforming diffusion policies trained from scratch and other fine-tuned models, with additional improvements possible through learning from rollout data to refine its world model and value function.</div>
<div class="mono" style="margin-top:8px">为了简化将强大视频生成模型应用于机器人技术的过程，避免复杂的多阶段训练或架构修改，本研究提出了Cosmos Policy。该方法通过单阶段微调预训练视频模型（Cosmos-Predict2）于机器人演示数据，直接在模型原有的扩散过程中以潜在帧形式生成机器人动作、未来状态图像和价值估计。实验结果表明，该方法在LIBERO和RoboCasa仿真基准上取得了最先进的性能（平均成功率分别为98.5%和67.1%），并在真实世界双手操作任务中表现最佳，优于从头训练的扩散策略和其他微调模型，且能通过规划和学习经验数据进一步提升成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates</div>
<div class="meta-line">Authors: Denise M. Case</div>
<div class="meta-line">First: 2026-01-22T17:51:02+00:00 · Latest: 2026-01-22T17:51:02+00:00</div>
<div class="meta-line">Comments: 29 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16152v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16152v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>持续分歧下的基底稳定性：中性本体论基底的结构约束</div>
<div class="mono" style="margin-top:8px">现代数据系统日益在持续的法律、政治与分析分歧条件下运行。在此类情境中，互操作性无法依赖共享解释、协商语义或集中权威，而必须通过作为中性基底的表征来实现——这些基底能在不相容的扩展中保持稳定的指称关系。本文探讨这一要求对本体设计施加的结构性约束。基于将解释非承诺性与扩展稳定性视为显式设计约束的中性框架，我们追问：若要在分歧下保持问责关系的可指称性与可比性，何种最小本体结构是必需的？此处的最小性并非简单简约：仅当简化不会将稳定性关键区分重新引入为隐藏角色、标记或语境谓词时，该简化才可接受。我们建立了条件性下界结果：任何能在持续分歧下支持问责的本体必须实现至少六种不同的同一性与持存机制。进一步证明，恰好包含六种此类机制的构造足以满足既定要求，且无需在基底中嵌入因果或规范性承诺。该结果并非普适本体方案，而是将中立性与稳定指称视为不可协商设计目标时对可能性的约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern data systems must operate amid persistent legal, political, and analytic disagreements, where interoperability cannot depend on shared interpretation or centralized authority. To address this, the paper investigates the structural constraints for designing neutral ontological substrates that maintain stable reference across incompatible extensions, treating interpretive non-commitment and stability under extension as explicit design goals. The method builds on a neutrality framework to determine the minimal ontological structure required for preserving accountability relationships, ensuring reductions do not hide critical distinctions as roles or predicates. Key experimental findings establish a conditional lower-bound result: any ontology supporting accountability under persistent disagreement must implement at least six distinct identity-and-persistence regimes, and a construction with exactly six regimes is shown to be sufficient without embedding causal or normative commitments.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在持续存在的法律、政治和分析分歧下运行的数据系统中，如何保持稳定指称和互操作性的挑战，这些系统缺乏共享解释或集中权威。方法基于一个中立性框架，将解释性非承诺和扩展下的稳定性作为明确的设计约束，探究在不嵌入隐藏承诺的情况下，保持问责关系可指称和可比所需的最小本体结构。关键实验结果确立了一个条件性下限结果：任何满足这些要求的本体必须实现至少六种不同的身份与持久性机制，并且论文证明，恰好包含六种此类机制的构造足以满足要求，而无需在基底中嵌入因果或规范性承诺。</div>
</details>
</div>
<div class="card">
<div class="title">Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization</div>
<div class="meta-line">Authors: Maximos Kaliakatsos-Papakostas, Dimos Makris, Konstantinos Soiledis, Konstantinos-Theodoros Tsamis, Vassilis Katsouros, Emilios Cambouropoulos</div>
<div class="meta-line">First: 2026-01-22T17:46:31+00:00 · Latest: 2026-01-22T17:46:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关注旋律：单编码器旋律和声生成的课程掩码策略</div>
<div class="mono" style="margin-top:8px">旋律和声生成是为给定旋律创作和声伴奏的任务，是计算音乐生成领域的核心挑战。近期基于单编码器Transformer的方法将其构建为掩码序列建模问题，但受离散扩散启发的现有训练课程常导致旋律与和声间的（交叉）注意力薄弱，限制了旋律线索的利用，尤其在域外场景中。本研究提出一种名为FF（全掩码至全可见）的训练课程，在训练初期保持所有和声标记被掩码，随后逐步揭示完整序列以强化旋律-和声交互。我们通过多维度实验系统评估该方法，包括时间量化（四分音符vs十六分音符）、小节级vs拍号条件、旋律表示（全音域vs音级）及推理时解掩策略。模型在HookTheory数据集上训练，并在域内和精选爵士标准曲集中评估，使用综合指标衡量和弦进行结构、和声-旋律对齐及节奏连贯性。结果表明，FF课程在几乎所有指标上均优于基线，在域外评估中提升尤为显著，其中对新旋律线索的和声适应性至关重要。进一步发现，四分音符量化、小节标记交织及音级旋律表示在FF框架中具有优势。本研究强调了训练课程对实现有效旋律条件化的重要性，表明全掩码至全可见策略为单编码器和声生成提供了稳健方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve melody-harmony interaction in single-encoder transformer models for melodic harmonization, which often exhibit weak cross-attention, this work introduces a full-to-full (FF) training curriculum. The method initially keeps all harmony tokens masked for several steps before progressively unmasking entire sequences during training to strengthen conditioning on the melody. Experimental evaluation on the HookTheory dataset and a collection of jazz standards shows that the FF curriculum outperforms prior baselines across most metrics, especially in out-of-domain settings, and benefits from quarter-note quantization, bar-token intertwining, and pitch-class melody representations.</div>
<div class="mono" style="margin-top:8px">本研究针对单编码器Transformer模型在和声编配任务中旋律与和声交互较弱的问题，提出了一种全掩码到全揭示（FF）的训练课程。该方法在训练初期保持所有和声标记被掩码，随后逐步揭示整个序列，以强化模型对旋律条件的利用。在HookTheory数据集和爵士标准曲集上的实验评估表明，该课程在多种指标上均优于基线方法，尤其在域外评估中表现突出，同时发现四分音符量化、小节标记交织和音高类别旋律表示在该设置下更具优势。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons</div>
<div class="meta-line">Authors: Terrence J. Sejnowski</div>
<div class="meta-line">First: 2025-12-17T19:05:18+00:00 · Latest: 2026-01-22T17:40:42+00:00</div>
<div class="meta-line">Comments: 31 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15891v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.15891v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of millisecond-precision spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could preserve and manipulate sensory information through spike timing. High temporal resolution enables a broader range of neural codes. It could also support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are stimulated synchronously. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于皮层神经元尖峰时序精度的长时工作记忆协调动力学机制</div>
<div class="mono" style="margin-top:8px">上世纪大多数关于皮层神经元的感知运动研究依赖于平均放电频率。速率编码适用于数秒内完成的快速感知运动处理。而对于时间尺度达数小时的长时工作记忆（Ericsson与Kintsch，1995），人们知之甚少。皮层神经元毫秒级精度的尖峰启动现象出人意料（Mainen与Sejnowski，1995）。更令人瞩目的是活体内神经对快速波动感觉输入产生的精准放电，这提示神经回路可能通过尖峰时序来保存和处理感觉信息。高时间分辨率拓展了神经编码的可能范围，亦可支持尖峰时序依赖性可塑性（STDP）——该机制由突触前后神经元毫秒级精度的尖峰相对时序触发。何种尖峰时序机制能在活体内调控STDP？皮层行波已在多个频段被观测到，其具有高时间精度。行波的波前可将尖峰时序与STDP相耦合：当波前穿过皮层柱时，锥体细胞与篮状细胞树突上的兴奋性突触被同步激活。抑制性篮状细胞在锥体细胞胞体上形成花萼样结构，强瞬态超极化后的抑制性反弹可触发反向传播动作电位，该电位稍晚于锥体树突的兴奋性输入抵达。由此激活的STDP可持续数小时，形成次级网络。这种临时网络可支撑长时工作记忆——一种建立在长期感知运动网络之上的认知网络。单独的行波与STDP尚未对皮层功能研究带来新突破，但二者的结合可能揭示思维活动的本质（Sejnowski，2025）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the gap in understanding long-term working memory, which operates over hours, contrasting with the well-studied fast sensorimotor processing based on average firing rates. It proposes a mechanism where cortical traveling waves, with their precise temporal structure, synchronize excitatory inputs and trigger inhibitory rebound, leading to backpropagating action potentials that activate spike-timing-dependent plasticity (STDP) in a millisecond-precision manner. Experimental and theoretical findings suggest this STDP-based process can form a temporary second-tier network that persists for hours, potentially supporting long-term working memory as a cognitive overlay on existing sensorimotor networks.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决对长达数小时的长时工作记忆机制的理解不足，这与基于平均放电率的快速感觉运动处理形成对比。它提出了一种机制，其中皮层行波以其精确的时间结构同步兴奋性输入并触发抑制性反弹，导致反向传播动作电位，从而以毫秒精度激活尖峰时序依赖可塑性（STDP）。实验和理论建模表明，这种STDP驱动的过程可以形成一个持续数小时的临时二级网络，可能作为认知覆盖层支持长时工作记忆，叠加在现有的感觉运动网络之上。</div>
</details>
</div>
<div class="card">
<div class="title">Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural Language Data</div>
<div class="meta-line">Authors: Paul Quinlan, Qingguo Li, Xiaodan Zhu</div>
<div class="meta-line">First: 2025-03-13T21:05:11+00:00 · Latest: 2026-01-22T17:37:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10883v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.10883v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are being rapidly deployed across many fields such as healthcare, finance, transportation, and energy, where time-series data are fundamental components. The current works are still limited in their ability to perform reasoning that involves both time-series and the corresponding textual content. We address this gap by introducing Chat-TS, a large language model (LLM) based framework designed to support reasoning over time series and textual data. Unlike traditional models, Chat-TS integrates time-series tokens into LLMs&#x27; vocabulary, enhancing its reasoning ability over both modalities without compromising core natural language capabilities. To support learning and evaluation, we contribute new datasets: the TS Instruct Training Dataset (pairing diverse time-series data with relevant text instructions and responses for instruction tuning), the TS Instruct Question and Answer (QA) Gold Dataset (multiple-choice questions to evaluate multimodal reasoning), and a TS Instruct Quantitative Probing Set (a small subset of TS Instruct QA reasoning tasks alongside math and decision-making questions for LLM evaluation). We design a training strategy to preserve the inherent reasoning capabilities of LLMs while augmenting them for time-series reasoning. Experiments show that Chat-TS achieves state-of-the-art performance in multimodal reasoning tasks by maintaining strong natural language proficiency while improving time-series reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Chat-TS：增强时间序列与自然语言数据的多模态推理能力</div>
<div class="mono" style="margin-top:8px">大语言模型正迅速应用于医疗、金融、交通和能源等多个领域，这些领域以时间序列数据为基础。现有研究在处理时间序列与对应文本内容的联合推理方面仍存在局限。为填补这一空白，我们提出了Chat-TS——一个基于大语言模型的框架，旨在支持对时间序列与文本数据的推理。与传统模型不同，Chat-TS将时间序列标记集成到大语言模型的词汇表中，在保持核心自然语言能力的同时，增强了对两种模态的推理能力。为支持学习与评估，我们贡献了新的数据集：TS Instruct训练数据集（将多样化时间序列数据与相关文本指令及响应配对，用于指令微调）、TS Instruct问答黄金数据集（通过多选题评估多模态推理），以及TS Instruct定量探测集（包含少量TS Instruct问答推理任务及数学与决策问题，用于大语言模型评估）。我们设计了训练策略，在保持大语言模型固有推理能力的同时增强其时间序列推理能力。实验表明，Chat-TS在保持强大自然语言能力的同时提升了时间序列推理，在多模态推理任务中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of large language models in performing joint reasoning over time-series data and corresponding textual content, which is crucial for fields like healthcare and finance. The proposed Chat-TS framework integrates time-series tokens into the LLM vocabulary through a novel training strategy that preserves core natural language capabilities while enhancing multimodal reasoning. Experimental results demonstrate that Chat-TS achieves state-of-the-art performance in multimodal reasoning tasks, maintaining strong natural language proficiency alongside improved time-series reasoning.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型在时间序列数据与对应文本内容联合推理方面的局限性，这在医疗、金融等领域应用中至关重要。提出的Chat-TS框架通过将时间序列标记集成到LLM词汇表中，采用一种保持核心自然语言能力同时增强多模态推理的训练策略。实验结果表明，Chat-TS在多模态推理任务中实现了最先进的性能，在保持强大自然语言能力的同时提升了时间序列推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">Mantis: A Foundation Model for Mechanistic Disease Forecasting</div>
<div class="meta-line">Authors: Carson Dudley, Reiden Magdaleno, Christopher Harding, Ananya Sharma, Emily Martin, Marisa Eisenberg</div>
<div class="meta-line">First: 2025-08-17T06:55:29+00:00 · Latest: 2026-01-22T17:34:42+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12260v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.12260v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Infectious disease forecasting in novel outbreaks or low-resource settings is hampered by the need for large disease and covariate data sets, bespoke training, and expert tuning, all of which can hinder rapid generation of forecasts for new settings. To help address these challenges, we developed Mantis, a foundation model trained entirely on mechanistic simulations, which enables out-of-the-box forecasting across diseases, regions, and outcomes, even in settings with limited historical data. We evaluated Mantis against 48 forecasting models across six diseases with diverse modes of transmission, assessing both point forecast accuracy (mean absolute error) and probabilistic performance (weighted interval score and coverage). Despite using no real-world data during training, Mantis achieved lower mean absolute error than all models in the CDC&#x27;s COVID-19 Forecast Hub when backtested on early pandemic forecasts which it had not previously seen. Across all other diseases tested, Mantis consistently ranked in the top two models across evaluation metrics. Mantis further generalized to diseases with transmission mechanisms not represented in its training data, demonstrating that it can capture fundamental contagion dynamics rather than memorizing disease-specific patterns. These capabilities illustrate that purely simulation-based foundation models such as Mantis can provide a practical foundation for disease forecasting: general-purpose, accurate, and deployable where traditional models struggle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mantis：一种用于机制性疾病预测的基础模型</div>
<div class="mono" style="margin-top:8px">在新发疫情或资源匮乏环境中，传染病预测常受限于对大规模疾病与协变量数据集的需求、定制化训练及专家调参，这些因素均会阻碍新环境下快速生成预测。为应对这些挑战，我们开发了Mantis——一个完全基于机制性模拟训练的基础模型，能够跨疾病、跨区域、跨预测指标实现开箱即用的预测，即使在历史数据有限的环境中也能运行。我们针对六种传播模式各异的疾病，将Mantis与48个预测模型进行比较，评估了点预测精度（平均绝对误差）和概率预测性能（加权区间评分与覆盖度）。尽管训练过程中未使用任何真实世界数据，在对未见过的早期疫情预测进行回溯测试时，Mantis的平均绝对误差低于美国疾控中心COVID-19预测中心的所有模型。在所有其他测试疾病中，Mantis在各项评估指标上均稳居前两名。该模型进一步泛化至训练数据中未包含传播机制的疾病，证明其能捕捉基础传染动力学规律而非记忆特定疾病模式。这些能力表明，如Mantis这类纯模拟驱动的基础模型可为疾病预测提供实用基础：在传统模型难以应对的场景中，实现通用、精准且可快速部署的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of rapid infectious disease forecasting in novel outbreaks or low-resource settings, where traditional models require extensive data and expert tuning. The authors developed Mantis, a foundation model trained exclusively on mechanistic simulations, enabling out-of-the-box forecasting across diseases, regions, and outcomes without real-world training data. Experimental evaluation against 48 models across six diseases showed Mantis achieved lower mean absolute error than all models in the CDC&#x27;s COVID-19 Forecast Hub on early pandemic backtests, and consistently ranked in the top two across other diseases, even generalizing to unseen transmission mechanisms.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决新型疫情或资源匮乏环境下传染病快速预测的难题，传统模型通常需要大量数据和定制化训练。作者开发了Mantis这一基础模型，完全基于机制模拟进行训练，使其能够在无需真实世界数据的情况下跨疾病、区域和结果生成预测。在对六种疾病的48个模型评估中，Mantis在早期疫情回溯测试中超越了美国疾控中心COVID-19预测中心的所有模型，并在其他疾病上始终位列前两名，甚至能泛化到训练数据中未包含的传播机制。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Watermark in the Latent Space of Generative Models</div>
<div class="meta-line">Authors: Sylvestre-Alvise Rebuffi, Tuan Tran, Valeriu Lacatusu, Pierre Fernandez, Tomáš Souček, Nikola Jovanović, Tom Sander, Hady Elsahar, Alexandre Mourachko</div>
<div class="meta-line">First: 2026-01-22T17:34:30+00:00 · Latest: 2026-01-22T17:34:30+00:00</div>
<div class="meta-line">Comments: Code and models are available at https://github.com/facebookresearch/distseal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16140v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16140v1">PDF</a> · <a href="https://github.com/facebookresearch/distseal">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在生成模型潜在空间中的水印学习</div>
<div class="mono" style="margin-top:8px">现有AI生成图像水印方法多采用像素空间的后处理技术，存在计算开销大且可能引入视觉伪影的问题。本研究探索潜在空间水印技术，提出适用于扩散模型与自回归模型的统一潜在水印框架DistSeal。该方法通过在生成模型潜在空间中训练后处理水印模型，证明潜在水印器可有效蒸馏至生成模型本体或潜在解码器中，实现模型内嵌水印。实验表明：相较于像素空间基线方法，所得潜在水印在保持竞争性鲁棒性的同时，具有相近的不可感知性，且速度提升最高达20倍。进一步研究发现，蒸馏潜在水印器的效果优于蒸馏像素空间水印器，提供了更高效、更鲁棒的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational overhead and visual artifacts of pixel-space watermarking for AI-generated images, this study proposes DistSeal, a latent space watermarking method applicable to diffusion and autoregressive models. The approach trains post-hoc watermarking models in the latent space of generative models and then distills them into the generative model or latent decoder for in-model watermarking. Experiments show that this method achieves competitive robustness and similar imperceptibility while offering up to 20x speedup over pixel-space baselines, with distillation in latent space proving more efficient and robust than in pixel space.</div>
<div class="mono" style="margin-top:8px">针对AI生成图像在像素空间水印方法存在的计算开销和视觉伪影问题，本研究提出了DistSeal方法，旨在生成模型（如扩散模型和自回归模型）的潜在空间中直接嵌入水印。该方法通过在潜在空间训练后处理水印模型，并将其蒸馏到生成模型或其解码器中，从而实现高效的内置水印。实验结果表明，这种潜在空间水印在保持竞争性鲁棒性和类似不可感知性的同时，相比像素空间基线实现了高达20倍的加速，并且潜在空间蒸馏比像素空间蒸馏效果更优、更鲁棒。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Prompt Evaluation for Educational Applications</div>
<div class="meta-line">Authors: Langdon Holmes, Adam Coscia, Scott Crossley, Joon Suh Choi, Wesley Morris</div>
<div class="meta-line">First: 2026-01-22T17:31:25+00:00 · Latest: 2026-01-22T17:31:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16134v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16134v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>教育应用中的大语言模型提示评估</div>
<div class="mono" style="margin-top:8px">随着大语言模型在教育应用中日渐普及，亟需基于实证的方法来设计和评估能产生个性化且符合教学目标的输出提示。本研究提出一种可推广的系统性提示评估方法，通过对结构化对话活动中LLM生成的后续问题进行分析来验证。研究设计并测试了六种提示模板，这些模板融合了成熟的提示工程模式，每种提示侧重不同的教学策略。通过适用于其他教育场景的锦标赛式评估框架对提示模板进行比较，该锦标赛采用Glicko2评分系统，由八位评委从格式、对话支持度、学习者适配性三个维度对问题组进行评价。数据来源于三个独立教育场景中的120组真实用户交互记录。结果显示，涉及策略性阅读的单一提示模板在成对比较中以81%至100%的胜率优于其他模板。该提示融合了角色设定与语境管理模块，旨在支持元认知学习策略（如自主学习）。本方法为教育技术研究者展示了如何系统评估并改进提示设计，推动教育应用从临时性提示工程向基于实证的提示开发范式转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing use of large language models (LLMs) in education and the need for evidence-based prompt design, this study develops a generalizable, systematic method for evaluating educational prompts. The method involves designing six prompt templates incorporating distinct pedagogical strategies and established engineering patterns, then comparing them using a tournament-style evaluation framework with the Glicko2 rating system, where eight judges assessed LLM-generated follow-up questions across three dimensions based on 120 authentic user interactions. Key experimental results show that a single prompt combining persona and context manager patterns to support metacognitive strategies significantly outperformed others, achieving win probabilities between 81% and 100% in pairwise comparisons.</div>
<div class="mono" style="margin-top:8px">本研究基于大语言模型在教育中日益普及以及需要基于证据设计提示的背景，提出了一种系统化、可推广的教育提示评估方法。该方法设计了六个融入不同教学策略和成熟工程模式的提示模板，并通过采用Glicko2评分系统的锦标赛式评估框架进行比较，由八位评委基于120个真实用户交互数据，从格式、对话支持和学习者适宜性三个维度评估大语言模型生成的后续问题。主要实验结果表明，一个专注于策略性阅读、结合了角色扮演和上下文管理器模式以支持元认知学习策略的提示模板，在成对比较中以81%至100%的胜率持续优于其他模板，这展示了系统化评估在改进教育技术提示设计方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">ViSymRe: Vision-guided Multimodal Symbolic Regression</div>
<div class="meta-line">Authors: Da Li, Junping Yin, Jin Xu, Xinxin Li, Juan Zhang</div>
<div class="meta-line">First: 2024-12-15T10:05:31+00:00 · Latest: 2026-01-22T17:29:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11139v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.11139v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSymRe：视觉引导的多模态符号回归</div>
<div class="mono" style="margin-top:8px">从观测数据集中提取简单数学表达式以描述复杂自然现象，是人工智能（AI）的核心目标之一，该领域称为符号回归（SR）。传统SR模型基于遗传编程（GP）或强化学习（RL），面临效率低下和过拟合等挑战。近期研究将SR与大型语言模型（LLMs）结合，通过从数百万数据集-表达式对中学习映射实现快速零样本推理。然而，由于输入与输出本质属于不同模态，此类模型常难以有效收敛。本文提出ViSymRe，一种视觉引导的多模态SR模型，通过引入第三种资源——表达式图——来弥合模态鸿沟。与传统多模态模型不同，ViSymRe训练从数据集中提取视觉信息（称为虚拟视觉），无需依赖表达式图的全局可用性，从而解决了视觉SR的关键挑战：推理时表达式图不可得。在多个主流基准上的评估结果表明，ViSymRe比当前最先进的仅使用数据集的基线模型具有更强竞争力。其预测的表达式不仅与数据集高度吻合，且结构简洁准确，实现了SR模型追求的核心目标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve symbolic regression (SR) for extracting mathematical expressions from data, addressing inefficiencies in genetic programming and reinforcement learning methods and the modality gap in large language model (LLM)-based approaches. The proposed ViSymRe model introduces a vision-guided multimodal method that uses expression graphs as an intermediate resource during training to bridge the modality gap; it learns to generate &#x27;virtual vision&#x27; from datasets without requiring expression graphs at inference time. Experimental evaluations on multiple benchmarks demonstrate that ViSymRe outperforms state-of-the-art dataset-only baselines, producing expressions that are accurate, simple, and structurally sound.</div>
<div class="mono" style="margin-top:8px">该研究针对符号回归（SR）的挑战展开，即从观测数据中发现简洁的数学表达式，指出传统的遗传规划和强化学习方法存在效率低和过拟合问题，而近期的大语言模型方法则受困于数值输入与符号输出之间的模态差异。提出的ViSymRe方法采用视觉引导的多模态架构，在训练中引入表达式图作为中间表示，但独特之处在于它学会了从数据集中提取“虚拟视觉”，而无需在推理时使用表达式图，从而弥合了模态差距。在多个主流基准上的实验评估表明，ViSymRe的性能优于最先进的仅使用数据集的基线方法，其预测的表达式在良好拟合数据的同时，保持了简洁性和结构准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Replicating Human Motivated Reasoning Studies with LLMs</div>
<div class="meta-line">Authors: Neeley Pate, Adiba Mahbub Proma, Hangfeng He, James N. Druckman, Daniel Molden, Gourab Ghoshal, Ehsan Hoque</div>
<div class="meta-line">First: 2026-01-22T17:29:07+00:00 · Latest: 2026-01-22T17:29:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16130v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motivated reasoning -- the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined -- has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expected human behavior. Furthermore, base LLM behavior across models shares some similarities, such as smaller standard deviations and inaccurate argument strength assessments. We emphasize the importance of these findings for researchers using LLMs to automate tasks such as survey data collection and argument assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型复现人类动机性推理研究</div>
<div class="mono" style="margin-top:8px">动机性推理——即个体在处理信息时可能受特定结论（无论其准确与否或是否预先确定）的驱动——作为人类心理现象已得到充分研究。然而，基础大语言模型是否会模仿这种动机性变化尚不明确。通过复现四项既有政治动机性推理研究，我们发现基础大语言模型的行为模式与预期的人类行为并不一致。此外，不同基础模型的行为表现出某些共性，例如标准差较小、论据强度评估失准等。我们强调这些发现对使用大语言模型自动化执行调查数据收集、论据评估等任务的研究者具有重要参考价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether base large language models (LLMs) exhibit motivated reasoning, a human cognitive bias where individuals process information to reach preferred conclusions. The researchers replicated four prior political motivated reasoning studies with base LLMs to compare their behavior against established human patterns. The experimental results show that base LLM behavior does not align with expected human motivated reasoning; instead, models display smaller standard deviations and inaccurate assessments of argument strength, with these patterns being consistent across different models. These findings highlight critical implications for using LLMs to automate tasks like survey data collection and argument evaluation, as their reasoning processes differ fundamentally from human motivational biases.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究基础大语言模型是否表现出动机性推理这一人类认知偏差，即个体为达成特定结论而处理信息的倾向。研究人员通过复现四项先前的政治动机性推理研究，使用基础大语言模型将其反应与已知的人类行为模式进行比较。实验结果表明，基础大语言模型的行为与预期的人类动机性推理并不一致；相反，模型表现出更小的标准差和对论证强度的不准确评估，这凸显了在自动化任务（如调查数据收集和论证评估）中使用大语言模型的显著局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging</div>
<div class="meta-line">Authors: Alphaeus Dmonte, Vidhi Gupta, Daniel J Perry, Mark Arehart</div>
<div class="meta-line">First: 2026-01-22T17:28:24+00:00 · Latest: 2026-01-22T17:28:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语言特定模型融合提升训练效率并降低维护成本</div>
<div class="mono" style="margin-top:8px">针对特定任务的多语言大语言模型（LLM）进行微调时，需使用包含所有目标语言示例的多语言数据集进行训练。若需用额外数据更新一个或多个支持语言，或添加新语言支持，则需重新训练模型，这可能导致计算效率低下并形成严重的维护瓶颈。近期关于多语言多任务模型融合的研究在提升质量方面展现出潜力，但其计算与维护效率尚未得到充分研究。本文首次从效率角度对该融合策略进行聚焦分析，并在三个独立任务中评估其表现。研究表明，该方法在保持质量相当的同时实现了显著的效率提升：融合策略可将初始训练时间减少高达50%。此外，在模型维护过程中，更新单一语言后重新融合，相比重新训练完整多语言模型，能降低超过60%的训练成本。我们在公开数据集和行业专有数据集上均验证了该方法的有效性，证实其不仅适用于学术场景，也能很好地满足工业应用需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational inefficiency and high maintenance costs of retraining multilingual large language models when updating or adding languages, which creates a bottleneck. The method involves merging language-specific models instead of fine-tuning a single multilingual model, analyzing this strategy across three tasks for efficiency. Experimental results show that this merging approach reduces initial training time by up to 50% and cuts training costs by over 60% for language updates, while maintaining quality parity on both public and proprietary datasets.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多语言大语言模型在更新语言或添加新语言支持时，因需要完整重新训练而导致的计算效率低下和维护瓶颈问题。其方法探索了语言特定模型合并策略，即先为不同语言分别训练模型再进行合并，以此替代联合多语言训练。在三个任务上的实验结果表明，该合并方法将初始训练时间减少了高达50%，并通过重新合并进行语言更新将训练成本降低了60%以上，同时保持了性能相当，这些结果在公开和专有的工业数据集上都得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs</div>
<div class="meta-line">Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang</div>
<div class="meta-line">First: 2025-11-17T11:45:41+00:00 · Latest: 2026-01-22T17:11:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13273v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioMotionBench：评估音频大语言模型中的听觉运动感知能力</div>
<div class="mono" style="margin-top:8px">大型音频-语言模型近期在语音识别、音频描述和听觉问答方面取得显著进展，但这些模型能否感知空间动态（尤其是声源运动）仍不明确。本研究揭示了当前音频-语言模型存在系统性运动感知缺陷。为探究此问题，我们提出了首个专门评估听觉运动理解的基准测试AudioMotionBench。该基准通过受控问答任务，评估音频-语言模型能否从双耳音频中推断移动声源的方向与轨迹。综合定量与定性分析表明，现有模型难以可靠识别运动线索或区分方向模式，平均准确率低于50%，暴露出听觉空间推理的根本性局限。本研究揭示了人类与模型在听觉空间推理上的本质差距，为未来增强音频-语言模型的空间认知提供了诊断工具与新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to assess whether Large Audio-Language Models (LALMs), despite their progress in tasks like speech recognition, can perceive the spatial dynamics of moving sound sources. The method introduces AudioMotionBench, a novel benchmark that uses controlled question-answering on binaural audio to evaluate models&#x27; ability to infer sound source direction and trajectory. Key experimental findings reveal a systematic deficit, with models struggling to recognize motion cues or distinguish directional patterns, achieving an average accuracy below 50%, which highlights a fundamental limitation in auditory spatial reasoning compared to humans.</div>
<div class="mono" style="margin-top:8px">当前的大型音频-语言模型在语音识别等任务上表现出色，但其感知空间动态（特别是声源运动）的能力尚不明确且可能存在缺陷。为系统评估该能力，研究者提出了AudioMotionBench，这是一个使用双耳音频、通过问答形式测试模型推断声源方向与轨迹能力的基准。综合分析表明，现有模型表现不佳，平均准确率低于50%，这揭示了其在听觉运动感知上的根本性局限以及与人类空间推理能力之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-22T17:01:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayesianVLA：基于潜在动作查询的视觉语言动作模型贝叶斯分解</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但常难以泛化至新指令或复杂多任务场景。我们发现在当前训练范式中存在一个关键缺陷：目标驱动的数据收集会导致数据集偏差。此类数据集中，仅凭视觉观察即可高度预测语言指令，导致指令与动作间的条件互信息消失——我们称之为“信息坍缩”。因此，模型退化为仅依赖视觉的策略，忽略语言约束并在分布外（OOD）场景中失效。为解决此问题，我们提出BayesianVLA——一种通过贝叶斯分解强制遵循指令的新框架。通过引入可学习的潜在动作查询，我们构建了双分支架构，分别估计仅视觉先验$p(a \mid v)$和语言条件后验$π(a \mid v, \ell)$。随后优化策略以最大化动作与指令间的条件点互信息（PMI）。该目标有效惩罚视觉捷径，同时奖励能显式解释语言命令的动作。无需新数据，BayesianVLA显著提升了泛化能力。在SimplerEnv和RoboCasa上的大量实验证明了其显著优势，包括在挑战性OOD基准SimplerEnv上提升11.3%，验证了本方法在动作中稳健锚定语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions become predictable from visual observations, leading to an Information Collapse that degrades models into vision-only policies. To counteract this, BayesianVLA introduces a Bayesian decomposition framework using learnable Latent Action Queries to separately model a vision-only prior and a language-conditioned posterior, optimizing the policy to maximize the conditional Pointwise Mutual Information between actions and instructions. Experiments on SimplerEnv and RoboCasa show the method substantially improves generalization without new data, achieving an 11.3% performance gain on an out-of-distribution benchmark.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差而泛化能力不足，其中语言指令仅从视觉上下文即可预测，导致信息坍缩并使模型退化为仅依赖视觉的策略。为解决该问题，提出的BayesianVLA框架通过引入可学习的潜在动作查询，构建了一个双分支架构来分别估计仅视觉先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息。在SimplerEnv和RoboCasa基准上的实验表明，该方法无需新数据即可显著提升分布外泛化能力，在具有挑战性的OOD基准上实现了11.3%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources</div>
<div class="meta-line">Authors: Marzieh Adeli Shamsabad, Hamed Ghodrati</div>
<div class="meta-line">First: 2026-01-22T16:55:48+00:00 · Latest: 2026-01-22T16:55:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16108v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16108v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态气候虚假信息检测：融合视觉语言模型与外部知识源</div>
<div class="mono" style="margin-top:8px">气候虚假信息已成为当今数字世界的重大挑战，尤其是在社交媒体上广泛传播的误导性图像和视频日益增多的情况下。这些虚假主张往往具有说服力且难以识别，可能延缓应对气候变化的行动。虽然视觉语言模型（VLMs）已被用于识别视觉虚假信息，但它们仅依赖于训练时可获取的知识，这限制了其处理近期事件或更新的能力。本文的主要目标是通过将VLMs与外部知识相结合来克服这一局限。通过检索最新信息（如反向图像搜索结果、在线事实核查和可信专家内容），该系统能更准确地评估图像及其主张是否真实、具有误导性、虚假或无法验证。该方法提升了模型处理现实世界气候虚假信息的能力，并在快速变化的信息环境中助力维护公众对科学的理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting climate disinformation in multimodal content, which is increasingly prevalent and convincing on social media, thereby hindering climate action. The proposed method enhances vision-language models by integrating them with external, up-to-date knowledge sources, such as reverse image search results, online fact-checks, and expert content, to assess the veracity of images and associated claims. Experimental findings demonstrate that this integration improves the model&#x27;s ability to accurately classify content as accurate, misleading, false, or unverifiable, enhancing robustness against real-world disinformation.</div>
<div class="mono" style="margin-top:8px">本研究针对多模态气候虚假信息检测的挑战，这类信息往往具有说服力且可能阻碍气候行动，旨在克服视觉语言模型依赖静态训练知识的局限。方法上，通过整合外部最新知识源，如反向图像搜索结果、在线事实核查和专家内容，来评估图像及其相关声明的真实性。实验结果表明，该方法能有效提升系统将内容准确分类为真实、误导性、虚假或无法验证的能力，从而增强了现实世界虚假信息的检测效果，并有助于维护公众对气候科学的理解。</div>
</details>
</div>
<div class="card">
<div class="title">TDFlow: Agentic Workflows for Test Driven Development</div>
<div class="meta-line">Authors: Kevin Han, Siddharth Maddikayala, Tim Knappe, Om Patel, Austen Liao, Amir Barati Farimani</div>
<div class="meta-line">First: 2025-10-27T18:44:59+00:00 · Latest: 2026-01-22T16:50:52+00:00</div>
<div class="meta-line">Comments: Published in the 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026 Main Conference)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23761v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23761v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce TDFlow, a novel test-driven agentic workflow that frames repository-scale software engineering as a test-resolution task, specifically designed to solve human-written tests. Given a set of tests, TDFlow repeatedly proposes, revises, and debugs repository-scale patches using precisely engineered sub-agents and tightly constrained tools. The workflow decomposes software engineering program repair into four components governed by respective sub-agents. This simple, forced decoupling of patch proposing, debugging, patch revision, and optional test generation (1) reduces long-context burden on any individual sub-agent, (2) focuses each sub-agent on specific, pre-defined sub-tasks, and (3) allows for specialized performance improvement on specific sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and 94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which were subsequently counted as failures. Furthermore, we show that the primary obstacle to human-level software engineering performance lies within writing successful reproduction tests. We envision a human-LLM interactive system powered by TDFlow where human developers write tests solved by LLM systems. Together, these results indicate that modern LLMs, when embedded in a narrowly engineered, test-driven workflow, already achieve human-level test resolution -- with the final frontier for fully autonomous repository repair being the accurate generation of valid reproduction tests.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TDFlow：面向测试驱动开发的智能体工作流</div>
<div class="mono" style="margin-top:8px">本文提出TDFlow——一种新颖的测试驱动智能体工作流，将仓库级软件工程构建为测试解析任务，专门用于解决人工编写的测试用例。给定测试集后，TDFlow通过精密设计的子智能体与严格约束的工具，持续提出、修订和调试仓库级代码补丁。该工作流将软件工程程序修复分解为四个由独立子智能体管控的组件。这种对补丁提出、调试、补丁修订及可选测试生成的强制解耦设计具有三重优势：（1）减轻单个子智能体的长上下文负担；（2）使各子智能体专注于预定义的特定子任务；（3）支持针对特定子任务的专项性能提升。在人工编写测试的场景下，TDFlow在SWE-Bench Lite上达到88.8%通过率（较次优系统绝对提升27.8%），在SWE-Bench Verified上达到94.3%通过率。对SWE-Bench Lite与Verified中800次TDFlow运行的手动检查仅发现7例测试篡改行为（已计为失败案例）。研究进一步表明，实现人类级软件工程性能的主要障碍在于编写有效的复现测试。我们构想基于TDFlow构建人机交互系统，由开发者编写测试用例并由大语言模型系统求解。这些结果表明：当现代大语言模型嵌入精心设计的测试驱动工作流时，已能实现人类级的测试解析能力——而实现完全自主仓库修复的最后壁垒在于准确生成有效的复现测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of automating repository-scale software engineering by framing it as a test-resolution task, motivated by the need to effectively leverage large language models (LLMs) for complex program repair. The method introduces TDFlow, an agentic workflow that decomposes the repair process into four specialized sub-agents for proposing, debugging, revising patches, and optionally generating tests, thereby reducing long-context burdens and focusing each agent on specific sub-tasks. Experimental results on SWE-Bench Lite show TDFlow achieves an 88.8% pass rate with human-written tests, a 27.8% absolute improvement over the next best system, and only 7 instances of test hacking were found in 800 runs, indicating that the primary remaining obstacle for fully autonomous repair is the accurate generation of valid reproduction tests.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过将仓库级软件工程构建为测试解决任务，以应对自动化复杂程序修复的挑战，其动机是有效利用大语言模型（LLM）进行大规模代码维护。方法上提出了TDFlow，一种智能体工作流，它将修复过程分解为四个子任务——补丁提议、调试、修订和可选的测试生成——每个子任务由专门设计的子智能体处理，并配合严格约束的工具，以减轻长上下文负担并提升任务专注度。在SWE-Bench上的实验结果表明，TDFlow在Lite数据集上达到了88.8%的通过率（相比次优系统绝对提升27.8%），在Verified数据集上达到94.3%，且仅观察到极少的测试作弊案例，这表明当提供人工编写的测试时，现代LLM在此工程化工作流中已能实现人类水平的测试解决能力。</div>
</details>
</div>
<div class="card">
<div class="title">Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals</div>
<div class="meta-line">Authors: Saar Cohen</div>
<div class="meta-line">First: 2026-01-22T16:42:05+00:00 · Latest: 2026-01-22T16:42:05+00:00</div>
<div class="meta-line">Comments: To Appear in the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16091v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16091v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point&#x27;s location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points&#x27; locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机到达在线非质心聚类中的延迟分配</div>
<div class="mono" style="margin-top:8px">聚类是一个基础性问题，旨在将一组元素（如智能体或数据点）划分为若干簇，使得同一簇内的元素彼此间距离小于与其他簇元素间的距离。本文提出一种研究带延迟的在线非质心聚类的新框架：元素作为有限度量空间中的点逐个到达，需被分配至某个簇，但分配不必立即执行。具体而言，每个点到达时其位置被揭示，在线算法必须不可撤销地将其分配至现有簇，或创建仅包含该点的新簇。但我们允许决策以延迟成本为代价进行推迟，而非遵循常见的到达即决策假设。这带来关键挑战：目标是最小化各簇内点间总距离成本与延迟分配产生的总延迟成本。在经典最坏情况到达模型（点以任意顺序到达）中，任何算法的竞争比均无法优于点数的亚对数级别。为突破这一强不可能性，我们聚焦于随机到达模型——点的位置随时间推移从有限度量空间上未知且固定的概率分布中独立抽取。我们为超越最坏情况对抗性假设提供了可能：设计了一种常数竞争算法，即随着点数增长，输出聚类与最优离线聚类的期望总成本之比受常数约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of online non-centroid clustering where immediate assignment of arriving points is not required, allowing for delayed decisions at a cost, which aims to balance clustering quality with delay penalties. The method introduces a stochastic arrival model where points are drawn independently from an unknown distribution over a finite metric space, and it devises an algorithm that makes irrevocable assignment decisions, either to existing clusters or by creating new ones, while permitting postponements. Key experimental findings show that, in contrast to the poor competitive ratios under worst-case arrivals, the algorithm achieves a constant competitive ratio as the number of points increases, meaning the expected total cost of its clustering is bounded by a constant factor relative to an optimal offline solution.</div>
<div class="mono" style="margin-top:8px">本研究针对在线非质心聚类中无需立即分配到达点、允许以成本延迟决策的挑战，旨在平衡聚类质量与延迟惩罚。方法引入了随机到达模型，其中点从未知分布中独立抽取，并设计了一种可推迟分配的在线算法。主要实验结果表明，与最坏情况到达下的较差竞争比相比，该算法实现了常数竞争比，即随着点数增加，其期望总成本与最优离线聚类相比在一个常数因子内。</div>
</details>
</div>
<div class="card">
<div class="title">Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics</div>
<div class="meta-line">Authors: Sukesh Subaharan</div>
<div class="meta-line">First: 2026-01-22T16:34:05+00:00 · Latest: 2026-01-22T16:34:05+00:00</div>
<div class="meta-line">Comments: Supplementary materials can be found here: https://github.com/drsukeshs/agent-behavior-ext-dynamics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16087v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16087v1">PDF</a> · <a href="https://github.com/drsukeshs/agent-behavior-ext-dynamics">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过显式状态动态控制语言模型智能体的长程行为</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）智能体在长时间交互中常出现语气与角色特征的突变，这反映了缺乏显式时间结构来调控智能体层面的状态。现有研究多关注轮次层面的情感分析或静态情绪分类，而显式情感动态在塑造长程智能体行为中的作用尚未充分探索。本研究探讨对外部情感状态施加动态结构能否在多轮对话中实现时间连贯性与可控恢复。我们引入一个智能体层面的情感子系统，该子系统在语言模型外部维持连续的效价-唤醒-支配（VAD）状态，并通过一阶与二阶更新规则进行调控。瞬时情感信号通过固定的无记忆估计器提取，并通过指数平滑或基于动量的动态机制进行时间积分。生成过程将整合所得情感状态，且不修改模型参数。采用固定的25轮对话协议，我们比较了无状态、一阶和二阶情感动态机制。无状态智能体无法展现连贯轨迹或恢复能力，而状态持续性支持延迟响应与可靠恢复。二阶动态机制引入的情感惯性与滞后效应随动量增强，揭示了稳定性与响应性之间的权衡关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the issue of abrupt shifts in tone and persona exhibited by large language model (LLM) agents during extended interactions, which stems from a lack of explicit temporal structure governing the agent&#x27;s state. To impose temporal coherence and enable controlled recovery, the method introduces an external affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state, updated via first- or second-order dynamics and integrated using techniques like exponential smoothing, with the state then injected into the LLM&#x27;s generation process without modifying its parameters. Experimental results from a fixed 25-turn dialogue protocol show that stateless agents fail to achieve coherent trajectories, while state persistence allows for delayed responses and reliable recovery, and second-order dynamics introduce affective inertia and hysteresis, revealing a trade-off between stability and responsiveness.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）智能体在长时间交互中表现出语气和角色突然转变的问题，这源于缺乏明确的时序结构来管理智能体层面的状态。方法引入了一个外部情感子系统，用于维护一个连续的效价-唤醒-支配（VAD）状态，该状态通过一阶或二阶动态规则进行更新，并采用指数平滑等技术进行时间积分，然后将该状态注入生成过程而不修改LLM参数。在固定的25轮对话协议中的实验结果表明，无状态智能体无法实现连贯的轨迹或恢复，而状态持久性则能实现延迟响应和可靠的恢复，二阶动态引入了情感惯性和滞后效应，揭示了稳定性与响应性之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Probably Approximately Correct Maximum A Posteriori Inference</div>
<div class="meta-line">Authors: Matthew Shorvon, Frederik Mallmann-Trenn, David S. Watson</div>
<div class="meta-line">First: 2026-01-22T16:28:01+00:00 · Latest: 2026-01-22T16:28:01+00:00</div>
<div class="meta-line">Comments: 7 pages main text, 16 total, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>概率近似正确最大后验推断</div>
<div class="mono" style="margin-top:8px">计算分布的条件众数，即通常所说的$\mathit{最大后验}$（MAP）赋值，是概率推断中的一项基础任务。然而，MAP估计通常难以精确求解，即便在许多常见的结构约束和近似方案下依然困难。本文引入了用于MAP推断的$\mathit{概率近似正确}$（PAC）算法，这些算法能在可变或固定的计算预算下提供可证明的最优解。我们利用可从有限样本中估计的信息论度量，刻画了PAC-MAP的可处理性条件。所提出的PAC-MAP求解器通过具有适当架构的概率电路高效实现。我们开发的随机化策略既可独立用作MAP推断技术，也可用于改进常见启发式方法，从而为其解提供严格的理论保证。实验在一系列基准测试中验证了本方法的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the intractability of maximum a posteriori (MAP) inference, a core problem in probabilistic reasoning that remains difficult even with common approximations. The method introduces probably approximately correct (PAC) algorithms for MAP inference, which guarantee optimal solutions within computational budgets, using information-theoretic measures estimated from samples and implemented efficiently via probabilistic circuits. Experimental results across benchmarks demonstrate the effectiveness of these solvers in providing rigorous guarantees and improving upon existing heuristics.</div>
<div class="mono" style="margin-top:8px">该研究针对最大后验概率推断这一概率推理核心问题的难解性，即使在常见近似下仍具挑战。方法引入了近似正确概率算法进行MAP推断，在计算预算内提供可证明的最优解，利用从样本估计的信息论度量来刻画可处理性条件，并通过概率电路高效实现。在多个基准测试中的实验结果表明该方法的有效性，既能作为独立技术使用，也能增强现有启发式方法并提供严格保证。</div>
</details>
</div>
<div class="card">
<div class="title">Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface</div>
<div class="meta-line">Authors: Paige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy, Gidey Gezae, Dante Conway, Abraham Glasser, Raja Kushalnagar, Christian Vogler</div>
<div class="meta-line">First: 2026-01-21T17:33:00+00:00 · Latest: 2026-01-22T16:01:25+00:00</div>
<div class="meta-line">Comments: Accepted for publication in ACM CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15209v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15209v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa&#x27;s automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered &quot;task prompter,&quot; which integrated the user&#x27;s history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>听障人士对智能个人助理的访问：语音交互方案与基于大语言模型的触控界面对比</div>
<div class="mono" style="margin-top:8px">本研究探讨了在日常交流中能够使用语音的聋哑及听障人士对智能个人助理的可访问性。由于智能个人助理无法理解包括聋人口音在内的多样化口音，导致非手语使用且依赖口语的听障群体难以有效使用。通过混合方法研究，我们利用Echo Show设备，比较了以下三种交互方式的可用性：基于英语口语的自然语言输入配合Alexa自动语音识别、由训练有素的协助者复述指令的“绿野仙踪”模拟设置，以及基于大语言模型的触控界面。触控方法通过大语言模型驱动的“任务提示器”进行导航，该提示器整合用户历史与智能环境以推荐情境适配的指令。定量结果显示，两种英语口语条件与基于大语言模型的触控界面之间无显著差异；定性结果则显示用户对不同方法的可用性评价存在差异。最终，实现智能个人助理原生支持对聋人口音的鲁棒识别至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the accessibility gap in intelligent personal assistants (IPAs) for deaf and hard of hearing (DHH) individuals who use spoken language, as current IPAs often fail to understand diverse accents including deaf speech. The study compared three input methods on an Echo Show: standard voice input using Alexa&#x27;s automatic speech recognition, a Wizard-of-Oz condition where a facilitator re-spoke commands, and a novel touch interface powered by a large language model (LLM) that suggested contextually-appropriate commands based on user history and environment. Experimental results indicated no significant quantitative performance differences between the two spoken conditions and the LLM-assisted touch method, though qualitative feedback revealed varied user preferences regarding usability, underscoring the need for IPAs to natively and robustly recognize deaf-accented speech.</div>
<div class="mono" style="margin-top:8px">本研究针对智能个人助手（IPA）对于能说话但可能带有口音或聋人语音的聋哑及听力障碍（DHH）人群的可访问性问题，因为标准自动语音识别（ASR）通常无法理解这类语音。研究者在Echo Show上比较了三种输入方法：标准的Alexa ASR、由协助者复述命令的“绿野仙踪”式设置，以及一种由大型语言模型（LLM）驱动的创新触摸界面，该界面能根据用户历史和环境推荐情境相关的命令。定量结果显示，两种语音输入条件与LLM辅助的触摸方法在性能上没有显著差异，而定性反馈则表明用户偏好存在多样性。研究最终强调，IPA必须实现对聋人语音的鲁棒原生识别。</div>
</details>
</div>
<div class="card">
<div class="title">Words to Describe What I&#x27;m Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning</div>
<div class="meta-line">Authors: Kellie Yu Hui Sim, Pin Sym Foong, Chenyu Zhao, Melanie Yi Ning Quek, Swarangi Subodh Mehta, Kenny Tsu Wei Choo</div>
<div class="meta-line">First: 2025-12-12T04:39:34+00:00 · Latest: 2026-01-22T15:58:47+00:00</div>
<div class="meta-line">Comments: Accepted at CHI 2026. 34 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11276v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11276v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Loss of decisional capacity, coupled with the increasing absence of reliable human proxies, raises urgent questions about how individuals&#x27; values can be represented in Advance Care Planning (ACP). To probe this fraught design space of high-risk, high-subjectivity decision support, we built an experience prototype (\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal ACP proxy. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings show a surprising 86.7\% agreement with \acpagent{}, arguing for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We propose that the key areas of future risk that must be addressed are the moderation of users&#x27; expectations and designing accountability and oversight over agent deployment and cutoffs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>描述我内心感受的词汇：探索人工智能代理在预先护理规划中高主观性决策的潜力</div>
<div class="mono" style="margin-top:8px">决策能力的丧失，加上可靠人类代理日益缺失，引发了关于个人价值观如何在预先护理规划中得以体现的紧迫问题。为探索这一高风险、高主观性决策支持的设计领域，我们构建了一个体验原型（ACP代理），并邀请4场工作坊的15名参与者训练其成为个人ACP代理。通过分析参与者的应对策略与功能需求，我们将结果映射至代理自主性与人类控制度的坐标轴。研究发现，参与者对ACP代理的认同度高达86.7%，这揭示了人工智能在ACP中可能扮演的新角色——作为个人权益倡导者，随时间推移建立双向理解。我们提出未来必须解决的核心风险领域包括：调节用户预期，以及设计代理部署与终止的问责与监督机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of representing individuals&#x27; values in Advance Care Planning when decisional capacity is lost and reliable human proxies are unavailable. The authors developed an experience prototype called ACPagent and conducted four workshops with 15 participants to explore how they would train it to act as a personal ACP proxy. Analysis revealed that 86.7% of participants agreed with the agent&#x27;s decisions, suggesting AI could serve as personal advocates in ACP by building mutual understanding over time, though future work must address moderating user expectations and designing accountability mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究针对在丧失决策能力且缺乏可靠人类代理的情况下，如何在预先护理计划中代表个人价值观的挑战。为探索这一高风险、高主观性的设计空间，作者开发了一个名为acpagent的体验原型，并举办了四场工作坊，让15名参与者训练该代理作为其个人ACP代理；通过分析参与者的应对策略和功能请求，并将其映射到代理自主性与人类控制的维度上。主要实验结果显示，参与者对acpagent决策的同意率高达86.7%，这表明AI在ACP中可能扮演个人倡导者的新角色，通过时间建立相互理解，而未来需解决的风险包括调节用户期望，以及设计代理部署和终止的问责与监督机制。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
