<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-07 03:45</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260207_0345</div>
    <div class="row"><div class="card">
<div class="title">EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Alan Yuille</div>
<div class="meta-line">Venue: Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pages 649-659</div>
<div class="meta-line">First: 2025-02-07T07:07:04+00:00 · Latest: 2026-02-05T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04700v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.04700v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenLoRAx：通过回收适配器寻找主成分子空间以实现资源高效适应与推理</div>
<div class="mono" style="margin-top:8px">大模型的快速发展因其高昂计算成本引发了环境影响与可访问性公平的担忧。低秩适配器（LoRA）为大模型微调提供了轻量级解决方案，催生了大量面向不同领域的公开适配器。我们提出：能否利用这些预训练适配器进一步简化新任务适应过程并应对相关挑战？本文介绍EigenLoRAx——一种参数高效的微调方法，通过回收现有适配器构建与其共享领域知识对齐的主成分子空间，并可在低资源场景中通过正交基向量进行扩展。该方法仅需学习子空间主成分上的轻量级系数即可快速适应新任务，无需微调完整适配器。EigenLoRAx显著减少了参数量和内存需求，提升了训练与推理效率。实验表明，该方法在多样化领域和任务中均表现优异，为边缘计算应用、个性化部署及资源受限环境下大模型的公平部署提供了可扩展方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid expansion of large models raises concerns about their computational cost and environmental impact, motivating the search for more efficient adaptation methods. This work introduces EigenLoRAx, a parameter-efficient finetuning approach that recycles a collection of existing Low-Rank Adapters (LoRA) to construct a principal subspace capturing shared domain knowledge, which can be expanded with orthogonal basis vectors when data is scarce. Adaptation to a new task is achieved by learning only lightweight coefficients on this subspace&#x27;s principal components, avoiding the need to finetune entire adapters. Experiments show the method requires significantly fewer parameters and memory for both training and inference while maintaining strong performance across diverse domains and tasks, offering a scalable solution for resource-constrained deployment.</div>
<div class="mono" style="margin-top:8px">大型模型的快速扩张引发了对其计算成本和环境影响的担忧，促使研究者寻求更高效的适配方法。本研究提出了EigenLoRAx，一种参数高效的微调方法，它通过回收一组现有的低秩适配器（LoRA）来构建一个捕捉共享领域知识的主成分子空间，在数据稀缺时可用正交基向量进行扩展。该方法通过仅在该子空间的主成分上学习轻量级系数来适应新任务，从而避免了微调整个新适配器的需求。实验结果表明，该方法在多种领域和任务上均表现出色，同时所需参数和内存显著减少，提升了资源受限环境下训练和推理的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Shared LoRA Subspaces for almost Strict Continual Learning</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>共享LoRA子空间实现近乎严格的持续学习</div>
<div class="mono" style="margin-top:8px">将大型预训练模型高效且持续地适应新任务对实际部署至关重要，但由于灾难性遗忘和重新训练的高成本，这仍具挑战性。虽然低秩适应（LoRA）等参数高效调优方法降低了计算需求，但它们缺乏严格的持续学习和知识整合机制，且不依赖数据回放或多个适配器。我们提出Share方法，这是一种参数高效的持续微调新方法，它学习并动态更新一个共享的低秩子空间，实现跨多任务和多模态的无缝适应。Share构建一个基础子空间，从过往任务中提取核心知识，并通过识别关键子空间方向逐步整合新信息。每个新任务的知识都被纳入这个不断演化的子空间，促进前向知识转移，同时最小化灾难性干扰。与传统LoRA方法相比，该方法实现了高达100倍的参数减少和281倍的内存节省，性能与联合训练模型相当。单个Share模型可替代数百个任务专用LoRA适配器，支持可扩展的异步持续学习。在图像分类、自然语言理解、3D姿态估计和文本到图像生成等任务上的实验验证了其有效性，使Share成为大规模AI系统中实用且可扩展的终身学习解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting large pretrained models to new tasks continually and efficiently, which is hindered by catastrophic forgetting and high computational costs. The proposed method, Share, introduces a parameter-efficient continual finetuning approach that learns and dynamically updates a single, shared low-rank subspace, extracting core knowledge from past tasks and incrementally integrating new information to enable forward knowledge transfer while minimizing interference. Experimental results demonstrate that Share achieves up to 100x parameter reduction and 281x memory savings compared to traditional LoRA methods, with performance comparable to jointly trained models, and validates its effectiveness across tasks including image classification, natural language understanding, 3D pose estimation, and text-to-image generation.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决大型预训练模型在持续学习场景中适应新任务时面临的灾难性遗忘和高计算成本问题，因为现有参数高效方法如LoRA缺乏严格的持续学习能力。提出的方法Share通过学习和动态更新一个共享的低秩子空间，从过去任务中提取核心知识，并通过识别关键子空间方向逐步整合新信息，从而实现前向知识传递并最小化干扰。实验结果表明，Share相比传统LoRA方法实现了高达100倍的参数减少和281倍的内存节省，性能与联合训练模型相当，并在图像分类、自然语言理解、3D姿态估计和文本到图像生成等任务中验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</div>
<div class="meta-line">Authors: Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao</div>
<div class="meta-line">First: 2026-02-05T18:59:51+00:00 · Latest: 2026-02-05T18:59:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06039v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06039v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager&#x27;s round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DyTopo：基于语义匹配的多智能体推理动态拓扑路由方法</div>
<div class="mono" style="margin-top:8px">基于提示大语言模型构建的多智能体系统能够提升多轮推理能力，但现有流程大多依赖固定、全轨迹的通信模式，难以匹配迭代问题求解中不同阶段的需求。本文提出DyTopo——一种管理者引导的多智能体框架，在每轮推理中动态重构稀疏有向通信图。各智能体根据管理者的轮次目标生成轻量级自然语言查询（需求）与关键信息（供给）描述符；DyTopo通过嵌入这些描述符并进行语义匹配，仅沿推导出的边路由私有消息。在代码生成与数学推理基准测试及四种大语言模型架构上的实验表明，DyTopo始终优于最强基线（平均提升6.2%）。除准确性外，DyTopo通过演化图生成可解释的协调轨迹，支持对跨轮次通信路径重构过程的定性分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing multi-agent reasoning systems often use fixed communication patterns that fail to adapt to the changing needs of iterative problem-solving. To address this, DyTopo introduces a manager-guided framework that dynamically reconstructs a sparse directed communication graph each round: agents generate natural-language need and offer descriptors, which are semantically matched to route private messages along induced edges. Experiments on code generation and mathematical reasoning benchmarks across four LLM backbones show DyTopo consistently outperforms the strongest baseline by an average of +6.2% in accuracy, while also providing interpretable coordination traces through evolving communication graphs.</div>
<div class="mono" style="margin-top:8px">本研究针对多智能体推理系统中固定通信模式难以适应迭代问题求解动态需求的局限性。提出的DyTopo框架采用管理者引导的方法，每轮动态重构稀疏有向通信图；智能体生成自然语言查询和关键描述符，通过语义匹配将私有消息沿诱导边路由。在代码生成和数学推理基准测试中，使用四种大语言模型骨干的实验结果表明，DyTopo平均比最强基线提升6.2分，同时通过演化的通信图提供了可解释的协调轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</div>
<div class="meta-line">Authors: Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:45+00:00 · Latest: 2026-02-05T18:59:45+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://comm-cp.github.io/">Project1</a> · <a href="https://comm-cp.github.io">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CommCP：基于大语言模型与置信预测的高效多智能体协同通信框架</div>
<div class="mono" style="margin-top:8px">为完成人类以自然语言下达的指令，机器人需解析命令、生成并回答场景理解相关问题，进而操控目标物体。实际部署常需多个具备不同操控能力的异构机器人协同处理各类任务。除专业操控技能外，有效的信息收集对任务完成至关重要。针对该问题，我们将完全协作环境中的信息收集过程形式化为一个尚未充分探索的多智能体多任务具身问答问题——这是经典具身问答任务的新拓展，其中高效通信对避免冗余协作至关重要。为此，我们提出CommCP：一种专为MM-EQA设计的新型基于大语言模型的去中心化通信框架。该框架采用置信预测校准生成信息，最大限度减少接收方干扰并提升通信可靠性。为评估框架性能，我们构建了包含多样化照片级真实家庭场景与具身问题的MM-EQA基准测试。实验表明，CommCP较基线方法显著提升了任务成功率和探索效率。实验视频、代码及数据集详见项目网站：https://comm-cp.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of coordinating multiple heterogeneous robots to complete natural language assignments in household environments, where effective information gathering and communication are critical. The authors formalize this as a Multi-Agent Multi-task Embodied Question Answering (MM-EQA) problem and propose CommCP, a decentralized LLM-based communication framework that uses conformal prediction to calibrate generated messages, reducing receiver distractions and improving reliability. Experiments on a new photo-realistic MM-EQA benchmark show that CommCP significantly improves task success rates and exploration efficiency compared to baseline methods.</div>
<div class="mono" style="margin-top:8px">为使多个异构机器人能在现实场景中协作完成自然语言指令，本研究将信息收集过程形式化为一个多智能体多任务具身问答问题，其中高效通信至关重要。所提出的方法CommCP是一个去中心化的通信框架，它利用大语言模型生成消息，并采用保形预测对这些消息进行校准，以减少接收方的干扰并提升通信可靠性。在一个包含多样化、照片级真实家庭场景的新MM-EQA基准测试上的实验结果表明，与基线方法相比，CommCP显著提高了任务成功率和探索效率。</div>
</details>
</div>
<div class="card">
<div class="title">Language Models and Logic Programs for Trustworthy Tax Reasoning</div>
<div class="meta-line">Authors: William Jurayj, Nils Holzenberger, Benjamin Van Durme</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-28T17:55:07+00:00 · Latest: 2026-02-05T18:58:31+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.21051v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.21051v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">According to the United States Internal Revenue Service, ``the average American spends $\$270$ and 13 hours filing their taxes&#x27;&#x27;. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the effectiveness of applying semantic parsing methods to statutory reasoning, and show promising economic feasibility of neuro-symbolic architectures for increasing access to reliable tax assistance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型与逻辑程序在可信税务推理中的应用</div>
<div class="mono" style="margin-top:8px">根据美国国税局数据，“美国人平均花费270美元和13小时申报税务”。即使在美国之外，税务申报也需要复杂的推理过程，涉及重叠规则的应用与数值计算。由于错误可能导致高昂罚款，任何自动化系统都必须具备高准确性与可审计性，这使得现代大语言模型（LLMs）难以胜任此任务。我们提出一种将LLMs与符号求解器结合计算税务义务的方法。在具有挑战性的法定推理评估（SARA）数据集上对该系统的变体进行评估，并提出一种基于实际税务错误处罚的新型部署成本估算方法。我们进一步证明：通过将明文规则预先转化为形式逻辑程序，并结合智能检索的形式化案例表征示例，可显著提升任务性能，并将成本降至远低于实际平均水平。研究结果展示了语义解析方法在法定推理中的有效性，并证明了神经符号架构在提升可靠税务援助可及性方面具有经济可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tax filing requires complex reasoning with high accuracy demands, making pure large language models (LLMs) unsuitable due to error risks. This work integrates LLMs with a symbolic solver to calculate tax obligations, employing semantic parsing to translate tax rules into formal logic programs and using retrieved exemplars for case representation. On the StAtutory Reasoning Assessment (SARA) dataset, this neuro-symbolic approach significantly improves performance and reduces estimated error costs well below real-world averages, demonstrating both technical effectiveness and economic feasibility for trustworthy tax assistance.</div>
<div class="mono" style="margin-top:8px">针对报税成本高、规则复杂且需要精确应用与计算的问题，本研究指出大型语言模型（LLM）在此高风险任务中的不足，提出将其与符号求解器集成的方法。该方法将明文税法规则转化为形式逻辑程序，并利用智能检索的示例来指导案例表示，从而构建一个用于计算税务义务的神经符号系统。在法定推理评估（SARA）数据集上的实验结果表明，该方法显著提升了性能，并且基于真实世界罚款率的新型成本估算显示，其错误预期成本远低于人类平均报税支出，证明了该技术在提供可靠税务援助方面兼具技术有效性和经济可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</div>
<div class="meta-line">Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang</div>
<div class="meta-line">First: 2026-02-05T18:57:09+00:00 · Latest: 2026-02-05T18:57:09+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/BudgetMem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06025v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/BudgetMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向运行时智能体内存的查询感知预算层级路由学习</div>
<div class="mono" style="margin-top:8px">内存对于超越单上下文窗口运行的大型语言模型（LLM）智能体日益关键，但现有系统多依赖离线、查询无关的内存构建方式，效率低下且可能丢失查询关键信息。尽管运行时内存利用是自然替代方案，先前研究常伴随显著开销，且对性能-成本权衡缺乏显式控制。本文提出\textbf{BudgetMem}——一种支持显式查询感知性能-成本控制的运行时智能体内存框架。该框架将内存处理构建为多模块结构，每个模块提供三种预算层级（即\textsc{低}/\textsc{中}/\textsc{高}）。轻量级路由器通过强化学习训练的紧凑神经策略，执行跨模块预算层级路由以平衡任务性能与内存构建成本。以BudgetMem为统一测试平台，我们研究实现预算层级的三种互补策略：实现方式（方法复杂度）、推理机制（推断行为）与容量配置（模块模型规模）。在LoCoMo、LongMemEval和HotpotQA数据集上，BudgetMem在优先性能（即高预算设置）时超越强基线模型，并在严格预算约束下提供更优的精度-成本边界。进一步分析揭示了不同层级策略的优劣特性，阐明了各维度在多变预算机制下的最佳权衡条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing memory systems for LLM agents, which often rely on offline, query-agnostic construction that can be inefficient and discard critical information, while runtime alternatives incur high overhead with limited control over performance-cost trade-offs. The method introduces BudgetMem, a runtime agent memory framework that structures memory as modules each offered in three budget tiers (Low/Mid/High), using a lightweight neural router trained with reinforcement learning to perform query-aware budget-tier routing across modules for explicit performance-cost control. Key experimental results on LoCoMo, LongMemEval, and HotpotQA show that BudgetMem outperforms strong baselines in high-budget settings and achieves better accuracy-cost trade-offs under tight budgets, with analysis revealing the distinct strengths of different tiering strategies—implementation, reasoning, and capacity—under varying budget regimes.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型智能体中离线、查询无关内存系统的低效问题，提出了BudgetMem框架，用于在运行时实现显式的、查询感知的性能与成本控制。该方法将内存组织为具有低、中、高三个预算层级的模块，并采用强化学习训练的轻量级神经路由策略进行资源分配。在LoCoMo、LongMemEval和HotpotQA数据集上的实验表明，BudgetMem在优先性能（即高预算设置）时优于基线方法，并在严格预算下实现了更优的精度-成本权衡；分析还揭示了实现、推理和容量等不同层级策略在不同预算条件下的优势与适用场景。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Event-Based Shooter Models from Virtual Reality Experiments</div>
<div class="meta-line">Authors: Christopher A. McClurg, Alan R. Wagner</div>
<div class="meta-line">First: 2026-02-05T18:56:49+00:00 · Latest: 2026-02-05T18:56:49+00:00</div>
<div class="meta-line">Comments: Preprint under review for conference publication. 9 pages, 4 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06023v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于虚拟现实实验的事件型枪手模型学习</div>
<div class="mono" style="margin-top:8px">虚拟现实（VR）已成为评估高风险场景（如校园枪击事件）中学校安全措施的有力工具，具有实验可控性和高行为保真度。然而，在VR中评估新干预措施需要为每种条件招募新的参与者群体，使得大规模或迭代评估变得困难。这些限制在学习有效干预策略时尤为突出，因为此类策略通常需要大量训练样本。为解决这一挑战，我们开发了一种数据驱动的离散事件模拟器（DES），该模型将枪手移动和区域内行动建模为从VR研究参与者行为中学习的随机过程。我们利用该模拟器检验基于机器人的枪手干预策略效果。该DES在验证能够复现关键实证模式后，即可实现干预策略的可扩展评估与学习，而这类策略直接通过人类受试者训练是不可行的。总体而言，本研究展示了一种高至中保真度的模拟工作流程，为开发和评估自主化校园安全干预措施提供了可扩展的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to overcome the scalability limitations of using virtual reality (VR) experiments for evaluating school security interventions, as recruiting new human participants for each condition is costly and hinders iterative strategy development. The authors propose a data-driven discrete-event simulator (DES) that models a shooter&#x27;s movement and actions as stochastic processes, learned directly from behavioral data collected in VR studies. Experimental validation shows the simulator reproduces key empirical patterns, enabling the scalable evaluation and learning of intervention strategies, such as a robot-based intervention, which would be infeasible to test extensively with human subjects.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决利用虚拟现实（VR）实验评估校园安全干预措施时存在的可扩展性限制，因为为每种实验条件招募新的人类参与者成本高昂，且阻碍了策略的迭代开发。作者提出了一种数据驱动的离散事件模拟器（DES），该模型将枪手的移动和行动建模为随机过程，其参数直接从VR研究中收集的行为数据学习得到。实验验证表明，该模拟器能够复现关键的实证行为模式，从而实现了对干预策略（例如基于机器人的干预）的可扩展评估与学习，而这些策略若直接使用人类受试者进行广泛测试将是不可行的。</div>
</details>
</div>
<div class="card">
<div class="title">Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</div>
<div class="meta-line">Authors: Miranda Muqing Miao, Young-Min Cho, Lyle Ungar</div>
<div class="meta-line">First: 2026-02-05T18:55:56+00:00 · Latest: 2026-02-05T18:55:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06022v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正确性优化的残差激活透镜（CORAL）：可迁移且感知校准的推理时引导方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）存在持续性的校准偏差问题，在指令微调和偏好对齐后尤为明显。虽然修改训练目标可改善校准效果，但重新训练成本高昂。推理时引导提供了一种轻量级替代方案，然而现有方法大多优化正确性的代理指标而非正确性本身。本文提出CORAL（正确性优化的残差激活透镜），这是一种正则化的推理时引导方法，通过权重衰减MLP探针从模型内部激活中捕获分布式正确性信号。我们在三个70亿参数模型上评估CORAL，发现其平均提升准确率10%、降低预期校准误差（ECE）50%。进一步实验表明，该方法无需重新训练即可迁移至四个保留基准测试（ARC-Challenge、HellaSwag、Math-MC、OpenBookQA）的完整公开测试集，平均提升准确率14%、改善ECE 49%。结果验证了以下假设：当单个神经元信息不足时，可通过正则化探针提取模型内部的分布式信息。因此，CORAL为提升推理时多项选择题性能提供了一种计算高效、可迁移且感知校准的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the persistent miscalibration of large language models after instruction tuning without expensive retraining, this paper introduces CORAL, a regularized inference-time steering method that uses weight-decay MLP probes to extract distributed correctness signals from model internal activations. The method consistently improved accuracy by 10% and reduced expected calibration error by 50% on average across three 7B-parameter models, with these gains transferring to four held-out benchmarks (averaging 14% accuracy improvement and 49% ECE improvement) without additional training.</div>
<div class="mono" style="margin-top:8px">为解决大语言模型在指令微调和偏好对齐后持续存在的校准问题，同时避免昂贵的重新训练，本研究提出了CORAL，一种直接优化正确性的推理时引导方法。该方法采用权重衰减正则化的多层感知机探针，从模型内部激活中提取分布式的正确性信号。在三个70亿参数模型上的实验表明，CORAL平均将准确率提升10%，并将预期校准误差降低50%；这些改进可迁移到保留的基准测试集上，平均带来14%的准确率提升和49%的校准误差改善。</div>
</details>
</div>
<div class="card">
<div class="title">Optimism Stabilizes Thompson Sampling for Adaptive Inference</div>
<div class="meta-line">Authors: Shunxing Yan, Han Zhong</div>
<div class="meta-line">First: 2026-02-05T18:52:54+00:00 · Latest: 2026-02-05T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm&#x27;s pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观性稳定自适应推断中的汤普森采样</div>
<div class="mono" style="margin-top:8px">汤普森采样（TS）广泛应用于随机多臂老虎机问题，但其在自适应数据收集下的推断性质较为微妙。由于各臂的样本量具有随机性，且通过动作选择规则与奖励耦合，经典样本均值的渐近理论可能失效。我们在$K$臂高斯老虎机中研究这一现象，并发现\emph{乐观性}是恢复\emph{稳定性}的关键机制——稳定性是有效渐近推断的充分条件，要求各臂的拉动次数集中在确定性尺度附近。首先，我们证明方差膨胀TS \citep{halder2025stable}对任意$K \ge 2$（包括多臂同时最优的挑战性场景）均具有稳定性，这通过将\citet{halder2025stable}的双臂结果推广至一般$K$臂设定，解决了其提出的开放性问题。其次，我们分析了一种保持后验方差不变但对后验均值添加显式奖励加成的乐观修正方案，并证明了相同的稳定性结论。总之，适当实施的乐观性能够稳定汤普森采样，使多臂老虎机中的渐近有效推断成为可能，且仅产生轻微的额外遗憾代价。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of performing valid statistical inference under adaptive data collection in multi-armed bandits using Thompson sampling (TS), where classical asymptotic theory fails due to the randomness and coupling of arm-specific sample sizes with rewards. The study identifies optimism as a key stabilizing mechanism and analyzes two specific implementations: variance-inflated TS and an alternative method that adds an explicit mean bonus to the posterior mean. The main experimental findings prove that both optimistic modifications ensure stability—where each arm&#x27;s pull count concentrates around a deterministic scale—for any number of arms K ≥ 2, including cases with multiple optimal arms, thereby enabling asymptotically valid inference with only a mild additional regret cost.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决随机多臂老虎机中汤普森采样（TS）的自适应数据收集所带来的挑战，该挑战会导致经典的渐近理论失效，因为臂特定的随机样本量与奖励通过动作选择规则相互耦合。方法上，研究探讨了乐观主义作为一种稳定机制的作用，分析了两种具体修改：方差膨胀TS和另一种保持后验方差不变但为后验均值添加显式奖励的乐观变体。主要实验结果证明，这两种乐观修改都能确保稳定性——即每个臂的拉动次数围绕一个确定性尺度集中——适用于任意臂数K ≥ 2，包括存在多个最优臂的情况，从而解决了一个开放性问题，并实现了渐近有效的推断，且仅带来轻微的遗憾代价。</div>
</details>
</div>
<div class="card">
<div class="title">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</div>
<div class="meta-line">Authors: Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-05T18:52:48+00:00 · Latest: 2026-02-05T18:52:48+00:00</div>
<div class="meta-line">Comments: Project Page: https://genarena.github.io/, Code: https://github.com/ruihanglix/genarena</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06013v1">PDF</a> · <a href="https://github.com/ruihanglix/genarena">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://genarena.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenArena：如何实现视觉生成任务的人类对齐评估？</div>
<div class="mono" style="margin-top:8px">视觉生成模型的快速发展已超越传统评估方法，亟需采用视觉语言模型作为替代评判者。本研究系统探究了主流绝对逐点评分标准在广泛视觉生成任务中的可靠性，分析表明该范式因随机不一致性及与人类感知对齐度低而存在局限。为突破这些限制，我们提出GenArena——一个基于成对比较范式的统一评估框架，确保稳定且人类对齐的评估。关键实验发现：仅采用此成对协议即可使现成开源模型超越顶级专有模型。该方法将评估准确率提升超20%，与权威LMArena排行榜的斯皮尔曼相关系数达0.86，显著优于逐点方法的0.36。基于GenArena，我们对前沿视觉生成模型进行多任务基准测试，为社区提供严谨自动的视觉生成评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid progress in visual generation models has rendered traditional evaluation methods inadequate, prompting the use of Vision-Language Models as judges; however, current absolute pointwise scoring standards suffer from stochastic inconsistency and misalignment with human judgment. To address this, the authors introduce GenArena, a unified evaluation framework that employs a pairwise comparison paradigm to ensure stable and human-aligned assessment. Experimental results demonstrate that adopting this pairwise protocol allows off-the-shelf open-source models to outperform top proprietary models, boosting evaluation accuracy by over 20% and achieving a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, far surpassing the 0.36 correlation of pointwise methods.</div>
<div class="mono" style="margin-top:8px">视觉生成模型的快速发展使得传统评估方法显得不足，促使研究者采用视觉语言模型作为评判者。本研究系统性地检验了当前主流的绝对点式评分标准在各种视觉生成任务中的可靠性，发现其存在随机不一致性和与人类感知的偏差。为解决这些问题，作者提出了GenArena，一个统一的评估框架，采用成对比较范式以确保稳定且与人类对齐的评估。关键实验结果表明，采用这种成对协议使得现成的开源模型能够超越顶级专有模型，将评估准确率提高了20%以上，并与权威的LMArena排行榜达到了0.86的斯皮尔曼相关性，远超点式方法的0.36相关性。</div>
</details>
</div>
<div class="card">
<div class="title">AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions</div>
<div class="meta-line">Authors: Xianyang Liu, Shangding Gu, Dawn Song</div>
<div class="meta-line">First: 2026-02-05T18:50:36+00:00 · Latest: 2026-02-05T18:50:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06008v1">PDF</a> · <a href="https://github.com/SafeRL-Lab/AgenticPay">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents are increasingly expected to negotiate, coordinate, and transact autonomously, yet existing benchmarks lack principled settings for evaluating language-mediated economic interaction among multiple agents. We introduce AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language. AgenticPay models markets in which buyers and sellers possess private constraints and product-dependent valuations, and must reach agreements through multi-round linguistic negotiation rather than numeric bidding alone. The framework supports a diverse suite of over 110 tasks ranging from bilateral bargaining to many-to-many markets, with structured action extraction and metrics for feasibility, efficiency, and welfare. Benchmarking state-of-the-art proprietary and open-weight LLMs reveals substantial gaps in negotiation performance and highlights challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce and language-based market interaction. Code and dataset are available at the link: https://github.com/SafeRL-Lab/AgenticPay.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgenticPay：面向买卖交易的多智能体大语言模型协商系统</div>
<div class="mono" style="margin-top:8px">基于大语言模型的智能体日益被期望能自主进行协商、协调与交易，但现有基准缺乏评估多智能体间语言驱动经济交互的原则性场景。本文提出AgenticPay——一个由自然语言驱动的多智能体买卖协商基准与仿真框架。该框架模拟买卖双方拥有私有约束和产品相关估值的市场环境，要求通过多轮语言协商（而非仅数字竞价）达成协议。框架支持涵盖双边议价至多对多市场的110余项任务，具备结构化动作提取机制，并提供可行性、效率与福利等评估指标。对前沿闭源及开源大语言模型的基准测试揭示了协商性能的显著差距，凸显了长程战略推理的挑战，从而确立AgenticPay作为研究智能体商业与语言化市场交互的基础平台。代码与数据集可通过链接获取：https://github.com/SafeRL-Lab/AgenticPay。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for principled evaluation settings for language-mediated economic interactions among LLM-based agents, as existing benchmarks lack such frameworks. The method introduces AgenticPay, a benchmark and simulation framework for multi-agent buyer-seller negotiation driven by natural language, which models markets with private constraints and product-dependent valuations, supporting over 110 tasks from bilateral bargaining to many-to-many markets with structured action extraction and economic metrics. Experimental results from benchmarking state-of-the-art LLMs reveal substantial gaps in negotiation performance and highlight challenges in long-horizon strategic reasoning, establishing AgenticPay as a foundation for studying agentic commerce.</div>
<div class="mono" style="margin-top:8px">本研究针对当前缺乏用于评估多智能体之间基于自然语言的经济交互的基准测试的问题。作者提出了AgenticPay，这是一个模拟框架和基准测试，用于对买卖双方市场进行建模，其中具有私有约束和估值的智能体必须通过多轮语言谈判达成协议。该框架包含110多个多样化任务，并配有结构化行动提取以及可行性、效率和福利等评估指标。对前沿大语言模型的实验性基准测试揭示了它们在谈判性能上的显著差距，并突显了在长视野战略推理方面的挑战，从而将该基准确立为研究智能体商业和基于语言的市场交互的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Speech Emotion Recognition Leveraging OpenAI&#x27;s Whisper Representations and Attentive Pooling Methods</div>
<div class="meta-line">Authors: Ali Shendabadi, Parnia Izadirad, Mostafa Salehi, Mahmoud Bijankhan</div>
<div class="meta-line">First: 2026-02-05T18:46:28+00:00 · Latest: 2026-02-05T18:46:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06000v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech Emotion Recognition (SER) research has faced limitations due to the lack of standard and sufficiently large datasets. Recent studies have leveraged pre-trained models to extract features for downstream tasks such as SER. This work explores the capabilities of Whisper, a pre-trained ASR system, in speech emotion recognition by proposing two attention-based pooling methods, Multi-head Attentive Average Pooling and QKV Pooling, designed to efficiently reduce the dimensionality of Whisper representations while preserving emotional features. We experiment on English and Persian, using the IEMOCAP and ShEMO datasets respectively, with Whisper Tiny and Small. Our multi-head QKV architecture achieves state-of-the-art results on the ShEMO dataset, with a 2.47% improvement in unweighted accuracy. We further compare the performance of different Whisper encoder layers and find that intermediate layers often perform better for SER on the Persian dataset, providing a lightweight and efficient alternative to much larger models such as HuBERT X-Large. Our findings highlight the potential of Whisper as a representation extractor for SER and demonstrate the effectiveness of attention-based pooling for dimension reduction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用OpenAI Whisper表征与注意力池化方法的语音情感识别</div>
<div class="mono" style="margin-top:8px">语音情感识别研究因缺乏标准化且规模足够大的数据集而面临局限。近期研究利用预训练模型提取特征以支持下游任务，如语音情感识别。本研究探索了预训练自动语音识别系统Whisper在语音情感识别中的能力，提出了两种基于注意力的池化方法——多头注意力平均池化与QKV池化，旨在高效降低Whisper表征的维度同时保留情感特征。我们分别使用IEMOCAP和ShEMO数据集在英语和波斯语上进行了实验，并采用Whisper Tiny和Small版本。我们的多头QKV架构在ShEMO数据集上取得了最先进的结果，未加权准确率提升了2.47%。我们进一步比较了不同Whisper编码器层的性能，发现中间层在波斯语数据集上的语音情感识别任务中通常表现更优，为替代HuBERT X-Large等大型模型提供了轻量高效的方案。本研究结果凸显了Whisper作为语音情感识别表征提取器的潜力，并证明了基于注意力的池化方法在降维中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Speech Emotion Recognition (SER) research is often constrained by limited and non-standard datasets, prompting the use of pre-trained models for feature extraction. This work investigates the suitability of OpenAI&#x27;s Whisper, a pre-trained automatic speech recognition model, for SER by introducing two attention-based pooling methods—Multi-head Attentive Average Pooling and QKV Pooling—to effectively condense Whisper&#x27;s representations while retaining emotional information. Experiments on the English IEMOCAP and Persian ShEMO datasets using Whisper Tiny and Small variants show that the multi-head QKV architecture achieves a state-of-the-art unweighted accuracy improvement of 2.47% on ShEMO, and intermediate encoder layers are found to be particularly effective for the Persian dataset, offering a lightweight yet competitive alternative to larger models like HuBERT X-Large.</div>
<div class="mono" style="margin-top:8px">本研究针对语音情感识别领域缺乏标准大型数据集的问题，探索了利用预训练语音识别模型Whisper作为特征提取器的潜力。方法上提出了两种基于注意力的池化技术——多头注意力平均池化和QKV池化，旨在高效降低Whisper表征的维度并保留情感特征。在英语（IEMOCAP）和波斯语（ShEMO）数据集上的实验表明，所提出的QKV池化架构在ShEMO上取得了最先进的性能，准确率提升了2.47%，同时发现Whisper的中间编码器层能作为HuBERT X-Large等大型模型的轻量高效替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps</div>
<div class="meta-line">Authors: Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz</div>
<div class="meta-line">First: 2026-02-05T18:42:00+00:00 · Latest: 2026-02-05T18:42:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05993v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05993v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow and diffusion models produce high-quality samples, but adapting them to user preferences or constraints post-training remains costly and brittle, a challenge commonly called reward alignment. We argue that efficient reward alignment should be a property of the generative model itself, not an afterthought, and redesign the model for adaptability. We propose &quot;Diamond Maps&quot;, stochastic flow map models that enable efficient and accurate alignment to arbitrary rewards at inference time. Diamond Maps amortize many simulation steps into a single-step sampler, like flow maps, while preserving the stochasticity required for optimal reward alignment. This design makes search, sequential Monte Carlo, and guidance scalable by enabling efficient and consistent estimation of the value function. Our experiments show that Diamond Maps can be learned efficiently via distillation from GLASS Flows, achieve stronger reward alignment performance, and scale better than existing methods. Our results point toward a practical route to generative models that can be rapidly adapted to arbitrary preferences and constraints at inference time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>钻石映射：基于随机流映射的高效奖励对齐方法</div>
<div class="mono" style="margin-top:8px">流模型与扩散模型能生成高质量样本，但训练后使其适应用户偏好或约束仍面临成本高昂且脆弱的挑战，这一难题通常称为奖励对齐。我们认为高效奖励对齐应是生成模型的内在特性而非事后补救，为此重新设计了具有适应性的模型架构。我们提出“钻石映射”——一种随机流映射模型，可在推理阶段实现对任意奖励函数的高效精准对齐。该模型将多步模拟过程摊销为单步采样器（如流映射），同时保留奖励对齐所需随机性。该设计通过实现价值函数的高效一致估计，使搜索、序列蒙特卡洛和引导方法具备可扩展性。实验表明：钻石映射可通过GLASS流蒸馏高效学习，获得更优的奖励对齐性能，且比现有方法具有更好的扩展性。我们的研究为构建能在推理阶段快速适应任意偏好与约束的生成模型提供了可行路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of adapting flow and diffusion models to user preferences post-training, which is typically costly and brittle. The authors propose Diamond Maps, stochastic flow map models designed for efficient reward alignment at inference time by amortizing multiple simulation steps into a single-step sampler while preserving necessary stochasticity. Experiments demonstrate that Diamond Maps, learned via distillation from GLASS Flows, achieve stronger reward alignment performance and better scalability than existing methods.</div>
<div class="mono" style="margin-top:8px">在训练后调整流模型和扩散模型以适应偏好或约束通常效率低下且不稳定，这被称为奖励对齐问题。为此，研究者提出了Diamond Maps，这是一种专为适应性设计的随机流映射模型，它将多个模拟步骤摊销为单步采样器，同时保留了必要的随机性。实验表明，通过从GLASS Flows高效蒸馏学习，Diamond Maps实现了比现有方法更优的奖励对齐性能和更好的可扩展性，为在推理时快速适应任意偏好的生成模型提供了一条实用路径。</div>
</details>
</div>
<div class="card">
<div class="title">SIRR-LMM: Single-image Reflection Removal via Large Multimodal Model</div>
<div class="meta-line">Authors: Yu Guo, Zhiqiang Lao, Xiyun Song, Yubin Zhou, Heather Yu</div>
<div class="meta-line">First: 2026-01-12T05:03:12+00:00 · Latest: 2026-02-05T18:37:54+00:00</div>
<div class="meta-line">Comments: 12 pages, 14 figures, accepted in WACVW 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07209v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07209v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Glass surfaces create complex interactions of reflected and transmitted light, making single-image reflection removal (SIRR) challenging. Existing datasets suffer from limited physical realism in synthetic data or insufficient scale in real captures. We introduce a synthetic dataset generation framework that path-traces 3D glass models over real background imagery to create physically accurate reflection scenarios with varied glass properties, camera settings, and post-processing effects. To leverage the capabilities of Large Multimodal Model (LMM), we concatenate the image layers into a single composite input, apply joint captioning, and fine-tune the model using task-specific LoRA rather than full-parameter training. This enables our approach to achieve improved reflection removal and separation performance compared to state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SIRR-LMM：基于大型多模态模型的单图像反射消除</div>
<div class="mono" style="margin-top:8px">玻璃表面会产生反射光与透射光的复杂交互，使得单图像反射消除（SIRR）具有挑战性。现有数据集存在合成数据物理真实性不足或真实采集规模有限的问题。我们提出一种合成数据集生成框架，通过对真实背景图像中的三维玻璃模型进行路径追踪，创建具有不同玻璃属性、相机设置和后处理效果的物理精确反射场景。为发挥大型多模态模型（LMM）的能力，我们将图像层拼接为复合输入，应用联合描述生成，并采用任务特定的LoRA进行微调而非全参数训练。该方法在反射消除与分离性能上优于当前最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of single-image reflection removal (SIRR), where glass surfaces create complex light interactions, and existing datasets are limited in physical realism or scale. The method introduces a synthetic dataset generation framework that path-traces 3D glass models over real backgrounds to create physically accurate reflection scenarios. It then leverages a Large Multimodal Model (LMM) by concatenating image layers into a composite input, applying joint captioning, and fine-tuning with task-specific LoRA. Experimental results show this approach achieves improved reflection removal and separation performance compared to state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">本研究针对单图像反射去除（SIRR）的挑战，即玻璃表面造成复杂的光线交互，而现有数据集在物理真实性或规模上存在不足。方法引入了一个合成数据集生成框架，通过对真实背景进行3D玻璃模型的光线追踪，创建具有不同属性的物理精确反射场景，并通过将图像层拼接为复合输入、应用联合描述、并使用任务特定的LoRA进行微调，来优化大型多模态模型（LMM）。实验结果表明，与现有先进方法相比，该方法在反射去除和分离性能上取得了提升。</div>
</details>
</div>
<div class="card">
<div class="title">RISE-Video: Can Video Generators Decode Implicit World Rules?</div>
<div class="meta-line">Authors: Mingxin Liu, Shuran Ma, Shibei Meng, Xiangyu Zhao, Zicheng Zhang, Shaofeng Zhang, Zhihang Zhong, Peixian Chen, Haoyu Cao, Xing Sun, Haodong Duan, Xue Yang</div>
<div class="meta-line">First: 2026-02-05T18:36:10+00:00 · Latest: 2026-02-05T18:36:10+00:00</div>
<div class="meta-line">Comments: 38 pages, 16 figures, 3 tables; Code: https://github.com/VisionXLab/RISE-Video; HuggingFace: https://huggingface.co/datasets/VisionXLab/RISE-Video</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05986v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05986v1">PDF</a> · <a href="https://github.com/VisionXLab/RISE-Video">Code1</a> · <a href="https://huggingface.co/datasets/VisionXLab/RISE-Video">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While generative video models have achieved remarkable visual fidelity, their capacity to internalize and reason over implicit world rules remains a critical yet under-explored frontier. To bridge this gap, we present RISE-Video, a pioneering reasoning-oriented benchmark for Text-Image-to-Video (TI2V) synthesis that shifts the evaluative focus from surface-level aesthetics to deep cognitive reasoning. RISE-Video comprises 467 meticulously human-annotated samples spanning eight rigorous categories, providing a structured testbed for probing model intelligence across diverse dimensions, ranging from commonsense and spatial dynamics to specialized subject domains. Our framework introduces a multi-dimensional evaluation protocol consisting of four metrics: \textit{Reasoning Alignment}, \textit{Temporal Consistency}, \textit{Physical Rationality}, and \textit{Visual Quality}. To further support scalable evaluation, we propose an automated pipeline leveraging Large Multimodal Models (LMMs) to emulate human-centric assessment. Extensive experiments on 11 state-of-the-art TI2V models reveal pervasive deficiencies in simulating complex scenarios under implicit constraints, offering critical insights for the advancement of future world-simulating generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RISE-Video：视频生成器能否解码隐含世界规则？</div>
<div class="mono" style="margin-top:8px">尽管生成式视频模型已实现卓越的视觉保真度，但其对隐含世界规则的内化与推理能力仍是关键却尚未充分探索的前沿领域。为填补这一空白，我们提出RISE-Video——一个面向文本-图像到视频（TI2V）合成的开创性推理基准，将评估重点从表层美学转向深层认知推理。RISE-Video包含467个经人工精细标注的样本，涵盖八个严谨类别，为从常识推理、空间动力学到专业领域等多维度探究模型智能提供了结构化测试平台。我们的框架引入包含四项指标的多维评估协议：\textit{推理对齐度}、\textit{时序一致性}、\textit{物理合理性}与\textit{视觉质量}。为支持可扩展评估，我们提出利用大型多模态模型（LMMs）模拟人类中心化评估的自动化流程。对11个前沿TI2V模型的广泛实验揭示了其在隐含约束下模拟复杂场景的普遍缺陷，为未来世界模拟生成模型的演进提供了关键洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the critical gap in evaluating whether video generation models can internalize and reason about implicit world rules, moving beyond mere visual fidelity. The authors introduce RISE-Video, a reasoning-oriented benchmark with 467 human-annotated samples across eight categories, and propose a multi-dimensional evaluation protocol with four metrics: Reasoning Alignment, Temporal Consistency, Physical Rationality, and Visual Quality. They also develop an automated pipeline using Large Multimodal Models to emulate human assessment. Experiments on 11 state-of-the-art Text-Image-to-Video models reveal widespread deficiencies in simulating complex scenarios under implicit constraints, highlighting significant limitations in current models&#x27; reasoning capabilities.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估先进视频生成模型能否理解和应用隐含的现实世界规则，而不仅仅是追求视觉质量。方法上提出了RISE-Video基准，包含467个人工标注样本，涵盖八个推理类别，并通过一个多维评估协议（包括推理对齐和时间一致性）进行评估，同时利用大型多模态模型构建了自动化评估流程。对11个前沿模型的实验结果表明，它们在隐含约束下模拟复杂场景的能力存在显著不足，揭示了当前生成式视频智能的一个关键缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins</div>
<div class="meta-line">Authors: Krešimir Kušić, Vinny Cahill, Ivana Dusparic</div>
<div class="meta-line">First: 2026-02-05T18:33:03+00:00 · Latest: 2026-02-05T18:33:03+00:00</div>
<div class="meta-line">Comments: IEEE IV2026 37th IEEE Intelligent Vehicles Symposium</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05983v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The operational effectiveness of digital-twin technology in motorway traffic management depends on the availability of a continuous flow of high-resolution real-time traffic data. To function as a proactive decision-making support layer within traffic management, a digital twin must also incorporate predicted traffic conditions in addition to real-time observations. Due to the spatio-temporal complexity and the time-variant, non-linear nature of traffic dynamics, predicting motorway traffic remains a difficult problem. Sequence-based deep-learning models offer clear advantages over classical machine learning and statistical models in capturing long-range, temporal dependencies in time-series traffic data, yet limitations in forecasting accuracy and model complexity point to the need for further improvements. To improve motorway traffic forecasting, this paper introduces a Geographically-aware Transformer-based Traffic Forecasting GATTF model, which exploits the geographical relationships between distributed sensors using their mutual information (MI). The model has been evaluated using real-time data from the Geneva motorway network in Switzerland and results confirm that incorporating geographical awareness through MI enhances the accuracy of GATTF forecasting compared to a standard Transformer, without increasing model complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向城市高速公路数字孪生的地理感知Transformer交通预测模型</div>
<div class="mono" style="margin-top:8px">数字孪生技术在高速公路交通管理中的运行效能依赖于持续的高分辨率实时交通数据流。要作为交通管理中的主动决策支持层，数字孪生除实时观测外还需纳入预测的交通状况。由于交通动态的时空复杂性、时变性和非线性特征，高速公路交通预测仍是难题。基于序列的深度学习模型在捕捉时序交通数据的长期依赖关系上较传统机器学习与统计模型具有明显优势，但预测精度与模型复杂度方面的局限表明仍需改进。为提升高速公路交通预测能力，本文提出一种地理感知Transformer交通预测（GATTF）模型，该模型利用分布式传感器间的互信息（MI）挖掘其地理关联性。通过瑞士日内瓦高速公路网络的实时数据验证表明，融入MI的地理感知机制在未增加模型复杂度的前提下，使GATTF相比标准Transformer模型取得了更高的预测精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance proactive traffic management in motorway digital twins by improving the accuracy of traffic forecasting, which is challenged by the spatio-temporal complexity and non-linear dynamics of traffic. The proposed method, the Geographically-aware Transformer-based Traffic Forecasting (GATTF) model, incorporates geographical relationships between distributed sensors by leveraging their mutual information to better capture spatial dependencies. Experimental evaluation using real-time data from the Geneva motorway network demonstrates that this geographical awareness improves forecasting accuracy compared to a standard Transformer model, without increasing model complexity.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过提高交通预测的准确性来增强高速公路数字孪生中的主动交通管理，而交通预测因交通的时空复杂性和非线性动态而面临挑战。所提出的方法，即基于地理感知的Transformer交通预测（GATTF）模型，通过利用分布式传感器之间的互信息来整合它们的地理关系，从而更好地捕捉空间依赖性。使用日内瓦高速公路网络的实时数据进行的实验评估表明，与标准Transformer模型相比，这种地理感知提高了预测准确性，且未增加模型复杂性。</div>
</details>
</div>
<div class="card">
<div class="title">Clifford Kolmogorov-Arnold Networks</div>
<div class="meta-line">Authors: Matthias Wolff, Francesco Alesiani, Christof Duhme, Xiaoyi Jiang</div>
<div class="meta-line">First: 2026-02-05T18:25:40+00:00 · Latest: 2026-02-05T18:25:40+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05977v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Clifford Kolmogorov-Arnold Network (ClKAN), a flexible and efficient architecture for function approximation in arbitrary Clifford algebra spaces. We propose the use of Randomized Quasi Monte Carlo grid generation as a solution to the exponential scaling associated with higher dimensional algebras. Our ClKAN also introduces new batch normalization strategies to deal with variable domain input. ClKAN finds application in scientific discovery and engineering, and is validated in synthetic and physics inspired tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>克利福德-柯尔莫哥洛夫-阿诺德网络</div>
<div class="mono" style="margin-top:8px">本文提出克利福德-柯尔莫哥洛夫-阿诺德网络（ClKAN），一种适用于任意克利福德代数空间的灵活高效函数逼近架构。针对高维代数伴随的指数级计算复杂度，我们采用随机拟蒙特卡洛网格生成方法予以解决。ClKAN还引入新的批归一化策略以处理变域输入。该架构在科学发现与工程领域具有应用价值，并在合成任务及物理启发任务中得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop an efficient architecture for function approximation within Clifford algebra spaces, addressing the challenge of exponential scaling in higher dimensions. The method introduces Clifford Kolmogorov-Arnold Networks (ClKAN), which employs Randomized Quasi Monte Carlo grid generation to manage dimensional complexity and incorporates novel batch normalization strategies for handling variable domain inputs. Experimental validation on synthetic and physics-inspired tasks demonstrates the model&#x27;s applicability in scientific discovery and engineering.</div>
<div class="mono" style="margin-top:8px">本研究提出了Clifford Kolmogorov-Arnold网络（ClKAN），这是一种新颖的架构，旨在Clifford代数空间中逼近函数，以满足科学和工程应用中对灵活高效模型的需求。该方法通过采用随机拟蒙特卡洛网格生成来解决高维空间中的指数级计算复杂度问题，并引入了新的批归一化策略来处理可变定义域输入。在合成任务和物理启发任务上的实验验证表明，该模型在这些复杂空间中的函数逼近是有效的。</div>
</details>
</div>
<div class="card">
<div class="title">Inverse Depth Scaling From Most Layers Being Similar</div>
<div class="meta-line">Authors: Yizhou Liu, Sara Kangaslahti, Ziming Liu, Jeff Gore</div>
<div class="meta-line">First: 2026-02-05T18:22:41+00:00 · Latest: 2026-02-05T18:22:41+00:00</div>
<div class="meta-line">Comments: 23 pages, 24 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05970v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural scaling laws relate loss to model size in large language models (LLMs), yet depth and width may contribute to performance differently, requiring more detailed studies. Here, we quantify how depth affects loss via analysis of LLMs and toy residual networks. We find loss scales inversely proportional to depth in LLMs, probably due to functionally similar layers reducing error through ensemble averaging rather than compositional learning or discretizing smooth dynamics. This regime is inefficient yet robust and may arise from the architectural bias of residual networks and target functions incompatible with smooth dynamics. The findings suggest that improving LLM efficiency may require architectural innovations to encourage compositional use of depth.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>源于多数层相似性的逆深度缩放</div>
<div class="mono" style="margin-top:8px">神经缩放定律描述了大型语言模型（LLM）中损失与模型规模的关系，但深度与宽度对性能的影响可能不同，需要更细致的研究。本文通过分析LLM与玩具残差网络，量化了深度对损失的影响。我们发现LLM中损失与深度成反比缩放，这可能源于功能相似的层通过集成平均降低误差，而非组合式学习或离散化平滑动态。这种机制虽低效但稳健，可能源自残差网络的结构偏置以及与平滑动态不兼容的目标函数。研究结果表明，提升LLM效率可能需要通过架构创新来促进深度的组合式利用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates how depth specifically influences performance in large language models, motivated by the need to move beyond general scaling laws that treat model size uniformly. By analyzing both real LLMs and toy residual networks, the method quantifies the relationship between loss and depth. The key experimental finding is that loss scales inversely with depth, a pattern attributed to most layers performing functionally similar operations, leading to an inefficient ensemble averaging effect rather than compositional learning. This suggests that current architectures may be biased toward this robust but suboptimal regime, pointing to a need for architectural innovations to better leverage depth for efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究深度如何具体影响大语言模型的性能，动机在于需要超越一般的缩放定律，以理解深度与宽度对性能的不同贡献。通过分析真实的大语言模型和玩具残差网络，该方法量化了损失与深度之间的关系，揭示了一种逆缩放定律，即损失随深度增加而成比例减少。关键的实验结果表明，这种效应可能源于功能相似的层通过集成平均来减少误差，而非实现组合学习或离散化平滑动态；这种机制虽然稳健但效率低下，表明残差网络的架构偏差和不兼容的目标函数可能限制了效率，因此需要架构创新来更好地组合利用深度。</div>
</details>
</div>
<div class="card">
<div class="title">LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation</div>
<div class="meta-line">Authors: Mirlan Karimov, Teodora Spasojevic, Markus Braun, Julian Wiederer, Vasileios Belagiannis, Marc Pollefeys</div>
<div class="meta-line">First: 2026-02-05T18:21:02+00:00 · Latest: 2026-02-05T18:21:02+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE IV 2026. 8 pages, 3 figures. Code available at https://github.com/mirlanium/LSA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05966v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05966v1">PDF</a> · <a href="https://github.com/mirlanium/LSA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Controllable video generation has emerged as a versatile tool for autonomous driving, enabling realistic synthesis of traffic scenarios. However, existing methods depend on control signals at inference time to guide the generative model towards temporally consistent generation of dynamic objects, limiting their utility as scalable and generalizable data engines. In this work, we propose Localized Semantic Alignment (LSA), a simple yet effective framework for fine-tuning pre-trained video generation models. LSA enhances temporal consistency by aligning semantic features between ground-truth and generated video clips. Specifically, we compare the output of an off-the-shelf feature extraction model between the ground-truth and generated video clips localized around dynamic objects inducing a semantic feature consistency loss. We fine-tune the base model by combining this loss with the standard diffusion loss. The model fine-tuned for a single epoch with our novel loss outperforms the baselines in common video generation evaluation metrics. To further test the temporal consistency in generated videos we adapt two additional metrics from object detection task, namely mAP and mIoU. Extensive experiments on nuScenes and KITTI datasets show the effectiveness of our approach in enhancing temporal consistency in video generation without the need for external control signals during inference and any computational overheads.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LSA：基于局部语义对齐增强交通视频生成时序一致性的方法</div>
<div class="mono" style="margin-top:8px">可控视频生成已成为自动驾驶领域的重要工具，能够合成逼真的交通场景。然而，现有方法依赖推理时的控制信号引导生成模型实现动态物体的时序一致性生成，限制了其作为可扩展通用数据引擎的实用性。本文提出局部语义对齐（LSA）框架，这是一种简单有效的预训练视频生成模型微调方法。LSA通过对齐真实视频片段与生成视频片段的语义特征来增强时序一致性。具体而言，我们使用现成的特征提取模型，在动态物体局部区域内对比真实与生成视频的特征输出，构建语义特征一致性损失。通过将该损失与标准扩散损失结合对基础模型进行微调。实验表明，采用新损失函数仅微调一个周期的模型，在主流视频生成评估指标上均优于基线方法。为深入评估生成视频的时序一致性，我们额外引入了目标检测任务中的mAP和mIoU指标。在nuScenes和KITTI数据集上的大量实验证明，该方法无需推理时外部控制信号且不增加计算开销，即可有效提升视频生成的时序一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the reliance on external control signals for temporal consistency in traffic video generation, which limits scalability and generalization, this work proposes Localized Semantic Alignment (LSA) for fine-tuning pre-trained models. The method aligns semantic features between ground-truth and generated video clips localized around dynamic objects, using an off-the-shelf feature extractor to compute a consistency loss combined with standard diffusion loss during fine-tuning. Experimental results on nuScenes and KITTI datasets demonstrate that models fine-tuned for a single epoch with LSA outperform baselines in standard video metrics and show improved temporal consistency as measured by adapted object detection metrics (mAP and mIoU), all without inference-time control signals or added computational overhead.</div>
<div class="mono" style="margin-top:8px">针对交通视频生成中依赖外部控制信号来保证时序一致性的问题，这限制了方法的可扩展性，本研究提出了局部语义对齐（LSA）来微调预训练模型。该方法通过在动态对象周围的局部区域，利用现成的特征提取器对齐真实视频片段与生成视频片段的语义特征，计算一致性损失并与标准扩散损失结合进行微调。在nuScenes和KITTI数据集上的实验表明，使用LSA仅微调一个周期的模型在标准视频评估指标上优于基线，并且通过适配的目标检测指标（mAP和mIoU）衡量，其生成的视频时序一致性得到提升，且无需推理时的控制信号或额外的计算开销。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Share: Selective Memory for Efficient Parallel Agentic Systems</div>
<div class="meta-line">Authors: Joseph Fioresi, Parth Parag Kulkarni, Ashmal Vayani, Song Wang, Mubarak Shah</div>
<div class="meta-line">First: 2026-02-05T18:20:21+00:00 · Latest: 2026-02-05T18:20:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://joefioresi718.github.io/LTS_webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic systems solve complex tasks by coordinating multiple agents that iteratively reason, invoke tools, and exchange intermediate results. To improve robustness and solution quality, recent approaches deploy multiple agent teams running in parallel to explore diverse reasoning trajectories. However, parallel execution comes at a significant computational cost: when different teams independently reason about similar sub-problems or execute analogous steps, they repeatedly perform substantial overlapping computation. To address these limitations, in this paper, we propose Learning to Share (LTS), a learned shared-memory mechanism for parallel agentic frameworks that enables selective cross-team information reuse while controlling context growth. LTS introduces a global memory bank accessible to all teams and a lightweight controller that decides whether intermediate agent steps should be added to memory or not. The controller is trained using stepwise reinforcement learning with usage-aware credit assignment, allowing it to identify information that is globally useful across parallel executions. Experiments on the AssistantBench and GAIA benchmarks show that LTS significantly reduces overall runtime while matching or improving task performance compared to memory-free parallel baselines, demonstrating that learned memory admission is an effective strategy for improving the efficiency of parallel agentic systems. Project page: https://joefioresi718.github.io/LTS_webpage/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学会共享：面向高效并行智能体系统的选择性记忆机制</div>
<div class="mono" style="margin-top:8px">智能体系统通过协调多个智能体进行迭代推理、调用工具并交换中间结果来解决复杂任务。为提高鲁棒性与求解质量，近期研究采用并行运行的多个智能体团队来探索多样化推理路径。然而并行执行会带来显著计算开销：当不同团队独立处理相似子问题或执行同类步骤时，会重复进行大量重叠计算。为突破此局限，本文提出&#x27;学会共享&#x27;（LTS）——一种用于并行智能体框架的习得式共享记忆机制，可在控制上下文增长的同时实现跨团队选择性信息复用。LTS构建了全局共享记忆库与轻量级控制器，后者通过逐步强化学习结合使用感知的信用分配机制进行训练，从而精准识别跨并行执行的全局有效信息。在AssistantBench和GAIA基准测试中，LTS在保持或提升任务性能的同时显著降低总体运行时间，证明习得式记忆准入是提升并行智能体系统效率的有效策略。项目页面：https://joefioresi718.github.io/LTS_webpage/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Parallel agentic systems improve robustness by exploring diverse reasoning paths but incur high computational costs due to redundant computations across teams. To address this, the authors propose Learning to Share (LTS), a method that introduces a global memory bank and a lightweight controller trained with stepwise reinforcement learning to selectively admit and reuse intermediate results across parallel agent teams, thereby controlling context growth. Experiments on AssistantBench and GAIA benchmarks demonstrate that LTS significantly reduces overall runtime while maintaining or improving task performance compared to memory-free parallel baselines.</div>
<div class="mono" style="margin-top:8px">并行智能体系统通过探索多样推理路径来提高鲁棒性，但团队间的重复计算导致高昂计算成本。为此，本文提出学习共享（LTS）方法，该方法引入全局记忆库和一个通过逐步强化学习训练的轻量控制器，以选择性存储和跨团队重用中间结果，从而控制上下文增长。在AssistantBench和GAIA基准上的实验表明，相比无记忆并行基线，LTS显著降低了总体运行时间，同时保持或提升了任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching</div>
<div class="meta-line">Authors: Junwan Kim, Jiho Park, Seonghu Jeon, Seungryong Kim</div>
<div class="meta-line">First: 2026-02-05T18:08:20+00:00 · Latest: 2026-02-05T18:08:20+00:00</div>
<div class="meta-line">Comments: Project Page: https://junwankimm.github.io/CSFM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05951v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05951v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://junwankimm.github.io/CSFM">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Flow matching has recently emerged as a promising alternative to diffusion-based generative models, particularly for text-to-image generation. Despite its flexibility in allowing arbitrary source distributions, most existing approaches rely on a standard Gaussian distribution, a choice inherited from diffusion models, and rarely consider the source distribution itself as an optimization target in such settings. In this work, we show that principled design of the source distribution is not only feasible but also beneficial at the scale of modern text-to-image systems. Specifically, we propose learning a condition-dependent source distribution under flow matching objective that better exploit rich conditioning signals. We identify key failure modes that arise when directly incorporating conditioning into the source, including distributional collapse and instability, and show that appropriate variance regularization and directional alignment between source and target are critical for stable and effective learning. We further analyze how the choice of target representation space impacts flow matching with structured sources, revealing regimes in which such designs are most effective. Extensive experiments across multiple text-to-image benchmarks demonstrate consistent and robust improvements, including up to a 3x faster convergence in FID, highlighting the practical benefits of a principled source distribution design for conditional flow matching.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更优源分布，更优流匹配：学习条件依赖的源分布以实现流匹配</div>
<div class="mono" style="margin-top:8px">流匹配技术近期已成为扩散生成模型的有力替代方案，尤其在文本到图像生成领域。尽管该方法允许使用任意源分布，但现有研究大多沿用扩散模型中的标准高斯分布，鲜少将源分布本身作为优化目标。本研究表明，在现代文本到图像生成系统中，对源分布进行原理性设计不仅可行，且能显著提升性能。具体而言，我们提出在流匹配目标下学习条件依赖的源分布，以更充分利用丰富的条件信号。我们揭示了将条件直接融入源分布时出现的关键失效模式（如分布坍缩和不稳定性），并证明适当的方差正则化及源-目标分布的方向对齐对稳定高效的学习至关重要。进一步分析了目标表示空间的选择如何影响结构化源分布的流匹配，揭示了此类设计最有效的应用场景。在多个文本到图像基准测试中的广泛实验表明，该方法能带来持续稳健的性能提升，包括FID指标收敛速度最高提升3倍，凸显了条件流匹配中原理性源分布设计的实用价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Flow matching offers a flexible generative modeling framework but typically uses a standard Gaussian source distribution, which may not optimally leverage conditioning signals. This work proposes learning a condition-dependent source distribution within the flow matching objective, addressing challenges like distributional collapse and instability through variance regularization and ensuring directional alignment between the source and target. Experiments on text-to-image generation benchmarks show that this principled source design yields consistent improvements, including up to a 3x faster convergence in FID scores.</div>
<div class="mono" style="margin-top:8px">流匹配作为一种灵活的生成建模方法，通常采用标准高斯分布作为源分布，而未针对条件任务进行优化。本研究提出在流匹配目标下学习条件依赖的源分布，以更好地利用丰富的条件信号，并通过方差正则化和源与目标之间的方向对齐来解决分布塌缩和不稳定性等关键问题。在多个文本到图像基准上的实验表明，这种原则性的源分布设计带来了持续的改进，包括FID收敛速度提升高达3倍，凸显了其在条件生成中的实际优势。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-02-05T18:03:03+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v2">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题探索新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时实施强化学习，使大型语言模型能够持续训练，并针对测试问题积累特定经验。这种持续学习形式具有特殊性：其目标是产生单一卓越解决方案而非平均意义上的多个良好方案，且专注于解决当前问题而非泛化至其他问题。因此，我们设计了优先探索最有潜力解决方案的学习目标与搜索子程序，并将该方法命名为“测试时训练发现法”。延续先前研究，我们聚焦于连续奖励问题，并在数学、GPU内核工程、算法设计和生物学领域报告了所有尝试问题的结果：1）埃尔德什最小重叠问题与自相关不等式；2）GPUMode内核竞赛（较现有技术提速达2倍）；3）往届AtCoder算法竞赛；4）单细胞分析中的去噪问题。所有方案均经专家或主办方评审。与先前依赖封闭前沿模型的最佳成果不同，本研究全部结果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现。测试时训练通过Thinking Machines的Tinker API执行，每个问题仅需数百美元成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance AI&#x27;s capability to discover novel state-of-the-art solutions for scientific problems, moving beyond prior test-time scaling methods that rely on prompting frozen LLMs. The proposed method, Test-Time Training to Discover (TTT-Discover), employs reinforcement learning at test time, allowing the LLM to continuously train with experience specific to the test problem, focusing on generating a single optimal solution rather than average performance across tasks. Key experimental results demonstrate that TTT-Discover achieves new state-of-the-art performance across diverse domains, including mathematics (Erdős&#x27; minimum overlap problem and an autocorrelation inequality), GPU kernel engineering (up to 2x speed improvement), algorithm design (past AtCoder competitions), and biology (single-cell analysis denoising), using an open model and reproducible code.</div>
<div class="mono" style="margin-top:8px">该研究旨在提升人工智能在科学问题中发现新颖最优解的能力，超越先前依赖提示冻结大语言模型的测试时扩展方法。所提出的方法——测试时训练发现（TTT-Discover），在测试时采用强化学习，使大语言模型能够针对特定测试问题持续训练；这是一种专注于为特定问题生成单一最优解而非追求平均泛化性能的持续学习形式。关键实验结果表明，TTT-Discover在多个领域确立了新的最优性能：它在埃尔德什最小重叠问题和自相关不等式上取得突破，生成的GPU内核比先前最优方案快达两倍，在过去的AtCoder算法竞赛中表现优异，并改善了单细胞分析中的去噪效果，所有解决方案均经专家验证，且使用开源模型以低成本实现。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics</div>
<div class="meta-line">Authors: Giovanni Briglia, Francesco Fabiano, Stefano Mariani</div>
<div class="meta-line">First: 2025-08-18T11:26:20+00:00 · Latest: 2026-02-05T18:00:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12840v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.12840v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for reasoning about both the physical world and the beliefs of agents, with applications in domains where information flow and awareness among agents are critical. The richness of MEP requires states to be represented as Kripke structures, i.e., directed labeled graphs. This representation limits the applicability of existing heuristics, hindering the scalability of epistemic solvers, which must explore an exponential search space without guidance, resulting often in intractability. To address this, we exploit Graph Neural Networks (GNNs) to learn patterns and relational structures within epistemic states, to guide the planning process. GNNs, which naturally capture the graph-like nature of Kripke models, allow us to derive meaningful estimates of state quality -- e.g., the distance from the nearest goal -- by generalizing knowledge obtained from previously solved planning instances. We integrate these predictive heuristics into an epistemic planning pipeline and evaluate them against standard baselines, showing improvements in the scalability of multi-agent epistemic planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图神经网络启发式方法扩展多智能体认知规划</div>
<div class="mono" style="margin-top:8px">多智能体认知规划是一种用于推理物理世界与智能体信念的自主规划框架，适用于信息流与智能体间认知至关重要的领域。其复杂性要求状态必须表示为克里普克结构（即有向标记图），这种表示方式限制了现有启发式方法的适用性，阻碍了认知求解器的可扩展性——此类求解器需在无引导情况下探索指数级搜索空间，常导致计算不可行。为此，我们利用图神经网络学习认知状态中的模式与关系结构，以指导规划过程。图神经网络天然契合克里普克模型的图结构特性，能够通过泛化已解决规划实例的知识，推导出有意义的评估指标（如当前状态与最近目标的距离）。我们将这些预测性启发式方法集成至认知规划流程，并与标准基线进行对比评估，结果表明多智能体认知规划的可扩展性获得了显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-agent epistemic planning (MEP) is crucial for domains requiring reasoning about agents&#x27; beliefs, but its scalability is hindered by the exponential search space of Kripke structures and the lack of effective heuristics. To address this, the authors propose using graph neural networks (GNNs) to learn relational patterns from solved instances and derive heuristic estimates, such as distance to goal, which are then integrated into the planning process. Experimental evaluation demonstrates that this GNN-based heuristic approach improves the scalability of epistemic solvers compared to standard baselines.</div>
<div class="mono" style="margin-top:8px">多智能体认知规划（MEP）对于需要同时推理物理状态和智能体信念的领域至关重要，但其可扩展性受到指数级搜索空间以及缺乏针对其基于图的克里普克状态表示的有效启发式方法的限制。为解决这一问题，作者提出使用图神经网络（GNN）从已解决的规划实例中学习关系模式，并推导出预测性启发式（如到目标的估计距离），然后将其集成到认知规划求解器中。实验评估表明，这种基于GNN的启发式方法相比标准基线方法，提高了求解器的可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Group-Adaptive Adversarial Learning for Robust Fake News Detection Against Malicious Comments</div>
<div class="meta-line">Authors: Zhao Tong, Chunlin Gong, Yimeng Gu, Haichao Shi, Qiang Liu, Shu Wu, Xiao-Yu Zhang</div>
<div class="meta-line">First: 2025-10-10T04:39:57+00:00 · Latest: 2026-02-05T17:52:31+00:00</div>
<div class="meta-line">Comments: 10 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09712v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.09712v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online fake news profoundly distorts public judgment and erodes trust in social platforms. While existing detectors achieve competitive performance on benchmark datasets, they remain notably vulnerable to malicious comments designed specifically to induce misclassification. This evolving threat landscape necessitates detection systems that simultaneously prioritize predictive accuracy and structural robustness. However, current detectors often fail to generalize across diverse and novel comment attack patterns. To bridge this gap, we propose AdComment, an adaptive adversarial training framework for robustness enhancement against diverse malicious comments. Based on cognitive psychology, we categorize adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation, and leverage LLMs to synthesize diverse, category-specific perturbations. Central to our framework is an InfoDirichlet Resampling (IDR) mechanism that dynamically adjusts malicious comment proportions during training, thereby steering optimization toward the model&#x27;s most susceptible regions. Experimental results demonstrate that our approach achieves state-of-the-art performance on three benchmark datasets, improving the F1 scores by 17.9%, 14.5% and 9.0%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向恶意评论的鲁棒虚假新闻检测：群体自适应对抗学习</div>
<div class="mono" style="margin-top:8px">网络虚假新闻严重扭曲公众判断并侵蚀对社交平台的信任。现有检测器在基准数据集上虽具竞争力，但面对专门诱导误判的恶意评论时仍显著脆弱。这一不断演变的威胁态势要求检测系统同时兼顾预测准确性与结构鲁棒性。然而，现有检测器往往难以泛化至多样新颖的评论攻击模式。为填补这一空白，我们提出AdComment——一种针对多样化恶意评论的适应性对抗训练框架。基于认知心理学，我们将对抗性评论划分为事实扭曲、逻辑混淆和情感操纵三类，并利用大语言模型合成多样化的类别特异性扰动。本框架的核心是信息狄利克雷重采样机制，该机制在训练过程中动态调整恶意评论比例，从而将优化导向模型最敏感区域。实验结果表明，我们的方法在三个基准数据集上均取得最先进性能，F1分数分别提升17.9%、14.5%和9.0%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the vulnerability of existing fake news detectors to malicious comments designed to induce misclassification, necessitating systems that balance predictive accuracy with structural robustness. The proposed method, AdComment, is an adaptive adversarial training framework that categorizes adversarial comments into Fact Distortion, Logical Confusion, and Emotional Manipulation based on cognitive psychology, uses LLMs to synthesize category-specific perturbations, and employs an InfoDirichlet Resampling mechanism to dynamically adjust malicious comment proportions during training. Key experimental findings show state-of-the-art performance on three benchmark datasets, with F1 score improvements of 17.9%, 14.5%, and 9.0% respectively.</div>
<div class="mono" style="margin-top:8px">本研究动机在于现有虚假新闻检测器易受诱导错误分类的恶意评论攻击，需要构建兼顾预测准确性和结构鲁棒性的系统。所提出的方法AdComment是一种自适应对抗训练框架，首先基于认知心理学将对抗性评论分为事实扭曲、逻辑混淆和情感操纵三类，并利用大语言模型合成多样化的类别特定扰动，同时采用信息狄利克雷重采样机制在训练中动态调整恶意评论比例以优化模型最脆弱区域。主要实验结果表明，该方法在三个基准数据集上取得了最先进的性能，分别将F1分数提高了17.9%、14.5%和9.0%。</div>
</details>
</div>
<div class="card">
<div class="title">Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems</div>
<div class="meta-line">Authors: Tony Feng, Trieu Trinh, Garrett Bingham, Jiwon Kang, Shengtong Zhang, Sang-hyun Kim, Kevin Barreto, Carl Schildkraut, Junehyuk Jung, Jaehyeon Seo, Carlo Pagano, Yuri Chervonyi, Dawsen Hwang, Kaiying Hou, Sergei Gukov, Cheng-Chiang Tsai, Hyunwoo Choi, Youngbeom Jin, Wei-Yuan Li, Hao-An Wu, Ruey-An Shiu, Yu-Sheng Shih, Quoc V. Le, Thang Luong</div>
<div class="meta-line">First: 2026-01-29T23:15:36+00:00 · Latest: 2026-02-05T17:48:07+00:00</div>
<div class="meta-line">Comments: Reclassify Erdos-935 as Independent Rediscovery, bringing the number of autonomous solutions down to 5. (Explanation in Addendum 4.1) Elaborate on Footnote 3. Slightly reword various phrases in the Introduction in response to feedback</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22401v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.22401v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled &#x27;Open&#x27; in Bloom&#x27;s Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked &#x27;Open&#x27; in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the &#x27;Open&#x27; status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of &#x27;&#x27;subconscious plagiarism&#x27;&#x27; by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Gemini的半自主数学发现：以埃尔德什问题为例的案例研究</div>
<div class="mono" style="margin-top:8px">本文通过使用Gemini系统对Bloom埃尔德什问题数据库中标记为&#x27;未解决&#x27;的700个猜想进行系统性评估，开展了一项半自主数学发现的案例研究。我们采用混合方法：先通过AI驱动的自然语言验证缩小搜索范围，再由人类专家评估正确性与创新性。我们处理了数据库中标记为&#x27;未解决&#x27;的13个问题：其中5个通过看似新颖的自主解决方案解决，8个通过识别现有文献中的已有解决方案解决。研究结果表明，这些问题之所以标记为&#x27;未解决&#x27;更多是由于隐蔽性而非难度。我们还识别并讨论了大规模应用AI处理数学猜想时产生的问题，重点指出文献识别的困难以及AI&#x27;潜意识抄袭&#x27;的风险。最后对AI辅助研究埃尔德什问题的经验教训进行了反思。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores semi-autonomous mathematics discovery by systematically evaluating 700 conjectures labeled &#x27;Open&#x27; in Bloom&#x27;s Erdős Problems database using the Gemini AI. The method combines AI-driven natural language verification to filter the search space with human expert evaluation to assess correctness and novelty. The research addressed 13 &#x27;Open&#x27; problems: 5 were resolved through seemingly novel autonomous solutions, while 8 were identified as having existing solutions in the literature, suggesting their &#x27;Open&#x27; status was due to obscurity rather than inherent difficulty. The work also highlights challenges in applying AI at scale, including difficulties in literature identification and risks of subconscious plagiarism.</div>
<div class="mono" style="margin-top:8px">本研究探索了半自主数学发现，通过应用Gemini人工智能系统评估Bloom的埃尔德什问题数据库中700个标记为&#x27;未解决&#x27;的猜想。方法结合了人工智能驱动的自然语言验证以筛选问题集，以及随后的人类专家评估以确定正确性和新颖性。该研究解决了13个先前未解决的问题，发现其中5个通过看似新颖的自主解决方案得以解决，8个通过识别现有文献中的解决方案得以解决，表明其未解决状态是由于 obscurity 而非固有难度。这项工作还凸显了人工智能大规模应用于数学领域的挑战，例如文献识别困难以及人工智能&#x27;潜意识抄袭&#x27;的风险。</div>
</details>
</div>
<div class="card">
<div class="title">Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025</div>
<div class="meta-line">Authors: Samar Ansari</div>
<div class="meta-line">First: 2026-02-05T17:43:35+00:00 · Latest: 2026-02-05T17:43:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05930v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05930v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly used in academic writing workflows, yet they frequently hallucinate by generating citations to sources that do not exist. This study analyzes 100 AI-generated hallucinated citations that appeared in papers accepted by the 2025 Conference on Neural Information Processing Systems (NeurIPS), one of the world&#x27;s most prestigious AI conferences. Despite review by 3-5 expert researchers per paper, these fabricated citations evaded detection, appearing in 53 published papers (approx. 1% of all accepted papers). We develop a five-category taxonomy that classifies hallucinations by their failure mode: Total Fabrication (66%), Partial Attribute Corruption (27%), Identifier Hijacking (4%), Placeholder Hallucination (2%), and Semantic Hallucination (1%). Our analysis reveals a critical finding: every hallucination (100%) exhibited compound failure modes. The distribution of secondary characteristics was dominated by Semantic Hallucination (63%) and Identifier Hijacking (29%), which often appeared alongside Total Fabrication to create a veneer of plausibility and false verifiability. These compound structures exploit multiple verification heuristics simultaneously, explaining why peer review fails to detect them. The distribution exhibits a bimodal pattern: 92% of contaminated papers contain 1-2 hallucinations (minimal AI use) while 8% contain 4-13 hallucinations (heavy reliance). These findings demonstrate that current peer review processes do not include effective citation verification and that the problem extends beyond NeurIPS to other major conferences, government reports, and professional consulting. We propose mandatory automated citation verification at submission as an implementable solution to prevent fabricated citations from becoming normalized in scientific literature.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>精英同行评审中的复合欺骗：NeurIPS 2025百例伪造引文的失效模式分类研究</div>
<div class="mono" style="margin-top:8px">大语言模型在学术写作流程中的应用日益广泛，但其常产生指向不存在文献的幻觉引文。本研究分析了2025年神经信息处理系统大会（NeurIPS）录用论文中出现的100例AI生成的幻觉引文。尽管每篇论文均经过3-5位专家评审，这些伪造引文仍未被发现，出现在53篇已发表论文中（约占录用论文总数的1%）。我们建立了五类失效模式分类体系：完全虚构（66%）、部分属性篡改（27%）、标识符劫持（4%）、占位符幻觉（2%）和语义幻觉（1%）。分析揭示关键发现：所有幻觉引文（100%）均呈现复合失效模式。次要特征分布以语义幻觉（63%）和标识符劫持（29%）为主，常与完全虚构结合形成表面合理性与虚假可验证性。这些复合结构同时利用多重验证启发式规则，解释了同行评审难以检测的原因。分布呈现双峰模式：92%的受影响论文含1-2处幻觉（AI使用最少），8%含4-13处幻觉（重度依赖）。研究表明当前同行评审缺乏有效引文验证机制，且该问题已从NeurIPS蔓延至其他重要会议、政府报告及专业咨询领域。我们建议在投稿阶段强制实施自动化引文验证，作为防止伪造引文在科学文献中常态化的可行解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the failure of peer review to detect AI-generated hallucinated citations in academic papers, motivated by the increasing use of large language models (LLMs) in scholarly writing and the risk of fabricated references entering the literature. The method involves analyzing 100 such fabricated citations from papers accepted at NeurIPS 2025, developing a taxonomy that classifies hallucinations into five failure modes: Total Fabrication, Partial Attribute Corruption, Identifier Hijacking, Placeholder Hallucination, and Semantic Hallucination. Key experimental findings reveal that all hallucinations exhibited compound failure modes, with 100% combining multiple types, such as Total Fabrication often paired with Semantic Hallucination or Identifier Hijacking to enhance plausibility; these fabricated citations appeared in 53 papers (about 1% of accepted papers), with a bimodal distribution showing most contaminated papers had 1-2 hallucinations while a small subset had 4-13, indicating varying levels of AI reliance.</div>
<div class="mono" style="margin-top:8px">本研究探讨了同行评审未能检测出学术论文中人工智能生成的虚假引用问题，其动机在于大型语言模型在学术写作中的使用日益增多及其倾向于捏造不存在来源的趋势。方法包括分析NeurIPS 2025会议录用论文中的100个此类虚假引用，并据此开发了一个五类分类法，按失败模式（如完全捏造和部分属性篡改）对幻觉进行分类。主要实验结果表明，所有幻觉都表现出复合失败模式，语义幻觉和标识符劫持等次要特征常与完全捏造结合以增强可信度；这些虚假引用出现在约1%的录用论文中，且呈现双峰分布：大多数受污染论文包含1-2个幻觉，而一小部分包含4-13个，这表明了对人工智能的依赖程度各异。</div>
</details>
</div>
<div class="card">
<div class="title">When Are Two RLHF Objectives the Same?</div>
<div class="meta-line">Authors: Madhava Gaikwad</div>
<div class="meta-line">First: 2025-09-14T14:42:39+00:00 · Latest: 2026-02-05T17:34:59+00:00</div>
<div class="meta-line">Comments: 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.11298v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.11298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The preference optimization literature contains many proposed objectives, often presented as distinct improvements. We introduce Opal, a canonicalization algorithm that determines whether two preference objectives are algebraically equivalent by producing either a canonical form or a concrete witness of non-equivalence. Applying Opal reveals that many widely used methods optimize the same underlying objective, while others are provably distinct. For example, batch normalization can cause the same response pair to receive different gradients depending on batch composition. We identify a small set of structural mechanisms that give rise to genuinely different objectives; most remaining differences are reparameterizations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时两个RLHF目标函数等价？</div>
<div class="mono" style="margin-top:8px">偏好优化文献提出了众多目标函数，常被表述为不同的改进。我们提出Opal——一种规范化算法，通过生成规范形式或提供不等价的具体反例，判定两个偏好目标函数是否代数等价。应用Opal分析表明，许多广泛使用的方法实际优化的是相同的底层目标，而另一些方法则被证明存在本质差异。例如，批归一化可能导致同一响应对因批次构成不同而获得不同的梯度。我们识别出一组能产生本质不同目标函数的结构性机制；其余多数差异仅为参数重表述。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the proliferation of preference optimization objectives in reinforcement learning from human feedback (RLHF) by investigating when different objectives are fundamentally equivalent. The authors introduce Opal, a canonicalization algorithm that determines algebraic equivalence between objectives by producing either a canonical form or a concrete counterexample. Applying Opal reveals that many widely used methods optimize the same underlying objective, while others are provably distinct due to structural mechanisms like batch normalization, which causes identical response pairs to receive different gradients depending on batch composition. The analysis identifies that most differences between objectives are merely reparameterizations, with only a small set of structural mechanisms giving rise to genuinely different objectives.</div>
<div class="mono" style="margin-top:8px">本研究动机源于人类反馈强化学习（RLHF）中偏好优化目标的激增，这些目标常被呈现为不同的改进，尽管可能存在潜在的相似性。方法引入了Opal，一种规范化算法，通过生成规范形式或非等价的具体见证，来判断两个偏好目标是否代数等价。关键实验结果表明，许多广泛使用的方法优化了相同的基础目标，而其他方法则由于结构机制（如批归一化导致梯度随批次组成而变化）而被证明是不同的，大多数剩余差异仅仅是重参数化。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem</div>
<div class="meta-line">Authors: Eva Andrés</div>
<div class="meta-line">First: 2026-02-05T17:32:14+00:00 · Latest: 2026-02-05T17:32:14+00:00</div>
<div class="meta-line">Comments: 22 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05920v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses the Capacitated Vehicle Routing Problem (CVRP) by comparing classical and quantum Reinforcement Learning (RL) approaches. An Advantage Actor-Critic (A2C) agent is implemented in classical, full quantum, and hybrid variants, integrating transformer architectures to capture the relationships between vehicles, clients, and the depot through self- and cross-attention mechanisms. The experiments focus on multi-vehicle scenarios with capacity constraints, considering 20 clients and 4 vehicles, and are conducted over ten independent runs. Performance is assessed using routing distance, route compactness, and route overlap. The results show that all three approaches are capable of learning effective routing policies. However, quantum-enhanced models outperform the classical baseline and produce more robust route organization, with the hybrid architecture achieving the best overall performance across distance, compactness, and route overlap. In addition to quantitative improvements, qualitative visualizations reveal that quantum-based models generate more structured and coherent routing solutions. These findings highlight the potential of hybrid quantum-classical reinforcement learning models for addressing complex combinatorial optimization problems such as the CVRP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Transformer的量子强化学习求解带容量约束的车辆路径问题</div>
<div class="mono" style="margin-top:8px">本文通过比较经典与量子强化学习方法，研究带容量约束的车辆路径问题。研究实现了经典、全量子和混合三种优势演员-评论家智能体变体，并集成Transformer架构，通过自注意力与交叉注意力机制捕捉车辆、客户与配送中心之间的关系。实验聚焦多车辆容量约束场景，设置20个客户点和4辆车辆，进行十次独立运行。性能评估采用路径距离、路径紧凑度和路径重叠度三项指标。结果表明，三种方法均能学习有效的路径策略，但量子增强模型在经典基准上表现更优，能生成更稳健的路径规划，其中混合架构在距离、紧凑度和路径重叠度方面综合表现最佳。除量化改进外，定性可视化显示量子模型能生成更具结构性和连贯性的路径方案。这些发现凸显了混合量子-经典强化学习模型在解决CVRP等复杂组合优化问题方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to tackle the complex combinatorial optimization challenge of the Capacitated Vehicle Routing Problem (CVRP) by exploring the potential of quantum-enhanced methods. The method implements an Advantage Actor-Critic (A2C) reinforcement learning agent in classical, fully quantum, and hybrid variants, integrating transformer architectures with self- and cross-attention mechanisms to model relationships between vehicles, clients, and the depot. Experimental results on multi-vehicle scenarios with 20 clients and 4 vehicles, evaluated over ten runs using metrics like routing distance, compactness, and overlap, demonstrate that all three approaches learn effective policies. Crucially, the quantum-enhanced models, particularly the hybrid architecture, outperform the classical baseline, yielding shorter distances, more robust route organization, and qualitatively more structured routing solutions.</div>
<div class="mono" style="margin-top:8px">本研究针对带容量约束的车辆路径问题（CVRP），旨在探索量子增强方法在复杂组合优化中的潜力。作者实现了经典、全量子和混合三种变体的优势演员-评论家（A2C）强化学习智能体，并集成具有自注意力和交叉注意力机制的Transformer架构，以建模车辆、客户和仓库之间的关系。在包含20个客户和4辆车的多车辆场景上进行十次独立运行的实验评估表明，所有方法都能学习到有效的策略，但量子增强模型，特别是混合架构，在路径距离、路径紧凑性和路径重叠方面优于经典基线，产生了更结构化、更稳健的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">RAG4Tickets: AI-Powered Ticket Resolution via Retrieval-Augmented Generation on JIRA and GitHub Data</div>
<div class="meta-line">Authors: Mohammad Baqar</div>
<div class="meta-line">First: 2025-10-09T16:33:00+00:00 · Latest: 2026-02-05T17:25:30+00:00</div>
<div class="meta-line">Comments: 13 Pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08667v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08667v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern software teams frequently encounter delays in resolving recurring or related issues due to fragmented knowledge scattered across JIRA tickets, developer discussions, and GitHub pull requests (PRs). To address this challenge, we propose a Retrieval-Augmented Generation (RAG) framework that integrates Sentence-Transformers for semantic embeddings with FAISS-based vector search to deliver context-aware ticket resolution recommendations. The approach embeds historical JIRA tickets, user comments, and linked PR metadata to retrieve semantically similar past cases, which are then synthesized by a Large Language Model (LLM) into grounded and explainable resolution suggestions. The framework contributes a unified pipeline linking JIRA and GitHub data, an embedding and FAISS indexing strategy for heterogeneous software artifacts, and a resolution generation module guided by retrieved evidence. Experimental evaluation using precision, recall, resolution time reduction, and developer acceptance metrics shows that the proposed system significantly improves resolution accuracy, fix quality, and knowledge reuse in modern DevOps environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAG4Tickets：基于JIRA与GitHub数据的检索增强生成式AI工单解决方案</div>
<div class="mono" style="margin-top:8px">现代软件团队常因知识分散在JIRA工单、开发者讨论和GitHub拉取请求中，导致重复或相关问题解决延迟。为此，我们提出一种检索增强生成框架，通过Sentence-Transformers语义嵌入与FAISS向量搜索相结合，提供上下文感知的工单解决建议。该方法嵌入历史JIRA工单、用户评论及关联PR元数据，检索语义相似的过往案例，并由大语言模型综合生成基于证据、可解释的解决方案。该框架贡献包括：连接JIRA与GitHub数据的统一流程、面向异构软件制品的嵌入与FAISS索引策略、基于检索证据的解决方案生成模块。通过精确率、召回率、解决时间缩减及开发者接受度等指标的实验评估表明，该系统能显著提升现代DevOps环境中的解决准确率、修复质量与知识复用效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address delays in resolving recurring software issues caused by fragmented knowledge across JIRA tickets and GitHub pull requests, this research proposes a Retrieval-Augmented Generation (RAG) framework. The method integrates Sentence-Transformers for creating semantic embeddings and FAISS for vector search to retrieve similar past cases from historical JIRA tickets, comments, and PR metadata, which are then synthesized by a Large Language Model into resolution suggestions. Experimental evaluation using precision, recall, resolution time reduction, and developer acceptance metrics demonstrates that the system significantly improves resolution accuracy, fix quality, and knowledge reuse.</div>
<div class="mono" style="margin-top:8px">为解决因JIRA工单和GitHub拉取请求中知识碎片化导致的重复软件问题解决延迟，本研究提出了一个检索增强生成（RAG）框架。该方法集成Sentence-Transformers创建语义嵌入，并利用FAISS进行向量搜索，从历史的JIRA工单、评论和PR元数据中检索相似过往案例，随后通过大语言模型综合生成解决建议。使用精确率、召回率、解决时间减少和开发者接受度等指标的实验评估表明，该系统显著提高了解决准确性、修复质量和知识复用率。</div>
</details>
</div>
<div class="card">
<div class="title">SelfReflect: Can LLMs Communicate Their Internal Answer Distribution?</div>
<div class="meta-line">Authors: Michael Kirchhof, Luca Füger, Adam Goliński, Eeshan Gunesh Dhekane, Arno Blaas, Seong Joon Oh, Sinead Williamson</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-26T17:59:53+00:00 · Latest: 2026-02-05T17:25:06+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20295v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.20295v4">PDF</a> · <a href="https://github.com/apple/ml-selfreflect">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The common approach to communicate a large language model&#x27;s (LLM) uncertainty is to add a percentage number or a hedging word to its response. But is this all we can do? Instead of generating a single answer and then hedging it, an LLM that is fully transparent to the user needs to be able to reflect on its internal belief distribution and output a summary of all options it deems possible, and how likely they are. To test whether LLMs possess this capability, we develop the SelfReflect metric, an information-theoretic distance between a given summary and a distribution over answers. In interventional and human studies, we find that SelfReflect indicates even slight deviations, yielding a fine measure of faithfulness between a summary string and an LLM&#x27;s actual internal distribution over answers. With SelfReflect, we make a resounding negative observation: modern LLMs are, across the board, incapable of revealing what they are uncertain about, neither through reasoning, nor chains-of-thoughts, nor explicit finetuning. However, we do find that LLMs are able to generate faithful summaries of their uncertainties if we help them by sampling multiple outputs and feeding them back into the context. This simple approach shines a light at the universal way of communicating LLM uncertainties whose future development the SelfReflect score enables. To support the development of this universal form of LLM uncertainties, we publish the code that implements our metric for arbitrary LLMs under https://github.com/apple/ml-selfreflect .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SelfReflect：大语言模型能否传达其内部答案分布？</div>
<div class="mono" style="margin-top:8px">当前传达大语言模型不确定性的常见方法是在其回答后添加百分比数值或模糊修饰词。但这是否足够？一个对用户完全透明的模型，不应仅生成单一答案再加以修饰，而需能反思其内部信念分布，输出所有可能选项及其概率的摘要。为测试大语言模型是否具备此能力，我们开发了SelfReflect指标——一种衡量给定摘要与答案分布之间信息理论距离的度量。通过干预实验和人类研究，我们发现SelfReflect能检测细微偏差，精准衡量摘要字符串与模型真实内部答案分布的一致性。基于此指标，我们得出明确否定结论：现代大语言模型普遍无法通过推理、思维链或显式微调揭示其不确定性。然而，若通过采样多个输出并反馈至上下文，模型能生成忠实反映不确定性的摘要。这一简单方法为构建通用的大语言模型不确定性传达机制指明方向，而SelfReflect评分将推动该领域的未来发展。为支持此通用形式的研究，我们在https://github.com/apple/ml-selfreflect开源了适用于任意大语言模型的指标实现代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether large language models (LLMs) can transparently communicate their internal uncertainty by summarizing their belief distribution over possible answers, rather than just hedging a single output. The authors introduce the SelfReflect metric, an information-theoretic measure to quantify the faithfulness between a model&#x27;s generated summary and its actual internal answer distribution. Experimental results reveal that modern LLMs are fundamentally incapable of accurately revealing their uncertainties through reasoning or fine-tuning, but they can generate faithful summaries when assisted by sampling multiple outputs and providing them as context.</div>
<div class="mono" style="margin-top:8px">本研究探讨大型语言模型（LLM）能否通过总结其对可能答案的信念分布来透明地传达其内部不确定性，而非仅仅对单一输出进行模糊修饰。作者提出了SelfReflect指标，这是一种信息论度量，用于量化模型生成的摘要与其实际内部答案分布之间的忠实度。实验结果表明，现代LLM通过推理或微调根本无法准确揭示其不确定性，但在通过采样多个输出并将其反馈为上下文进行辅助时，能够生成忠实的摘要。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0629.html">20260206_0629</a>
<a href="archive/20260206_0531.html">20260206_0531</a>
<a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
