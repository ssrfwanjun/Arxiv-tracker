<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 03:34</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0334</div>
    <div class="row"><div class="card">
<div class="title">Iterative Refinement Improves Compositional Image Generation</div>
<div class="meta-line">Authors: Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak</div>
<div class="meta-line">First: 2026-01-21T18:59:40+00:00 · Latest: 2026-01-21T18:59:40+00:00</div>
<div class="meta-line">Comments: Project webpage: https://iterative-img-gen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15286v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iterative-img-gen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代优化提升组合式图像生成质量</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型已取得显著进展，但在处理需要同时满足多对象、关系和属性的复杂提示时仍面临挑战。现有的推理时策略（如带验证器的并行采样或单纯增加去噪步数）虽能提升提示对齐度，但在需要满足多重约束的复杂组合场景中仍显不足。受大语言模型中思维链推理成功的启发，我们提出一种迭代式测试时策略：T2I模型在视觉语言模型作为循环评判者的反馈指导下，通过多步骤渐进优化生成结果。该方法无需外部工具或先验知识，可灵活适配各类图像生成器与视觉语言模型。实验表明，该方法在多个基准测试中均取得稳定提升：在ConceptMix（k=7）的全正确率提升16.9%，在T2I-CompBench（3D空间类别）提升13.8%，在Visual Jenga场景解构任务中提升12.5%（均与计算量匹配的并行采样对比）。除量化指标外，迭代优化通过将复杂提示分解为序列修正，生成结果更具忠实度——人工评估者对该方法的偏好率达58.7%（基线为41.3%）。这些发现共同表明，迭代式自校正可作为组合式图像生成的普适性优化原则。完整结果与可视化案例详见项目网页。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image models often fail to accurately generate images from complex prompts involving multiple objects and relations. To address this, the authors propose an iterative refinement strategy where a vision-language model provides feedback to guide the image generator across multiple denoising steps, decomposing the complex prompt into sequential corrections. Experimental results show significant improvements, including a 16.9% increase in all-correct rate on the ConceptMix benchmark and a 58.7% human preference rate over a parallel sampling baseline, demonstrating the effectiveness of iterative self-correction for compositional generation.</div>
<div class="mono" style="margin-top:8px">文本到图像模型在处理涉及多个对象和关系的复杂提示时常常表现不佳。为此，研究者提出了一种迭代优化策略，利用视觉语言模型提供反馈，在多步去噪过程中引导图像生成器，将复杂提示分解为顺序修正。实验结果表明该方法带来了显著提升，如在ConceptMix基准测试上的全正确率提高了16.9%，并在人类评估中以58.7%的偏好率优于并行采样基线，证明了迭代自校正对于组合式图像生成的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Generation Model for the Embodied World</div>
<div class="meta-line">Authors: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-21T18:59:18+00:00 · Latest: 2026-01-21T18:59:18+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15282v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/ReVidgen/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/ReVidgen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向具身世界的视频生成模型再思考</div>
<div class="mono" style="margin-top:8px">视频生成模型显著推动了具身智能的发展，为生成捕捉物理世界中感知、推理与行动的多样化机器人数据开辟了新可能。然而，合成能准确反映真实机器人交互的高质量视频仍具挑战，且缺乏标准化基准限制了公平比较与进展。为此，我们提出了综合性机器人基准RBench，旨在通过五个任务领域和四种不同具身形态评估面向机器人的视频生成。该基准通过可复现的子指标（包括结构一致性、物理合理性和动作完整性）评估任务级正确性与视觉保真度。对25个代表性模型的评估揭示了它们在生成物理真实机器人行为方面的显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达0.96，验证了其有效性。尽管RBench为识别这些不足提供了必要视角，但实现物理真实性需超越评估层面，解决高质量训练数据严重短缺的问题。基于这些洞见，我们提出了精炼的四阶段数据流程，由此构建了RoVid-X——目前最大的开源机器人视频生成数据集，包含400万个标注视频片段，涵盖数千项任务，并配有全面的物理属性标注。这一评估与数据协同的生态系统为视频模型的严谨评估与可扩展训练奠定了坚实基础，加速具身AI向通用智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of synthesizing high-quality videos that accurately reflect real-world robotic interactions and the lack of a standardized benchmark for fair comparison. The method introduces a comprehensive robotics benchmark, RBench, to evaluate robot-oriented video generation across multiple task domains and embodiments, assessing task correctness and visual fidelity through specific sub-metrics. Key experimental findings reveal significant deficiencies in existing models for generating physically realistic robot behaviors, and the benchmark demonstrates a high correlation (Spearman coefficient of 0.96) with human evaluations; additionally, to address data scarcity, the work develops a refined data pipeline to create RoVid-X, a large-scale annotated dataset for video generation.</div>
<div class="mono" style="margin-top:8px">该研究旨在提升视频生成模型在具身智能中的应用，因为现有模型难以生成物理真实的机器人交互，且缺乏标准化评估。方法上提出了RBench这一综合性机器人基准，包含任务正确性和视觉保真度指标，并通过改进的数据流程构建了大规模标注数据集RoVid-X。主要实验结果表明，评估的25个模型在生成物理合理的机器人行为方面存在显著不足，而RBench与人类评估的斯皮尔曼相关系数达到0.96，验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs</div>
<div class="meta-line">Authors: Christoph Bartmann, Johannes Schimunek, Mykyta Ielanskyi, Philipp Seidl, Günter Klambauer, Sohvi Luukkonen</div>
<div class="meta-line">First: 2026-01-21T18:58:01+00:00 · Latest: 2026-01-21T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15279v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A molecule&#x27;s properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolecularIQ：通过分子图符号验证表征化学推理能力</div>
<div class="mono" style="margin-top:8px">分子的性质根本上由其分子图所编码的组成与结构决定。因此，对分子性质进行推理需要解析和理解分子图的能力。大语言模型正日益应用于化学领域，处理诸如分子名称转换、描述生成、文本引导生成以及性质或反应预测等任务。现有基准大多侧重于通用化学知识，依赖文献或存在泄漏或偏差风险的替代标签，或将评估简化为多项选择题。我们提出MolecularIQ，一个专注于符号可验证任务的分子结构推理基准。MolecularIQ支持对分子图推理进行细粒度评估，并揭示将模型失败定位到特定任务和分子结构的能力模式。这为当前化学大语言模型的优势与局限提供了可操作的见解，并指导开发能够对分子结构进行可靠推理的模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate how well Large Language Models (LLMs) can reason about molecular properties, which are fundamentally determined by a molecule&#x27;s graph structure, as existing benchmarks often rely on general knowledge or multiple-choice formats that may introduce bias. The method introduces MolecularIQ, a benchmark composed of symbolically verifiable tasks specifically designed to assess fine-grained reasoning capabilities over molecular graphs. Key experimental findings show that this approach reveals distinct patterns in model capabilities, localizing failures to specific tasks and molecular substructures, thereby providing actionable insights for developing more faithful molecular reasoning models.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要评估大语言模型（LLMs）对分子特性进行推理的能力，因为分子特性根本上由其图结构决定，而现有基准测试通常依赖通用知识、可能存在偏见的标签或简化的多项选择题形式。方法上，研究引入了MolecularIQ基准，该基准由一系列可符号验证的任务组成，要求解析和理解分子图，从而实现对推理能力的细粒度分析。关键的实验结果揭示了不同的能力模式，将模型失败定位到特定任务和分子结构，从而为当前化学大语言模型的优势和局限性提供了可操作的见解，并指导开发更具忠实推理能力的模型。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions</div>
<div class="meta-line">Authors: Yiran Hu, Huanghai Liu, Chong Wang, Kunran Li, Tien-Hsuan Wu, Haitao Li, Xinran Xu, Siqing Huo, Weihang Su, Ning Zheng, Siyuan Zheng, Qingyao Ai, Yun Liu, Renjun Bian, Yiqun Liu, Charles L. A. Clarke, Weixing Shen, Ben Kao</div>
<div class="meta-line">First: 2026-01-21T18:51:37+00:00 · Latest: 2026-01-21T18:51:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15267v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15267v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型在法律应用中的评估：挑战、方法与未来方向</div>
<div class="mono" style="margin-top:8px">大语言模型正日益融入法律应用，包括司法决策支持、法律实务辅助和面向公众的法律服务。尽管大语言模型在处理法律知识与任务方面展现出强大潜力，但其在实际法律场景中的部署引发了超越表层准确性的关键问题，涉及法律推理过程的严谨性以及公平性、可靠性等可信度议题。因此，对法律任务中大语言模型性能进行系统评估已成为其负责任应用的必要前提。本综述基于现实法律实践，梳理了评估法律任务大语言模型面临的核心挑战，分析了评估模型在法律领域表现的主要难点，包括结果正确性、推理可靠性与可信度。基于这些挑战，我们依据任务设计、数据集和评估指标对现有评估方法与基准进行了梳理与分类，进一步探讨当前方法应对这些挑战的程度，指出其局限性，并展望未来研究方向，以构建更贴近现实、更可靠且扎根法律实践的大语言模型评估框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The integration of large language models (LLMs) into legal applications such as judicial support and legal services necessitates systematic evaluation to address critical concerns beyond surface accuracy, including the soundness of legal reasoning and trustworthiness factors like fairness and reliability. This survey analyzes the key challenges in evaluating LLMs for legal tasks, focusing on outcome correctness, reasoning reliability, and trustworthiness, and reviews existing evaluation methods and benchmarks by categorizing them according to task design, datasets, and metrics. The findings highlight the limitations of current approaches and outline future research directions aimed at developing more realistic, reliable, and legally grounded evaluation frameworks for LLMs in the legal domain.</div>
<div class="mono" style="margin-top:8px">随着大语言模型越来越多地应用于司法决策支持、法律实务辅助等法律领域，其部署引发了超越表面准确性的关键关切，包括法律推理过程的严谨性以及公平性、可靠性等可信问题，因此系统评估其表现至关重要。本文基于真实法律实践，分析了评估大语言模型在法律任务中表现的主要难点，包括结果正确性、推理可靠性和可信度，并依据任务设计、数据集和评估指标对现有评估方法与基准进行了回顾和分类。研究进一步讨论了当前方法在应对这些挑战方面的局限性，并展望了未来研究方向，旨在为法律领域构建更贴近现实、可靠且基于法律根基的评估框架。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Stewardship of an LLM-Assisted Clinical Benchmark with Physician Oversight</div>
<div class="meta-line">Authors: Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati</div>
<div class="meta-line">First: 2025-12-22T18:59:34+00:00 · Latest: 2026-01-21T18:48:54+00:00</div>
<div class="meta-line">Comments: Project codebase: https://github.com/junzeye/validate-medcalc-labels</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19691v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19691v2">PDF</a> · <a href="https://github.com/junzeye/validate-medcalc-labels">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We examine the reliability of a widely used clinical AI benchmark whose reference labels were partially generated by LLMs, and find that a substantial fraction are clinically misaligned. We introduce a phased stewardship procedure to amplify the positive impact of physician experts&#x27; feedback and then demonstrate, via a controlled RL experiment, how uncaught label bias can materially affect downstream LLM evaluation and alignment. Our results demonstrate that partially LLM-generated labels can embed systemic errors that distort not only evaluation but also downstream model alignment. By adopting a hybrid oversight system, we can prioritize scarce expert feedback to maintain benchmarks as living, clinically-grounded documents. Ensuring this alignment is a prerequisite for the safe deployment of LLMs in high-stakes medical decision support.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于医师监督的大语言模型辅助临床基准的可扩展管理机制</div>
<div class="mono" style="margin-top:8px">本研究针对一项广泛使用的临床人工智能基准展开可靠性分析，发现其部分参考标签由大语言模型生成，且存在相当比例与临床实践不符的情况。我们提出分阶段管理流程，以放大医师专家反馈的积极影响，并通过受控强化学习实验证明未发现的标签偏差如何实质影响下游大语言模型评估与对齐。结果表明，部分由大语言模型生成的标签可能嵌入系统性误差，不仅扭曲评估结果，还会影响下游模型对齐。采用混合监督系统可优先分配稀缺的专家反馈资源，使基准保持为动态更新且扎根临床的文档体系。确保这种对齐是实现大语言模型在高风险医疗决策支持场景中安全部署的前提条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reliability of a widely used clinical AI benchmark, revealing that a substantial fraction of its reference labels, which were partially generated by large language models (LLMs), are clinically misaligned. To address this, the authors introduce a phased stewardship procedure that amplifies the impact of physician expert feedback, and through a controlled reinforcement learning experiment, they demonstrate how uncaught label bias can materially affect downstream LLM evaluation and alignment. The key experimental findings show that partially LLM-generated labels can embed systemic errors that distort not only evaluation but also downstream model alignment, and that a hybrid oversight system can effectively prioritize scarce expert feedback to maintain benchmarks as clinically-grounded documents.</div>
<div class="mono" style="margin-top:8px">本研究调查了一个广泛使用的临床AI基准的可靠性，发现其部分由大语言模型生成的参考标签存在大量临床偏差。为解决此问题，作者引入了一个分阶段的监管流程，旨在放大有限医师专家反馈的影响，并通过一项受控的强化学习实验证明，此类未被发现的标签偏差会实质性地扭曲下游大语言模型的评估与对齐。关键实验结果表明，部分由大语言模型生成的标签会嵌入系统性错误，影响基准准确性及后续模型训练，而所提出的混合监督系统能有效优先利用专家输入，以维持基于临床事实的基准。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI</div>
<div class="meta-line">Authors: Haocheng Lin</div>
<div class="meta-line">First: 2025-12-09T20:25:24+00:00 · Latest: 2026-01-21T18:42:26+00:00</div>
<div class="meta-line">Comments: Improved structure and clarity of the introduction and literature review; explicit articulation of the paper&#x27;s contributions; refined the integration of AI across labour, UBI, and governance</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11893v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11893v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid expansion of generative artificial intelligence (AI) is transforming work, creativity, and economic security in ways that extend beyond automation and productivity. This paper examines four interconnected dimensions of contemporary AI deployment: (1) transformations in employment and task composition (2) unequal diffusion of AI across sectors and socio-demographic groups (3) the role of universal basic income (UBI) as a stabilising response to AI-induced volatility (4) the effects of model alignment and content governance on human creativity, autonomy, and decision-making
  Using a hybrid approach that integrates labour market task exposure modelling, sectoral diffusion analysis, policy review, and qualitative discourse critique, the study develops an Inclusive AI Governance Framework. It introduces Level 1.5 autonomy as a human centred design principle that preserves evaluative authority while enabling partial automation, and highlights evidence of creative regression and emergent sycophancy in newer model generations. The paper argues that UBI should be embedded within a broader socio-technical governance ecosystem encompassing skills development, proportional regulation, and creativity preservation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越自动化：生成式人工智能时代对工作、创造力与治理的再思考</div>
<div class="mono" style="margin-top:8px">生成式人工智能的快速扩张正在以超越自动化和生产效率的方式，深刻改变着工作形态、创造力模式与经济安全。本文审视当代人工智能部署的四个相互关联维度：（1）就业结构与任务构成的转型（2）人工智能在行业与社会人口群体间的不均衡扩散（3）全民基本收入作为应对人工智能引发波动的稳定机制（4）模型对齐与内容治理对人类创造力、自主权及决策机制的影响。研究采用融合劳动力市场任务暴露建模、行业扩散分析、政策评估与质性话语批判的混合方法，构建了包容性人工智能治理框架。提出以保留人类评估权威为前提、允许部分自动化的“1.5级自主性”人本设计原则，并揭示了新一代模型中出现的创造力退化与谄媚生成现象。论文主张将全民基本收入纳入涵盖技能发展、比例监管与创造力保护的社会技术治理生态系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how generative AI&#x27;s impact extends beyond simple automation, reshaping employment, creativity, and economic stability. The study employs a hybrid methodology combining labor market task exposure modeling, sectoral diffusion analysis, policy review, and qualitative discourse critique to analyze four key dimensions: shifts in work tasks, unequal AI adoption, the stabilizing potential of Universal Basic Income (UBI), and the effects of model alignment on human creativity. Key findings include the proposal of an Inclusive AI Governance Framework, the introduction of a &#x27;Level 1.5 autonomy&#x27; design principle to preserve human evaluative control, and evidence of creative regression and emergent sycophancy in newer AI models. The paper concludes that UBI must be integrated into a broader governance ecosystem that includes skills development and creativity preservation.</div>
<div class="mono" style="margin-top:8px">本研究探讨生成式人工智能的影响如何超越单纯自动化，波及就业、不平等、经济安全和人类创造力。该研究采用混合方法，结合劳动力市场任务暴露建模、部门扩散分析、政策审查和定性话语批判，以构建一个包容性人工智能治理框架。主要发现包括新模型中出现创造性退化和新兴谄媚行为的证据，从而提出了以人为中心的设计原则——1.5级自主性，并论证全民基本收入必须融入一个更广泛的治理生态系统中。</div>
</details>
</div>
<div class="card">
<div class="title">Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?</div>
<div class="meta-line">Authors: Felix Schur, Niklas Pfister, Peng Ding, Sach Mukherjee, Jonas Peters</div>
<div class="meta-line">First: 2026-01-21T18:36:34+00:00 · Latest: 2026-01-21T18:36:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15254v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\ell_1$-regularized estimation and post-selection refitting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多实验、少重复、非配对数据与稀疏效应：因果推断是否可行？</div>
<div class="mono" style="margin-top:8px">我们研究在隐藏混杂因素下估计因果效应的问题，数据为非配对设置：在不同实验条件（环境）下观测到协变量$X$和结果$Y$，但未同时观测二者；仅观测到$X$或$Y$。在适当正则条件下，该问题可转化为工具变量（IV）回归，其中环境作为（可能高维的）工具变量。当环境数量多但每个环境观测极少时，标准双样本IV估计量无法保持一致性。我们提出一种基于工具变量-协变量样本交叉折叠分割的GMM型估计量，并证明其在环境数量增长而每个环境样本量固定时具有一致性。进一步通过$\ell_1$正则化估计与后选择重拟合方法，将本方法扩展至稀疏因果效应场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of estimating causal effects from unpaired observational data where covariates and outcomes are recorded under different experimental conditions but never jointly observed, a scenario common in many real-world studies with hidden confounding. The authors propose a GMM-type estimator that leverages cross-fold sample splitting of the instrument-covariate sample, treating the experimental environment as a high-dimensional instrumental variable, and extend it to sparse effects via ℓ₁-regularization and refitting. Their theoretical analysis demonstrates that this estimator achieves consistency as the number of environments increases, even when each environment contains only a few observations, overcoming limitations of standard two-sample IV methods.</div>
<div class="mono" style="margin-top:8px">本文针对从非配对观测数据中估计因果效应的问题，其中协变量和结果在不同实验环境中分别测量，这种场景常见于重复次数较少的实验。该方法将环境视为高维工具变量，并利用交叉折叠样本分割开发了一种GMM型估计器，以处理每个环境观测稀疏的问题，确保随着环境数量增加而保持一致性。实验结果表明，所提出的估计器即使在每个环境样本量恒定的情况下也能实现一致的因果推断，且通过ℓ₁正则化扩展后，能通过后选择重拟合有效恢复稀疏因果效应。</div>
</details>
</div>
<div class="card">
<div class="title">Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism</div>
<div class="meta-line">Authors: Garrett G. Wen, Buxin Su, Natalie Collina, Zhun Deng, Weijie Su</div>
<div class="meta-line">First: 2026-01-21T18:30:42+00:00 · Latest: 2026-01-21T18:30:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15249v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors&#x27; assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions&#x27; ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于等渗机制的ML/AI会议最佳论文奖推荐方法</div>
<div class="mono" style="margin-top:8px">NeurIPS、ICML等机器学习与人工智能会议如今常收到数万篇投稿，这对维持同行评审的质量与一致性构成重大挑战。最佳论文奖的评选尤为突出——它虽是评审流程的重要环节，近年却日益引发争议。本文提出一种作者辅助机制以优化最佳论文评选。该方法采用等渗机制收集作者对自身投稿的排序评估，并据此调整原始评审分数，从而最优估计论文的真实质量。我们证明：当作者的效用函数为调整后分数的凸可加函数时，作者有动机如实报告；利用ICLR（2019-2023）和NeurIPS（2021-2023）公开评审数据，我们验证了最佳论文奖场景下该凸性假设的合理性。关键的是，在作者仅有一个提名配额（即只能提名一篇论文）的特殊情况下，我们证明即使效用函数仅为非递减可加函数，真实性依然成立——这显著放宽了既有研究所需的假设条件。为实际应用，我们扩展该机制以适应常见的作者重叠场景。仿真结果表明，本机制能显著提升获奖论文质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The surge in submissions to major ML/AI conferences like NeurIPS and ICML has strained the peer review process, particularly in selecting credible best paper awards. To address this, the authors propose an author-assisted mechanism that uses the Isotonic Mechanism to elicit authors&#x27; private rankings of their own submissions; these rankings are then used to isotonically adjust raw review scores to better estimate the papers&#x27; true quality. Theoretical analysis shows authors are incentivized to report truthfully under convex additive utility, a condition validated on ICLR (2019-2023) and NeurIPS (2021-2023) review data, with a key relaxation proving truthfulness for single-quota authors under merely nondecreasing additive utility. The mechanism is extended to handle overlapping authorship, and simulations confirm it significantly enhances the quality of awarded papers.</div>
<div class="mono" style="margin-top:8px">随着NeurIPS、ICML等主要机器学习与人工智能会议投稿量激增，同行评审的质量与一致性面临严峻挑战，最佳论文奖的评选尤其引发争议。为此，研究者提出一种作者辅助机制，采用等渗机制（Isotonic Mechanism）收集作者对其投稿的私下排序，并利用这些排序对原始评审分数进行等渗调整，以更准确地估计论文的真实质量。理论分析表明，在凸可加效用函数下作者有如实报告的激励，这一凸性假设在ICLR（2019-2023）和NeurIPS（2021-2023）的公开评审数据中得到验证；关键的是，对于仅有一个提名配额（即只能提名一篇论文）的作者，研究证明了在仅需非递减可加效用的更弱假设下真实性依然成立，这大幅放松了先前工作的要求。该机制还扩展至处理常见的作者重叠情形，模拟实验结果显示其能显著提升获奖论文的质量。</div>
</details>
</div>
<div class="card">
<div class="title">On the Reliability and Stability of Selective Methods in Malware Classification Tasks</div>
<div class="meta-line">Authors: Alexander Herzog, Aliai Eusebi, Lorenzo Cavallaro</div>
<div class="meta-line">First: 2025-05-28T20:22:43+00:00 · Latest: 2026-01-21T18:26:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22843v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.22843v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance figures of modern drift-adaptive malware classifiers appear promising, but does this translate to genuine operational reliability? The standard evaluation paradigm primarily focuses on baseline performance metrics, neglecting confidence-error alignment and operational stability. While prior works established the importance of temporal evaluation and introduced selective classification in malware classification tasks, we take a complementary direction by investigating whether malware classifiers maintain reliable and stable confidence estimates under distribution shifts and exploring the tensions between scientific advancement and practical impacts when they do not. We propose Aurora, a framework to evaluate malware classifiers based on their confidence quality and operational resilience. Aurora subjects the confidence profile of a given model to verification to assess the reliability of its estimates. Unreliable confidence estimates erode operational trust, waste valuable annotation budgets on non-informative samples for active learning, and leave error-prone instances undetected in selective classification. Aurora is further complemented by a set of metrics designed to go beyond point-in-time performance, striving towards a more holistic assessment of operational stability throughout temporal evaluation periods. The fragility we observe in SOTA frameworks across datasets of varying drift severity suggests it may be time to revisit the underlying assumptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论恶意软件分类任务中选择性方法的可靠性与稳定性</div>
<div class="mono" style="margin-top:8px">现代漂移自适应恶意软件分类器的性能数据看似良好，但这能否转化为真正的操作可靠性？标准评估范式主要关注基线性能指标，忽视了置信度-误差对齐与操作稳定性。尽管先前研究已确立时序评估的重要性，并将选择性分类引入恶意软件分类任务，我们采取互补方向：探究恶意软件分类器在分布漂移下是否保持可靠稳定的置信度估计，并当置信度失准时，剖析科学进展与实际影响之间的张力。我们提出Aurora框架，基于置信度质量与操作韧性评估恶意软件分类器。该框架通过对给定模型的置信度分布进行验证，以评估其估计的可靠性。不可靠的置信度估计会削弱操作信任、在主动学习中浪费宝贵标注资源于非信息性样本，并导致选择性分类中易错实例未被察觉。Aurora进一步辅以一套超越单点性能的指标，致力于在时序评估周期内实现更全面的操作稳定性评估。我们在不同漂移严重程度数据集中观察到的SOTA框架脆弱性表明，或许应重新审视其底层假设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study questions whether the promising performance of drift-adaptive malware classifiers translates to genuine operational reliability, noting that standard evaluations neglect confidence-error alignment and long-term stability. To address this, the authors propose Aurora, a framework that verifies a model&#x27;s confidence profile to assess the reliability of its estimates and operational resilience under distribution shifts, complemented by metrics for holistic temporal stability assessment. Experimental findings reveal significant fragility in state-of-the-art frameworks across datasets with varying drift severity, indicating unreliable confidence estimates that undermine operational trust, waste annotation budgets in active learning, and leave errors undetected in selective classification.</div>
<div class="mono" style="margin-top:8px">本研究质疑具有漂移适应能力的恶意软件分类器的优异性能是否确保真正的操作可靠性，指出标准评估忽略了置信度-误差对齐和长期稳定性。作者提出了Aurora框架，通过验证模型的置信度分布来评估其在分布偏移下估计的可靠性和操作韧性，并辅以旨在进行整体时间稳定性评估的指标。实验结果表明，在不同漂移严重程度的数据集上，最先进的框架表现出显著的脆弱性，其不可靠的置信度估计会损害操作信任、在主动学习中浪费标注预算，并在选择性分类中遗留未被检测的错误。</div>
</details>
</div>
<div class="card">
<div class="title">Feasibility Preservation under Monotone Retrieval Truncation</div>
<div class="meta-line">Authors: Sean Plummer</div>
<div class="meta-line">First: 2026-01-21T18:25:16+00:00 · Latest: 2026-01-21T18:25:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15241v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation.
  We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.
  Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单调检索截断下的可行性保持</div>
<div class="mono" style="margin-top:8px">基于检索的系统通过仅暴露可用证据的截断子集来近似访问语料库。即使语料库中存在相关信息，截断也可能阻止兼容证据同时出现，导致基于相关性的评估无法捕捉的失败情况。本文从结构视角研究检索，将查询应答建模为截断条件下的可行性问题。
我们形式化检索为候选证据序列，并刻画极限可行性蕴含有限检索深度可行性的条件。证明单调截断足以保证单个查询的有限可见证性。对于查询类，我们确定见证证书的有限生成是获得一致检索边界所需的附加条件，并证明该条件是必要的。进一步通过尖锐反例展示非单调截断、非有限生成查询类及纯槽位覆盖下的失败情形。
这些结果共同将可行性保持确立为独立于相关性评分或优化的检索正确性准则，并阐明了基于截断的检索固有的结构局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Retrieval systems often truncate results, which can prevent relevant evidence from co-occurring and cause failures not captured by relevance metrics. This paper models query answering as a feasibility problem under truncation, analyzing conditions where feasibility in the limit ensures feasibility at finite retrieval depth. It proves monotone truncation guarantees finite witnessability for individual queries, and identifies finite generation of witness certificates as necessary for uniform bounds across query classes, with counterexamples showing failures under non-monotone truncation or non-finitely-generated classes. The results establish feasibility preservation as a structural correctness criterion independent of scoring.</div>
<div class="mono" style="margin-top:8px">本文研究了基于截断证据集的检索系统的结构局限性，指出即使语料库中存在相关信息，截断也可能阻止兼容证据同时出现，这是一种标准相关性评估无法捕捉的失败模式。作者将查询应答建模为截断下的可行性问题，将检索形式化为候选证据集序列，并建立了在极限可行性下意味着有限检索深度可行性的条件——特别是单调截断对于单个查询的保证。对于查询类，他们确定了见证证书的有限生成是获得统一检索界限的必要且充分条件，并提供了尖锐的反例，展示了在非单调截断、非有限生成查询类以及纯槽位覆盖下的失败情况。这些结果将可行性保持确立为独立于相关性评分或优化的检索基本正确性准则。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion</div>
<div class="meta-line">Authors: Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang</div>
<div class="meta-line">First: 2026-01-20T05:00:26+00:00 · Latest: 2026-01-21T18:21:39+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13599v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13599v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a &#x27;draft-then-refine&#x27; framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model&#x27;s global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散中的扩散：在半自回归扩散模型中重获全局连贯性</div>
<div class="mono" style="margin-top:8px">全局离散扩散语言模型最引人注目的特征之一是其全局双向上下文能力。然而，现有的基于块的扩散研究倾向于引入自回归先验，这虽然带来益处，却可能导致模型在宏观层面失去全局连贯性。为在保留半自回归范式优势的同时重获全局上下文理解能力，我们提出&#x27;扩散中的扩散&#x27;——一种&#x27;先草拟后精修&#x27;的框架，旨在克服块扩散模型固有的不可逆性与短视问题。该方法首先采用块扩散通过小模块快速生成草稿，随后通过具有更大双向感受野的全局双向扩散对这些草稿进行精修。我们利用快照置信度重掩码技术识别需要修改的最关键词元，并应用混合尺度训练来扩展块扩散模型的全局能力。实验结果表明，我们的方法在OpenWebText数据集上为离散扩散模型设立了新基准。仅使用基线模型26%的微调预算，便将生成困惑度从25.7降至21.9，显著缩小了与自回归模型的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the loss of global coherence in block-based diffusion language models due to autoregressive priors, aiming to restore bidirectional contextual understanding while retaining semi-autoregressive efficiency. The proposed Diffusion in Diffusion framework employs a two-stage draft-then-refine process: first generating drafts via block diffusion with small blocks, then refining them through global bidirectional diffusion with a larger receptive field, enhanced by snapshot confidence remasking to pinpoint tokens needing modification and mix-scale training to improve global capabilities. Experimental results on OpenWebText show the method sets a new benchmark for discrete diffusion models, reducing generative perplexity from 25.7 to 21.9 with only 26% of the fine-tuning budget of baselines, thereby narrowing the performance gap with autoregressive models.</div>
<div class="mono" style="margin-top:8px">本研究针对基于块的扩散语言模型因自回归先验而丧失全局连贯性的问题，提出了“扩散中的扩散”框架，旨在恢复双向上下文理解能力，同时保持半自回归范式的效率。该方法采用“起草-精炼”的两阶段流程：首先通过小块块扩散快速生成草稿，然后利用具有更大双向感受野的全局双向扩散进行精炼，并通过快照置信度重掩码识别关键修改令牌，结合混合尺度训练以扩展模型的全局能力。在OpenWebText数据集上的实验结果表明，该方法为离散扩散模型设立了新基准，仅使用基线模型26%的微调预算，便将生成困惑度从25.7降至21.9，显著缩小了与自回归模型的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification</div>
<div class="meta-line">Authors: Fabi Nahian Madhurja, Rusab Sarmun, Muhammad E. H. Chowdhury, Adam Mushtak, Israa Al-Hashimi, Sohaib Bassam Zoghoul</div>
<div class="meta-line">First: 2026-01-21T18:15:47+00:00 · Latest: 2026-01-21T18:15:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15235v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model&#x27;s performance with expert radiologists, demonstrating competitive results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在二维笔画中追踪三维解剖结构：一种用于颈椎骨折识别的多阶段投影驱动方法</div>
<div class="mono" style="margin-top:8px">颈椎骨折是需精确高效检测以进行有效临床处理的关键医疗状况。本研究探讨了基于二维投影的椎骨分割在三维CT体积中实现椎骨级骨折检测的可行性，提出了一种用于自动化分析颈椎（C1-C7）的端到端流程。通过优化的二维轴向、矢状和冠状投影近似三维体积，利用YOLOv8模型从所有视图中识别感兴趣区域并组合以近似三维颈椎区域，实现了94.45%的三维mIoU。与传统三维分割方法相比，这种基于投影的定位策略在保持高性能的同时降低了计算复杂度。随后采用基于DenseNet121-Unet的多标签分割，利用基于方差和能量的投影，实现了87.86%的Dice分数。从这些二维分割掩模中战略性地近似三维椎骨掩模，能够提取单个椎骨体积。通过集成2.5D时空序列模型分析这些体积的骨折情况，该模型结合每个椎骨的原始切片和投影进行互补评估，在椎骨级和患者级分别实现了68.15和82.26的F1分数，以及91.62和83.04的ROC-AUC分数。我们进一步通过可解释性研究验证了该方法，该研究提供了突出诊断相关解剖区域的显著性图可视化，并通过与专家放射科医生比较模型性能的观察者间变异性分析，展示了具有竞争力的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable precise and efficient detection of cervical spine fractures from 3D CT scans, this study develops an automated pipeline that reduces computational complexity by using 2D projections. The method first localizes the spine by combining YOLOv8 detections on optimized axial, sagittal, and coronal projections, achieving a 3D mIoU of 94.45%. It then segments individual vertebrae using a DenseNet121-Unet model on variance- and energy-based projections, with a Dice score of 87.86%, and finally classifies fractures via an ensemble of 2.5D Spatio-Sequential models analyzing both raw slices and projections. The ensemble achieved vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively, with explainability studies and interobserver analysis showing competitive performance against expert radiologists.</div>
<div class="mono" style="margin-top:8px">颈椎骨折的临床管理需要精确检测，这促使研究开发一种自动化方法从3D CT扫描中识别骨折，同时降低计算复杂度。该方法采用多阶段流程：首先，通过优化的轴向、矢状和冠状二维投影近似三维体积，使用YOLOv8模型从所有视图定位感兴趣区域，实现94.45%的3D mIoU；其次，基于DenseNet121-Unet的模型在方差和能量投影上进行多标签分割，获得87.86%的Dice分数；最后，集成2.5D时空序列模型分析提取的椎骨体积以检测骨折，在椎骨级别和患者级别分别达到68.15和82.26的F1分数，以及91.62和83.04的ROC-AUC分数。实验验证包括通过显著性图的可解释性研究和观察者间分析，表明模型性能与专业放射科医生具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">From Construction to Injection: Edit-Based Fingerprints for Large Language Models</div>
<div class="meta-line">Authors: Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo, Linlin Wang</div>
<div class="meta-line">First: 2025-09-03T08:22:04+00:00 · Latest: 2026-01-21T17:56:42+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03122v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.03122v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Establishing reliable and verifiable fingerprinting mechanisms is fundamental to controlling the unauthorized redistribution of large language models (LLMs). However, existing approaches face two major challenges: (a) ensuring imperceptibility, including resistance to statistical identification and avoidance of accidental activation during fingerprint construction, and (b) preserving both model utility and fingerprint detectability under subsequent model modifications. To address these challenges, we propose an end-to-end fingerprinting framework with two components. First, we design a rule-based code-mixing fingerprint (CF) that maps natural-query-like prompts to multi-candidate targets, reducing accidental triggering via high-complexity code-mixing formulations. Second, we introduce Multi-Candidate Editing (MCEdit), which jointly optimizes multi-candidate targets and enforces margins between target and non-target outputs to improve post-modification detectability. Extensive experiments demonstrate that our framework provides a robust and practical solution for fingerprinting LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从构建到注入：基于编辑的大型语言模型指纹方法</div>
<div class="mono" style="margin-top:8px">建立可靠且可验证的指纹机制对于控制大型语言模型（LLMs）的未经授权再分发至关重要。然而，现有方法面临两大挑战：（a）确保不可感知性，包括抵抗统计识别并避免指纹构建过程中的意外激活；（b）在后续模型修改下同时保持模型效用和指纹可检测性。为解决这些挑战，我们提出了一个包含两个组件的端到端指纹框架。首先，我们设计了一种基于规则的代码混合指纹（CF），将类自然查询的提示映射到多候选目标，通过高复杂度的代码混合公式减少意外触发。其次，我们引入了多候选编辑（MCEdit），联合优化多候选目标并强制目标与非目标输出之间的边界，以提高修改后的可检测性。大量实验表明，我们的框架为LLMs指纹识别提供了稳健且实用的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for reliable and verifiable fingerprinting mechanisms to prevent unauthorized redistribution of large language models (LLMs), tackling challenges in ensuring imperceptibility and preserving utility and detectability after model modifications. The proposed method is an end-to-end framework featuring a rule-based code-mixing fingerprint that maps natural prompts to multi-candidate targets to reduce accidental activation, combined with Multi-Candidate Editing (MCEdit) to jointly optimize these targets and enforce output margins for improved post-modification detectability. Experimental results show the framework offers a robust and practical solution for fingerprinting LLMs.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型（LLMs）未经授权分发的问题，通过建立可靠且可验证的指纹机制，应对指纹不可感知性和模型修改后鲁棒性的挑战。方法上提出了一个端到端框架，包含基于规则的代码混合指纹以减少意外触发，以及多候选编辑（MCEdit）技术来联合优化多目标输出并提升修改后的可检测性。大量实验表明，该框架为LLMs指纹识别提供了稳健且实用的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework</div>
<div class="meta-line">Authors: Kaihua Ding</div>
<div class="meta-line">First: 2025-12-11T15:53:19+00:00 · Latest: 2026-01-21T17:50:24+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures and 3 tables, under submission to IEEE FIE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10758v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.10758v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of generative AI tools has rendered traditional modular assessments in computing and data-centric education increasingly ineffective, creating a disconnect between academic evaluation and authentic skill measurement. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and empirical validation.
  We make three primary contributions. First, we establish two formal propositions. (1) Assessments composed of interconnected problems, in which outputs serve as inputs to subsequent tasks, are inherently more AI-resilient than modular assessments due to their reliance on multi-step reasoning and sustained context. (2) Semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution templates. These results challenge widely cited recommendations in recent institutional and policy guidance that promote open-ended assessments as inherently more robust to AI assistance.
  Second, we validate these propositions through empirical analysis of three university data science courses (N = 117). We observe a substantial AI inflation effect: students achieve near-perfect scores on AI-assisted modular homework, while performance drops by approximately 30 percentage points on proctored exams (Cohen d = 1.51). In contrast, interconnected projects remain strongly aligned with modular assessments (r = 0.954, p &lt; 0.001) while maintaining AI resistance, whereas proctored exams show weaker alignment (r = 0.726, p &lt; 0.001).
  Third, we translate these findings into a practical assessment design procedure that enables educators to construct evaluations that promote deeper engagement, reflect industry practice, and resist trivial AI delegation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于关联性问题设计抗人工智能评估：一个理论扎实且经验验证的框架</div>
<div class="mono" style="margin-top:8px">生成式人工智能工具的普及使得计算与数据导向教育中的传统模块化评估日益失效，导致学术评价与真实技能测量之间出现脱节。本文提出一个理论扎实的抗人工智能评估设计框架，并通过形式化分析与实证验证予以支持。
我们作出三项主要贡献：首先，确立两个形式化命题：（1）由关联性问题构成的评估（即前序任务的输出作为后续任务的输入）因其依赖多步推理与持续语境，本质上比模块化评估更具抗人工智能特性；（2）具有确定性成功标准的半结构化问题比完全开放式项目更能可靠衡量学生能力，后者易使人工智能系统套用熟悉解题模板。这些结论挑战了近期机构与政策指南中广泛推崇开放式评估对人工智能辅助更具稳健性的观点。
其次，通过对三门大学数据科学课程（N=117）的实证分析验证这些命题。我们观察到显著的人工智能分数膨胀效应：学生在人工智能辅助的模块化作业中取得接近满分成绩，而在监考考试中表现下降约30个百分点（科恩d值=1.51）。相比之下，关联性项目在保持抗人工智能特性的同时与模块化评估保持高度一致性（r=0.954，p&lt;0.001），而监考考试则呈现较弱相关性（r=0.726，p&lt;0.001）。
第三，我们将这些发现转化为可操作的评估设计流程，帮助教育工作者构建能促进深度参与、反映行业实践且抵御人工智能简单代劳的评估体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rise of generative AI has undermined the validity of traditional modular assessments in computing education, necessitating new evaluation methods that can authentically measure skills. The paper proposes a framework for AI-resilient assessments based on two formal propositions: that interconnected problems requiring multi-step reasoning are more resistant to AI, and that semi-structured problems with clear criteria are superior to open-ended projects. Empirical validation across three university data science courses (N=117) showed a large AI inflation effect on modular homework, while interconnected projects maintained strong alignment with modular scores and resisted AI assistance, unlike proctored exams which showed weaker alignment.</div>
<div class="mono" style="margin-top:8px">生成式人工智能的普及削弱了计算教育中传统模块化评估的有效性，导致学术评价与真实技能测量之间出现脱节。为此，本文提出了一个设计抗人工智能评估的框架，其核心是基于相互关联的问题（即一个任务的输出作为后续任务的输入）以及具有明确成功标准的半结构化问题，论证了由于对多步推理和持续上下文的依赖，这些形式比模块化或完全开放式的评估更能抵抗人工智能辅助。通过对三门大学数据科学课程（N=117）的实证验证，发现了显著的人工智能分数膨胀效应：学生在人工智能辅助的模块化作业中得分接近满分，而在监考考试中成绩下降约30个百分点，而相互关联的项目则与模块化评估保持高度一致并展现出抗人工智能特性。</div>
</details>
</div>
<div class="card">
<div class="title">SPECTRE: Conditional System Prompt Poisoning to Hijack LLMs</div>
<div class="meta-line">Authors: Viet Pham, Thai Le</div>
<div class="meta-line">First: 2025-05-22T16:47:15+00:00 · Latest: 2026-01-21T17:45:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16888v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.16888v3">PDF</a> · <a href="https://github.com/vietph34/CAIN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed via third-party system prompts downloaded from public marketplaces. We identify a critical supply-chain vulnerability: conditional system prompt poisoning, where an adversary injects a ``sleeper agent&#x27;&#x27; into a benign-looking prompt. Unlike traditional jailbreaks that aim for broad refusal-breaking, our proposed framework, SPECTRE, optimizes system prompts to trigger LLMs to output targeted, compromised responses only for specific queries (e.g., ``Who should I vote for the US President?&#x27;&#x27;) while maintaining high utility on benign inputs. Operating in a strict black-box setting without model weight access, SPECTRE utilizes a two-stage optimization including a global semantic search followed by a greedy lexical refinement. Tested on open-source models and commercial APIs (GPT-4o-mini, GPT-3.5), SPECTRE achieves up to 70% F1 reduction on targeted queries with minimal degradation to general capabilities. We further demonstrate that these poisoned prompts evade standard defenses, including perplexity filters and typo-correction, by exploiting the natural noise found in real-world system prompts. Our code and data are available at https://github.com/vietph34/CAIN. WARNING: Our paper contains examples that might be sensitive to the readers!</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPECTRE：通过条件性系统提示词投毒劫持大型语言模型</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地通过从公共市场下载的第三方系统提示词进行部署。我们发现一个关键的供应链漏洞：条件性系统提示词投毒，攻击者可将“潜伏代理”注入看似良性的提示词中。与传统旨在广泛突破拒绝机制的越狱攻击不同，我们提出的SPECTRE框架通过优化系统提示词，仅在特定查询（例如“我应该投票给哪位美国总统候选人？”）时触发LLMs输出定向的受控响应，同时在良性输入上保持高效用。在严格的黑盒设置下（无模型权重访问权限），SPECTRE采用两阶段优化方法，包括全局语义搜索和贪婪词汇精炼。在开源模型和商业API（GPT-4o-mini、GPT-3.5）上的测试表明，SPECTRE能在目标查询上实现高达70%的F1值下降，而对通用能力影响极小。我们进一步证明，这些被投毒的提示词通过利用真实系统提示词中的自然噪声，可规避包括困惑度过滤器和拼写纠正在内的标准防御机制。代码与数据已开源：https://github.com/vietph34/CAIN。警告：本文包含可能引起读者敏感的内容示例！</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the security risks of deploying LLMs with third-party system prompts from public marketplaces, which are vulnerable to supply-chain attacks. The proposed method, SPECTRE, introduces a conditional system prompt poisoning framework that optimizes prompts to act as sleeper agents, triggering compromised responses only for specific targeted queries while preserving utility on benign inputs, using a two-stage black-box optimization involving global semantic search and greedy lexical refinement. Experimental results show SPECTRE achieves up to a 70% F1 reduction on targeted queries with minimal general capability degradation on models like GPT-4o-mini and GPT-3.5, and it evades standard defenses such as perplexity filters and typo-correction by exploiting natural noise in real-world prompts.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型使用第三方系统提示的安全风险，识别出供应链漏洞，即攻击者可在看似良性的提示中嵌入条件性触发机制。SPECTRE方法采用两阶段黑盒优化，结合全局语义搜索和贪婪词汇精炼，生成仅在特定查询下引发妥协响应、同时保持良性输入实用性的中毒提示。实验结果表明，SPECTRE在目标查询上使F1分数降低高达70%，且对一般性能影响极小，并能通过模拟真实提示中的自然噪声规避困惑度过滤等标准防御措施。</div>
</details>
</div>
<div class="card">
<div class="title">Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface</div>
<div class="meta-line">Authors: Paige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy. Gidey Gezae, Dante Conway, Abraham Glasser, Raja Kushalnagar, Christian Vogler</div>
<div class="meta-line">First: 2026-01-21T17:33:00+00:00 · Latest: 2026-01-21T17:33:00+00:00</div>
<div class="meta-line">Comments: Accepted for publication in ACM CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15209v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa&#x27;s automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered &quot;task prompter,&quot; which integrated the user&#x27;s history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>听障人士对智能个人助手的访问：语音交互方案与基于大语言模型的触控界面比较</div>
<div class="mono" style="margin-top:8px">本研究探讨了在日常交流中能使用语音的聋哑及听障人士对智能个人助手的可访问性。由于智能助手无法理解包括聋人发音在内的多样化口音，导致非手语使用者及口语表达的听障群体难以有效使用。通过混合研究方法，我们在Echo Show设备上比较了三种交互方式：基于英语自然语音输入配合Alexa自动语音识别、由训练有素的协助者复述指令的模拟交互，以及基于大语言模型的触控界面。触控方法通过集成用户历史与智能环境的大语言模型“任务提示器”实现情境化指令建议。定量结果显示两种语音条件与LLM辅助触控界面无显著差异；定性分析则显示用户对不同方法的可用性评价存在差异。最终研究表明，智能助手需原生支持对聋人口音的鲁棒性语音识别。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the accessibility gap in intelligent personal assistants (IPAs) for deaf and hard of hearing (DHH) individuals who use speech, as standard IPAs often fail to understand diverse accents including deaf speech. The study compared the usability of voice-based input (using Alexa&#x27;s ASR and a Wizard-of-Oz setup with a facilitator re-speaking commands) against an LLM-powered touch interface that suggested contextually-appropriate commands via a &quot;task prompter,&quot; integrating user history and the smart environment. Experimental results from a mixed-methods study showed no significant quantitative differences in usability between the spoken conditions and the LLM-assisted touch method, though qualitative feedback revealed varied user opinions, underscoring the need for IPAs to natively and robustly recognize deaf-accented speech.</div>
<div class="mono" style="margin-top:8px">本研究针对使用口语的聋人和听力障碍人士在智能个人助理使用中面临的障碍，即语音助手难以识别包括聋人语音在内的多样化口音。研究人员在Echo Show设备上比较了三种输入方式的可用性：标准的Alexa语音识别、由协调员复述指令的Wizard-of-Oz条件，以及一种由大型语言模型驱动的触摸界面，该界面能根据用户历史和环境提供情境感知的命令建议。定量结果显示，两种语音条件与LLM辅助的触摸方法在性能上没有显著差异；定性反馈则揭示了用户对不同方法的偏好存在差异，这强调了智能个人助理需要原生且鲁棒地识别聋人语音的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs</div>
<div class="meta-line">Authors: Jean-Charles Noirot Ferrand, Yohan Beugin, Eric Pauley, Ryan Sheatsley, Patrick McDaniel</div>
<div class="meta-line">First: 2025-01-27T22:13:05+00:00 · Latest: 2026-01-21T17:29:42+00:00</div>
<div class="meta-line">Comments: Accepted to 2026 IEEE Secure and Trustworthy Machine Learning Conference (SaTML)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.16534v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.16534v3">PDF</a> · <a href="https://github.com/jcnf0/targeting-alignment">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment embeds a safety classifier in the LLM responsible for deciding between refusal and compliance, and seek to extract an approximation of this classifier: a surrogate classifier. To this end, we build candidate classifiers from subsets of the LLM. We first evaluate the degree to which candidate classifiers approximate the LLM&#x27;s safety classifier in benign and adversarial settings. Then, we attack the candidates and measure how well the resulting adversarial inputs transfer to the LLM. Our evaluation shows that the best candidates achieve accurate agreement (an F1 score above 80%) using as little as 20% of the model architecture. Further, we find that attacks mounted on the surrogate classifiers can be transferred to the LLM with high success. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70% with half the memory footprint and runtime -- a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is an effective and efficient means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks. The code is available at https://github.com/jcnf0/targeting-alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>目标对齐：提取对齐大语言模型的安全分类器</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的对齐旨在强化安全性等准则，但在通过修改输入诱导不安全输出的越狱攻击面前，对齐机制可能失效。本文提出并评估了一种新的越狱攻击技术。我们观察到对齐机制在LLM中嵌入了一个负责在拒绝与遵从之间决策的安全分类器，并尝试提取该分类器的近似替代分类器。为此，我们从LLM的子集中构建候选分类器。首先评估候选分类器在正常与对抗场景下对LLM安全分类器的近似程度；随后攻击候选分类器，并测量所得对抗性输入向LLM的迁移效果。评估表明，最佳候选分类器仅需使用20%的模型架构即可实现精确匹配（F1分数超过80%）。此外，针对替代分类器发起的攻击能以高成功率迁移至LLM。例如，仅使用Llama 2模型50%参数的替代分类器实现了70%的攻击成功率（ASR），其内存占用和运行时间减半——相较于直接攻击LLM（仅观测到22% ASR）有显著提升。这些结果表明，提取替代分类器是建模（进而应对）对齐模型越狱攻击脆弱性的高效方法。代码发布于https://github.com/jcnf0/targeting-alignment。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of aligned large language models (LLMs) to jailbreak attacks that bypass safety guidelines. The method proposes extracting a surrogate safety classifier from a subset of the LLM&#x27;s architecture to approximate its internal safety decision mechanism. Experimental results demonstrate that surrogate classifiers using only 20-50% of the model parameters can achieve high agreement with the original model (F1 &gt;80%) and enable more efficient jailbreak attacks, with a 50% surrogate for Llama 2 achieving a 70% attack success rate compared to 22% when attacking the full model directly.</div>
<div class="mono" style="margin-top:8px">本研究针对对齐后的大型语言模型（LLM）易受越狱攻击绕过安全准则的漏洞。方法是从LLM架构的子集中提取一个代理安全分类器，以近似其内部安全决策机制。实验结果表明，仅使用模型20%的代理分类器在匹配LLM安全决策时能达到超过80%的F1分数，并且在Llama 2的50%代理上生成的攻击实现了70%的攻击成功率，显著高于直接攻击完整模型22%的成功率，证明了这是一种高效的漏洞建模方法。</div>
</details>
</div>
<div class="card">
<div class="title">OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions</div>
<div class="meta-line">Authors: Maxim Popov, Regina Kurkova, Mikhail Iumanov, Jaafar Mahmoud, Sergey Kolyubin</div>
<div class="meta-line">Venue: IROS</div>
<div class="meta-line">First: 2025-03-13T13:07:51+00:00 · Latest: 2026-01-21T17:25:25+00:00</div>
<div class="meta-line">Comments: Project page: https://be2rlab.github.io/OSMa-Bench/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10331v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.10331v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://be2rlab.github.io/OSMa-Bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open Semantic Mapping (OSM) is a key technology in robotic perception, combining semantic segmentation and SLAM techniques. This paper introduces a dynamically configurable and highly automated LLM/LVLM-powered pipeline for evaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark). The study focuses on evaluating state-of-the-art semantic mapping algorithms under varying indoor lighting conditions, a critical challenge in indoor environments. We introduce a novel dataset with simulated RGB-D sequences and ground truth 3D reconstructions, facilitating the rigorous analysis of mapping performance across different lighting conditions. Through experiments on leading models such as ConceptGraphs, BBQ, and OpenScene, we evaluate the semantic fidelity of object recognition and segmentation. Additionally, we introduce a Scene Graph evaluation method to analyze the ability of models to interpret semantic structure. The results provide insights into the robustness of these models, forming future research directions for developing resilient and adaptable robotic systems. Project page is available at https://be2rlab.github.io/OSMa-Bench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSMa-Bench：评估不同光照条件下的开放语义建图性能</div>
<div class="mono" style="margin-top:8px">开放语义建图（OSM）是机器人感知领域的关键技术，融合了语义分割与SLAM技术。本文提出了一种动态可配置、高度自动化的LLM/LVLM驱动评估流程——OSMa-Bench（开放语义建图基准），用于评估OSM解决方案。研究聚焦于评估先进语义建图算法在室内多变光照条件下的表现，这是室内环境中的关键挑战。我们提出了包含模拟RGB-D序列与真实三维重建标注的全新数据集，支持对不同光照条件下建图性能的严谨分析。通过对ConceptGraphs、BBQ、OpenScene等前沿模型的实验，评估了物体识别与分割的语义保真度。此外，我们引入了场景图评估方法，以分析模型解析语义结构的能力。实验结果揭示了这些模型的鲁棒性特征，为开发具有适应性与韧性的机器人系统指明了未来研究方向。项目页面详见：https://be2rlab.github.io/OSMa-Bench/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to evaluate the robustness of Open Semantic Mapping (OSM) systems, which integrate semantic segmentation with SLAM, under challenging and variable indoor lighting conditions. The authors propose OSMa-Bench, a benchmark framework powered by LLMs/LVLMs, and introduce a novel dataset of simulated RGB-D sequences with ground truth 3D reconstructions to systematically test mapping performance across different lighting scenarios. Experiments on models like ConceptGraphs, BBQ, and OpenScene, evaluated for semantic fidelity and using a novel Scene Graph method, reveal insights into their robustness, highlighting critical performance variations and informing future directions for developing more resilient robotic perception systems.</div>
<div class="mono" style="margin-top:8px">本研究针对在变化室内光照条件下评估开放语义建图（OSM）系统的挑战，OSM是结合语义分割与SLAM的机器人感知关键技术。作者提出了OSMa-Bench基准，包含一个动态可配置、基于LLM/LVLM的自动化评估流程，以及一个带有真实3D重建的模拟RGB-D序列新数据集，以支持严格测试。通过对ConceptGraphs、BBQ和OpenScene等模型进行实验评估，使用语义保真度指标和新颖的场景图方法，结果揭示了这些模型在不同光照下的鲁棒性和语义结构解析能力，为开发适应性强的机器人系统指明了未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-21T17:15:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayesianVLA：基于潜在动作查询的视觉语言动作模型贝叶斯分解</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但常难以泛化至新指令或复杂多任务场景。我们指出当前训练范式中存在一个关键缺陷：目标驱动的数据收集导致数据集偏差。在此类数据集中，仅凭视觉观测即可高度预测语言指令，致使指令与动作间的条件互信息趋近于零，这一现象我们称为“信息坍缩”。因此，模型退化为仅依赖视觉的策略，忽略语言约束并在分布外（OOD）场景中失效。为解决此问题，我们提出BayesianVLA，一种通过贝叶斯分解强制遵循指令的新框架。通过引入可学习的潜在动作查询，我们构建双分支架构以同时估计仅视觉先验$p(a \mid v)$和语言条件后验$π(a \mid v, \ell)$，进而优化策略以最大化动作与指令间的条件点互信息（PMI）。该目标有效惩罚视觉捷径，并奖励能显式解释语言命令的动作。无需新增数据，BayesianVLA显著提升了泛化能力。在SimplerEnv和RoboCasa上的大量实验证明了其显著优势，如在挑战性OOD基准SimplerEnv上实现11.3%的性能提升，验证了本方法在动作中稳健关联语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions become predictable from visual observations, leading to an Information Collapse that degrades models into vision-only policies. To counteract this, BayesianVLA introduces a Bayesian decomposition framework using learnable Latent Action Queries to separately model a vision-only prior and a language-conditioned posterior, optimizing the policy to maximize the conditional Pointwise Mutual Information between actions and instructions. Experiments on SimplerEnv and RoboCasa show the method significantly improves out-of-distribution generalization, achieving an 11.3% performance gain on a challenging OOD benchmark without requiring new data.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差导致泛化能力不足，其中语言指令可从视觉观察中预测，引发信息坍缩，使模型退化为仅依赖视觉的策略。为解决该问题，BayesianVLA提出一种贝叶斯分解框架，通过可学习的潜在动作查询分别建模视觉先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息。在SimplerEnv和RoboCasa上的实验表明，该方法无需新数据即可显著提升泛化性能，在分布外基准测试中取得了11.3%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</div>
<div class="meta-line">Authors: Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee</div>
<div class="meta-line">First: 2026-01-21T17:12:46+00:00 · Latest: 2026-01-21T17:12:46+00:00</div>
<div class="meta-line">Comments: Accepted at International Mining Software Repositories Conference (MSR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15195v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project&#x27;s CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编程代理在何处失败？GitHub中失败代理式拉取请求的实证研究</div>
<div class="mono" style="margin-top:8px">AI编程代理现正向软件项目提交拉取请求（PR），不仅作为辅助工具，更成为自主贡献者。随着这类代理贡献在实际代码库中快速增长，人们对其实际行为模式及大量PR未能合并的原因知之甚少。本文对GitHub上五种编程代理生成的3.3万条代理编写PR展开大规模研究：（RQ1）首先从四个维度定量分析已合并与未合并PR：1）跨任务类型的合并结果，2）代码变更，3）CI构建结果，4）评审动态。研究发现文档、CI和构建更新类任务合并成功率最高，性能优化与缺陷修复任务表现最差。未合并PR往往涉及更大代码变更、触及更多文件，且常未通过项目CI/CD流水线验证。（RQ2）为深入探究代理PR未合并原因，对600条PR进行定性分析，构建了拒绝模式的分层分类体系。该分析通过揭示定量指标未捕捉的拒绝原因（包括缺乏实质性评审互动、重复PR、非预期功能实现、代理行为失准），补充了RQ1的定量发现。综合研究结果凸显了影响未来代理工作流成功的关键社会技术因素与人机协作要素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates why many AI coding agents&#x27; pull requests (PRs) on GitHub fail to be merged, given their increasing role as autonomous contributors. The authors conducted a large-scale empirical analysis of 33,000 agent-authored PRs from five agents, quantitatively characterizing merged versus non-merged PRs across task types, code changes, CI build results, and review dynamics. They found that documentation, CI, and build update tasks had the highest merge success, while performance and bug-fix tasks performed worst; non-merged PRs often involved larger changes, touched more files, and failed CI validation. A subsequent qualitative analysis of 600 PRs revealed a taxonomy of rejection reasons, including lack of reviewer engagement, duplicate PRs, unwanted features, and agent misalignment, highlighting socio-technical and human-AI collaboration factors critical for improving agentic workflows.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究AI编码代理在现实软件项目中提交的许多拉取请求（PR）为何未能被合并，鉴于其自主贡献日益增多。作者对GitHub上五个代理生成的33,000个PR进行了大规模实证分析，从任务类型、代码变更、CI构建结果和评审动态四个维度定量比较了已合并和未合并的PR。研究发现，文档和CI更新类任务的合并成功率最高，而性能和错误修复任务表现最差；未合并的PR通常涉及更大规模、多文件的变更且未通过CI验证。随后对600个PR的定性分析揭示了一系列拒绝原因的分类，包括缺乏有意义的评审参与、重复PR、不需要的功能实现以及代理错位，这些发现凸显了改进代理工作流所需的关键社会技术因素。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback</div>
<div class="meta-line">Authors: Stephan Wallraven, Tim Köhne, Hartmut Westenberger, Andreas Moser</div>
<div class="meta-line">First: 2026-01-21T17:06:41+00:00 · Latest: 2026-01-21T17:06:41+00:00</div>
<div class="meta-line">Comments: 20 pages, 10 figures, Author: Hartmut Westenberger (ORCID: 0009-0009-9063-8318)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15188v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型在ABAP代码生成中的基准测试：基于编译器反馈迭代改进的实证研究</div>
<div class="mono" style="margin-top:8px">本研究探讨大型语言模型生成ABAP代码的性能。尽管生成式AI已在多种编程语言中成功应用，但迄今鲜有对ABAP代码生成的系统分析。本研究旨在实证分析：各类LLM生成语法正确且功能完整的ABAP代码的能力边界、利用编译器反馈进行迭代改进的效果，以及哪些任务类型构成特殊挑战。为此构建了包含180项任务的基准测试集，涵盖改编的HumanEval任务与真实SAP场景。结果显示模型间性能差异显著：更强LLM经数次迭代后成功率约达75%，且能充分利用编译器反馈；较小模型表现明显较弱。总体而言，本研究揭示了高性能LLM在ABAP开发流程中的巨大潜力，尤其在迭代纠错方面。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the lack of systematic analysis on ABAP code generation by Large Language Models (LLMs), despite their success in other programming languages. The research empirically evaluates multiple LLMs&#x27; ability to produce syntactically correct and functional ABAP code through a benchmark of 180 tasks, including adapted HumanEval problems and practical SAP scenarios, with a focus on iterative improvement using compiler feedback. Experimental results reveal substantial performance gaps: more powerful LLMs achieve approximately 75% success rates after several iterations and benefit significantly from compiler feedback, whereas smaller models perform markedly worse, demonstrating the high potential of advanced LLMs for ABAP development, particularly in iterative error correction.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型在ABAP代码生成方面缺乏系统分析的问题展开，尽管这些模型在其他编程语言中已取得成功。该研究通过实证方法评估了各种大型语言模型生成语法正确且功能正常的ABAP代码的能力，特别考察了它们利用编译器反馈进行迭代改进的效能，并识别了具有特殊挑战的任务类型。研究方法采用包含180个任务的基准测试，结合了改编的HumanEval问题和实际SAP场景，通过多轮编译器反馈迭代测试模型性能。主要实验结果表明，性能更强的模型经过数次迭代后成功率可达约75%，且能从编译器反馈中显著获益，而较小模型表现明显较差，这凸显了先进大型语言模型在ABAP开发流程中迭代纠错方面的巨大潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks</div>
<div class="meta-line">Authors: Lorenzo Fernández Maimó, Alberto Huertas Celdrán, Manuel Gil Pérez, Félix J. García Clemente, Gregorio Martínez Pérez</div>
<div class="meta-line">First: 2026-01-21T16:54:19+00:00 · Latest: 2026-01-21T16:54:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15177v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15177v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向5G网络的深度学习异常检测系统动态管理</div>
<div class="mono" style="margin-top:8px">雾计算与移动边缘计算（MEC）将在即将到来的第五代（5G）移动网络中发挥关键作用，通过高度分布式的计算模型，在网络内部支持去中心化应用、数据分析与管理。此外，提供以用户为中心的网络安全解决方案日益受到关注，这尤其需要在5G网络中收集、处理和分析海量数据流量及庞大网络连接。为此，本文提出一种面向5G移动网络的MEC解决方案，以实时、自主的方式检测网络异常。该方案采用深度学习技术分析网络流量并检测异常，同时利用策略机制对异常检测过程中使用的计算资源进行高效动态管理。文中详细阐述了方案部署的相关要点及展示其性能的实验结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for decentralized, real-time cybersecurity in 5G networks, which generate massive data traffic, this paper proposes a mobile edge computing (MEC)-oriented anomaly detection system. The method employs deep learning to analyze network flows for anomaly detection and utilizes policies for the dynamic management of the computing resources involved in this process. Experimental results demonstrate the system&#x27;s performance in real-time, autonomic anomaly detection.</div>
<div class="mono" style="margin-top:8px">为应对5G网络海量数据流量带来的去中心化、实时网络安全需求，本文提出了一种面向移动边缘计算（MEC）的异常检测系统。该方法利用深度学习技术分析网络流量以检测异常，并结合基于策略的机制，对检测过程中使用的计算资源进行动态管理。实验结果表明，该系统能够在5G MEC框架内实现高效、自主的实时异常检测。</div>
</details>
</div>
<div class="card">
<div class="title">Finding Kissing Numbers with Game-theoretic Reinforcement Learning</div>
<div class="meta-line">Authors: Chengdong Ma, Théo Tao Zhaowei, Pengyu Li, Minghao Liu, Haojun Chen, Zihao Mao, Yuan Cheng, Yuan Qi, Yaodong Yang</div>
<div class="meta-line">First: 2025-11-17T14:02:00+00:00 · Latest: 2026-01-21T16:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13391v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13391v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert&#x27;s 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game that can be fully parallelized at large scale, and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions, discovers over 6000 new structures in 14 and other dimensions, and establishes new records for generalized kissing configurations under various angular constraints. These results demonstrate AI&#x27;s power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于博弈论强化学习的接吻数求解</div>
<div class="mono" style="margin-top:8px">自艾萨克·牛顿于1694年首次研究接吻数问题以来，确定中心球体外围可容纳非重叠球体的最大数量始终是基础性难题。该问题是希尔伯特第18问题（球体堆积）的局部类比，连接了几何学、数论与信息论。尽管通过格点与编码理论已取得重要进展，但高维几何的不规则性及8维以上指数级增长的组合复杂度（超过围棋复杂度），限制了现有方法的可扩展性。本研究将该问题建模为可大规模并行化的双玩家矩阵补全博弈，并训练博弈论强化学习系统PackingStar以高效探索高维空间。矩阵元素表示球心向量间的成对余弦值：一方填充元素，另一方修正次优项，协同最大化矩阵规模（对应接吻数）。这种合作动力学显著提升样本质量，使极大规模空间可计算。PackingStar复现了已有构型，并在25至31维突破所有人类已知记录——其中25维构型几何对应Leech格点，暗示其潜在最优性。系统首次在13维突破1971年以来的有理结构局限，在14维及其他维度发现6000余个新结构，并在多种角度约束下建立了广义接吻构型的新记录。这些成果彰显了人工智能探索超越人类直觉的高维空间的能力，为接吻数问题及更广泛的几何问题开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Kissing Number Problem, which seeks the maximum number of non-overlapping spheres that can touch a central sphere, is a long-standing geometric challenge with connections to sphere packing and high-dimensional complexity. To address the combinatorial explosion in dimensions beyond eight, this work models the problem as a two-player matrix completion game, where entries represent pairwise cosines of sphere center vectors; one player proposes entries while another corrects suboptimal ones, jointly maximizing the matrix size through a cooperative reinforcement learning system called PackingStar. This method efficiently explores high-dimensional spaces, reproducing known configurations and surpassing all previous human records from dimensions 25 to 31, achieving a breakthrough beyond rational structures in dimension 13 for the first time since 1971, discovering over 6000 new structures in dimension 14 and others, and setting new records for generalized kissing configurations under various angular constraints.</div>
<div class="mono" style="margin-top:8px">自牛顿时代以来，亲吻数问题作为一个长期存在的几何挑战，旨在寻找一个中心球体周围可接触的最大非重叠球体数量，其中高维情况因指数级计算复杂性而难以处理。为解决此问题，作者将该问题建模为一个双玩家矩阵补全游戏：一名玩家填充表示球心向量对之间余弦的矩阵条目，另一名玩家修正次优条目，共同最大化矩阵大小以寻找亲吻数；这种合作的博弈论强化学习系统名为PackingStar，能够高效并行探索高维空间。实验上，PackingStar不仅复现了已知构型，还超越了25至31维的所有人类已知记录，在25维中暗示了与Leech格点对应的可能最优性，在13维实现了自1971年以来的首次突破，在14维及其他维度发现了超过6000个新结构，并为各种角度约束下的广义亲吻构型设立了新记录，展示了AI探索超越人类直觉的高维空间的能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</div>
<div class="meta-line">Authors: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</div>
<div class="meta-line">First: 2026-01-21T16:41:58+00:00 · Latest: 2026-01-21T16:41:58+00:00</div>
<div class="meta-line">Comments: Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15165v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15165v1">PDF</a> · <a href="https://github.com/LeapLabTHU/JustGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://nzl-thu.github.io/the-flexibility-trap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活性陷阱：为何任意顺序限制扩散语言模型的推理潜力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）打破了传统LLM严格的从左到右约束，实现了任意顺序的标记生成。直观上，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学和编程等通用任务解锁了更优的推理潜力。因此，许多研究采用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但没有扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用这种顺序灵活性来规避对探索至关重要的高不确定性标记，导致解空间过早坍缩。这一观察挑战了现有dLLMs强化学习方法的前提——这些方法往往投入大量复杂度（如处理组合轨迹和难解似然）以保持这种灵活性。我们证明，通过有意放弃任意顺序并改用标准组相对策略优化（GRPO），能更有效地激发推理能力。我们的方法JustGRPO极简却出奇有效（例如在GSM8K上达到89.1%准确率），同时完全保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates the reasoning capabilities of diffusion language models (dLLMs), which allow token generation in arbitrary orders, theoretically expanding the solution space compared to fixed left-to-right autoregressive models. The authors find that this flexibility paradoxically harms reasoning, as models exploit it to bypass high-uncertainty tokens crucial for exploration, leading to a premature collapse of the solution space. They demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order generation and applying standard Group Relative Policy Optimization (GRPO); their minimalist approach, JustGRPO, achieves strong results (e.g., 89.1% accuracy on GSM8K) while retaining parallel decoding ability.</div>
<div class="mono" style="margin-top:8px">本文研究了扩散语言模型（dLLMs）的推理能力，这类模型允许以任意顺序生成词元，理论上比固定的自左向右自回归模型更具灵活性。然而，与直觉相反，作者发现这种灵活性会导致模型绕过对探索至关重要的高不确定性词元，从而过早地缩小解空间。为解决此问题，他们提出了JustGRPO方法，该方法有意放弃任意顺序生成，转而应用标准的组相对策略优化，结果证明非常有效——在GSM8K上达到89.1%的准确率——同时保留了并行解码能力。</div>
</details>
</div>
<div class="card">
<div class="title">V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks</div>
<div class="meta-line">Authors: Yaru Liu, Ao-bo Wang, Nanyang Ye</div>
<div class="meta-line">First: 2026-01-21T16:41:51+00:00 · Latest: 2026-01-21T16:41:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15164v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15164v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently &quot;succeed&quot; without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., &quot;get ready for work&quot;) into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out &quot;silent failures&quot; where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-CAGE：面向可扩展长程具身任务的情境感知生成与验证框架</div>
<div class="mono" style="margin-top:8px">从合成数据中学习长程具身行为仍面临挑战：生成场景常违反物理规律，语言驱动程序常在不满足任务语义时即显示“成功”，高层指令需具体化为可执行动作序列。为突破这些局限，我们提出V-CAGE——一个用于大规模生成鲁棒、语义对齐操作数据的闭环框架。首先，我们提出情境感知实例化机制，在场景合成时强制保持几何一致性。通过动态维护物体放置过程中的空间禁置区域映射，系统能防止物体穿模，确保杂乱环境中生成可达且无冲突的配置。其次，为弥合抽象意图与底层控制间的鸿沟，我们采用分层指令分解模块，将高层目标（如“准备工作”）拆解为组合式动作基元，促进连贯的长程规划。关键的是，我们通过基于视觉语言模型的验证循环强制执行语义正确性：该模型作为视觉评判器，在每个子任务后执行严格拒绝采样，过滤代码可执行但未达成视觉目标的“静默失败”。实验表明，相较于未验证基线，V-CAGE生成的数据集具有更优的物理与语义保真度，能显著提升下游策略的成功率与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating physically plausible scenes and semantically correct action sequences for long-horizon embodied tasks from synthetic data, where existing methods often produce implausible scenes or suffer from &#x27;silent failures&#x27; where programs execute without achieving the intended goal. The proposed V-CAGE framework introduces a context-aware instantiation mechanism to ensure geometric consistency in scene synthesis by maintaining a map of prohibited areas to prevent object interpenetration, and a hierarchical instruction decomposition module to break high-level goals into executable action primitives; it further employs a VLM-based verification loop to perform rejection sampling after each subtask, filtering out semantically incorrect outcomes. Experimental results show that V-CAGE generates datasets with improved physical and semantic fidelity, leading to higher success rates and better generalization in downstream policy learning compared to non-verified baselines.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决从合成数据学习长程具身行为时的挑战，即生成的场景常缺乏物理合理性，且语言程序可能在未满足任务语义的情况下“成功”。提出的V-CAGE框架包含一个上下文感知实例化机制，通过动态追踪禁止区域来防止物体穿插，确保场景合成中的几何一致性；同时采用分层指令分解模块，将高级目标分解为可执行的动作基元。关键创新在于基于视觉语言模型（VLM）的验证循环，它在每个子任务后执行拒绝采样，以过滤掉语义错误的结果。实验表明，V-CAGE生成的数据集具有更高的物理和语义保真度，与非验证基线相比，显著提升了下游策略的成功率和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems</div>
<div class="meta-line">Authors: Yinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz</div>
<div class="meta-line">First: 2026-01-21T16:40:41+00:00 · Latest: 2026-01-21T16:40:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15161v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15161v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动化评估量表在医疗对话系统可靠评估中的应用</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在临床决策支持中的应用日益广泛，但其产生的幻觉和不安全建议可能直接威胁患者安全。这些风险尤其棘手，因为它们常表现为细微的临床错误，难以被通用指标检测，而专家编写的细粒度评估量表又成本高昂且难以扩展。本文提出一种检索增强的多智能体框架，旨在自动化生成针对具体实例的评估量表。该方法通过将检索内容分解为原子事实，并结合用户交互约束，形成可验证的细粒度评估标准，从而将评估建立在权威医学证据基础上。在HealthBench上的评估显示，该框架的临床意图对齐（CIA）得分达到60.12%，较GPT-4o基线（55.16%）有统计显著提升。在判别性测试中，本方法生成的量表平均得分差（μ_Δ = 8.658）和AUROC值（0.977）接近GPT-4o基线（4.972）的两倍。除评估外，该量表还能有效指导响应优化，将质量提升9.2%（从59.0%提高至68.2%）。这为评估和改进医疗LLMs提供了可扩展且透明的技术基础。代码发布于https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical need for reliable evaluation of medical dialogue systems, as Large Language Models (LLMs) used in clinical support can produce subtle but dangerous errors that generic metrics miss, while expert-authored rubrics are costly and non-scalable. The proposed method is a retrieval-augmented multi-agent framework that automates the generation of instance-specific evaluation rubrics by grounding them in authoritative medical evidence; it decomposes retrieved content into atomic facts and synthesizes them with user constraints to create verifiable, fine-grained criteria. Key experimental results on HealthBench show the framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, significantly outperforming a GPT-4o baseline (55.16%), and in discriminative tests, it yields a mean score delta of 8.658 and an AUROC of 0.977, nearly doubling the quality separation of the baseline; additionally, the rubrics guide response refinement, improving quality by 9.2% from 59.0% to 68.2%.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要可靠地评估使用大语言模型的医疗对话系统，因为通用指标常忽略细微的临床错误，而专家编写的细粒度评估标准成本高且难以扩展。方法提出了一种检索增强的多智能体框架，通过基于权威医学证据自动化生成实例特定的评估标准，将检索内容分解为原子事实，并结合用户约束合成可验证的细粒度准则。在HealthBench上的关键实验结果表明，该框架的临床意图对齐得分达到60.12%，显著优于GPT-4o基线（55.16%），判别性测试中平均得分差为8.658，AUROC为0.977，几乎是基线质量分离度的两倍，且该标准还能指导响应优化，将质量提升9.2%。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning</div>
<div class="meta-line">Authors: Yuval Kansal, Niraj K. Jha</div>
<div class="meta-line">First: 2026-01-21T16:38:59+00:00 · Latest: 2026-01-21T16:38:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &quot;compositional bridge&quot;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识图谱作为隐式奖励模型：路径衍生信号赋能组合推理</div>
<div class="mono" style="margin-top:8px">大型语言模型在数学和编程等结构化推理领域已接近专家水平，但在专业科学领域进行组合式多跳推理的能力仍有限。我们提出一种自底向上的学习范式，使模型基于公理化领域事实进行组合以解决复杂未知任务。为此，我们提出一种结合监督微调与强化学习的后训练流程，其中知识图谱充当隐式奖励模型。通过从知识图谱路径中衍生新颖的奖励信号，我们提供可验证、可扩展且具根基的监督机制，促使模型在强化学习过程中组合中间公理而非仅优化最终答案。我们在医学领域验证该方法，使用短跳推理路径（1-3跳）训练140亿参数模型，并评估其对复杂多跳查询（4-5跳）的零样本泛化能力。实验表明，路径衍生奖励充当“组合桥梁”，使我们的模型在最困难推理任务上显著超越GPT-5.2和Gemini 3 Pro等更大规模前沿系统。此外，我们通过选项重排压力测试验证了该方法对对抗性扰动的鲁棒性。本研究表明，将推理过程锚定于结构化知识是实现智能推理的可扩展高效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of large language models in performing compositional multi-hop reasoning within specialized scientific domains, despite their proficiency in structured reasoning areas like mathematics. The proposed method introduces a bottom-up learning paradigm that grounds models in axiomatic domain facts, using a post-training pipeline combining supervised fine-tuning and reinforcement learning where knowledge graphs serve as implicit reward models. By deriving reward signals from knowledge graph paths, the approach provides verifiable and grounded supervision to encourage compositional reasoning over intermediate steps rather than focusing solely on final answers. Experimental validation in the medical domain with a 14B model trained on short-hop reasoning paths demonstrates significant zero-shot generalization to complex multi-hop queries, outperforming larger models and frontier systems like GPT-5.2 and Gemini 3 Pro on difficult tasks, while also showing robustness to adversarial perturbations in option-shuffling stress tests.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型在专业科学领域进行组合式多跳推理能力有限的问题，尽管它们在数学等结构化领域表现优异。提出的方法采用自底向上的学习范式，将模型基于公理化的领域事实，通过结合监督微调和强化学习的后训练流程，利用知识图谱作为隐式奖励模型。通过从知识图谱路径中推导奖励信号，该方法提供了可验证且基于事实的监督，以鼓励模型在推理过程中组合中间公理而非仅优化最终答案。在医学领域的实验中，使用一个140亿参数的模型在短跳路径（1-3跳）上训练，并在复杂多跳查询（4-5跳）上实现零样本泛化，结果表明其在最困难的推理任务上显著优于更大的模型及前沿系统如GPT-5.2和Gemini 3 Pro，同时在对选项重排的压力测试中展现出对抗扰动的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</div>
<div class="meta-line">Authors: Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-05-29T04:09:19+00:00 · Latest: 2026-01-21T16:37:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23062v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.23062v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating pre-collected offline data can substantially improve the sample efficiency of reinforcement learning (RL), but its benefits can break down when the transition dynamics in the offline dataset differ from those encountered online. Existing approaches typically mitigate this issue by penalizing or filtering offline transitions in regions with large dynamics gap. However, their dynamics-gap estimators often rely on KL divergence or mutual information, which can be ill-defined when offline and online dynamics have mismatched support. To address this challenge, we propose CompFlow, a principled framework built on the theoretical connection between flow matching and optimal transport. Specifically, we model the online dynamics as a conditional flow built upon the output distribution of a pretrained offline flow, rather than learning it directly from a Gaussian prior. This composite structure provides two advantages: (1) improved generalization when learning online dynamics under limited interaction data, and (2) a well-defined and stable estimate of the dynamics gap via the Wasserstein distance between offline and online transitions. Building on this dynamics-gap estimator, we further develop an optimistic active data collection strategy that prioritizes exploration in high-gap regions, and show theoretically that it reduces the performance gap to the optimal policy. Empirically, CompFlow consistently outperforms strong baselines across a range of RL benchmarks with shifted-dynamics data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向动态偏移数据强化学习的复合流匹配方法</div>
<div class="mono" style="margin-top:8px">利用预收集的离线数据可显著提升强化学习的样本效率，但当离线数据集中的状态转移动态与在线环境存在差异时，其优势可能失效。现有方法通常通过对动态差异较大区域的离线转移施加惩罚或过滤来缓解此问题，但其动态差异估计器常依赖KL散度或互信息，在离线与在线动态分布支撑集不匹配时可能失效。为此，我们提出CompFlow框架，该框架基于流匹配与最优传输的理论关联构建。具体而言，我们将在线动态建模为基于预训练离线流输出分布的条件流，而非直接从高斯先验学习。这种复合结构具有双重优势：(1) 在有限交互数据下学习在线动态时提升泛化能力；(2) 通过离线与在线转移的Wasserstein距离获得定义明确且稳定的动态差异估计。基于此估计器，我们进一步提出乐观主动数据收集策略，优先探索高差异区域，并从理论上证明该策略能缩小与最优策略的性能差距。实验表明，在多个含动态偏移数据的强化学习基准测试中，CompFlow均稳定优于现有强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of leveraging offline data in reinforcement learning when the transition dynamics differ from the online environment, as existing methods using KL divergence or mutual information can fail when dynamics have mismatched support. The proposed CompFlow framework models online dynamics as a conditional flow built upon a pretrained offline flow&#x27;s output distribution, utilizing the connection between flow matching and optimal transport. This approach improves generalization with limited online data and provides a stable dynamics-gap estimate via Wasserstein distance, enabling an optimistic active data collection strategy that prioritizes high-gap regions. Experimental results demonstrate that CompFlow consistently outperforms strong baselines across various RL benchmarks with shifted-dynamics data.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习中离线数据与在线环境动态转移不一致时难以有效利用的问题，现有方法依赖KL散度或互信息估计动态差异，在动态分布支撑集不匹配时会失效。提出的CompFlow框架基于流匹配与最优传输的理论联系，将在线动态建模为基于预训练离线流输出分布的条件流，这种复合结构在有限交互数据下改善了泛化能力，并通过Wasserstein距离提供了稳定的动态差异估计器，从而实现了优先探索高差异区域的乐观主动数据收集策略。实验结果表明，在多个动态偏移的强化学习基准测试中，CompFlow consistently outperforms strong baselines across various RL benchmarks with shifted-dynamics data，理论分析表明该方法能缩小与最优策略的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-01-21T16:36:19+00:00</div>
<div class="meta-line">Comments: 80 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动梯度下降发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的梯度流动力学来解决这一问题——该任务若无思维链则无法解决，但存在简单的迭代解法。我们证明，尽管仅基于最终答案正确性进行训练，梯度流仍会驱动模型收敛至一种结构化、可解释的逐顶点迭代遍历算法。我们刻画了这种涌现现象所需的分布特性，指出“简单示例”（需要较少推理步骤的实例）的关键作用：当训练分布中此类简单实例具有足够权重时，模型可学习能泛化至更长链的遍历策略；若该权重消失，则基于梯度的学习将不可行。我们通过合成数据实验及真实世界语言模型的数学推理任务验证了理论结果，证明理论发现适用于实际场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to understand how Transformers trained with outcome-based Reinforcement Learning (RL) can spontaneously develop systematic reasoning steps (Chain-of-Thought) from sparse rewards, a mechanism that remains unclear. The method involves analyzing gradient flow dynamics in single-layer Transformers on a synthetic graph traversal task that necessitates Chain-of-Thought, proving that training solely on final-answer correctness leads the model to converge to an interpretable, iterative traversal algorithm. Key experimental findings show that the emergence of this reasoning ability critically depends on the training distribution containing sufficient &#x27;simple examples&#x27; (instances with fewer steps), which enables generalization to longer chains; without such examples, learning becomes infeasible, a result corroborated by experiments on synthetic data and real-world language models on mathematical reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究基于结果的强化学习如何使Transformer模型发展出思维链推理能力，这一机制此前未被充分理解。作者通过分析单层Transformer在合成图遍历任务上的梯度流动态，证明了仅使用最终答案的稀疏奖励进行训练，模型会收敛到一个可解释的迭代算法。研究指出，训练数据中“简单示例”的存在对于这种能力的涌现以及向更长推理链的泛化至关重要，这一理论发现在合成数据和真实世界数学推理任务的实验中都得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework</div>
<div class="meta-line">Authors: Choro Ulan uulu, Mikhail Kulyabin, Iris Fuhrmann, Jan Joosten, Nuno Miguel Martins Pacheco, Filippos Petridis, Rebecca Johnson, Jan Bosch, Helena Holmström Olsson</div>
<div class="meta-line">First: 2026-01-21T16:23:22+00:00 · Latest: 2026-01-21T16:23:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15153v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline&#x27;s poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>如何通过将人类专家领域知识编码化增强大语言模型以构建AI智能体？一个软件工程框架</div>
<div class="mono" style="margin-top:8px">关键领域知识通常仅掌握在少数专家手中，导致组织在可扩展性和决策方面出现瓶颈。非专业人员难以创建有效的可视化图表，导致洞察力不足并挤占专家时间。本文通过工业案例研究，探讨如何将人类领域知识捕获并嵌入AI智能体系统。我们提出一个软件工程框架，通过为大型语言模型（LLM）增强请求分类器、用于代码生成的检索增强生成（RAG）系统、编码化的专家规则以及可视化设计原则，构建能展现自主、反应、主动及社交行为的智能体，从而捕获人类领域知识以开发仿真数据可视化AI智能体。在涵盖多个工程领域的五个场景中，由12名评估者进行的测试表明：输出质量提升206%，我们的智能体在所有案例中均获得专家级评分（基线表现较差），同时保持更优的代码质量与更低方差。我们的贡献包括：一个基于智能体的自动化可视化生成系统，以及一个经过验证的框架——用于系统化捕获人类领域知识、将隐性专家知识编码化融入AI智能体，证明非专家能在专业领域实现专家级成果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the bottleneck caused by limited access to human domain experts, which hinders scalability and decision-making, particularly in specialized tasks like simulation data visualization where non-experts produce suboptimal results. The authors propose a software engineering framework to build AI agents by augmenting a Large Language Model with a request classifier, a Retrieval-Augmented Generation system for code, and codified expert rules and design principles, enabling the agent to exhibit autonomous, reactive, proactive, and social behaviors. In an evaluation across five engineering scenarios with 12 evaluators, the agent demonstrated a 206% improvement in output quality, achieving expert-level ratings in all cases compared to a poor-performing baseline, while also generating higher-quality code with lower variance.</div>
<div class="mono" style="margin-top:8px">本研究针对关键领域知识仅由少数专家掌握所导致的组织可扩展性和决策瓶颈问题，特别是在可视化任务中，非专家常产生次优结果。作者提出了一个软件工程框架，通过增强大型语言模型，结合请求分类器、用于代码生成的检索增强生成系统，以及编码化的专家规则和可视化设计原则，构建了一个能够自主、反应式、主动式和社交式行为的AI智能体，用于仿真数据可视化。在多个工程领域的五个场景中，由12名评估者进行的实验评估显示，输出质量提升了206%，该智能体在所有案例中均获得专家级评分，而基线表现较差，同时保持了更优且更稳定的代码质量。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
