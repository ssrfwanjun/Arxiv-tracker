<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-07 04:51</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260207_0451</div>
    <div class="row"><div class="card">
<div class="title">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</div>
<div class="meta-line">Authors: Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-05T18:52:48+00:00 · Latest: 2026-02-05T18:52:48+00:00</div>
<div class="meta-line">Comments: Project Page: https://genarena.github.io/, Code: https://github.com/ruihanglix/genarena</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06013v1">PDF</a> · <a href="https://github.com/ruihanglix/genarena">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://genarena.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenArena：如何实现视觉生成任务的人类对齐评估？</div>
<div class="mono" style="margin-top:8px">视觉生成模型的快速发展已超越传统评估方法，亟需采用视觉语言模型作为替代评判者。本研究系统探究了主流绝对点式评分标准在广泛视觉生成任务中的可靠性，分析表明该范式因随机不一致性及与人类感知对齐性差而受限。为解决这些局限，我们提出GenArena——一个采用成对比较范式的统一评估框架，以确保稳定且人类对齐的评估。关键实验发现：仅采用此成对协议即可使现成开源模型超越顶级专有模型。该方法将评估准确率提升超20%，与权威LMArena排行榜的斯皮尔曼相关系数达0.86，显著超越点式方法的0.36。基于GenArena，我们对前沿视觉生成模型进行多任务基准测试，为社区提供严谨的自动化视觉生成评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid progress in visual generation models has rendered traditional evaluation methods inadequate, prompting the use of Vision-Language Models as judges; however, current absolute pointwise scoring standards suffer from stochastic inconsistency and misalignment with human judgment. To address this, the authors introduce GenArena, a unified evaluation framework that employs a pairwise comparison paradigm to ensure stable and human-aligned assessment. Experimental results demonstrate that adopting this pairwise protocol allows off-the-shelf open-source models to outperform top proprietary models, boosting evaluation accuracy by over 20% and achieving a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, far surpassing the 0.36 correlation of pointwise methods.</div>
<div class="mono" style="margin-top:8px">视觉生成模型的快速发展使得传统评估方法不足，促使采用视觉语言模型作为评判者；然而，当前绝对点式评分标准存在随机不一致性和与人类感知的偏差。为解决这一问题，GenArena提出了一个基于成对比较的统一评估框架，以提高稳定性和人类对齐性。实验表明，这种成对方法使开源模型能够超越专有模型，将评估准确率提升超过20%，并与权威基准的斯皮尔曼相关性达到0.86，远超点式方法的0.36相关性。</div>
</details>
</div>
<div class="card">
<div class="title">GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra</div>
<div class="meta-line">Authors: Mateusz Michalkiewicz, Anekha Sokhal, Tadeusz Michalkiewicz, Piotr Pawlikowski, Mahsa Baktashmotlagh, Varun Jampani, Guha Balakrishnan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-09T20:11:21+00:00 · Latest: 2026-02-05T16:06:21+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. Camera ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08194v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.08194v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern monocular 3D reconstruction methods and vision-language models (VLMs) demonstrate impressive results on standard benchmarks, yet recent works cast doubt on their true understanding of geometric properties. We introduce GOQ, a comprehensive benchmark specifically designed to evaluate the geometric reasoning capabilities of vision and vision-language foundation models. GIQ comprises synthetic and real-world images and corresponding 3D meshes of diverse polyhedra covering varying levels of complexity and symmetry, from Platonic, Archimedean, Johnson, and Catalan solids to stellations and compound shapes. Through systematic experiments involving monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification tasks, we reveal significant shortcomings in current models. State-of-the-art reconstruction algorithms trained on extensive 3D datasets struggle to reconstruct even basic geometric Platonic solids accurately. Next, although foundation models may be shown via linear and non-linear probing to capture specific 3D symmetry elements, they falter significantly in tasks requiring detailed geometric differentiation, such as mental rotation. Moreover, advanced vision-language assistants such as ChatGPT, Gemini and Claud exhibit remarkably low accuracy in interpreting basic shape properties such as face geometry, convexity, and compound structures of complex polyhedra. GIQ is publicly available at toomanymatts.github.io/giq-benchmark/, providing a structured platform to benchmark critical gaps in geometric intelligence and facilitate future progress in robust, geometry-aware representation learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIQ：基于模拟与真实多面体的视觉基础模型三维几何推理能力基准测试</div>
<div class="mono" style="margin-top:8px">现代单目三维重建方法与视觉语言模型在标准基准测试中表现优异，但近期研究对其几何特性理解能力提出质疑。我们提出GIQ基准测试，专门用于评估视觉与视觉语言基础模型的几何推理能力。GIQ包含合成及真实场景图像，以及对应不同复杂度与对称性的多面体三维网格，涵盖柏拉图立体、阿基米德立体、约翰逊立体、卡塔兰立体，以及星形多面体与复合形体。通过系统实验（包括单目三维重建、三维对称性检测、心理旋转测试和零样本形状分类任务），我们揭示了当前模型的显著缺陷：在大量三维数据集上训练的最优重建算法难以准确重建基础柏拉图立体；基础模型虽能通过线性和非线性探测捕获特定三维对称元素，但在需要精细几何区分的任务（如心理旋转）中表现欠佳；ChatGPT、Gemini和Claud等先进视觉语言助手在解析复杂多面体的面几何、凸性及复合结构等基础形状属性时准确率极低。GIQ已公开于toomanymatts.github.io/giq-benchmark/，为评估几何智能关键缺陷提供结构化平台，助力未来鲁棒性几何感知表征学习的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by concerns that modern vision and vision-language models may lack a true understanding of geometric properties despite strong benchmark performance, this work introduces GIQ, a benchmark for evaluating 3D geometric reasoning using diverse polyhedra from synthetic and real images. The method involves systematic experiments including monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification. Key findings reveal significant shortcomings: state-of-the-art reconstruction models struggle to accurately reconstruct basic Platonic solids; foundation models capture some symmetry elements but fail at detailed geometric differentiation like mental rotation; and advanced vision-language assistants exhibit low accuracy in interpreting basic shape properties of complex polyhedra.</div>
<div class="mono" style="margin-top:8px">本研究针对现代视觉和视觉语言模型几何理解能力有限的问题，引入了GIQ基准，该基准包含多样多面体的合成与真实图像，用于评估三维几何推理能力。方法包括对单目三维重建、对称性检测、心理旋转和零样本形状分类进行系统实验。主要实验结果表明，最先进的重建模型难以准确重建基本的柏拉图立体，基础模型在心理旋转等需要精细几何区分的任务上表现不佳，而先进的视觉语言助手在解释复杂多面体的面几何、凸性和复合结构等基本形状属性时准确率极低。</div>
</details>
</div>
<div class="card">
<div class="title">Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning</div>
<div class="meta-line">Authors: Enwei Tong, Yuanchao Bai, Yao Zhu, Junjun Jiang, Xianming Liu</div>
<div class="meta-line">First: 2026-02-05T16:02:48+00:00 · Latest: 2026-02-05T16:02:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05809v1">PDF</a> · <a href="https://github.com/ILOT-code/FSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>聚焦-扫描-精炼：从人类视觉感知到高效视觉令牌剪枝</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）常生成大量视觉令牌，显著增加推理延迟和内存占用；尽管免训练的令牌剪枝提供了实用解决方案，现有方法在激进压缩下仍难以平衡局部证据与全局上下文。我们提出聚焦-扫描-精炼（FSR），一种受人类启发的即插即用剪枝框架，模拟人类回答视觉问题的过程：聚焦关键证据，必要时全局扫描，并通过聚合相关细节精炼扫描内容。FSR首先结合视觉重要性与指令相关性聚焦关键证据，避免偏向视觉显著但与查询无关的区域；随后基于聚焦集扫描补充上下文，选择与聚焦证据差异最大的令牌；最后通过基于相似度的分配和分数加权融合，将邻近信息令牌聚合至扫描锚点，在不增加令牌预算的情况下精炼扫描内容。跨多个VLM骨干和视觉语言基准的广泛实验表明，FSR在精度-效率权衡上持续优于现有先进剪枝方法。源代码位于 https://github.com/ILOT-code/FSR</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models suffer from high computational costs due to massive visual tokens, and existing training-free pruning methods struggle to balance local evidence and global context under aggressive compression. To address this, the authors propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics human visual question-answering by first focusing on key evidence using visual importance and instruction relevance, then scanning for complementary context conditioned on the focused set, and finally refining the scanned context by aggregating nearby informative tokens via similarity-based assignment and score-weighted merging without increasing the token budget. Experimental results across multiple VLM backbones and benchmarks demonstrate that FSR consistently improves the accuracy-efficiency trade-off over state-of-the-art pruning methods.</div>
<div class="mono" style="margin-top:8px">视觉语言模型因生成大量视觉令牌而导致计算成本高昂，现有的免训练剪枝方法在激进压缩下难以平衡局部证据与全局上下文。为此，研究者提出受人类视觉启发的即插即用剪枝框架Focus-Scan-Refine（FSR），其首先通过结合视觉重要性与指令相关性聚焦关键证据，随后基于聚焦集扫描补充性上下文，最后通过基于相似性的分配和分数加权合并来聚合邻近信息令牌以精炼扫描内容。在多个VLM骨干模型和视觉语言基准上的实验表明，FSR相比现有先进剪枝方法能持续实现更优的精度-效率权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation</div>
<div class="meta-line">Authors: Hengyi Wang, Ruiqiang Zhang, Chang Liu, Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</div>
<div class="meta-line">First: 2026-02-05T15:45:39+00:00 · Latest: 2026-02-05T15:45:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05789v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05789v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>他中心感知器：通过框架实例化从他中心视觉先验中解耦他中心推理</div>
<div class="mono" style="margin-top:8px">随着视觉语言导航/动作等空间定位任务需求的增长，视觉语言模型的他中心感知能力日益受到关注。然而，在需要显式视角转换的他中心空间查询任务中，视觉语言模型仍显脆弱——这类任务的答案需基于目标中心框架而非观察相机视角进行推理。为此，我们提出&#x27;他中心感知器&#x27;，这是一种免训练策略：首先利用现成几何专家从单幅或多幅图像恢复度量三维状态，随后实例化一个与指令语义意图对齐的查询条件化他中心参考框架。通过将重建几何确定性地转换至目标框架，并向骨干视觉语言模型提供结构化、几何基础的表征，本方法将心理旋转从隐式推理转移至显式计算。我们在空间推理基准测试中对多种骨干模型进行评估，发现该方法在他中心任务上获得持续显著提升（约10%），同时保持强大的自我中心性能，且超越了经过空间感知微调的模型以及当前最先进的开源与专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the brittleness of Vision-Language Models (VLMs) in allocentric spatial reasoning, where answers depend on a target-centric frame rather than the observed egocentric view, which is crucial for tasks like Vision-Language Navigation. The method, Allocentric Perceiver, is a training-free strategy that recovers metric 3D states from images using off-the-shelf geometric tools, instantiates a query-conditioned allocentric reference frame, and transforms the reconstructed geometry into this target frame to provide structured, geometry-grounded representations to a backbone VLM, thereby offloading mental rotation to explicit computation. Experimental results on spatial reasoning benchmarks show consistent and substantial gains (approximately 10%) on allocentric tasks while maintaining strong egocentric performance, outperforming both fine-tuned models and state-of-the-art open-source and proprietary models.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在以目标为中心的异中心空间推理任务中的脆弱性问题，这类任务的答案依赖于目标参照系而非相机自我中心视角，对于视觉语言导航等应用至关重要。所提出的方法“异中心感知器”是一种免训练策略，它利用现成的几何工具从图像中恢复度量3D状态，实例化一个与查询语义意图对齐的异中心参考系，并将重建的几何结构转换到该目标系中，从而为骨干视觉语言模型提供结构化的、基于几何的表征，将心理旋转从隐式推理卸载为显式计算。实验结果表明，该方法在多个骨干视觉语言模型家族上，对异中心任务实现了持续且显著的性能提升（约10%），同时保持了强大的自我中心任务性能，超越了经过空间感知微调的模型以及最先进的开源和专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Ethology of Latent Spaces</div>
<div class="meta-line">Authors: Philippe Boisnard</div>
<div class="meta-line">First: 2026-02-05T14:37:31+00:00 · Latest: 2026-02-05T14:37:31+00:00</div>
<div class="meta-line">Comments: 23. pages, 14 figures, presented Hyperheritage International Symposium 9 ( https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf ) and accepted for publication in double-blind peer review in French in 2026-2027</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.
  Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.
  We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault&#x27;s notion of the archive, Jameson&#x27;s ideologeme, and Simondon&#x27;s theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在空间的动物行为学研究</div>
<div class="mono" style="margin-top:8px">本研究通过动物行为学视角审视视觉语言模型（VLMs）中潜在空间的算法行为，挑战了其预设的中立性。潜在空间并非均质不确定性的空间，而是展现出模型特定的算法敏感性，这种敏感性可理解为由训练数据和架构选择塑造的感知显著性差异机制。
通过对三个模型（OpenAI CLIP、OpenCLIP LAION、SigLIP）应用于301件艺术作品（15至20世纪）的语料库进行对比分析，我们揭示了其在政治与文化类别归属上的显著分歧。利用源自向量类比（Mikolov et al., 2013）的双极语义轴，我们发现SigLIP将59.4%的作品归类为政治参与型，而OpenCLIP仅4%。非洲面具在SigLIP中获得最高政治评分，而在OpenAI CLIP中则保持非政治性。在美学殖民轴上，模型间差异达到72.6个百分点。
我们提出三个操作概念：计算性潜在政治化（描述政治类别在无意识编码下的涌现）、涌现性偏差（无法简化为统计或规范偏差，仅通过对比分析可检测）以及三种算法视域机制：熵化（LAION）、制度化（OpenAI）和符号化（SigLIP），它们构建了不同的可见性模式。借鉴福柯的档案概念、詹姆逊的意识形态素与西蒙东的个体化理论，我们认为训练数据集发挥着准档案功能，其话语形构在潜在空间中结晶。本研究有助于批判性重估VLMs应用于数字艺术史的条件，并呼吁在将文化阐释委托给算法代理时，需整合学习架构的方法论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study challenges the assumption that latent spaces in vision-language models are neutral, arguing they exhibit model-specific algorithmic sensitivities shaped by training data and architecture. By comparing OpenAI CLIP, OpenCLIP LAION, and SigLIP on 301 artworks using bipolar semantic axes derived from vector analogies, the research reveals substantial divergences in attributing political and cultural categories; for instance, SigLIP classified 59.4% of artworks as politically engaged versus only 4% for OpenCLIP, with discrepancies on an aesthetic colonial axis reaching 72.6 percentage points. The findings introduce concepts like computational latent politicization and emergent bias, identifying three algorithmic scopic regimes—entropic, institutional, and semiotic—that structure distinct modes of visibility, thereby calling for critical reassessment in applying such models to digital art history.</div>
<div class="mono" style="margin-top:8px">本研究从行为学视角质疑视觉语言模型（VLMs）中潜在空间的中立性假设，认为这些空间因训练数据和架构选择而形成了模型特定的算法敏感性。方法上，对三种模型（OpenAI CLIP、OpenCLIP LAION 和 SigLIP）进行了比较分析，将其应用于301件15至20世纪的艺术作品，并利用向量类比衍生的双极语义轴来评估政治和文化分类。主要实验结果显示显著差异：例如，SigLIP将59.4%的作品归类为具有政治参与性，而OpenCLIP仅为4%；非洲面具在SigLIP中获得高政治评分，但在OpenAI CLIP中则无；在美学殖民轴上的模型间差异达到72.6个百分点，揭示了涌现性偏见和不同的算法视觉机制。</div>
</details>
</div>
<div class="card">
<div class="title">LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation</div>
<div class="meta-line">Authors: Junyang Chen, Xiangbo Lv, Zhiqiang Kou, Xingdong Sheng, Ning Xu, Yiguo Qiao</div>
<div class="meta-line">First: 2026-02-05T12:03:11+00:00 · Latest: 2026-02-05T12:03:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05578v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LoGoSeg：融合局部与全局特征的开集词汇语义分割方法</div>
<div class="mono" style="margin-top:8px">开集词汇语义分割（OVSS）通过任意文本描述对可见与不可见类别进行像素级标注，拓展了传统闭集分割范式。现有方法虽利用CLIP等视觉语言模型，但其依赖图像级预训练常导致空间对齐不精确，在模糊或复杂场景中产生误分割。然而多数现有方法缺乏强目标先验和区域级约束，易引发目标幻觉或漏检，进一步影响性能。为此，我们提出LoGoSeg——一种高效的单阶段框架，集成三项关键创新：（i）通过全局图文相似度动态加权相关类别的目标存在先验，有效抑制幻觉；（ii）建立精确区域级视觉-文本对应关系的区域感知对齐模块；（iii）局部结构信息与全局语义上下文最优融合的双流机制。与先前工作不同，LoGoSeg无需外部掩码建议、额外骨干网络或附加数据集，兼顾高效性。在六个基准数据集（A-847、PC-459、A-150、PC-59、PAS-20及PAS-20b）上的大量实验表明，该方法在开集词汇场景中具有竞争优势与强泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing open-vocabulary semantic segmentation methods, which often suffer from imprecise spatial alignment and object hallucination due to their reliance on image-level pretrained vision-language models. The proposed LoGoSeg framework introduces an object existence prior, a region-aware alignment module, and a dual-stream fusion mechanism to integrate local structural and global semantic features efficiently in a single stage without requiring external proposals or extra data. Experiments on six benchmarks demonstrate the method&#x27;s competitive performance and strong generalization capability in segmenting both seen and unseen categories.</div>
<div class="mono" style="margin-top:8px">本研究针对现有开放词汇语义分割方法因依赖图像级预训练视觉语言模型而常出现空间对齐不精确和物体幻觉的问题。提出的LoGoSeg框架集成了三个关键创新：通过物体存在先验减少幻觉，通过区域感知对齐模块建立精确的视觉-文本对应，以及通过双流融合机制结合局部与全局特征，所有组件均集成于无需外部掩码提议或额外数据的单阶段架构中。在六个基准测试上的广泛实验验证了该方法在开放词汇设定下具有竞争力的性能和强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective</div>
<div class="meta-line">Authors: Haokui Zhang, Congyang Ou, Dawei Yan, Peng Wang, Qingsen Yan, Ying Li, Rong Xiao, Chunhua Shen</div>
<div class="meta-line">First: 2026-02-04T15:33:10+00:00 · Latest: 2026-02-05T12:00:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04657v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04657v2">PDF</a> · <a href="https://github.com/ocy1/PIO-FVLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\times$ prefill speedup, 2.11$\times$ inference speedup, 6.22$\times$ lower FLOPs, and 6.05$\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PIO-FVLM：从推理目标视角重构视觉语言模型加速的无训练视觉令牌缩减方法</div>
<div class="mono" style="margin-top:8px">近期，通过减少视觉语言模型中的冗余视觉令牌以加速推理成为研究热点。然而，现有方法多基于视觉令牌间相似性或跨模态视觉-文本相似性构建启发式规则，在压缩性能与实际部署方面存在局限。本文从推理目标出发，提出PIO-FVLM方法，将视觉令牌压缩转化为保持输出结果不变性问题，并依据该目标的重要性筛选令牌。具体而言，通过设计的层局部代理损失（一种从当前层到最终结果的粗粒度约束）生成令牌级梯度显著性，以此指导视觉令牌重排序，再遵循非极大值抑制原则选取最具价值的视觉令牌。该方法无需训练、兼容FlashAttention，便于实际应用部署：既可独立作为无编码器方法使用，也可与VisionZip等编码器压缩方法结合。在LLaVA-Next-7B上，PIO-FVLM仅保留11.1%的视觉令牌即可维持97.2%的原始性能，实现预填充速度提升2.67倍、推理速度提升2.11倍、计算量降低6.22倍、KV缓存开销减少6.05倍。代码已开源：https://github.com/ocy1/PIO-FVLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To accelerate vision-language model inference by reducing redundant visual tokens, this work identifies limitations in existing heuristic methods based on token similarity. It proposes PIO-FVLM, a training-free method that reorients token compression toward preserving output invariance, using a layer-local proxy loss to compute token-level gradient saliency for reordering and non-maximum suppression for selection. Experiments on LLaVA-Next-7B show the method retains only 11.1% of visual tokens while maintaining 97.2% of original performance, achieving significant speedups (2.67× prefill, 2.11× inference) and efficiency gains (6.22× lower FLOPs, 6.05× reduced KV Cache).</div>
<div class="mono" style="margin-top:8px">为通过减少视觉语言模型中的冗余视觉令牌来加速推理，本研究指出了现有基于令牌相似性启发式方法的局限性。提出了PIO-FVLM，这是一种无需训练的方法，将令牌压缩重新定位为保持输出不变性，通过设计的层局部代理损失计算令牌级梯度显著性进行重排序，并采用非极大值抑制进行选择。在LLaVA-Next-7B上的实验表明，该方法仅保留11.1%的视觉令牌，却能维持97.2%的原始性能，同时实现了显著的推理加速和计算开销降低。</div>
</details>
</div>
<div class="card">
<div class="title">VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator</div>
<div class="meta-line">Authors: Bessie Dominguez-Dager, Sergio Suescun-Ferrandiz, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla</div>
<div class="meta-line">First: 2026-02-05T11:23:11+00:00 · Latest: 2026-02-05T11:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05552v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05552v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLN-Pilot：大型视觉语言模型作为自主室内无人机操作员</div>
<div class="mono" style="margin-top:8px">本文介绍了VLN-Pilot，一种创新框架，其中大型视觉语言模型（VLLM）扮演室内无人机导航的人类飞行员角色。通过利用VLLM的多模态推理能力，VLN-Pilot解析自由形式的自然语言指令，并将其与视觉观察相结合，在无GPS的室内环境中规划并执行无人机轨迹。与传统的基于规则或几何路径规划方法不同，该框架将语言驱动的语义理解与视觉感知相融合，实现了上下文感知的高层飞行行为，且无需大量任务特定工程。VLN-Pilot通过推理空间关系、避障及对突发事件的动态响应，支持无人机完全自主的指令跟随。我们在定制的逼真室内仿真基准上验证了该框架，并展示了VLLM驱动智能体在复杂指令跟随任务（包括多语义目标的长程导航）中实现高成功率的能力。实验结果凸显了用语言引导自主智能体替代远程无人机飞行员的潜力，为室内无人机在巡检、搜救和设施监控等任务中实现可扩展、人性化控制开辟了新途径。我们的研究表明，基于VLLM的飞行员可显著降低操作员工作量，同时在受限室内环境中提升安全性与任务灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to replace human remote pilots for indoor drone operations by developing an autonomous agent that can understand high-level natural language commands. The proposed VLN-Pilot framework employs a large Vision-and-Language Model (VLLM) to interpret free-form instructions, ground them in visual observations from the drone, and directly plan and execute flight trajectories in GPS-denied environments, integrating semantic understanding with visual perception. Experiments on a custom photorealistic indoor simulation benchmark demonstrate that the VLLM-driven agent achieves high success rates on complex, long-horizon navigation tasks with multiple semantic targets, showing promise for reducing operator workload and improving safety in applications like inspection and search-and-rescue.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种能够理解高级自然语言指令的自主智能体，以替代室内无人机操作中的人类远程飞行员。提出的VLN-Pilot框架采用大型视觉语言模型（VLLM）来解析自由形式的指令，将其与无人机摄像头的视觉观测相结合，直接在无GPS的室内环境中规划并执行飞行轨迹，从而将语义理解与视觉感知融合以实现情境感知导航。在定制的逼真室内模拟基准上的实验表明，该VLLM驱动的智能体在涉及多个语义目标的复杂、长时程指令跟随任务中取得了较高的成功率，证明了其在减少操作员工作量、提升安全检查与任务灵活性方面的潜力，适用于巡检、搜救等应用。</div>
</details>
</div>
<div class="card">
<div class="title">RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation</div>
<div class="meta-line">Authors: Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-12-30T13:25:22+00:00 · Latest: 2026-02-05T09:33:50+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24212v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24212v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RANGER：基于上下文适应的单目零样本语义导航框架</div>
<div class="mono" style="margin-top:8px">在复杂环境中高效寻找目标是现实世界具身应用的基础。尽管多模态基础模型的最新进展已实现零样本目标导航，使机器人无需微调即可搜索任意物体，但现有方法存在两个关键局限：（1）严重依赖模拟器提供的精确深度与位姿信息，限制了实际场景的适用性；（2）缺乏上下文学习能力，难以快速适应新环境（如利用短视频）。为应对这些挑战，我们提出RANGER——一种仅使用单目相机的新型零样本开放词汇语义导航框架。该框架借助强大的3D基础模型，在消除对深度与位姿依赖的同时展现出强大的上下文学习能力。仅通过观察新环境的短视频，系统即可显著提升任务效率，无需架构修改或微调。框架集成多个关键组件：基于关键帧的3D重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应路径点选择与底层动作执行。在HM3D基准测试和真实环境中的实验表明，RANGER在导航成功率和探索效率方面达到竞争性性能，同时展现出卓越的上下文学习适应能力，且无需预先构建环境3D地图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing zero-shot object goal navigation methods, which heavily rely on precise depth and pose data from simulators and lack in-context learning (ICL) capability for quick adaptation to new environments. The authors propose RANGER, a monocular zero-shot semantic navigation framework that leverages 3D foundation models to eliminate dependency on depth and pose, and integrates keyframe-based 3D reconstruction, semantic point cloud generation, VLM-driven exploration value estimation, and adaptive waypoint selection. Experimental results on the HM3D benchmark and real-world environments show that RANGER achieves competitive navigation success and exploration efficiency, and demonstrates superior ICL adaptability by improving task efficiency after observing a short environmental video without requiring architectural changes or fine-tuning.</div>
<div class="mono" style="margin-top:8px">本研究针对现有零样本目标导航方法过度依赖模拟器提供的精确深度与位姿信息，且缺乏上下文学习能力以快速适应新环境的问题，提出了RANGER框架。该方法仅使用单目相机，利用3D基础模型消除对深度和位姿的依赖，并整合了基于关键帧的3D重建、语义点云生成、视觉语言模型驱动的探索价值估计、自适应路径点选择和底层动作执行等关键组件。在HM3D基准测试和真实环境中的实验表明，RANGER在导航成功率和探索效率方面达到了有竞争力的性能，同时通过观察新环境的短视频即可显著提升任务效率，无需修改架构或微调，展现了卓越的上下文学习适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</div>
<div class="meta-line">Authors: Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang</div>
<div class="meta-line">First: 2025-12-26T18:59:47+00:00 · Latest: 2026-02-05T08:49:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22120v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22120v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少看而精看：面向多模态推理的双向感知塑造</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）通常受益于中间视觉线索，这些线索或通过外部工具注入，或在推理过程中生成为潜在视觉标记。然而，现有机制仍会忽略细粒度视觉证据（如图表中的折线），跨领域泛化能力较差，且推理成本高昂。本文提出双向感知塑造（BiPS），该方法将问题条件化的掩码视图转化为双向的&#x27;看哪里&#x27;信号，从而在训练过程中塑造感知。BiPS首先在原始图像与仅保留问题相关区域的证据保留视图之间施加KL一致性约束，以鼓励对支持性像素进行粗略但完整的覆盖。随后，在原始图像与关键像素被掩码的证据消除视图之间施加KL分离约束，使图像不再支持原始答案，从而抑制纯文本捷径（即仅从文本作答）并强制模型依赖细粒度视觉信息。在八个基准测试中，BiPS将Qwen2.5-VL-7B模型的平均性能提升了8.2%，并在未见过的数据集和图像类型上展现出强大的跨领域泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing vision-language models in capturing fine-grained visual evidence and avoiding text-only shortcuts, this paper introduces Bi-directional Perceptual Shaping (BiPS). The method shapes model perception during training by applying a KL-consistency constraint between the original image and an evidence-preserving view to ensure complete coverage of relevant pixels, and a KL-separation constraint between the original and an evidence-ablated view to enforce reliance on fine-grained visual details. Experimental results show that BiPS improves the Qwen2.5-VL-7B model by an average of 8.2% across eight benchmarks and demonstrates strong generalization to unseen datasets and image types.</div>
<div class="mono" style="margin-top:8px">针对现有大规模视觉语言模型在捕捉细粒度视觉证据、实现领域泛化和降低推理成本方面的不足，本文提出了双向感知塑造（BiPS）方法。该方法通过在训练过程中施加原始图像与保留证据视图之间的KL一致性约束以确保对相关像素的粗略覆盖，以及施加原始图像与消除证据视图之间的KL分离约束以防止仅依赖文本的捷径并强制模型关注细粒度视觉细节。在八个基准测试上的实验结果表明，BiPS将Qwen2.5-VL-7B模型的平均性能提升了8.2%，并对未见过的数据集和图像类型展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting</div>
<div class="meta-line">Authors: Hao Feng, Wei Shi, Ke Zhang, Xiang Fei, Lei Liao, Dingkang Yang, Yongkun Du, Xuecheng Wu, Jingqun Tang, Yang Liu, Hong Chen, Can Huang</div>
<div class="meta-line">First: 2026-02-05T07:09:57+00:00 · Latest: 2026-02-05T07:09:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05384v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05384v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dolphin-v2：基于可扩展锚点提示的通用文档解析</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLM）提升OCR能力，文档解析领域受到广泛关注。然而，该领域仍被数十种各具优势的专用模型所割裂，用户需应对复杂的模型选择问题，且系统可扩展性受限。现有两阶段方法依赖轴对齐边界框进行版面检测，难以有效处理扭曲或拍摄文档。为此，我们提出Dolphin-v2——一个两阶段文档图像解析模型，在原始Dolphin基础上实现显著改进。第一阶段，Dolphin-v2同步执行文档类型分类（数字原生文档与拍摄文档）与版面分析；针对数字原生文档，通过阅读顺序预测进行细粒度元素检测。第二阶段采用混合解析策略：拍摄文档通过整体页面级解析处理几何畸变，数字原生文档则在检测到的版面锚点引导下进行元素级并行解析，实现高效内容提取。相较于原始Dolphin，Dolphin-v2引入三项关键增强：（1）通过整体页面理解实现拍摄文档的鲁棒解析，（2）支持21类细粒度元素检测及作者信息、文档元数据等语义属性提取，（3）具备现有系统通常缺失的代码块识别与缩进保留功能。在DocPTBench、OmniDocBench及自建RealDoc-160基准上的综合评估表明：在挑战性数据集OmniDocBench上整体提升14.78分，拍摄文档错误率降低91%，同时通过并行处理保持高效推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Document parsing suffers from fragmentation across specialized models and struggles with distorted photographed documents due to reliance on axis-aligned bounding boxes. To address this, Dolphin-v2 introduces a two-stage universal parsing model that first classifies document type and performs layout analysis, then applies a hybrid strategy: holistic page parsing for photographed documents and anchor-guided parallel element parsing for digital-born documents. Experiments on DocPTBench, OmniDocBench, and RealDoc-160 show a 14.78-point overall improvement on OmniDocBench and a 91% error reduction on photographed documents while maintaining efficient inference.</div>
<div class="mono" style="margin-top:8px">文档解析领域存在模型碎片化问题，且现有方法依赖轴对齐边界框，难以有效处理扭曲的拍摄文档。为此，Dolphin-v2提出一种两阶段通用解析模型：第一阶段联合进行文档类型分类与版面分析；第二阶段采用混合策略，对拍摄文档进行整体页面解析以处理几何畸变，对数字文档则基于检测到的布局锚点进行并行元素解析。在DocPTBench、OmniDocBench和自建RealDoc-160基准上的实验表明，模型在OmniDocBench上整体提升14.78分，拍摄文档错误率降低91%，同时实现了更细粒度的元素检测和保留缩进的代码块识别。</div>
</details>
</div>
<div class="card">
<div class="title">VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs</div>
<div class="meta-line">Authors: Tina Khezresmaeilzadeh, Jike Zhong, Konstantinos Psounis</div>
<div class="meta-line">First: 2026-02-05T07:07:27+00:00 · Latest: 2026-02-05T07:07:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05382v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05382v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VRIQ：视觉语言模型的视觉推理智商基准测试与分析</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）的最新进展引发了关于其能否可靠执行非语言推理的疑问。为此，我们提出了VRIQ（视觉推理智商）——一个旨在评估和分析VLMs视觉推理能力的新型基准。我们在两类任务上评估模型：抽象谜题式任务和自然图像推理任务。研究发现，在抽象谜题上，模型表现接近随机水平，平均准确率约28%；而自然图像任务表现稍好但仍较弱，准确率为45%。工具增强推理仅带来有限改进。为探究这一弱点的根源，我们引入了针对感知与推理的诊断探针。分析表明：约56%的失败仅源于感知缺陷，43%同时涉及感知与推理，仅1%纯属推理问题。这促使我们设计针对特定感知类别（如形状、数量、位置、3D/深度）的细粒度诊断探题，揭示某些类别会导致更多错误。本基准测试与分析表明，当前VLMs（即使配备视觉推理工具）仍不可靠作为抽象推理系统，主要受限于感知能力缺陷，并为改进多模态系统的视觉推理提供了理论依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To systematically evaluate whether Vision Language Models (VLMs) can perform reliable nonverbal reasoning, this study introduces the VRIQ benchmark, which assesses models on both abstract puzzle-style and natural-image reasoning tasks. The method involves evaluating standard and tool-augmented VLMs on these tasks and employing diagnostic probes to isolate failures in perception versus reasoning. Key experimental findings show that models achieve near-random accuracy (around 28%) on abstract puzzles and only 45% on natural tasks, with tool augmentation yielding modest gains; diagnostic analysis reveals that 56% of failures stem from perception alone, 43% from combined perception and reasoning issues, and only 1% from reasoning alone, highlighting perception as the primary bottleneck.</div>
<div class="mono" style="margin-top:8px">为了系统评估视觉语言模型（VLMs）是否能够进行可靠的非语言推理，本研究引入了VRIQ基准，该基准通过抽象谜题风格和自然图像推理任务来评估模型。方法包括评估标准模型和工具增强模型在这些任务上的表现，并使用诊断探针来分离感知与推理环节的失败原因。主要实验结果表明，模型在抽象谜题上的表现接近随机（准确率28%），在自然任务上仅略有改善（45%），工具增强带来的提升有限；诊断分析揭示，56%的失败源于纯感知错误，43%源于感知与推理的共同问题，仅1%源于纯推理错误，这突显了感知能力是当前模型的主要瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</div>
<div class="meta-line">Authors: Xinlei Yu, Chengming Xu, Guibin Zhang, Zhangquan Chen, Yudong Zhang, Yongbo He, Peng-Tao Jiang, Jiangning Zhang, Xiaobin Hu, Shuicheng Yan</div>
<div class="meta-line">First: 2025-11-14T06:51:34+00:00 · Latest: 2026-02-05T05:44:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11007v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11007v2">PDF</a> · <a href="https://github.com/YU-deep/VisMem.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a &quot;visual processing bottleneck&quot;: a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.0% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisMem：潜在视觉记忆解锁视觉语言模型潜力</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）取得了显著成功，但其在一系列复杂视觉任务上的表现常受制于“视觉处理瓶颈”：即在长序列生成过程中容易丧失视觉证据的锚定，并表现出情境化视觉经验的缺失。受人类认知记忆理论（区分短期视觉主导记忆与长期语义主导记忆）的启发，我们提出VisMem——一个认知对齐框架，为VLMs配备动态潜在视觉记忆系统，包含用于细粒度感知保持的短期模块和用于抽象语义整合的长期模块。这些记忆在推理过程中被无缝调用，使VLMs能在思维与生成过程中同时保持感知保真度与语义一致性。在涵盖理解、推理与生成的多样化视觉基准测试中，VisMem相较于原始模型实现平均11.0%的性能提升，并超越所有对比模型，确立了潜在空间记忆增强的新范式。代码将发布于：https://github.com/YU-deep/VisMem.git。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) often suffer from a visual processing bottleneck, losing grounding in visual evidence and lacking contextualized experience during extended generation. To address this, the authors propose VisMem, a framework inspired by human cognitive memory that equips VLMs with dynamic latent vision memories, including a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. Experiments across diverse visual understanding, reasoning, and generation benchmarks show that VisMem delivers an average performance improvement of 11.0% over the vanilla model and outperforms all existing counterparts.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉语言模型在长序列生成中存在的“视觉处理瓶颈”问题，即模型容易丢失视觉细节和上下文经验。受人类记忆认知理论启发，研究者提出了VisMem框架，该框架为模型配备了动态的潜在视觉记忆，包括用于保留细粒度感知的短期模块和用于巩固抽象语义的长期模块，在推理过程中调用这些记忆以维持感知保真度和语义一致性。在多种视觉理解、推理和生成基准上的广泛实验表明，VisMem相比原始模型平均性能提升了11.0%，并优于所有现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling</div>
<div class="meta-line">Authors: Shivanshu Shekhar, Uttaran Bhattacharya, Raghavendra Addanki, Mehrab Tanjim, Somdeb Sarkhel, Tong Zhang</div>
<div class="meta-line">First: 2026-02-05T01:54:01+00:00 · Latest: 2026-02-05T01:54:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05202v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05202v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GT-SVJ：基于生成式Transformer的自监督视频评判器——面向高效视频奖励建模</div>
<div class="mono" style="margin-top:8px">将视频生成模型与人类偏好对齐仍具挑战：现有方法依赖视觉语言模型进行奖励建模，但这些模型难以捕捉细微的时间动态。我们提出一种根本不同的方法：将本身为建模时间结构而设计的视频生成模型改造为奖励模型。我们提出基于生成式Transformer的自监督视频评判器，这是一种将前沿视频生成模型转化为强大时间感知奖励模型的新型评估模型。核心洞见在于：生成模型可重构为基于能量的模型——高质量视频被赋予低能量值，劣质视频被赋予高能量值，通过对比目标训练后能以显著精度判别视频质量。为防止模型利用真实视频与生成视频间的表面差异，我们通过受控潜空间扰动设计具有挑战性的合成负样本视频：时间切片、特征交换和帧重排，以模拟真实而细微的视觉退化。这迫使模型学习有意义的时空特征而非琐碎伪影。该模型仅用3万条人工标注即在GenAI-Bench和MonteBench上达到最优性能，标注量仅为现有基于视觉语言模型方法的1/6至1/65。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of aligning video generative models with human preferences, noting that current Vision-Language Model (VLM)-based reward models fail to capture subtle temporal dynamics. The method proposes repurposing video generative models as reward models by reformulating them as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, trained via contrastive objectives with synthetic negative videos created through latent-space perturbations like temporal slicing and frame shuffling to force learning of meaningful spatiotemporal features. Key experimental results show that the model achieves state-of-the-art performance on GenAI-Bench and MonteBench benchmarks using only 30K human annotations, which is 6 to 65 times fewer than existing VLM-based approaches.</div>
<div class="mono" style="margin-top:8px">该研究针对视频生成模型与人类偏好对齐的挑战，指出当前基于视觉语言模型（VLM）的奖励模型难以捕捉细微的时间动态。方法提出了GT-SVJ，通过将视频生成模型重新构建为基于能量的模型，并采用对比目标进行训练，将其转化为具有时间感知能力的奖励模型；为确保鲁棒学习，该方法通过潜在空间扰动（如时间切片和帧重排）生成合成负样本视频，以模拟真实的视觉退化。主要实验结果表明，GT-SVJ在GenAI-Bench和MonteBench基准测试中取得了最先进的性能，同时仅需3万个人工标注，比现有VLM方法减少了6至65倍。</div>
</details>
</div>
<div class="card">
<div class="title">ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation</div>
<div class="meta-line">Authors: Jia Li, Wenjie Zhao, Shijian Deng, Bolin Lai, Yuheng Wu, RUijia Chen, Jon E. Froehlich, Yuhang Zhao, Yapeng Tian</div>
<div class="meta-line">First: 2026-02-04T23:33:16+00:00 · Latest: 2026-02-04T23:33:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05132v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARGaze：用于在线第一人称视线估计的自回归变换器</div>
<div class="mono" style="margin-top:8px">在线第一人称视线估计仅利用过去和当前帧，从第一人称视频中预测佩戴相机者的注视位置，这一任务对增强现实和辅助技术至关重要。与第三人称视线估计不同，该场景缺乏明确的头部或眼部信号，要求模型从稀疏的间接线索（如手物交互和显著场景内容）推断当前视觉注意力。我们观察到，在目标导向活动中，视线表现出强时间连续性：了解个体近期注视位置能为预测其下一步注视提供有力先验。受视觉语言模型中视觉条件自回归解码的启发，我们提出ARGaze，将视线估计重构为序列预测：在每个时间步，变换器解码器通过条件化（i）当前视觉特征和（ii）固定长度的近期注视目标估计值组成的“视线上下文窗口”，来预测当前视线。该设计强制因果性并支持有限资源流式推理。我们在在线评估的多个第一人称基准测试中实现了最先进性能，大量消融实验验证了有限视线历史的自回归建模对鲁棒预测的关键作用。我们将公开源代码和预训练模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Online egocentric gaze estimation is crucial for augmented reality and assistive technologies but lacks explicit head or eye signals, requiring inference from indirect cues like hand-object interactions. To address this, the authors propose ARGaze, an autoregressive transformer model that reformulates gaze estimation as a sequential prediction task, where a decoder predicts current gaze by conditioning on current visual features and a fixed-length window of recent gaze estimates, ensuring causality and efficient streaming inference. Experiments show that ARGaze achieves state-of-the-art performance on multiple egocentric benchmarks under online evaluation, with ablations confirming the importance of autoregressive modeling with bounded gaze history for robust prediction.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在线第一人称视线估计的挑战，该任务仅使用过去和当前帧从第一人称视频预测佩戴者视线方向，对增强现实和辅助技术至关重要，但缺乏明确的头部或眼部信号使其复杂化。方法ARGaze将视线估计重新定义为序列预测问题，采用自回归变换器解码器；在每个时间步，它基于当前视觉特征和固定长度的近期视线目标估计的上下文窗口进行预测，确保了因果性并支持高效的流式推理。实验结果表明，该方法在多个第一人称基准测试的在线评估中取得了最先进的性能，消融实验验证了带有限视线历史的自回归建模对于鲁棒预测的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</div>
<div class="meta-line">Authors: Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Mahlagha Fazeli, Abolfazl Razi</div>
<div class="meta-line">First: 2025-10-30T16:08:25+00:00 · Latest: 2026-02-04T21:42:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26641v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26641v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶车辆目标检测全要素：从像素、点云与提示到新一代融合技术与多模态大语言模型/视觉语言模型</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆正通过智能感知、决策与控制系统的进步重塑未来交通格局。然而其成功核心在于复杂多模态环境中可靠的目标检测能力。尽管计算机视觉与人工智能领域近期取得突破性进展，该领域仍面临关键挑战：多模态感知、情境推理与协同智能的知识体系仍呈碎片化。本综述通过前瞻性分析自动驾驶目标检测技术弥合这一鸿沟，重点探讨视觉语言模型、大语言模型与生成式人工智能等新兴范式，而非重新审视过时技术。我们首先系统回顾自动驾驶基础传感器谱系（摄像头、超声波、激光雷达、毫米波雷达）及其融合策略，不仅阐明其在动态驾驶环境中的能力与局限，更揭示其与LLM/VLM驱动感知框架最新进展的整合潜力。继而提出超越简单数据集的自动驾驶数据集结构化分类体系，涵盖自车视角、基础设施视角及协同数据集（如车车通信、车路通信、车万物通信、设施间通信），并对数据结构与特征进行交叉分析。最后，我们剖析从二维/三维检测流程到混合传感器融合的前沿检测方法，特别关注由视觉Transformer、大小语言模型及视觉语言模型驱动的Transformer新兴技术路径。通过整合多维视角，本综述清晰勾勒出现有能力图谱、开放挑战与未来机遇。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey addresses the fragmented knowledge in autonomous vehicle object detection by providing a forward-looking analysis that emphasizes emerging paradigms like Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI, rather than revisiting outdated techniques. The method involves a systematic review of fundamental AV sensors and their fusion strategies, a structured categorization of datasets including ego-vehicle and cooperative types, and an analysis of cutting-edge detection methodologies from 2D/3D pipelines to transformer-driven approaches. Key findings highlight the potential integration of sensor data with LLM/VLM frameworks and outline a roadmap of current capabilities, open challenges, and future opportunities in the field.</div>
<div class="mono" style="margin-top:8px">本综述针对自动驾驶中物体检测知识碎片化的问题，通过前瞻性分析，重点探讨了视觉语言模型、大语言模型和生成式AI等新兴范式，而非重新审视过时技术。研究方法包括系统回顾自动驾驶传感器及其融合策略，对自车、基础设施和协同数据集进行结构化分类，并分析从2D/3D检测流程到基于Transformer的先进方法。主要发现强调了传感器数据与LLM/VLM框架融合的潜力，并勾勒出该领域当前能力、开放挑战和未来机遇的清晰路线图。</div>
</details>
</div>
<div class="card">
<div class="title">VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models</div>
<div class="meta-line">Authors: Yiye Chen, Yanan Jian, Xiaoyi Dong, Shuxin Cao, Jing Wu, Patricio Vela, Benjamin E. Lundell, Dongdong Chen</div>
<div class="meta-line">First: 2026-02-04T20:59:29+00:00 · Latest: 2026-02-04T20:59:29+00:00</div>
<div class="meta-line">Comments: In submission. Project website: https://vista-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05049v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05049v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vista-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VISTA：通过轨迹跟随偏好优化增强视觉-语言-动作模型中的视觉条件化能力</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型已在多种机器人操作任务中展现出卓越性能。然而，将大型预训练视觉-语言模型（VLM）扩展至动作空间时，可能引发视觉-动作失准问题，即动作预测对当前视觉状态的依赖较弱，导致输出不可靠。本研究从视觉条件化角度分析VLA模型，并通过实验证明：成功的任务执行轨迹始终比失败的轨迹表现出更强的视觉依赖性。基于此发现，我们提出一种显式增强VLA模型视觉条件化的训练框架。该方法首先通过轨迹跟随代理任务的偏好优化实现动作预测与视觉输入的对齐，随后在监督微调阶段通过潜在空间蒸馏将增强的对齐能力迁移至指令跟随任务。在不改变模型架构或增加数据收集的情况下，本方法在离散动作的OpenVLA中同时提升了视觉条件化能力与任务性能，并在扩展至连续动作的OpenVLA-OFT设定时获得持续增益。项目网站：https://vista-vla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the vision-action misalignment observed when extending pretrained Vision-Language Models (VLMs) to action spaces, which leads to unreliable robotic manipulation outputs. The proposed method, VISTA, enhances visual conditioning by first applying preference optimization on a track-following surrogate task to align actions with visual input, followed by latent-space distillation during supervised finetuning to transfer this alignment to instruction-following tasks. Experimental results show that this training framework, without architectural changes or new data, improves visual conditioning and task performance for discrete OpenVLA models and yields consistent gains in the continuous OpenVLA-OFT setting.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉-语言-动作（VLA）模型中存在的视觉-动作错位问题，即动作预测对当前视觉状态的依赖较弱，导致输出不可靠。提出的VISTA框架通过先在轨迹跟踪代理任务上进行偏好优化以对齐动作与视觉输入，然后在监督微调期间通过潜在空间蒸馏将增强的对齐能力迁移到指令跟随任务中，从而强化视觉条件依赖。实验结果表明，该方法在不改变架构或收集额外数据的情况下，提升了离散OpenVLA模型的视觉条件依赖和任务性能，并在扩展到连续OpenVLA-OFT设置时也取得了稳定的增益。</div>
</details>
</div>
<div class="card">
<div class="title">When LLaVA Meets Objects: Token Composition for Vision-Language-Models</div>
<div class="meta-line">Authors: Soumya Jahagirdar, Walid Bousselham, Anna Kukleva, Hilde Kuehne</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当LLaVA遇见物体：视觉语言模型的令牌组合方法</div>
<div class="mono" style="margin-top:8px">当前自回归视觉语言模型通常依赖大量视觉令牌表示图像，导致推理时计算需求较高。为解决此问题，我们提出Mask-LLaVA框架，该框架利用多层级视觉特征为自回归视觉语言模型构建紧凑而信息丰富的视觉表示。具体而言，我们将基于掩码的物体表征与全局令牌及局部补丁令牌相结合。训练阶段使用全部令牌，而实验表明所得模型在测试时可灵活减少基于掩码的物体令牌数量，实现无需重新训练即可动态调整推理时的令牌数量，且性能无明显下降。我们在标准基准测试集上评估该方法，结果显示其性能与当前令牌高效方法相当，仅使用少量视觉令牌即可达到原始LLaVA基线水平。分析表明，多层级特征组合能以较少令牌实现高效学习，并在测试时通过动态令牌选择保持良好性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To reduce the high computational cost of autoregressive vision-language models (VLMs) caused by their reliance on numerous visual tokens, this work introduces Mask-LLaVA, a framework that constructs a compact visual representation by combining multi-level features: mask-based object tokens, global tokens, and local patch tokens. During training, all token types are utilized, but the model learns to flexibly drop a significant portion of the mask-based object tokens at inference time, enabling dynamic token count adaptation without retraining. Experimental evaluation on standard benchmarks shows that Mask-LLaVA achieves performance competitive with other token-efficient methods and comparable to the original LLaVA baseline, while using only a fraction of the visual tokens, demonstrating effective efficiency and adaptability.</div>
<div class="mono" style="margin-top:8px">为降低自回归视觉语言模型通常需要大量视觉令牌的计算成本，本研究提出了Mask-LLaVA框架，该框架通过结合掩码对象令牌、全局令牌和局部补丁令牌等多层次特征，构建紧凑的视觉表示。训练时使用所有令牌类型，但模型在推理时可以灵活减少掩码对象令牌的数量，实现无需重新训练的动态令牌选择。在标准基准测试上的实验结果表明，该方法性能与其他令牌高效方法相当，且在使用少量视觉令牌的情况下与原始LLaVA基线效果可比，证明了其高效学习和适应性推理的能力。</div>
</details>
</div>
<div class="card">
<div class="title">VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?</div>
<div class="meta-line">Authors: Qing&#x27;an Liu, Juntong Feng, Yuhao Wang, Xinzhe Han, Yujie Cheng, Yue Zhu, Haiwen Diao, Yunzhi Zhuge, Huchuan Lu</div>
<div class="meta-line">First: 2026-02-04T17:48:55+00:00 · Latest: 2026-02-04T17:48:55+00:00</div>
<div class="meta-line">Comments: 27 pages, 19 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04802v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04802v1">PDF</a> · <a href="https://github.com/QingAnLiu/VISTA-Bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VISTA-Bench：视觉语言模型是否真正理解可视化文本与纯文本？</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在文本与视觉输入的跨模态理解方面取得了显著进展，但现有基准主要关注纯文本查询。在实际场景中，语言常以图像中嵌入的可视化文本形式出现，这引发了对当前VLMs处理此类输入能力的质疑。我们提出VISTA-Bench——一个涵盖多模态感知、推理到单模态理解领域的系统性基准。该基准通过在受控渲染条件下对比纯文本与可视化文本问题，评估模型对可视化文本的理解能力。对20余个代表性VLM的广泛评估揭示了显著的模态差距：在纯文本查询中表现优异的模型，当相同语义内容以可视化文本呈现时性能大幅下降。这种差距随感知难度增加而扩大，表明模型对渲染变化敏感，尽管语义保持不变。总体而言，VISTA-Bench提供了诊断该局限性的原则性评估框架，并为实现符号化文本与像素间更统一的语言表征指引方向。源数据集发布于https://github.com/QingAnLiu/VISTA-Bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the gap in evaluating Vision-Language Models (VLMs) on their ability to understand visualized text embedded in images, a common real-world scenario, as existing benchmarks focus primarily on pure-text queries. The authors introduce VISTA-Bench, a systematic benchmark that contrasts pure-text and visualized-text questions across multimodal perception, reasoning, and unimodal understanding domains under controlled rendering conditions. Experiments on over 20 VLMs reveal a significant modality gap, where models perform substantially worse on visualized text compared to semantically equivalent pure text, with the performance degradation increasing with perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics.</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型在纯文本查询上表现出色，但对于图像中嵌入的可视化文本（一种常见的现实场景）的处理能力尚不明确。为此，研究者提出了VISTA-Bench这一系统性基准，通过在受控的渲染条件下直接比较模型对纯文本与可视化文本问题的响应，来评估模型在多模态感知、推理和单模态理解方面的能力。对超过20个代表性模型的广泛测试揭示了一个显著的模态差距：模型在可视化文本上的表现明显更差，且随着感知难度的增加，性能下降加剧，这表明尽管语义内容相同，模型对渲染变化仍非常敏感。</div>
</details>
</div>
<div class="card">
<div class="title">Annotation Free Spacecraft Detection and Segmentation using Vision Language Models</div>
<div class="meta-line">Authors: Samet Hicsonmez, Jose Sosa, Dan Pineau, Inder Pal Singh, Arunkumar Rathinam, Abd El Rahman Shabayek, Djamila Aouada</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-04T16:07:29+00:00 · Latest: 2026-02-04T16:07:29+00:00</div>
<div class="meta-line">Comments: ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04699v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04699v1">PDF</a> · <a href="https://github.com/giddyyupp/annotation-free-spacecraft-segmentation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的免标注航天器检测与分割方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型在开放世界零样本视觉识别任务中已展现出卓越性能，但其在航天领域的应用潜力尚未得到充分探索。在航天场景中，受低可见度、光照变化及目标与行星背景融合等因素影响，精确的人工标注尤为困难。因此，开发无需大量人工标注即可检测与分割航天器及轨道目标的方法至关重要。本研究提出一种基于视觉语言模型的航天目标免标注检测与分割流程：首先利用预训练视觉语言模型为少量未标注真实数据自动生成伪标签，随后通过师生标签蒸馏框架训练轻量化模型。尽管伪标签存在固有噪声，但蒸馏过程相比直接零样本视觉语言模型推理仍带来显著性能提升。在SPARK-2024、SPEED+和TANGO数据集上的分割实验表明，平均精度最高可提升10个百分点。代码与模型已开源：https://github.com/giddyyupp/annotation-free-spacecraft-segmentation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting and segmenting spacecraft in space imagery where manual annotation is difficult due to low visibility and background blending. The method employs a pre-trained Vision Language Model (VLM) to automatically generate pseudo-labels for a small unlabeled dataset, which are then used within a teacher-student distillation framework to train lightweight models. Experimental results on SPARK-2024, SPEED+, and TANGO datasets show that this approach yields consistent performance improvements, increasing average precision by up to 10 points over direct zero-shot VLM inference.</div>
<div class="mono" style="margin-top:8px">本研究针对太空图像中航天器检测与分割的标注难题，该难题源于低可见度、光照变化及背景融合等因素。方法提出了一种无需标注的流程：首先使用预训练的视觉语言模型为少量未标注数据生成伪标签，然后通过师生蒸馏框架训练轻量级模型。在SPARK-2024、SPEED+和TANGO数据集上的实验结果表明，该方法显著优于直接的零样本视觉语言模型推理，在分割任务的平均精度上实现了最高10个百分点的稳定提升。</div>
</details>
</div>
<div class="card">
<div class="title">AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation</div>
<div class="meta-line">Authors: Jin-Chuan Shi, Binhong Ye, Tao Liu, Junzhe He, Yangjinhui Xu, Xiaoyang Liu, Zeju Li, Hao Chen, Chunhua Shen</div>
<div class="meta-line">First: 2026-02-04T15:42:58+00:00 · Latest: 2026-02-04T15:42:58+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04672v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AGILE：基于智能体生成从视频重建手-物交互</div>
<div class="mono" style="margin-top:8px">从单目视频重建动态手-物交互对于灵巧操作数据采集以及为机器人与虚拟现实创建逼真数字孪生至关重要。然而，现有方法面临两大瓶颈：(1) 依赖神经渲染常导致严重遮挡下几何结构破碎且无法直接用于仿真；(2) 依赖脆弱的运动恢复结构初始化易导致野外视频频繁失败。为突破这些局限，我们提出AGILE框架，将交互学习的范式从重建转向智能体生成。首先，采用智能体流程：通过视觉语言模型引导生成模型合成完整、密闭的高保真纹理物体网格，规避视频遮挡影响。其次，完全绕过脆弱的SfM，提出鲁棒的锚点跟踪策略：基于基础模型在单帧交互起始帧初始化物体位姿，并利用生成资产与视频观测间的强视觉相似性进行时序传播。最后，通过融合语义、几何与交互稳定性的接触感知优化确保物理合理性。在HO3D、DexYCB及野外视频上的大量实验表明，AGILE在全局几何精度上超越基线方法，且在现有方法常失效的挑战性序列中展现卓越鲁棒性。通过优先保障物理有效性，本方法产出的仿真就绪资产已通过机器人应用的真实-仿真重定向验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of current methods for reconstructing dynamic hand-object interactions from monocular videos, which often produce fragmented geometries and fail on in-the-wild footage due to reliance on neural rendering and brittle SfM initialization. The proposed AGILE framework shifts the paradigm to agentic generation, employing a Vision-Language Model to guide the synthesis of a complete object mesh and a robust anchor-and-track strategy for pose estimation, followed by a contact-aware optimization for physical plausibility. Experimental results on HO3D, DexYCB, and in-the-wild videos show that AGILE outperforms baselines in geometric accuracy and robustness, producing simulation-ready assets validated for robotic retargeting.</div>
<div class="mono" style="margin-top:8px">本研究针对现有从单目视频重建动态手物交互方法存在的局限性，这些方法常因依赖神经渲染和脆弱的运动恢复结构初始化，产生破碎的几何体并在真实场景视频中失败。提出的AGILE框架将范式转向智能体生成，利用视觉语言模型引导合成完整的物体网格，并通过稳健的锚定跟踪策略进行姿态估计，最后进行接触感知优化以确保物理合理性。在HO3D、DexYCB和真实场景视频上的实验表明，AGILE在几何精度上优于基线方法，并在挑战性序列上展现出卓越的鲁棒性，生成了经过机器人应用验证的、可用于仿真的资产。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Degradation with Vision Language Model</div>
<div class="meta-line">Authors: Guanzhou Lan, Chenyi Liao, Yuqi Yang, Qianli Ma, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li</div>
<div class="meta-line">First: 2026-02-04T13:51:15+00:00 · Latest: 2026-02-04T13:51:15+00:00</div>
<div class="meta-line">Comments: 17 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04565v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04565v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的退化理解研究</div>
<div class="mono" style="margin-top:8px">理解视觉退化是计算机视觉领域关键而具挑战性的问题。当前视觉语言模型虽擅长定性描述，却难以理解图像退化背后的参数化物理机制。本研究将退化理解重新定义为层次化结构化预测任务，需同时估计退化类型、参数键及其连续物理值。尽管子任务处于不同空间，我们证明其可通过自回归下一词元预测范式统一实现，其误差受值空间量化网格约束。基于此，我们提出DU-VLM——采用监督微调与结构化奖励强化学习训练的多模态思维链模型。进一步研究表明，DU-VLM可作为预训练扩散模型的零样本控制器，无需微调生成主干即可实现高保真图像复原。同时发布包含11万组洁净-退化配对数据与物理标注的大规模数据集\textbf{DU-110k}。大量实验表明，该方法在精度与鲁棒性上显著超越通用基线模型，并展现出对未见分布的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling vision-language models to understand the parametric physics of image degradations, moving beyond qualitative descriptions. The method introduces DU-VLM, a multimodal chain-of-thought model that unifies degradation type, parameter key, and continuous value estimation under an autoregressive next-token prediction framework, trained with supervised fine-tuning and reinforcement learning using structured rewards. Experimental results show that DU-VLM significantly outperforms generalist baselines in accuracy and robustness, generalizes to unseen distributions, and can serve as a zero-shot controller for pre-trained diffusion models to achieve high-fidelity image restoration without fine-tuning the generative backbone.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决让视觉语言模型理解图像退化的参数化物理原理这一挑战，超越定性描述。作者提出了一种分层结构化预测任务，将退化类型、参数键和连续物理值的估计统一在单一的自回归下一令牌预测框架下，并具有有界误差。他们引入了DU-VLM模型，该模型通过监督微调和带有结构化奖励的强化学习进行训练，实验表明其在准确性和鲁棒性上显著优于基线模型，能够作为预训练扩散模型的零样本控制器进行高保真图像复原，并在新的大规模数据集DU-110k上进行了评估。</div>
</details>
</div>
<div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：基于视觉语言模型将人形机器人任务规划空间感知地融入以自我为中心的动作中</div>
<div class="mono" style="margin-top:8px">在现实环境中部署人形机器人面临根本性挑战，因其需要在部分信息观测和动态变化环境下，紧密整合感知、移动与操作能力，并稳健实现不同类型子任务间的过渡。为应对这些挑战，我们提出一项新任务——EgoActing，要求将高层指令直接映射为多样化、精确且具有空间感知的人形机器人动作。我们进一步通过引入EgoActor实例化该任务：这是一个统一且可扩展的视觉语言模型（VLM），能够预测移动基元（如行走、转向、侧移、高度调整）、头部运动、操作指令及人机交互行为，以实时协调感知与执行。我们利用来自真实世界演示的纯RGB自我中心数据、空间推理问答及模拟环境演示进行广泛监督，使EgoActor（包括80亿和40亿参数版本）能在1秒内做出稳健的上下文感知决策并执行流畅的动作推断。在模拟和真实环境中的大量实验表明，EgoActor有效衔接了抽象任务规划与具体运动执行，并能泛化至多样化任务及未见环境。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of deploying humanoid robots in dynamic real-world settings, which requires integrating perception, locomotion, and manipulation under partial observations. The authors propose the EgoActing task and introduce EgoActor, a unified vision-language model that grounds high-level instructions into spatially-aware actions like locomotion primitives, head movements, and manipulation commands. The model is trained with broad supervision from real-world egocentric RGB data, spatial reasoning QA, and simulated demonstrations, enabling robust, context-aware decision-making and fluent action inference in under one second. Experimental evaluations in simulated and real-world environments show that EgoActor effectively bridges abstract task planning with concrete motor execution and generalizes to diverse, unseen tasks.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决人形机器人在动态、部分可观测环境中部署的挑战，这需要整合感知、移动和操作能力，并在子任务间灵活切换。所提出的方法EgoActor是一个统一的视觉-语言模型，能够将高层指令转化为空间感知的动作，预测移动基元、头部运动、操作命令和人机交互。实验结果表明，通过利用真实世界演示的自我中心RGB数据、空间推理问答和模拟环境演示进行训练，EgoActor实现了鲁棒的上下文感知决策和流畅的动作推断（在1秒内），在模拟和真实环境中均能有效连接抽象任务规划与具体运动执行。</div>
</details>
</div>
<div class="card">
<div class="title">OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</div>
<div class="meta-line">Authors: Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng</div>
<div class="meta-line">First: 2026-01-29T12:43:02+00:00 · Latest: 2026-02-04T12:53:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21639v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21639v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OCRVerse：迈向端到端视觉语言模型中的全息OCR</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型的发展推动了对海量多模态数据管理及应用的需求，使得从视觉图像中提取信息的OCR技术日益普及。然而，现有OCR方法主要聚焦于从图像或扫描文档中识别文本元素（文本中心OCR），而忽视了从视觉信息密集的图像源（视觉中心OCR）中识别视觉元素，如图表、网页和科学绘图。现实中，这类视觉信息密集的图像在互联网中广泛存在，具有重要的实际应用价值，如数据可视化和网页分析。本技术报告提出OCRVerse，首个以端到端方式实现文本中心OCR与视觉中心OCR统一的全息OCR方法。为此，我们构建了全面的数据工程，涵盖报纸、杂志、书籍等广泛文本中心文档，以及图表、网页、科学绘图等视觉中心渲染复合体。此外，我们为OCRVerse提出了一种两阶段SFT-RL多领域训练方法：SFT直接混合跨领域数据进行训练以建立初始领域知识，而RL则针对各领域特性设计个性化奖励策略。具体而言，由于不同领域需要多样化的输出格式与预期结果，我们在RL阶段提供充分灵活性，为每个领域定制灵活的奖励信号，从而提升跨领域融合能力并避免数据冲突。实验结果表明，OCRVerse在文本中心与视觉中心数据类型上均取得优异效果，其性能甚至可与大规模开源及闭源模型相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for OCR technology that goes beyond traditional text recognition to also interpret visually information-dense images like charts and web pages, which are prevalent online and valuable for applications such as data visualization. The method introduces OCRVerse, an end-to-end holistic OCR model, which employs comprehensive data engineering to cover both text-centric documents and vision-centric rendered composites, and a two-stage SFT-RL multi-domain training approach where supervised fine-tuning establishes initial knowledge and reinforcement learning uses personalized reward strategies tailored to each domain&#x27;s output format requirements to enhance cross-domain fusion. Key experimental findings show that OCRVerse achieves competitive performance across both text-centric and vision-centric data types, performing comparably to large-scale open-source and closed-source models.</div>
<div class="mono" style="margin-top:8px">本研究针对现有OCR方法主要关注文本识别而忽视图表、网页等视觉信息密集图像的视觉中心OCR任务这一局限。作者提出了OCRVerse，一种端到端的整体OCR方法，该方法通过涵盖文本和视觉领域的全面数据工程，以及一个两阶段的SFT-RL多领域训练策略：监督微调建立初始领域知识，而强化学习则针对各领域特点设计个性化奖励策略以处理不同的输出格式。实验结果表明，OCRVerse在文本中心和视觉中心数据类型上均取得了有竞争力的性能，可与大规模开源和闭源模型相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">Less Precise Can Be More Reliable: A Systematic Evaluation of Quantization&#x27;s Impact on CLIP Beyond Accuracy</div>
<div class="meta-line">Authors: Aymen Bouguerra, Daniel Montoya, Alexandra Gomez-Villa, Chokri Mraidha, Fabio Arnez</div>
<div class="meta-line">First: 2025-09-25T13:54:34+00:00 · Latest: 2026-02-04T09:44:34+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21173v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.21173v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) such as CLIP have revolutionized zero-shot classification and safety-critical tasks, including Out-of-Distribution (OOD) detection. However, their high computational cost hinders efficient real-world deployment. While quantization is a standard solution for efficiency, its broader impact on reliability metrics beyond simple Top-1 accuracy remains critically under-explored. In this study, we conduct a large-scale evaluation of VLM quantization across a comprehensive experimental suite of over 700k evaluation runs with varying configurations. We find that, contrary to the assumption that quantization&#x27;s noise degrades performance, it can simultaneously improve accuracy, calibration, OOD detection, and robustness to noise, though not to covariate shift or spurious correlations. We leverage these counterintuitive findings to characterize the mechanics of quantization beyond simple regularization: we show that quantization dampens high-rank spectral components, compelling the model to rely more heavily on robust, low-rank features. Ultimately, this spectral filtering effect drives the observed improvements in generalization and noise tolerance, establishing a pathway to deploy faster, more reliable VLMs by utilizing quantization beyond its conventional role.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降低精度反而更可靠：量化对CLIP模型影响的系统性评估——超越准确率的视角</div>
<div class="mono" style="margin-top:8px">以CLIP为代表的视觉语言模型（VLM）彻底改变了零样本分类和安全关键任务（包括分布外检测）的实现方式。然而，其高昂的计算成本阻碍了实际场景的高效部署。量化虽是提升效率的常规方案，但其对可靠性指标（超越简单的Top-1准确率）的广泛影响仍缺乏深入研究。本研究通过超过70万次不同配置的实验，对VLM量化进行了大规模评估。我们发现：与量化噪声会降低性能的假设相反，量化能同时提升准确率、校准能力、分布外检测及噪声鲁棒性（但对协变量偏移和伪相关性无效）。基于这些反直觉的发现，我们揭示了超越简单正则化的量化机制：量化抑制了高秩谱分量，迫使模型更依赖鲁棒的低秩特征。这种谱滤波效应最终驱动了泛化能力和噪声容忍度的提升，为通过突破量化的传统角色来部署更快、更可靠的VLM开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the under-explored impact of quantization on the reliability of Vision-Language Models like CLIP, beyond just accuracy, as their computational cost limits deployment. Through a large-scale evaluation involving over 700,000 runs, the method systematically assesses quantization&#x27;s effects on various metrics. Key findings reveal that quantization can paradoxically enhance accuracy, calibration, out-of-distribution detection, and noise robustness by acting as a spectral filter that dampens high-rank features, thereby promoting reliance on more robust low-rank components, though it does not improve performance against covariate shift or spurious correlations.</div>
<div class="mono" style="margin-top:8px">本研究旨在降低CLIP等视觉语言模型的实际部署计算成本，同时认识到量化对简单准确率之外可靠性指标的影响尚不明确。研究方法是对VLM量化进行大规模系统评估，通过超过70万次实验运行来分析其对各项性能的影响。主要实验结果表明，量化可以反常地提升准确率、校准性、分布外检测和噪声鲁棒性，这归因于其频谱滤波效应抑制了高秩特征，迫使模型依赖更鲁棒的低秩成分。</div>
</details>
</div>
<div class="card">
<div class="title">When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models</div>
<div class="meta-line">Authors: Jaehyun Kwak, Nam Cao, Boryeong Cho, Segyu Lee, Sumyeong Ahn, Se-Young Yun</div>
<div class="meta-line">First: 2026-02-04T09:29:10+00:00 · Latest: 2026-02-04T09:29:10+00:00</div>
<div class="meta-line">Comments: Pre-print</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04356v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04356v1">PDF</a> · <a href="https://github.com/jackwaky/SAGA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时何地发起攻击？面向大型视觉语言模型的阶段式注意力引导对抗攻击</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型（LVLMs）的对抗攻击对于揭示现代多模态系统的安全漏洞至关重要。基于输入变换（如随机裁剪）的近期研究表明，空间局部扰动可能比全局图像操作更有效。然而，对整个图像进行随机裁剪本质上具有随机性，且无法高效利用有限的像素级扰动预算。我们提出两个关键发现：（一）区域注意力分数与对抗损失敏感度呈正相关；（二）攻击高注意力区域会引发注意力向后续显著区域的结构性重分配。基于这些发现，我们提出阶段式注意力引导攻击（SAGA），该框架通过注意力引导逐步将扰动集中于高注意力区域。SAGA能更高效地利用受限扰动预算，在十种LVLM上持续实现最先进的攻击成功率，同时生成高度不易察觉的对抗样本。源代码发布于https://github.com/jackwaky/SAGA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to efficiently expose safety vulnerabilities in Large Vision-Language Models (LVLMs) through adversarial attacks. The proposed method, Stage-wise Attention-Guided Attack (SAGA), leverages the observation that regional attention scores correlate with adversarial loss sensitivity and that attacking high-attention regions redistributes attention to other salient areas. SAGA progressively concentrates perturbations on these high-attention regions to maximize the use of a constrained per-pixel budget. Experimental results demonstrate that SAGA generates highly imperceptible adversarial examples and consistently achieves state-of-the-art attack success rates across ten different LVLMs.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发针对大型视觉语言模型（LVLMs）的更高效对抗攻击，以揭示其安全漏洞。所提出的方法——阶段式注意力引导攻击（SAGA），基于高注意力区域对对抗损失更敏感且攻击这些区域会重定向模型注意力的观察，逐步将对抗扰动集中在图像的高注意力区域。实验结果表明，与随机裁剪等先前方法相比，SAGA能更高效地利用有限的扰动预算，在十个LVLM上实现了最先进的攻击成功率，同时保持了高度的不可感知性。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning</div>
<div class="meta-line">Authors: Qian-Wei Wang, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2026-02-04T09:01:55+00:00 · Latest: 2026-02-04T09:01:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04340v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双提示调优的主动CLIP适配显式不确定性建模</div>
<div class="mono" style="margin-top:8px">CLIP等预训练视觉语言模型虽具备较强的迁移能力，但在有限标注预算下将其适配至下游图像分类任务仍具挑战性。在主动学习场景中，模型需从未标注数据池中选择最具信息量的样本进行标注。现有方法通常通过基于熵的准则或表征聚类来估计不确定性，而未从模型视角显式建模不确定性。本研究提出一种基于双提示调优的鲁棒不确定性建模框架，用于主动CLIP适配。我们在CLIP的文本分支中引入两个可学习提示：正提示通过增强与轻量调优视觉嵌入对应的任务特定文本嵌入的判别性，提升分类可靠性；负提示则通过反向训练方式显式建模预测标签正确的概率，为主动样本选择提供理论依据的不确定性信号。跨不同微调范式的实验表明，在相同标注预算下，本方法持续优于现有主动学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of adapting CLIP to downstream image classification tasks with limited labeled data, this work proposes an active learning framework that explicitly models prediction uncertainty via dual-prompt tuning. The method introduces a positive prompt to enhance discriminability of task-specific textual embeddings and a negative prompt, trained in a reversed manner, to explicitly estimate the probability that a predicted label is correct, thereby providing a principled uncertainty signal for sample selection. Experimental results across various fine-tuning paradigms show that this approach consistently outperforms existing active learning methods under the same annotation budget.</div>
<div class="mono" style="margin-top:8px">本研究针对在主动学习场景下，如何以有限标注预算将预训练的CLIP模型适配到下游图像分类任务中的挑战。该方法提出了一个双提示调优框架，在CLIP的文本分支中引入两个可学习的提示：正提示通过增强任务特定文本嵌入的判别性来提高分类可靠性，而负提示则以反向方式训练，显式建模预测正确的概率，从而为主动样本选择提供原则性的不确定性信号。在不同微调范式下的广泛实验表明，该方法在相同标注预算下持续优于现有的主动学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner</div>
<div class="meta-line">Authors: Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2026-02-04T09:00:12+00:00 · Latest: 2026-02-04T09:00:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04337v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04337v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需人工标注的预训练视觉语言模型微调方法</div>
<div class="mono" style="margin-top:8px">CLIP等大规模视觉语言模型虽具备较强的零样本泛化能力，但适应下游任务通常需要昂贵的标注数据。现有无监督自训练方法依赖伪标注，但常面临置信度过滤不可靠、确认偏误及低置信度样本利用不足等问题。我们提出协同微调框架，通过双模型跨模态协作机制利用未标注数据。该框架采用包含正负文本提示的双提示学习策略，以样本依赖方式显式建模伪标注纯净度，无需人工设定阈值或噪声假设。负提示同时正则化轻量视觉适配模块，提升噪声监督下的鲁棒性。该框架采用两阶段训练方案：从高置信度样本的参数高效微调过渡至协同过滤伪标签指导的完整微调。在其基础上，增强版本通过迭代微调、动量对比学习与大语言模型生成提示进一步提升适应性能。大量实验表明，该方法在无监督方法乃至少样本监督基线上均取得持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To adapt large-scale vision-language models to downstream tasks without costly human annotations, this work proposes Collaborative Fine-Tuning (CoFT), an unsupervised framework that leverages a dual-model, cross-modal collaboration mechanism. The method employs a dual-prompt learning strategy with positive and negative textual prompts to model pseudo-label cleanliness per sample, avoiding hand-crafted thresholds, and uses a two-phase training scheme transitioning from parameter-efficient to full fine-tuning. Experiments show that CoFT, and its enhanced version CoFT+, achieve consistent performance gains over existing unsupervised methods and can even surpass few-shot supervised baselines.</div>
<div class="mono" style="margin-top:8px">为使CLIP等大规模视觉-语言模型适应下游任务时无需昂贵的人工标注，本研究提出了协作微调（CoFT），这是一个利用未标注数据、基于双模型跨模态协作机制的无监督适应框架。该方法引入了包含正负文本提示的双提示学习策略，以样本依赖的方式显式建模伪标签的清洁度，避免了手动设定阈值，并采用从参数高效微调到全微调的两阶段训练方案。实验结果表明，CoFT及其增强版本CoFT+的性能持续优于现有的无监督方法，甚至超过了少样本监督基线。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement</div>
<div class="meta-line">Authors: Zipeng Zhu, Zhanghao Hu, Qinglin Zhu, Yuxi Hong, Yijun Liu, Jingyong Su, Yulan He, Lin Gui</div>
<div class="meta-line">First: 2026-02-04T08:13:01+00:00 · Latest: 2026-02-04T08:13:01+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04304v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static &quot;magic layer&quot; empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越静态裁剪：层自适应视觉定位与解码增强</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型通过将视觉图块与文本嵌入空间对齐而快速发展，但固定的视觉标记预算迫使图像被调整为统一的预训练分辨率，这通常会抹去细粒度细节，并因过度依赖语言先验而导致幻觉。近期的注意力引导增强方法（如裁剪或区域聚焦注意力分配）缓解了这一问题，但通常依赖于在简单识别基准上凭经验选择的静态“魔法层”，因此可能无法迁移到复杂推理任务。与这种静态假设相反，我们提出了视觉定位的动态视角。通过分层敏感性分析，我们证明视觉定位是一个动态过程：简单物体识别任务依赖中间层，而复杂的视觉搜索和推理任务需要在更深层重新激活视觉信息。基于这一观察，我们引入了查询驱动的视觉激活（VAQ）指标，该指标通过测量注意力对输入查询的敏感性，识别出注意力图与查询特定视觉定位最相关的层级。在VAQ基础上，我们进一步提出LASER（面向推理的层自适应注意力引导选择性视觉与解码增强），这是一种无需训练的自适应推理过程，能够为视觉定位和问答任务自适应选择适合的层级。在多样化视觉问答基准上的实验表明，LASER能显著提升不同复杂度任务的视觉问答准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of large vision-language models (LVLMs) that use a fixed visual-token budget, which forces image resizing and can erase fine details, leading to hallucinations. The authors propose a dynamic perspective on visual grounding, introducing Visual Activation by Query (VAQ) to identify the most relevant attention layer for query-specific grounding based on a layer-wise sensitivity analysis. They then develop LASER, a training-free inference method that adaptively selects layers for visual localization and decoding. Experimental results on various VQA benchmarks demonstrate that LASER significantly improves accuracy across tasks of varying complexity.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型中固定视觉令牌预算会抹去细节并导致幻觉的问题，以及超越基于简单基准经验性选择静态“魔法层”进行注意力引导增强的局限，本研究提出了视觉定位的动态视角。该方法通过层间敏感性分析，引入了基于查询的视觉激活度量来识别与查询最相关的注意力层，并开发了LASER这一无需训练的自适应推理框架，用于选择任务合适的层进行视觉定位和解码增强。在多样的视觉问答基准测试上的实验表明，LASER显著提升了不同复杂度任务中的问答准确性。</div>
</details>
</div>
<div class="card">
<div class="title">MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Xiongtao Sun, Hui Li, Jiaming Zhang, Yujie Yang, Kaili Liu, Ruxin Feng, Wen Jun Tan, Wei Yang Bryan Lim</div>
<div class="meta-line">First: 2025-11-21T04:33:11+00:00 · Latest: 2026-02-04T07:29:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16940v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16940v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) pose significant individual-level privacy risks by linking fragmented multimodal data to identifiable individuals through hierarchical chain-of-thought reasoning. However, existing privacy benchmarks remain structurally insufficient for this threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM&#x27;s ability to infer and link distributed information to construct individual profiles. To address this gap, we propose MultiPriv, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the Privacy Perception and Reasoning (PPR) framework and construct a bilingual multimodal dataset with synthetic individual profiles, where identifiers (e.g., faces, names) are linked to sensitive attributes. This design enables nine challenging tasks spanning attribute detection, cross-image re-identification, and chained inference. We conduct a large-scale evaluation of over 50 open-source and commercial VLMs. Our analysis shows that 60 percent of widely used VLMs can perform individual-level privacy reasoning with up to 80 percent accuracy, posing a significant threat to personal privacy. MultiPriv provides a foundation for developing and assessing privacy-preserving VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MultiPriv：视觉语言模型中个体级隐私推理的基准测试</div>
<div class="mono" style="margin-top:8px">现代视觉语言模型（VLMs）通过分层思维链推理将碎片化的多模态数据与可识别个体关联，带来显著的个体级隐私风险。然而，现有隐私基准在结构上仍不足以应对此威胁，因其主要评估隐私感知，未能解决更关键的隐私推理风险：即VLM推断并关联分散信息以构建个体画像的能力。为填补这一空白，我们提出MultiPriv——首个系统评估VLM个体级隐私推理的基准。我们引入隐私感知与推理（PPR）框架，构建了包含合成个体画像的双语多模态数据集，其中标识符（如人脸、姓名）与敏感属性相关联。该设计涵盖属性检测、跨图像重识别及链式推理等九项挑战性任务。我们对超过50个开源及商业VLM进行了大规模评估，分析表明60%的常用VLM能以高达80%的准确率执行个体级隐私推理，对个人隐私构成严重威胁。MultiPriv为开发与评估隐私保护型VLM奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern Vision-Language Models (VLMs) can infer and link fragmented multimodal data to specific individuals, a privacy risk not captured by existing benchmarks that focus only on privacy perception. To address this, the authors introduce MultiPriv, a benchmark built on a Privacy Perception and Reasoning framework and a bilingual dataset of synthetic profiles linking identifiers to sensitive attributes, which is used to evaluate VLMs across nine tasks including attribute detection and chained inference. In a large-scale evaluation of over 50 models, the study finds that 60% of widely used VLMs can perform individual-level privacy reasoning with up to 80% accuracy, highlighting a significant privacy threat and establishing a foundation for developing privacy-preserving models.</div>
<div class="mono" style="margin-top:8px">现代视觉语言模型（VLMs）能够通过关联碎片化数据推断敏感个人信息，但现有基准仅评估基本的隐私感知，而非构建个人档案的、更危险的层次化推理能力。为此，研究者提出了MultiPriv基准，其基于隐私感知与推理框架，使用包含将标识符与敏感属性关联的合成个人档案的双语多模态数据集，设计了属性检测、跨图像重识别和链式推理等九项任务。对超过50个开源和商业VLMs的大规模评估表明，60%的常用模型能以高达80%的准确率进行此类隐私推理，揭示了严重的隐私威胁，并为开发隐私保护模型奠定了基础。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260207_0345.html">20260207_0345</a>
<a href="archive/20260206_0629.html">20260206_0629</a>
<a href="archive/20260206_0531.html">20260206_0531</a>
<a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
