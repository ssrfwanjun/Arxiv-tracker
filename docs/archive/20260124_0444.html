<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-24 04:44</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260124_0444</div>
    <div class="row"><div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：面向文档的具身视觉语言前端系统</div>
<div class="mono" style="margin-top:8px">GutenOCR是通过微调Qwen2.5-VL-3B和Qwen2.5-VL-7B获得的一系列具身OCR前端模型。所得的单检查点视觉语言模型通过统一的提示驱动接口，实现文本读取、检测与定位功能。基于商业文档、科学文献及合成定位数据训练，该模型支持整页与局部读取，提供行级与段落级边界框，并能响应“X在哪里？”的条件查询。我们提出了具身OCR评估框架，实验表明在1.05万份保留的商业与科学文档上，GutenOCR-7B的复合具身OCR分数较其骨干网络Qwen2.5-VL-7B提升超一倍（0.40至0.82）。在Fox与OmniDocBench v1.5基准测试中，本方法显著提升了区域/行级OCR性能及文本检测召回率，但在页面级线性化、色彩引导OCR及公式密集版式处理方面存在权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for unified document understanding models that combine reading, detection, and grounding capabilities. The authors introduce GutenOCR, a family of vision-language models created by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on business documents, scientific articles, and synthetic grounding data, enabling a prompt-based interface for full-page and localized reading with bounding box outputs. Experimental results show that GutenOCR-7B more than doubles the composite grounded OCR score of its backbone model (from 0.40 to 0.82) on 10.5K held-out pages, substantially improves region- and line-level OCR and text-detection recall on Fox and OmniDocBench v1.5 benchmarks, while revealing trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div>
<div class="mono" style="margin-top:8px">该研究旨在开发能够统一实现文档阅读、检测与空间定位能力的文档理解模型。方法通过对Qwen2.5-VL视觉语言模型（30亿和70亿参数）在商业文档、科学论文及合成定位数据上进行微调，构建了GutenOCR系列模型，其通过提示接口支持整页/局部阅读及空间查询。实验结果表明，GutenOCR-7B在10.5K份保留文档上的综合定位OCR分数相比其骨干模型提升了一倍以上（从0.40到0.82），在Fox和OmniDocBench基准测试中显著提升了区域/行级OCR精度和文本检测召回率，但也揭示了在页面线性化、颜色引导OCR及公式密集布局方面的性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models</div>
<div class="meta-line">Authors: Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan, Yangfan He, Yuchen Li, Jingqun Tang</div>
<div class="meta-line">First: 2026-01-22T16:02:56+00:00 · Latest: 2026-01-22T16:02:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16065v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16065v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as &#x27;distracting tokens&#x27;. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model&#x27;s visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DTP：一种面向视觉语言动作模型的高效干扰令牌剪枝框架</div>
<div class="mono" style="margin-top:8px">视觉语言动作（VLA）模型通过利用视觉语言模型（VLM）强大的感知能力理解环境并直接输出动作，在机器人操作领域取得了显著进展。然而，VLA模型默认可能过度关注任务无关区域的图像令牌（即“干扰令牌”），这会干扰模型在每一步生成预期动作令牌的过程，影响任务成功率。本文提出了一种即插即用的干扰令牌剪枝（DTP）框架，能动态检测并剪除这些干扰图像令牌。通过修正模型的视觉注意力模式，我们在不改变原始架构或增加额外输入的前提下，旨在提升任务成功率，并探索模型的性能上限。在SIMPLER基准测试（Li等人，2024）上的实验表明，该方法在不同类型的新型VLA模型中均能持续提升任务成功率，证明了其对基于Transformer的VLA模型的普适性。进一步分析揭示了所有测试模型的任务成功率与任务无关区域注意力强度呈负相关，这指出了VLA模型的共性现象，可为未来研究提供指引。代码已开源：https://anonymous.4open.science/r/CBD3。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Action (VLA) models for robotic manipulation can be disrupted by overly attending to irrelevant image regions, termed &#x27;distracting tokens&#x27;, which degrades action generation and task success. To address this, the authors propose a plug-and-play Distracting Token Pruning (DTP) framework that dynamically identifies and removes these distracting tokens to correct visual attention patterns without modifying the model architecture. Experiments on the SIMPLER Benchmark show that DTP consistently improves task success rates across various transformer-based VLA models, with further analysis revealing a negative correlation between success rate and attention to task-irrelevant regions, highlighting a common issue for future research.</div>
<div class="mono" style="margin-top:8px">用于机器人操作的视觉-语言-动作（VLA）模型可能会因关注图像中不相关的区域（即“干扰令牌”）而受到误导，从而影响动作生成和任务成功率。为此，研究者提出了一种即插即用的干扰令牌剪枝（DTP）框架，该框架能动态识别并移除这些干扰令牌，以纠正视觉注意力模式，且无需修改基础模型架构。在SIMPLER基准测试上的实验表明，DTP能持续提升多种基于Transformer的VLA模型的任务成功率；分析进一步揭示了任务成功率与模型对任务无关区域的注意力呈负相关，这为未来研究指出了一个普遍现象。</div>
</details>
</div>
<div class="card">
<div class="title">DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning</div>
<div class="meta-line">Authors: Junha Lee, Eunha Park, Minsu Cho</div>
<div class="meta-line">First: 2026-01-22T15:23:35+00:00 · Latest: 2026-01-22T15:23:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16046v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16046v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DextER：基于语言驱动的灵巧抓取生成与具身推理</div>
<div class="mono" style="margin-top:8px">语言驱动的灵巧抓取生成要求模型理解任务语义、三维几何及复杂的手-物交互。尽管视觉-语言模型已应用于此问题，现有方法直接将观测映射至抓取参数，缺乏对物理交互的中间推理。本文提出DextER（具身推理的灵巧抓取生成），引入基于接触的具身推理以实现多指操控。核心洞见在于：预测手部哪些连杆接触物体表面的哪些位置，可形成一种具身感知的中间表示，从而连接任务语义与物理约束。DextER通过自回归生成具身接触令牌（指定手指连杆与物体表面的接触位置），再生成编码手部构型的抓取令牌。在DexGYS数据集上，DextER达成67.14%的成功率，较最优方法提升3.83个百分点，意图对齐度提升96.4%。我们还通过部分接触指定实现了可引导生成，为抓取合成提供细粒度控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating dexterous grasps from language instructions, which requires understanding task semantics, 3D geometry, and complex hand-object interactions. The proposed method, DextER, introduces an embodied reasoning approach that first autoregressively generates contact tokens specifying which finger links contact specific object surfaces, and then produces grasp tokens encoding the full hand configuration. Experimental results on the DexGYS benchmark show the model achieves a 67.14% success rate, outperforming the previous state-of-the-art by 3.83 percentage points and improving intention alignment by 96.4%, while also enabling steerable generation through partial contact specification.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决根据语言指令生成灵巧抓握的挑战，这需要理解任务语义、三维几何和复杂的手-物体交互。提出的DextER方法引入了一种具身推理方法，自回归地生成中间接触标记，指定哪些手指链接接触物体表面的特定位置，然后生成编码完整手部配置的抓握标记。在DexGYS基准测试上的实验结果显示，成功率达到67.14%，比先前的最先进方法高出3.83个百分点，在意图对齐上实现了96.4%的改进，同时通过部分接触指定实现了可引导的生成。</div>
</details>
</div>
<div class="card">
<div class="title">RadJEPA: Radiology Encoder for Chest X-Rays via Joint Embedding Predictive Architecture</div>
<div class="meta-line">Authors: Anas Anwarul Haq Khan, Mariam Husain, Kshitij Jadhav</div>
<div class="meta-line">First: 2026-01-22T12:11:53+00:00 · Latest: 2026-01-22T12:11:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15891v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in medical vision language models guide the learning of visual representations; however, this form of supervision is constrained by the availability of paired image text data, raising the question of whether robust radiology encoders can be learned without relying on language supervision. In this work, we introduce RadJEPA, a self-supervised framework built on a Joint Embedding Predictive Architecture that learns without language supervision. Pre-trained solely on unlabeled chest X-ray images, the model learns to predict latent representations of masked image regions. This predictive objective differs fundamentally from both image text pre-training and DINO-style self-distillation: rather than aligning global representations across views or modalities, RadJEPA explicitly models latent-space prediction. We evaluate the learned encoder on disease classification, semantic segmentation, and report generation tasks. Across benchmarks, RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RadJEPA：基于联合嵌入预测架构的胸部X光放射学编码器</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型的最新进展指导了视觉表征的学习，但这种监督形式受限于配对图文数据的可用性，引发了能否在不依赖语言监督的情况下学习稳健放射学编码器的问题。本研究提出RadJEPA，一种基于联合嵌入预测架构的自监督框架，无需语言监督即可学习。该模型仅通过未标注的胸部X光图像进行预训练，学习预测掩码图像区域的潜在表征。这种预测目标与图文预训练和DINO式自蒸馏有本质区别：RadJEPA不追求跨视图或跨模态的全局表征对齐，而是显式建模潜在空间预测。我们在疾病分类、语义分割和报告生成任务上评估所学编码器。在多项基准测试中，RadJEPA的表现均超越包括Rad-DINO在内的前沿方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of medical vision-language models that depend on paired image-text data by proposing RadJEPA, a self-supervised framework that learns robust radiology encoders without language supervision. The method is based on a Joint Embedding Predictive Architecture pre-trained solely on unlabeled chest X-ray images, where the model learns to predict latent representations of masked image regions, explicitly modeling latent-space prediction rather than aligning global representations. Experimental evaluation on disease classification, semantic segmentation, and report generation tasks demonstrates that RadJEPA achieves performance exceeding state-of-the-art approaches, including Rad-DINO.</div>
<div class="mono" style="margin-top:8px">本研究针对依赖配对图文数据的医学视觉语言模型的局限性，提出了RadJEPA，这是一种无需语言监督的自监督框架，用于学习稳健的放射学编码器。该方法基于联合嵌入预测架构，仅使用未标记的胸部X光图像进行预训练，模型学习预测被遮蔽图像区域的潜在表示，明确地对潜在空间预测进行建模，而非对齐全局表示。在疾病分类、语义分割和报告生成任务上的实验评估表明，RadJEPA的性能超过了包括Rad-DINO在内的最先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning</div>
<div class="meta-line">Authors: Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang, Zheng Wei</div>
<div class="meta-line">First: 2026-01-21T08:09:25+00:00 · Latest: 2026-01-22T12:09:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14750v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14750v2">PDF</a> · <a href="https://github.com/TencentBAC/RoT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维渲染：将文本思维链转化为图像以实现视觉潜在推理</div>
<div class="mono" style="margin-top:8px">思维链提示在释放大语言模型的推理能力方面取得了显著成功。尽管思维链提示增强了推理能力，但其冗长性带来了巨大的计算开销。近期研究往往仅关注结果对齐，缺乏对中间推理过程的监督，这些缺陷使得潜在推理链的可分析性变得模糊。为解决这些问题，我们提出了思维渲染框架——首个通过将文本推理步骤渲染为图像来具象化推理链的框架，使潜在逻辑变得显式且可追溯。具体而言，我们利用现有视觉语言模型的视觉编码器作为语义锚点，将视觉嵌入与文本空间对齐。该设计确保了即插即用的实现方式，无需额外预训练开销。在数学与逻辑推理基准上的大量实验表明，相较于显式思维链，我们的方法实现了3-4倍的标记压缩和显著的推理加速，同时保持与其他方法相当的竞争力，验证了该范式的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational inefficiency and lack of intermediate supervision in traditional Chain-of-Thought (CoT) prompting for Large Language Models. The proposed Render-of-Thought (RoT) framework converts verbose textual reasoning steps into images, using vision encoders from existing Vision Language Models to align visual embeddings with textual semantics without requiring additional pre-training. Experimental results on mathematical and logical reasoning benchmarks show that RoT achieves 3-4x token compression and significant inference speedup compared to explicit CoT, while maintaining competitive task performance.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）中传统思维链（CoT）提示的计算效率低下和中间过程缺乏监督的问题，提出了Render-of-Thought（RoT）框架，将冗长的文本推理步骤转换为紧凑的图像，使潜在的推理过程变得显式且可分析。该方法利用现有视觉语言模型（VLMs）的视觉编码器将视觉嵌入与文本语义对齐，实现了即插即用的部署，无需额外预训练。在数学和逻辑推理基准测试上的实验结果表明，与显式CoT相比，RoT实现了3-4倍的令牌压缩和显著的推理加速，同时保持了有竞争力的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video</div>
<div class="meta-line">Authors: Pascal Benschop, Justin Dauwels, Jan van Gemert</div>
<div class="meta-line">First: 2026-01-22T09:14:11+00:00 · Latest: 2026-01-22T09:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15780v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15780v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning in vision language models (VLMs) remains fragile when semantics hinge on subtle temporal or geometric cues. We introduce a synthetic benchmark that probes two complementary skills: situational awareness (recognizing whether an interaction is harmful or benign) and spatial awareness (tracking who does what to whom, and reasoning about relative positions and motion). Through minimal video pairs, we test three challenges: distinguishing violence from benign activity, binding assailant roles across viewpoints, and judging fine-grained trajectory alignment. While we evaluate recent VLMs in a training-free setting, the benchmark is applicable to any video classification model. Results show performance only slightly above chance across tasks. A simple aid, stable color cues, partly reduces assailant role confusions but does not resolve the underlying weakness. By releasing data and code, we aim to provide reproducible diagnostics and seed exploration of lightweight spatial priors to complement large-scale pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于合成生成视频评估视觉语言模型的情境与空间感知能力</div>
<div class="mono" style="margin-top:8px">当语义依赖于细微的时间或几何线索时，视觉语言模型的空间推理能力仍显脆弱。我们引入一个合成基准测试，探究两项互补技能：情境感知（判断交互行为是否具有危害性）和空间感知（追踪行为主体与对象的关系，并推理相对位置与运动）。通过极简视频对，我们测试三个挑战：区分暴力与良性活动、跨视角绑定施暴者角色、判断细粒度轨迹对齐。我们在免训练设置下评估近期VLMs，但该基准适用于任何视频分类模型。结果显示各任务性能仅略高于随机水平。简单的稳定颜色线索辅助能部分减少施暴者角色混淆，但未解决根本缺陷。通过开源数据与代码，我们旨在提供可复现的诊断工具，并推动轻量级空间先验的探索，以补充大规模预训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the fragile spatial reasoning of vision-language models (VLMs) when semantics depend on subtle temporal or geometric cues. The method introduces a synthetic video benchmark to probe situational awareness (distinguishing harmful from benign interactions) and spatial awareness (tracking roles and reasoning about positions and motion) through minimal video pairs testing three challenges: violence classification, role binding across viewpoints, and trajectory alignment judgment. Key experimental findings show that recent VLMs perform only slightly above chance across tasks, and while stable color cues partly reduce role confusions, they do not resolve the underlying weakness, highlighting the need for improved spatial reasoning.</div>
<div class="mono" style="margin-top:8px">当语义依赖于细微的时间或几何线索时，视觉语言模型（VLMs）的空间推理能力仍然薄弱。为此，研究者构建了一个合成视频基准，用于测试两种互补技能：情境感知（区分有害与良性互动）和空间感知（追踪施动者角色并推理相对位置与运动）。该方法以无需训练的方式评估了近期VLMs在三个具体挑战上的表现，这些挑战通过极简视频对进行测试。实验结果表明，模型在所有任务上的性能仅略高于随机水平；虽然提供稳定的颜色线索部分减少了施动者角色混淆，但并未解决根本弱点。通过开源数据和代码，该研究旨在提供可复现的诊断工具，并推动探索轻量级空间先验以补充大规模预训练。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-22T09:09:47+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v4">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但其仍易受对抗性越狱攻击影响——攻击者通过精心构造的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等局限，难以应用于现实黑盒场景。为此，我们提出基于同步扰动随机逼近零阶优化（ZO-SPSA）的黑盒越狱攻击方法，具备三大优势：（1）通过输入-输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低GPU内存占用的资源需求。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，且扰动不可感知性与白盒方法相当。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱的现实可行性，并暴露了当前LVLMs安全机制的关键缺陷。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods in real-world settings, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free, model-agnostic approach approximates gradients through input-output interactions without requiring internal model knowledge, thereby reducing computational resource demands. Experimental evaluation on models including InstructBLIP, LLaVA, and MiniGPT-4 demonstrates a high jailbreak success rate of 83.0% on InstructBLIP with imperceptible perturbations, and shows strong adversarial transferability, with attacks from MiniGPT-4 achieving a 64.18% attack success rate on other LVLMs, highlighting critical safety weaknesses.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型（LVLMs）易受对抗性越狱攻击的漏洞，以及现有白盒方法在现实黑盒场景中不切实际的问题，提出了一种基于零阶优化和同步扰动随机逼近（ZO-SPSA）的黑盒攻击方法。该方法无需梯度信息或模型内部知识，通过输入-输出交互进行梯度近似，具有模型无关性和较低的资源需求。在InstructBLIP、LLaVA和MiniGPT-4上的实验结果表明，该方法在InstructBLIP上实现了83.0%的最高越狱成功率，并且从MiniGPT-4生成的对抗样本对其他模型表现出强迁移性，攻击成功率可达64.18%，从而揭示了当前LVLMs安全机制中的关键缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</div>
<div class="meta-line">Authors: Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T17:59:44+00:00 · Latest: 2026-01-22T08:52:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project page: https://faceong.github.io/VIKI-R/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09049v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09049v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://faceong.github.io/VIKI-R/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIKI-R：基于强化学习的具身多智能体协同协调框架</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体仍是人工智能的核心挑战，需要感知驱动的推理与可扩展的协作策略。尽管近期研究利用大语言模型进行多智能体规划，少数工作开始探索视觉语言模型在视觉推理中的应用，但这些基于VLM的方法对多样化具身形态的支持仍显不足。本研究提出首个面向具身多智能体协作的层次化基准VIKI-Bench，包含智能体激活、任务规划与轨迹感知三层结构，涵盖多类机器人形态、多视角视觉观测及结构化监督信号，以评估基于视觉输入的推理能力。为验证其效用，我们提出两阶段框架VIKI-R：先通过思维链标注样本微调预训练视觉语言模型，再基于多层次奖励信号进行强化学习。实验表明VIKI-R在所有任务层级均显著优于基线方法，且强化学习能促使异构智能体涌现组合式协作模式。VIKI-Bench与VIKI-R共同为具身AI系统的多智能体视觉驱动协作提供了统一测试平台与方法体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of coordinating multiple embodied agents in dynamic environments, which requires both visual reasoning and scalable cooperation strategies. The authors introduce VIKI-Bench, a hierarchical benchmark for embodied multi-agent cooperation with structured levels for agent activation, task planning, and trajectory perception, and propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model with Chain-of-Thought demonstrations and then applies reinforcement learning with multi-level rewards. Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels and enables the emergence of compositional cooperation patterns among heterogeneous agents.</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体需要感知驱动的推理和可扩展的合作策略，但现有的视觉语言模型方法通常缺乏对多样化机器人具身的支持。为此，研究者提出了VIKI-Bench，这是一个为具身多智能体合作设计的层次化基准测试，包含智能体激活、任务规划和轨迹感知三个结构化层级，并具有多样化的具身形式和多视角视觉观测。他们提出了VIKI-R，一个两阶段框架：首先使用思维链标注的演示数据对预训练的视觉语言模型进行微调，然后通过多层级奖励信号进行强化学习。实验结果表明，VIKI-R在所有任务层级上均显著优于基线方法，并能在异构智能体之间实现组合式合作模式的涌现。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework</div>
<div class="meta-line">Authors: Shubham Shukla, Kunal Sonalkar</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-01-22T07:33:41+00:00 · Latest: 2026-01-22T07:33:41+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026 Workshop on Physical Retail AI (PRAW)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained attribute prediction is essential for fashion retail applications including catalog enrichment, visual search, and recommendation systems. Vision-Language Models (VLMs) offer zero-shot prediction without task-specific training, yet their systematic evaluation on multi-attribute fashion tasks remains underexplored. A key challenge is that fashion attributes are often conditional. For example, &quot;outer fabric&quot; is undefined when no outer garment is visible. This requires models to detect attribute applicability before attempting classification. We introduce a three-tier evaluation framework that decomposes this challenge: (1) overall task performance across all classes (including NA class: suggesting attribute is not applicable) for all attributes, (2) attribute applicability detection, and (3) fine-grained classification when attributes are determinable. Using DeepFashion-MultiModal, which explicitly defines NA (meaning attribute doesn&#x27;t exist or is not visible) within attribute label spaces, we benchmark nine VLMs spanning flagship (GPT-5, Gemini 2.5 Pro), efficient (GPT-5 Mini, Gemini 2.5 Flash), and ultra-efficient tiers (GPT-5 Nano, Gemini 2.5 Flash-Lite) against classifiers trained on pretrained Fashion-CLIP embeddings on 5,000 images across 18 attributes. Our findings reveal that: (1) zero-shot VLMs achieve 64.0% macro-F1, a threefold improvement over logistic regression on pretrained Fashion-CLIP embeddings; (2) VLMs excel at fine-grained classification (Tier 3: 70.8% F1) but struggle with applicability detection (Tier 2: 34.1% NA-F1), identifying a key bottleneck; (3) efficient models achieve over 90% of flagship performance at lower cost, offering practical deployment paths. This diagnostic framework enables practitioners to pinpoint whether errors stem from visibility detection or classification, guiding targeted improvements for production systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉-语言模型的零样本产品属性标注：三层评估框架</div>
<div class="mono" style="margin-top:8px">细粒度属性预测对于时尚零售应用（包括商品目录丰富化、视觉搜索和推荐系统）至关重要。视觉-语言模型（VLM）无需任务特定训练即可实现零样本预测，但其在多属性时尚任务上的系统性评估仍待深入探索。关键挑战在于时尚属性常具有条件性，例如当外衣不可见时“外部面料”属性即无定义，这要求模型在尝试分类前先检测属性适用性。我们提出三层评估框架以分解该挑战：（1）所有属性在全类别（含“不适用”类）上的整体任务性能；（2）属性适用性检测；（3）属性可确定时的细粒度分类。基于明确在属性标签空间中定义“不适用”（指属性不存在或不可见）的DeepFashion-MultiModal数据集，我们在18个属性的5000张图像上，对涵盖旗舰级（GPT-5、Gemini 2.5 Pro）、高效级（GPT-5 Mini、Gemini 2.5 Flash）和超高效级（GPT-5 Nano、Gemini 2.5 Flash-Lite）的九种VLM进行基准测试，并与基于预训练Fashion-CLIP嵌入训练的分类器对比。研究发现：（1）零样本VLM宏平均F1达64.0%，较基于预训练Fashion-CLIP嵌入的逻辑回归提升三倍；（2）VLM擅长细粒度分类（第三层：70.8% F1），但在适用性检测上表现欠佳（第二层：34.1% NA-F1），这揭示了关键瓶颈；（3）高效模型能以更低成本实现旗舰模型90%以上的性能，为实际部署提供可行路径。该诊断框架可帮助从业者定位错误源于可见性检测还是分类过程，从而指导生产系统的针对性改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for fine-grained attribute prediction in fashion retail applications, where existing Vision-Language Models (VLMs) lack systematic evaluation, especially for conditional attributes that may not be applicable. The authors propose a three-tier evaluation framework to decompose the task into overall performance, attribute applicability detection, and fine-grained classification when applicable. Benchmarking nine VLMs on the DeepFashion-MultiModal dataset reveals that zero-shot VLMs significantly outperform logistic regression on pretrained embeddings, excel at fine-grained classification, but struggle notably with detecting attribute applicability, while efficient models retain most of the flagship performance at a lower cost.</div>
<div class="mono" style="margin-top:8px">本研究针对时尚零售应用（如目录丰富化和视觉搜索）中细粒度属性预测的需求，现有视觉-语言模型在多属性、条件性属性的任务上缺乏系统评估。作者提出了一个三层评估框架来分解这一挑战：评估整体性能、属性适用性检测以及属性可确定时的细粒度分类。他们使用DeepFashion-MultiModal数据集，对九种视觉-语言模型与基于Fashion-CLIP嵌入训练的模型进行了基准测试。主要实验结果表明，零样本视觉-语言模型实现了64.0%的宏观F1分数，比逻辑回归基线提高了三倍，在细粒度分类上表现出色（70.8% F1），但在适用性检测上存在困难（34.1% NA-F1），同时高效模型能以更低成本实现旗舰模型90%以上的性能，从而识别了关键瓶颈并提供了实用的部署路径。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-event Video-Text Retrieval</div>
<div class="meta-line">Authors: Gengyuan Zhang, Jisen Ren, Jindong Gu, Volker Tresp</div>
<div class="meta-line">First: 2023-08-22T16:32:46+00:00 · Latest: 2026-01-22T06:58:13+00:00</div>
<div class="meta-line">Comments: [fixed typos in equations] accepted to ICCV2023 Poster; some figures are not supported when viewed online, please download the file and view locally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2308.11551v3">Abs</a> · <a href="https://arxiv.org/pdf/2308.11551v3">PDF</a> · <a href="https://github.com/gengyuanmax/MeVTR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We present a simple model, Me-Retriever, which incorporates key event video representation and a new MeVTR loss for the MeVTR task. Comprehensive experiments show that this straightforward framework outperforms other models in the Video-to-Text and Text-to-Video tasks, effectively establishing a robust baseline for the MeVTR task. We believe this work serves as a strong foundation for future studies. Code is available at https://github.com/gengyuanmax/MeVTR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多事件视频-文本检索</div>
<div class="mono" style="margin-top:8px">视频-文本检索（VTR）是互联网海量视频-文本数据时代的关键多模态任务。主流方法通常采用双流视觉-语言模型架构学习视频-文本对的联合表征。然而，这些模型基于视频-文本双射对应的假设，忽略了更实际的场景：视频内容通常包含多个事件，而用户查询或网页元数据等文本往往针对单一事件。这导致传统训练目标与实际应用之间存在差距，可能造成早期模型在推理时性能下降。本研究提出多事件视频-文本检索（MeVTR）任务，针对视频包含多个不同事件的场景，作为传统VTR任务的细分场景。我们提出了简洁的Me-Retriever模型，结合关键事件视频表征与新型MeVTR损失函数。综合实验表明，该框架在视频到文本和文本到视频任务中均优于其他模型，为MeVTR任务建立了稳健基线。本工作为未来研究奠定了坚实基础。代码发布于https://github.com/gengyuanmax/MeVTR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the practical gap in Video-Text Retrieval (VTR) where existing models assume one-to-one video-text correspondences, while real-world videos often contain multiple events paired with specific, single-event texts. To tackle this Multi-event VTR (MeVTR) scenario, the authors propose Me-Retriever, a model that employs key event video representation and a novel MeVTR loss. Experimental results demonstrate that this framework outperforms other models in both Video-to-Text and Text-to-Video retrieval tasks, establishing a strong baseline for the MeVTR problem.</div>
<div class="mono" style="margin-top:8px">本研究针对传统视频-文本检索模型假设视频与文本一一对应的局限性，而实际视频常包含多个事件，文本查询却针对单一事件。方法上提出了多事件视频-文本检索任务及Me-Retriever模型，该模型采用关键事件视频表示和新的MeVTR损失函数来处理多事件场景。实验结果表明，该框架在视频到文本和文本到视频检索任务上均优于现有模型，为多事件视频-文本检索任务建立了坚实的基线。</div>
</details>
</div>
<div class="card">
<div class="title">DevPrompt: Deviation-Based Prompt Learning for One-Normal ShotImage Anomaly Detection</div>
<div class="meta-line">Authors: Morteza Poudineh, Marc Lalonde</div>
<div class="meta-line">First: 2026-01-21T20:35:51+00:00 · Latest: 2026-01-21T20:35:51+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15453v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-normal shot anomaly detection (FNSAD) aims to detect abnormal regions in images using only a few normal training samples, making the task highly challenging due to limited supervision and the diversity of potential defects. Recent approaches leverage vision-language models such as CLIP with prompt-based learning to align image and text features. However, existing methods often exhibit weak discriminability between normal and abnormal prompts and lack principled scoring mechanisms for patch-level anomalies. We propose a deviation-guided prompt learning framework that integrates the semantic power of vision-language models with the statistical reliability of deviation-based scoring. Specifically, we replace fixed prompt prefixes with learnable context vectors shared across normal and abnormal prompts, while anomaly-specific suffix tokens enable class-aware alignment. To enhance separability, we introduce a deviation loss with Top-K Multiple Instance Learning (MIL), modeling patch-level features as Gaussian deviations from the normal distribution. This allows the network to assign higher anomaly scores to patches with statistically significant deviations, improving localization and interpretability. Experiments on the MVTecAD and VISA benchmarks demonstrate superior pixel-level detection performance compared to PromptAD and other baselines. Ablation studies further validate the effectiveness of learnable prompts, deviation-based scoring, and the Top-K MIL strategy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DevPrompt：基于偏差的提示学习用于单正常样本图像异常检测</div>
<div class="mono" style="margin-top:8px">少样本正常图像异常检测（FNSAD）旨在仅用少量正常训练样本检测图像中的异常区域，由于监督有限和潜在缺陷的多样性，该任务极具挑战性。现有方法利用视觉-语言模型（如CLIP）通过基于提示的学习对齐图像与文本特征，但常存在正常与异常提示间区分度弱、缺乏针对局部异常的原则性评分机制等问题。本文提出一种偏差引导的提示学习框架，将视觉-语言模型的语义能力与基于偏差评分的统计可靠性相结合。具体而言，我们使用可学习的上下文向量替代固定提示前缀（正常与异常提示共享），并通过异常专属后缀令牌实现类别感知对齐。为增强可分性，引入结合Top-K多示例学习（MIL）的偏差损失，将局部特征建模为偏离正态分布的高斯偏差，使网络能为统计显著偏离的局部区域分配更高异常分数，从而提升定位能力与可解释性。在MVTecAD和VISA基准上的实验表明，本方法在像素级检测性能上优于PromptAD等基线模型。消融实验进一步验证了可学习提示、基于偏差的评分及Top-K MIL策略的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of few-normal shot anomaly detection (FNSAD), where limited normal samples and diverse defects hinder effective anomaly localization. The method, DevPrompt, introduces a deviation-guided prompt learning framework that combines CLIP&#x27;s vision-language capabilities with statistical deviation scoring, using learnable context vectors and anomaly-specific suffix tokens for class-aware feature alignment, and employs a deviation loss with Top-K Multiple Instance Learning to model patch features as Gaussian deviations for improved discriminability. Experimental results on MVTecAD and VISA benchmarks show superior pixel-level detection performance over PromptAD and other baselines, with ablations confirming the contributions of learnable prompts, deviation scoring, and the Top-K MIL strategy.</div>
<div class="mono" style="margin-top:8px">本研究针对少样本正常图像异常检测（FNSAD）的挑战，即有限的正常样本和多样的缺陷使得异常定位困难。方法DevPrompt提出了一种基于偏差的提示学习框架，将视觉-语言模型与统计偏差评分相结合，使用可学习的上下文向量和异常特定后缀令牌进行类别感知对齐，并采用带有Top-K多示例学习的偏差损失，将补丁级特征建模为高斯偏差以增强区分性。在MVTecAD和VISA基准测试上的实验结果表明，其像素级检测性能优于PromptAD等基线方法，消融研究进一步验证了可学习提示、基于偏差的评分和Top-K MIL策略的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">CURE: Curriculum-guided Multi-task Training for Reliable Anatomy Grounded Report Generation</div>
<div class="meta-line">Authors: Pablo Messina, Andrés Villa, Juan León Alcázar, Karen Sánchez, Carlos Hinojosa, Denis Parra, Álvaro Soto, Bernard Ghanem</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-21T19:19:41+00:00 · Latest: 2026-01-21T19:19:41+00:00</div>
<div class="meta-line">Comments: 31 pages, 7 figures, submitted to CVPR 2026 (under review)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15408v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15408v1">PDF</a> · <a href="https://github.com/PabloMessina/CURE">Code1</a> · <a href="https://huggingface.co/pamessina/medgemma-4b-it-cure">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical vision-language models can automate the generation of radiology reports but struggle with accurate visual grounding and factual consistency. Existing models often misalign textual findings with visual evidence, leading to unreliable or weakly grounded predictions. We present CURE, an error-aware curriculum learning framework that improves grounding and report quality without any additional data. CURE fine-tunes a multimodal instructional model on phrase grounding, grounded report generation, and anatomy-grounded report generation using public datasets. The method dynamically adjusts sampling based on model performance, emphasizing harder samples to improve spatial and textual alignment. CURE improves grounding accuracy by +0.37 IoU, boosts report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%. CURE is a data-efficient framework that enhances both grounding accuracy and report reliability. Code is available at https://github.com/PabloMessina/CURE and model weights at https://huggingface.co/pamessina/medgemma-4b-it-cure</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CURE：基于课程引导的多任务训练实现可靠解剖学基础报告生成</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型可自动化生成放射学报告，但在准确视觉定位与事实一致性方面存在局限。现有模型常出现文本发现与视觉证据错位，导致预测结果不可靠或基础薄弱。我们提出CURE——一种基于错误感知的课程学习框架，无需额外数据即可提升定位能力与报告质量。CURE利用公开数据集，在短语定位、基础报告生成和解剖学基础报告生成三个任务上对多模态指令模型进行微调。该方法根据模型性能动态调整样本采样策略，侧重困难样本以提升空间与文本对齐度。CURE将定位精度提升0.37 IoU，报告质量提高0.188 CXRFEScore，幻觉现象减少18.6%。该框架具有数据高效性，同步增强了定位精度与报告可靠性。代码发布于https://github.com/PabloMessina/CURE，模型权重发布于https://huggingface.co/pamessina/medgemma-4b-it-cure</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the issues of inaccurate visual grounding and factual inconsistency in automated radiology report generation, this study introduces CURE, a curriculum-guided multi-task training framework. The method fine-tunes a multimodal instructional model on three tasks—phrase grounding, grounded report generation, and anatomy-grounded report generation—using public datasets, and it dynamically adjusts sample weighting based on model errors to focus on harder cases. Experimental results demonstrate that CURE improves grounding accuracy by +0.37 IoU, enhances report quality by +0.188 CXRFEScore, and reduces hallucinations by 18.6%, thereby boosting both grounding precision and report reliability without requiring additional data.</div>
<div class="mono" style="margin-top:8px">为解决自动化放射学报告生成中视觉定位不准确和事实不一致的问题，本研究提出了CURE，一个课程引导的多任务训练框架。该方法在公开数据集上，通过逐步微调多模态指令模型完成短语定位、定位报告生成和解剖学定位报告生成三项任务，并基于模型误差动态调整样本选择以聚焦困难案例。实验结果表明，CURE在无需额外数据的情况下，将定位精度提升了0.37 IoU，将报告整体质量提高了0.188 CXRFEScore，并将幻觉现象减少了18.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Understanding Best Practices for Quantization of Vision-Language Models</div>
<div class="meta-line">Authors: Gautom Das, Vincent La, Ethan Lau, Abhinav Shrivastava, Matthew Gwilliam</div>
<div class="meta-line">First: 2026-01-21T18:59:51+00:00 · Latest: 2026-01-21T18:59:51+00:00</div>
<div class="meta-line">Comments: 15 pages, 12 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15287v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15287v1">PDF</a> · <a href="https://github.com/gautomdas/mmq">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型量化最佳实践探究</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在多种任务中展现出卓越性能，但顶尖系统需要配备大容量内存的高速GPU。为降低系统内存占用与延迟，实践者通常将学习参数量化为半精度。当前研究日益关注如何在更激进的比特位宽下保持模型性能，部分工作已尝试将量化策略应用于视觉Transformer等其他模型。本研究系统探讨了包括前沿GPTQ与AWQ在内的多种量化方法如何有效应用于由视觉模型、语言模型及其连接器构成的多模态流程。我们分析了比特位宽、量化方法及量化应用环节对图像描述、检索与问答任务性能的影响。实验表明：尽管参数量级差异显著，ViT与LLM对模型性能具有同等重要性；对LLM进行低位宽量化可在降低每权重比特数（bpw）的同时保持较高精度。这些发现为多模态大语言模型（MLLMs）的高效部署提供了实用洞见，并凸显了探索多模态模型组件敏感度的重要价值。代码已开源：https://github.com/gautomdas/mmq。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the high memory and latency demands of large language models (LLMs) and vision-language models by exploring efficient quantization strategies for deployment. The study systematically applies various quantization methods, including GPTQ and AWQ, to different components of multimodal pipelines—vision models, language models, and connectors—evaluating their impact on tasks like captioning, retrieval, and question answering. Key findings show that both vision transformers (ViT) and LLMs are similarly critical for performance despite parameter size differences, and that lower-bit quantization of LLMs can maintain high accuracy while significantly reducing bits per weight, offering practical guidance for optimizing multimodal model efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型视觉语言模型（VLMs）部署中内存和延迟过高的问题，因为现有系统需要高性能GPU和大量内存。通过系统应用GPTQ和AWQ等先进量化方法，对多模态管道中的视觉模型、语言模型及其连接器进行量化，评估其对图像描述、检索和问答等任务的影响。实验结果表明，尽管参数规模差异显著，视觉变换器（ViT）和大型语言模型（LLM）对整体性能具有相似的重要性；同时，对LLM进行低位宽量化能在显著降低每权重比特数的同时保持高精度，为多模态大模型的高效部署提供了实用见解。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Refinement Improves Compositional Image Generation</div>
<div class="meta-line">Authors: Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak</div>
<div class="meta-line">First: 2026-01-21T18:59:40+00:00 · Latest: 2026-01-21T18:59:40+00:00</div>
<div class="meta-line">Comments: Project webpage: https://iterative-img-gen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15286v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iterative-img-gen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代优化提升组合式图像生成质量</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型已取得显著进展，但在处理需要同时满足多对象、关系和属性的复杂提示时仍面临挑战。现有的推理时策略（如带验证器的并行采样或单纯增加去噪步数）虽能提升提示对齐度，但在需要满足多重约束的复杂组合场景中仍显不足。受大语言模型中思维链推理成功的启发，我们提出一种迭代式测试时策略：T2I模型在视觉语言模型作为循环评判者的反馈指导下，通过多步骤渐进优化生成结果。该方法无需外部工具或先验知识，可灵活适配各类图像生成器与视觉语言模型。实验表明，该方法在多个基准测试中持续提升图像生成性能：在ConceptMix（k=7）的全正确率提升16.9%，在T2I-CompBench（3D空间类别）提升13.8%，在Visual Jenga场景解构任务提升12.5%（均与计算量匹配的并行采样对比）。除量化提升外，迭代优化通过将复杂提示分解为序列化修正，生成结果更具忠实度——人工评估者对本方法的偏好率达58.7%（基线方法为41.3%）。这些发现共同表明，迭代式自纠正是组合式图像生成中具有广泛适用性的核心原则。完整结果与可视化内容详见项目网页。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image models often fail to accurately generate images from complex prompts involving multiple objects, relations, and attributes. To address this, the authors propose an iterative refinement strategy where a vision-language model provides feedback to guide the image generator across multiple denoising steps, decomposing the complex prompt into sequential corrections. This method significantly improves performance, achieving a 16.9% increase in all-correct rate on ConceptMix, a 13.8% improvement on T2I-CompBench&#x27;s 3D-Spatial category, and a 12.5% gain on Visual Jenga, with human evaluators preferring the refined outputs 58.7% of the time.</div>
<div class="mono" style="margin-top:8px">文本到图像模型在处理涉及多个对象和关系的复杂提示时常常失败。为解决此问题，作者提出了一种迭代优化策略，利用视觉语言模型提供反馈，在多步去噪过程中引导图像生成器，将复杂提示分解为顺序修正。实验结果表明该方法显著提升了生成效果：在ConceptMix基准上的全正确率提高了16.9%，在T2I-CompBench的3D空间类别上提升了13.8%，且人类评估者在58.7%的情况下更偏好该迭代方法而非并行采样基线。</div>
</details>
</div>
<div class="card">
<div class="title">PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu</div>
<div class="meta-line">First: 2026-01-21T17:56:59+00:00 · Latest: 2026-01-21T17:56:59+00:00</div>
<div class="meta-line">Comments: Website: https://progresslm.github.io/ProgressLM/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15224v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://progresslm.github.io/ProgressLM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PROGRESSLM：迈向视觉语言模型中的进程推理</div>
<div class="mono" style="margin-top:8px">任务进程估计需要对长时程动态进行推理，而非识别静态视觉内容。尽管现代视觉语言模型擅长描述可见内容，但其能否通过局部观察推断任务进展程度尚不明确。为此，我们提出Progress-Bench基准，用于系统评估VLM的进程推理能力。除基准测试外，我们进一步通过免训练提示和基于精选数据集ProgressLM-45K的训练方法，探索了受人类启发的两阶段进程推理范式。对14个VLM的实验表明，多数模型尚未具备任务进程估计能力，存在对演示模态和视角变化的敏感性，以及对不可回答情况的处理能力不足。虽然强制结构化进程推理的免训练提示仅产生有限且模型依赖的改进，但基于训练的ProgressLM-3B即使在小规模模型下也实现了稳定提升——尽管其训练任务集与评估任务完全不相交。进一步分析揭示了典型错误模式，并阐明了进程推理成功或失败的条件与原因。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of enabling Vision-Language Models (VLMs) to reason about task progress from partial observations, moving beyond static visual recognition. It introduces Progress-Bench, a benchmark for evaluating progress reasoning, and explores a two-stage reasoning paradigm through both training-free prompting and a training-based approach using the curated ProgressLM-45K dataset. Experiments on 14 VLMs reveal that most models struggle with progress estimation, showing sensitivity to demonstration modality and viewpoint changes, and poor handling of unanswerable cases; while prompting yields limited gains, the training-based ProgressLM-3B model achieves consistent improvements even on disjoint tasks, with analyses identifying characteristic error patterns.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型从部分观察中推理任务进展的挑战，超越静态视觉识别。作者引入了Progress-Bench基准来评估进展推理能力，并通过免训练提示和基于ProgressLM-45K数据集的训练方法探索了两阶段推理范式。在14个视觉语言模型上的实验表明，大多数模型在进展估计上表现不佳，对演示模态和视角变化敏感，且难以处理不可回答的情况。虽然结构化提示带来的改进有限且依赖模型，但基于训练的ProgressLM-3B模型即使在训练任务与评估任务完全不同的情况下也取得了稳定提升，进一步分析揭示了典型的错误模式。</div>
</details>
</div>
<div class="card">
<div class="title">CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</div>
<div class="meta-line">Authors: V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin</div>
<div class="meta-line">First: 2025-12-23T13:44:41+00:00 · Latest: 2026-01-21T16:42:28+00:00</div>
<div class="meta-line">Comments: 37 pages, 42 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20362v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20362v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping. We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free and model-agnostic framework for multimodal image generation. CRAFT transforms a user prompt into a set of explicit, dependency-structured visual constraints, verifies generated images using a vision-language model, and performs targeted prompt updates only when specific constraints are violated. This iterative process includes an explicit stopping criterion, resulting in an interpretable and controllable inference-time refinement loop. Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRAFT：面向多模态文本到图像生成的持续推理与智能反馈调优</div>
<div class="mono" style="margin-top:8px">近期研究表明，推理时的反思与修正能提升文本到图像生成质量且无需重新训练。然而，现有方法多依赖隐式整体评价或无约束提示词改写，导致其行为难以解释、控制或可靠终止。相比之下，大语言模型已从基于验证、定向修正和提前终止的显式结构化“思考”中获益。本文提出CRAFT（持续推理与智能反馈调优），这是一个免训练且模型无关的多模态图像生成框架。CRAFT将用户提示转化为显式的依赖结构化视觉约束集合，通过视觉语言模型验证生成图像，仅在特定约束被违反时执行定向提示更新。该迭代过程包含显式终止准则，形成可解释、可控的推理时优化循环。在多种模型架构和挑战性基准测试中，CRAFT持续提升组合准确性、文本渲染能力和偏好评估表现，对轻量级生成器的提升尤为显著。重要的是，这些改进仅带来可忽略的推理时间开销，使小型或经济型模型能接近昂贵系统的生成质量。我们的结果表明，显式结构化、约束驱动的推理时推理是提升多模态生成模型可靠性的关键要素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for more interpretable and controllable inference-time reasoning in text-to-image generation, as existing methods often rely on implicit critiques or unconstrained prompt rewrites. The method introduces CRAFT, a training-free framework that transforms a user prompt into explicit, dependency-structured visual constraints, verifies generated images with a vision-language model, and iteratively updates the prompt only when specific constraints are violated, incorporating an explicit stopping criterion. Key experimental findings show that CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations across multiple model families and benchmarks, with particularly strong gains for lightweight generators, while adding negligible inference-time overhead.</div>
<div class="mono" style="margin-top:8px">针对现有文本到图像生成推理时优化方法缺乏可解释性和可控性的问题，本研究提出了CRAFT框架，该免训练方法将用户提示转换为显式的依赖结构视觉约束。该方法利用视觉语言模型迭代验证生成图像，仅在特定约束被违反时进行针对性提示更新，并包含明确的停止准则。在多个模型系列和基准测试上的实验结果表明，该方法能持续提升组合准确性、文本渲染和基于偏好的评估效果，尤其对轻量级生成器提升显著，且仅带来可忽略的推理时间开销。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning</div>
<div class="meta-line">Authors: Shuonan Yang, Yuchen Zhang, Zeyu Fu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T15:52:26+00:00 · Latest: 2026-01-21T15:52:26+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026. \c{opyright} 2026 IEEE. This is the author accepted manuscript. The final published version will be available via IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15115v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15115v1">PDF</a> · <a href="https://github.com/Multimodal-Intelligence-Lab-MIL/MARS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hateful videos pose serious risks by amplifying discrimination, inciting violence, and undermining online safety. Existing training-based hateful video detection methods are constrained by limited training data and lack of interpretability, while directly prompting large vision-language models often struggle to deliver reliable hate detection. To address these challenges, this paper introduces MARS, a training-free Multi-stage Adversarial ReaSoning framework that enables reliable and interpretable hateful content detection. MARS begins with the objective description of video content, establishing a neutral foundation for subsequent analysis. Building on this, it develops evidence-based reasoning that supports potential hateful interpretations, while in parallel incorporating counter-evidence reasoning to capture plausible non-hateful perspectives. Finally, these perspectives are synthesized into a conclusive and explainable decision. Extensive evaluation on two real-world datasets shows that MARS achieves up to 10% improvement under certain backbones and settings compared to other training-free approaches and outperforms state-of-the-art training-based methods on one dataset. In addition, MARS produces human-understandable justifications, thereby supporting compliance oversight and enhancing the transparency of content moderation workflows. The code is available at https://github.com/Multimodal-Intelligence-Lab-MIL/MARS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多阶段对抗推理的无训练可解释仇恨视频检测</div>
<div class="mono" style="margin-top:8px">仇恨视频通过放大歧视、煽动暴力和破坏在线安全构成严重风险。现有基于训练的仇恨视频检测方法受限于有限的训练数据和可解释性不足，而直接提示大型视觉语言模型往往难以实现可靠的仇恨检测。为应对这些挑战，本文提出MARS——一种无需训练的多阶段对抗推理框架，能够实现可靠且可解释的仇恨内容检测。MARS首先对视频内容进行客观描述，为后续分析建立中立基础。在此基础上，它构建支持潜在仇恨解读的证据推理，同时并行整合反证据推理以捕捉合理的非仇恨视角。最后，将这些视角综合成具有解释性的结论性判断。在两个真实数据集上的广泛评估表明，在某些骨干网络和设置下，MARS相比其他无训练方法提升达10%，并在一个数据集上优于最先进的基于训练的方法。此外，MARS生成人类可理解的判定依据，从而支持合规审查并增强内容审核工作流程的透明度。代码发布于https://github.com/Multimodal-Intelligence-Lab-MIL/MARS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hateful videos amplify discrimination and incite violence, but existing detection methods are limited by scarce training data and lack interpretability, while direct prompting of large vision-language models proves unreliable. To address this, the authors propose MARS, a training-free Multi-stage Adversarial Reasoning framework that first generates a neutral objective description of video content, then develops parallel reasoning chains for both hateful and non-hateful interpretations based on evidence, and finally synthesizes these perspectives into a conclusive decision. Experimental results on two real-world datasets show that MARS achieves up to a 10% improvement over other training-free methods under certain backbones and settings, and even outperforms state-of-the-art training-based methods on one dataset, while also producing human-understandable justifications for its decisions.</div>
<div class="mono" style="margin-top:8px">针对现有基于训练的仇恨视频检测方法受限于训练数据不足且缺乏可解释性，而直接提示大型视觉语言模型又往往不可靠的问题，本文提出了MARS，一种无需训练的多阶段对抗推理框架。该方法首先生成视频内容的客观中立描述，随后并行进行支持仇恨解读的证据推理与支持非仇恨视角的反证据推理，最终综合得出可解释的判定。在两个真实数据集上的广泛评估表明，在某些骨干模型和设置下，MARS相比其他无需训练的方法取得了最高10%的性能提升，并在一个数据集上超越了最先进的基于训练的方法，同时能为判定生成人类可理解的解释依据。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Multi-Dataset Training for TBPS</div>
<div class="meta-line">Authors: Nilanjana Chatterjee, Sidharatha Garg, A V Subramanyam, Brejesh Lall</div>
<div class="meta-line">First: 2026-01-21T13:26:28+00:00 · Latest: 2026-01-21T13:26:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14978v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向TBPS的统一多数据集训练</div>
<div class="mono" style="margin-top:8px">基于文本的行人检索（TBPS）在视觉-语言模型（VLMs）的推动下取得了显著进展，但仍受限于训练数据不足以及VLMs本身并非针对行人中心识别进行预训练。现有TBPS方法依赖以数据集为中心的微调来处理分布偏移，导致针对不同数据集需训练多个独立模型。虽然合成数据可扩大微调VLMs所需的规模，但无法消除数据集特定的适应性需求。这引出一个根本性问题：能否跨多个数据集训练一个统一的TBPS模型？我们发现，对所有数据集进行简单联合训练仍非最优解，因为当前训练范式难以扩展到大量独立行人身份，且易受噪声图文对影响。为此，我们提出Scale-TBPS方法，包含两项贡献：（i）噪声感知的统一数据集构建策略，可融合多样化的TBPS数据集；（ii）可扩展的判别性身份学习框架，在大量独立身份下仍保持高效。在CUHK-PEDES、ICFG-PEDES、RSTPReid、IIITD-20K和UFine6926数据集上的大量实验表明，单一Scale-TBPS模型性能优于以数据集为中心的优化模型及简单联合训练方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of Text-Based Person Search (TBPS), which relies on vision-language models that are not pre-trained for pedestrian recognition and require dataset-specific fine-tuning, leading to multiple independent models. To enable a single unified model across datasets, the method introduces Scale-TBPS, featuring a noise-aware unified dataset curation strategy to merge diverse datasets cohesively and a scalable discriminative identity learning framework to handle many unique identities effectively. Experimental results on five datasets show that the unified Scale-TBPS model outperforms both dataset-centric optimized models and naive joint training approaches.</div>
<div class="mono" style="margin-top:8px">本研究基于文本行人检索（TBPS）的局限性展开，该领域依赖的视觉语言模型并非针对行人识别预训练，且需要针对不同数据集进行微调，导致多个独立模型并存。为实现跨数据集的统一模型，方法提出了Scale-TBPS，包括一个噪声感知的统一数据集整合策略，以协调合并多样化数据集，以及一个可扩展的判别性身份学习框架，以有效处理大量独特身份。在CUHK-PEDES、ICFG-PEDES等五个数据集上的实验结果表明，Scale-TBPS模型优于数据集中心化优化模型和简单的联合训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis</div>
<div class="meta-line">Authors: Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis</div>
<div class="meta-line">First: 2025-02-13T18:52:14+00:00 · Latest: 2026-01-21T12:51:46+00:00</div>
<div class="meta-line">Comments: 26 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.09598v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.09598v2">PDF</a> · <a href="https://github.com/Orion-AI-Lab/GAIA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 201,005 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA&#x27;s construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks. We make our dataset, automated processing framework and fine-tuned model weights publicly available on our project&#x27;s GitHub repository: https://github.com/Orion-AI-Lab/GAIA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAIA：面向遥感图像分析的全球多模态多尺度视觉语言数据集</div>
<div class="mono" style="margin-top:8px">现有视觉语言模型主要基于网络爬取的噪声图像-文本数据训练，对遥感专业领域接触有限，导致在遥感特定任务上表现不佳。常用数据集常缺乏科学准确的详细文本描述，仅强调日期、位置等属性。为填补这一关键空白，我们提出GAIA——一个专为多尺度、多传感器、多模态遥感图像分析设计的新数据集。GAIA包含201,005个精心筛选的遥感图像-文本对，涵盖不同空间分辨率的多种遥感模态。与现有遥感视觉语言数据集不同，GAIA重点关注多样化的遥感应用场景，提供环境变化、自然灾害等动态现象的独特信息。数据集具有时空平衡分布，覆盖全球范围及过去25年，观测时间分布均衡。GAIA构建采用两阶段流程：(1)从权威遥感来源定向爬取图像及配套文本；(2)通过精心设计的提示词调用GPT-4o的先进视觉语言能力，为每幅图像生成五个高质量科学合成描述。基于CLIP和BLIP2模型的微调实验表明，GAIA显著提升了遥感图像分类、跨模态检索与图像描述任务的性能。数据集、自动化处理框架及微调模型权重已公开于项目GitHub仓库：https://github.com/Orion-AI-Lab/GAIA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limited performance of general vision-language models on remote sensing tasks due to noisy web data and insufficient domain-specific captions, this paper introduces GAIA, a multi-scale, multi-sensor, multi-modal dataset. The dataset was constructed through targeted web-scraping from reputable sources followed by generating five high-quality synthetic captions per image using GPT-4o, resulting in 201,005 curated image-text pairs with global coverage over 25 years. Experimental fine-tuning of CLIP and BLIP2 models demonstrated that GAIA significantly enhances performance on remote sensing image classification, cross-modal retrieval, and image captioning tasks.</div>
<div class="mono" style="margin-top:8px">针对通用视觉语言模型因网络数据噪声大和领域特定描述不足而在遥感任务上性能有限的问题，本文提出了GAIA，一个包含201,005个精心策划的遥感图像-文本对的多尺度、多模态数据集。该数据集通过从权威来源进行针对性网络爬取，并利用GPT-4o结合科学设计的提示为每张图像生成五个高质量合成描述而构建，确保了全球空间覆盖和25年的时间跨度。对CLIP和BLIP2模型的实验性微调表明，GAIA显著提升了遥感图像分类、跨模态检索和图像描述任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis</div>
<div class="meta-line">Authors: Keita Takeda, Tomoya Sakai</div>
<div class="meta-line">Venue: ISBI short</div>
<div class="meta-line">First: 2026-01-21T08:53:40+00:00 · Latest: 2026-01-21T08:53:40+00:00</div>
<div class="meta-line">Comments: A short version paper of this research has been accepted for The IEEE International Symposium on Biomedical Imaging (ISBI) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14774v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14774v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates the feature representations produced by publicly available open source medical vision-language models (VLMs). While medical VLMs are expected to capture diagnostically relevant features, their learned representations remain underexplored, and standard evaluations like classification accuracy do not fully reveal if they acquire truly discriminative, lesion-specific features. Understanding these representations is crucial for revealing medical image structures and improving downstream tasks in medical image analysis. This study aims to investigate the feature distributions learned by medical VLMs and evaluate the impact of medical specialization. We analyze the feature distribution of multiple image modalities extracted by some representative medical VLMs across lesion classification datasets on multiple modalities. These distributions were compared them with non-medical VLMs to assess the domain-specific medical training. Our experiments showed that medical VLMs can extract discriminative features that are effective for medical classification tasks. Moreover, it was found that non-medical VLMs with recent improvement with contextual enrichment such as LLM2CLIP produce more refined feature representations. Our results imply that enhancing text encoder is more crucial than training intensively on medical images when developing medical VLMs. Notably, non-medical models are particularly vulnerable to biases introduced by overlaied text strings on images. These findings underscore the need for careful consideration on model selection according to downstream tasks besides potential risks in inference due to background biases such as textual information in images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>医学视觉语言模型的专业化是否增强判别力？：基于特征分布分析的全面探究</div>
<div class="mono" style="margin-top:8px">本研究探讨了公开开源医学视觉语言模型（VLMs）生成的特征表示。尽管医学VLMs预期能捕捉诊断相关特征，但其学习到的表征尚未被充分探索，且分类准确性等标准评估无法完全揭示它们是否真正获得了具有判别性的病灶特异性特征。理解这些表征对于揭示医学图像结构及改进医学图像分析的下游任务至关重要。本研究旨在探究医学VLMs学习到的特征分布，并评估医学专业化的影响。我们分析了多个代表性医学VLMs在多模态病灶分类数据集上提取的多模态图像特征分布，并与非医学VLMs进行比较，以评估领域特异性医学训练的效果。实验表明，医学VLMs能够提取对医学分类任务有效的判别性特征。此外，研究发现，经过上下文增强（如LLM2CLIP）改进的非医学VLMs能生成更精细的特征表示。结果表明，在开发医学VLMs时，增强文本编码器比密集训练医学图像更为关键。值得注意的是，非医学模型尤其容易受到图像上叠加文本字符串引入的偏差影响。这些发现强调，除了图像中文本信息等背景偏差可能带来的推理风险外，还需根据下游任务谨慎选择模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether medical specialization in vision-language models (VLMs) enhances their ability to learn discriminative features for medical image analysis, as standard evaluations like classification accuracy may not fully reveal the acquisition of lesion-specific features. The method involves analyzing and comparing the feature distributions extracted by representative medical and non-medical VLMs across multiple image modalities and lesion classification datasets to assess the impact of domain-specific training. Key experimental findings show that while medical VLMs can extract effective discriminative features for classification, recent non-medical VLMs with enhanced text encoders, such as LLM2CLIP, produce more refined feature representations, suggesting that improving the text encoder is more crucial than intensive medical image training; additionally, non-medical models are particularly vulnerable to biases from overlaid text on images, highlighting risks in model selection and inference.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究医学视觉语言模型（VLMs）的专业化是否能增强其从医学图像中学习具有区分度的病灶特异性特征的能力，因为标准的准确率指标可能无法完全揭示所学表征的质量。方法是通过特征分布分析，在多种模态的病灶分类数据集上比较代表性医学VLMs与非医学VLMs（包括LLM2CLIP等先进模型），以评估领域特定训练的影响。主要实验结果表明，虽然医学VLMs能提取对分类有效的特征，但近期具有增强上下文文本编码器的非医学VLMs能产生更精细的表征，这表明在开发医学VLMs时，增强文本编码器比密集的医学图像训练更为关键。此外，非医学模型更容易受到图像上叠加文本引入的偏差影响，强调了需要根据下游任务谨慎选择模型，并注意背景偏差的风险。</div>
</details>
</div>
<div class="card">
<div class="title">A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection</div>
<div class="meta-line">Authors: Guiying Zhu, Bowen Yang, Yin Zhuang, Tong Zhang, Guanqun Wang, Zhihao Che, He Chen, Lianlin Li</div>
<div class="meta-line">First: 2026-01-17T05:14:42+00:00 · Latest: 2026-01-21T08:41:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11910v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11910v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of &quot;guess what&quot;. Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种免训练的猜图视觉语言模型：从片段到开放词汇目标检测</div>
<div class="mono" style="margin-top:8px">开放词汇目标检测旨在发展检测任意对象的能力。尽管大规模预训练已构建出展现出色零样本能力的通用基础模型以促进OVOD，但根据已有预训练基础模型建立对任意对象认知的通用理解常被忽视。为此，本文提出一种免训练的猜图视觉语言模型GW-VLM，通过精心设计的多尺度视觉语言搜索与上下文概念提示构建通用理解范式。该方法使预训练视觉语言模型与大型语言模型参与“猜图”游戏：MS-VLS利用多尺度视觉语言软对齐从类别无关检测结果生成片段，CCP则参照MS-VLS形成概念流以帮助LLM理解OVOD片段。最终在自然与遥感数据集上的实验表明，GW-VLM无需训练即可超越现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the overlooked need for universal object understanding in Open-Vocabulary Object Detection (OVOD) by proposing GW-VLM, a training-free method that leverages pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) in a &quot;guess what&quot; framework. It employs Multi-Scale Visual Language Searching (MS-VLS) to generate object snippets from class-agnostic detections and Contextual Concept Prompt (CCP) to enable LLMs to interpret these snippets for detection. Experimental results on datasets like COCO val, Pascal VOC, DIOR, and NWPU-10 demonstrate that GW-VLM achieves state-of-the-art OVOD performance without requiring any training.</div>
<div class="mono" style="margin-top:8px">该研究针对现有基础模型在开放词汇目标检测中普遍物体理解能力不足的问题，提出了一种无需训练的GW-VLM方法。该方法通过多尺度视觉语言搜索从类别无关检测中生成片段，并结合上下文概念提示让大语言模型理解这些片段，使预训练的视觉语言模型和大语言模型协同完成“猜物体”任务。在COCO、Pascal VOC、DIOR和NWPU-10数据集上的实验表明，GW-VLM无需训练即可实现最先进的开放词汇目标检测性能。</div>
</details>
</div>
<div class="card">
<div class="title">DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling</div>
<div class="meta-line">Authors: Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Nga-Chun Ng, Gwing Kei Yip, Gerald W. Y. Cheng, Yunlin Mao, Jing Cai, Liang-ting Lin, Jung Sun Yoo</div>
<div class="meta-line">First: 2026-01-21T07:41:59+00:00 · Latest: 2026-01-21T07:41:59+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14732v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14732v1">PDF</a> · <a href="https://github.com/1anj/DeepMoLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI models for drug discovery and chemical literature mining must interpret molecular images and generate outputs consistent with 3D geometry and stereochemistry. Most molecular language models rely on strings or graphs, while vision-language models often miss stereochemical details and struggle to map continuous 3D structures into discrete tokens. We propose DeepMoLM: Deep Molecular Language M odeling, a dual-view framework that grounds high-resolution molecular images in geometric invariants derived from molecular conformations. DeepMoLM preserves high-frequency evidence from 1024 $\times$ 1024 inputs, encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints, and fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates. DeepMoLM improves PubChem captioning with a 12.3% relative METEOR gain over the strongest generalist baseline while staying competitive with specialist methods. It produces valid numeric outputs for all property queries and attains MAE 13.64 g/mol on Molecular Weight and 37.89 on Complexity in the specialist setting. On ChEBI-20 description generation from images, it exceeds generalist baselines and matches state-of-the-art vision-language models. Code is available at https://github.com/1anj/DeepMoLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepMoLM：利用视觉与几何结构信息进行分子-文本建模</div>
<div class="mono" style="margin-top:8px">用于药物发现和化学文献挖掘的AI模型必须能解读分子图像，并生成符合三维几何与立体化学的输出。现有分子语言模型多依赖字符串或图结构，而视觉语言模型常忽略立体化学细节，且难以将连续三维结构映射为离散标记。我们提出DeepMoLM（深度分子语言建模）——一种双视角框架，将高分辨率分子图像锚定于分子构象衍生的几何不变量中。该模型保留1024×1024输入的高频细节，将构象邻域编码为离散的扩展三维指纹，并通过交叉注意力融合视觉与几何流，实现无需原子坐标的物理解释性生成。在PubChem描述生成任务中，DeepMoLM相比最强通用基线获得12.3%的相对METEOR提升，同时保持与专业方法的竞争力。其对所有性质查询均能生成有效数值输出，在专业设定下分子量预测MAE为13.64 g/mol，复杂度预测MAE为37.89。在ChEBI-20图像描述生成任务中，其表现超越通用基线并与前沿视觉语言模型相当。代码发布于https://github.com/1anj/DeepMoLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for AI models in drug discovery and chemical literature mining to accurately interpret molecular images while preserving 3D geometric and stereochemical information, as existing methods often rely on simplified representations that miss these critical details. The proposed DeepMoLM framework integrates high-resolution molecular images with geometric invariants from molecular conformations, encoding conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints and fusing visual and geometric streams via cross-attention to enable physically grounded generation without explicit atom coordinates. Experimental results show that DeepMoLM achieves a 12.3% relative improvement in METEOR score over the strongest generalist baseline on PubChem captioning, produces valid numeric outputs for all property queries with a MAE of 13.64 g/mol on Molecular Weight, and matches state-of-the-art vision-language models on ChEBI-20 description generation from images.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决药物发现和化学文献挖掘中AI模型需要解释分子图像并符合三维几何与立体化学约束的问题，因为现有方法通常依赖简化表示而忽略这些细节。提出的DeepMoLM框架将高分辨率分子图像与分子构象的几何不变量相结合，将构象邻域编码为离散的扩展三维指纹，并通过交叉注意力融合视觉和几何信息，从而在不依赖原子坐标的情况下实现基于物理的文本生成。实验结果表明，在PubChem描述生成任务中，DeepMoLM相比最强通用基线在METEOR指标上取得了12.3%的相对提升，对所有属性查询均能生成有效数值输出（分子量MAE为13.64 g/mol，复杂度MAE为37.89），并在ChEBI-20图像描述生成任务上达到了与最先进视觉语言模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</div>
<div class="meta-line">Authors: Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang</div>
<div class="meta-line">First: 2025-12-19T01:39:43+00:00 · Latest: 2026-01-21T07:00:03+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17160v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17160v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)&quot; framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合成图像能否作为有效且高效的类别原型？</div>
<div class="mono" style="margin-top:8px">视觉-语言模型在零样本图像分类任务中表现出色，但现有方法（包括对比语言-图像预训练）均依赖标注的文本-图像对来实现视觉与文本模态的对齐。这种依赖性在准备高质量数据集时带来了高昂成本和精度要求。同时，双模态数据处理通常需要双塔编码器，也限制了模型的轻量化。为突破这些局限，我们提出了“基于大语言模型生成的对比语言-图像预训练”框架。该框架利用大语言模型生成类别特定的提示词，引导扩散模型合成参考图像。这些生成图像作为视觉原型，通过提取真实图像的视觉特征并与原型特征进行比对来实现分类预测。通过大语言模型优化提示生成并仅使用视觉编码器，该框架保持了轻量化与高效性。关键优势在于整个实验过程仅需类别标签作为输入，无需人工标注的图文对和额外预处理。实验结果验证了该框架的可行性与高效性，在零样本分类任务中表现优异，为分类任务建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the high cost and accuracy requirements of preparing annotated text-image pairs for Vision-Language Models (VLMs) in zero-shot classification, along with the computational burden of dual-tower encoders. The proposed method, LGCLIP, uses a Large Language Model to generate class-specific prompts for a diffusion model to synthesize visual prototypes, then compares features of real images to these prototypes using only a visual encoder for prediction. Experimental results demonstrate that this lightweight framework, requiring only class labels as input, achieves strong performance in zero-shot classification tasks.</div>
<div class="mono" style="margin-top:8px">现有视觉语言模型（如CLIP）依赖标注的图文对进行零样本分类，成本高昂且通常需要双塔编码器，限制了其轻量化。为此，本研究提出LGCLIP框架，利用大语言模型生成类别特定的提示词，引导扩散模型合成参考图像作为视觉原型，然后仅使用视觉编码器提取真实图像特征并与原型特征比较以实现分类。实验结果表明，LGCLIP在零样本分类任务中表现优异，且框架轻量高效，整个流程仅需类别标签，无需人工标注的图文对。</div>
</details>
</div>
<div class="card">
<div class="title">AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving</div>
<div class="meta-line">Authors: Zecong Tang, Zixu Wang, Yifei Wang, Weitong Lian, Tianjian Gao, Haoran Li, Tengju Ru, Lingyi Meng, Zhejun Cui, Yichen Zhu, Qi Kang, Kaixuan Wang, Yu Zhang</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2026-01-21T06:29:09+00:00 · Latest: 2026-01-21T06:29:09+00:00</div>
<div class="meta-line">Comments: 23 pages. Submitted to ACL ARR 2026 January</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14702v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14702v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models&#x27; reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoDriDM：自动驾驶中视觉语言模型决策能力的可解释性基准</div>
<div class="mono" style="margin-top:8px">自动驾驶是极具挑战性的领域，需要在复杂场景中实现可靠感知与安全决策。近期视觉语言模型展现出推理与泛化能力，为自动驾驶开辟了新可能；然而现有基准与指标过度强调感知能力，未能充分评估决策过程。本研究提出AutoDriDM——一个以决策为核心、包含6,650道问题的渐进式基准，涵盖物体、场景与决策三个维度。我们评估主流视觉语言模型以界定自动驾驶中感知至决策的能力边界，相关性分析显示感知与决策性能存在弱关联性。我们进一步对模型推理过程进行可解释性分析，识别出逻辑推理错误等关键失效模式，并引入分析器模型实现大规模自动标注。AutoDriDM弥合了以感知为中心和以决策为中心的评估体系间的鸿沟，为构建更安全可靠的现实世界自动驾驶视觉语言模型提供指引。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap in current autonomous driving benchmarks, which focus excessively on perception while neglecting decision-making evaluation, despite vision-language models (VLMs) showing potential for reasoning in this domain. The authors introduce AutoDriDM, a decision-centric benchmark with 6,650 questions across Object, Scene, and Decision dimensions, to systematically assess VLMs&#x27; perception-to-decision capabilities. Experimental evaluation of mainstream VLMs reveals a weak correlation between perception and decision-making performance, and explainability analyses identify key failure modes like logical reasoning errors, with an analyzer model developed to automate large-scale annotation for such assessments.</div>
<div class="mono" style="margin-top:8px">本研究针对当前自动驾驶基准测试的不足，即过于强调感知能力而未能充分评估决策过程，尽管视觉语言模型在复杂驾驶场景中具有提升推理能力的潜力。作者提出了AutoDriDM，一个以决策为中心的基准测试，包含对象、场景和决策三个维度的6650个问题，用于系统评估视觉语言模型从感知到决策的能力。实验评估显示感知与决策性能之间相关性较弱，可解释性分析识别出逻辑推理错误等关键失败模式，并引入了一个分析器模型来自动化大规模标注，以改进评估效果。</div>
</details>
</div>
<div class="card">
<div class="title">T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs</div>
<div class="meta-line">Authors: Shao-Jun Xia, Huixin Zhang, Zhengzhong Tu</div>
<div class="meta-line">First: 2025-11-20T07:02:06+00:00 · Latest: 2026-01-21T06:18:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16107v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16107v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across twelve cross-task scenarios and second-tier performance in nine additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T2T-VICL：通过隐式文本驱动视觉语言模型突破跨任务视觉上下文学习的边界</div>
<div class="mono" style="margin-top:8px">在大型语言模型（LLM）中，上下文学习（ICL）指通过输入上下文中提供的少量示例来执行新任务。视觉上下文学习（VICL）的最新进展表明，统一视觉语言模型（VLM）在解决下游任务方面展现出巨大潜力。当视觉提示与目标图像源自不同视觉任务时，VLM是否仍能实现VICL？本文提出一个全协作流程T2T-VICL，用于探索VLM在跨任务VICL中的潜力。核心机制是设计生成并筛选能最有效隐式描述两种低层视觉任务差异的文本提示，并构建首个跨任务VICL数据集。在此基础上，提出一种融合感知评分推理与传统评估指标的新型推断框架，以执行跨任务VICL。该方法在十二个跨任务场景中取得顶尖性能，在另外九个场景中位列第二梯队，突破了VLM在跨任务VICL中的能力边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses whether vision-language models can perform visual in-context learning when demonstration and target images come from different low-level vision tasks. The authors propose T2T-VICL, a pipeline that generates and selects text prompts that implicitly describe task differences, constructs a cross-task dataset, and introduces an inference framework combining perceptual scoring with traditional metrics. Their method achieves top-tier results in twelve cross-task scenarios and second-tier performance in nine others, demonstrating expanded cross-task VICL capabilities.</div>
<div class="mono" style="margin-top:8px">本研究探讨了当演示图像和目标图像来自不同低级视觉任务时，视觉语言模型是否仍能进行视觉上下文学习。作者提出了T2T-VICL流程，通过生成和选择文本提示来隐式描述任务差异，构建了首个跨任务数据集，并引入了一个结合感知评分推理与传统指标的推断框架。该方法在十二个跨任务场景中取得顶级性能，在另外九个场景中取得次优性能，展现了跨任务视觉上下文学习的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks</div>
<div class="meta-line">Authors: Fei Kong</div>
<div class="meta-line">First: 2025-07-27T08:31:24+00:00 · Latest: 2026-01-21T05:06:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20174v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.20174v2">PDF</a> · <a href="https://github.com/kong13661/LRR-Bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world applications, such as autonomous driving and humanoid robot manipulation, require precise spatial perception. However, it remains underexplored how Vision-Language Models (VLMs) recognize spatial relationships and perceive spatial movement. In this work, we introduce a spatial evaluation pipeline and construct a corresponding benchmark. Specifically, we categorize spatial understanding into two main types: absolute spatial understanding, which involves querying the absolute spatial position (e.g., left, right) of an object within an image, and 3D spatial understanding, which includes movement and rotation. Notably, our dataset is entirely synthetic, enabling the generation of test samples at a low cost while also preventing dataset contamination. We conduct experiments on multiple state-of-the-art VLMs and observe that there is significant room for improvement in their spatial understanding abilities. Explicitly, in our experiments, humans achieve near-perfect performance on all tasks, whereas current VLMs attain human-level performance only on the two simplest tasks. For the remaining tasks, the performance of VLMs is distinctly lower than that of humans. In fact, the best-performing Vision-Language Models even achieve near-zero scores on multiple tasks. The dataset and code are available on https://github.com/kong13661/LRR-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LRR-Bench：左、右还是旋转？视觉语言模型在空间理解任务中仍面临挑战</div>
<div class="mono" style="margin-top:8px">自动驾驶和人形机器人操控等实际应用需要精确的空间感知能力。然而，视觉语言模型如何识别空间关系与感知空间运动仍缺乏深入探索。本研究提出一种空间评估流程并构建了相应基准。具体而言，我们将空间理解分为两类：绝对空间理解（涉及查询图像中物体的绝对空间位置，如左、右）和三维空间理解（包含移动与旋转）。值得注意的是，本数据集完全通过合成生成，能以低成本创建测试样本，同时避免数据污染。我们在多个前沿视觉语言模型上开展实验，发现其空间理解能力仍有显著提升空间。实验数据显示，人类在所有任务中均接近完美表现，而当前视觉语言模型仅在两项最简单任务上达到人类水平。对于其余任务，模型表现明显低于人类。事实上，表现最佳的视觉语言模型在多项任务中甚至接近零分。数据集与代码已发布于 https://github.com/kong13661/LRR-Bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for precise spatial perception in applications like autonomous driving and robot manipulation, this study investigates the spatial understanding capabilities of Vision-Language Models (VLMs). The method involves creating a synthetic benchmark, LRR-Bench, which categorizes spatial understanding into absolute position recognition and 3D movement/rotation tasks, enabling low-cost, contamination-free evaluation. Experimental results reveal a significant gap: while humans achieve near-perfect performance, current VLMs only match human levels on the simplest tasks and show near-zero scores on several more complex spatial reasoning challenges, indicating substantial room for improvement.</div>
<div class="mono" style="margin-top:8px">受自动驾驶和机器人操控等应用对精确空间感知需求的驱动，本研究探究了视觉语言模型的空间理解能力。方法上，研究引入了一个合成基准测试LRR-Bench，将空间理解分为绝对位置识别和三维运动/旋转任务，实现了低成本且无数据污染的评估。关键实验结果表明存在显著的性能差距：人类在各项任务上接近完美表现，而当前视觉语言模型仅在最简单任务上达到人类水平，在多个更复杂的空间推理挑战中准确率甚至接近零。</div>
</details>
</div>
<div class="card">
<div class="title">Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis</div>
<div class="meta-line">Authors: James Brock, Ce Zhang, Nantheera Anantrasirichai</div>
<div class="meta-line">First: 2026-01-21T04:23:33+00:00 · Latest: 2026-01-21T04:23:33+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures, 7 tables, Submitted to Ecological Informatics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14637v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14637v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Forest-Chat：面向交互式森林变化分析的视觉-语言智能体适配</div>
<div class="mono" style="margin-top:8px">高分辨率卫星影像的日益普及与深度学习的进步，为增强森林监测工作流程创造了新机遇。该领域的两大核心挑战是像素级变化检测与语义变化解译，尤其在复杂森林动态场景中。虽然大语言模型（LLMs）在数据探索中的应用日益广泛，但其与视觉-语言模型（VLMs）在遥感影像变化解译（RSICI）中的融合仍待深入探索，特别是在非城市环境领域。本文提出Forest-Chat——一个面向集成式森林变化分析的LLM驱动智能体。该框架支持自然语言查询，涵盖多种RSICI任务，包括变化检测、变化描述、目标计数、森林砍伐比例估算及变化归因分析。Forest-Chat基于多层变化解译（MCI）视觉-语言主干架构，采用LLM进行任务调度，集成基于基础变化检测模型的零样本检测能力与交互式点提示界面，以支持细粒度用户引导。为促进森林环境中的适配与评估，我们构建了Forest-Change数据集，包含双时相卫星影像、像素级变化掩码，以及通过人工标注与规则方法生成的多粒度语义变化描述。实验结果表明，Forest-Chat在Forest-Change数据集及以树木为核心的LEVIR-MCI-Trees子集上，对联合变化检测与描述任务均表现出优异性能，彰显了交互式LLM驱动RSICI系统在提升森林变化分析的可访问性、可解释性与分析效率方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for more accessible and interpretable tools to analyze complex forest dynamics from satellite imagery, moving beyond urban-focused remote sensing image change interpretation (RSICI) systems. The method introduces Forest-Chat, an LLM-driven agent that integrates a multi-level change interpretation vision-language backbone with LLM orchestration, a zero-shot change detection foundation model, and an interactive point-prompt interface for user guidance, enabling natural language querying for tasks like change detection, captioning, and reasoning. Key experimental findings show that Forest-Chat achieves strong performance on the newly introduced Forest-Change dataset and the tree-focused LEVIR-MCI-Trees subset, demonstrating its effectiveness in joint change detection and captioning for forest environments.</div>
<div class="mono" style="margin-top:8px">该研究针对森林监测中像素级变化检测和复杂森林动态语义解释的需求，旨在拓展遥感图像变化解释在非城市环境中的应用。方法上提出了Forest-Chat，这是一个基于大语言模型的智能体，它整合了多级变化解释的视觉-语言骨干网络与大语言模型编排，通过基础变化检测模型实现零样本检测，并配备交互式点提示界面以支持细粒度用户引导，从而支持自然语言查询进行变化检测、描述和推理等任务。在新建的Forest-Change数据集和LEVIR-MCI-Trees子集上的实验结果表明，该系统在联合变化检测和描述任务中表现优异，凸显了其在提升森林变化分析的可访问性、可解释性和效率方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Consistent Taxonomic Classification through Hierarchical Reasoning</div>
<div class="meta-line">Authors: Zhenghong Li, Kecheng Zheng, Haibin Ling</div>
<div class="meta-line">First: 2026-01-21T03:00:00+00:00 · Latest: 2026-01-21T03:00:00+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14610v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14610v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) excel at visual understanding, they often fail to grasp hierarchical knowledge. This leads to common errors where VLMs misclassify coarser taxonomic levels even when correctly identifying the most specific level (leaf level). Existing approaches largely overlook this issue by failing to model hierarchical reasoning. To address this gap, we propose VL-Taxon, a two-stage, hierarchy-based reasoning framework designed to improve both leaf-level accuracy and hierarchical consistency in taxonomic classification. The first stage employs a top-down process to enhance leaf-level classification accuracy. The second stage then leverages this accurate leaf-level output to ensure consistency throughout the entire taxonomic hierarchy. Each stage is initially trained with supervised fine-tuning to instill taxonomy knowledge, followed by reinforcement learning to refine the model&#x27;s reasoning and generalization capabilities. Extensive experiments reveal a remarkable result: our VL-Taxon framework, implemented on the Qwen2.5-VL-7B model, outperforms its original 72B counterpart by over 10% in both leaf-level and hierarchical consistency accuracy on average on the iNaturalist-2021 dataset. Notably, this significant gain was achieved by fine-tuning on just a small subset of data, without relying on any examples generated by other VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过层次化推理实现一致性的分类学分类</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言模型在视觉理解方面表现出色，但它们往往难以掌握层次化知识。这导致常见错误：即使正确识别了最具体的层级（叶层级），VLM仍会错误分类更粗粒度的分类学层级。现有方法大多忽视了这一问题，未能对层次化推理进行建模。为填补这一空白，我们提出了VL-Taxon——一个基于层次的两阶段推理框架，旨在提升分类学分类中的叶层级准确性和层次一致性。第一阶段采用自上而下的流程来提升叶层级分类准确率；第二阶段则利用准确的叶层级输出来确保整个分类学层次的一致性。每个阶段首先通过监督微调注入分类学知识，再通过强化学习优化模型的推理与泛化能力。大量实验显示：在iNaturalist-2021数据集上，基于Qwen2.5-VL-7B模型实现的VL-Taxon框架，其叶层级与层次一致性准确率平均超越原版72B模型10%以上。值得注意的是，这一显著提升仅通过对少量数据子集的微调实现，且未依赖任何其他VLM生成的示例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) often lack hierarchical reasoning, leading to inconsistent taxonomic classifications where they correctly identify specific species but misclassify broader categories. To address this, the authors propose VL-Taxon, a two-stage framework that first improves leaf-level classification accuracy via a top-down process and then uses this output to enforce consistency across the entire taxonomic hierarchy. The method is trained with supervised fine-tuning followed by reinforcement learning. Experiments on the iNaturalist-2021 dataset show that VL-Taxon, implemented on the Qwen2.5-VL-7B model, outperforms the original 72B model by over 10% in both leaf-level and hierarchical consistency accuracy, achieving this gain with fine-tuning on only a small data subset.</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）通常缺乏层次知识，导致分类不一致：它们能正确识别具体物种，却错误分类更广泛的类别。为解决此问题，研究者提出了VL-Taxon，一个两阶段框架：第一阶段通过自上而下的过程提升叶节点分类准确率，第二阶段利用该输出确保整个分类层次的一致性。每个阶段均采用监督微调与强化学习进行训练。在iNaturalist-2021数据集上的实验表明，基于Qwen2.5-VL-7B模型实现的VL-Taxon，在叶节点准确率和层次一致性上均比原72B模型平均提升超过10%，且仅使用少量数据进行微调。</div>
</details>
</div>
<div class="card">
<div class="title">3D Space as a Scratchpad for Editable Text-to-Image Generation</div>
<div class="meta-line">Authors: Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen</div>
<div class="meta-line">First: 2026-01-21T02:40:19+00:00 · Latest: 2026-01-21T02:40:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://oindrilasaha.github.io/3DScratchpad/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三维空间作为可编辑文本到图像生成的草稿板</div>
<div class="mono" style="margin-top:8px">近期大语言模型的研究表明，将中间思考过程外化至显式工作空间（如思维链轨迹或工具增强推理）能提升推理能力。然而，视觉语言模型缺乏类似的空间推理机制，限制了其生成准确反映几何关系、物体识别与构图意图的图像。我们提出空间草稿板概念——一种连接语言意图与图像合成的三维推理基底。给定文本提示，本框架解析主体与背景元素，将其实例化为可编辑三维网格，并采用智能场景规划进行布局、朝向与视角选择。生成的三维布局通过保持识别特征的线索渲染回图像域，使视觉语言模型能生成空间一致且视觉连贯的输出。与先前基于二维布局的方法不同，本方法支持直观的三维编辑，并能可靠传递至最终图像。实验表明，该方法在GenAI-Bench上实现文本对齐度32%的提升，证实了显式三维推理对精确可控图像生成的益处。我们的成果为视觉语言模型揭示了新范式：不仅在语言层面，更在空间维度进行推演。代码与可视化资源详见：https://oindrilasaha.github.io/3DScratchpad/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that while large language models benefit from externalizing intermediate reasoning steps, visual language models lack a similar mechanism for spatial reasoning, which limits their ability to generate images with accurate geometric relations and object composition. The method introduces a spatial scratchpad, a 3D reasoning substrate that parses text prompts into subjects and background elements, instantiates them as editable 3D meshes, and uses agentic scene planning for placement, orientation, and viewpoint selection before rendering back to the image domain with identity-preserving cues. Experimental results show a 32% improvement in text alignment on GenAI-Bench, demonstrating that explicit 3D reasoning enables more precise and controllable image generation compared to prior 2D layout-based approaches.</div>
<div class="mono" style="margin-top:8px">该研究的动机是观察到视觉语言模型缺乏类似于大语言模型中思维链的显式空间推理机制，这限制了其生成具有准确几何关系和物体组合的图像的能力。方法引入了空间草稿本，这是一个3D推理框架，将文本提示解析为主题和背景元素，将它们实例化为可编辑的3D网格，并使用智能体场景规划进行放置、方向和视角选择，然后通过身份保留线索渲染回图像域。实验结果表明，在GenAI-Bench上文本对齐度提高了32%，证明与先前的2D布局方法相比，显式3D推理能够实现更空间一致、视觉连贯且可直观编辑的图像生成。</div>
</details>
</div>
<div class="card">
<div class="title">Coding the Visual World: From Image to Simulation Using Vision Language Models</div>
<div class="meta-line">Authors: Sagi Eppel</div>
<div class="meta-line">First: 2026-01-08T19:49:05+00:00 · Latest: 2026-01-20T21:37:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05344v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05344v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) have the ability to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>编码视觉世界：基于视觉语言模型从图像到仿真的实现</div>
<div class="mono" style="margin-top:8px">构建世界的心智模型是理解能力的核心体现。类似地，视觉理解可视为构建图像所描绘系统的表征模型的能力。本研究通过Im2Sim方法，探索视觉语言模型识别并仿真图像中系统与机制的能力。模型接收真实世界系统（如城市、云层、植被）的自然图像，需描述该系统并编写能仿真生成该系统的代码。执行此生成代码后得到合成图像，再与原图进行对比。该方法在多种复杂涌现系统上进行了测试，涵盖物理系统（波浪、光线、云层）至植被、城市、材料及地质构造。通过对模型生成代码与图像的分析，我们检验了其对图像系统的理解程度。结果表明，领先的视觉语言模型（GPT、Gemini）具备跨抽象层级与多领域理解并建模复杂多组件系统的能力，但同时存在对图像精细细节与底层模式排列还原能力有限的问题。这一发现揭示了有趣的不对称性：视觉语言模型兼具高层次深度视觉理解能力与对细节感知的局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether Vision Language Models (VLMs) can achieve a deeper visual understanding by constructing executable mental models of the systems depicted in images. The proposed Im2Sim method prompts a VLM to analyze a natural image, describe the depicted system, and then generate executable code that simulates it; the output of this code is a synthetic image, which is compared to the original for evaluation. Experiments across diverse domains like physical systems, vegetation, and cities show that leading VLMs (GPT, Gemini) can understand and model complex systems at a high, abstract level, but they struggle to replicate fine-grained details and low-level pattern arrangements, revealing an asymmetry in their visual comprehension.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究视觉语言模型是否能够通过构建图像中描绘系统的可执行心智模型来实现更深层的视觉理解。所提出的Im2Sim方法提示VLM分析自然图像、描述所描绘的系统，并生成可执行代码来模拟该系统；该代码的输出是一幅合成图像，用于与原始图像进行比较。在物理系统、植被和城市等多个领域的实验表明，领先的VLM（如GPT、Gemini）能够在高层次、抽象层面理解和建模复杂系统，但它们在复制精细细节和低层模式排列方面存在困难，这揭示了其视觉理解能力的不对称性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
