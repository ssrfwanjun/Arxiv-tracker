<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-29 04:50</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260129_0450</div>
    <div class="row"><div class="card">
<div class="title">Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</div>
<div class="meta-line">Authors: Kevin Robbins, Xiaotong Liu, Yu Wu, Le Sun, Grady McPeak, Abby Stylianou, Robert Pless</div>
<div class="meta-line">First: 2026-01-24T17:30:23+00:00 · Latest: 2026-01-27T18:04:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17535v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17535v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>它会零样本吗？：预测任意查询的零样本分类性能</div>
<div class="mono" style="margin-top:8px">像CLIP这样的视觉-语言模型为文本和图像创建了对齐的嵌入空间，使得任何人都能通过简单命名想要区分的类别来构建视觉分类器。然而，在一个领域表现良好的模型可能在另一个领域失效，非专业用户没有直接的方法来评估他们选择的VLM是否适用于其问题。我们基于先前仅使用文本比较来评估模型在给定自然语言任务中表现的工作，探索了同时生成与该任务相关的合成图像以评估和改进零样本准确率预测的方法。研究表明，与仅使用文本的基线评分相比，生成的图像显著提升了这些预测的质量。此外，它为用户提供了用于评估的图像类型的反馈。在标准CLIP基准数据集上的实验证明，这种基于图像的方法能帮助用户在没有标注样本的情况下，预测VLM是否对其应用有效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge for non-expert users in predicting whether a Vision-Language Model (VLM) like CLIP will perform effectively for their specific zero-shot classification task, as model performance can vary unpredictably across domains. The method improves upon prior text-only evaluation by generating synthetic images relevant to the user&#x27;s query and using these images alongside text comparisons to assess and refine the prediction of zero-shot accuracy. Experimental results on standard CLIP benchmarks show that incorporating generated imagery significantly enhances prediction quality compared to text-only baselines and provides users with interpretable feedback on the image types used for assessment.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决非专家用户面临的挑战：难以评估CLIP等视觉语言模型在特定零样本分类任务中的性能，因为模型效果因领域而异。该方法改进了先前仅使用文本的评估方式，通过生成与用户查询相关的合成图像，并结合文本比较来预测和优化零样本准确率。在标准CLIP基准数据集上的实验结果表明，引入生成图像显著提高了预测质量，优于仅使用文本的基线方法，并为用户提供了评估过程中所使用的图像类型的反馈。</div>
</details>
</div>
<div class="card">
<div class="title">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</div>
<div class="meta-line">Authors: Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T17:58:12+00:00 · Latest: 2026-01-27T17:58:12+00:00</div>
<div class="meta-line">Comments: Accepted in ICLR 2026, Codebase: https://github.com/Nicous20/EgoHandICL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19850v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19850v1">PDF</a> · <a href="https://github.com/Nicous20/EgoHandICL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoHandICL：基于上下文学习的以自我为中心视角三维手部重建</div>
<div class="mono" style="margin-top:8px">在以自我为中心的视觉中，由于深度模糊性、自遮挡及复杂的手-物交互作用，实现鲁棒的三维手部重建具有挑战性。现有方法通过扩充训练数据或添加辅助线索来缓解这些问题，但在未见场景中往往表现不佳。我们提出EgoHandICL，首个面向三维手部重建的上下文学习框架，可在挑战性以自我为中心条件下提升语义对齐性、视觉一致性与鲁棒性。该框架引入基于视觉-语言模型的互补范例检索机制、适配多模态上下文的ICL专用分词器，以及采用掩码自编码器架构并辅以手部几何与感知目标进行训练。在ARCTIC和EgoExo4D数据集上的实验表明，其性能持续超越现有最优方法。我们还展示了实际场景的泛化能力，并通过将重建手部作为视觉提示，提升了EgoVLM的手-物交互推理性能。代码与数据：https://github.com/Nicous20/EgoHandICL</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of robust 3D hand reconstruction in egocentric vision, where depth ambiguity, self-occlusion, and complex hand-object interactions hinder performance. The proposed EgoHandICL framework introduces an in-context learning approach featuring complementary exemplar retrieval guided by vision-language models, a specialized tokenizer for multimodal context, and a masked autoencoder architecture trained with hand-specific geometric and perceptual objectives. Experimental results on ARCTIC and EgoExo4D datasets demonstrate consistent improvements over state-of-the-art methods, with additional validation showing enhanced real-world generalization and improved hand-object interaction reasoning in EgoVLM when using reconstructed hands as visual prompts.</div>
<div class="mono" style="margin-top:8px">由于深度模糊、自遮挡和复杂的手物交互，从第一人称视角进行鲁棒的3D手部重建具有挑战性。针对现有方法在未见场景中表现不佳的局限，本研究提出了EgoHandICL，一个通过上下文学习来提升语义对齐和鲁棒性的框架。该方法利用视觉语言模型进行互补范例检索，采用专门的多模态上下文分词器，并构建了一个基于掩码自编码器的架构，该架构通过手部几何和感知目标进行训练。在ARCTIC和EgoExo4D数据集上的实验表明，其性能持续优于现有先进方法，并且重建的手部模型被证明可以提升视觉语言模型在手物交互推理上的能力。</div>
</details>
</div>
<div class="card">
<div class="title">ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting</div>
<div class="meta-line">Authors: Abhijit Mishra, Mingda Li, Hsiang Fu, Richard Noh, Minji Kim</div>
<div class="meta-line">First: 2025-02-20T18:01:41+00:00 · Latest: 2026-01-27T17:16:10+00:00</div>
<div class="meta-line">Comments: In Proceedings of the IJCNLP-AACL 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14780v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.14780v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (&lt;500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReVision：面向隐私保护任务型视觉指令重写的数据集与基线视觉语言模型</div>
<div class="mono" style="margin-top:8px">随着增强现实、虚拟现实及配备高性能摄像头的现代智能手机成为人机交互的主要界面，高效且保护隐私的多模态交互变得至关重要。现有支持多模态交互的强大视觉语言模型通常依赖云端处理，这引发两大核心问题：(1) 向服务器传输敏感视觉数据带来的隐私风险；(2) 模型在实时设备端部署的能力受限。本文提出视觉指令重写这一创新方法，将多模态指令转化为纯文本命令，使轻量级设备端指令重写视觉语言模型（2.5亿参数）能够与现有对话式AI系统无缝集成，从而增强视觉数据隐私保护。为此，我们构建了涵盖14个领域、超过3.9万条样本的数据集，并开发了紧凑型视觉语言模型——该模型基于图像描述数据集进行预训练，并针对指令重写任务进行微调。实验通过BLEU、METEOR、ROUGE等自然语言生成指标及语义解析分析表明，即使量化后的模型（存储占用&lt;500MB）仍能实现有效的指令重写，为注重隐私的多模态AI应用提供了可行方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses privacy and efficiency concerns in multimodal interactions, where cloud-based large vision-language models (VLMs) risk exposing sensitive visual data and lack real-time on-device usability. The authors propose Visual Instruction Rewriting, a method that converts multimodal instructions into text-only commands using a compact, on-device VLM with 250M parameters, pretrained on image captioning data and fine-tuned for this task. Experiments on a dataset of over 39,000 examples across 14 domains show that even a quantized model (&lt;500MB) achieves effective rewriting as measured by NLG metrics (BLEU, METEOR, ROUGE) and semantic parsing, enabling privacy-preserving multimodal AI applications.</div>
<div class="mono" style="margin-top:8px">本研究针对云端视觉语言模型（VLM）的隐私和延迟问题，提出了视觉指令重写方法，将多模态输入转换为纯文本指令以供设备端处理。该方法构建了一个包含14个领域、超过39,000个样本的数据集，并训练了一个250M参数的紧凑VLM，先在图像描述数据上预训练，再针对指令重写进行微调。实验结果表明，即使量化后的模型（&lt;500MB）也能通过BLEU、METEOR、ROUGE和语义解析评估实现有效的指令重写，从而支持隐私保护的多模态应用。</div>
</details>
</div>
<div class="card">
<div class="title">Youtu-VL: Unleashing Visual Potential via Unified Vision-Language Supervision</div>
<div class="meta-line">Authors: Zhixiang Wei, Yi Li, Zhehan Kan, Xinghua Jiang, Zuwei Long, Shifeng Liu, Hongze Shen, Wei Liu, Xiaoyu Tan, Haojia Lin, Yubo Zhu, Qianyu Li, Di Yin, Haoyu Cao, Weibo Gu, Xin Li, Yinsong Liu, Deqiang Jiang, Xing Sun, Yunsheng Wu, Mingkong Tang, Shuangyin Liu, Lexiang Tang, Haodong Lin, Junru Lu, Jiarui Qin, Lingfeng Qiao, Ruizhi Qiao, Bo Ke, Jianfeng He, Ke Li, Yangning Li, Yunhang Shen, Mengdan Zhang, Peixian Chen, Kun Yin, Bing Liu, Yunfei Wu, Huang Chen, Zhongpeng Cai, Xiaotian Li</div>
<div class="meta-line">First: 2026-01-27T17:01:16+00:00 · Latest: 2026-01-27T17:01:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19798v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19798v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the significant advancements represented by Vision-Language Models (VLMs), current architectures often exhibit limitations in retaining fine-grained visual information, leading to coarse-grained multimodal comprehension. We attribute this deficiency to a suboptimal training paradigm inherent in prevailing VLMs, which exhibits a text-dominant optimization bias by conceptualizing visual signals merely as passive conditional inputs rather than supervisory targets. To mitigate this, we introduce Youtu-VL, a framework leveraging the Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm, which fundamentally shifts the optimization objective from ``vision-as-input&#x27;&#x27; to ``vision-as-target.&#x27;&#x27; By integrating visual tokens directly into the prediction stream, Youtu-VL applies unified autoregressive supervision to both visual details and linguistic content. Furthermore, we extend this paradigm to encompass vision-centric tasks, enabling a standard VLM to perform vision-centric tasks without task-specific additions. Extensive empirical evaluations demonstrate that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, establishing a robust foundation for the development of comprehensive generalist visual agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Youtu-VL：通过统一视觉-语言监督释放视觉潜能</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言模型（VLMs）已取得显著进展，但当前架构在保留细粒度视觉信息方面常显不足，导致多模态理解停留在粗粒度层面。我们将此缺陷归因于主流VLM训练范式的固有局限——其存在文本主导的优化偏差，仅将视觉信号视为被动条件输入而非监督目标。为此，我们提出Youtu-VL框架，采用视觉-语言统一自回归监督（VLUAS）范式，将优化目标从“视觉作为输入”根本性转向“视觉作为目标”。通过将视觉标记直接整合至预测流，Youtu-VL对视觉细节与语言内容实施统一的自回归监督。此外，我们将该范式扩展至视觉中心任务，使标准VLM无需任务特定适配即可执行此类任务。大量实验评估表明，Youtu-VL在通用多模态任务与视觉中心任务上均取得优异性能，为开发全面通用视觉智能体奠定了坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of current Vision-Language Models (VLMs) in retaining fine-grained visual information, which stems from a text-dominant training paradigm that treats visual signals as passive inputs rather than supervisory targets. To overcome this, the authors propose Youtu-VL, a framework based on a Vision-Language Unified Autoregressive Supervision (VLUAS) paradigm that shifts the optimization objective to treat vision as a target, integrating visual tokens directly into the prediction stream for unified autoregressive supervision over both visual and linguistic content. Experimental results show that Youtu-VL achieves competitive performance on both general multimodal tasks and vision-centric tasks, providing a robust foundation for comprehensive generalist visual agents.</div>
<div class="mono" style="margin-top:8px">当前的视觉语言模型由于采用文本主导的训练范式，将视觉信号视为被动输入而非监督目标，常常丢失细粒度的视觉细节。为解决这一问题，本文提出了Youtu-VL框架，其基于视觉语言统一自回归监督范式，将优化目标转变为将视觉作为目标，将视觉标记直接整合到自回归预测流中，对视觉和语言内容进行统一监督。大量实验评估表明，Youtu-VL在通用多模态任务和以视觉为中心的任务上均取得了有竞争力的性能，为开发全面的通用视觉智能体奠定了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">SCoPE VLM: Selective Context Processing for Efficient Document Navigation in Vision-Language Models</div>
<div class="meta-line">Authors: Gyubeum Lim, Yemo Koo, Vijay Krishna Madisetti</div>
<div class="meta-line">First: 2025-10-22T17:47:12+00:00 · Latest: 2026-01-27T16:39:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21850v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21850v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding long-context visual information remains a fundamental challenge for vision-language models, particularly in agentic tasks such as GUI control and web navigation. While web pages and GUI environments are inherently structured documents, current VLMs typically neglect decision-oriented document understanding in their training objectives. Existing approaches primarily extend visual embeddings to process long, high-resolution inputs, but these methods are memory-intensive and impractical for locally deployable solutions. To address these issues, we propose SCoPE VLM, a document navigation expert that leverages a novel Chain of Scroll mechanism to selectively and recursively navigate documents, focusing exclusively on relevant segments. We introduce a dedicated data generation pipeline to construct informative Chain of Scroll trajectories and Episodic Group Relative Policy Optimization, a tailored reinforcement learning method to bridge the gap between training and inference. Our method substantially reduces memory usage and effectively models human-like reading behaviors. To the best of our knowledge, SCoPE VLM is the first framework to explicitly model agentic reading patterns in multi-page document question answering, advancing the capabilities of multimodal agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCoPE VLM：基于选择性上下文处理的高效文档导航视觉语言模型</div>
<div class="mono" style="margin-top:8px">理解长上下文视觉信息仍是视觉语言模型面临的核心挑战，尤其在GUI控制和网页导航等代理任务中。虽然网页和GUI环境本质上是结构化文档，但现有VLM的训练目标通常忽略了面向决策的文档理解。当前方法主要通过扩展视觉嵌入来处理长序列高分辨率输入，但这些方案内存消耗大，难以在本地部署。为此，我们提出SCoPE VLM——一种采用新型滚动链机制的选择性递归文档导航专家模型，能精准聚焦相关片段。我们构建了专门的数据生成流程来构建信息化的滚动链轨迹，并提出情景化组相对策略优化这一定制强化学习方法，以弥合训练与推理的差距。该方法显著降低了内存占用，并有效模拟了类人阅读行为。据我们所知，SCoPE VLM是首个在多页文档问答中显式建模代理阅读模式的框架，推动了多模态智能体的能力边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling vision-language models to efficiently process long-context visual documents for agentic tasks like web navigation, where current methods are memory-intensive and lack training for decision-oriented understanding. The proposed SCoPE VLM introduces a Chain of Scroll mechanism for selective, recursive navigation of document segments and employs a dedicated data generation pipeline along with Episodic Group Relative Policy Optimization for reinforcement learning to align training with inference. Experimental results show the method substantially reduces memory usage, effectively models human-like reading behaviors, and advances multimodal agent capabilities in multi-page document question answering.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决视觉语言模型在处理长上下文视觉文档（如GUI控制和网页导航等代理任务）时效率低下的挑战，现有方法通常内存消耗大且缺乏面向决策的训练。提出的SCoPE VLM采用了一种新颖的链式滚动机制，选择性地递归导航文档片段，并通过专门的数据生成管道构建训练轨迹，以及一种定制的强化学习方法——情景组相对策略优化，以弥合训练与推理之间的差距。实验结果表明，该方法显著降低了内存使用，有效模拟了类人阅读行为，并在多页面文档问答中提升了性能，是首个明确建模此类代理阅读模式的框架。</div>
</details>
</div>
<div class="card">
<div class="title">KeepLoRA: Continual Learning with Residual Gradient Adaptation</div>
<div class="meta-line">Authors: Mao-Lin Luo, Zi-Hao Zhou, Yi-Lin Zhang, Yuanyu Wan, Tong Wei, Min-Ling Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T14:38:57+00:00 · Latest: 2026-01-27T14:38:57+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19659v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19659v1">PDF</a> · <a href="https://github.com/MaolinLuo/KeepLoRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning for pre-trained vision-language models requires balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from a sequence of learned tasks, and maintaining the plasticity to acquire new knowledge. This paper presents a simple but effective approach called KeepLoRA to effectively balance these objectives. We first analyze the knowledge retention mechanism within the model parameter space and find that general knowledge is mainly encoded in the principal subspace, while task-specific knowledge is encoded in the residual subspace. Motivated by this finding, KeepLoRA learns new tasks by restricting LoRA parameter updates in the residual subspace to prevent interfering with previously learned capabilities. Specifically, we infuse knowledge for a new task by projecting its gradient onto a subspace orthogonal to both the principal subspace of pre-trained model and the dominant directions of previous task features. Our theoretical and empirical analyses confirm that KeepLoRA balances the three objectives and achieves state-of-the-art performance. The implementation code is available at https://github.com/MaolinLuo/KeepLoRA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KeepLoRA：基于残差梯度适应的持续学习方法</div>
<div class="mono" style="margin-top:8px">预训练视觉语言模型的持续学习需要平衡三个相互竞争的目标：保留预训练知识、维护已学习任务序列的知识、保持获取新知识的可塑性。本文提出一种简洁高效的KeepLoRA方法以有效平衡这些目标。我们首先分析模型参数空间中的知识保留机制，发现通用知识主要编码在主成分子空间，而任务特定知识编码在残差子空间。基于此发现，KeepLoRA通过将LoRA参数更新限制在残差子空间来学习新任务，避免干扰已习得能力。具体而言，我们将新任务的梯度投影到与预训练模型主成分子空间及先前任务特征主导方向均正交的子空间来实现知识注入。理论与实验分析证实，KeepLoRA能平衡三个目标并取得最先进的性能。实现代码发布于https://github.com/MaolinLuo/KeepLoRA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge in continual learning for pre-trained vision-language models of balancing three competing objectives: retaining pre-trained knowledge, preserving knowledge from previous tasks, and maintaining plasticity for new tasks. The method, KeepLoRA, is motivated by an analysis showing that general knowledge resides in the principal parameter subspace while task-specific knowledge is in the residual subspace; it learns new tasks by restricting Low-Rank Adaptation (LoRA) parameter updates to this residual subspace, specifically by projecting gradients onto a subspace orthogonal to both the pre-trained model&#x27;s principal subspace and dominant directions of previous task features. Experimental results confirm that this approach effectively balances the three objectives and achieves state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本文针对预训练视觉语言模型的持续学习挑战，旨在平衡保留预训练知识、维持已学任务知识以及保持学习新任务的可塑性这三个目标。方法KeepLoRA的动机源于分析发现：通用知识主要编码在参数的主子空间中，而任务特定知识则编码在残差子空间中。该方法通过将LoRA参数更新限制在残差子空间中来学习新任务，具体是将梯度投影到与预训练模型主子空间及先前任务特征主方向均正交的子空间上，以避免干扰。实验结果表明，KeepLoRA有效平衡了这三个目标，并取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Detect Unseen Jailbreak Attacks in Large Vision-Language Models</div>
<div class="meta-line">Authors: Shuang Liang, Zhihao Xu, Jiaqi Weng, Jialing Tao, Hui Xue, Xiting Wang</div>
<div class="meta-line">First: 2025-08-08T16:13:28+00:00 · Latest: 2026-01-27T13:58:13+00:00</div>
<div class="meta-line">Comments: 12 pages; Previously this version appeared as arXiv:2510.15430 which was submitted as a new work by accident</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09201v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.09201v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite extensive alignment efforts, Large Vision-Language Models (LVLMs) remain vulnerable to jailbreak attacks. To mitigate these risks, existing detection methods are essential, yet they face two major challenges: generalization and accuracy. While learning-based methods trained on specific attacks fail to generalize to unseen attacks, learning-free methods based on hand-crafted heuristics suffer from limited accuracy and reduced efficiency. To address these limitations, we propose Learning to Detect (LoD), a learnable framework that eliminates the need for any attack data or hand-crafted heuristics. LoD operates by first extracting layer-wise safety representations directly from the model&#x27;s internal activations using Multi-modal Safety Concept Activation Vectors classifiers, and then converting the high-dimensional representations into a one-dimensional anomaly score for detection via a Safety Pattern Auto-Encoder. Extensive experiments demonstrate that LoD consistently achieves state-of-the-art detection performance (AUROC) across diverse unseen jailbreak attacks on multiple LVLMs, while also significantly improving efficiency. Code is available at https://anonymous.4open.science/r/Learning-to-Detect-51CB.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习检测大型视觉语言模型中的未见越狱攻击</div>
<div class="mono" style="margin-top:8px">尽管进行了广泛的模型对齐，大型视觉语言模型（LVLMs）仍易受越狱攻击。现有检测方法虽对缓解风险至关重要，但面临泛化性与准确性两大挑战：基于特定攻击训练的检测方法难以泛化至未见攻击，而基于人工启发式的无学习方法则存在准确性不足与效率低下的问题。为此，我们提出“学习检测”（LoD）框架，该可学习框架无需任何攻击数据或人工启发式设计。LoD首先通过多模态安全概念激活向量分类器直接从模型内部激活中提取层级安全表征，再通过安全模式自编码器将高维表征转换为一维异常分数以进行检测。大量实验表明，LoD在多种LVLMs上针对不同未见越狱攻击均能取得最先进的检测性能（AUROC），同时显著提升效率。代码发布于 https://anonymous.4open.science/r/Learning-to-Detect-51CB。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing jailbreak detection methods for Large Vision-Language Models (LVLMs), which struggle with generalization to unseen attacks and suffer from accuracy or efficiency issues, this work proposes a learnable framework called Learning to Detect (LoD). The method extracts layer-wise safety representations from the model&#x27;s internal activations using Multi-modal Safety Concept Activation Vectors and then compresses these into a one-dimensional anomaly score via a Safety Pattern Auto-Encoder for detection, requiring no prior attack data or hand-crafted heuristics. Experiments show that LoD achieves state-of-the-art detection performance (AUROC) across diverse unseen jailbreak attacks on multiple LVLMs and significantly improves efficiency.</div>
<div class="mono" style="margin-top:8px">针对现有大型视觉语言模型越狱攻击检测方法泛化性差、准确率或效率不足的问题，本研究提出了可学习检测框架（LoD）。该方法通过多模态安全概念激活向量从模型内部激活中提取分层安全表征，并利用安全模式自编码器将其压缩为一维异常分数进行检测，无需先验攻击数据或人工启发式规则。实验表明，LoD在多种大型视觉语言模型上对各类未见越狱攻击实现了最先进的检测性能（AUROC），并显著提升了效率。</div>
</details>
</div>
<div class="card">
<div class="title">ScenePilot-Bench: A Large-Scale Dataset and Benchmark for Evaluation of Vision-Language Models in Autonomous Driving</div>
<div class="meta-line">Authors: Yujin Wang, Yutong Zheng, Wenxian Fan, Tianyi Wang, Hongqing Chu, Daxin Tian, Bingzhao Gao, Jianqiang Wang, Hong Chen</div>
<div class="meta-line">First: 2026-01-27T13:17:50+00:00 · Latest: 2026-01-27T13:17:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19582v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19582v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we introduce ScenePilot-Bench, a large-scale first-person driving benchmark designed to evaluate vision-language models (VLMs) in autonomous driving scenarios. ScenePilot-Bench is built upon ScenePilot-4K, a diverse dataset comprising 3,847 hours of driving videos, annotated with multi-granularity information including scene descriptions, risk assessments, key participant identification, ego trajectories, and camera parameters. The benchmark features a four-axis evaluation suite that assesses VLM capabilities in scene understanding, spatial perception, motion planning, and GPT-Score, with safety-aware metrics and cross-region generalization settings. We benchmark representative VLMs on ScenePilot-Bench, providing empirical analyses that clarify current performance boundaries and identify gaps for driving-oriented reasoning. ScenePilot-Bench offers a comprehensive framework for evaluating and advancing VLMs in safety-critical autonomous driving contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScenePilot-Bench：面向自动驾驶的大规模视觉语言模型评估数据集与基准</div>
<div class="mono" style="margin-top:8px">本文提出ScenePilot-Bench，这是一个用于评估自动驾驶场景中视觉语言模型的大规模第一人称驾驶基准。该基准基于ScenePilot-4K数据集构建，该数据集包含3,847小时多样化驾驶视频，并标注了多粒度信息，包括场景描述、风险评估、关键参与者识别、自车轨迹和相机参数。基准采用四维评估体系，从场景理解、空间感知、运动规划和GPT-Score四个维度评估视觉语言模型能力，同时引入安全感知指标和跨区域泛化设置。我们在ScenePilot-Bench上对代表性视觉语言模型进行基准测试，通过实证分析揭示当前性能边界，并指出面向驾驶推理的改进方向。ScenePilot-Bench为安全关键型自动驾驶场景中的视觉语言模型评估与推进提供了完整框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for rigorous evaluation of vision-language models in safety-critical autonomous driving applications, this work introduces ScenePilot-Bench, a large-scale benchmark built upon the ScenePilot-4K dataset containing 3,847 hours of driving videos annotated with multi-granular information. The method employs a four-axis evaluation suite assessing scene understanding, spatial perception, motion planning, and GPT-Score, incorporating safety-aware metrics and cross-region generalization tests. Experimental benchmarking of representative VLMs reveals current performance boundaries and identifies specific gaps in driving-oriented reasoning, providing a comprehensive framework for model advancement.</div>
<div class="mono" style="margin-top:8px">为在安全关键的自动驾驶场景中严格评估视觉语言模型，本研究提出了ScenePilot-Bench，一个大规模的第一人称驾驶基准。该方法基于ScenePilot-4K数据集构建基准，该数据集包含3,847小时的驾驶视频，并标注了多粒度信息；同时采用一个四维评估套件，通过安全感知指标评估场景理解、空间感知、运动规划和GPT-Score能力。对代表性模型进行基准测试的主要实验结果表明了当前性能边界，并揭示了面向驾驶的推理中存在的具体差距，为模型发展提供了一个全面的评估框架。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Evolving Vision-Language Models for Image Quality Assessment via Voting and Ranking</div>
<div class="meta-line">Authors: Wen Wen, Tianwu Zhi, Kanglong Fan, Yang Li, Xinge Peng, Yabin Zhang, Yiting Liao, Junlin Li, Li Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-30T04:57:26+00:00 · Latest: 2026-01-27T11:48:21+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25787v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.25787v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Improving vision-language models (VLMs) in the post-training stage typically relies on supervised fine-tuning or reinforcement learning, methods that necessitate costly, human-annotated data. While self-supervised techniques have proven effective for enhancing reasoning capabilities, their application to perceptual domains such as image quality assessment (IQA) remains largely unexplored. In this work, we introduce EvoQuality, a novel framework that enables a VLM to autonomously refine its quality perception capabilities without any ground-truth labels. EvoQuality adapts the principle of self-consistency to the ranking-based nature of IQA. It generates pseudo-labels by performing pairwise majority voting on the VLM&#x27;s own outputs to establish a consensus on relative quality. These pseudo-rankings are then formulated into a fidelity reward that guides the model&#x27;s iterative evolution through group relative policy optimization (GRPO). By iteratively leveraging its own predictions, EvoQuality progressively refines the VLM&#x27;s perceptual capability. Extensive experiments show that EvoQuality boosts the base VLM&#x27;s zero-shot performance by 31.8% on PLCC across diverse IQA benchmarks. Remarkably, despite being entirely self-supervised, EvoQuality achieves performance that is competitive with, or even surpasses, state-of-the-art supervised VLM-based IQA models, outperforming these models on 5 out of 7 IQA benchmarks. Furthermore, the framework demonstrates significant flexibility, allowing it to be stacked with pre-trained IQA models to bolster generalization on unseen datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于投票与排序的自演进视觉语言模型在图像质量评估中的应用</div>
<div class="mono" style="margin-top:8px">后训练阶段提升视觉语言模型（VLM）通常依赖监督微调或强化学习，这些方法需要昂贵的人工标注数据。虽然自监督技术在增强推理能力方面已被证明有效，但其在图像质量评估（IQA）等感知领域的应用仍鲜有探索。本研究提出EvoQuality框架，使VLM能在无真实标注的情况下自主优化其质量感知能力。该框架将自一致性原理适配于IQA的排序特性，通过对VLM自身输出进行成对多数投票生成伪标签以建立相对质量共识，进而将伪排序转化为保真度奖励，通过组相对策略优化（GRPO）指导模型迭代演进。实验表明：EvoQuality将基础VLM在多个IQA基准上的零样本PLCC性能提升31.8%；尽管完全自监督，其性能在7个基准中的5个超越当前最优监督VLM模型，并展现出色灵活性——可与预训练IQA模型堆叠以增强未知数据集的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the high cost of human-annotated data required for improving vision-language models (VLMs) in perceptual tasks like image quality assessment (IQA). The proposed method, EvoQuality, is a self-supervised framework that refines a VLM&#x27;s quality perception without ground-truth labels by generating pseudo-labels through pairwise majority voting on the model&#x27;s own outputs to establish consensus rankings, which are then used as a fidelity reward for iterative optimization via group relative policy optimization (GRPO). Experimental results demonstrate that EvoQuality improves the base VLM&#x27;s zero-shot performance by 31.8% in PLCC across benchmarks, achieves competitive or superior performance compared to supervised state-of-the-art VLM-based IQA models on 5 out of 7 benchmarks, and shows flexibility in stacking with pre-trained models to enhance generalization.</div>
<div class="mono" style="margin-top:8px">为了在无需昂贵人工标注的情况下提升视觉语言模型（VLM）的图像质量评估（IQA）能力，本研究提出了自监督框架EvoQuality。该方法将自一致性原理应用于IQA，通过对VLM自身输出进行成对多数投票来生成伪标签，建立相对质量排序，并以此作为保真度奖励，通过群体相对策略优化（GRPO）进行迭代模型精炼。实验结果表明，EvoQuality将基础VLM在多个基准上的零样本性能提升了31.8%（PLCC），在7个基准中的5个上达到或超越了监督式VLM-based IQA模型的性能，并展现出与预训练模型堆叠以提升泛化能力的灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">RoamScene3D: Immersive Text-to-3D Scene Generation via Adaptive Object-aware Roaming</div>
<div class="meta-line">Authors: Jisheng Chu, Wenrui Li, Rui Zhao, Wangmeng Zuo, Shifeng Chen, Xiaopeng Fan</div>
<div class="meta-line">First: 2026-01-27T10:10:55+00:00 · Latest: 2026-01-27T10:10:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19433v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19433v1">PDF</a> · <a href="https://github.com/JS-CHU/RoamScene3D">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating immersive 3D scenes from texts is a core task in computer vision, crucial for applications in virtual reality and game development. Despite the promise of leveraging 2D diffusion priors, existing methods suffer from spatial blindness and rely on predefined trajectories that fail to exploit the inner relationships among salient objects. Consequently, these approaches are unable to comprehend the semantic layout, preventing them from exploring the scene adaptively to infer occluded content. Moreover, current inpainting models operate in 2D image space, struggling to plausibly fill holes caused by camera motion. To address these limitations, we propose RoamScene3D, a novel framework that bridges the gap between semantic guidance and spatial generation. Our method reasons about the semantic relations among objects and produces consistent and photorealistic scenes. Specifically, we employ a vision-language model (VLM) to construct a scene graph that encodes object relations, guiding the camera to perceive salient object boundaries and plan an adaptive roaming trajectory. Furthermore, to mitigate the limitations of static 2D priors, we introduce a Motion-Injected Inpainting model that is fine-tuned on a synthetic panoramic dataset integrating authentic camera trajectories, making it adaptive to camera motion. Extensive experiments demonstrate that with semantic reasoning and geometric constraints, our method significantly outperforms state-of-the-art approaches in producing consistent and photorealistic scenes. Our code is available at https://github.com/JS-CHU/RoamScene3D.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoamScene3D：通过自适应对象感知漫游实现沉浸式文本到三维场景生成</div>
<div class="mono" style="margin-top:8px">从文本生成沉浸式三维场景是计算机视觉的核心任务，对虚拟现实和游戏开发应用至关重要。尽管利用二维扩散先验具有潜力，现有方法存在空间盲区，且依赖预定义轨迹而未能利用显著对象间的内在关联，导致无法理解语义布局，难以自适应探索场景以推断被遮挡内容。此外，当前修复模型在二维图像空间运行，难以合理填补相机运动造成的空洞。为突破这些局限，我们提出RoamScene3D——一个连接语义引导与空间生成的新型框架。该方法通过推理对象间的语义关系，生成一致且逼真的场景。具体而言，我们采用视觉语言模型构建编码对象关系的场景图，引导相机感知显著对象边界并规划自适应漫游轨迹。同时，为克服静态二维先验的局限，我们提出运动注入修复模型，该模型在集成真实相机轨迹的合成全景数据集上微调，使其能自适应相机运动。大量实验表明，通过语义推理与几何约束，本方法在生成一致逼真场景方面显著优于现有先进技术。代码发布于https://github.com/JS-CHU/RoamScene3D。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing text-to-3D scene generation methods, which suffer from spatial blindness, reliance on predefined camera trajectories, and poor handling of occlusions due to camera motion. The proposed RoamScene3D framework introduces an adaptive object-aware roaming strategy, using a vision-language model to construct a scene graph that guides camera trajectory planning based on semantic object relations, and a Motion-Injected Inpainting model fine-tuned on synthetic panoramic data to plausibly fill holes from camera movement. Experimental results show that this approach, combining semantic reasoning with geometric constraints, significantly outperforms state-of-the-art methods in generating consistent and photorealistic 3D scenes.</div>
<div class="mono" style="margin-top:8px">本研究针对现有文本到3D场景生成方法存在的空间盲区、依赖预定义相机轨迹以及2D修复模型处理遮挡内容效果差等局限性。提出的RoamScene3D框架采用视觉语言模型构建编码物体关系的场景图，以指导自适应的相机漫游轨迹，并引入在合成全景数据上微调的运动注入修复模型来处理相机运动。实验结果表明，这种结合语义推理与几何约束的方法，在生成一致且逼真的3D场景方面显著优于现有先进技术。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-enhanced Pretraining for Vision-language Pathology Foundation Model on Cancer Diagnosis</div>
<div class="meta-line">Authors: Xiao Zhou, Luoyi Sun, Dexuan He, Wenbin Guan, Ge Wang, Ruifen Wang, Lifeng Wang, Xiaojun Yuan, Xin Sun, Ya Zhang, Kun Sun, Yanfeng Wang, Weidi Xie</div>
<div class="meta-line">First: 2024-12-17T17:45:21+00:00 · Latest: 2026-01-27T06:24:09+00:00</div>
<div class="meta-line">Comments: V2: fixed typos, updated experimental results, added ablation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.13126v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.13126v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language foundation models have shown great promise in computational pathology but remain primarily data-driven, lacking explicit integration of medical knowledge. We introduce KEEP (KnowledgE-Enhanced Pathology), a foundation model that systematically incorporates disease knowledge into pretraining for cancer diagnosis. KEEP leverages a comprehensive disease knowledge graph encompassing 11,454 diseases and 139,143 attributes to reorganize millions of pathology image-text pairs into 143,000 semantically structured groups aligned with disease ontology hierarchies. This knowledge-enhanced pretraining aligns visual and textual representations within hierarchical semantic spaces, enabling deeper understanding of disease relationships and morphological patterns. Across 18 public benchmarks (over 14,000 whole-slide images) and 4 institutional rare cancer datasets (926 cases), KEEP consistently outperformed existing foundation models, showing substantial gains for rare subtypes. These results establish knowledge-enhanced vision-language modeling as a powerful paradigm for advancing computational pathology.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向癌症诊断的知识增强视觉-语言病理学基础模型预训练</div>
<div class="mono" style="margin-top:8px">视觉-语言基础模型在计算病理学中展现出巨大潜力，但当前方法主要依赖数据驱动，缺乏对医学知识的显式整合。本文提出KEEP（知识增强病理学模型），一种在癌症诊断预训练中系统融入疾病知识的基础模型。KEEP利用包含11,454种疾病和139,143个属性的综合疾病知识图谱，将数百万病理图像-文本对重组为143,000个与疾病本体层次对齐的语义结构化组。这种知识增强的预训练方法在层次化语义空间中对齐视觉与文本表征，从而实现对疾病关联和形态学模式的深层理解。在18个公共基准数据集（涵盖超14,000张全切片图像）和4个机构罕见癌症数据集（926例病例）上，KEEP持续超越现有基础模型，在罕见亚型诊断中取得显著提升。这些结果表明，知识增强的视觉-语言建模是推进计算病理学发展的有效范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language foundation models in computational pathology are typically data-driven and lack explicit medical knowledge integration. To address this, the authors propose KEEP, a knowledge-enhanced pretraining method that systematically incorporates a comprehensive disease knowledge graph containing 11,454 diseases and 139,143 attributes to reorganize millions of pathology image-text pairs into 143,000 semantically structured groups aligned with disease ontology hierarchies. This approach aligns visual and textual representations within hierarchical semantic spaces, enabling a deeper understanding of disease relationships and morphological patterns. Experimental results across 18 public benchmarks involving over 14,000 whole-slide images and 4 institutional rare cancer datasets with 926 cases demonstrate that KEEP consistently outperforms existing foundation models, showing substantial performance gains, particularly for rare cancer subtypes.</div>
<div class="mono" style="margin-top:8px">计算病理学中的视觉-语言基础模型主要依赖数据驱动，缺乏明确的医学知识整合。为此，研究者提出了KEEP，一种知识增强的预训练方法，该方法系统性地整合了一个包含11,454种疾病和139,143个属性的综合疾病知识图谱，将数百万个病理图像-文本对重组为143,000个与疾病本体层次结构对齐的语义结构化组。这种方法在层次化语义空间中对齐视觉和文本表示，从而实现对疾病关系和形态模式的更深入理解。在18个公共基准数据集（超过14,000张全切片图像）和4个机构罕见癌症数据集（926个病例）上的实验结果表明，KEEP持续优于现有基础模型，特别是在罕见癌症亚型上显示出显著优势。</div>
</details>
</div>
<div class="card">
<div class="title">Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning</div>
<div class="meta-line">Authors: Ganlin Yang, Tianyi Zhang, Haoran Hao, Weiyun Wang, Yibin Liu, Dehui Wang, Guanzhou Chen, Zijian Cai, Junting Chen, Weijie Su, Wengang Zhou, Yu Qiao, Jifeng Dai, Jiangmiao Pang, Gen Luo, Wenhai Wang, Yao Mu, Zhi Hou</div>
<div class="meta-line">First: 2025-10-13T05:51:22+00:00 · Latest: 2026-01-27T05:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.11027v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.11027v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vlaser：具备协同具身推理能力的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">尽管现有研究主要集中于利用视觉-语言模型（VLMs）开发具身推理能力，或将先进VLMs集成至视觉-语言-动作（VLA）模型以实现端到端机器人控制，但鲜有研究直接解决上游VLM推理与下游VLA策略学习之间的关键断层。本研究通过提出Vlaser——一种具备协同具身推理能力的视觉-语言-动作模型，迈出了连接具身推理与VLA策略学习的初步探索。该基础视觉-语言模型专为具身智能体设计，旨在融合高层推理与底层控制。基于高质量的Vlaser-6M数据集，Vlaser在空间推理、具身基础、具身问答及任务规划等一系列具身推理基准测试中取得最先进性能。此外，我们系统研究了不同VLM初始化对监督式VLA微调的影响，为缓解互联网规模预训练数据与具身专用策略学习数据间的领域偏移提供了新见解。基于这些发现，我们的方法在WidowX基准测试中取得最优结果，并在Google Robot基准测试中展现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap between high-level reasoning in Vision-Language Models (VLMs) and low-level control in Vision-Language-Action (VLA) policy learning for embodied agents. The method introduces Vlaser, a foundational model that synergistically integrates embodied reasoning and control, trained on the Vlaser-6M dataset. Key experimental results show that Vlaser achieves state-of-the-art performance on embodied reasoning benchmarks for spatial reasoning, grounding, QA, and task planning, and systematic analysis of VLM initialization leads to top results on the WidowX benchmark and competitive performance on the Google Robot benchmark.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身智能体中，基于视觉语言模型的高层推理与视觉语言动作模型的低层策略学习之间的关键差距。作者提出了Vlaser模型，这是一个基于Vlaser-6M数据集训练的基础模型，旨在协同整合推理与控制能力。实验结果表明，Vlaser在空间推理、具身基础、问答和任务规划等一系列具身推理基准测试中取得了领先性能；通过对视觉语言模型初始化进行系统分析以优化视觉语言动作模型的微调，该方法在WidowX基准测试中取得了最佳结果，并在Google Robot基准测试中表现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Spectral Rectification: Test-Time Defense towards Zero-shot Adversarial Robustness of CLIP</div>
<div class="meta-line">Authors: Sen Nie, Jie Zhang, Zhuo Wang, Shiguang Shan, Xilin Chen</div>
<div class="meta-line">First: 2026-01-27T05:24:45+00:00 · Latest: 2026-01-27T05:24:45+00:00</div>
<div class="meta-line">Comments: 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19210v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19210v1">PDF</a> · <a href="https://github.com/Summu77/CSR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) such as CLIP have demonstrated remarkable zero-shot generalization, yet remain highly vulnerable to adversarial examples (AEs). While test-time defenses are promising, existing methods fail to provide sufficient robustness against strong attacks and are often hampered by high inference latency and task-specific applicability. To address these limitations, we start by investigating the intrinsic properties of AEs, which reveals that AEs exhibit severe feature inconsistency under progressive frequency attenuation. We further attribute this to the model&#x27;s inherent spectral bias. Leveraging this insight, we propose an efficient test-time defense named Contrastive Spectral Rectification (CSR). CSR optimizes a rectification perturbation to realign the input with the natural manifold under a spectral-guided contrastive objective, which is applied input-adaptively. Extensive experiments across 16 classification benchmarks demonstrate that CSR outperforms the SOTA by an average of 18.1% against strong AutoAttack with modest inference overhead. Furthermore, CSR exhibits broad applicability across diverse visual tasks. Code is available at https://github.com/Summu77/CSR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对比频谱校正：面向CLIP零样本对抗鲁棒性的测试时防御方法</div>
<div class="mono" style="margin-top:8px">以CLIP为代表的视觉语言模型（VLMs）展现出卓越的零样本泛化能力，但对对抗样本（AEs）仍高度脆弱。尽管测试时防御策略前景广阔，现有方法难以抵御强攻击，且常受高推理延迟与任务特定适用性的限制。为突破这些局限，本研究首先探究对抗样本的内在特性，发现其在渐进频率衰减下呈现严重的特征不一致性，并归因于模型固有的频谱偏差。基于此洞见，我们提出名为对比频谱校正（CSR）的高效测试时防御方法。CSR通过频谱引导的对比目标，优化校正扰动以将输入自适应地重对齐至自然流形。在16个分类基准上的大量实验表明，CSR以适度推理开销在强AutoAttack下平均超越现有最优方法18.1%，且在多类视觉任务中展现广泛适用性。代码发布于https://github.com/Summu77/CSR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of vision-language models like CLIP to adversarial examples (AEs) during zero-shot inference, where existing test-time defenses suffer from insufficient robustness, high latency, and limited task applicability. The method, Contrastive Spectral Rectification (CSR), is motivated by an analysis showing AEs exhibit feature inconsistency under frequency attenuation due to model spectral bias; CSR efficiently optimizes an input-adaptive rectification perturbation using a spectral-guided contrastive objective to realign inputs with the natural manifold. Experimental results on 16 classification benchmarks show CSR outperforms state-of-the-art methods by an average of 18.1% against strong AutoAttack with modest inference overhead and demonstrates broad applicability across diverse visual tasks.</div>
<div class="mono" style="margin-top:8px">视觉语言模型如CLIP展现出强大的零样本泛化能力，但对对抗样本高度敏感，现有的测试时防御方法常存在鲁棒性不足、延迟高和任务适用性有限的问题。为解决这些限制，本研究首先分析对抗样本，发现由于模型固有的频谱偏差，对抗样本在渐进频率衰减下表现出严重的特征不一致性；基于这一洞察，方法提出了对比频谱校正（CSR），这是一种高效的测试时防御，通过频谱引导的对比目标自适应地优化校正扰动，将输入重新对齐到自然流形。在16个分类基准上的大量实验表明，CSR在对抗强AutoAttack时平均优于最先进方法18.1%，且推理开销适中，并在多种视觉任务中展现出广泛的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">MATA: A Trainable Hierarchical Automaton System for Multi-Agent Visual Reasoning</div>
<div class="meta-line">Authors: Zhixi Cai, Fucai Ke, Kevin Leo, Sukai Huang, Maria Garcia de la Banda, Peter J. Stuckey, Hamid Rezatofighi</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T05:06:54+00:00 · Latest: 2026-01-27T05:06:54+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19204v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19204v1">PDF</a> · <a href="https://github.com/ControlNet/MATA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent vision-language models have strong perceptual ability but their implicit reasoning is hard to explain and easily generates hallucinations on complex queries. Compositional methods improve interpretability, but most rely on a single agent or hand-crafted pipeline and cannot decide when to collaborate across complementary agents or compete among overlapping ones. We introduce MATA (Multi-Agent hierarchical Trainable Automaton), a multi-agent system presented as a hierarchical finite-state automaton for visual reasoning whose top-level transitions are chosen by a trainable hyper agent. Each agent corresponds to a state in the hyper automaton, and runs a small rule-based sub-automaton for reliable micro-control. All agents read and write a shared memory, yielding transparent execution history. To supervise the hyper agent&#x27;s transition policy, we build transition-trajectory trees and transform to memory-to-next-state pairs, forming the MATA-SFT-90K dataset for supervised finetuning (SFT). The finetuned LLM as the transition policy understands the query and the capacity of agents, and it can efficiently choose the optimal agent to solve the task. Across multiple visual reasoning benchmarks, MATA achieves the state-of-the-art results compared with monolithic and compositional baselines. The code and dataset are available at https://github.com/ControlNet/MATA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MATA：一种用于多智能体视觉推理的可训练分层自动机系统</div>
<div class="mono" style="margin-top:8px">当前视觉语言模型虽具备较强的感知能力，但其隐式推理过程难以解释，且在复杂查询时易产生幻觉。组合式方法提升了可解释性，但多数依赖单一智能体或人工设计的流程，无法动态决定何时在互补智能体间协作或在重叠智能体间竞争。本文提出MATA（多智能体分层可训练自动机），这是一种以分层有限状态自动机形式呈现的多智能体视觉推理系统，其顶层状态转移由可训练的超智能体决策。每个智能体对应超自动机中的一个状态，并运行基于规则的小型子自动机以实现可靠的微观控制。所有智能体通过读写共享内存形成透明的执行历史。为监督超智能体的转移策略，我们构建转移轨迹树并转化为“记忆-下一状态”配对数据，形成包含90K样本的MATA-SFT数据集用于监督微调。经微调后作为转移策略的大语言模型能够理解查询需求与各智能体能力，从而高效选择最优智能体完成任务。在多项视觉推理基准测试中，MATA相比单体模型与组合式基线均取得了最先进的性能。代码与数据集已开源：https://github.com/ControlNet/MATA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of vision-language models, which often produce opaque reasoning and hallucinations on complex queries, and compositional methods that lack dynamic collaboration or competition among agents. The authors propose MATA, a multi-agent system structured as a hierarchical finite-state automaton where a trainable hyper agent selects transitions between specialized agents, each running rule-based sub-automata and interacting via shared memory for transparency. To train the hyper agent, they create the MATA-SFT-90K dataset from transition-trajectory trees, enabling supervised fine-tuning of an LLM to optimally route queries. Experiments on multiple visual reasoning benchmarks show that MATA achieves state-of-the-art performance, surpassing both monolithic and compositional baselines.</div>
<div class="mono" style="margin-top:8px">为解决单一视觉语言模型在复杂查询上可解释性差、易产生幻觉的问题，本研究提出了MATA，一个以分层有限状态自动机结构组织的多智能体视觉推理系统。该方法采用一个可训练的超级智能体来选择在专用规则智能体间的状态转移，这些智能体基于共享内存进行操作，从而生成透明的执行历史。通过从转移轨迹树构建的新数据集（MATA-SFT-90K）进行监督训练，该系统在多个视觉推理基准测试中取得了最先进的性能，相较于单一模型和现有组合式基线，在准确性和可解释性上均有提升。</div>
</details>
</div>
<div class="card">
<div class="title">Epistemic-aware Vision-Language Foundation Model for Fetal Ultrasound Interpretation</div>
<div class="meta-line">Authors: Xiao He, Huangxuan Zhao, Guojia Wan, Wei Zhou, Yanxing Liu, Juhua Liu, Yongchao Xu, Yong Luo, Dacheng Tao, Bo Du</div>
<div class="meta-line">First: 2025-10-14T19:57:03+00:00 · Latest: 2026-01-27T04:30:00+00:00</div>
<div class="meta-line">Comments: This paper contains fundamental errors and will not be replaced</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.12953v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.12953v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hexiao0275.github.io/FetalMind">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent medical vision-language models have shown promise on tasks such as VQA, report generation, and anomaly detection. However, most are adapted to structured adult imaging and underperform in fetal ultrasound, which poses challenges of multi-view image reasoning, numerous diseases, and image diversity. To bridge this gap, we introduce FetalMind, a medical AI system tailored to fetal ultrasound for both report generation and diagnosis. Guided by clinical workflow, we propose Salient Epistemic Disentanglement (SED), which injects an expert-curated bipartite graph into the model to decouple view-disease associations and to steer preference selection along clinically faithful steps via reinforcement learning. This design mitigates variability across diseases and heterogeneity across views, reducing learning bottlenecks while aligning the model&#x27;s inference with obstetric practice. To train FetalMind at scale, we curate FetalSigma-1M dataset, the first large-scale fetal ultrasound report corpus, comprising 20K reports from twelve medical centers, addressing the scarcity of domain data. Extensive experiments show that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving +14% average gains and +61.2% higher accuracy on critical conditions while remaining efficient, stable, and scalable. Project Page: https://hexiao0275.github.io/FetalMind.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向胎儿超声解读的认知感知视觉-语言基础模型</div>
<div class="mono" style="margin-top:8px">近期医学视觉-语言模型在视觉问答、报告生成和异常检测等任务中展现出潜力，但多数模型适配结构化成人影像，在胎儿超声领域表现欠佳。胎儿超声面临多视图图像推理、疾病种类繁多和图像多样性等挑战。为填补这一空白，我们推出专为胎儿超声设计的医学AI系统FetalMind，兼具报告生成与诊断功能。在临床工作流指导下，我们提出显著认知解耦方法，通过向模型注入专家构建的二部图来解耦视图-疾病关联，并借助强化学习引导模型沿临床可信步骤进行偏好选择。该设计缓解了疾病间的变异性和视图间的异质性，减少学习瓶颈，同时使模型推理与产科实践保持一致。为大规模训练FetalMind，我们构建了首个大规模胎儿超声报告数据集FetalSigma-1M，包含来自12个医疗中心的2万份报告，解决了领域数据稀缺问题。大量实验表明，FetalMind在所有孕周阶段均优于开源和闭源基线模型，平均性能提升14%，关键病症准确率提高61.2%，同时保持高效、稳定和可扩展性。项目页面：https://hexiao0275.github.io/FetalMind。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of existing medical vision-language models, which are primarily adapted to structured adult imaging and underperform in fetal ultrasound due to challenges like multi-view reasoning, numerous diseases, and image diversity. To bridge this gap, the authors introduce FetalMind, a system tailored for fetal ultrasound report generation and diagnosis, which employs Salient Epistemic Disentanglement (SED) to inject an expert-curated bipartite graph into the model, decoupling view-disease associations and steering preference selection via reinforcement learning to align with clinical workflow. Experimental results demonstrate that FetalMind outperforms open- and closed-source baselines across all gestational stages, achieving an average gain of +14% and a +61.2% higher accuracy on critical conditions while remaining efficient and scalable.</div>
<div class="mono" style="margin-top:8px">本研究针对现有医学视觉-语言模型主要针对结构化成人影像设计、在胎儿超声中因多视图推理、疾病种类多和图像多样性等挑战而表现不佳的问题，提出了专门用于胎儿超声报告生成和诊断的系统FetalMind。该方法引入了新颖的显著认知解耦技术，通过注入专家构建的二部图来解耦视图与疾病的关联，并利用强化学习使推理过程与临床工作流程对齐，从而减少疾病和视图间的异质性。基于新构建的包含来自12个医疗中心的2万份报告的大规模数据集FetalSigma-1M的实验表明，FetalMind在所有孕周阶段均优于开源和闭源基线模型，平均性能提升14%，在关键病症上的准确率提高了61.2%，同时保持了高效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">m2sv: A Scalable Benchmark for Map-to-Street-View Spatial Reasoning</div>
<div class="meta-line">Authors: Yosub Shin, Michael Buriek, Igor Molybog</div>
<div class="meta-line">First: 2026-01-27T02:01:56+00:00 · Latest: 2026-01-27T02:01:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19099v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19099v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision--language models (VLMs) achieve strong performance on many multimodal benchmarks but remain brittle on spatial reasoning tasks that require aligning abstract overhead representations with egocentric views. We introduce m2sv, a scalable benchmark for map-to-street-view spatial reasoning that asks models to infer camera viewing direction by aligning a north-up overhead map with a Street View image captured at the same real-world intersection. We release m2sv-20k, a geographically diverse benchmark with controlled ambiguity, along with m2sv-sft-11k, a curated set of structured reasoning traces for supervised fine-tuning.
  Despite strong performance on existing multimodal benchmarks, the best evaluated VLM achieves only 65.2% accuracy on m2sv, far below the human baseline of 95%. While supervised fine-tuning and reinforcement learning yield consistent gains, cross-benchmark evaluations reveal limited transfer. Beyond aggregate accuracy, we systematically analyze difficulty in map-to-street-view reasoning using both structural signals and human effort, and conduct an extensive failure analysis of adapted open models. Our findings highlight persistent gaps in geometric alignment, evidence aggregation, and reasoning consistency, motivating future work on grounded spatial reasoning across viewpoints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>m2sv：面向地图到街景空间推理的可扩展基准</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在多模态基准测试中表现优异，但在需要将抽象俯视表征与第一人称视角对齐的空间推理任务中仍显脆弱。我们提出m2sv——一个可扩展的地图到街景空间推理基准，要求模型通过将正北朝向的俯视地图与同一真实路口采集的街景图像进行对齐，以推断相机拍摄方向。我们发布了m2sv-20k（一个具有地理多样性且控制模糊度的基准）和m2sv-sft-11k（用于监督微调的结构化推理轨迹数据集）。尽管现有多模态基准表现强劲，最优VLM在m2sv上仅达到65.2%准确率，远低于人类基线（95%）。监督微调与强化学习能带来稳定提升，但跨基准评估显示迁移效果有限。除总体准确率外，我们通过结构信号与人工标注系统分析地图-街景推理的难点，并对适配开源模型进行广泛失败案例剖析。研究结果揭示了几何对齐、证据整合与推理一致性方面的持续缺陷，为跨视角空间推理的后续研究指明方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the brittleness of vision-language models in spatial reasoning tasks that require aligning abstract overhead maps with egocentric street views. The method introduces m2sv, a scalable benchmark where models infer camera direction by matching a north-up map to a corresponding Street View image, and releases two datasets: m2sv-20k for evaluation and m2sv-sft-11k for fine-tuning. Experimental results show the best model achieves only 65.2% accuracy, far below the human baseline of 95%, with fine-tuning offering limited gains and cross-benchmark transfer, while failure analysis reveals persistent gaps in geometric alignment and reasoning consistency.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉-语言模型在需要对齐抽象俯视地图与自我中心街景的空间推理任务中的脆弱性问题。方法引入了m2sv这一可扩展基准，要求模型通过匹配正北朝上的地图与对应街景图像来推断相机方向，并发布了用于评估的m2sv-20k和用于微调的m2sv-sft-11k两个数据集。实验结果表明，最佳视觉-语言模型的准确率仅为65.2%，远低于95%的人类基线，微调和强化学习带来的提升有限且跨基准迁移能力差，揭示了在几何对齐和推理一致性方面存在持续差距。</div>
</details>
</div>
<div class="card">
<div class="title">DiVE-k: Differential Visual Reasoning for Fine-grained Image Recognition</div>
<div class="meta-line">Authors: Raja Kumar, Arka Sadhu, Ram Nevatia</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-11-23T06:04:50+00:00 · Latest: 2026-01-27T01:24:21+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18305v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18305v2">PDF</a> · <a href="https://github.com/raja-kumar/DiVE-k}{here}$">Code1</a> · <a href="https://github.com/raja-kumar/DiVE-k">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision Language Models (LVLMs) possess extensive text knowledge but struggles to utilize this knowledge for fine-grained image recognition, often failing to differentiate between visually similar categories. Existing fine-tuning methods using Reinforcement Learning (RL) with exact-match reward signals are often brittle, encourage memorization of training categories, and fail to elicit differential reasoning needed for generalization to unseen classes. To address this, we propose $\textbf{DiVE-k}$, $\textbf{Di}$fferential $\textbf{V}$isual r$\textbf{E}$asoning using top-$\textbf{k}$ generations, framework that leverages model&#x27;s own top-k predictions as a training signal. For each training image, DiVE-k creates a multiple-choice question from the model&#x27;s top-k outputs and uses RL to train the model to select the correct answer. This approach requires the model to perform fine-grained differential reasoning among plausible options and provides a simple, verifiable reward signal that mitigates memorization and improves generalization. Experiments on five standard fine-grained datasets show that our method significantly outperforms existing approaches. In the standard base-to-novel generalization setting, DiVE-k surpasses the QWEN2.5-VL-7B and ViRFT by 10.04% and 6.16% on the Harmonic Mean metric, respectively. Further experiments show similar gains in mixed-domain and few-shot scenarios. Our code is available $\href{https://github.com/raja-kumar/DiVE-k}{here}$</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiVE-k：面向细粒度图像识别的差分视觉推理</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）虽具备丰富的文本知识，但在细粒度图像识别中难以有效利用这些知识，常无法区分视觉相似的类别。现有基于强化学习（RL）和精确匹配奖励信号的微调方法通常较为脆弱，易导致模型仅记忆训练类别，且无法激发泛化至未见类别所需的差分推理能力。为此，我们提出$\textbf{DiVE-k}$框架——基于前$\textbf{k}$项生成的$\textbf{差分视觉推理}$，该框架利用模型自身的前k项预测作为训练信号。针对每个训练图像，DiVE-k根据模型的前k个输出构建选择题，并通过RL训练模型选择正确答案。该方法要求模型在多个合理选项间进行细粒度差分推理，并提供简单可验证的奖励信号，从而缓解记忆效应并提升泛化能力。在五个标准细粒度数据集上的实验表明，本方法显著优于现有方法。在标准基类-新类泛化设定中，DiVE-k在调和平均数指标上分别超越QWEN2.5-VL-7B和ViRFT模型10.04%和6.16%。进一步实验显示，在混合领域和少样本场景中亦取得类似提升。代码已开源于$\href{https://github.com/raja-kumar/DiVE-k}{此处}$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Large Vision Language Models (LVLMs) in applying their textual knowledge to fine-grained image recognition, where they often fail to distinguish between visually similar categories. The proposed method, DiVE-k, introduces a differential visual reasoning framework that uses the model&#x27;s own top-k predictions to form multiple-choice questions for each training image, then employs Reinforcement Learning to train the model to select the correct answer. Experimental results on five fine-grained datasets demonstrate that DiVE-k significantly outperforms existing methods, achieving improvements of 10.04% and 6.16% over QWEN2.5-VL-7B and ViRFT respectively in base-to-novel generalization, with similar gains in mixed-domain and few-shot scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型（LVLMs）在细粒度图像识别中难以利用其广泛文本知识来区分视觉相似类别的问题。提出的DiVE-k方法采用了一种差分视觉推理框架，利用模型自身的top-k预测作为训练信号；该方法从这些输出中构建选择题，并运用强化学习训练模型选择正确答案，从而鼓励模型在合理选项间进行细粒度比较而非单纯记忆。在五个标准细粒度数据集上的实验结果表明，DiVE-k显著优于现有方法，在基础到新类别的泛化设置中，其调和平均数指标分别超过QWEN2.5-VL-7B和ViRFT模型10.04%和6.16%，在混合领域和少样本场景中也取得了类似的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning</div>
<div class="meta-line">Authors: Po-han Li, Shenghui Chen, Ufuk Topcu, Sandeep Chinchali</div>
<div class="meta-line">First: 2026-01-14T20:14:47+00:00 · Latest: 2026-01-26T20:53:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09851v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09851v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSIL：多模态视频描述中信息损失的统一评估</div>
<div class="mono" style="margin-top:8px">多模态视频描述将密集视频内容压缩为关键帧与自然语言的结构化格式。通过创建统一的多模态摘要，该方法将生成式AI锚定于丰富的语义证据，并作为高效检索的轻量级代理。然而，传统指标（如BLEU或ROUGE）无法量化跨不同模态的信息覆盖度，例如比较文本段落与关键帧序列。为此，我们提出视频摘要信息损失（ViSIL）分数，这是一个基于信息论的框架，通过视觉语言模型（VLM）推理量化摘要未捕获的视频信息。通过测量信息损失，ViSIL作为一种统一指标，能够直接比较结构各异的多模态摘要格式。实验结果表明，ViSIL分数与人类及VLM在视频问答（VQA）任务上的表现均呈现统计显著相关性。ViSIL还可用于摘要选择，以优化信息损失与处理速度之间的权衡，建立帕累托最优前沿，在VQA准确率上超越文本摘要7%，且不增加处理负载。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inability of traditional metrics like BLEU or ROUGE to quantify information coverage across different modalities in multimodal video captioning, which combines keyframes and text. To solve this, the authors propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that uses vision-language model inference to measure the video information not captured by a summary. Experimental results show ViSIL scores correlate significantly with human and VLM performance on Video Question Answering tasks, and using ViSIL for summary selection establishes a Pareto-optimal frontier that improves VQA accuracy by 7% over text summaries without increasing processing load.</div>
<div class="mono" style="margin-top:8px">本研究针对多模态视频描述任务中，传统指标（如BLEU或ROUGE）无法量化跨模态（关键帧与文本）信息覆盖度的问题。作者提出了视频摘要信息损失（ViSIL）分数，这是一个基于信息论的框架，利用视觉语言模型推理来测量多模态摘要未捕获的视频信息。实验结果表明，ViSIL分数与人类及视觉语言模型在视频问答任务上的表现存在显著相关性，并且使用ViSIL进行摘要选择可建立一个帕累托最优前沿，在不增加处理负载的情况下，其视频问答准确率比纯文本摘要高出7%。</div>
</details>
</div>
<div class="card">
<div class="title">CMOOD: Concept-based Multi-label OOD Detection</div>
<div class="meta-line">Authors: Zhendong Liu, Yi Nian, Yuehan Qin, Henry Peng Zou, Li Li, Xiyang Hu, Yue Zhao</div>
<div class="meta-line">First: 2024-11-15T08:15:48+00:00 · Latest: 2026-01-26T20:35:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.13578v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.13578v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can models effectively detect out-of-distribution (OOD) samples in complex, multi-label settings without extensive retraining? Existing OOD detection methods struggle to capture the intricate semantic relationships and label co-occurrences inherent in multi-label settings, often requiring large amounts of training data and failing to generalize to unseen label combinations. While large language models have revolutionized zero-shot OOD detection, they primarily focus on single-label scenarios, leaving a critical gap in handling real-world tasks where samples can be associated with multiple interdependent labels. To address these challenges, we introduce COOD, a novel zero-shot multi-label OOD detection framework. COOD leverages pre-trained vision-language models, enhancing them with a concept-based label expansion strategy and a new scoring function. By enriching the semantic space with both positive and negative concepts for each label, our approach models complex label dependencies, precisely differentiating OOD samples without the need for additional training. Extensive experiments demonstrate that our method significantly outperforms existing approaches, achieving approximately 95% average AUROC on both VOC and COCO datasets, while maintaining robust performance across varying numbers of labels and different types of OOD samples.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CMOOD：基于概念的多标签分布外检测</div>
<div class="mono" style="margin-top:8px">模型如何在无需大量重新训练的情况下，有效检测复杂多标签场景中的分布外样本？现有OOD检测方法难以捕捉多标签场景中固有的复杂语义关系和标签共现模式，通常需要大量训练数据且无法泛化至未见过的标签组合。尽管大语言模型已革新零样本OOD检测领域，但其主要聚焦单标签场景，在处理现实世界中样本可能关联多个相互依赖标签的任务时存在关键空白。为此，我们提出COOD——一种新颖的零样本多标签OOD检测框架。该框架利用预训练视觉-语言模型，通过基于概念的标签扩展策略和新型评分函数进行增强。通过为每个标签注入正向与负向概念以丰富语义空间，我们的方法能够建模复杂标签依赖关系，在无需额外训练的情况下精准区分OOD样本。大量实验表明，本方法显著优于现有方案，在VOC和COCO数据集上平均AUROC达到约95%，并在不同标签数量和OOD样本类型下保持稳健性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting out-of-distribution (OOD) samples in multi-label settings, where existing methods struggle with semantic relationships and label co-occurrences, often requiring extensive retraining. The proposed CMOOD framework introduces a zero-shot approach that leverages pre-trained vision-language models, employing a concept-based label expansion strategy and a novel scoring function to model label dependencies by enriching the semantic space with both positive and negative concepts. Experimental results show that CMOOD significantly outperforms prior methods, achieving approximately 95% average AUROC on VOC and COCO datasets while maintaining robustness across varying label counts and OOD sample types.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多标签场景中分布外样本检测的难题，现有方法难以捕捉复杂的标签依赖关系且需要大量重新训练。提出的CMOOD框架采用零样本方法，利用预训练的视觉-语言模型，并通过基于概念的标签扩展策略和新的评分函数来建模标签共现关系，同时引入正负概念。实验结果表明，CMOOD在VOC和COCO数据集上平均AUROC达到约95%，显著优于现有方法，并在不同标签数量和分布外样本类型上保持鲁棒性能。</div>
</details>
</div>
<div class="card">
<div class="title">DeFM: Learning Foundation Representations from Depth for Robotics</div>
<div class="meta-line">Authors: Manthan Patel, Jonas Frey, Mayank Mittal, Fan Yang, Alexander Hansson, Amir Bar, Cesar Cadena, Marco Hutter</div>
<div class="meta-line">First: 2026-01-26T19:45:31+00:00 · Latest: 2026-01-26T19:45:31+00:00</div>
<div class="meta-line">Comments: Under review, 19 pages, 15 Figures, 9 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18923v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18923v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://de-fm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Depth sensors are widely deployed across robotic platforms, and advances in fast, high-fidelity depth simulation have enabled robotic policies trained on depth observations to achieve robust sim-to-real transfer for a wide range of tasks. Despite this, representation learning for depth modality remains underexplored compared to RGB, where large-scale foundation models now define the state of the art. To address this gap, we present DeFM, a self-supervised foundation model trained entirely on depth images for robotic applications. Using a DINO-style self-distillation objective on a curated dataset of 60M depth images, DeFM learns geometric and semantic representations that generalize to diverse environments, tasks, and sensors. To retain metric awareness across multiple scales, we introduce a novel input normalization strategy. We further distill DeFM into compact models suitable for resource-constrained robotic systems. When evaluated on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, DeFM achieves state-of-the-art performance and demonstrates strong generalization from simulation to real-world environments. We release all our pretrained models, which can be adopted off-the-shelf for depth-based robotic learning without task-specific fine-tuning. Webpage: https://de-fm.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeFM：从深度信息中学习机器人基础表征</div>
<div class="mono" style="margin-top:8px">深度传感器已广泛应用于机器人平台，快速高保真深度模拟技术的进步使得基于深度观测训练的机器人策略能在多种任务中实现稳健的仿真到现实迁移。尽管如此，与已由大规模基础模型定义技术前沿的RGB模态相比，深度模态的表征学习仍待深入探索。为填补这一空白，我们提出DeFM——一个完全基于深度图像训练的自监督基础模型，专为机器人应用设计。通过在6000万张深度图像构成的精选数据集上采用DINO风格的自蒸馏目标，DeFM学习到的几何与语义表征能泛化至多样环境、任务及传感器。为保持跨多尺度的度量感知能力，我们提出一种新颖的输入归一化策略。进一步将DeFM蒸馏为适用于资源受限机器人系统的紧凑模型。在基于深度的分类、分割、导航、运动与操作基准测试中，DeFM均达到最先进性能，并展现出从仿真到真实环境的强大泛化能力。我们开源所有预训练模型，这些模型可直接用于基于深度的机器人学习而无需任务特定微调。项目主页：https://de-fm.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the underexplored potential of depth representation learning compared to RGB, this work introduces DeFM, a self-supervised foundation model for depth images in robotics. The method employs a DINO-style self-distillation objective on a curated dataset of 60M depth images, incorporates a novel input normalization strategy to retain metric awareness across scales, and distills the model for resource-constrained systems. Key experimental results show that DeFM achieves state-of-the-art performance on depth-based classification, segmentation, navigation, locomotion, and manipulation benchmarks, and demonstrates strong generalization from simulation to real-world environments.</div>
<div class="mono" style="margin-top:8px">针对深度表征学习相较于RGB模态探索不足的问题，本研究提出了DeFM，一个在6000万张深度图像上训练的自监督基础模型，专为机器人应用设计。该方法采用DINO风格的自蒸馏目标，并引入了一种新颖的输入归一化策略以保持跨尺度的度量感知能力，同时将模型蒸馏为紧凑版本以适应资源受限的系统。主要实验结果表明，DeFM在基于深度的分类、分割、导航、运动与操作基准测试中达到了最先进的性能，并在从仿真到真实世界的迁移中展现出强大的泛化能力，无需针对特定任务进行微调。</div>
</details>
</div>
<div class="card">
<div class="title">Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge</div>
<div class="meta-line">Authors: Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen, Ziye Wang, Ximeng Meng, Stone Tao, Yiran Qin, Xiaohong Liu, Ruimao Zhang, Lei Bai, Yilun Du, Hao Su, Philip Torr, Zhenfei Yin, Ruihao Gong, Yejun Zeng, Fengjun Zhong, Shenghao Jin, Jinyang Guo, Xianglong Liu, Xiaojun Jia, Tianqi Shan, Wenqi Ren, Simeng Qin, Jialing Yang, Xiaoyu Ma, Tianxing Chen, Zixuan Li, Zijian Cai, Yan Qin, Yusen Qin, Qiangyu Chen, Kaixuan Wang, Zhaoming Han, Yao Mu, Ping Luo, Yuanqi Yao, Haoming Song, Jan-Nico Zaech, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-26T17:56:19+00:00 · Latest: 2026-01-26T17:56:19+00:00</div>
<div class="meta-line">Comments: MARS Challenge @ NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI. Challenge page: https://mars-eai.github.io/MARS-Challenge-Webpage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18733v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mars-eai.github.io/MARS-Challenge-Webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体机器人系统（MARS）挑战赛的进展与创新</div>
<div class="mono" style="margin-top:8px">多模态大语言模型与视觉-语言-行动模型的最新进展显著推动了具身人工智能领域的发展。随着该领域向更复杂的任务场景过渡，多智能体系统框架正成为实现可扩展、高效协作解决方案的关键。这一转变主要受三大因素驱动：智能体能力提升、通过任务委派增强系统效率，以及实现高级人机交互。为应对多智能体协作带来的挑战，我们在NeurIPS 2025 SpaVLE研讨会上提出举办多智能体机器人系统（MARS）挑战赛。竞赛聚焦两大关键领域：规划与控制——参赛者将探索利用视觉语言模型进行多智能体具身规划以协调任务，并通过策略执行在动态环境中完成机器人操作。通过评估参赛方案，本挑战赛为具身多智能体系统的设计与协调提供宝贵见解，推动先进协作式人工智能系统的未来发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to address complex task scenarios in Embodied AI, where multi-agent systems are essential for scalable and collaborative solutions. The method involves organizing the Multi-Agent Robotic System (MARS) Challenge, which focuses on multi-agent embodied planning using vision-language models and policy execution for robotic manipulation in dynamic environments. Key experimental findings from evaluating participant submissions provide insights into the design and coordination of embodied multi-agent systems, advancing the development of collaborative AI.</div>
<div class="mono" style="margin-top:8px">该研究的动机源于应对具身人工智能中复杂任务场景的需求，其中多智能体系统对于实现可扩展和协作的解决方案至关重要。方法包括组织多智能体机器人系统（MARS）挑战赛，重点关注使用视觉语言模型进行动态环境中的任务协调和机器人操作的规划与控制。通过评估参赛者提交的方案，关键实验结果为具身多智能体系统的设计与协调提供了宝贵见解，推动了协作人工智能的发展。</div>
</details>
</div>
<div class="card">
<div class="title">MMedAgent-RL: Optimizing Multi-Agent Collaboration for Multimodal Medical Reasoning</div>
<div class="meta-line">Authors: Peng Xia, Jinglu Wang, Yibo Peng, Kaide Zeng, Zihan Dong, Xian Wu, Xiangru Tang, Hongtu Zhu, Yun Li, Linjun Zhang, Shujie Liu, Yan Lu, Huaxiu Yao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-05-31T13:22:55+00:00 · Latest: 2026-01-26T17:15:26+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00555v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.00555v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Large Vision-Language Models (Med-LVLMs) have shown strong potential in multimodal diagnostic tasks. However, existing single-agent models struggle to generalize across diverse medical specialties, limiting their performance. Recent efforts introduce multi-agent collaboration frameworks inspired by clinical workflows, where general practitioners (GPs) and specialists interact in a fixed sequence. Despite improvements, these static pipelines lack flexibility and adaptability in reasoning. To address this, we propose MMedAgent-RL, a reinforcement learning (RL)-based multi-agent framework that enables dynamic, optimized collaboration among medical agents. Specifically, we train two GP agents based on Qwen2.5-VL via RL: the triage doctor learns to assign patients to appropriate specialties, while the attending physician integrates the judgments from multi-specialists and its own knowledge to make final decisions. To address the inconsistency in specialist outputs, we introduce a curriculum learning (CL)-guided RL strategy with dynamic entropy regulation, progressively teaching the attending physician to balance between imitating specialists and correcting their mistakes. Experiments on five medical VQA benchmarks demonstrate that MMedAgent-RL outperforms both open-source and proprietary Med-LVLMs. Notably, it achieves an average performance gain of 23.6% over strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMedAgent-RL：优化多智能体协作以实现多模态医学推理</div>
<div class="mono" style="margin-top:8px">医学大型视觉语言模型（Med-LVLMs）在多模态诊断任务中展现出强大潜力。然而，现有单智能体模型难以泛化至不同医学专科，限制了其性能。近期研究借鉴临床工作流程，引入了全科医生与专科医生按固定顺序交互的多智能体协作框架。尽管有所改进，这些静态流程在推理中缺乏灵活性与适应性。为此，我们提出MMedAgent-RL——一种基于强化学习（RL）的多智能体框架，可实现医学智能体间的动态优化协作。具体而言，我们通过RL训练两个基于Qwen2.5-VL的全科医生智能体：分诊医生学习将患者分配至合适专科，而主治医生则整合多专科判断与自身知识做出最终决策。为解决专科输出不一致问题，我们引入课程学习（CL）引导的RL策略，通过动态熵调节逐步指导主治医生在模仿专科医生与纠正其错误之间取得平衡。在五个医学VQA基准上的实验表明，MMedAgent-RL性能优于开源及专有Med-LVLMs，较基线模型平均提升23.6%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing single-agent medical large vision-language models face challenges in generalizing across diverse medical specialties, and recent multi-agent frameworks with fixed collaboration sequences lack flexibility. To address this, the authors propose MMedAgent-RL, a reinforcement learning-based multi-agent framework that enables dynamic collaboration: a triage doctor agent assigns patients to specialists, and an attending physician agent integrates multi-specialist judgments with its own knowledge. A curriculum learning-guided RL strategy with dynamic entropy regulation helps the attending physician balance imitating specialists and correcting their inconsistencies. Experiments on five medical VQA benchmarks show that MMedAgent-RL outperforms both open-source and proprietary models, achieving an average performance gain of 23.6% over strong baselines.</div>
<div class="mono" style="margin-top:8px">本研究针对单一智能体医疗大视觉语言模型在跨专科泛化能力上的不足，以及现有多智能体协作框架中固定顺序缺乏灵活性的问题，提出了MMedAgent-RL，一种基于强化学习的多智能体动态优化协作框架。该方法基于Qwen2.5-VL，通过强化学习训练两个全科医生智能体：分诊医生学习将患者分配给合适的专科，主治医生则整合多位专科医生的判断与自身知识做出最终决策；为处理专科输出不一致性，引入了课程学习引导的强化学习策略，并动态调节熵以渐进式教导主治医生平衡模仿专科医生与纠正其错误。在五个医疗视觉问答基准上的实验表明，MMedAgent-RL超越了开源和专有的医疗大视觉语言模型，平均性能较强劲基线提升了23.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Are Video Generation Models Geographically Fair? An Attraction-Centric Evaluation of Global Visual Knowledge</div>
<div class="meta-line">Authors: Xiao Liu, Jiawei Zhang</div>
<div class="meta-line">First: 2026-01-26T17:14:57+00:00 · Latest: 2026-01-26T17:14:57+00:00</div>
<div class="meta-line">Comments: Work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18698v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in text-to-video generation have produced visually compelling results, yet it remains unclear whether these models encode geographically equitable visual knowledge. In this work, we investigate the geo-equity and geographically grounded visual knowledge of text-to-video models through an attraction-centric evaluation. We introduce Geo-Attraction Landmark Probing (GAP), a systematic framework for assessing how faithfully models synthesize tourist attractions from diverse regions, and construct GEOATTRACTION-500, a benchmark of 500 globally distributed attractions spanning varied regions and popularity levels. GAP integrates complementary metrics that disentangle overall video quality from attraction-specific knowledge, including global structural alignment, fine-grained keypoint-based alignment, and vision-language model judgments, all validated against human evaluation. Applying GAP to the state-of-the-art text-to-video model Sora 2, we find that, contrary to common assumptions of strong geographic bias, the model exhibits a relatively uniform level of geographically grounded visual knowledge across regions, development levels, and cultural groupings, with only weak dependence on attraction popularity. These results suggest that current text-to-video models express global visual knowledge more evenly than expected, highlighting both their promise for globally deployed applications and the need for continued evaluation as such systems evolve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成模型是否具有地理公平性？基于景点中心的全球视觉知识评估</div>
<div class="mono" style="margin-top:8px">近期文本到视频生成技术取得了视觉上引人注目的进展，但这些模型是否编码了地理公平的视觉知识仍不明确。本研究通过以景点为中心的评估，探究文本到视频模型的地理公平性与地理锚定视觉知识。我们提出地理景点地标探测框架——一种系统评估模型对全球不同地区旅游景点合成忠实度的框架，并构建了包含500个全球分布景点、涵盖不同区域与知名度层级的GEOATTRACTION-500基准数据集。该框架整合了互补性度量指标，将整体视频质量与景点特定知识解耦，包括全局结构对齐、基于细粒度关键点的对齐以及视觉语言模型判断，所有指标均通过人工评估验证。将该框架应用于当前最先进的文本到视频模型Sora 2时发现：与普遍存在强烈地理偏见的假设相反，该模型在不同地区、发展水平和文化群体间展现出相对统一的地理锚定视觉知识水平，仅与景点知名度呈弱相关性。这些结果表明，当前文本到视频模型表达的全球视觉知识比预期更为均衡，既凸显了其在全球部署应用中的潜力，也强调了随着系统演进持续评估的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates whether text-to-video generation models encode geographically equitable visual knowledge, motivated by the need to assess potential biases as these models are deployed globally. The authors propose the Geo-Attraction Landmark Probing (GAP) framework, which systematically evaluates how models synthesize tourist attractions from diverse regions using a new benchmark, GEOATTRACTION-500, and complementary metrics for structural, keypoint-based, and semantic alignment. Applying GAP to Sora 2 reveals that the model exhibits a relatively uniform level of geographically grounded visual knowledge across different regions, development levels, and cultural groups, with only a weak correlation to attraction popularity, suggesting a more equitable representation of global visual content than commonly assumed.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究文本到视频生成模型是否编码了地理上公平的视觉知识，其动机源于评估这些模型在全球部署时的潜在偏见需求。作者提出了地理吸引力地标探测框架，通过构建包含500个全球分布景点的GEOATTRACTION-500基准，并采用结构对齐、关键点对齐和视觉语言模型判断等互补指标，系统评估模型对不同地区景点的合成忠实度。将该框架应用于Sora 2模型的实验结果表明，该模型在不同地区、发展水平和文化群体中表现出相对均匀的地理视觉知识水平，对景点流行度的依赖较弱，这表明其全球知识表征比通常假设的更为均衡。</div>
</details>
</div>
<div class="card">
<div class="title">A-TPT: Angular Diversity Calibration Properties for Test-Time Prompt Tuning of Vision-Language Models</div>
<div class="meta-line">Authors: Shihab Aaqil Ahamed, Udaya S. K. P. Miriya Thanthrige, Ranga Rodrigo, Muhammad Haris Khan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-30T12:45:24+00:00 · Latest: 2026-01-26T17:12:54+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26441v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26441v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time prompt tuning (TPT) has emerged as a promising technique for adapting large vision-language models (VLMs) to unseen tasks without relying on labeled data. However, the lack of dispersion between textual features can hurt calibration performance, which raises concerns about VLMs&#x27; reliability, trustworthiness, and safety. Current TPT approaches primarily focus on improving prompt calibration by either maximizing average textual feature dispersion or enforcing orthogonality constraints to encourage angular separation. However, these methods may not always have optimal angular separation between class-wise textual features, which implies overlooking the critical role of angular diversity. To address this, we propose A-TPT, a novel TPT framework that introduces angular diversity to encourage uniformity in the distribution of normalized textual features induced by corresponding learnable prompts. This uniformity is achieved by maximizing the minimum pairwise angular distance between features on the unit hypersphere. We show that our approach consistently surpasses state-of-the-art TPT methods in reducing the aggregate average calibration error while maintaining comparable accuracy through extensive experiments with various backbones on different datasets. Notably, our approach exhibits superior zero-shot calibration performance on natural distribution shifts and generalizes well to medical datasets. We provide extensive analyses, including theoretical aspects, to establish the grounding of A-TPT. These results highlight the potency of promoting angular diversity to achieve well-dispersed textual features, significantly improving VLM calibration during test-time adaptation. Our code will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A-TPT：视觉语言模型测试时提示调优的角度多样性校准特性</div>
<div class="mono" style="margin-top:8px">测试时提示调优（TPT）已成为一种无需标注数据即可将大型视觉语言模型（VLM）适配到未见任务的前沿技术。然而，文本特征间缺乏分散性会损害校准性能，引发对VLM可靠性、可信度与安全性的担忧。现有TPT方法主要通过最大化平均文本特征分散度或施加正交约束以促进角度分离来改进提示校准，但这些方法可能无法始终实现类间文本特征的最优角度分离，忽视了角度多样性的关键作用。为此，我们提出A-TPT——一种新颖的TPT框架，通过引入角度多样性来促进可学习提示所生成归一化文本特征的均匀分布。该均匀性通过最大化单位超球面上特征间的最小成对角距离实现。我们在不同数据集上使用多种骨干网络进行大量实验，证明该方法在保持相当准确度的同时，持续超越最先进的TPT方法以降低聚合平均校准误差。值得注意的是，该方法在自然分布偏移上展现出卓越的零样本校准性能，并能良好泛化至医学数据集。我们提供了包含理论层面的全面分析以确立A-TPT的基础。这些结果凸显了通过促进角度多样性实现充分分散的文本特征，能显著提升VLM在测试时适配过程中的校准能力。代码将公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Test-time prompt tuning (TPT) adapts vision-language models to new tasks without labeled data, but poor textual feature dispersion can degrade calibration, affecting model reliability. To address this, A-TPT introduces angular diversity by maximizing the minimum pairwise angular distance between normalized textual features on a unit hypersphere, ensuring a uniform distribution. Experiments across various backbones and datasets show that A-TPT consistently reduces average calibration error while maintaining accuracy, and it demonstrates strong zero-shot calibration on distribution shifts and medical datasets.</div>
<div class="mono" style="margin-top:8px">测试时提示调优可在无标注数据下使视觉语言模型适应新任务，但文本特征分散性不足会损害校准性能和可靠性。为此，A-TPT通过最大化单位超球面上归一化文本特征之间的最小成对角距离，引入角度多样性以确保更均匀的分布。在不同骨干网络和数据集上的实验表明，A-TPT在保持准确性的同时持续降低了平均校准误差，在自然分布偏移和医学数据集上表现出优异的零样本校准性能，证明了角度多样性对提升校准的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Pragmatic VLA Foundation Model</div>
<div class="meta-line">Authors: Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng</div>
<div class="meta-line">First: 2026-01-26T17:08:04+00:00 · Latest: 2026-01-26T17:08:04+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18692v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18692v1">PDF</a> · <a href="https://github.com/Robbyant/lingbot-vla/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种实用的VLA基础模型</div>
<div class="mono" style="margin-top:8px">在机器人操作领域展现出巨大潜力，一种强大的视觉-语言-动作（VLA）基础模型有望在任务和平台间实现忠实泛化，同时确保成本效益（例如适应所需的数据和GPU时数）。为此，我们利用来自9种主流双机械臂配置约20,000小时的真实世界数据开发了LingBot-VLA。通过对3个机器人平台进行系统评估（每个平台完成100项任务，每项任务包含130个训练后测试回合），该模型展现出明显优于同类模型的性能，体现了其卓越的执行力和广泛的泛化能力。我们还构建了高效代码库，在8-GPU训练配置下实现每秒每GPU处理261个样本的吞吐量，相比现有VLA专用代码库提速1.5~2.8倍（具体取决于所基于的VLM基础模型）。上述特性确保我们的模型非常适合实际部署。为推进机器人学习领域发展，我们公开提供代码、基础模型和基准数据，重点关注支持更具挑战性的任务并促进健全的评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for a cost-efficient and generalizable Vision-Language-Action (VLA) foundation model for robotic manipulation, this work develops LingBot-VLA using approximately 20,000 hours of real-world data from nine dual-arm robot configurations. The method involves systematic training and evaluation across three robotic platforms, each performing 100 tasks with 130 post-training episodes per task. Experimental results demonstrate clear superiority over competitors in performance and generalizability, and the accompanying efficient codebase achieves a throughput of 261 samples per second per GPU on an 8-GPU setup, representing a 1.5 to 2.8 times speedup over existing VLA-oriented codebases, facilitating real-world deployment.</div>
<div class="mono" style="margin-top:8px">为满足机器人操作领域对成本高效且泛化性强的视觉-语言-动作基础模型的需求，本研究利用来自九种流行双臂机器人配置的大约20,000小时真实世界数据，开发了LingBot-VLA模型。该方法通过在三个机器人平台上进行系统评估（每个平台完成100项任务，每项任务包含130次训练后测试），并构建了一个高效的代码库，实现了每GPU每秒261个样本的吞吐量。实验结果表明，该模型在性能和泛化能力上明显优于现有方法，其代码库相比现有VLA框架实现了1.5至2.8倍的加速，验证了其适用于实际部署的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP&#x27;s Visual Embedding Projector is a Few-shot Cornucopia</div>
<div class="meta-line">Authors: Mohammad Fahes, Tuan-Hung Vu, Andrei Bursuc, Patrick Pérez, Raoul de Charette</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2024-10-07T17:59:59+00:00 · Latest: 2026-01-26T14:50:34+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.05270v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.05270v4">PDF</a> · <a href="https://github.com/astra-vision/ProLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ProLIP, a simple and architecture-agnostic method for adapting contrastively pretrained vision-language models, such as CLIP, to few-shot classification. ProLIP fine-tunes the vision encoder&#x27;s projection matrix with Frobenius norm regularization on its deviation from the pretrained weights. It achieves state-of-the-art performance on 11 few-shot classification benchmarks under both ``few-shot validation&#x27;&#x27; and ``validation-free&#x27;&#x27; settings. Moreover, by rethinking the non-linear CLIP-Adapter through ProLIP&#x27;s lens, we design a Regularized Linear Adapter (RLA) that performs better, requires no hyperparameter tuning, is less sensitive to learning rate values, and offers an alternative to ProLIP in black-box scenarios where model weights are inaccessible. Beyond few-shot classification, ProLIP excels in cross-dataset transfer, domain generalization, base-to-new class generalization, and test-time adaptation--where it outperforms prompt tuning while being an order of magnitude faster to train. Code is available at https://github.com/astra-vision/ProLIP .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLIP视觉嵌入投影器：小样本学习的宝库</div>
<div class="mono" style="margin-top:8px">我们提出ProLIP——一种简单且架构无关的方法，用于将对比预训练的视觉语言模型（如CLIP）适配到小样本分类任务。ProLIP通过Frobenius范数正则化微调视觉编码器的投影矩阵，约束其与预训练权重的偏差。该方法在11个小样本分类基准测试中，于“小样本验证”和“免验证”两种设置下均达到最先进性能。此外，通过ProLIP的视角重新审视非线性CLIP-Adapter，我们设计了正则化线性适配器（RLA），其性能更优、无需超参数调优、对学习率值更不敏感，并在模型权重不可访问的黑盒场景中为ProLIP提供了替代方案。除小样本分类外，ProLIP在跨数据集迁移、领域泛化、基类到新类泛化以及测试时适配方面表现卓越——其训练速度比提示调优快一个数量级的同时性能更优。代码发布于https://github.com/astra-vision/ProLIP。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to efficiently adapt contrastively pretrained vision-language models like CLIP for few-shot classification without extensive retraining. The method, ProLIP, fine-tunes only the vision encoder&#x27;s projection matrix with Frobenius norm regularization on its deviation from the pretrained weights, making it architecture-agnostic. Experiments show it achieves state-of-the-art performance on 11 few-shot benchmarks, and its analysis leads to a Regularized Linear Adapter (RLA) that is hyperparameter-robust; ProLIP also demonstrates strong results in cross-dataset transfer, domain generalization, and test-time adaptation, outperforming prompt tuning while training much faster.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决如何高效地将CLIP等预训练视觉语言模型适配到少样本分类任务中的问题。提出的方法ProLIP通过使用Frobenius范数正则化来微调视觉编码器的投影矩阵，使其与预训练权重的偏差最小化，这是一种简单且与架构无关的方法。实验结果表明，ProLIP在11个少样本基准测试中取得了最先进的性能，并在跨数据集迁移、领域泛化和测试时适应等任务中表现出色，其训练速度比提示调优快一个数量级。</div>
</details>
</div>
<div class="card">
<div class="title">DisasterInsight: A Multimodal Benchmark for Function-Aware and Grounded Disaster Assessment</div>
<div class="meta-line">Authors: Sara Tehrani, Yonghao Xu, Leif Haglund, Amanda Berg, Michael Felsberg</div>
<div class="meta-line">First: 2026-01-26T13:48:11+00:00 · Latest: 2026-01-26T13:48:11+00:00</div>
<div class="meta-line">Comments: Under review at ICPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18493v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18493v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Timely interpretation of satellite imagery is critical for disaster response, yet existing vision-language benchmarks for remote sensing largely focus on coarse labels and image-level recognition, overlooking the functional understanding and instruction robustness required in real humanitarian workflows. We introduce DisasterInsight, a multimodal benchmark designed to evaluate vision-language models (VLMs) on realistic disaster analysis tasks. DisasterInsight restructures the xBD dataset into approximately 112K building-centered instances and supports instruction-diverse evaluation across multiple tasks, including building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian assessment guidelines.
  To establish domain-adapted baselines, we propose DI-Chat, obtained by fine-tuning existing VLM backbones on disaster-specific instruction data using parameter-efficient Low-Rank Adaptation (LoRA). Extensive experiments on state-of-the-art generic and remote-sensing VLMs reveal substantial performance gaps across tasks, particularly in damage understanding and structured report generation. DI-Chat achieves significant improvements on damage-level and disaster-type classification as well as report generation quality, while building-function classification remains challenging for all evaluated models. DisasterInsight provides a unified benchmark for studying grounded multimodal reasoning in disaster imagery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DisasterInsight：面向功能感知与实体化灾害评估的多模态基准</div>
<div class="mono" style="margin-top:8px">卫星影像的及时解译对灾害响应至关重要，但现有遥感领域的视觉-语言基准大多聚焦粗粒度标签和图像级识别，忽视了实际人道主义工作流程所需的功能性理解和指令鲁棒性。本文提出DisasterInsight——一个为评估视觉-语言模型在真实灾害分析任务表现而设计的多模态基准。该基准将xBD数据集重构为约11.2万个以建筑物为中心的实例，支持跨多任务的指令多样性评估，包括建筑功能分类、损毁程度与灾害类型分类、计数，以及符合人道主义评估指南的结构化报告生成。为建立领域适配基线，我们提出DI-Chat模型，通过对现有视觉-语言模型骨干网络使用参数高效的LoRA方法进行灾害专项指令数据微调获得。在先进通用模型与遥感专用模型上的大量实验表明，各任务间存在显著性能差距，尤其在损毁理解和结构化报告生成方面。DI-Chat在损毁程度分类、灾害类型分类及报告生成质量上取得显著提升，而建筑功能分类对所有评估模型仍具挑战。DisasterInsight为研究灾害影像的实体化多模态推理提供了统一基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap in existing vision-language benchmarks for remote sensing, which often lack functional understanding and instruction robustness needed for real-world disaster response. The authors introduce DisasterInsight, a multimodal benchmark built from the xBD dataset with approximately 112K building-centered instances, designed to evaluate vision-language models on tasks such as building-function classification, damage-level and disaster-type classification, counting, and structured report generation aligned with humanitarian guidelines. To establish baselines, they propose DI-Chat, a model fine-tuned from existing VLM backbones using Low-Rank Adaptation (LoRA) on disaster-specific instruction data. Experimental results show substantial performance gaps in existing models, particularly in damage understanding and report generation, while DI-Chat achieves significant improvements in damage-level and disaster-type classification and report quality, though building-function classification remains challenging for all models.</div>
<div class="mono" style="margin-top:8px">该研究针对现有遥感视觉语言基准在功能理解和指令鲁棒性方面的不足，这些不足限制了其在真实灾害响应中的应用。作者提出了DisasterInsight，一个基于xBD数据集构建的多模态基准，包含约11.2万个以建筑物为中心的实例，用于评估视觉语言模型在建筑物功能分类、损坏程度和灾害类型分类、计数以及结构化报告生成等任务上的性能。为建立领域适应的基线，他们提出了DI-Chat模型，该模型通过使用低秩适应（LoRA）对现有视觉语言模型骨干进行灾害特定指令数据的微调得到。实验结果表明，尽管当前最先进的通用和遥感视觉语言模型在损坏理解和报告生成等任务上存在显著性能差距，但DI-Chat在损坏程度和灾害类型分类以及报告质量方面取得了明显改进，不过建筑物功能分类对所有模型而言仍具挑战性。</div>
</details>
</div>
<div class="card">
<div class="title">Ask Me Again Differently: GRAS for Measuring Bias in Vision Language Models on Gender, Race, Age, and Skin Tone</div>
<div class="meta-line">Authors: Shaivi Malik, Hasnat Md Abdullah, Sriparna Saha, Amit Sheth</div>
<div class="meta-line">First: 2025-08-26T12:41:35+00:00 · Latest: 2026-01-26T12:18:24+00:00</div>
<div class="meta-line">Comments: Accepted to the Findings of EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.18989v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.18989v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Vision Language Models (VLMs) become integral to real-world applications, understanding their demographic biases is critical. We introduce GRAS, a benchmark for uncovering demographic biases in VLMs across gender, race, age, and skin tone, offering the most diverse coverage to date. We further propose the GRAS Bias Score, an interpretable metric for quantifying bias. We benchmark five state-of-the-art VLMs and reveal concerning bias levels, with the least biased model attaining a GRAS Bias Score of only 2 out of 100. Our findings also reveal a methodological insight: evaluating bias in VLMs with visual question answering (VQA) requires considering multiple formulations of a question. Our code, data, and evaluation results are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>换个方式再问：用于测量视觉语言模型在性别、种族、年龄和肤色方面偏见的GRAS基准</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLMs）在现实应用中日益重要，理解其人口统计学偏见至关重要。我们提出了GRAS基准，用于揭示VLMs在性别、种族、年龄和肤色方面的偏见，提供了迄今为止最多样化的覆盖范围。我们进一步提出了GRAS偏见分数，这是一种可解释的量化偏见指标。我们对五种最先进的VLM进行了基准测试，揭示了令人担忧的偏见水平，其中偏见最小的模型仅获得100分中的2分。我们的研究还揭示了一个方法论洞见：使用视觉问答（VQA）评估VLM偏见时，需要考虑问题的多种表述方式。我们的代码、数据和评估结果已公开提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand demographic biases in Vision Language Models (VLMs) as they are increasingly deployed in real-world applications. The method introduces the GRAS benchmark, which provides diverse coverage across gender, race, age, and skin tone, and proposes an interpretable GRAS Bias Score for quantification. Key experimental results from benchmarking five state-of-the-art VLMs reveal concerning bias levels, with the least biased model scoring only 2 out of 100, and highlight that accurate bias evaluation in visual question answering requires considering multiple question formulations.</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLMs）在现实应用中的普及，理解其人口统计学偏差变得至关重要。为此，本研究引入了GRAS基准，旨在揭示VLMs在性别、种族、年龄和肤色方面的偏差，提供了迄今为止最多样化的覆盖范围，并提出了可解释的GRAS偏差分数进行量化。该方法通过视觉问答（VQA）对五个最先进的VLMs进行评估，关键方法学见解是评估偏差时需要考虑问题的多种表述形式。实验结果显示，所有模型都存在令人担忧的偏差水平，其中偏差最小的模型在GRAS偏差分数上也仅得2分（满分100），突显了当前模型的严重问题。</div>
</details>
</div>
<div class="card">
<div class="title">ARMOR: Agentic Reasoning for Methods Orchestration and Reparameterization for Robust Adversarial Attacks</div>
<div class="meta-line">Authors: Gabriel Lee Jun Rong, Christos Korgialas, Dion Jia Xu Ho, Pai Chet Ng, Xiaoxiao Miao, Konstantinos N. Plataniotis</div>
<div class="meta-line">First: 2026-01-26T11:36:34+00:00 · Latest: 2026-01-26T11:36:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18386v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18386v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing automated attack suites operate as static ensembles with fixed sequences, lacking strategic adaptation and semantic awareness. This paper introduces the Agentic Reasoning for Methods Orchestration and Reparameterization (ARMOR) framework to address these limitations. ARMOR orchestrates three canonical adversarial primitives, Carlini-Wagner (CW), Jacobian-based Saliency Map Attack (JSMA), and Spatially Transformed Attacks (STA) via Vision Language Models (VLM)-guided agents that collaboratively generate and synthesize perturbations through a shared ``Mixing Desk&quot;. Large Language Models (LLMs) adaptively tune and reparameterize parallel attack agents in a real-time, closed-loop system that exploits image-specific semantic vulnerabilities. On standard benchmarks, ARMOR achieves improved cross-architecture transfer and reliably fools both settings, delivering a blended output for blind targets and selecting the best attack or blended attacks for white-box targets using a confidence-and-SSIM score.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARMOR：面向鲁棒对抗攻击的方法编排与参数重调的智能体推理框架</div>
<div class="mono" style="margin-top:8px">现有自动化攻击套件采用静态集成与固定序列，缺乏策略适应性与语义感知。本文提出智能体推理方法编排与参数重调（ARMOR）框架以解决这些局限。ARMOR通过视觉语言模型（VLM）引导的智能体，协同编排三种经典对抗攻击原语——Carlini-Wagner（CW）、基于雅可比显著图攻击（JSMA）和空间变换攻击（STA），并借助共享的“混音台”生成与合成扰动。大型语言模型（LLM）在实时闭环系统中自适应调优并重参数化并行攻击智能体，以利用图像特定的语义漏洞。在标准基准测试中，ARMOR实现了更优的跨架构迁移性，稳定欺骗黑白盒两种场景：对盲目标生成混合输出，对白盒目标则基于置信度与结构相似性（SSIM）评分选择最佳攻击或混合攻击方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing automated adversarial attack ensembles are static and lack adaptive strategy, limiting their effectiveness. To overcome this, ARMOR introduces a framework where Vision Language Models (VLM) guide agents to orchestrate three core attack primitives—Carlini-Wagner, JSMA, and Spatially Transformed Attacks—through a shared &quot;Mixing Desk&quot; to collaboratively generate perturbations, while Large Language Models (LLMs) adaptively tune these agents in real-time to exploit semantic vulnerabilities. Experimental results on standard benchmarks show that ARMOR improves cross-architecture transferability and reliably fools both white-box and blind targets, using a blended output for blind settings and selecting optimal attacks based on confidence and SSIM scores for white-box scenarios.</div>
<div class="mono" style="margin-top:8px">现有的自动化对抗攻击集成方法多为静态组合，缺乏策略适应性和语义感知能力。为此，本文提出了ARMOR框架，通过智能体推理实现方法编排与参数重配置。该框架利用视觉语言模型引导的智能体，协同操作CW、JSMA和空间变换攻击这三种基本攻击方法，通过一个共享的“混合台”生成综合扰动，并由大语言模型实时调整智能体参数以利用图像特定的语义漏洞。在标准基准测试中，ARMOR提升了跨架构的迁移攻击成功率，能可靠地欺骗白盒和盲目标，通过置信度与结构相似性评分，为盲目标输出混合攻击，为白盒目标选择最佳攻击或混合攻击。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Rigid: Benchmarking Non-Rigid Video Editing</div>
<div class="meta-line">Authors: Bingzheng Qu, Kehai Chen, Xuefeng Bai, Jun Yu, Min Zhang</div>
<div class="meta-line">First: 2026-01-26T10:28:09+00:00 · Latest: 2026-01-26T10:28:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18340v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the remarkable progress in text-driven video editing, generating coherent non-rigid deformations remains a critical challenge, often plagued by physical distortion and temporal flicker. To bridge this gap, we propose NRVBench, the first dedicated and comprehensive benchmark designed to evaluate non-rigid video editing. First, we curate a high-quality dataset consisting of 180 non-rigid motion videos from six physics-based categories, equipped with 2,340 fine-grained task instructions and 360 multiple-choice questions. Second, we propose NRVE-Acc, a novel evaluation metric based on Vision-Language Models that can rigorously assess physical compliance, temporal consistency, and instruction alignment, overcoming the limitations of general metrics in capturing complex dynamics. Third, we introduce a training-free baseline, VM-Edit, which utilizes a dual-region denoising mechanism to achieve structure-aware control, balancing structural preservation and dynamic deformation. Extensive experiments demonstrate that while current methods have shortcomings in maintaining physical plausibility, our method achieves excellent performance across both standard and proposed metrics. We believe the benchmark could serve as a standard testing platform for advancing physics-aware video editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越刚性：非刚性视频编辑基准评测</div>
<div class="mono" style="margin-top:8px">尽管文本驱动视频编辑已取得显著进展，但生成连贯的非刚性形变仍是关键挑战，常受物理失真与时间闪烁问题困扰。为填补这一空白，我们提出首个专用于评估非刚性视频编辑的综合性基准NRVBench。首先，我们构建了包含六个物理类别180个非刚性运动视频的高质量数据集，配备2340条细粒度任务指令和360道选择题。其次，我们提出基于视觉语言模型的新型评估指标NRVE-Acc，可严格评估物理合规性、时间一致性与指令对齐性，克服了通用指标在捕捉复杂动态方面的局限。第三，我们引入免训练基线方法VM-Edit，通过双区域去噪机制实现结构感知控制，平衡结构保持与动态形变。大量实验表明，当前方法在保持物理合理性方面存在不足，而我们的方法在标准指标与新建指标上均表现优异。我们相信该基准可作为推进物理感知视频编辑的标准测试平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating physically plausible non-rigid deformations in text-driven video editing, which often suffers from distortions and flickering. To evaluate this, the authors introduce NRVBench, a benchmark comprising a dataset of 180 videos across six physics-based categories with detailed instructions, and propose NRVE-Acc, a new Vision-Language Model-based metric for assessing physical compliance, temporal consistency, and instruction alignment. They also present VM-Edit, a training-free baseline method using a dual-region denoising mechanism for structure-aware control. Experiments show existing methods struggle with physical plausibility, while VM-Edit performs well on both standard and the proposed metrics, positioning the benchmark as a standard platform for physics-aware video editing.</div>
<div class="mono" style="margin-top:8px">针对文本驱动视频编辑中生成连贯非刚性变形所面临的物理失真和时间闪烁等关键挑战，本研究提出了首个专用的综合性基准NRVBench。方法包括构建一个包含180个非刚性运动视频的高质量数据集，提出一种基于视觉语言模型的新评估指标（NRVE-Acc）以严格评估物理合规性和时间一致性，并引入一种无需训练的基线方法VM-Edit，该方法利用双区域去噪机制实现结构感知控制。主要实验结果表明，尽管现有方法在保持物理合理性方面存在不足，但所提出的方法在标准和新建指标上均取得了优异性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
