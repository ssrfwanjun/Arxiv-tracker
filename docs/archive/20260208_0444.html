<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-08 04:44</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260208_0444</div>
    <div class="row"><div class="card">
<div class="title">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</div>
<div class="meta-line">Authors: Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-05T18:52:48+00:00 · Latest: 2026-02-05T18:52:48+00:00</div>
<div class="meta-line">Comments: Project Page: https://genarena.github.io/, Code: https://github.com/ruihanglix/genarena</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06013v1">PDF</a> · <a href="https://github.com/ruihanglix/genarena">Code1</a> · <a href="https://genarena.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenArena：如何实现视觉生成任务的人类对齐评估？</div>
<div class="mono" style="margin-top:8px">视觉生成模型的快速发展已超越传统评估方法，亟需采用视觉语言模型作为替代评判者。本研究系统探究了主流绝对点式评分标准在广泛视觉生成任务中的可靠性，分析表明该范式因随机不一致性及与人类感知对齐度低而受限。为解决这些局限，我们提出GenArena——一个采用成对比较范式的统一评估框架，以确保稳定且人类对齐的评估。关键实验发现：仅采用此成对协议即可使现成开源模型超越顶级专有模型。该方法将评估准确率提升超20%，与权威LMArena排行榜的斯皮尔曼相关系数达0.86，显著超越点式方法的0.36。基于GenArena，我们对前沿视觉生成模型进行多任务基准测试，为学界提供严谨的自动化视觉生成评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid progress of visual generation models has rendered traditional evaluation methods inadequate, prompting the use of Vision-Language Models as judges. This work systematically examines the reliability of the prevalent absolute pointwise scoring standard across various visual generation tasks, identifying limitations such as stochastic inconsistency and poor alignment with human perception. To address these issues, the authors introduce GenArena, a unified evaluation framework that employs a pairwise comparison paradigm to ensure stable and human-aligned assessment. Key experimental findings reveal that adopting this pairwise protocol allows off-the-shelf open-source models to outperform top-tier proprietary models, boosting evaluation accuracy by over 20% and achieving a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, significantly surpassing the 0.36 correlation of pointwise methods.</div>
<div class="mono" style="margin-top:8px">视觉生成模型的快速发展使得传统评估方法显得不足，促使研究者采用视觉语言模型作为评判者。本研究系统性地检验了当前主流的绝对点式评分标准在各种视觉生成任务中的可靠性，发现了其局限性，包括随机不一致性和与人类感知的对齐性差。为解决这些问题，作者提出了GenArena，一个统一的评估框架，采用成对比较范式以确保稳定且与人类对齐的评估。关键实验结果表明，采用这种成对协议使得现成的开源模型能够超越顶级专有模型，将评估准确率提升超过20%，并与权威的LMArena排行榜达到0.86的斯皮尔曼相关性，显著优于点式方法的0.36相关性。</div>
</details>
</div>
<div class="card">
<div class="title">GIQ: Benchmarking 3D Geometric Reasoning of Vision Foundation Models with Simulated and Real Polyhedra</div>
<div class="meta-line">Authors: Mateusz Michalkiewicz, Anekha Sokhal, Tadeusz Michalkiewicz, Piotr Pawlikowski, Mahsa Baktashmotlagh, Varun Jampani, Guha Balakrishnan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-09T20:11:21+00:00 · Latest: 2026-02-05T16:06:21+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. Camera ready version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08194v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.08194v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern monocular 3D reconstruction methods and vision-language models (VLMs) demonstrate impressive results on standard benchmarks, yet recent works cast doubt on their true understanding of geometric properties. We introduce GOQ, a comprehensive benchmark specifically designed to evaluate the geometric reasoning capabilities of vision and vision-language foundation models. GIQ comprises synthetic and real-world images and corresponding 3D meshes of diverse polyhedra covering varying levels of complexity and symmetry, from Platonic, Archimedean, Johnson, and Catalan solids to stellations and compound shapes. Through systematic experiments involving monocular 3D reconstruction, 3D symmetry detection, mental rotation tests, and zero-shot shape classification tasks, we reveal significant shortcomings in current models. State-of-the-art reconstruction algorithms trained on extensive 3D datasets struggle to reconstruct even basic geometric Platonic solids accurately. Next, although foundation models may be shown via linear and non-linear probing to capture specific 3D symmetry elements, they falter significantly in tasks requiring detailed geometric differentiation, such as mental rotation. Moreover, advanced vision-language assistants such as ChatGPT, Gemini and Claud exhibit remarkably low accuracy in interpreting basic shape properties such as face geometry, convexity, and compound structures of complex polyhedra. GIQ is publicly available at toomanymatts.github.io/giq-benchmark/, providing a structured platform to benchmark critical gaps in geometric intelligence and facilitate future progress in robust, geometry-aware representation learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GIQ：基于仿真与真实多面体评测视觉基础模型的几何推理能力</div>
<div class="mono" style="margin-top:8px">现代单目三维重建方法与视觉语言模型在标准评测集上表现优异，但近期研究对其几何属性理解能力提出质疑。我们提出GIQ——专为评估视觉与视觉语言基础模型几何推理能力设计的综合评测基准。该基准涵盖合成与真实场景图像及对应三维网格数据，包含从柏拉图立体、阿基米德立体、约翰逊立体、卡塔兰立体到星形多面体与复合结构等不同复杂度与对称性的多面体。通过系统实验（单目三维重建、三维对称性检测、心理旋转测试、零样本形状分类），我们揭示了当前模型的显著缺陷：基于海量三维数据训练的前沿重建算法难以准确重建基础柏拉图立体；基础模型虽能通过线性/非线性探针捕获特定三维对称元素，但在需要精细几何辨别的任务（如心理旋转）中表现欠佳；ChatGPT、Gemini、Claud等先进视觉语言助手在理解复杂多面体的面几何、凸性、复合结构等基础属性时准确率极低。GIQ已开源（toomanymatts.github.io/giq-benchmark/），为系统评测几何智能关键缺陷、推动鲁棒几何感知表征学习提供结构化平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses concerns about the limited geometric understanding of modern vision and vision-language models by introducing GIQ, a benchmark comprising synthetic and real-world images of diverse polyhedra to evaluate 3D geometric reasoning. The method involves systematic experiments on monocular 3D reconstruction, symmetry detection, mental rotation, and zero-shot shape classification. Key findings reveal that state-of-the-art reconstruction models struggle to accurately reconstruct basic Platonic solids, foundation models fail at detailed geometric differentiation like mental rotation, and advanced vision-language assistants show low accuracy in interpreting fundamental shape properties.</div>
<div class="mono" style="margin-top:8px">本研究针对现代视觉和视觉语言模型几何理解能力有限的问题，引入了GIQ基准，该基准包含多样多面体的合成与真实图像，用于评估三维几何推理能力。方法包括对单目三维重建、对称性检测、心理旋转和零样本形状分类等任务的系统实验。主要实验结果表明，最先进的重建模型难以准确重建基本的柏拉图立体，基础模型尽管能捕捉某些对称性，但在心理旋转等需要精细几何区分的任务上表现不佳，且先进的视觉语言助手在解释基本形状属性（如面几何、凸性和复合结构）时准确率极低。</div>
</details>
</div>
<div class="card">
<div class="title">Focus-Scan-Refine: From Human Visual Perception to Efficient Visual Token Pruning</div>
<div class="meta-line">Authors: Enwei Tong, Yuanchao Bai, Yao Zhu, Junjun Jiang, Xianming Liu</div>
<div class="meta-line">First: 2026-02-05T16:02:48+00:00 · Latest: 2026-02-05T16:02:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05809v1">PDF</a> · <a href="https://github.com/ILOT-code/FSR">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) often generate massive visual tokens that greatly increase inference latency and memory footprint; while training-free token pruning offers a practical remedy, existing methods still struggle to balance local evidence and global context under aggressive compression. We propose Focus-Scan-Refine (FSR), a human-inspired, plug-and-play pruning framework that mimics how humans answer visual questions: focus on key evidence, then scan globally if needed, and refine the scanned context by aggregating relevant details. FSR first focuses on key evidence by combining visual importance with instruction relevance, avoiding the bias toward visually salient but query-irrelevant regions. It then scans for complementary context conditioned on the focused set, selecting tokens that are most different from the focused evidence. Finally, FSR refines the scanned context by aggregating nearby informative tokens into the scan anchors via similarity-based assignment and score-weighted merging, without increasing the token budget. Extensive experiments across multiple VLM backbones and vision-language benchmarks show that FSR consistently improves the accuracy-efficiency trade-off over existing state-of-the-art pruning methods. The source codes can be found at https://github.com/ILOT-code/FSR</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>聚焦-扫描-精炼：从人类视觉感知到高效视觉令牌剪枝</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）常生成海量视觉令牌，显著增加推理延迟和内存占用；尽管免训练的令牌剪枝提供了实用解决方案，现有方法在激进压缩下仍难以平衡局部证据与全局上下文。我们提出受人类启发的即插即用剪枝框架“聚焦-扫描-精炼”（FSR），模拟人类回答视觉问题的认知过程：聚焦关键证据→按需全局扫描→聚合相关细节精炼扫描上下文。FSR首先通过融合视觉重要性与指令相关性聚焦关键证据，避免偏向视觉显著但查询无关的区域；随后基于聚焦集扫描互补上下文，选择与聚焦证据差异最大的令牌；最后通过基于相似度的分配和分数加权合并，将邻近信息令牌聚合至扫描锚点，在不增加令牌预算的前提下精炼扫描上下文。跨多个VLM骨干和视觉语言基准的广泛实验表明，FSR在精度-效率权衡上持续优于现有前沿剪枝方法。源代码详见：https://github.com/ILOT-code/FSR</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the inefficiency of vision-language models caused by excessive visual tokens, this study introduces Focus-Scan-Refine (FSR), a training-free pruning framework inspired by human visual question-answering. The method first focuses on key evidence by integrating visual importance and instruction relevance, then scans for globally complementary context by selecting tokens distinct from the focused set, and finally refines the scanned context through similarity-based aggregation without increasing token count. Experiments on multiple VLM backbones and benchmarks demonstrate that FSR consistently achieves a better accuracy-efficiency trade-off compared to existing state-of-the-art pruning methods.</div>
<div class="mono" style="margin-top:8px">视觉语言模型因处理大量视觉令牌而产生高昂计算成本。为解决此问题，作者提出Focus-Scan-Refine（FSR），这是一种受人类视觉问答启发的免训练剪枝框架，它依次通过视觉和指令相关性聚焦关键证据，全局扫描补充上下文，并通过聚合邻近信息令牌来精炼上下文，且不增加令牌数量。在多个VLM骨干网络和基准测试上的实验表明，FSR相比现有最先进的剪枝方法，实现了更优的精度与效率权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation</div>
<div class="meta-line">Authors: Hengyi Wang, Ruiqiang Zhang, Chang Liu, Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang</div>
<div class="meta-line">First: 2026-02-05T15:45:39+00:00 · Latest: 2026-02-05T15:45:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05789v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05789v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rising need for spatially grounded tasks such as Vision-Language Navigation/Action, allocentric perception capabilities in Vision-Language Models (VLMs) are receiving growing focus. However, VLMs remain brittle on allocentric spatial queries that require explicit perspective shifts, where the answer depends on reasoning in a target-centric frame rather than the observed camera view. Thus, we introduce Allocentric Perceiver, a training-free strategy that recovers metric 3D states from one or more images with off-the-shelf geometric experts, and then instantiates a query-conditioned allocentric reference frame aligned with the instruction&#x27;s semantic intent. By deterministically transforming reconstructed geometry into the target frame and prompting the backbone VLM with structured, geometry-grounded representations, Allocentric Perceriver offloads mental rotation from implicit reasoning to explicit computation. We evaluate Allocentric Perciver across multiple backbone families on spatial reasoning benchmarks, observing consistent and substantial gains ($\sim$10%) on allocentric tasks while maintaining strong egocentric performance, and surpassing both spatial-perception-finetuned models and state-of-the-art open-source and proprietary models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>他者中心感知器：通过框架实例化从他者视觉先验中解耦他者中心推理</div>
<div class="mono" style="margin-top:8px">随着视觉语言导航/动作等空间定位任务需求的增长，视觉语言模型中的他者中心感知能力日益受到关注。然而，在需要显式视角转换的他者中心空间查询任务中，视觉语言模型仍显脆弱——这类任务的答案需基于目标中心框架而非观察相机视角进行推理。为此，我们提出&#x27;他者中心感知器&#x27;，这是一种免训练策略：首先利用现成几何专家从单幅或多幅图像恢复度量三维状态，随后实例化一个与指令语义意图对齐的查询条件化他者中心参考框架。通过将重建几何确定性地转换至目标框架，并向骨干视觉语言模型提供结构化、几何基底的表征，本方法将心理旋转从隐式推理转移至显式计算。我们在空间推理基准测试中对多种骨干网络进行评估，发现他者中心任务获得持续显著提升（约10%），同时保持强大的自我中心性能，其表现超越空间感知微调模型及当前最先进的开源与专有模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the brittleness of Vision-Language Models (VLMs) in allocentric spatial reasoning, where answers depend on a target-centric frame rather than the observed egocentric view, which is crucial for tasks like Vision-Language Navigation. The method, Allocentric Perceiver, is a training-free strategy that recovers metric 3D states from images using off-the-shelf geometric experts, instantiates a query-conditioned allocentric reference frame, and transforms the reconstructed geometry into this target frame to provide structured, geometry-grounded representations to the backbone VLM, thereby offloading mental rotation to explicit computation. Experimental results show consistent and substantial gains (approximately 10%) on allocentric tasks across multiple backbone VLM families while maintaining strong egocentric performance, surpassing both spatially fine-tuned models and state-of-the-art open-source and proprietary models.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在以目标为中心的异中心空间推理中的脆弱性问题，这类任务的答案依赖于目标坐标系而非相机的自我中心视角，对于视觉语言导航等任务至关重要。所提出的方法“异中心感知器”是一种免训练策略，它利用现成的几何工具从图像中恢复度量3D状态，实例化一个查询条件化的异中心参考系，并将重建的几何转换到该目标坐标系中，从而为骨干视觉语言模型提供结构化的、基于几何的表征，将心理旋转任务卸载给显式计算。实验结果表明，该方法在多个骨干视觉语言模型家族上，在异中心推理基准测试中取得了持续且显著的提升（约10%），同时保持了强大的自我中心性能，超越了经过空间感知微调的模型以及最先进的开源和专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Ethology of Latent Spaces</div>
<div class="meta-line">Authors: Philippe Boisnard</div>
<div class="meta-line">First: 2026-02-05T14:37:31+00:00 · Latest: 2026-02-05T14:37:31+00:00</div>
<div class="meta-line">Comments: 23. pages, 14 figures, presented Hyperheritage International Symposium 9 ( https://paragraphe.univ-paris8.fr/IMG/pdf/programme_colloque_his9_campuscondorcet_v3.pdf ) and accepted for publication in double-blind peer review in French in 2026-2027</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05710v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05710v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study challenges the presumed neutrality of latent spaces in vision language models (VLMs) by adopting an ethological perspective on their algorithmic behaviors. Rather than constituting spaces of homogeneous indeterminacy, latent spaces exhibit model-specific algorithmic sensitivities, understood as differential regimes of perceptual salience shaped by training data and architectural choices.
  Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) applied to a corpus of 301 artworks (15th to 20th), we reveal substantial divergences in the attribution of political and cultural categories. Using bipolar semantic axes derived from vector analogies (Mikolov et al., 2013), we show that SigLIP classifies 59.4% of the artworks as politically engaged, compared to only 4% for OpenCLIP. African masks receive the highest political scores in SigLIP while remaining apolitical in OpenAI CLIP. On an aesthetic colonial axis, inter-model discrepancies reach 72.6 percentage points.
  We introduce three operational concepts: computational latent politicization, describing the emergence of political categories without intentional encoding; emergent bias, irreducible to statistical or normative bias and detectable only through contrastive analysis; and three algorithmic scopic regimes: entropic (LAION), institutional (OpenAI), and semiotic (SigLIP), which structure distinct modes of visibility. Drawing on Foucault&#x27;s notion of the archive, Jameson&#x27;s ideologeme, and Simondon&#x27;s theory of individuation, we argue that training datasets function as quasi-archives whose discursive formations crystallize within latent space. This work contributes to a critical reassessment of the conditions under which VLMs are applied to digital art history and calls for methodologies that integrate learning architectures into any delegation of cultural interpretation to algorithmic agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>潜在空间的动物行为学研究</div>
<div class="mono" style="margin-top:8px">本研究通过采用动物行为学视角分析视觉语言模型（VLMs）的算法行为，挑战了其潜在空间具有预设中立性的观点。潜在空间并非均质的不确定性空间，而是展现出模型特定的算法敏感性——这种敏感性可理解为由训练数据和架构选择塑造的差异化感知突显机制。
通过对三个模型（OpenAI CLIP、OpenCLIP LAION、SigLIP）应用于301件艺术作品（15至20世纪）的对比分析，我们揭示了其在政治与文化类别归因上的显著分歧。利用基于向量类比（Mikolov et al., 2013）的双极语义轴分析显示：SigLIP将59.4%的作品归类为政治参与型，而OpenCLIP仅归类4%。非洲面具在SigLIP中获得最高政治评分，在OpenAI CLIP中却呈现非政治化特征。在美学殖民轴上，模型间差异达到72.6个百分点。
我们提出三个操作概念：计算性潜在政治化（描述政治类别在无意识编码下的涌现）、涌现性偏差（无法简化为统计或规范偏差，仅能通过对比分析检测）以及三种算法视域体制：熵化体制（LAION）、制度化体制（OpenAI）、符号化体制（SigLIP），它们构建了不同的可见性模式。借鉴福柯的档案理论、詹姆逊的意识形态素与西蒙东的个体化理论，我们认为训练数据集发挥着准档案功能，其话语形构在潜在空间中结晶。本研究推动了对VLMs应用于数字艺术史的条件进行批判性重估，并呼吁建立将学习架构整合至算法代理文化阐释委托的方法论体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research questions the assumed neutrality of latent spaces in vision-language models by examining their algorithmic behaviors from an ethological perspective, revealing that these spaces are not homogeneous but instead exhibit model-specific sensitivities shaped by training data and architecture. Through a comparative analysis of three models (OpenAI CLIP, OpenCLIP LAION, SigLIP) on 301 artworks, the study employs bipolar semantic axes from vector analogies to uncover significant divergences in political and cultural categorizations, such as SigLIP classifying 59.4% of artworks as politically engaged versus only 4% for OpenCLIP, with inter-model discrepancies reaching 72.6 percentage points on an aesthetic colonial axis. The findings introduce concepts like computational latent politicization and emergent bias, identifying three algorithmic scopic regimes—entropic, institutional, and semiotic—that structure distinct modes of visibility, ultimately arguing that training datasets function as quasi-archives whose discursive formations crystallize in latent space and calling for methodologies that integrate learning architectures in cultural interpretation delegations.</div>
<div class="mono" style="margin-top:8px">本研究从行为学视角质疑视觉语言模型中潜在空间的中立性假设，认为这些空间因训练数据和架构选择而形成模型特定的算法敏感性。方法上，对OpenAI CLIP、OpenCLIP LAION和SigLIP三种模型进行了比较分析，将其应用于301件15至20世纪的艺术作品，并通过向量类比衍生的双极语义轴评估政治与文化分类。主要实验结果显示显著差异：SigLIP将59.4%的作品归类为政治参与型，而OpenCLIP仅为4%；非洲面具在SigLIP中获得高政治评分，但在OpenAI CLIP中则无；在美学殖民轴线上，模型间差异达72.6个百分点，揭示了涌现性偏见和不同的算法视觉机制。</div>
</details>
</div>
<div class="card">
<div class="title">LoGoSeg: Integrating Local and Global Features for Open-Vocabulary Semantic Segmentation</div>
<div class="meta-line">Authors: Junyang Chen, Xiangbo Lv, Zhiqiang Kou, Xingdong Sheng, Ning Xu, Yiguo Qiao</div>
<div class="meta-line">First: 2026-02-05T12:03:11+00:00 · Latest: 2026-02-05T12:03:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05578v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05578v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary semantic segmentation (OVSS) extends traditional closed-set segmentation by enabling pixel-wise annotation for both seen and unseen categories using arbitrary textual descriptions. While existing methods leverage vision-language models (VLMs) like CLIP, their reliance on image-level pretraining often results in imprecise spatial alignment, leading to mismatched segmentations in ambiguous or cluttered scenes. However, most existing approaches lack strong object priors and region-level constraints, which can lead to object hallucination or missed detections, further degrading performance. To address these challenges, we propose LoGoSeg, an efficient single-stage framework that integrates three key innovations: (i) an object existence prior that dynamically weights relevant categories through global image-text similarity, effectively reducing hallucinations; (ii) a region-aware alignment module that establishes precise region-level visual-textual correspondences; and (iii) a dual-stream fusion mechanism that optimally combines local structural information with global semantic context. Unlike prior works, LoGoSeg eliminates the need for external mask proposals, additional backbones, or extra datasets, ensuring efficiency. Extensive experiments on six benchmarks (A-847, PC-459, A-150, PC-59, PAS-20, and PAS-20b) demonstrate its competitive performance and strong generalization in open-vocabulary settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LoGoSeg：融合局部与全局特征的开集词汇语义分割方法</div>
<div class="mono" style="margin-top:8px">开集词汇语义分割（OVSS）通过任意文本描述对可见与不可见类别进行像素级标注，拓展了传统闭集分割范式。现有方法虽利用CLIP等视觉语言模型，但其依赖图像级预训练常导致空间对齐不精确，在模糊或复杂场景中产生误分割。多数现有方法缺乏强目标先验和区域级约束，易引发目标幻觉或漏检，进一步影响性能。为此，我们提出LoGoSeg——一种高效的单阶段框架，集成三项关键创新：（1）通过全局图文相似度动态加权相关类别的目标存在先验，有效抑制幻觉；（2）建立精确区域级视觉-文本对应的区域感知对齐模块；（3）局部结构信息与全局语义上下文最优融合的双流机制。与先前工作不同，LoGoSeg无需外部掩码建议、额外骨干网络或附加数据集，兼顾高效性。在六个基准数据集（A-847、PC-459、A-150、PC-59、PAS-20及PAS-20b）上的大量实验表明，该方法在开集词汇场景中具有竞争力性能与强泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing open-vocabulary semantic segmentation methods, which often suffer from imprecise spatial alignment and object hallucination due to their reliance on image-level pretrained vision-language models. The proposed LoGoSeg framework introduces an object existence prior to reduce hallucinations, a region-aware alignment module for precise visual-textual correspondence, and a dual-stream fusion mechanism to combine local and global features, all within a single-stage architecture without requiring external proposals or extra data. Experiments on six benchmarks demonstrate the method&#x27;s competitive performance and strong generalization capability.</div>
<div class="mono" style="margin-top:8px">该研究针对开放词汇语义分割中，现有基于视觉语言模型的方法因依赖图像级预训练而导致空间对齐不精确和物体幻觉的问题。提出的LoGoSeg方法采用单阶段框架，集成了用于减少幻觉的物体存在先验、实现精确区域级视觉-文本对应的区域感知对齐模块，以及融合局部与全局特征的双流机制，且无需外部掩码提议或额外数据集。在六个基准测试上的实验结果表明，该模型在开放词汇设置中具有竞争性的性能和强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective</div>
<div class="meta-line">Authors: Haokui Zhang, Congyang Ou, Dawei Yan, Peng Wang, Qingsen Yan, Ying Li, Rong Xiao, Chunhua Shen</div>
<div class="meta-line">First: 2026-02-04T15:33:10+00:00 · Latest: 2026-02-05T12:00:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04657v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.04657v2">PDF</a> · <a href="https://github.com/ocy1/PIO-FVLM">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\times$ prefill speedup, 2.11$\times$ inference speedup, 6.22$\times$ lower FLOPs, and 6.05$\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PIO-FVLM：从推理目标视角重构视觉语言模型的免训练视觉令牌压缩加速方法</div>
<div class="mono" style="margin-top:8px">近期，通过减少视觉语言模型中的冗余视觉令牌以加速推理成为研究热点。然而，现有方法多基于视觉令牌间相似性或跨模态视觉-文本相似性构建启发式规则，在压缩性能与实际部署方面存在局限。本文从推理目标出发，提出PIO-FVLM方法，将视觉令牌压缩转化为保持输出结果不变性的问题，并依据令牌对该目标的重要性进行筛选。具体而言，通过设计的层局部代理损失（一种从当前层到最终结果的粗粒度约束）生成令牌级梯度显著性，以此指导视觉令牌重排序，再遵循非极大值抑制原则选取最具价值的视觉令牌。该方法无需训练、兼容FlashAttention，便于实际应用部署：既可独立作为无编码器方法使用，也可与VisionZip等编码器压缩方法结合。在LLaVA-Next-7B上，PIO-FVLM仅保留11.1%的视觉令牌即可维持97.2%的原始性能，实现预填充阶段2.67倍加速、推理阶段2.11倍加速、计算量降低6.22倍、KV缓存开销减少6.05倍。代码已开源：https://github.com/ocy1/PIO-FVLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To accelerate vision-language model inference by reducing redundant visual tokens, this work identifies limitations in existing heuristic methods based on token similarity. It proposes PIO-FVLM, a training-free method that reorients token compression toward preserving output invariance, using a layer-local proxy loss to compute token-level gradient saliency for reordering and non-maximum suppression for selection. Experiments on LLaVA-Next-7B show the method retains only 11.1% of visual tokens while maintaining 97.2% of original performance, achieving significant speedups (2.67× prefill, 2.11× inference) and efficiency gains (6.22× lower FLOPs, 6.05× reduced KV Cache).</div>
<div class="mono" style="margin-top:8px">为通过减少视觉语言模型中的冗余视觉令牌来加速推理，本研究指出基于令牌相似性的现有启发式方法存在局限。为此，提出了PIO-FVLM这一无需训练的方法，将令牌压缩重新定位为保持输出不变性，通过设计的层局部代理损失计算令牌级梯度显著性以进行重排序，并利用非极大值抑制进行选择。在LLaVA-Next-7B上的实验表明，该方法仅保留11.1%的视觉令牌，却能维持97.2%的原始性能，同时实现了显著的推理加速和计算开销降低。</div>
</details>
</div>
<div class="card">
<div class="title">VLN-Pilot: Large Vision-Language Model as an Autonomous Indoor Drone Operator</div>
<div class="meta-line">Authors: Bessie Dominguez-Dager, Sergio Suescun-Ferrandiz, Felix Escalona, Francisco Gomez-Donoso, Miguel Cazorla</div>
<div class="meta-line">First: 2026-02-05T11:23:11+00:00 · Latest: 2026-02-05T11:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05552v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05552v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces VLN-Pilot, a novel framework in which a large Vision-and-Language Model (VLLM) assumes the role of a human pilot for indoor drone navigation. By leveraging the multimodal reasoning abilities of VLLMs, VLN-Pilot interprets free-form natural language instructions and grounds them in visual observations to plan and execute drone trajectories in GPS-denied indoor environments. Unlike traditional rule-based or geometric path-planning approaches, our framework integrates language-driven semantic understanding with visual perception, enabling context-aware, high-level flight behaviors with minimal task-specific engineering. VLN-Pilot supports fully autonomous instruction-following for drones by reasoning about spatial relationships, obstacle avoidance, and dynamic reactivity to unforeseen events. We validate our framework on a custom photorealistic indoor simulation benchmark and demonstrate the ability of the VLLM-driven agent to achieve high success rates on complex instruction-following tasks, including long-horizon navigation with multiple semantic targets. Experimental results highlight the promise of replacing remote drone pilots with a language-guided autonomous agent, opening avenues for scalable, human-friendly control of indoor UAVs in tasks such as inspection, search-and-rescue, and facility monitoring. Our results suggest that VLLM-based pilots may dramatically reduce operator workload while improving safety and mission flexibility in constrained indoor environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLN-Pilot：大型视觉语言模型作为自主室内无人机操作员</div>
<div class="mono" style="margin-top:8px">本文介绍了VLN-Pilot，一种创新框架，其中大型视觉语言模型（VLLM）扮演室内无人机导航的人类飞行员角色。通过利用VLLM的多模态推理能力，VLN-Pilot解析自由形式的自然语言指令，并将其与视觉观察相结合，在无GPS的室内环境中规划并执行无人机轨迹。与传统基于规则或几何路径规划方法不同，该框架将语言驱动的语义理解与视觉感知相融合，实现了上下文感知的高层飞行行为，且无需大量任务特定工程。VLN-Pilot通过推理空间关系、避障及对突发事件的动态响应，支持无人机完全自主的指令跟随。我们在定制的高真实感室内仿真基准上验证了该框架，并证明了VLLM驱动智能体在复杂指令跟随任务（包括多语义目标的长时程导航）中实现高成功率的能力。实验结果凸显了用语言引导自主智能体替代远程无人机飞行员的潜力，为室内无人机在巡检、搜救和设施监控等任务中实现可扩展、人性化控制开辟了新途径。研究表明，基于VLLM的飞行员可显著降低操作员工作量，同时在受限室内环境中提升安全性与任务灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to replace human remote pilots for indoor drone operations by developing an autonomous agent that can understand high-level natural language commands. The proposed VLN-Pilot framework employs a large Vision-and-Language Model (VLLM) to interpret free-form instructions, ground them in visual observations from the drone, and directly plan and execute flight trajectories in GPS-denied environments, integrating semantic understanding with visual perception for context-aware navigation. Experiments on a custom photorealistic indoor simulation benchmark show that the VLLM-driven agent achieves high success rates on complex, long-horizon instruction-following tasks with multiple semantic targets, demonstrating its potential to reduce operator workload and improve safety and mission flexibility for applications like inspection and search-and-rescue.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发能够理解高级自然语言指令的自主智能体，以替代室内无人机操作中的人类远程飞行员。提出的VLN-Pilot框架采用大型视觉语言模型来解析自由形式的指令，将其与无人机摄像头的视觉观测相结合，直接在无GPS的室内环境中规划并执行飞行轨迹，实现了语义理解与视觉感知的融合以进行上下文感知导航。在定制的逼真室内模拟基准上的实验表明，该视觉语言模型驱动的智能体在包含多个语义目标的复杂长程指令跟随任务中取得了高成功率，证明了其在减少操作员工作量、提升安全检查与任务灵活性方面的潜力，适用于巡检、搜救等应用场景。</div>
</details>
</div>
<div class="card">
<div class="title">RANGER: A Monocular Zero-Shot Semantic Navigation Framework through Contextual Adaptation</div>
<div class="meta-line">Authors: Ming-Ming Yu, Yi Chen, Börje F. Karlsson, Wenjun Wu</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2025-12-30T13:25:22+00:00 · Latest: 2026-02-05T09:33:50+00:00</div>
<div class="meta-line">Comments: Accepted at ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24212v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24212v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently finding targets in complex environments is fundamental to real-world embodied applications. While recent advances in multimodal foundation models have enabled zero-shot object goal navigation, allowing robots to search for arbitrary objects without fine-tuning, existing methods face two key limitations: (1) heavy reliance on precise depth and pose information provided by simulators, which restricts applicability in real-world scenarios; and (2) lack of in-context learning (ICL) capability, making it difficult to quickly adapt to new environments, as in leveraging short videos. To address these challenges, we propose RANGER, a novel zero-shot, open-vocabulary semantic navigation framework that operates using only a monocular camera. Leveraging powerful 3D foundation models, RANGER eliminates the dependency on depth and pose while exhibiting strong ICL capability. By simply observing a short video of a new environment, the system can also significantly improve task efficiency without requiring architectural modifications or fine-tuning. The framework integrates several key components: keyframe-based 3D reconstruction, semantic point cloud generation, vision-language model (VLM)-driven exploration value estimation, high-level adaptive waypoint selection, and low-level action execution. Experiments on the HM3D benchmark and real-world environments demonstrate that RANGER achieves competitive performance in terms of navigation success rate and exploration efficiency, while showing superior ICL adaptability, with no previous 3D mapping of the environment required.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RANGER：一种通过上下文适应的单目零样本语义导航框架</div>
<div class="mono" style="margin-top:8px">在复杂环境中高效寻找目标是现实世界具身应用的基础。尽管多模态基础模型的最新进展已实现零样本目标导航，使机器人无需微调即可搜索任意物体，但现有方法面临两个关键局限：（1）严重依赖模拟器提供的精确深度与位姿信息，限制了现实场景的适用性；（2）缺乏上下文学习能力，难以快速适应新环境（如利用短视频）。为应对这些挑战，我们提出RANGER——一种仅使用单目相机的新型零样本开放词汇语义导航框架。该框架借助强大的3D基础模型，在消除对深度与位姿依赖的同时展现出强大的上下文学习能力。仅通过观察新环境的短视频，系统即可显著提升任务效率，且无需架构修改或微调。框架集成多个关键组件：基于关键帧的3D重建、语义点云生成、视觉语言模型驱动的探索价值估计、高层自适应路径点选择与底层动作执行。在HM3D基准测试和真实环境中的实验表明，RANGER在导航成功率和探索效率方面均取得优异性能，同时展现出卓越的上下文学习适应能力，且无需预先构建环境3D地图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in zero-shot object goal navigation, specifically the heavy reliance on precise depth and pose data from simulators and the lack of in-context learning ability for quick environmental adaptation. The proposed RANGER framework uses only a monocular camera, leveraging 3D foundation models for keyframe-based 3D reconstruction, semantic point cloud generation, and VLM-driven exploration to enable depth- and pose-free navigation with in-context learning. Experimental results on the HM3D benchmark and in real-world settings show that RANGER achieves competitive navigation success and exploration efficiency, with superior adaptability through short video observation without requiring prior 3D maps or fine-tuning.</div>
<div class="mono" style="margin-top:8px">该研究针对现有零样本目标导航方法的局限性，即过度依赖模拟器提供的精确深度和姿态信息，且缺乏快速适应新环境的上下文学习能力。提出的RANGER框架仅使用单目相机，利用3D基础模型消除对深度和姿态的依赖，并集成了基于关键帧的3D重建、语义点云生成、视觉语言模型驱动的探索价值估计、自适应路径点选择和底层动作执行等关键组件。在HM3D基准测试和真实环境中的实验结果表明，RANGER在导航成功率和探索效率方面达到了有竞争力的性能，同时展现出卓越的上下文学习适应性，仅通过观察短环境视频即可显著提升任务效率，无需修改架构或进行微调。</div>
</details>
</div>
<div class="card">
<div class="title">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</div>
<div class="meta-line">Authors: Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang</div>
<div class="meta-line">First: 2025-12-26T18:59:47+00:00 · Latest: 2026-02-05T08:49:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22120v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.22120v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少看而精看：面向多模态推理的双向感知塑形</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（VLM）常受益于中间视觉线索——无论是通过外部工具注入，还是在推理过程中生成为潜在视觉标记——但这些机制仍会忽略细粒度视觉证据（如图表中的折线），跨领域泛化能力较差，且推理成本高昂。本文提出双向感知塑形（BiPS），将问题条件化的掩码视图转化为双向的“关注何处”信号，在训练过程中塑造感知。BiPS首先在原始图像与仅保留问题相关区域的证据保留视图之间施加KL一致性约束，鼓励对支持像素进行粗略但完整的覆盖；随后在原始图像与掩码关键像素的证据消融视图之间施加KL分离约束（该视图不再支持原始答案），以抑制纯文本捷径（即仅从文本作答），并强化对细粒度视觉信息的依赖。在八个基准测试中，BiPS平均提升Qwen2.5-VL-7B模型性能8.2%，并对未见数据集与图像类型展现出强大的跨领域泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing vision-language models in capturing fine-grained visual evidence and avoiding text-only shortcuts, this paper introduces Bi-directional Perceptual Shaping (BiPS). The method shapes model perception during training by applying a KL-consistency constraint between the original image and an evidence-preserving view to ensure complete coverage of relevant pixels, and a KL-separation constraint between the original and an evidence-ablated view to enforce reliance on fine-grained visual details. Experimental results show that BiPS improves the Qwen2.5-VL-7B model by an average of 8.2% across eight benchmarks and demonstrates strong generalization to unseen datasets and image types.</div>
<div class="mono" style="margin-top:8px">针对现有视觉语言模型在捕捉细粒度视觉证据和避免仅依赖文本捷径方面的不足，本文提出了双向感知塑造（BiPS）方法。该方法通过双向感知信号，在原始图像与保留证据的视图之间施加KL一致性约束以确保对相关像素的粗略覆盖，并在原始图像与消除证据的视图之间施加KL分离约束以强制模型依赖细粒度视觉细节。实验结果表明，BiPS在八个基准测试上将Qwen2.5-VL-7B模型的性能平均提升了8.2%，并对未见过的数据集和图像类型展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Dolphin-v2: Universal Document Parsing via Scalable Anchor Prompting</div>
<div class="meta-line">Authors: Hao Feng, Wei Shi, Ke Zhang, Xiang Fei, Lei Liao, Dingkang Yang, Yongkun Du, Xuecheng Wu, Jingqun Tang, Yang Liu, Hong Chen, Can Huang</div>
<div class="meta-line">First: 2026-02-05T07:09:57+00:00 · Latest: 2026-02-05T07:09:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05384v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05384v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Document parsing has garnered widespread attention as vision-language models (VLMs) advance OCR capabilities. However, the field remains fragmented across dozens of specialized models with varying strengths, forcing users to navigate complex model selection and limiting system scalability. Moreover, existing two-stage approaches depend on axis-aligned bounding boxes for layout detection, failing to handle distorted or photographed documents effectively. To this end, we present Dolphin-v2, a two-stage document image parsing model that substantially improves upon the original Dolphin. In the first stage, Dolphin-v2 jointly performs document type classification (digital-born versus photographed) alongside layout analysis. For digital-born documents, it conducts finer-grained element detection with reading order prediction. In the second stage, we employ a hybrid parsing strategy: photographed documents are parsed holistically as complete pages to handle geometric distortions, while digital-born documents undergo element-wise parallel parsing guided by the detected layout anchors, enabling efficient content extraction. Compared with the original Dolphin, Dolphin-v2 introduces several crucial enhancements: (1) robust parsing of photographed documents via holistic page-level understanding, (2) finer-grained element detection (21 categories) with semantic attribute extraction such as author information and document metadata, and (3) code block recognition with indentation preservation, which existing systems typically lack. Comprehensive evaluations are conducted on DocPTBench, OmniDocBench, and our self-constructed RealDoc-160 benchmark. The results demonstrate substantial improvements: +14.78 points overall on the challenging OmniDocBench and 91% error reduction on photographed documents, while maintaining efficient inference through parallel processing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dolphin-v2：基于可扩展锚点提示的通用文档解析</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLM）提升OCR能力，文档解析领域受到广泛关注。然而，该领域仍被数十种各具优势的专用模型割裂，用户需应对复杂的模型选择问题，且系统可扩展性受限。现有两阶段方法依赖轴对齐边界框进行版面检测，难以有效处理扭曲或拍摄文档。为此，我们提出Dolphin-v2——一个两阶段文档图像解析模型，在原始Dolphin基础上实现显著改进。第一阶段，Dolphin-v2同步执行文档类型分类（数字原生文档与拍摄文档）与版面分析。针对数字原生文档，通过阅读顺序预测进行细粒度元素检测。第二阶段采用混合解析策略：拍摄文档通过整体页面级解析处理几何畸变，数字原生文档则在检测到的版面锚点引导下进行元素级并行解析，实现高效内容提取。相较于原始Dolphin，Dolphin-v2引入三项关键增强：（1）通过整体页面理解实现拍摄文档的鲁棒解析，（2）支持21类细粒度元素检测及作者信息、文档元数据等语义属性提取，（3）具备现有系统通常缺失的代码块识别与缩进保留功能。在DocPTBench、OmniDocBench及自建RealDoc-160基准上的综合评估表明：在挑战性OmniDocBench上整体提升14.78分，拍摄文档错误率降低91%，同时通过并行处理保持高效推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Document parsing suffers from fragmentation across specialized models and struggles with distorted photographed documents due to reliance on axis-aligned bounding boxes. This work introduces Dolphin-v2, a two-stage model that first classifies document type and performs layout analysis, then applies a hybrid parsing strategy: holistic page-level parsing for photographed documents and anchor-guided parallel element-wise parsing for digital-born documents. Experiments on DocPTBench, OmniDocBench, and RealDoc-160 show a +14.78 point overall improvement on OmniDocBench, a 91% error reduction on photographed documents, and efficient inference through parallel processing.</div>
<div class="mono" style="margin-top:8px">文档解析领域存在模型碎片化问题，且现有基于轴对齐边界框的两阶段方法难以有效处理扭曲的拍摄文档。Dolphin-v2提出一种两阶段方法：第一阶段联合执行文档类型分类（数字原生与拍摄）和布局分析；第二阶段采用混合解析策略，对拍摄文档进行整体页面级解析以处理几何畸变，对数字原生文档则基于检测到的布局锚点进行元素级并行解析。在DocPTBench、OmniDocBench和自建RealDoc-160基准上的综合评估表明，Dolphin-v2在OmniDocBench上整体提升14.78分，对拍摄文档的错误率降低91%，同时新增了细粒度元素检测（21类）、语义属性提取和保留缩进的代码块识别等能力。</div>
</details>
</div>
<div class="card">
<div class="title">VRIQ: Benchmarking and Analyzing Visual-Reasoning IQ of VLMs</div>
<div class="meta-line">Authors: Tina Khezresmaeilzadeh, Jike Zhong, Konstantinos Psounis</div>
<div class="meta-line">First: 2026-02-05T07:07:27+00:00 · Latest: 2026-02-05T07:07:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05382v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05382v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in Vision Language Models (VLMs) has raised the question of whether they can reliably perform nonverbal reasoning. To this end, we introduce VRIQ (Visual Reasoning IQ), a novel benchmark designed to assess and analyze the visual reasoning ability of VLMs. We evaluate models on two sets of tasks: abstract puzzle-style and natural-image reasoning tasks. We find that on abstract puzzles, performance remains near random with an average accuracy of around 28%, while natural tasks yield better but still weak results with 45% accuracy. We also find that tool-augmented reasoning demonstrates only modest improvements. To uncover the source of this weakness, we introduce diagnostic probes targeting perception and reasoning. Our analysis demonstrates that around 56% of failures arise from perception alone, 43% from both perception and reasoning, and only a mere 1% from reasoning alone. This motivates us to design fine-grained diagnostic probe questions targeting specific perception categories (e.g., shape, count, position, 3D/depth), revealing that certain categories cause more failures than others. Our benchmark and analysis establish that current VLMs, even with visual reasoning tools, remain unreliable abstract reasoners, mostly due to perception limitations, and offer a principled basis for improving visual reasoning in multimodal systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VRIQ：视觉语言模型视觉推理智商的基准测试与分析</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）的最新进展引发了关于其能否可靠执行非语言推理的疑问。为此，我们提出了VRIQ（视觉推理智商），这是一个旨在评估和分析VLMs视觉推理能力的新型基准。我们在两类任务上评估模型：抽象谜题式任务和自然图像推理任务。研究发现，在抽象谜题上，模型表现接近随机水平，平均准确率约为28%；而自然任务表现稍好但仍较弱，准确率为45%。我们还发现，工具增强的推理仅带来有限改进。为探究这一弱点的根源，我们引入了针对感知与推理的诊断探针。分析表明，约56%的失败仅源于感知问题，43%源于感知与推理的共同缺陷，仅1%单独由推理导致。这促使我们设计针对特定感知类别（如形状、数量、位置、3D/深度）的细粒度诊断探针问题，揭示某些类别导致的失败更多。我们的基准与分析表明，当前VLMs（即使配备视觉推理工具）仍不可靠作为抽象推理系统，主要受限于感知能力，并为改进多模态系统的视觉推理提供了原则性基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To systematically evaluate whether Vision Language Models (VLMs) can perform reliable nonverbal reasoning, this work introduces the VRIQ benchmark, which assesses models on both abstract puzzle-style and natural-image reasoning tasks. The method involves evaluating standard and tool-augmented VLMs on these tasks and employing diagnostic probes to isolate failures in perception versus reasoning. Experimental results show poor performance, with near-random accuracy (28%) on abstract puzzles and weak results (45%) on natural tasks, while tool augmentation yields only modest gains; diagnostic analysis reveals that the majority of failures (56%) stem from perception errors alone, 43% from combined perception-reasoning issues, and only 1% from pure reasoning, with specific perceptual categories like shape and count being particularly problematic.</div>
<div class="mono" style="margin-top:8px">该研究旨在评估视觉语言模型（VLMs）是否能够可靠地进行非语言推理，为此引入了VRIQ基准。方法包括在抽象谜题风格和自然图像推理任务上评估VLMs，并使用诊断探针分析感知和推理失败。主要实验结果表明，在抽象谜题上的表现接近随机（准确率约28%），而自然任务的结果虽较好但仍较弱（准确率45%），工具增强的推理仅带来有限改进；诊断分析显示，56%的失败源于感知单独问题，43%源于感知和推理共同问题，仅1%源于推理单独问题，表明感知限制是当前模型的主要弱点。</div>
</details>
</div>
<div class="card">
<div class="title">VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</div>
<div class="meta-line">Authors: Xinlei Yu, Chengming Xu, Guibin Zhang, Zhangquan Chen, Yudong Zhang, Yongbo He, Peng-Tao Jiang, Jiangning Zhang, Xiaobin Hu, Shuicheng Yan</div>
<div class="meta-line">First: 2025-11-14T06:51:34+00:00 · Latest: 2026-02-05T05:44:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11007v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11007v2">PDF</a> · <a href="https://github.com/YU-deep/VisMem.git">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a &quot;visual processing bottleneck&quot;: a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.0% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisMem：潜在视觉记忆解锁视觉语言模型潜力</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）取得了显著成功，但其在复杂视觉任务上的表现常受制于&#x27;视觉处理瓶颈&#x27;：即在长序列生成过程中易丧失视觉证据的锚定，并缺乏情境化的视觉经验。受人类认知记忆理论（区分短期视觉主导记忆与长期语义主导记忆）的启发，我们提出VisMem——一个认知对齐的框架，为VLMs配备动态潜在视觉记忆系统，包含用于细粒度感知保持的短期模块和用于抽象语义整合的长期模块。这些记忆在推理过程中被无缝调用，使VLMs能在思维与生成过程中同时保持感知保真度与语义一致性。在涵盖理解、推理和生成的多样化视觉基准测试中，VisMem相较原始模型实现平均11.0%的性能提升，并超越所有对比模型，确立了潜在空间记忆增强的新范式。代码已开源：https://github.com/YU-deep/VisMem.git。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) often suffer from a visual processing bottleneck, losing grounding in visual evidence and contextual experience during extended generation. To address this, the authors propose VisMem, a framework inspired by human cognitive memory that equips VLMs with dynamic latent vision memories, including a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation, which are invoked during inference to maintain perceptual fidelity and semantic consistency. Experiments across diverse visual understanding, reasoning, and generation benchmarks show that VisMem achieves an average performance improvement of 11.0% over the baseline model and outperforms all existing counterparts.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉语言模型在长序列生成中存在的视觉处理瓶颈问题，即模型容易丢失视觉细节和上下文经验。受人类记忆系统启发，提出的VisMem框架引入了动态潜在视觉记忆，包含用于保留细粒度感知细节的短期模块和用于巩固抽象语义的长期模块，在推理过程中调用以增强感知保真度和语义一致性。在多种视觉理解、推理和生成基准上的实验结果表明，VisMem相比基线模型平均性能提升11.0%，并超越了所有现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">GT-SVJ: Generative-Transformer-Based Self-Supervised Video Judge For Efficient Video Reward Modeling</div>
<div class="meta-line">Authors: Shivanshu Shekhar, Uttaran Bhattacharya, Raghavendra Addanki, Mehrab Tanjim, Somdeb Sarkhel, Tong Zhang</div>
<div class="meta-line">First: 2026-02-05T01:54:01+00:00 · Latest: 2026-02-05T01:54:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05202v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05202v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Aligning video generative models with human preferences remains challenging: current approaches rely on Vision-Language Models (VLMs) for reward modeling, but these models struggle to capture subtle temporal dynamics. We propose a fundamentally different approach: repurposing video generative models, which are inherently designed to model temporal structure, as reward models. We present the Generative-Transformer-based Self-Supervised Video Judge (\modelname), a novel evaluation model that transforms state-of-the-art video generation models into powerful temporally-aware reward models. Our key insight is that generative models can be reformulated as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, enabling them to discriminate video quality with remarkable precision when trained via contrastive objectives. To prevent the model from exploiting superficial differences between real and generated videos, we design challenging synthetic negative videos through controlled latent-space perturbations: temporal slicing, feature swapping, and frame shuffling, which simulate realistic but subtle visual degradations. This forces the model to learn meaningful spatiotemporal features rather than trivial artifacts. \modelname achieves state-of-the-art performance on GenAI-Bench and MonteBench using only 30K human-annotations: $6\times$ to $65\times$ fewer than existing VLM-based approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GT-SVJ：基于生成式Transformer的自监督视频评判器——面向高效视频奖励建模</div>
<div class="mono" style="margin-top:8px">将视频生成模型与人类偏好对齐仍具挑战：现有方法依赖视觉语言模型进行奖励建模，但这些模型难以捕捉细微的时间动态。我们提出一种根本不同的方法：将本身为建模时间结构而设计的视频生成模型改造为奖励模型。我们提出基于生成式Transformer的自监督视频评判器，这是一种将前沿视频生成模型转化为强大时间感知奖励模型的新型评估模型。核心洞见在于：生成模型可重构为基于能量的模型——高质量视频被赋予低能量值，劣质视频被赋予高能量值，通过对比目标训练后能以显著精度判别视频质量。为防止模型利用真实视频与生成视频间的表面差异，我们通过受控潜空间扰动设计具有挑战性的合成负样本视频：时间切片、特征交换和帧重排，以模拟真实而细微的视觉退化。这迫使模型学习有意义的时空特征而非琐碎伪影。该模型仅用3万条人工标注即在GenAI-Bench和MonteBench上达到最优性能，标注量仅为现有基于视觉语言模型方法的1/6至1/65。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of aligning video generative models with human preferences, noting that current Vision-Language Model (VLM)-based reward models fail to capture subtle temporal dynamics. The method proposes repurposing video generative models as reward models by reformulating them as energy-based models (EBMs) that assign low energy to high-quality videos and high energy to degraded ones, trained via contrastive objectives with synthetic negative videos created through latent-space perturbations like temporal slicing and frame shuffling to force learning of meaningful spatiotemporal features. Key experimental results show that the model achieves state-of-the-art performance on GenAI-Bench and MonteBench benchmarks using only 30K human annotations, which is 6 to 65 times fewer than existing VLM-based approaches.</div>
<div class="mono" style="margin-top:8px">该研究针对视频生成模型与人类偏好对齐的挑战，指出当前基于视觉语言模型（VLM）的奖励模型难以捕捉细微的时间动态。方法提出将视频生成模型重新用作奖励模型，通过将其重构为基于能量的模型（EBM），为高质量视频分配低能量、为低质量视频分配高能量，并采用对比目标进行训练，同时通过潜在空间扰动（如时间切片和帧重排）创建合成负样本视频，以迫使模型学习有意义的时空特征。关键实验结果表明，该模型在GenAI-Bench和MonteBench基准测试中取得了最先进的性能，仅使用了3万个人工标注，比现有基于VLM的方法减少了6至65倍。</div>
</details>
</div>
<div class="card">
<div class="title">ARGaze: Autoregressive Transformers for Online Egocentric Gaze Estimation</div>
<div class="meta-line">Authors: Jia Li, Wenjie Zhao, Shijian Deng, Bolin Lai, Yuheng Wu, RUijia Chen, Jon E. Froehlich, Yuhang Zhao, Yapeng Tian</div>
<div class="meta-line">First: 2026-02-04T23:33:16+00:00 · Latest: 2026-02-04T23:33:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05132v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05132v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online egocentric gaze estimation predicts where a camera wearer is looking from first-person video using only past and current frames, a task essential for augmented reality and assistive technologies. Unlike third-person gaze estimation, this setting lacks explicit head or eye signals, requiring models to infer current visual attention from sparse, indirect cues such as hand-object interactions and salient scene content. We observe that gaze exhibits strong temporal continuity during goal-directed activities: knowing where a person looked recently provides a powerful prior for predicting where they look next. Inspired by vision-conditioned autoregressive decoding in vision-language models, we propose ARGaze, which reformulates gaze estimation as sequential prediction: at each timestep, a transformer decoder predicts current gaze by conditioning on (i) current visual features and (ii) a fixed-length Gaze Context Window of recent gaze target estimates. This design enforces causality and enables bounded-resource streaming inference. We achieve state-of-the-art performance across multiple egocentric benchmarks under online evaluation, with extensive ablations validating that autoregressive modeling with bounded gaze history is critical for robust prediction. We will release our source code and pre-trained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARGaze：用于在线第一人称视线估计的自回归Transformer</div>
<div class="mono" style="margin-top:8px">在线第一人称视线估计仅利用过去和当前帧，从第一人称视频中预测佩戴相机者的注视位置，这对增强现实和辅助技术至关重要。与第三人称视线估计不同，该场景缺乏明确的头部或眼部信号，需通过手物交互和显著场景内容等稀疏间接线索推断当前视觉注意力。我们观察到，在目标导向活动中视线具有强时间连续性：了解近期注视位置可为预测下一注视点提供有力先验。受视觉语言模型中视觉条件自回归解码的启发，我们提出ARGaze，将视线估计重构为序列预测：在每个时间步，Transformer解码器通过结合（i）当前视觉特征与（ii）固定长度的近期注视目标估计值（视线上下文窗口）来预测当前视线。该设计确保因果性并支持有限资源流式推理。我们在在线评估的多个第一人称基准测试中取得最先进性能，大量消融实验验证了有限视线历史的自回归建模对鲁棒预测的关键作用。我们将公开源代码与预训练模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of online egocentric gaze estimation, which predicts a camera wearer&#x27;s visual attention from first-person video using only past and current frames, a task crucial for augmented reality and assistive technologies. The method, ARGaze, reformulates gaze estimation as a sequential prediction problem using an autoregressive transformer decoder; at each timestep, it conditions predictions on current visual features and a fixed-length Gaze Context Window of recent gaze target estimates to enforce causality and enable streaming inference. Experimental results demonstrate state-of-the-art performance on multiple egocentric benchmarks under online evaluation, with ablations confirming that autoregressive modeling with bounded gaze history is essential for robust prediction.</div>
<div class="mono" style="margin-top:8px">在线第一人称视线估计旨在仅使用过去和当前帧从第一人称视频中预测佩戴者的视觉注意力，这对增强现实等应用至关重要，但由于缺乏明确的头部或眼部信号而具有挑战性。为此，作者提出了ARGaze方法，该方法使用自回归变换器解码器将视线估计重新定义为序列预测任务；在每个时间步，模型基于当前视觉特征和固定长度的近期视线估计上下文窗口进行预测，从而确保因果性并实现高效的流式推理。实验结果表明，该方法在多个第一人称基准测试的在线评估中取得了最先进的性能，消融研究证实了具有有限视线历史的自回归建模对于鲁棒预测的关键作用。</div>
</details>
</div>
<div class="card">
<div class="title">All You Need for Object Detection: From Pixels, Points, and Prompts to Next-Gen Fusion and Multimodal LLMs/VLMs in Autonomous Vehicles</div>
<div class="meta-line">Authors: Sayed Pedram Haeri Boroujeni, Niloufar Mehrabi, Hazim Alzorgan, Mahlagha Fazeli, Abolfazl Razi</div>
<div class="meta-line">First: 2025-10-30T16:08:25+00:00 · Latest: 2026-02-04T21:42:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26641v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.26641v3">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous Vehicles (AVs) are transforming the future of transportation through advances in intelligent perception, decision-making, and control systems. However, their success is tied to one core capability, reliable object detection in complex and multimodal environments. While recent breakthroughs in Computer Vision (CV) and Artificial Intelligence (AI) have driven remarkable progress, the field still faces a critical challenge as knowledge remains fragmented across multimodal perception, contextual reasoning, and cooperative intelligence. This survey bridges that gap by delivering a forward-looking analysis of object detection in AVs, emphasizing emerging paradigms such as Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI rather than re-examining outdated techniques. We begin by systematically reviewing the fundamental spectrum of AV sensors (camera, ultrasonic, LiDAR, and Radar) and their fusion strategies, highlighting not only their capabilities and limitations in dynamic driving environments but also their potential to integrate with recent advances in LLM/VLM-driven perception frameworks. Next, we introduce a structured categorization of AV datasets that moves beyond simple collections, positioning ego-vehicle, infrastructure-based, and cooperative datasets (e.g., V2V, V2I, V2X, I2I), followed by a cross-analysis of data structures and characteristics. Ultimately, we analyze cutting-edge detection methodologies, ranging from 2D and 3D pipelines to hybrid sensor fusion, with particular attention to emerging transformer-driven approaches powered by Vision Transformers (ViTs), Large and Small Language Models (SLMs), and VLMs. By synthesizing these perspectives, our survey delivers a clear roadmap of current capabilities, open challenges, and future opportunities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶车辆目标检测全要素：从像素、点云与提示到新一代融合技术与多模态大语言模型/视觉语言模型</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆正通过智能感知、决策与控制系统的进步重塑未来交通格局，其核心成功要素在于复杂多模态环境中可靠的目标检测能力。尽管计算机视觉与人工智能领域近期取得突破性进展，但多模态感知、情境推理与协同智能的知识体系仍呈碎片化状态。本综述通过前瞻性分析弥合这一鸿沟，聚焦视觉语言模型、大语言模型与生成式人工智能等新兴范式，而非重述陈旧技术。首先系统梳理自动驾驶基础传感器谱系（摄像头、超声波、激光雷达、毫米波雷达）及其融合策略，既阐明其在动态驾驶环境中的能力与局限，也揭示其与LLM/VLM驱动感知框架的整合潜力；继而提出超越简单数据集的层级化分类体系，涵盖自车视角、基础设施与协同数据集（如V2V、V2I、V2X、I2I），并对其数据结构与特征进行交叉分析；最终解析从二维/三维检测流程到混合传感器融合的前沿方法，特别关注由视觉Transformer、大小语言模型及VLMs驱动的Transformer新兴范式。通过整合多维视角，本研究清晰勾勒出现有能力图谱、开放挑战与未来机遇。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey addresses the fragmented knowledge in autonomous vehicle object detection by providing a forward-looking analysis that emphasizes emerging paradigms like Vision-Language Models (VLMs), Large Language Models (LLMs), and Generative AI, rather than revisiting outdated techniques. The method involves a systematic review of fundamental AV sensors and their fusion strategies, a structured categorization of datasets including ego-vehicle and cooperative types, and an analysis of cutting-edge detection methodologies from 2D/3D pipelines to transformer-driven approaches. Key findings highlight the potential integration of sensor data with LLM/VLM frameworks and map out current capabilities, open challenges, and future opportunities in the field.</div>
<div class="mono" style="margin-top:8px">本综述针对自动驾驶目标检测领域知识碎片化的问题，通过前瞻性分析，重点探讨了视觉语言模型、大语言模型和生成式人工智能等新兴范式，而非重新审视过时技术。研究方法包括系统回顾自动驾驶基本传感器及其融合策略，对自车、基于基础设施和协同数据集进行结构化分类，并分析了从2D/3D检测流程到基于Transformer的先进检测方法。主要成果综合了这些视角，为该领域当前能力、开放挑战和未来机遇提供了清晰的路线图。</div>
</details>
</div>
<div class="card">
<div class="title">VISTA: Enhancing Visual Conditioning via Track-Following Preference Optimization in Vision-Language-Action Models</div>
<div class="meta-line">Authors: Yiye Chen, Yanan Jian, Xiaoyi Dong, Shuxin Cao, Jing Wu, Patricio Vela, Benjamin E. Lundell, Dongdong Chen</div>
<div class="meta-line">First: 2026-02-04T20:59:29+00:00 · Latest: 2026-02-04T20:59:29+00:00</div>
<div class="meta-line">Comments: In submission. Project website: https://vista-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05049v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.05049v1">PDF</a> · <a href="https://vista-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have demonstrated strong performance across a wide range of robotic manipulation tasks. Despite the success, extending large pretrained Vision-Language Models (VLMs) to the action space can induce vision-action misalignment, where action predictions exhibit weak dependence on the current visual state, leading to unreliable action outputs. In this work, we study VLA models through the lens of visual conditioning and empirically show that successful rollouts consistently exhibit stronger visual dependence than failed ones. Motivated by this observation, we propose a training framework that explicitly strengthens visual conditioning in VLA models. Our approach first aligns action prediction with visual input via preference optimization on a track-following surrogate task, and then transfers the enhanced alignment to instruction-following task through latent-space distillation during supervised finetuning. Without introducing architectural modifications or additional data collection, our method improves both visual conditioning and task performance for discrete OpenVLA, and further yields consistent gains when extended to the continuous OpenVLA-OFT setting. Project website: https://vista-vla.github.io/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VISTA：通过轨迹跟随偏好优化增强视觉-语言-动作模型中的视觉条件化</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型已在多种机器人操作任务中展现出优异性能。然而，将大型预训练视觉-语言模型（VLM）扩展至动作空间时，可能引发视觉-动作失准问题，即动作预测对当前视觉状态的依赖较弱，导致输出动作不可靠。本研究从视觉条件化视角分析VLA模型，并通过实验证明：成功的任务执行轨迹始终比失败轨迹表现出更强的视觉依赖性。基于此发现，我们提出一种显式增强VLA模型视觉条件化的训练框架。该方法首先通过轨迹跟随代理任务的偏好优化实现动作预测与视觉输入的对齐，随后在监督微调阶段通过潜空间蒸馏将增强的对齐能力迁移至指令跟随任务。在不引入架构修改或额外数据收集的情况下，本方法不仅提升了离散型OpenVLA的视觉条件化能力与任务性能，在扩展至连续型OpenVLA-OFT设定时亦能获得持续增益。项目网站：https://vista-vla.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vision-action misalignment in Vision-Language-Action (VLA) models, where action predictions often show weak dependence on the current visual state, leading to unreliable outputs. The method proposes a training framework that first aligns action with visual input via preference optimization on a track-following surrogate task, and then transfers this enhanced alignment to instruction-following tasks through latent-space distillation during supervised finetuning. Experimental results show that this approach, without architectural changes or extra data, improves visual conditioning and task performance for discrete OpenVLA and yields consistent gains when extended to the continuous OpenVLA-OFT setting.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，将预训练的视觉语言模型扩展到动作空间时会出现视觉-动作错位，导致对视觉状态的依赖较弱，从而产生不可靠的机器人操作。所提出的VISTA方法通过偏好优化显式增强视觉条件作用：首先在轨迹跟踪的代理任务上对齐动作预测与视觉输入，然后在监督微调期间通过潜在空间蒸馏将这种增强的对齐能力迁移到指令跟随任务中。实验结果表明，该训练框架无需修改架构或收集新数据，即可改善离散OpenVLA的视觉条件作用和任务性能，并在扩展到连续OpenVLA-OFT设置时获得一致的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">When LLaVA Meets Objects: Token Composition for Vision-Language-Models</div>
<div class="meta-line">Authors: Soumya Jahagirdar, Walid Bousselham, Anna Kukleva, Hilde Kuehne</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04864v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当LLaVA遇见物体：视觉语言模型的令牌组合方法</div>
<div class="mono" style="margin-top:8px">当前自回归视觉语言模型通常依赖大量视觉令牌表示图像，导致推理时计算需求较高。为解决此问题，我们提出Mask-LLaVA框架，利用多层级视觉特征构建紧凑而信息丰富的视觉表示。具体而言，我们将基于掩码的物体表征与全局令牌及局部补丁令牌相结合。训练阶段使用全部令牌，而测试阶段模型可灵活减少基于掩码的物体令牌数量，实现无需重新训练即可动态调整推理令牌数且性能无明显下降。在标准基准测试中，该方法仅用少量视觉令牌即取得与当前令牌高效方法相当、且与原版LLaVA基线可比的结果。分析表明，多层级特征组合能以较少令牌实现高效学习，并在测试时支持动态令牌选择以保持良好性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high computational cost of autoregressive Vision Language Models (VLMs) caused by their reliance on numerous visual tokens, this work introduces Mask-LLaVA, a framework that creates compact visual representations by combining multi-level features: mask-based object tokens, global tokens, and local patch tokens. The method trains the model with all tokens but enables flexible reduction, particularly of object tokens, during inference without retraining. Experimental results on standard benchmarks show performance competitive with other token-efficient methods and comparable to the original LLaVA baseline while using only a fraction of the visual tokens, demonstrating that multi-level feature integration supports efficient learning and dynamic token selection for maintained performance.</div>
<div class="mono" style="margin-top:8px">为解决自回归视觉语言模型因依赖大量视觉标记而导致计算成本高的问题，本研究提出了Mask-LLaVA框架，该框架通过结合掩码对象标记、全局标记和局部图像块标记等多层次特征，来创建紧凑而信息丰富的视觉表示。训练时使用所有类型的标记，但在推理时可以灵活地丢弃大部分基于掩码的对象标记，从而无需重新训练即可动态调整标记数量。在标准基准测试上的实验结果表明，该方法仅使用一小部分视觉标记，性能即可与其他高效的标记方法竞争，并与原始LLaVA基线相当，证明了多层次特征组合能够支持高效学习与动态标记选择，以保持良好性能。</div>
</details>
</div>
<div class="card">
<div class="title">VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?</div>
<div class="meta-line">Authors: Qing&#x27;an Liu, Juntong Feng, Yuhao Wang, Xinzhe Han, Yujie Cheng, Yue Zhu, Haiwen Diao, Yunzhi Zhuge, Huchuan Lu</div>
<div class="meta-line">First: 2026-02-04T17:48:55+00:00 · Latest: 2026-02-04T17:48:55+00:00</div>
<div class="meta-line">Comments: 27 pages, 19 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04802v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04802v1">PDF</a> · <a href="https://github.com/QingAnLiu/VISTA-Bench">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VISTA-Bench：视觉语言模型真的能像理解纯文本一样理解可视化文本吗？</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在跨文本与视觉模态的理解任务中取得了显著性能，但现有基准主要关注纯文本查询。在实际场景中，语言也常以图像中嵌入的可视化文本形式出现，这引发了对当前VLMs能否同等处理此类输入请求的疑问。我们提出了VISTA-Bench——一个涵盖多模态感知、推理到单模态理解领域的系统性基准。该基准通过在受控渲染条件下对比纯文本与可视化文本问题，评估模型对可视化文本的理解能力。对20余个代表性VLMs的大规模评估揭示出显著的模态鸿沟：在纯文本查询中表现优异的模型，当相同语义内容以可视化文本形式呈现时，性能往往大幅下降。感知难度的增加会进一步放大这一鸿沟，表明模型对渲染变化具有敏感性，尽管语义内容保持不变。总体而言，VISTA-Bench提供了一个原则性评估框架，可用于诊断这一局限性，并指导模型在符号化文本与像素层面实现更统一的语言表征。源数据集发布于https://github.com/QingAnLiu/VISTA-Bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the gap in evaluating Vision-Language Models (VLMs) on their ability to understand visualized text embedded in images, as opposed to pure text, which is common in real-world scenarios but underrepresented in existing benchmarks. The authors introduce VISTA-Bench, a systematic benchmark that contrasts pure-text and visualized-text questions across multimodal perception, reasoning, and unimodal understanding domains under controlled rendering conditions. Experiments with over 20 VLMs reveal a significant modality gap, where models perform substantially worse on visualized text, with the performance degradation increasing with perceptual difficulty, indicating sensitivity to rendering variations despite unchanged semantic content.</div>
<div class="mono" style="margin-top:8px">当前的视觉语言模型在纯文本查询上表现出色，但对于图像中嵌入的可视化文本（一种常见的现实场景）的处理能力尚不明确。为此，研究者提出了VISTA-Bench，这是一个系统性基准，通过对比模型在受控渲染条件下对纯文本与可视化文本问题的响应，评估其在多模态感知、推理和单模态理解领域的表现。对超过20个代表性模型的广泛测试揭示了一个显著的模态差距：尽管语义内容相同，模型在可视化文本上的性能大幅下降，且随着感知难度的增加，这一差距进一步扩大，表明模型对渲染变化非常敏感。</div>
</details>
</div>
<div class="card">
<div class="title">Annotation Free Spacecraft Detection and Segmentation using Vision Language Models</div>
<div class="meta-line">Authors: Samet Hicsonmez, Jose Sosa, Dan Pineau, Inder Pal Singh, Arunkumar Rathinam, Abd El Rahman Shabayek, Djamila Aouada</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-04T16:07:29+00:00 · Latest: 2026-02-04T16:07:29+00:00</div>
<div class="meta-line">Comments: ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04699v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04699v1">PDF</a> · <a href="https://github.com/giddyyupp/annotation-free-spacecraft-segmentation">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的免标注航天器检测与分割方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在开放世界零样本视觉识别任务中展现出卓越性能，但其在航天领域的应用潜力尚未得到充分探索。在航天场景中，由于低可见度、光照变化及目标与行星背景融合等因素，精确的人工标注尤为困难。因此，开发无需大量人工标注即可检测与分割航天器及轨道目标的方法至关重要。本研究提出一种基于VLM的免标注空间目标检测与分割流程：首先利用预训练VLM自动为少量未标注真实数据生成伪标签，随后通过师生标签蒸馏框架训练轻量化模型。尽管伪标签存在固有噪声，但蒸馏过程相比直接零样本VLM推理仍带来显著性能提升。在SPARK-2024、SPEED+和TANGO数据集上的分割实验表明，平均精度（AP）最高可提升10个百分点。代码与模型已开源：https://github.com/giddyyupp/annotation-free-spacecraft-segmentation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting and segmenting spacecraft in space imagery where manual annotation is difficult due to low visibility and background blending. The method employs a pre-trained Vision Language Model (VLM) to automatically generate pseudo-labels for a small unlabeled dataset, which are then used in a teacher-student distillation framework to train lightweight models. Experiments on SPARK-2024, SPEED+, and TANGO datasets show that this approach improves average precision by up to 10 points over direct zero-shot VLM inference, demonstrating effective annotation-free performance.</div>
<div class="mono" style="margin-top:8px">本研究针对太空图像中航天器检测与分割的标注难题，该难题源于低可见度、光照变化和背景融合等因素。方法提出了一种无需标注的流程：首先使用预训练的视觉语言模型为少量未标注数据生成伪标签，然后通过师生蒸馏框架训练轻量级模型。在SPARK-2024、SPEED+和TANGO数据集上的实验结果表明，该方法显著优于直接的零样本视觉语言模型推理，在分割任务的平均精度上实现了最高10个百分点的稳定提升。</div>
</details>
</div>
<div class="card">
<div class="title">AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation</div>
<div class="meta-line">Authors: Jin-Chuan Shi, Binhong Ye, Tao Liu, Junzhe He, Yangjinhui Xu, Xiaoyang Liu, Zeju Li, Hao Chen, Chunhua Shen</div>
<div class="meta-line">First: 2026-02-04T15:42:58+00:00 · Latest: 2026-02-04T15:42:58+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04672v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04672v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AGILE：基于智能体生成机制从视频重建手-物交互</div>
<div class="mono" style="margin-top:8px">从单目视频重建动态手-物交互对于灵巧操作数据采集以及为机器人与VR创建真实数字孪生体至关重要。然而现有方法面临两大瓶颈：(1)依赖神经渲染在严重遮挡下常产生破碎、无法直接用于仿真的几何体；(2)依赖脆弱的运动恢复结构初始化导致对真实场景视频频繁失效。为突破这些限制，我们提出AGILE框架，将范式从重建转向面向交互学习的智能体生成。首先采用智能体流程：视觉语言模型引导生成模型合成完整、封闭的高保真纹理物体网格，不受视频遮挡影响。其次完全绕过脆弱的SfM，提出鲁棒的锚点跟踪策略：通过基础模型在单帧交互起始帧初始化物体位姿，并利用生成资产与视频观测间的强视觉相似性进行时序传播。最后通过接触感知优化整合语义、几何与交互稳定性约束以确保物理合理性。在HO3D、DexYCB及真实场景视频上的大量实验表明，AGILE在全局几何精度上超越基线方法，且在现有方法常失效的挑战性序列中展现卓越鲁棒性。通过优先保障物理有效性，本方法产出可直接用于仿真的资产，并已通过机器人应用的实-仿重定向验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing methods for reconstructing dynamic hand-object interactions from monocular videos, which often produce fragmented geometries unsuitable for simulation and rely on brittle initialization techniques that fail on in-the-wild footage. The proposed AGILE framework shifts from traditional reconstruction to an agentic generation paradigm, employing a Vision-Language Model to guide the synthesis of complete, watertight object meshes and a robust anchor-and-track strategy that bypasses Structure-from-Motion by initializing pose with a foundation model and propagating it temporally. Experimental results on HO3D, DexYCB, and in-the-wild videos demonstrate that AGILE outperforms baselines in geometric accuracy and shows exceptional robustness on challenging sequences where prior methods fail, producing simulation-ready assets validated through real-to-sim retargeting for robotics.</div>
<div class="mono" style="margin-top:8px">本研究针对现有从单目视频重建动态手物交互方法存在的局限性，这些方法因依赖神经渲染和脆弱的运动恢复结构初始化，常产生破碎几何且在野外视频上频繁失败。提出的AGILE框架将范式转向智能体生成，利用视觉语言模型引导合成完整、密封的物体网格，通过稳健的锚定跟踪姿态估计策略绕过运动恢复结构，并采用接触感知优化确保物理合理性。在HO3D、DexYCB和野外视频上的大量实验表明，AGILE在几何精度上优于基线方法，在先前方法常失效的挑战性序列上表现出卓越的鲁棒性，生成了经过机器人重定向验证的仿真就绪资产。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Degradation with Vision Language Model</div>
<div class="meta-line">Authors: Guanzhou Lan, Chenyi Liao, Yuqi Yang, Qianli Ma, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li</div>
<div class="meta-line">First: 2026-02-04T13:51:15+00:00 · Latest: 2026-02-04T13:51:15+00:00</div>
<div class="meta-line">Comments: 17 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04565v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04565v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的退化理解研究</div>
<div class="mono" style="margin-top:8px">理解视觉退化是计算机视觉中关键而具挑战性的问题。尽管当前视觉语言模型在定性描述方面表现优异，却常难以理解图像退化背后的参数化物理机制。本研究将退化理解重新定义为层次化结构化预测任务，要求同时估计退化类型、参数键及其连续物理值。虽然这些子任务处于不同空间，我们证明其可通过自回归下一词元预测范式统一实现，其误差受值空间量化网格约束。基于此，我们提出DU-VLM——一种采用监督微调与结构化奖励强化学习训练的多模态思维链模型。进一步研究表明，DU-VLM可作为预训练扩散模型的零样本控制器，无需微调生成主干即可实现高保真图像复原。同时，我们构建了包含11万组清洁-退化图像对及物理标注的大规模数据集\textbf{DU-110k}。大量实验表明，该方法在精度与鲁棒性上显著超越通用基线模型，并展现出对未见分布的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling vision-language models to understand the parametric physics of image degradations, moving beyond qualitative descriptions. The authors propose a hierarchical structured prediction task that unifies degradation type, parameter key, and continuous value estimation under a single autoregressive next-token prediction paradigm, with bounded error. Their method, DU-VLM, is a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards, and it can also act as a zero-shot controller for pre-trained diffusion models for image restoration. Experimental results on the newly introduced DU-110k dataset show the approach significantly outperforms generalist baselines in accuracy and robustness, with strong generalization to unseen distributions.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决让视觉语言模型理解图像退化的参数化物理原理这一挑战，超越定性描述。作者提出了一种分层结构化预测任务，将退化类型、参数键和连续物理值的估计统一在自回归的下一个令牌预测范式下，并具有有界误差。他们引入了通过监督微调和带有结构化奖励的强化学习训练的DU-VLM模型，以及一个新的数据集DU-110k。实验结果表明，DU-VLM在准确性和鲁棒性上显著优于通用基线，能泛化到未见过的分布，并且可以作为预训练扩散模型的零样本控制器来执行图像恢复。</div>
</details>
</div>
<div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：基于视觉语言模型将人形机器人任务规划空间感知地融入以自我为中心的动作中</div>
<div class="mono" style="margin-top:8px">在现实环境中部署人形机器人面临根本性挑战，因其需要在部分信息观测和动态变化环境下，紧密整合感知、移动与操作能力，并稳健实现不同类型子任务间的过渡。为应对这些挑战，我们提出一项新任务——EgoActing，要求将高层指令直接映射为多样化、精确且具有空间感知的人形机器人动作。我们进一步通过引入EgoActor实例化该任务：这是一个统一且可扩展的视觉语言模型（VLM），能够预测移动基元（如行走、转向、侧移、高度调整）、头部运动、操作指令及人机交互行为，以实时协调感知与执行。我们利用来自真实世界演示的纯RGB自我中心数据、空间推理问答及模拟环境演示进行广泛监督，使EgoActor（包括80亿和40亿参数版本）能在1秒内做出稳健的上下文感知决策并执行流畅的动作推断。在模拟和真实环境中的大量实验表明，EgoActor有效衔接了抽象任务规划与具体运动执行，并能泛化至多样化任务及未见环境。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deploying humanoid robots in real world environments is challenging due to the need to integrate perception, locomotion, and manipulation under partial observations and dynamic conditions. To address this, the authors propose the EgoActing task and introduce EgoActor, a unified vision-language model that grounds high-level instructions into spatially-aware egocentric actions, including locomotion primitives, head movements, and manipulation commands. The model is trained with broad supervision from real-world egocentric RGB demonstrations, spatial reasoning QA, and simulated data, enabling robust, context-aware decision-making and fluent real-time inference. Experimental results in both simulated and real-world settings show that EgoActor effectively bridges abstract task planning with concrete motor execution and generalizes to diverse, unseen tasks and environments.</div>
<div class="mono" style="margin-top:8px">在现实世界中部署人形机器人面临根本性挑战，因其需要在部分观测和动态变化的环境下，紧密整合感知、移动和操作能力。为应对这些挑战，本研究提出了EgoActing任务，旨在将高层指令直接转化为精确且具有空间感知的人形机器人动作，并引入了EgoActor模型，这是一个统一的视觉-语言模型，能够预测移动基元、头部运动、操作命令和人机交互，以实现实时感知与执行的协调。该模型利用来自真实世界第一人称RGB演示、空间推理问答以及模拟环境演示的广泛监督进行训练，从而能够做出鲁棒的、上下文感知的决策，并在1秒内完成流畅的动作推理。在模拟和真实环境中的广泛评估表明，EgoActor有效地连接了抽象任务规划与具体运动执行，并能泛化到多样化的任务和未见过的环境中。</div>
</details>
</div>
<div class="card">
<div class="title">OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</div>
<div class="meta-line">Authors: Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng</div>
<div class="meta-line">First: 2026-01-29T12:43:02+00:00 · Latest: 2026-02-04T12:53:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21639v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21639v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OCRVerse：迈向端到端视觉语言模型中的全栈OCR</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型的发展推动了对海量多模态数据管理及应用的需求，使得从视觉图像中提取信息的OCR技术日益普及。然而，现有OCR方法主要聚焦于从图像或扫描文档中识别文本元素（文本中心OCR），而忽视了从视觉信息密集的图像源（视觉中心OCR）中识别视觉元素，如图表、网页和科学绘图。现实中，这类视觉信息密集的图像在互联网中广泛存在，具有重要的实际应用价值，如数据可视化和网页分析。本技术报告提出OCRVerse，首个以端到端方式实现文本中心OCR与视觉中心OCR统一的全栈OCR方法。为此，我们构建了全面的数据工程，涵盖报纸、杂志、书籍等广泛文本中心文档，以及图表、网页、科学绘图等视觉中心渲染合成图像。此外，我们为OCRVerse提出了一种两阶段SFT-RL多领域训练方法：SFT直接混合跨领域数据进行训练以建立初始领域知识，而RL则针对各领域特性设计个性化奖励策略。具体而言，由于不同领域需要多样化的输出格式和预期结果，我们在RL阶段提供充分灵活性，为每个领域定制灵活的奖励信号，从而提升跨领域融合能力并避免数据冲突。实验结果表明，OCRVerse在文本中心与视觉中心数据类型上均取得具有竞争力的效果，甚至可与大规模开源及闭源模型相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for OCR technology that goes beyond traditional text recognition to also interpret visually information-dense images like charts and web pages, which are prevalent and valuable for applications such as data visualization. The method, OCRVerse, is an end-to-end holistic OCR approach that unifies text-centric and vision-centric tasks through comprehensive data engineering covering diverse document types and a two-stage SFT-RL multi-domain training strategy, where supervised fine-tuning establishes initial knowledge and reinforcement learning employs personalized reward signals for each domain to enhance cross-domain fusion. Key experimental findings show that OCRVerse achieves competitive performance across both text-centric and vision-centric data types, performing comparably to large-scale open-source and closed-source models.</div>
<div class="mono" style="margin-top:8px">该研究的动机是，现有的OCR方法主要专注于从图像或扫描文档中识别文本元素，而忽略了从图表、网页等视觉信息密集的图像源中识别视觉元素，这些图像在互联网上广泛存在且具有重要的实际应用价值。方法上，提出了OCRVerse，这是一种端到端的整体OCR方法，通过构建全面的数据工程覆盖文本中心和视觉中心的数据源，并采用两阶段的SFT-RL多领域训练策略，其中监督微调混合跨领域数据建立初始知识，而强化学习则为每个领域设计个性化的奖励策略以处理不同的输出格式。实验结果表明，OCRVerse在文本中心和视觉中心数据类型上均取得了有竞争力的结果，甚至可与大规模开源和闭源模型相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">Less Precise Can Be More Reliable: A Systematic Evaluation of Quantization&#x27;s Impact on CLIP Beyond Accuracy</div>
<div class="meta-line">Authors: Aymen Bouguerra, Daniel Montoya, Alexandra Gomez-Villa, Chokri Mraidha, Fabio Arnez</div>
<div class="meta-line">First: 2025-09-25T13:54:34+00:00 · Latest: 2026-02-04T09:44:34+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21173v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.21173v4">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) such as CLIP have revolutionized zero-shot classification and safety-critical tasks, including Out-of-Distribution (OOD) detection. However, their high computational cost hinders efficient real-world deployment. While quantization is a standard solution for efficiency, its broader impact on reliability metrics beyond simple Top-1 accuracy remains critically under-explored. In this study, we conduct a large-scale evaluation of VLM quantization across a comprehensive experimental suite of over 700k evaluation runs with varying configurations. We find that, contrary to the assumption that quantization&#x27;s noise degrades performance, it can simultaneously improve accuracy, calibration, OOD detection, and robustness to noise, though not to covariate shift or spurious correlations. We leverage these counterintuitive findings to characterize the mechanics of quantization beyond simple regularization: we show that quantization dampens high-rank spectral components, compelling the model to rely more heavily on robust, low-rank features. Ultimately, this spectral filtering effect drives the observed improvements in generalization and noise tolerance, establishing a pathway to deploy faster, more reliable VLMs by utilizing quantization beyond its conventional role.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降低精度反而提升可靠性：量化对CLIP模型影响的系统性评估——超越准确率的视角</div>
<div class="mono" style="margin-top:8px">以CLIP为代表的视觉-语言模型（VLM）彻底改变了零样本分类和安全关键任务（包括分布外检测）的实现方式。然而，其高昂的计算成本阻碍了实际场景的高效部署。量化虽是提升效率的常规方案，但其对可靠性指标（超越简单的Top-1准确率）的广泛影响仍缺乏深入研究。本研究通过超过70万次不同配置的实验，对VLM量化进行了大规模评估。我们发现：与量化噪声会降低性能的假设相反，量化能同时提升准确率、校准能力、分布外检测及噪声鲁棒性（但对协变量偏移和伪相关性无效）。基于这些反直觉的发现，我们揭示了超越简单正则化的量化机制：量化会抑制高秩谱成分，迫使模型更依赖鲁棒的低秩特征。这种谱滤波效应最终驱动了泛化能力和噪声容忍度的提升，为通过突破量化的传统角色来部署更快速、更可靠的VLM开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models like CLIP are computationally expensive, limiting deployment, and while quantization is used for efficiency, its effects on reliability beyond accuracy are not well understood. The study systematically evaluates quantization&#x27;s impact through over 700,000 experimental runs, finding it can paradoxically enhance accuracy, calibration, out-of-distribution detection, and noise robustness, though not performance against covariate shift or spurious correlations. Analysis reveals that quantization acts as a spectral filter, dampening high-rank components and forcing the model to rely on more robust low-rank features, thereby improving generalization and establishing a method for more efficient and reliable models.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型（如CLIP）计算成本高、难以在现实世界安全关键应用中高效部署的问题，尤其关注量化这一标准效率技术对准确性之外可靠性指标的潜在影响。研究方法是对VLM量化进行系统性的大规模评估，通过超过70万次实验运行，全面分析量化对一系列可靠性指标的作用。主要实验结果表明，量化可以矛盾地提升模型的多个可靠性方面，包括准确性、校准性、分布外检测和噪声鲁棒性，其机制是量化作为一种频谱滤波器，抑制了高秩特征并迫使模型依赖更鲁棒的低秩表示，但量化并未提升模型对协变量偏移或虚假相关性的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models</div>
<div class="meta-line">Authors: Jaehyun Kwak, Nam Cao, Boryeong Cho, Segyu Lee, Sumyeong Ahn, Se-Young Yun</div>
<div class="meta-line">First: 2026-02-04T09:29:10+00:00 · Latest: 2026-02-04T09:29:10+00:00</div>
<div class="meta-line">Comments: Pre-print</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04356v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04356v1">PDF</a> · <a href="https://github.com/jackwaky/SAGA">Code1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时何地发起攻击？面向大型视觉语言模型的阶段式注意力引导对抗攻击</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型（LVLMs）的对抗攻击对于揭示现代多模态系统的安全漏洞至关重要。基于输入变换（如随机裁剪）的最新研究表明，空间局部化扰动可能比全局图像操作更有效。然而，对整个图像进行随机裁剪具有内在随机性，且无法高效利用有限的像素级扰动预算。我们提出两个关键发现：（1）区域注意力分数与对抗损失敏感度呈正相关；（2）攻击高注意力区域会引发注意力向后续显著区域的结构化重分配。基于这些发现，我们提出阶段式注意力引导攻击（SAGA），这是一种逐步将扰动集中于高注意力区域的注意力引导框架。SAGA能更高效地利用受限扰动预算，在十种LVLM上持续实现最先进的攻击成功率的同时，生成高度不易察觉的对抗样本。源代码发布于 https://github.com/jackwaky/SAGA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for efficient adversarial attacks on Large Vision-Language Models (LVLMs) to expose safety vulnerabilities. The proposed Stage-wise Attention-Guided Attack (SAGA) method progressively concentrates perturbations on high-attention image regions, guided by the observations that these regions correlate with adversarial loss sensitivity and that attacking them redistributes attention to subsequent salient areas. Experimental results demonstrate that SAGA achieves state-of-the-art attack success rates across ten LVLMs while maintaining high imperceptibility through efficient use of constrained perturbation budgets.</div>
<div class="mono" style="margin-top:8px">本研究旨在针对大型视觉语言模型（LVLMs）开发高效对抗攻击以揭示其安全漏洞。提出的方法为阶段式注意力引导攻击（SAGA），该方法基于高注意力区域与对抗损失敏感性正相关、且攻击这些区域会引导注意力向后续显著区域重分布的观察，逐步将扰动集中在高注意力区域。实验结果表明，SAGA在十个LVLM上实现了最先进的攻击成功率，同时通过高效利用有限的扰动预算，保持了对抗样本的高度不可感知性。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning</div>
<div class="meta-line">Authors: Qian-Wei Wang, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2026-02-04T09:01:55+00:00 · Latest: 2026-02-04T09:01:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04340v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04340v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双提示调优的主动CLIP适配显式不确定性建模</div>
<div class="mono" style="margin-top:8px">CLIP等预训练视觉语言模型虽具备较强的迁移能力，但在有限标注预算下将其适配至下游图像分类任务仍具挑战性。在主动学习场景中，模型需从未标注数据池中选择最具信息量的样本进行标注。现有方法通常通过基于熵的准则或表征聚类来估计不确定性，而未从模型视角显式建模不确定性。本研究提出一种基于双提示调优的鲁棒不确定性建模框架，用于主动CLIP适配。我们在CLIP的文本分支中引入两个可学习提示：正提示通过增强与轻量调优视觉嵌入对应的任务特定文本嵌入的判别性，提升分类可靠性；负提示则通过反向训练方式显式建模预测标签正确的概率，为主动样本选择提供理论依据的不确定性信号。跨不同微调范式的实验表明，在相同标注预算下，本方法持续优于现有主动学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of adapting pre-trained vision-language models like CLIP to downstream image classification tasks with limited annotation budgets in active learning, this work proposes an explicit uncertainty modeling framework based on dual-prompt tuning. The method introduces two learnable prompts in CLIP&#x27;s textual branch: a positive prompt enhances discriminability of task-specific textual embeddings, while a negative prompt is trained in a reversed manner to explicitly model the correctness probability of predictions, providing a principled uncertainty signal for active sample selection. Experimental results across different fine-tuning paradigms show that this approach consistently outperforms existing active learning methods under the same annotation budget.</div>
<div class="mono" style="margin-top:8px">本研究针对在主动学习场景下，以有限标注预算将预训练的CLIP模型适配到下游图像分类任务的挑战。该方法提出了一个双提示调优框架，在CLIP的文本分支中引入两个可学习的提示：正提示通过增强任务特定文本嵌入的判别性来提高分类可靠性，而负提示则以反向方式进行训练，显式建模预测正确的概率，从而为主动样本选择提供原则性的不确定性信号。在不同微调范式下的广泛实验表明，该方法在相同标注预算下持续优于现有的主动学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner</div>
<div class="meta-line">Authors: Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2026-02-04T09:00:12+00:00 · Latest: 2026-02-04T09:00:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04337v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04337v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需人工标注的预训练视觉语言模型微调方法</div>
<div class="mono" style="margin-top:8px">CLIP等大规模视觉语言模型虽具备较强的零样本泛化能力，但适应下游任务通常需要昂贵的标注数据。现有无监督自训练方法依赖伪标注，但常面临置信度过滤不可靠、确认偏误及低置信度样本利用不足等问题。我们提出协同微调框架，通过双模型跨模态协作机制利用未标注数据。该框架采用包含正负文本提示的双提示学习策略，以样本依赖方式显式建模伪标注纯净度，无需人工设定阈值或噪声假设。负提示同时正则化轻量视觉适配模块，提升噪声监督下的鲁棒性。该框架采用两阶段训练方案：从高置信度样本的参数高效微调过渡至协同过滤伪标签指导的完整微调。在其基础上，增强版本通过迭代微调、动量对比学习与大语言模型生成提示进一步提升适应性能。大量实验表明，该方法在无监督方法乃至少样本监督基线上均取得持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To adapt large-scale vision-language models like CLIP to downstream tasks without costly human annotations, this work proposes Collaborative Fine-Tuning (CoFT), an unsupervised framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. The method introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, avoiding hand-crafted thresholds, and employs a two-phase training scheme transitioning from parameter-efficient to full fine-tuning. Experiments show that CoFT and its enhanced variant CoFT+ achieve consistent performance gains over existing unsupervised methods and even surpass few-shot supervised baselines.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大规模视觉语言模型（如CLIP）在下游任务适应中无需昂贵人工标注的挑战。作者提出了协作微调（CoFT），这是一种无监督框架，采用双模型跨模态协作机制，通过正负文本提示以样本依赖的方式显式建模伪标签的清洁度，从而无需手动设定阈值。该方法采用两阶段训练方案，从参数高效微调过渡到基于协作过滤伪标签的完全微调。实验结果表明，CoFT在性能上持续优于现有的无监督方法，甚至超越了少样本监督基线；其增强版本CoFT+通过迭代微调和动量对比学习进一步提升了适应效果。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement</div>
<div class="meta-line">Authors: Zipeng Zhu, Zhanghao Hu, Qinglin Zhu, Yuxi Hong, Yijun Liu, Jingyong Su, Yulan He, Lin Gui</div>
<div class="meta-line">First: 2026-02-04T08:13:01+00:00 · Latest: 2026-02-04T08:13:01+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04304v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04304v1">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static &quot;magic layer&quot; empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越静态裁剪：层级自适应视觉定位与解码增强</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）通过将视觉图块与文本嵌入空间对齐而快速发展，但固定的视觉标记预算迫使图像必须调整至统一的预训练分辨率，这往往会抹除细粒度细节，并因过度依赖语言先验而导致幻觉现象。近期基于注意力引导的增强方法（如裁剪或区域聚焦注意力分配）缓解了这一问题，但通常依赖于在简单识别基准上经验性选择的静态“魔法层”，因而可能无法迁移至复杂推理任务。与此静态假设相反，我们提出了视觉定位的动态视角。通过层级敏感性分析，我们证明视觉定位是一个动态过程：简单物体识别任务依赖中间层，而复杂的视觉搜索与推理任务需要在更深层重新激活视觉信息。基于这一发现，我们提出了基于查询的视觉激活（VAQ）指标，该指标通过测量注意力对输入查询的敏感性，识别出与查询特定视觉定位最相关的注意力图所在层级。在此基础上，我们进一步提出LASER（面向推理的层级自适应注意力引导选择性视觉与解码增强），这是一种无需训练的自适应推理流程，能够为视觉定位与问题解答动态选择适合任务的层级。在多样化视觉问答基准上的实验表明，LASER能显著提升不同复杂度任务的视觉问答准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of fixed visual-token budgets in Large Vision-Language Models, which erase fine details and cause hallucinations, this research challenges the static use of attention layers for enhancement. It introduces a dynamic perspective, showing through layer-wise sensitivity analysis that complex reasoning tasks require visual reactivation at deeper layers. The method proposes Visual Activation by Query (VAQ) to identify the most relevant layer for query-specific grounding and builds LASER, a training-free inference procedure that adaptively selects layers for localization and answering. Experiments on VQA benchmarks demonstrate that LASER significantly improves accuracy across tasks of varying complexity.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型中固定视觉标记预算会抹去细节并导致幻觉的问题，本研究挑战了静态使用单一层进行视觉增强的方法。作者提出动态视角，通过分层敏感性分析证明，最优的视觉基础层随任务复杂度而变化。他们引入了基于查询的视觉激活（VAQ）来识别最相关的层，并开发了LASER，这是一种无需训练的自适应推理方法，能动态选择层进行视觉定位和解码。在多样化的VQA基准测试上的实验表明，LASER显著提高了不同复杂度任务的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Xiongtao Sun, Hui Li, Jiaming Zhang, Yujie Yang, Kaili Liu, Ruxin Feng, Wen Jun Tan, Wei Yang Bryan Lim</div>
<div class="meta-line">First: 2025-11-21T04:33:11+00:00 · Latest: 2026-02-04T07:29:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16940v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16940v2">PDF</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) pose significant individual-level privacy risks by linking fragmented multimodal data to identifiable individuals through hierarchical chain-of-thought reasoning. However, existing privacy benchmarks remain structurally insufficient for this threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM&#x27;s ability to infer and link distributed information to construct individual profiles. To address this gap, we propose MultiPriv, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the Privacy Perception and Reasoning (PPR) framework and construct a bilingual multimodal dataset with synthetic individual profiles, where identifiers (e.g., faces, names) are linked to sensitive attributes. This design enables nine challenging tasks spanning attribute detection, cross-image re-identification, and chained inference. We conduct a large-scale evaluation of over 50 open-source and commercial VLMs. Our analysis shows that 60 percent of widely used VLMs can perform individual-level privacy reasoning with up to 80 percent accuracy, posing a significant threat to personal privacy. MultiPriv provides a foundation for developing and assessing privacy-preserving VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MultiPriv：视觉语言模型中个体级隐私推理的基准测试</div>
<div class="mono" style="margin-top:8px">现代视觉语言模型（VLMs）通过分层思维链推理将碎片化的多模态数据与可识别个体关联，带来显著的个体级隐私风险。然而，现有隐私基准在结构上仍不足以应对此威胁，因其主要评估隐私感知，未能解决更关键的隐私推理风险：即VLM推断并关联分散信息以构建个体画像的能力。为填补这一空白，我们提出MultiPriv——首个系统评估VLM个体级隐私推理的基准。我们引入隐私感知与推理（PPR）框架，构建了包含合成个体画像的双语多模态数据集，其中标识符（如人脸、姓名）与敏感属性相关联。该设计涵盖属性检测、跨图像重识别及链式推理等九项挑战性任务。我们对超过50个开源及商业VLM进行了大规模评估，分析表明60%的常用VLM能以高达80%的准确率执行个体级隐私推理，对个人隐私构成严重威胁。MultiPriv为开发与评估隐私保护型VLM奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern Vision-Language Models (VLMs) can infer and link fragmented multimodal data to identify individuals, a privacy risk not captured by existing benchmarks that focus only on privacy perception. To address this, the authors introduce MultiPriv, a benchmark built on a Privacy Perception and Reasoning framework and a bilingual dataset of synthetic profiles linking identifiers to sensitive attributes, enabling nine tasks like attribute detection and chained inference. Evaluating over 50 VLMs reveals that 60% can perform individual-level privacy reasoning with up to 80% accuracy, highlighting a significant privacy threat and establishing a foundation for developing privacy-preserving models.</div>
<div class="mono" style="margin-top:8px">现代视觉语言模型（VLMs）能够通过关联碎片化数据推断敏感个人信息，但现有基准仅评估基本的隐私感知，而非更危险的、能构建个人档案的层次化推理能力。为此，研究者提出了MultiPriv基准，其基于隐私感知与推理框架，使用一个将标识符与敏感属性关联的双语合成个人档案数据集，设计了涵盖属性检测、跨图像重识别和链式推理的九项任务。对超过50个VLMs的大规模评估表明，60%的常用模型能以高达80%的准确率进行此类隐私推理，这构成了重大的隐私威胁，并为开发隐私保护模型奠定了基础。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260208_0331.html">20260208_0331</a>
<a href="archive/20260207_0630.html">20260207_0630</a>
<a href="archive/20260207_0534.html">20260207_0534</a>
<a href="archive/20260207_0451.html">20260207_0451</a>
<a href="archive/20260207_0345.html">20260207_0345</a>
<a href="archive/20260206_0629.html">20260206_0629</a>
<a href="archive/20260206_0531.html">20260206_0531</a>
<a href="archive/20260206_0450.html">20260206_0450</a>
<a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
