<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-01 06:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260201_0625</div>
    <div class="row"><div class="card">
<div class="title">DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</div>
<div class="meta-line">Authors: Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-29T18:59:51+00:00 · Latest: 2026-01-29T18:59:51+00:00</div>
<div class="meta-line">Comments: Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22153v1">PDF</a> · <a href="https://github.com/hzxie/DynamicVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://www.infinitescript.com/project/dynamic-vla/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynamicVLA：面向动态物体操作的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">动态物体操作仍是视觉-语言-动作模型面临的开放挑战。尽管在静态操作中表现出强大的泛化能力，现有模型在需要快速感知、时序预测和持续控制的动态场景中仍存在局限。本文提出DynamicVLA动态物体操作框架，通过三项核心设计整合时序推理与闭环适应能力：1）采用卷积视觉编码器的紧凑型0.4B参数VLA模型，实现空间高效且结构保真的编码，支持快速多模态推理；2）连续推理机制，通过重叠式推理与执行降低延迟，实时适应物体运动；3）潜在感知动作流技术，通过时序对齐的动作执行弥合感知与执行的间隙。为填补动态操作数据空白，我们构建了动态物体操作基准数据集，通过自动化数据采集流程从零创建了涵盖2.8K场景、206类物体的20万条合成轨迹，并高效采集了2K条无需遥操作的真实世界轨迹。大量实验表明，该框架在响应速度、感知能力和泛化性能上取得显著提升，确立了DynamicVLA作为跨具身形态的通用动态物体操作统一框架的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Manipulating dynamic objects is a significant challenge for Vision-Language-Action (VLA) models, which perform well in static settings but lack the rapid perception and continuous control needed for moving targets. To address this, the authors propose DynamicVLA, a framework that integrates temporal reasoning and closed-loop adaptation via a compact 0.4B VLA model with a convolutional vision encoder for efficient encoding, a Continuous Inference mechanism for overlapping reasoning and execution to reduce latency, and Latent-aware Action Streaming to align perception with action temporally. They also introduce the Dynamic Object Manipulation (DOM) benchmark, comprising 200K synthetic and 2K real-world episodes collected via an automated pipeline. Experimental results show that DynamicVLA achieves substantial improvements in response speed, perception accuracy, and generalization, establishing it as a unified solution for dynamic manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉-语言-动作（VLA）模型在动态物体操控上的挑战，这类模型缺乏处理此类任务所需的时间推理和低延迟控制能力。提出的DynamicVLA框架整合了一个使用卷积视觉编码器的紧凑0.4B VLA模型、用于重叠推理与执行的连续推理机制，以及潜在感知动作流以实现感知与动作的时间对齐。为支持训练，作者引入了动态物体操控（DOM）基准，包含20万条合成和2千条真实世界数据片段。实验结果表明，DynamicVLA在响应速度、感知和泛化能力上取得了显著提升，使其成为一个适用于动态操控的统一框架。</div>
</details>
</div>
<div class="card">
<div class="title">ReactEMG Stroke: Healthy-to-Stroke Few-shot Adaptation for sEMG-Based Intent Detection</div>
<div class="meta-line">Authors: Runsheng Wang, Katelyn Lee, Xinyue Zhu, Lauren Winterbottom, Dawn M. Nilsen, Joel Stein, Matei Ciocarlie</div>
<div class="meta-line">First: 2026-01-29T18:26:51+00:00 · Latest: 2026-01-29T18:26:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22090v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22090v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Surface electromyography (sEMG) is a promising control signal for assist-as-needed hand rehabilitation after stroke, but detecting intent from paretic muscles often requires lengthy, subject-specific calibration and remains brittle to variability. We propose a healthy-to-stroke adaptation pipeline that initializes an intent detector from a model pretrained on large-scale able-bodied sEMG, then fine-tunes it for each stroke participant using only a small amount of subject-specific data. Using a newly collected dataset from three individuals with chronic stroke, we compare adaptation strategies (head-only tuning, parameter-efficient LoRA adapters, and full end-to-end fine-tuning) and evaluate on held-out test sets that include realistic distribution shifts such as within-session drift, posture changes, and armband repositioning. Across conditions, healthy-pretrained adaptation consistently improves stroke intent detection relative to both zero-shot transfer and stroke-only training under the same data budget; the best adaptation methods improve average transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78. These results suggest that transferring a reusable healthy-domain EMG representation can reduce calibration burden while improving robustness for real-time post-stroke intent detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReactEMG Stroke：基于表面肌电图的意图检测从健康到中风的少样本适应方法</div>
<div class="mono" style="margin-top:8px">表面肌电图（sEMG）是中风后手部按需辅助康复的一种有前景的控制信号，但从瘫痪肌肉中检测意图通常需要冗长的个体化校准，且对变异性敏感。我们提出了一种从健康到中风的适应流程：首先利用在大规模健康人群sEMG数据上预训练的模型初始化意图检测器，然后仅使用少量个体数据对每位中风参与者进行微调。基于新收集的三名慢性中风患者数据集，我们比较了多种适应策略（仅调整头部层、参数高效的LoRA适配器、端到端全微调），并在包含实际分布偏移（如会话内漂移、姿势变化、臂带重新定位）的保留测试集上评估。在所有条件下，相较于相同数据量下的零样本迁移和仅使用中风数据的训练，基于健康数据预训练的适应方法持续提升了中风意图检测性能；最佳适应方法将平均转移准确率从0.42提升至0.61，原始准确率从0.69提升至0.78。结果表明，迁移可复用的健康领域肌电表征能降低校准负担，同时提升实时中风后意图检测的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Surface electromyography (sEMG)-based intent detection for stroke rehabilitation requires extensive subject-specific calibration and is sensitive to variability. To address this, the study proposes a healthy-to-stroke adaptation pipeline that first pretrains a model on large-scale able-bodied sEMG data and then fine-tunes it for individual stroke participants using minimal subject-specific data. Experiments on a new dataset from three chronic stroke individuals, evaluating adaptation strategies like head-only tuning, LoRA adapters, and full fine-tuning under distribution shifts, show that healthy-pretrained adaptation consistently outperforms zero-shot transfer and stroke-only training, improving transition accuracy from 0.42 to 0.61 and raw accuracy from 0.69 to 0.78, thereby reducing calibration burden and enhancing robustness.</div>
<div class="mono" style="margin-top:8px">表面肌电图（sEMG）是中风后手部康复的一种有前景的控制信号，但从麻痹肌肉中检测运动意图通常需要大量针对特定受试者的校准，且对变异性敏感。为此，研究者提出了一种从健康人到中风患者的适应流程：首先使用在大规模健康人sEMG数据上预训练的模型初始化意图检测器，然后仅用少量特定中风受试者的数据进行微调。在一个新收集的三名慢性中风患者数据集上，针对包括会话内漂移、姿势变化和臂带重新定位等现实分布偏移进行评估，结果显示，与零样本迁移和同等数据量下的纯中风训练相比，采用LoRA等适应策略的方法持续提升了性能，将平均过渡准确率从0.42提高到0.61，原始准确率从0.69提高到0.78，这表明迁移健康领域的肌电表征可以降低校准负担并增强鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">mjlab: A Lightweight Framework for GPU-Accelerated Robot Learning</div>
<div class="meta-line">Authors: Kevin Zakka, Qiayuan Liao, Brent Yi, Louis Le Lay, Koushil Sreenath, Pieter Abbeel</div>
<div class="meta-line">First: 2026-01-29T18:11:26+00:00 · Latest: 2026-01-29T18:11:26+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/mujocolab/mjlab</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22074v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22074v1">PDF</a> · <a href="https://github.com/mujocolab/mjlab">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present mjlab, a lightweight, open-source framework for robot learning that combines GPU-accelerated simulation with composable environments and minimal setup friction. mjlab adopts the manager-based API introduced by Isaac Lab, where users compose modular building blocks for observations, rewards, and events, and pairs it with MuJoCo Warp for GPU-accelerated physics. The result is a framework installable with a single command, requiring minimal dependencies, and providing direct access to native MuJoCo data structures. mjlab ships with reference implementations of velocity tracking, motion imitation, and manipulation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mjlab：面向GPU加速机器人学习的轻量级框架</div>
<div class="mono" style="margin-top:8px">我们推出mjlab——一个轻量级开源机器人学习框架，集成了GPU加速仿真、可组合环境与极简配置流程。该框架采用Isaac Lab提出的管理器式API，用户可通过模块化组件构建观测、奖励与事件系统，并配合MuJoCo Warp实现GPU加速物理计算。框架支持单命令安装，依赖项极少，且能直接访问原生MuJoCo数据结构。mjlab内置速度跟踪、运动模仿与操作任务的参考实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To facilitate efficient robot learning with minimal setup complexity, this work introduces mjlab, a lightweight open-source framework. The method integrates a manager-based API for composing modular environment components with MuJoCo Warp to enable GPU-accelerated physics simulation, resulting in a system installable via a single command and providing direct access to native simulation data structures. Experimental validation includes reference implementations demonstrating the framework&#x27;s capability for velocity tracking, motion imitation, and manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决机器人学习领域对简化设置流程并利用GPU加速的轻量级框架的需求。方法上提出了mjlab，这是一个轻量级开源框架，它结合了用于组合模块化环境组件的管理器API和用于GPU加速物理模拟的MuJoCo Warp，确保了最小的依赖性和对原生数据结构的直接访问。主要的实验成果包括成功实现了速度跟踪、运动模仿和操作任务等参考示例，证明了该框架的实用价值。</div>
</details>
</div>
<div class="card">
<div class="title">AsterNav: Autonomous Aerial Robot Navigation In Darkness Using Passive Computation</div>
<div class="meta-line">Authors: Deepak Singh, Shreyas Khobragade, Nitin J. Sanket</div>
<div class="meta-line">First: 2026-01-24T18:41:40+00:00 · Latest: 2026-01-29T18:09:09+00:00</div>
<div class="meta-line">Comments: 8 pages, 10 figures, Published in IEEE Robotics And Automation Letters</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17550v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17550v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous aerial navigation in absolute darkness is crucial for post-disaster search and rescue operations, which often occur from disaster-zone power outages. Yet, due to resource constraints, tiny aerial robots, perfectly suited for these operations, are unable to navigate in the darkness to find survivors safely. In this paper, we present an autonomous aerial robot for navigation in the dark by combining an Infra-Red (IR) monocular camera with a large-aperture coded lens and structured light without external infrastructure like GPS or motion-capture. Our approach obtains depth-dependent defocus cues (each structured light point appears as a pattern that is depth dependent), which acts as a strong prior for our AsterNet deep depth estimation model. The model is trained in simulation by generating data using a simple optical model and transfers directly to the real world without any fine-tuning or retraining. AsterNet runs onboard the robot at 20 Hz on an NVIDIA Jetson Orin$^\text{TM}$ Nano. Furthermore, our network is robust to changes in the structured light pattern and relative placement of the pattern emitter and IR camera, leading to simplified and cost-effective construction. We successfully evaluate and demonstrate our proposed depth navigation approach AsterNav using depth from AsterNet in many real-world experiments using only onboard sensing and computation, including dark matte obstacles and thin ropes (diameter 6.25mm), achieving an overall success rate of 95.5% with unknown object shapes, locations and materials. To the best of our knowledge, this is the first work on monocular, structured-light-based quadrotor navigation in absolute darkness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AsterNav：基于被动计算的全黑暗环境下自主空中机器人导航系统</div>
<div class="mono" style="margin-top:8px">全黑暗环境下的自主空中导航对灾后搜救至关重要，此类场景常因灾区断电陷入黑暗。然而受资源限制，本适用于此类任务的微型空中机器人无法在黑暗中安全导航寻找幸存者。本文提出一种结合大孔径编码镜头红外单目相机与结构光的自主空中机器人黑暗导航方案，无需GPS或动作捕捉等外部基础设施。该方法通过获取与深度相关的离焦线索（每个结构光点呈现深度依赖的光斑模式），为AsterNet深度估计模型提供强先验知识。模型通过简易光学模型生成的仿真数据训练，无需微调即可直接迁移至现实场景。AsterNet在NVIDIA Jetson Orin™ Nano平台上以20Hz频率机载运行。该网络对结构光图案变化及图案发射器与红外相机的相对位置具有鲁棒性，可实现简易低成本构建。我们通过纯机载感知与计算，在包含暗色哑光障碍物与细绳（直径6.25mm）的多组现实实验中验证了AsterNav深度导航方案，在未知物体形状、位置与材质的条件下取得95.5%的综合成功率。据我们所知，这是首项基于单目结构光的全黑暗四旋翼导航研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling tiny aerial robots to navigate autonomously in complete darkness for post-disaster search and rescue, where power outages are common. The method, AsterNav, combines an IR monocular camera with a large-aperture coded lens and structured light, using depth-dependent defocus cues from the structured light points as a prior for a deep learning model called AsterNet, which is trained solely in simulation and deployed without fine-tuning. Experimental results show the system runs at 20 Hz onboard, is robust to hardware variations, and achieves a 95.5% success rate in real-world tests navigating dark obstacles including thin ropes, using only onboard sensing and computation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决微型空中机器人在完全黑暗环境（如灾后断电的搜救场景）中自主导航的难题。所提出的AsterNav方法结合了红外单目相机、大孔径编码透镜和结构光，通过捕获结构光点产生的深度相关离焦线索，为AsterNet深度学习模型提供先验信息以估计深度；该模型使用简单光学模型在仿真中生成数据进行训练，无需微调即可直接迁移到真实世界。实验结果表明，该系统在机载NVIDIA Jetson Orin Nano上以20赫兹频率运行，对结构光图案和硬件布局的变化具有鲁棒性，在仅使用机载传感与计算的实际测试中，成功导航通过暗色障碍物和细绳（直径6.25毫米），整体成功率达到了95.5%。</div>
</details>
</div>
<div class="card">
<div class="title">EROAM: Event-based Camera Rotational Odometry and Mapping in Real-time</div>
<div class="meta-line">Authors: Wanli Xing, Shijie Lin, Linhan Yang, Zeqing Zhang, Yanjun Du, Maolin Lei, Yipeng Pan, Chen Wang, Jia Pan</div>
<div class="meta-line">First: 2024-11-17T08:50:47+00:00 · Latest: 2026-01-29T17:25:30+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Transactions on Robotics (T-RO), 2026. Project page: https://wlxing1901.github.io/eroam/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.11004v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.11004v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://wlxing1901.github.io/eroam/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents EROAM, a novel event-based rotational odometry and mapping system that achieves real-time, accurate camera rotation estimation. Unlike existing approaches that rely on event generation models or contrast maximization, EROAM employs a spherical event representation by projecting events onto a unit sphere and introduces Event Spherical Iterative Closest Point (ES-ICP), a novel geometric optimization framework designed specifically for event camera data. The spherical representation simplifies rotational motion formulation while operating in a continuous spherical domain, enabling enhanced spatial resolution. Our system features an efficient map management approach using incremental k-d tree structures and intelligent regional density control, ensuring optimal computational performance during long-term operation. Combined with parallel point-to-line optimization, EROAM achieves efficient computation without compromising accuracy. Extensive experiments on both synthetic and real-world datasets show that EROAM significantly outperforms state-of-the-art methods in terms of accuracy, robustness, and computational efficiency. Our method maintains consistent performance under challenging conditions, including high angular velocities and extended sequences, where other methods often fail or show significant drift. Additionally, EROAM produces high-quality panoramic reconstructions with preserved fine structural details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EROAM：基于事件相机的实时旋转里程计与建图系统</div>
<div class="mono" style="margin-top:8px">本文提出EROAM，一种创新的基于事件的旋转里程计与建图系统，可实现实时、精确的相机旋转估计。与依赖事件生成模型或对比度最大化的现有方法不同，EROAM通过将事件投影至单位球面建立球面事件表示，并提出了专为事件相机数据设计的几何优化框架——事件球面迭代最近点（ES-ICP）。球面表示在连续球域中简化了旋转运动建模，同时提升了空间分辨率。本系统采用基于增量k-d树的高效地图管理方法与智能区域密度控制，确保长期运行时的最优计算性能。结合并行点线优化，EROAM在不损失精度的前提下实现了高效计算。在合成与真实数据集上的大量实验表明，EROAM在精度、鲁棒性和计算效率方面显著优于现有先进方法。该方法在高角速度、长序列等挑战性条件下仍保持稳定性能，而其他方法常出现失效或显著漂移。此外，EROAM能生成保留精细结构细节的高质量全景重建结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for real-time, accurate rotational odometry and mapping using event cameras, which often struggle with high-speed motion and long-term drift. The proposed EROAM system projects events onto a unit sphere for a simplified rotational formulation and introduces Event Spherical Iterative Closest Point (ES-ICP), a geometric optimization framework tailored for event data, alongside efficient map management via incremental k-d trees and parallel point-to-line optimization. Experimental results on synthetic and real-world datasets demonstrate that EROAM surpasses state-of-the-art methods in accuracy, robustness, and computational efficiency, maintaining consistent performance under high angular velocities and in extended sequences while producing high-quality panoramic reconstructions.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决事件相机在实时精确旋转里程计与建图方面的需求，这类相机常面临高速运动和长期漂移的挑战。提出的EROAM系统将事件投影到单位球面上以简化旋转建模，并引入了专为事件数据设计的几何优化框架——事件球面迭代最近点（ES-ICP），同时通过增量k-d树和并行点线优化实现高效地图管理。在合成和真实数据集上的实验结果表明，EROAM在精度、鲁棒性和计算效率上均优于现有先进方法，能在高角速度和长序列等挑战性条件下保持稳定性能，并生成高质量的全景重建。</div>
</details>
</div>
<div class="card">
<div class="title">PocketDP3: Efficient Pocket-Scale 3D Visuomotor Policy</div>
<div class="meta-line">Authors: Jinhao Zhang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Wenlong Xia, Haoming Song, Youmin Gong, Jie Me</div>
<div class="meta-line">First: 2026-01-29T17:23:25+00:00 · Latest: 2026-01-29T17:23:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22018v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22018v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, 3D vision-based diffusion policies have shown strong capability in learning complex robotic manipulation skills. However, a common architectural mismatch exists in these models: a tiny yet efficient point-cloud encoder is often paired with a massive decoder. Given a compact scene representation, we argue that this may lead to substantial parameter waste in the decoder. Motivated by this observation, we propose PocketDP3, a pocket-scale 3D diffusion policy that replaces the heavy conditional U-Net decoder used in prior methods with a lightweight Diffusion Mixer (DiM) built on MLP-Mixer blocks. This architecture enables efficient fusion across temporal and channel dimensions, significantly reducing model size. Notably, without any additional consistency distillation techniques, our method supports two-step inference without sacrificing performance, improving practicality for real-time deployment. Across three simulation benchmarks--RoboTwin2.0, Adroit, and MetaWorld--PocketDP3 achieves state-of-the-art performance with fewer than 1% of the parameters of prior methods, while also accelerating inference. Real-world experiments further demonstrate the practicality and transferability of our method in real-world settings. Code will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PocketDP3：高效的袖珍规模三维视觉运动策略</div>
<div class="mono" style="margin-top:8px">近期，基于三维视觉的扩散策略在学习复杂机器人操作技能方面展现出强大能力。然而，这些模型普遍存在架构不匹配问题：通常将微小而高效的点云编码器与庞大的解码器配对。考虑到紧凑的场景表示，我们认为这可能导致解码器参数的大量浪费。基于此观察，我们提出PocketDP3——一种袖珍规模的三维扩散策略，用基于MLP-Mixer模块构建的轻量级扩散混合器替代先前方法中使用的重型条件U-Net解码器。该架构能高效融合时间和通道维度，显著减小模型规模。值得注意的是，无需任何额外的一致性蒸馏技术，我们的方法支持两步推理且不牺牲性能，提升了实时部署的实用性。在RoboTwin2.0、Adroit和MetaWorld三个仿真基准测试中，PocketDP3以少于先前方法1%的参数实现最先进性能，同时加速推理。真实世界实验进一步验证了该方法在实际场景中的实用性和可迁移性。代码将开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the architectural inefficiency in existing 3D vision-based diffusion policies, where a compact point-cloud encoder is paired with a disproportionately large decoder, leading to parameter waste. To resolve this, the authors propose PocketDP3, which replaces the heavy conditional U-Net decoder with a lightweight Diffusion Mixer (DiM) based on MLP-Mixer blocks, enabling efficient fusion across temporal and channel dimensions and drastically reducing model size. Experimental results on simulation benchmarks (RoboTwin2.0, Adroit, MetaWorld) show that PocketDP3 achieves state-of-the-art performance with under 1% of the parameters of prior methods, accelerates inference, and supports efficient two-step inference without consistency distillation, with real-world experiments confirming its practicality and transferability.</div>
<div class="mono" style="margin-top:8px">本研究针对现有3D视觉运动扩散策略中紧凑点云编码器与庞大解码器配对导致的架构低效和参数浪费问题，提出了PocketDP3方法。该方法用基于MLP-Mixer模块构建的轻量级扩散混合器（DiM）取代了笨重的条件U-Net解码器，实现了跨时间和通道维度的高效融合，从而大幅减少了模型规模。在RoboTwin2.0、Adroit和MetaWorld三个仿真基准上的实验结果表明，PocketDP3以少于先前方法1%的参数实现了最先进的性能，同时加速了推理过程，并支持无需一致性蒸馏的高效两步推理，真实世界实验进一步验证了其实用性和可迁移性。</div>
</details>
</div>
<div class="card">
<div class="title">Causal World Modeling for Robot Control</div>
<div class="meta-line">Authors: Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu</div>
<div class="meta-line">First: 2026-01-29T17:07:43+00:00 · Latest: 2026-01-29T17:07:43+00:00</div>
<div class="meta-line">Comments: Project page: https://technology.robbyant.com/lingbot-va Code: https://github.com/robbyant/lingbot-va</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21998v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21998v1">PDF</a> · <a href="https://github.com/robbyant/lingbot-va">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向机器人控制的因果世界建模</div>
<div class="mono" style="margin-top:8px">本研究指出，视频世界建模与视觉语言预训练相结合，为机器人学习建立了全新且独立的基础。直观而言，视频世界模型通过理解动作与视觉动态之间的因果关系，具备预测近期未来的能力。受此启发，我们提出LingBot-VA——一种同时学习帧预测与策略执行的自回归扩散框架。该模型具备三项精心设计：(1) 基于混合变换器架构的共享潜空间，融合视觉与动作标记；(2) 闭环推演机制，支持通过真实观测持续获取环境反馈；(3) 异步推理管道，并行执行动作预测与运动控制以实现高效操控。我们在仿真基准测试和现实场景中评估模型，其在长周期操作、训练后数据效率以及对新配置的强泛化能力方面均展现出显著潜力。代码与模型已开源以促进社区研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the potential of video world modeling and vision-language pre-training to serve as a new foundation for robot learning, as they enable the imagination of future states by understanding action-visual causality. The method introduces LingBot-VA, an autoregressive diffusion framework that concurrently learns frame prediction and policy execution through a shared latent space with a Mixture-of-Transformers architecture, a closed-loop rollout mechanism for continuous environmental feedback, and an asynchronous inference pipeline for efficient control. Experimental results on simulation benchmarks and real-world scenarios demonstrate the model&#x27;s significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalization to novel configurations.</div>
<div class="mono" style="margin-top:8px">本研究基于视频世界建模和视觉语言预训练为机器人学习提供新基础的潜力，因为它们能通过理解动作与视觉动态的因果关系来预测未来状态。方法上提出了LingBot-VA，这是一个自回归扩散框架，通过混合Transformer架构的共享潜在空间、用于持续环境反馈的闭环推演机制以及支持高效控制的异步推理管道，同时学习帧预测与策略执行。在仿真基准和真实场景中的实验结果表明，该模型在长时程操作、训练后数据效率以及对新配置的泛化能力方面均表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized Information Gathering Under Dynamics Uncertainty</div>
<div class="meta-line">Authors: Fernando Palafox, Jingqi Li, Jesse Milzman, David Fridovich-Keil</div>
<div class="meta-line">First: 2026-01-29T17:00:35+00:00 · Latest: 2026-01-29T17:00:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21988v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey&#x27;s directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态不确定性下的广义信息采集</div>
<div class="mono" style="margin-top:8px">在未知动态系统中运行的智能体需通过观测学习系统动态。主动信息采集能加速这一学习过程，但现有方法需针对特定建模选择（动态模型、信念更新流程、观测模型与规划器）设计专用代价函数。本文提出一种统一框架，通过显式揭示参数、信念与控制间的因果依赖关系，使信息采集代价与这些建模选择解耦。基于该框架，我们推导出基于马西定向信息的通用信息采集代价函数，该函数仅假设具有加性噪声的马尔可夫动态特性，而对其他建模选择保持中立。我们证明现有文献中使用的互信息代价是本代价函数的特例。进而利用该框架，在线性化贝叶斯估计中建立互信息代价与信息增益的显式关联，从而为基于互信息的主动学习方法提供理论依据。最后，通过线性系统、非线性系统及多智能体系统的实验验证了本框架的实际效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of active information gathering in unknown dynamical systems, where existing methods require custom-designed costs for specific modeling assumptions. The authors propose a unifying framework that decouples modeling choices from the information-gathering cost by explicitly modeling causal dependencies between parameters, beliefs, and controls. They derive a general cost function based on Massey&#x27;s directed information, applicable to Markov dynamics with additive noise, and prove that the mutual information cost used in prior work is a special case. Experimental results demonstrate the framework&#x27;s utility across linear, nonlinear, and multi-agent systems, while theoretical analysis establishes a connection between mutual information costs and information gain in linearized Bayesian estimation.</div>
<div class="mono" style="margin-top:8px">针对未知动态系统中主动信息采集的挑战，现有方法依赖于与特定建模假设绑定的专用成本，本研究提出了一个统一框架，将建模选择与信息采集目标解耦。该方法显式地建模参数、信念和控制之间的因果依赖关系，基于Massey的有向信息推导出一个通用成本，仅需满足加性噪声的马尔可夫动力学假设。关键实验结果证明了该框架在线性、非线性和多智能体系统中的实际效用，同时理论分析表明，广泛使用的互信息成本是所提成本的特例，并建立了其在线性化贝叶斯估计中与信息增益的明确联系。</div>
</details>
</div>
<div class="card">
<div class="title">Macro-Scale Electrostatic Origami Motor</div>
<div class="meta-line">Authors: Alex S. Miller, Leo McElroy, Jeffrey H. Lang</div>
<div class="meta-line">First: 2026-01-29T16:53:24+00:00 · Latest: 2026-01-29T16:53:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21976v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21976v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foldable robots have been an active area of robotics research due to their high volume-to-mass ratio, easy packability, and shape adaptability. For locomotion, previously developed foldable robots have either embedded linear actuators in, or attached non-folding rotary motors to, their structure. Further, those actuators directly embedded in the structure of the folding medium all contributed to linear or folding motion, not to continuous rotary motion. On the macro-scale there has not yet been a folding continuous rotary actuator. This paper details the development and testing of the first macro-scale origami rotary motor that can be folded flat, and then unfurled to operate. Using corona discharge for torque production, the prototype motor achieved an expansion ratio of 2.5:1, reached a top speed of 1440 rpm when driven at -29 kV, and exhibited a maximum output torque over 0.15 mN m with an active component torque density of 0.04 Nm/kg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>宏观尺度静电折纸电机</div>
<div class="mono" style="margin-top:8px">可折叠机器人因其高体积质量比、易打包性和形状适应性，一直是机器人研究的热点领域。在运动方式上，先前开发的可折叠机器人要么在结构中嵌入线性驱动器，要么附加非折叠旋转电机。此外，那些直接嵌入折叠介质结构中的驱动器均用于线性或折叠运动，而非连续旋转运动。在宏观尺度上，尚未出现可折叠的连续旋转驱动器。本文详细介绍了首款可折叠至平面状态、展开后即可运行的宏观尺度折纸旋转电机的开发与测试。该原型电机利用电晕放电产生扭矩，实现了2.5:1的展开比，在-29 kV驱动下最高转速达1440 rpm，最大输出扭矩超过0.15 mN·m，有效部件扭矩密度为0.04 Nm/kg。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of a macro-scale, foldable continuous rotary actuator for origami robots, which typically rely on linear actuators or non-folding motors for locomotion. The method involves developing an origami rotary motor that uses electrostatic corona discharge to generate torque, enabling the entire structure to be folded flat and then unfurled for operation. Key experimental results show the prototype achieved a 2.5:1 expansion ratio, reached a top speed of 1440 rpm at -29 kV, and produced a maximum output torque exceeding 0.15 mN·m with an active component torque density of 0.04 Nm/kg.</div>
<div class="mono" style="margin-top:8px">本研究针对折纸机器人缺乏宏观尺度、可折叠的连续旋转驱动器的问题，这类机器人通常依赖线性驱动器或不可折叠的电机，限制了紧凑性和运动多样性。方法涉及设计一种静电折纸电机，利用电晕放电产生扭矩，使其能够折叠平整并在展开后运行。实验结果表明，原型机实现了2.5:1的扩展比，在-29 kV驱动下最高转速达1440 rpm，最大输出扭矩超过0.15 mN·m，主动部件扭矩密度为0.04 Nm/kg。</div>
</details>
</div>
<div class="card">
<div class="title">MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts</div>
<div class="meta-line">Authors: Lorenzo Mazza, Ariel Rodriguez, Rayan Younis, Martin Lelis, Ortrun Hellig, Chenpan Li, Sebastian Bodenstedt, Martin Wagner, Stefanie Speidel</div>
<div class="meta-line">First: 2026-01-29T16:50:14+00:00 · Latest: 2026-01-29T16:50:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21971v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21971v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoE-ACT：通过监督混合专家模型提升手术模仿学习策略</div>
<div class="mono" style="margin-top:8px">模仿学习在机器人操作领域已取得显著成功，但其在手术机器人中的应用仍面临数据稀缺、工作空间受限以及对极高安全性和可预测性要求的挑战。本文提出一种专为分阶段结构手术操作任务设计的监督混合专家架构，可集成于任何自主策略之上。与先前依赖多摄像头设置或数千次演示的手术机器人学习方法不同，我们证明：当配备本架构时，轻量级动作解码器策略（如动作分块变换器）仅需不到150次演示，即可仅凭立体内窥镜图像学习复杂的长时程操作。我们在肠道抓持与牵拉协作手术任务中评估本方法，该任务要求机器人助手解读外科医生的视觉线索、对可变形组织执行精准抓持并进行持续牵拉。我们将本方法与前沿的视觉-语言-动作模型及标准动作分块变换器基线进行对比。结果显示：通用视觉-语言-动作模型即使在标准分布内条件下也完全无法掌握该任务；标准动作分块变换器在分布内取得中等成功率，而采用监督混合专家架构能显著提升其性能——在分布内获得更高成功率，并在分布外场景（包括新抓持位置、照明减弱和部分遮挡）中展现出卓越鲁棒性。值得注意的是，该方法能泛化至未见过的测试视角，并可零样本迁移至离体猪组织而无需额外训练，为体内部署提供了可行路径。为此，我们展示了在活体猪手术中策略执行的定性初步结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges of applying imitation learning to surgical robotics, such as data scarcity and stringent safety requirements, by proposing a supervised Mixture-of-Experts (MoE) architecture that enhances autonomous policies for phase-structured surgical tasks. The method integrates this MoE framework with a lightweight Action Chunking Transformer (ACT) policy, enabling learning from fewer than 150 demonstrations using only stereo endoscopic images. Experimental evaluation on bowel grasping and retraction shows that while generalist Vision-Language-Action models fail and standard ACT achieves moderate in-distribution success, the MoE-enhanced ACT significantly improves both in-distribution performance and robustness to out-of-distribution conditions like novel grasps and occlusions, with zero-shot transfer to ex vivo tissue and preliminary in vivo validation.</div>
<div class="mono" style="margin-top:8px">本研究针对模仿学习在手术机器人应用中面临的数据稀缺、工作空间受限和安全性要求高等挑战。该方法为阶段化结构的手术操作任务设计了一种监督混合专家架构，可增强基于动作分块变换器的策略，使其仅使用立体内窥镜图像和不足150次演示就能进行学习。在肠道抓取与牵拉任务上的实验表明，通用视觉-语言-动作模型完全失败，标准动作分块变换器策略在分布内仅有中等成功率，而采用混合专家架构后，策略的分布内成功率显著提升，并对分布外场景（如新的抓取位置、遮挡）表现出更强的鲁棒性，还能零样本迁移到离体猪组织上。</div>
</details>
</div>
<div class="card">
<div class="title">Information Filtering via Variational Regularization for Robot Manipulation</div>
<div class="meta-line">Authors: Jinhao Zhang, Wenlong Xia, Yaojia Wang, Zhexuan Zhou, Huizhe Li, Yichen Lai, Haoming Song, Youmin Gong, Jie Me</div>
<div class="meta-line">First: 2026-01-29T16:17:42+00:00 · Latest: 2026-01-29T16:17:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21926v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21926v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based visuomotor policies built on 3D visual representations have achieved strong performance in learning complex robotic skills. However, most existing methods employ an oversized denoising decoder. While increasing model capacity can improve denoising, empirical evidence suggests that it also introduces redundancy and noise in intermediate feature blocks. Crucially, we find that randomly masking backbone features at inference time (without changing training) can improve performance, confirming the presence of task-irrelevant noise in intermediate features. To this end, we propose Variational Regularization (VR), a lightweight module that imposes a timestep-conditioned Gaussian over backbone features and applies a KL-divergence regularizer, forming an adaptive information bottleneck. Extensive experiments on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that, compared to the baseline DP3, our approach improves the success rate by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld, achieving new state-of-the-art results. Real-world experiments further demonstrate that our method performs well in practical deployments. Code will released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于变分正则化的信息过滤在机器人操作中的应用</div>
<div class="mono" style="margin-top:8px">基于三维视觉表征的扩散式视觉运动策略在学习复杂机器人技能方面已取得显著成效。然而，现有方法大多采用过大的去噪解码器。虽然增加模型容量可提升去噪效果，但实证研究表明这也会在中间特征块中引入冗余与噪声。关键发现是：在推理阶段随机掩码主干特征（无需改变训练过程）能提升性能，这证实了中间特征中存在任务无关噪声。为此，我们提出变分正则化——一种轻量级模块，通过对主干特征施加时间步条件高斯分布并应用KL散度正则化器，构建自适应信息瓶颈。在三大仿真基准（RoboTwin2.0、Adroit和MetaWorld）上的大量实验表明：相较于基线DP3，本方法在RoboTwin2.0上成功率提升6.1%，在Adroit与MetaWorld上各提升4.1%，达到最新最优性能。真实场景实验进一步验证了该方法在实际部署中的有效性。代码即将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the problem of redundancy and noise in intermediate feature blocks of diffusion-based visuomotor policies for robot manipulation, which can hinder performance despite increased model capacity. The proposed solution, Variational Regularization (VR), introduces a lightweight module that imposes a timestep-conditioned Gaussian distribution over backbone features and applies a KL-divergence regularizer to form an adaptive information bottleneck, thereby filtering out task-irrelevant information. Experimental results on three simulation benchmarks (RoboTwin2.0, Adroit, and MetaWorld) show that VR improves success rates by 6.1% on RoboTwin2.0 and by 4.1% on Adroit and MetaWorld compared to the baseline DP3, achieving state-of-the-art performance, with real-world experiments confirming its practical effectiveness.</div>
<div class="mono" style="margin-top:8px">基于扩散的视觉运动策略利用3D视觉表示在机器人操作中表现良好，但现有方法通常采用过大的去噪解码器，这会在中间特征块中引入冗余和噪声，从而影响性能。为此，研究者提出了变分正则化（VR），这是一个轻量级模块，通过对骨干网络特征施加时间步条件的高斯分布并应用KL散度正则化器，形成一个自适应信息瓶颈来过滤任务无关的噪声。在三个仿真基准（RoboTwin2.0、Adroit和MetaWorld）上的大量实验表明，与基线DP3相比，该方法在RoboTwin2.0上的成功率提高了6.1%，在Adroit和MetaWorld上提高了4.1%，达到了新的最先进水平，真实世界实验进一步验证了其在实际部署中的良好表现。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Modular MANTA-RAY: A Modular Soft Surface Platform for Distributed Multi-Object Manipulation</div>
<div class="meta-line">Authors: Pratik Ingle, Jørn Lambertsen, Kasper Støy, Andres Faina</div>
<div class="meta-line">First: 2026-01-29T15:46:50+00:00 · Latest: 2026-01-29T15:46:50+00:00</div>
<div class="meta-line">Comments: 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21884v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manipulation surfaces control objects by actively deforming their shape rather than directly grasping them. While dense actuator arrays can generate complex deformations, they also introduce high degrees of freedom (DOF), increasing system complexity and limiting scalability. The MANTA-RAY (Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation densitY) platform addresses these challenges by leveraging a soft, fabric-based surface with reduced actuator density to manipulate fragile and heterogeneous objects. Previous studies focused on single-module implementations supported by four actuators, whereas the feasibility and benefits of a scalable, multi-module configuration remain unexplored. In this work, we present a distributed, modular, and scalable variant of the MANTA-RAY platform that maintains manipulation performance with a reduced actuator density. The proposed multi-module MANTA-RAY platform and control strategy employs object passing between modules and a geometric transformation driven PID controller that directly maps tilt-angle control outputs to actuator commands, eliminating the need for extensive data-driven or black-box training. We evaluate system performance in simulation across surface configurations of varying modules (3x3 and 4x4) and validate its feasibility through experiments on a physical 2x2 hardware prototype. The system successfully manipulates objects with diverse geometries, masses, and textures including fragile items such as eggs and apples as well as enabling parallel manipulation. The results demonstrate that the multi-module MANTA-RAY improves scalability and enables coordinated manipulation of multiple objects across larger areas, highlighting its potential for practical, real-world applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模块MANTA-RAY：用于分布式多物体操控的模块化软表面平台</div>
<div class="mono" style="margin-top:8px">操控表面通过主动变形而非直接抓取来控制物体。密集致动器阵列虽能产生复杂形变，但也引入了高自由度（DOF），增加了系统复杂性并限制了可扩展性。MANTA-RAY（低致动密度自适应非刚性织物操控）平台通过采用低致动密度的柔性织物表面来操控脆弱及异质物体，以应对这些挑战。先前研究集中于由四个致动器支撑的单模块实现，而可扩展的多模块配置的可行性与优势尚未探索。本研究提出一种分布式、模块化、可扩展的MANTA-RAY平台变体，在降低致动密度的同时保持操控性能。所提出的多模块MANTA-RAY平台及控制策略采用模块间物体传递和几何变换驱动的PID控制器，直接将倾斜角控制输出映射至致动器指令，无需大量数据驱动或黑箱训练。我们通过仿真评估了不同模块数量（3x3和4x4）表面配置的系统性能，并通过2x2物理硬件原型实验验证了其可行性。该系统成功操控了包括鸡蛋、苹果等脆弱物品在内的多种几何形状、质量及纹理的物体，并实现了并行操控。结果表明，多模块MANTA-RAY提升了可扩展性，支持更大区域内多物体的协调操控，凸显了其在实际应用中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high complexity and limited scalability of dense actuator arrays in manipulation surfaces, this work introduces a modular and scalable variant of the MANTA-RAY platform for distributed multi-object manipulation. The method employs a multi-module configuration where objects are passed between modules, using a geometric transformation driven PID controller that directly maps tilt-angle commands to actuator inputs, avoiding data-intensive training. Experimental results from simulation and a physical 2x2 prototype show successful manipulation of diverse, fragile objects like eggs and apples, demonstrating improved scalability and coordinated parallel manipulation across larger areas.</div>
<div class="mono" style="margin-top:8px">本研究针对密集致动器阵列在操控表面中可扩展性不足的问题，提出了模块化软表面平台Multi-Modular MANTA-RAY，该平台在降低致动器密度的同时保持了操控能力。该方法采用分布式模块化设计，其控制策略支持模块间物体传递，并利用基于几何变换的PID控制器将倾斜角控制输出直接映射为致动器指令，无需数据驱动的训练。仿真和2x2物理原型的实验结果表明，该系统能成功操控包括鸡蛋和苹果在内的多种几何形状、质量和质地的易碎物体，并实现并行操控，证实了其提升的可扩展性及在大面积上协调操控多物体的能力。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Driven Scenario-Aware Planning for Autonomous Driving</div>
<div class="meta-line">Authors: He Li, Zhaowei Chen, Rui Gao, Guoliang Li, Qi Hao, Shuai Wang, Chengzhong Xu</div>
<div class="meta-line">First: 2026-01-29T15:42:13+00:00 · Latest: 2026-01-29T15:42:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21876v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21876v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid planner switching framework (HPSF) for autonomous driving needs to reconcile high-speed driving efficiency with safe maneuvering in dense traffic. Existing HPSF methods often fail to make reliable mode transitions or sustain efficient driving in congested environments, owing to heuristic scene recognition and low-frequency control updates. To address the limitation, this paper proposes LAP, a large language model (LLM) driven, adaptive planning method, which switches between high-speed driving in low-complexity scenes and precise driving in high-complexity scenes, enabling high qualities of trajectory generation through confined gaps. This is achieved by leveraging LLM for scene understanding and integrating its inference into the joint optimization of mode configuration and motion planning. The joint optimization is solved using tree-search model predictive control and alternating minimization. We implement LAP by Python in Robot Operating System (ROS). High-fidelity simulation results show that the proposed LAP outperforms other benchmarks in terms of both driving time and success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的场景感知自动驾驶规划方法</div>
<div class="mono" style="margin-top:8px">自动驾驶混合规划器切换框架需兼顾高速行驶效率与密集车流中的安全操控。现有方法因启发式场景识别与低频控制更新，常在拥堵环境中难以实现可靠模式切换或保持高效行驶。为此，本文提出LAP——一种基于大语言模型的自适应规划方法，通过在低复杂度场景采用高速驾驶模式、高复杂度场景采用精确驾驶模式的动态切换，实现穿越狭窄间隙的高质量轨迹生成。该方法利用LLM进行场景理解，并将其推理结果融入模式配置与运动规划的联合优化中，通过树搜索模型预测控制与交替最小化求解。我们在机器人操作系统（ROS）中以Python实现LAP，高保真仿真结果表明，该方法在行驶时间与成功率方面均优于现有基准模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing hybrid planner switching frameworks (HPSF) in autonomous driving, which struggle with reliable mode transitions and efficient driving in dense traffic due to heuristic scene recognition and low-frequency control. The proposed method, LAP, leverages a large language model (LLM) for scene understanding and integrates its inference into a joint optimization of mode configuration and motion planning, solved via tree-search model predictive control and alternating minimization. Experimental results from high-fidelity simulations demonstrate that LAP outperforms benchmark methods in both driving time and success rate.</div>
<div class="mono" style="margin-top:8px">本研究针对自动驾驶中混合规划器切换框架（HPSF）的局限性展开，这些框架由于启发式场景识别和低频控制，在密集交通中常难以实现可靠模式切换和持续高效驾驶。所提出的LAP方法利用大语言模型（LLM）进行场景理解，并将其推理集成到模式配置与运动规划的联合优化中，通过树搜索模型预测控制和交替最小化求解。高保真仿真实验结果表明，LAP在驾驶时间和成功率方面均优于其他基准方法。</div>
</details>
</div>
<div class="card">
<div class="title">GAZELOAD A Multimodal Eye-Tracking Dataset for Mental Workload in Industrial Human-Robot Collaboration</div>
<div class="meta-line">Authors: Bsher Karbouj, Baha Eddin Gaaloul, Jorg Kruger</div>
<div class="meta-line">First: 2026-01-29T15:12:58+00:00 · Latest: 2026-01-29T15:12:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This article describes GAZELOAD, a multimodal dataset for mental workload estimation in industrial human-robot collaboration. The data were collected in a laboratory assembly testbed where 26 participants interacted with two collaborative robots (UR5 and Franka Emika Panda) while wearing Meta ARIA smart glasses. The dataset time-synchronizes eye-tracking signals (pupil diameter, fixations, saccades, eye gaze, gaze transition entropy, fixation dispersion index) with environmental real-time and continuous measurements (illuminance) and task and robot context (bench, task block, induced faults), under controlled manipulations of task difficulty and ambient conditions. For each participant and workload-graded task block, we provide CSV files with ocular metrics aggregated into 250 ms windows, environmental logs, and self-reported mental workload ratings on a 1-10 Likert scale, organized in participant-specific folders alongside documentation. These data can be used to develop and benchmark algorithms for mental workload estimation, feature extraction, and temporal modeling in realistic industrial HRC scenarios, and to investigate the influence of environmental factors such as lighting on eye-based workload markers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAZELOAD：面向工业人机协作中脑力负荷评估的多模态眼动追踪数据集</div>
<div class="mono" style="margin-top:8px">本文介绍了GAZELOAD——一个用于工业人机协作场景中脑力负荷评估的多模态数据集。数据采集于实验室装配测试平台，26名参与者佩戴Meta ARIA智能眼镜与两台协作机器人（UR5和Franka Emika Panda）进行交互。该数据集在任务难度与环境条件的受控调控下，实现了眼动信号（瞳孔直径、注视点、扫视、视线方向、注视转移熵、注视分散指数）与环境实时连续测量数据（照度）及任务与机器人上下文信息（工作台、任务模块、诱发故障）的时间同步。针对每位参与者及按负荷分级的任务模块，我们提供了以250毫秒窗口聚合的眼动指标CSV文件、环境日志、基于1-10李克特量表的自报告脑力负荷评分，并按参与者独立文件夹形式与文档一并组织。这些数据可用于开发与验证真实工业人机协作场景中的脑力负荷估计算法、特征提取及时序建模方法，并探究照明等环境因素对眼动负荷表征指标的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To support the development of mental workload estimation algorithms in realistic industrial human-robot collaboration (HRC) settings, this research introduces the GAZELOAD dataset. The method involved collecting synchronized multimodal data from 26 participants performing assembly tasks with two collaborative robots while wearing AR smart glasses; the data integrates eye-tracking metrics (e.g., pupil diameter, gaze entropy) with environmental illuminance and task context under controlled manipulations of difficulty and lighting. Key experimental results provide a structured dataset with ocular metrics aggregated in 250 ms windows, environmental logs, and self-reported workload ratings, enabling benchmarking of algorithms for workload estimation and analysis of environmental influences on ocular markers.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决工业人机协作中需要真实多模态数据进行心理负荷估计的问题。作者介绍了GAZELOAD数据集，该数据集通过26名参与者在佩戴AR智能眼镜下与两台协作机器人执行装配任务时收集；数据集在受控的任务难度和环境光照操作下，将眼动追踪指标（如瞳孔直径、注视熵）与环境照度及任务上下文进行时间同步。主要实验结果提供了一个结构化的资源，包含250毫秒窗口的眼动指标、环境日志和自评负荷等级，可用于开发和基准测试负荷估计算法，并研究如光照等环境因素对眼动负荷标记的影响。</div>
</details>
</div>
<div class="card">
<div class="title">SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction</div>
<div class="meta-line">Authors: Linyong Gan, Zimo Li, Wenxin Xu, Xingjian Li, Jianhua Z. Huang, Enmei Tu, Shuhang Chen</div>
<div class="meta-line">First: 2026-01-26T14:42:31+00:00 · Latest: 2026-01-29T15:03:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18537v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18537v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SKETCH：基于语义关键点条件建模的长时程船舶轨迹预测</div>
<div class="mono" style="margin-top:8px">由于复杂航行行为与环境因素叠加的不确定性，准确的长时程船舶轨迹预测仍具挑战性。现有方法常难以保持全局航向一致性，导致长时外推时出现轨迹漂移或失准。为此，我们提出一种语义关键点条件轨迹建模框架，通过以捕捉航行意图的高层‘下一关键点’为条件预测未来轨迹。该框架将长时程预测分解为全局语义决策与局部运动建模，有效将未来轨迹的可行域约束在语义合理的子集中。为从历史观测中高效估计下一关键点先验，我们采用预训练-微调策略。基于真实AIS数据的大量实验表明，所提方法在长航时、航向精度及细粒度轨迹预测方面均持续优于现有先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve long-horizon vessel trajectory prediction, which is difficult due to complex navigation behaviors and environmental uncertainty, often causing existing methods to produce globally inconsistent or implausible paths. The proposed method, SKETCH, introduces a semantic key-point conditioning framework that predicts future trajectories by conditioning on a high-level Next Key Point (NKP) representing navigational intent, thereby decomposing the problem into global semantic decision-making and local motion modeling to restrict predictions to semantically feasible subsets. Experimental results on real-world AIS data show that this approach consistently outperforms state-of-the-art methods, especially in long-duration prediction, directional accuracy, and fine-grained trajectory details.</div>
<div class="mono" style="margin-top:8px">由于复杂的航行行为和环境不确定性，准确的长时域船舶轨迹预测具有挑战性，现有方法在长时间外推时常常出现方向不一致和轨迹不合理的问题。为此，研究者提出了SKETCH框架，通过以表征航行意图的高层下一关键点（NKP）为条件进行轨迹预测，从而将问题分解为全局语义决策和局部运动建模，将预测限制在语义可行的子集内；采用预训练-微调策略来高效估计NKP先验。在真实AIS数据上的大量实验表明，该方法在长时航行、方向准确性和细粒度轨迹预测方面持续优于现有先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">Flocking behavior for dynamic and complex swarm structures</div>
<div class="meta-line">Authors: Carmen D. R. Pita-Romero, Pedro Arias-Perez, Miguel Fernandez-Cortizas, Rafael Perez-Segui, Pascual Campoy</div>
<div class="meta-line">Venue: 2025 International Conference on Unmanned Aircraft Systems (ICUAS), pages 1011-1018</div>
<div class="meta-line">First: 2026-01-29T14:22:53+00:00 · Latest: 2026-01-29T14:22:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21772v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21772v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Maintaining the formation of complex structures with multiple UAVs and achieving complex trajectories remains a major challenge. This work presents an algorithm for implementing the flocking behavior of UAVs based on the concept of Virtual Centroid to easily develop a structure for the flock. The approach builds on the classical virtual-based behavior, providing a theoretical framework for incorporating enhancements to dynamically control both the number of agents and the formation of the structure. Simulation tests and real-world experiments were conducted, demonstrating its simplicity even with complex formations and complex trajectories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态复杂集群结构中的群体聚集行为</div>
<div class="mono" style="margin-top:8px">利用多架无人机维持复杂结构编队并实现复杂轨迹仍是一大挑战。本研究提出一种基于虚拟质心概念的无人机群体聚集行为算法，以简便构建集群结构。该方法在经典虚拟行为基础上，提供了动态控制智能体数量与结构编队的增强理论框架。仿真测试与真实实验表明，该算法即使在复杂编队与轨迹下仍具简洁性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of maintaining formation and executing complex trajectories for multi-UAV swarms with dynamic structures. The method introduces a flocking algorithm based on a Virtual Centroid concept, extending classical virtual-structure approaches to provide a theoretical framework for dynamically controlling both the number of agents and the swarm&#x27;s geometric formation. Simulation and real-world experiments demonstrated the algorithm&#x27;s effectiveness and simplicity in managing complex formations and trajectories.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多无人机集群在复杂队形与轨迹飞行中的控制难题。该方法提出了一种基于虚拟质心概念的集群算法，扩展了经典的虚拟结构方法，以动态控制智能体数量和集群队形。仿真与实物实验表明，该算法能有效且简洁地管理复杂的集群结构和飞行轨迹。</div>
</details>
</div>
<div class="card">
<div class="title">OMP: One-step Meanflow Policy with Directional Alignment</div>
<div class="meta-line">Authors: Han Fang, Yize Huang, Yuheng Zhao, Paul Weng, Xiao Li, Yutong Ban</div>
<div class="meta-line">First: 2025-12-22T12:45:35+00:00 · Latest: 2026-01-29T14:02:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19347v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19347v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot manipulation has increasingly adopted data-driven generative policy frameworks, yet the field faces a persistent trade-off: diffusion models suffer from high inference latency, while flow-based methods often require complex architectural constraints. Although in image generation domain, the MeanFlow paradigm offers a path to single-step inference, its direct application to robotics is impeded by critical theoretical pathologies, specifically spectral bias and gradient starvation in low-velocity regimes. To overcome these limitations, we propose the One-step MeanFlow Policy (OMP), a novel framework designed for high-fidelity, real-time manipulation. We introduce a lightweight directional alignment mechanism to explicitly synchronize predicted velocities with true mean velocities. Furthermore, we implement a Differential Derivation Equation (DDE) to approximate the Jacobian-Vector Product (JVP) operator, which decouples forward and backward passes to significantly reduce memory complexity. Extensive experiments on the Adroit and Meta-World benchmarks demonstrate that OMP outperforms state-of-the-art methods in success rate and trajectory accuracy, particularly in high-precision tasks, while retaining the efficiency of single-step generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OMP：具有方向对齐的单步均值流策略</div>
<div class="mono" style="margin-top:8px">机器人操作领域日益采用数据驱动的生成式策略框架，但该领域始终面临一个权衡困境：扩散模型存在高推理延迟，而基于流的方法通常需要复杂的架构约束。虽然在图像生成领域，均值流范式提供了单步推理的路径，但其直接应用于机器人技术受到关键理论缺陷的阻碍，特别是低频速度区域中的谱偏差和梯度匮乏问题。为克服这些限制，我们提出了单步均值流策略（OMP），这是一个专为高保真实时操作设计的新型框架。我们引入了一种轻量级方向对齐机制，以显式同步预测速度与真实平均速度。此外，我们实现了微分推导方程（DDE）来近似雅可比-向量积（JVP）算子，该算子通过解耦前向与后向传播显著降低了内存复杂度。在Adroit和Meta-World基准测试上的大量实验表明，OMP在成功率和轨迹精度方面优于现有最优方法，尤其在高精度任务中表现突出，同时保持了单步生成的高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the trade-off in robot manipulation between the high inference latency of diffusion models and the architectural complexity of flow-based methods, this paper proposes the One-step Meanflow Policy (OMP) for real-time, high-fidelity control. The method introduces a directional alignment mechanism to synchronize predicted and true mean velocities and employs a Differential Derivation Equation (DDE) to approximate the Jacobian-Vector Product, thereby reducing memory complexity. Experiments on the Adroit and Meta-World benchmarks show that OMP achieves higher success rates and trajectory accuracy than state-of-the-art methods, especially in high-precision tasks, while maintaining efficient single-step inference.</div>
<div class="mono" style="margin-top:8px">该研究针对机器人操作中扩散模型推理延迟高与流式方法架构复杂之间的权衡问题，指出现有MeanFlow方法在低速状态下存在谱偏差和梯度匮乏的理论缺陷。提出的单步MeanFlow策略（OMP）引入了方向对齐机制来同步预测速度与真实平均速度，并采用微分推导方程（DDE）近似雅可比向量积，通过解耦前向与反向传播降低了内存复杂度。在Adroit和Meta-World基准测试上的实验表明，OMP在成功率和轨迹精度上优于现有先进方法，尤其在高精度任务中表现突出，同时保持了单步推理的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Disentangling perception and reasoning for improving data efficiency in learning cloth manipulation without demonstrations</div>
<div class="meta-line">Authors: Donatien Delehelle, Fei Chen, Darwin Caldwell</div>
<div class="meta-line">First: 2026-01-29T13:41:35+00:00 · Latest: 2026-01-29T13:41:35+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cloth manipulation is a ubiquitous task in everyday life, but it remains an open challenge for robotics. The difficulties in developing cloth manipulation policies are attributed to the high-dimensional state space, complex dynamics, and high propensity to self-occlusion exhibited by fabrics. As analytical methods have not been able to provide robust and general manipulation policies, reinforcement learning (RL) is considered a promising approach to these problems. However, to address the large state space and complex dynamics, data-based methods usually rely on large models and long training times. The resulting computational cost significantly hampers the development and adoption of these methods. Additionally, due to the challenge of robust state estimation, garment manipulation policies often adopt an end-to-end learning approach with workspace images as input. While this approach enables a conceptually straightforward sim-to-real transfer via real-world fine-tuning, it also incurs a significant computational cost by training agents on a highly lossy representation of the environment state. This paper questions this common design choice by exploring an efficient and modular approach to RL for cloth manipulation. We show that, through careful design choices, model size and training time can be significantly reduced when learning in simulation. Furthermore, we demonstrate how the resulting simulation-trained model can be transferred to the real world. We evaluate our approach on the SoftGym benchmark and achieve significant performance improvements over available baselines on our task, while using a substantially smaller model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦感知与推理以提升无演示学习的布料操作数据效率</div>
<div class="mono" style="margin-top:8px">布料操作是日常生活中的常见任务，但对机器人而言仍是一个开放挑战。开发布料操作策略的难点在于布料的高维状态空间、复杂动力学特性及易自遮挡特性。由于分析方法未能提供鲁棒且通用的操作策略，强化学习被视为解决这些问题的有前景途径。然而，为应对庞大状态空间和复杂动力学，基于数据的方法通常依赖大型模型和长训练时间，其计算成本严重阻碍了这些方法的发展与应用。此外，由于鲁棒状态估计的挑战，布料操作策略常采用以工作空间图像为输入的端到端学习方法。虽然该方法通过现实世界微调实现了概念上直观的仿真到现实迁移，但基于环境状态的高损耗表示训练智能体也带来了显著计算成本。本文通过探索一种高效模块化的布料操作强化学习方法，对这一常见设计选择提出质疑。我们证明，通过精心设计，在仿真中学习时可显著减小模型规模与训练时间。进一步展示了仿真训练模型如何迁移至现实世界。我们在SoftGym基准上评估本方法，在任务中相比现有基线取得显著性能提升，同时使用模型规模大幅减小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the data inefficiency and high computational costs of reinforcement learning (RL) for cloth manipulation, which is challenging due to the fabric&#x27;s high-dimensional state space and complex dynamics. The proposed method disentangles perception from reasoning, moving away from end-to-end learning on raw images to a more modular RL approach that uses a carefully designed, efficient state representation learned in simulation. Experimental evaluation on the SoftGym benchmark shows that this approach achieves significant performance improvements over baselines while using a substantially smaller model and enabling successful sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">本研究针对布料操作中强化学习数据效率低、计算成本高的问题，该任务因布料的高维状态空间、复杂动力学和自遮挡而充满挑战。所提出的方法将感知与推理解耦，采用模块化强化学习方法，避免从原始图像进行端到端学习，从而减少了仿真中的模型大小和训练时间。在SoftGym基准上的实验评估表明，该方法在使用显著更小模型的同时，性能相比现有基线有大幅提升，并且仿真训练的模型成功迁移到了真实世界的操作任务中。</div>
</details>
</div>
<div class="card">
<div class="title">CoFreeVLA: Collision-Free Dual-Arm Manipulation via Vision-Language-Action Model and Risk Estimation</div>
<div class="meta-line">Authors: Xuanran Zhai, Binkai Ou, Yemin Wang, Hui Yi Leong, Qiaojun Yu, Ce Hao, Yaohua Liu</div>
<div class="meta-line">First: 2026-01-29T13:40:46+00:00 · Latest: 2026-01-29T13:40:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21712v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Action (VLA) models enable instruction following manipulation, yet dualarm deployment remains unsafe due to under modeled selfcollisions between arms and grasped objects. We introduce CoFreeVLA, which augments an endtoend VLA with a short horizon selfcollision risk estimator that predicts collision likelihood from proprioception, visual embeddings, and planned actions. The estimator gates risky commands, recovers to safe states via risk-guided adjustments, and shapes policy refinement for safer rollouts. It is pre-trained with model-based collision labels and posttrained on real robot rollouts for calibration. On five bimanual tasks with the PiPER robot arm, CoFreeVLA reduces selfcollisions and improves success rates versus RDT and APEX.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoFreeVLA：基于视觉-语言-动作模型与风险估计的无碰撞双臂协同操作</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型能够实现指令跟随式操作，但由于对双臂及抓持物体间自碰撞建模不足，其双臂部署仍存在安全隐患。本文提出CoFreeVLA，通过为端到端VLA模型增加短期自碰撞风险估计器来预测本体感知、视觉嵌入与规划动作的碰撞概率。该估计器可拦截高风险指令，通过风险引导调整恢复至安全状态，并优化策略以实现更安全的操作执行。系统采用基于模型的碰撞标签进行预训练，并在真实机器人操作数据上进行后训练校准。在PiPER机器人手臂的五项双手任务中，相较于RDT与APEX方法，CoFreeVLA显著降低了自碰撞率并提升了任务成功率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models for dual-arm manipulation are prone to unsafe self-collisions between arms and grasped objects. To address this, CoFreeVLA integrates a short-horizon self-collision risk estimator that uses proprioception, visual embeddings, and planned actions to predict collision likelihood; this estimator gates risky commands, enables risk-guided recovery to safe states, and refines the policy for safer execution. The method is pre-trained with model-based collision labels and fine-tuned on real robot data. Experimental results on five bimanual tasks with a PiPER robot show that CoFreeVLA reduces self-collisions and achieves higher success rates compared to RDT and APEX baselines.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉-语言-动作模型在双臂操作中的安全挑战，即手臂与抓取物体之间自碰撞建模不足带来的风险。方法上提出了CoFreeVLA，它在端到端VLA模型中集成了一个短时域自碰撞风险估计器，该估计器基于本体感知、视觉嵌入和规划动作预测碰撞可能性，用于拦截风险指令、通过风险引导调整恢复至安全状态，并优化策略训练。在PiPER机器人手臂的五项双手任务实验中，CoFreeVLA相比基线方法RDT和APEX，有效减少了自碰撞并提高了任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">FLARE: Agile Flights for Quadrotor Cable-Suspended Payload System via Reinforcement Learning</div>
<div class="meta-line">Authors: Dongcheng Cao, Jin Zhou, Xian Wang, Shuo Li</div>
<div class="meta-line">First: 2025-08-13T13:27:32+00:00 · Latest: 2026-01-29T13:09:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.09797v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.09797v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agile flight for the quadrotor cable-suspended payload system is a formidable challenge due to its underactuated, highly nonlinear, and hybrid dynamics. Traditional optimization-based methods often struggle with high computational costs and the complexities of cable mode transitions, limiting their real-time applicability and maneuverability exploitation. In this letter, we present FLARE, a reinforcement learning (RL) framework that directly learns agile navigation policy from high-fidelity simulation. Our method is validated across three designed challenging scenarios, notably outperforming a state-of-the-art optimization-based approach by a 3x speedup during gate traversal maneuvers. Furthermore, the learned policies achieve successful zero-shot sim-to-real transfer, demonstrating remarkable agility and safety in real-world experiments, running in real time on an onboard computer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLARE：基于强化学习的四旋翼缆索悬挂载荷系统敏捷飞行控制</div>
<div class="mono" style="margin-top:8px">四旋翼缆索悬挂载荷系统的敏捷飞行控制因其欠驱动、高度非线性和混合动力学特性而极具挑战。传统基于优化的方法常受限于高计算成本与缆索模式转换的复杂性，难以实现实时应用与机动性开发。本文提出FLARE强化学习框架，可直接通过高保真仿真学习敏捷导航策略。该方法在三种设计的挑战性场景中得到验证，在穿越门框机动中较当前最优的优化方法提速3倍。所习得策略成功实现零样本仿真到现实的迁移，在搭载板载计算机的实时运行中，展现出卓越的敏捷性与安全性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of achieving agile flight for quadrotor cable-suspended payload systems, which have complex underactuated and hybrid dynamics that hinder traditional optimization-based methods due to high computational cost and difficulty handling cable mode transitions. The proposed method, FLARE, employs a reinforcement learning framework to directly learn agile navigation policies from high-fidelity simulation. Experimental results show the learned policies outperform a state-of-the-art optimization-based method with a 3x speedup in gate traversal and successfully transfer zero-shot from simulation to real-world agile flights, running in real-time on an onboard computer.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决四旋翼吊挂负载系统实现敏捷飞行的挑战，该系统具有欠驱动、高度非线性的混合动力学特性，传统基于优化的方法因计算成本高且难以处理缆绳模式切换而受限。所提出的FLARE方法采用强化学习框架，直接从高保真仿真中学习敏捷导航策略。实验结果表明，学习到的策略在穿越门框任务中比先进的优化方法快3倍，并成功实现了从仿真到真实环境的零样本迁移，在机载计算机上实时运行，展现了卓越的敏捷性和安全性。</div>
</details>
</div>
<div class="card">
<div class="title">Visual Localization via Semantic Structures in Autonomous Photovoltaic Power Plant Inspection</div>
<div class="meta-line">Authors: Viktor Kozák, Karel Košnar, Jan Chudoba, Miroslav Kulich, Libor Přeučil</div>
<div class="meta-line">First: 2025-01-24T15:48:41+00:00 · Latest: 2026-01-29T13:02:39+00:00</div>
<div class="meta-line">Comments: 50 pages, 23 figures. Submitted for review to Array</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.14587v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.14587v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspection systems utilizing unmanned aerial vehicles (UAVs) equipped with thermal cameras are increasingly popular for the maintenance of photovoltaic (PV) power plants. However, automation of the inspection task is a challenging problem as it requires precise navigation to capture images from optimal distances and viewing angles. This paper presents a novel localization pipeline that directly integrates PV module detection with UAV navigation, allowing precise positioning during inspection. The detections are used to identify the power plant structures in the image. These are associated with the power plant model and used to infer the UAV position relative to the inspected PV installation. We define visually recognizable anchor points for the initial association and use object tracking to discern global associations. Additionally, we present three different methods for visual segmentation of PV modules and evaluate their performance in relation to the proposed localization pipeline. The presented methods were verified and evaluated using custom aerial inspection data sets, demonstrating their robustness and applicability for real-time navigation. Additionally, we evaluate the influence of the power plant model precision on the localization methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语义结构视觉定位的光伏电站自主巡检方法</div>
<div class="mono" style="margin-top:8px">搭载热成像相机的无人机巡检系统在光伏电站维护中日益普及，但巡检任务自动化面临精准导航的挑战，需在最优距离与视角下采集图像。本文提出一种将光伏组件检测与无人机导航直接融合的新型定位流程，实现巡检过程中的精确定位。通过检测识别图像中的电站结构，关联电站模型并推算无人机相对于被检光伏装置的位姿。定义了视觉可识别的锚点用于初始关联，并采用目标跟踪实现全局关联。此外，提出了三种光伏组件视觉分割方法，评估了其与定位流程的适配性能。基于定制航检数据集的验证表明，该方法具有鲁棒性且适用于实时导航。同时评估了电站模型精度对定位方法的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable precise UAV navigation for automated photovoltaic power plant inspection, this study develops a localization pipeline that integrates PV module detection with UAV positioning. The method uses visual detection to identify power plant structures, associates them with a plant model via anchor points and object tracking, and infers the UAV&#x27;s relative position. Experimental evaluation on custom aerial datasets shows the robustness of the approach for real-time navigation and assesses the impact of model precision on localization accuracy.</div>
<div class="mono" style="margin-top:8px">为实现光伏电站自动化热巡检中无人机的精确导航，本研究提出了一种将光伏组件检测与无人机定位相结合的新方法。该方法通过检测组件识别电站结构，利用锚点和目标跟踪将其与已知电站模型关联，从而推算无人机的相对位置。在自定义航拍数据集上的实验验证了该方法的鲁棒性和实时应用性，同时评估了模型精度对定位效果的影响。</div>
</details>
</div>
<div class="card">
<div class="title">From Instruction to Event: Sound-Triggered Mobile Manipulation</div>
<div class="meta-line">Authors: Hao Ju, Shaofei Huang, Hongyu Li, Zihan Ding, Si Liu, Meng Wang, Zhedong Zheng</div>
<div class="meta-line">First: 2026-01-29T13:02:10+00:00 · Latest: 2026-01-29T13:02:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21667v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21667v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current mobile manipulation research predominantly follows an instruction-driven paradigm, where agents rely on predefined textual commands to execute tasks. However, this setting confines agents to a passive role, limiting their autonomy and ability to react to dynamic environmental events. To address these limitations, we introduce sound-triggered mobile manipulation, where agents must actively perceive and interact with sound-emitting objects without explicit action instructions. To support these tasks, we develop Habitat-Echo, a data platform that integrates acoustic rendering with physical interaction. We further propose a baseline comprising a high-level task planner and low-level policy models to complete these tasks. Extensive experiments show that the proposed baseline empowers agents to actively detect and respond to auditory events, eliminating the need for case-by-case instructions. Notably, in the challenging dual-source scenario, the agent successfully isolates the primary source from overlapping acoustic interference to execute the first interaction, and subsequently proceeds to manipulate the secondary object, verifying the robustness of the baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从指令到事件：声音触发的移动操控</div>
<div class="mono" style="margin-top:8px">当前移动操控研究主要遵循指令驱动范式，智能体依赖预定义的文本命令执行任务。然而，这种设定将智能体限制在被动角色，制约了其自主性和对环境动态事件的响应能力。为突破这些局限，我们提出了声音触发的移动操控，使智能体无需明确动作指令即可主动感知并操作发声物体。为支持此类任务，我们开发了集成声学渲染与物理交互的数据平台Habitat-Echo。进一步提出包含高层任务规划器与底层策略模型的基线系统以完成任务。大量实验表明，所提基线使智能体能够主动检测并响应听觉事件，无需逐例指令。值得注意的是，在具有挑战性的双声源场景中，智能体成功从重叠的声学干扰中分离主声源执行首次交互，继而操作次要物体，验证了基线的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current mobile manipulation agents are largely passive, relying on explicit textual instructions, which limits their autonomy in dynamic environments. To enable agents to actively perceive and respond to environmental events, this work introduces sound-triggered mobile manipulation, where agents must interact with sound-emitting objects without step-by-step commands. The method involves the Habitat-Echo platform for acoustic-physical simulation and a baseline model with a high-level planner and low-level policies. Experiments demonstrate that the agent can actively detect and respond to auditory events, successfully isolating and manipulating a primary sound source amidst overlapping acoustic interference before handling a secondary object, confirming the approach&#x27;s robustness.</div>
<div class="mono" style="margin-top:8px">本研究针对指令驱动移动操作中智能体被动执行预设命令的局限性，引入了声音触发任务，要求智能体主动感知并与发声物体交互。方法包括开发集成了声学渲染与物理交互的平台Habitat-Echo，以及一个结合高层任务规划器与底层策略模型的基线系统。实验结果表明，该智能体能够在没有具体指令的情况下主动检测并响应听觉事件，在双声源场景中成功从重叠的声学干扰中隔离主要声源以执行首次交互，随后操作次要物体，验证了系统的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">AIR-VLA: Vision-Language-Action Systems for Aerial Manipulation</div>
<div class="meta-line">Authors: Jianli Sun, Bin Tian, Qiyao Zhang, Chengxiang Li, Zihan Song, Zhiyong Cui, Yisheng Lv, Yonglin Tian</div>
<div class="meta-line">First: 2026-01-29T12:09:00+00:00 · Latest: 2026-01-29T12:09:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models have achieved remarkable success in ground-based embodied intelligence, their application to Aerial Manipulation Systems (AMS) remains a largely unexplored frontier. The inherent characteristics of AMS, including floating-base dynamics, strong coupling between the UAV and the manipulator, and the multi-step, long-horizon nature of operational tasks, pose severe challenges to existing VLA paradigms designed for static or 2D mobile bases. To bridge this gap, we propose AIR-VLA, the first VLA benchmark specifically tailored for aerial manipulation. We construct a physics-based simulation environment and release a high-quality multimodal dataset comprising 3000 manually teleoperated demonstrations, covering base manipulation, object &amp; spatial understanding, semantic reasoning, and long-horizon planning. Leveraging this platform, we systematically evaluate mainstream VLA models and state-of-the-art VLM models. Our experiments not only validate the feasibility of transferring VLA paradigms to aerial systems but also, through multi-dimensional metrics tailored to aerial tasks, reveal the capabilities and boundaries of current models regarding UAV mobility, manipulator control, and high-level planning. AIR-VLA establishes a standardized testbed and data foundation for future research in general-purpose aerial robotics. The resource of AIR-VLA will be available at https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AIR-VLA：面向空中操作的视觉-语言-动作系统</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作模型在地面具身智能领域已取得显著成功，但其在空中操作系统中的应用仍属未充分探索的前沿。空中操作系统固有的浮动基座动力学、无人机与机械臂的强耦合性，以及操作任务的多步骤长时域特性，对现有针对静态或二维移动基座设计的VLA范式构成严峻挑战。为填补这一空白，我们提出首个专为空中操作定制的VLA基准AIR-VLA。我们构建了基于物理的仿真环境，并发布了包含3000条人工遥操作演示的高质量多模态数据集，涵盖基座操控、物体与空间理解、语义推理及长时域规划。依托该平台，我们系统评估了主流VLA模型与前沿视觉语言模型。实验不仅验证了VLA范式迁移至空中系统的可行性，更通过针对空中任务定制的多维度量指标，揭示了当前模型在无人机机动性、机械臂控制及高层规划方面的能力与局限。AIR-VLA为通用空中机器人研究建立了标准化测试平台与数据基础，相关资源将通过https://anonymous.4open.science/r/AIR-VLA-dataset-B5CC/开放获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of Vision-Language-Action (VLA) models for Aerial Manipulation Systems (AMS), which face unique challenges like floating-base dynamics and complex, long-horizon tasks. The method introduces AIR-VLA, a benchmark comprising a physics-based simulation and a multimodal dataset of 3000 teleoperated demonstrations for tasks such as base manipulation and semantic reasoning. Experimental results systematically evaluate existing VLA and VLM models, confirming the feasibility of transferring VLA to aerial systems while delineating their current capabilities and limitations in UAV mobility and high-level planning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉-语言-动作（VLA）模型在航空操控系统（AMS）中应用不足的问题，该系统面临浮动基座动力学和复杂长程任务等独特挑战。方法上提出了AIR-VLA基准，包括一个基于物理的仿真环境和包含3000次遥操作演示的多模态数据集，涵盖基础操控、物体与空间理解等任务。实验结果系统评估了现有VLA和VLM模型，验证了将VLA范式迁移到航空系统的可行性，并揭示了当前模型在无人机机动性和任务规划方面的能力与边界。</div>
</details>
</div>
<div class="card">
<div class="title">Bring My Cup! Personalizing Vision-Language-Action Models with Visual Attentive Prompting</div>
<div class="meta-line">Authors: Sangoh Lee, Sangwoo Mo, Wook-Shin Han</div>
<div class="meta-line">First: 2025-12-23T03:13:39+00:00 · Latest: 2026-01-29T12:02:16+00:00</div>
<div class="meta-line">Comments: Project page with videos and code: https://vap-project.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20014v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20014v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vap-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language-Action (VLA) models generalize well to generic instructions, they struggle with personalized commands such as &quot;bring my cup,&quot; where the robot must act on one specific instance among visually similar objects. We study this setting of manipulating personal objects, in which a VLA must identify and control a user-specific object unseen during training using only a few reference images. To address this challenge, we propose Visual Attentive Prompting (VAP), a simple-yet-effective training-free perceptual adapter that equips frozen VLAs with top-down selective attention. VAP treats the reference images as a non-parametric visual memory, grounds the personal object in the scene through open-vocabulary detection and embedding-based matching, and then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. We construct two simulation benchmarks, Personalized-SIMPLER and Personalized-VLABench, and a real-world tabletop benchmark to evaluate personalized manipulation across multiple robots and tasks. Experiments show that VAP consistently outperforms generic policies and token-learning baselines in both success rate and correct-object manipulation, helping to bridge the gap between semantic understanding and instance-level control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递我的杯子！基于视觉注意力提示的视觉-语言-动作模型个性化方法</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言-动作模型能较好泛化至通用指令，但在处理如&#x27;递我的杯子&#x27;这类个性化指令时存在困难——机器人需在视觉相似物体中识别并操作特定实例。本研究聚焦个性化物体操控场景：VLA模型需仅凭少量参考图像识别并操控训练时未见的用户专属物体。为此，我们提出视觉注意力提示——一种无需训练的高效感知适配器，为冻结的VLA模型注入自上而下的选择性注意力机制。该方法将参考图像视为非参数化视觉记忆，通过开放词汇检测与嵌入匹配在场景中定位目标物体，进而通过高亮目标与指令重写实现视觉提示注入。我们构建了Personalized-SIMPLER与Personalized-VLABench两个仿真基准，以及真实桌面操作基准，用于评估跨机器人多任务的个性化操控性能。实验表明，VAP在成功率与正确物体操作率上均优于通用策略与词元学习基线，有效弥合语义理解与实例级控制之间的鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Vision-Language-Action (VLA) models in executing personalized commands like &quot;bring my cup,&quot; which require identifying and manipulating a specific user object unseen during training from among visually similar alternatives. The proposed method, Visual Attentive Prompting (VAP), is a training-free perceptual adapter that equips frozen VLAs with selective attention by using a few reference images as a visual memory; it grounds the personal object via open-vocabulary detection and embedding matching, then injects this grounding as a visual prompt by highlighting the object and rewriting the instruction. Experimental evaluation on simulation benchmarks (Personalized-SIMPLER and Personalized-VLABench) and a real-world tabletop benchmark demonstrates that VAP consistently outperforms generic policies and token-learning baselines in success rate and correct-object manipulation, effectively narrowing the gap between semantic understanding and instance-level control.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉-语言-动作（VLA）模型在执行如“拿我的杯子”这类个性化指令时的局限性，这类指令要求从视觉相似的物体中识别并操控一个训练时未见过的、用户指定的特定物体。所提出的方法——视觉注意力提示（VAP），是一种无需训练、为冻结的VLA模型添加选择性注意力的感知适配器；它利用少量参考图像作为视觉记忆，通过开放词汇检测和基于嵌入的匹配来定位个性化物体，然后通过高亮场景中的物体并重写指令来注入这种定位信息。在两个仿真基准（Personalized-SIMPLER和Personalized-VLABench）和一个真实世界桌面基准上的实验评估表明，VAP在成功率和正确物体操控方面持续优于通用策略和基于令牌学习的基线方法，有效缩小了语义理解与精确实例级控制之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Virtual Reflections on a Dynamic 2D Eye Model Improve Spatial Reference Identification</div>
<div class="meta-line">Authors: Matti Krüger, Yutaka Oshima, Yu Fang</div>
<div class="meta-line">First: 2024-12-10T09:37:25+00:00 · Latest: 2026-01-29T11:34:37+00:00</div>
<div class="meta-line">Comments: This article has been accepted for publication in IEEE Transactions on Human-Machine Systems. Citation information: DOI 10.1109/THMS.2026.3651818</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.07344v4">Abs</a> · <a href="https://arxiv.org/pdf/2412.07344v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The visible orientation of human eyes creates some transparency about people&#x27;s spatial attention and other mental states. This leads to a dual role of the eyes as a means of sensing and communication. Accordingly, artificial eye models are being explored as communication media in human-machine interaction scenarios. One challenge in the use of eye models for communication consists of resolving spatial reference ambiguities, especially for screen-based models. To address this challenge, we introduce an approach that incorporates reflection-like features that are contingent on the movements of artificial eyes. We conducted a user study with 30 participants in which participants had to use spatial references provided by dynamic eye models to advance in a fast-paced group interaction task. Compared to a non-reflective eye model and a pure reflection mode, the superimposition of screen-based eyes with gaze-contingent virtual reflections resulted in a higher identification accuracy and user experience, suggesting a synergistic benefit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态二维眼球模型的虚拟反射特征提升空间参照识别精度</div>
<div class="mono" style="margin-top:8px">人眼可见的朝向能部分反映个体的空间注意力及其他心理状态，使眼睛兼具感知与交流的双重功能。因此，人工眼球模型正被探索作为人机交互场景中的沟通媒介。使用眼球模型进行交流的挑战之一在于消除空间参照歧义，尤其对于基于屏幕的模型。为解决该问题，我们提出一种融合类反射特征的方法，该特征随人工眼球的运动而动态变化。我们开展了一项30名参与者用户研究，要求参与者利用动态眼球模型提供的空间参照完成快节奏群体交互任务。与无反射眼球模型及纯反射模式相比，基于屏幕的眼球模型叠加注视关联虚拟反射后，识别准确率与用户体验均显著提升，显示出协同增益效应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address spatial reference ambiguities when using artificial eye models for communication in human-machine interaction, this research introduces a method that adds gaze-contingent virtual reflections to a dynamic 2D eye model on a screen. The approach was evaluated in a user study where 30 participants performed a fast-paced group interaction task using spatial references from the eye models. Experimental results showed that the model with gaze-contingent virtual reflections achieved higher identification accuracy and better user experience compared to a non-reflective model and a model with pure reflections, indicating a synergistic benefit.</div>
<div class="mono" style="margin-top:8px">为解决人机交互中使用人工眼模型进行通信时的空间指代模糊性问题，本研究提出了一种方法，为屏幕上的动态二维眼模型添加了与注视方向相关的虚拟反射特征。该方法通过一项用户研究进行评估，30名参与者使用眼模型提供的空间参考完成了一项快节奏的群组交互任务。实验结果表明，与无反射模型和纯反射模式相比，结合了注视相关虚拟反射的模型获得了更高的识别准确率和更好的用户体验，显示出协同效益。</div>
</details>
</div>
<div class="card">
<div class="title">EmboCoach-Bench: Benchmarking AI Agents on Developing Embodied Robots</div>
<div class="meta-line">Authors: Zixing Lei, Genjia Liu, Yuanshuo Zhang, Qipeng Liu, Chuan Wen, Shanghang Zhang, Wenzhao Lian, Siheng Chen</div>
<div class="meta-line">First: 2026-01-29T11:33:49+00:00 · Latest: 2026-01-29T11:33:49+00:00</div>
<div class="meta-line">Comments: 37 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21570v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21570v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The field of Embodied AI is witnessing a rapid evolution toward general-purpose robotic systems, fueled by high-fidelity simulation and large-scale data collection. However, this scaling capability remains severely bottlenecked by a reliance on labor-intensive manual oversight from intricate reward shaping to hyperparameter tuning across heterogeneous backends. Inspired by LLMs&#x27; success in software automation and science discovery, we introduce \textsc{EmboCoach-Bench}, a benchmark evaluating the capacity of LLM agents to autonomously engineer embodied policies. Spanning 32 expert-curated RL and IL tasks, our framework posits executable code as the universal interface. We move beyond static generation to assess a dynamic closed-loop workflow, where agents leverage environment feedback to iteratively draft, debug, and optimize solutions, spanning improvements from physics-informed reward design to policy architectures such as diffusion policies. Extensive evaluations yield three critical insights: (1) autonomous agents can qualitatively surpass human-engineered baselines by 26.5\% in average success rate; (2) agentic workflow with environment feedback effectively strengthens policy development and substantially narrows the performance gap between open-source and proprietary models; and (3) agents exhibit self-correction capabilities for pathological engineering cases, successfully resurrecting task performance from near-total failures through iterative simulation-in-the-loop debugging. Ultimately, this work establishes a foundation for self-evolving embodied intelligence, accelerating the paradigm shift from labor-intensive manual tuning to scalable, autonomous engineering in embodied AI field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EmboCoach-Bench：基于具身机器人开发的AI智能体基准测试</div>
<div class="mono" style="margin-top:8px">在高保真仿真与大规模数据采集的推动下，具身人工智能领域正朝着通用机器人系统方向快速发展。然而，从复杂的奖励塑形到跨异构后端的超参数调优，当前规模化能力仍严重受限于劳动密集型的人工监督。受大语言模型在软件自动化与科学发现领域成功的启发，我们提出\textsc{EmboCoach-Bench}基准，用于评估大语言模型智能体自主设计具身策略的能力。该框架涵盖32项专家精选的强化学习与模仿学习任务，将可执行代码确立为通用接口。我们超越静态代码生成，评估动态闭环工作流——智能体利用环境反馈迭代完成方案起草、调试与优化，改进范围涵盖从基于物理知识的奖励设计到扩散策略等策略架构。大量实验得出三项关键结论：（1）自主智能体在平均成功率上可超越人工设计基线26.5%；（2）结合环境反馈的智能工作流能有效强化策略开发，显著缩小开源模型与专有模型间的性能差距；（3）智能体对异常工程案例展现出自我修正能力，通过仿真闭环迭代调试，成功将接近完全失败的任务性能恢复至正常水平。本研究为具身智能的自演进奠定了基石，加速了具身AI领域从劳动密集型人工调优向可扩展自主工程范式的转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the bottleneck in scaling Embodied AI due to labor-intensive manual oversight by proposing EmboCoach-Bench, a benchmark to evaluate LLM agents&#x27; ability to autonomously engineer robotic policies. The method assesses agents through a dynamic closed-loop workflow across 32 RL and IL tasks, using executable code as an interface to iteratively draft, debug, and optimize solutions based on environment feedback. Key findings show that autonomous agents outperform human-engineered baselines by 26.5% in average success rate, narrow the performance gap between open-source and proprietary models, and demonstrate self-correction capabilities to recover from near-total failures.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身人工智能领域在扩展通用机器人系统时面临的瓶颈，即依赖劳动密集型的人工监督，包括复杂的奖励塑造和跨异构后端的超参数调整。方法上，我们提出了EmboCoach-Bench基准测试，通过动态闭环工作流评估LLM代理自主设计具身策略的能力，以可执行代码为通用接口，覆盖32个专家策划的强化学习和模仿学习任务，代理利用环境反馈迭代起草、调试和优化解决方案。主要实验结果表明，自主代理在平均成功率上比人工设计的基线高出26.5%，带有反馈的代理工作流有效缩小了开源与专有模型之间的性能差距，并且代理通过迭代调试展现出自我纠正能力，能够从近乎完全失败中恢复任务性能。</div>
</details>
</div>
<div class="card">
<div class="title">Training slow silicon neurons to control extremely fast robots with spiking reinforcement learning</div>
<div class="meta-line">Authors: Irene Ambrosini, Ingo Blakowski, Dmitrii Zendrikov, Cristiano Capone, Luna Gava, Giacomo Indiveri, Chiara De Luca, Chiara Bartolozzi</div>
<div class="meta-line">First: 2026-01-29T11:05:23+00:00 · Latest: 2026-01-29T11:05:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21548v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Air hockey demands split-second decisions at high puck velocities, a challenge we address with a compact network of spiking neurons running on a mixed-signal analog/digital neuromorphic processor. By co-designing hardware and learning algorithms, we train the system to achieve successful puck interactions through reinforcement learning in a remarkably small number of trials. The network leverages fixed random connectivity to capture the task&#x27;s temporal structure and adopts a local e-prop learning rule in the readout layer to exploit event-driven activity for fast and efficient learning. The result is real-time learning with a setup comprising a computer and the neuromorphic chip in-the-loop, enabling practical training of spiking neural networks for robotic autonomous systems. This work bridges neuroscience-inspired hardware with real-world robotic control, showing that brain-inspired approaches can tackle fast-paced interaction tasks while supporting always-on learning in intelligent machines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用脉冲强化学习训练慢速硅神经元控制极速机器人</div>
<div class="mono" style="margin-top:8px">空气曲棍球需要在高速冰球运动中实现瞬间决策，我们通过运行在混合信号模拟/数字神经形态处理器上的紧凑脉冲神经元网络应对这一挑战。通过硬件与学习算法的协同设计，我们训练该系统在极少量尝试中通过强化学习实现成功的冰球交互。该网络利用固定随机连接捕捉任务的时间结构，并在读出层采用局部e-prop学习规则，以利用事件驱动活动实现快速高效的学习。最终构建了包含计算机与神经形态芯片的实时闭环学习系统，为机器人自主系统提供了实用的脉冲神经网络训练方案。这项工作将神经科学启发的硬件与现实世界的机器人控制相连接，表明类脑方法能够处理快速交互任务，并支持智能机器的持续在线学习。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to control high-speed robots, like an air hockey playing system, which requires extremely fast sensorimotor responses. The method employs a spiking neural network implemented on a mixed-signal neuromorphic processor, using a reinforcement learning approach with a local e-prop rule in the readout layer and fixed random connectivity to process temporal dynamics. Experimental results demonstrate that the system achieves successful real-time puck interactions through efficient, event-driven learning, requiring only a remarkably small number of training trials on a setup with the neuromorphic chip in-the-loop.</div>
<div class="mono" style="margin-top:8px">本研究旨在控制需要极快响应的机器人系统，例如进行空气曲棍球对战的装置。方法采用在混合信号神经形态处理器上实现的脉冲神经网络，通过强化学习框架，在读出层使用局部e-prop学习规则，并利用固定的随机连接来处理任务的时间结构。实验结果表明，该系统通过高效的事件驱动学习，在神经形态芯片实时在环的设置中，仅需少量训练回合即可成功实现实时的冰球交互控制。</div>
</details>
</div>
<div class="card">
<div class="title">Industrial Internet Robot Collaboration System and Edge Computing Optimization</div>
<div class="meta-line">Authors: Haopeng Zhao, Dajun Tao, Tian Qi, Jingyuan Xu, Zijie Zhou, Lipeng Liu</div>
<div class="meta-line">First: 2025-04-03T11:15:10+00:00 · Latest: 2026-01-29T10:39:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.02492v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.02492v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In industrial Internet environments, mobile robots must generate collision-free global routes under stochastic obstacle layouts and random perturbations in commanded linear and angular velocities. This paper models a differential-drive robot with nonholonomic constraints, then decomposes motion into obstacle avoidance, target turning, and target approaching behaviors to parameterize the control variables. Global path planning is formulated as a constrained optimization problem and converted into a weighted energy function that balances path length and collision penalties. A three-layer neural network represents the planning model, while simulated annealing searches for near-global minima and mitigates local traps. During execution, a fuzzy controller uses heading and lateral-offset errors to output wheel-speed differentials for rapid correction; edge-side computation is discussed to reduce robot-server traffic and latency. Matlab 2024 simulations report deviation within +-5 cm, convergence within 10 ms, and shorter paths than two baseline methods. The approach improves robustness of global navigation in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>工业互联网机器人协同系统与边缘计算优化</div>
<div class="mono" style="margin-top:8px">在工业互联网环境中，移动机器人需在随机障碍布局及线速度与角速度指令受随机扰动的情况下生成无碰撞全局路径。本文对具有非完整约束的差速驱动机器人进行建模，将运动分解为避障、转向目标与接近目标三种行为以参数化控制变量。全局路径规划被构建为约束优化问题，并转化为平衡路径长度与碰撞惩罚的加权能量函数。采用三层神经网络表示规划模型，同时利用模拟退火算法搜索近全局最小值并规避局部陷阱。执行阶段，模糊控制器通过航向角与横向偏移误差输出轮速差以实现快速校正；文中探讨了边缘侧计算以减少机器人与服务器间的通信流量与延迟。基于Matlab 2024的仿真显示：路径偏差控制在±5厘米内，收敛时间小于10毫秒，且所得路径较两种基准方法更短。该方法在实践中提升了全局导航的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance the robustness of mobile robot navigation in industrial Internet settings with stochastic obstacles and velocity perturbations, this paper models a differential-drive robot and decomposes its motion into specific behaviors to parameterize control. The method formulates global path planning as a constrained optimization problem, represented by a weighted energy function and a three-layer neural network, and employs simulated annealing to find near-optimal solutions while a fuzzy controller corrects deviations during execution, with edge computing proposed to reduce latency. Experimental simulations in Matlab 2024 demonstrate the approach achieves path deviations within ±5 cm, converges within 10 ms, and generates shorter paths compared to two baseline methods.</div>
<div class="mono" style="margin-top:8px">为提升工业互联网环境中移动机器人在随机障碍和速度扰动下的导航鲁棒性，本文对差速驱动机器人进行建模，并将其运动分解为避障、转向和目标接近行为以参数化控制。全局路径规划被构建为一个约束优化问题，转化为加权能量函数，并采用三层神经网络结合模拟退火算法进行近全局最优解搜索。在Matlab 2024中的仿真实验表明，该方法路径偏差在±5厘米内，收敛时间小于10毫秒，且相比两种基线方法生成了更短的路径，同时讨论了边缘侧计算以降低延迟和通信流量。</div>
</details>
</div>
<div class="card">
<div class="title">IROS: A Dual-Process Architecture for Real-Time VLM-Based Indoor Navigation</div>
<div class="meta-line">Authors: Joonhee Lee, Hyunseung Shin, Jeonggil Ko</div>
<div class="meta-line">First: 2026-01-29T10:25:14+00:00 · Latest: 2026-01-29T10:25:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21506v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21506v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Indoor mobile robot navigation requires fast responsiveness and robust semantic understanding, yet existing methods struggle to provide both. Classical geometric approaches such as SLAM offer reliable localization but depend on detailed maps and cannot interpret human-targeted cues (e.g., signs, room numbers) essential for indoor reasoning. Vision-Language-Action (VLA) models introduce semantic grounding but remain strictly reactive, basing decisions only on visible frames and failing to anticipate unseen intersections or reason about distant textual cues. Vision-Language Models (VLMs) provide richer contextual inference but suffer from high computational latency, making them unsuitable for real-time operation on embedded platforms. In this work, we present IROS, a real-time navigation framework that combines VLM-level contextual reasoning with the efficiency of lightweight perceptual modules on low-cost, on-device hardware. Inspired by Dual Process Theory, IROS separates fast reflexive decisions (System One) from slow deliberative reasoning (System Two), invoking the VLM only when necessary. Furthermore, by augmenting compact VLMs with spatial and textual cues, IROS delivers robust, human-like navigation with minimal latency. Across five real-world buildings, IROS improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IROS：基于实时视觉语言模型室内导航的双过程架构</div>
<div class="mono" style="margin-top:8px">室内移动机器人导航需兼具快速响应与鲁棒语义理解能力，现有方法难以同时满足。传统几何方法（如SLAM）虽能实现可靠定位，但依赖精细地图且无法解析室内推理必需的人为标识（如指示牌、房间号）。视觉-语言-动作模型引入了语义基础，但严格遵循反应式决策，仅依据可见帧进行判断，无法预判未见的交叉路口或推理远距离文本线索。视觉语言模型虽能提供更丰富的上下文推理，但存在高计算延迟问题，不适用于嵌入式平台的实时操作。本研究提出IROS实时导航框架，在低成本设备硬件上融合VLM级上下文推理与轻量感知模块的效率。受双过程理论启发，IROS将快速反射决策（系统一）与慢速审慎推理（系统二）分离，仅在必要时调用VLM。通过为紧凑型VLM增强空间与文本线索，IROS以最小延迟实现类人鲁棒导航。在五座真实建筑中的实验表明，相较于持续基于VLM的导航方案，IROS将决策准确率提升66%，并显著降低延迟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of achieving both real-time responsiveness and robust semantic understanding in indoor robot navigation, where classical geometric methods lack semantic interpretation and vision-language models (VLMs) suffer from high latency. The proposed IROS framework employs a dual-process architecture inspired by cognitive theory, separating fast reflexive decisions from slower deliberative reasoning to selectively invoke a VLM only when necessary, while augmenting compact VLMs with spatial and textual cues for contextual inference. Experimental results from five real-world buildings demonstrate that IROS significantly improves decision accuracy and reduces latency by 66% compared to continuous VLM-based navigation.</div>
<div class="mono" style="margin-top:8px">本研究针对现有室内导航方法在语义理解不足或计算延迟高方面的局限，提出了IROS，一种双过程架构，仅在必要时将快速反射性决策与视觉语言模型（VLM）的慢速审慎推理相结合。该方法将导航分离为轻量级感知模块用于实时操作，以及增强的VLM用于上下文推断，并通过空间和文本线索提升鲁棒性。在五个真实建筑中的实验结果表明，与连续基于VLM的导航相比，IROS提高了决策准确性并降低了66%的延迟。</div>
</details>
</div>
<div class="card">
<div class="title">Don&#x27;t double it: Efficient Agent Prediction in Occlusions</div>
<div class="meta-line">Authors: Anna Rothenhäusler, Markus Mazzola, Andreas Look, Raghu Rajan, Joschka Bödecker</div>
<div class="meta-line">First: 2026-01-29T10:22:38+00:00 · Latest: 2026-01-29T10:22:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21504v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21504v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Occluded traffic agents pose a significant challenge for autonomous vehicles, as hidden pedestrians or vehicles can appear unexpectedly, yet this problem remains understudied. Existing learning-based methods, while capable of inferring the presence of hidden agents, often produce redundant occupancy predictions where a single agent is identified multiple times. This issue complicates downstream planning and increases computational load. To address this, we introduce MatchInformer, a novel transformer-based approach that builds on the state-of-the-art SceneInformer architecture. Our method improves upon prior work by integrating Hungarian Matching, a state-of-the-art object matching algorithm from object detection, into the training process to enforce a one-to-one correspondence between predictions and ground truth, thereby reducing redundancy. We further refine trajectory forecasts by decoupling an agent&#x27;s heading from its motion, a strategy that improves the accuracy and interpretability of predicted paths. To better handle class imbalances, we propose using the Matthews Correlation Coefficient (MCC) to evaluate occupancy predictions. By considering all entries in the confusion matrix, MCC provides a robust measure even in sparse or imbalanced scenarios. Experiments on the Waymo Open Motion Dataset demonstrate that our approach improves reasoning about occluded regions and produces more accurate trajectory forecasts than prior methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>避免重复预测：遮挡场景下的高效智能体预测</div>
<div class="mono" style="margin-top:8px">遮挡交通参与者对自动驾驶车辆构成重大挑战，隐藏的行人或车辆可能突然出现，但该问题研究仍不充分。现有基于学习的方法虽能推断隐藏智能体的存在，却常产生冗余的占据预测——单个智能体被重复识别。这一问题使下游规划复杂化并增加计算负荷。为此，我们提出MatchInformer，一种基于先进SceneInformer架构的新型Transformer方法。该方法通过将目标检测领域先进的匈牙利匹配算法整合至训练过程，强制实现预测与真实值的一一对应，从而减少冗余。我们进一步通过解耦智能体航向与运动来优化轨迹预测，该策略提升了预测路径的准确性与可解释性。为更好处理类别不平衡问题，我们提出使用马修斯相关系数评估占据预测。MCC通过综合考虑混淆矩阵所有条目，即使在稀疏或不平衡场景下也能提供稳健度量。在Waymo开放运动数据集上的实验表明，本方法提升了对遮挡区域的推理能力，并较现有方法产生了更精确的轨迹预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of predicting occluded traffic agents for autonomous vehicles, where existing learning-based methods often generate redundant occupancy predictions that complicate planning and increase computational load. The proposed MatchInformer method enhances the SceneInformer architecture by integrating Hungarian Matching during training to enforce a one-to-one correspondence between predictions and ground truth, reducing redundancy, and decouples agent heading from motion to refine trajectory forecasts. Experiments on the Waymo Open Motion Dataset show that the approach improves reasoning in occluded regions and yields more accurate trajectory predictions compared to prior methods.</div>
<div class="mono" style="margin-top:8px">本研究针对自动驾驶中预测被遮挡交通参与者的挑战，现有基于学习的方法常产生冗余的占用预测，从而增加规划复杂性和计算负担。提出的MatchInformer方法基于SceneInformer架构，通过在训练中集成匈牙利匹配算法来强制预测与真实值之间的一一对应，减少冗余，并通过解耦智能体的航向与运动来优化轨迹预测。在Waymo开放运动数据集上的实验表明，该方法提升了对遮挡区域的推理能力，并产生了比先前方法更准确的轨迹预测，同时使用马修斯相关系数在类别不平衡情况下稳健评估占用预测。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
