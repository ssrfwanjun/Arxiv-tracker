<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 05:29</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0529</div>
    <div class="row"><div class="card">
<div class="title">Iterative Refinement Improves Compositional Image Generation</div>
<div class="meta-line">Authors: Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak</div>
<div class="meta-line">First: 2026-01-21T18:59:40+00:00 · Latest: 2026-01-21T18:59:40+00:00</div>
<div class="meta-line">Comments: Project webpage: https://iterative-img-gen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15286v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iterative-img-gen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代优化提升组合式图像生成质量</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型已取得显著进展，但在处理需要同时满足多对象、关系和属性的复杂提示时仍面临挑战。现有的推理时策略（如带验证器的并行采样或单纯增加去噪步数）虽能提升提示对齐度，但在需要满足多重约束的复杂组合场景中仍显不足。受大语言模型中思维链推理成功的启发，我们提出一种迭代式测试时策略：T2I模型在视觉语言模型作为循环评判者的反馈指导下，通过多步骤渐进优化生成结果。该方法无需外部工具或先验知识，可灵活适配各类图像生成器与视觉语言模型。实验表明，该方法在多个基准测试中均取得稳定提升：在ConceptMix（k=7）的全正确率提升16.9%，在T2I-CompBench（3D空间类别）提升13.8%，在Visual Jenga场景解构任务中提升12.5%（均与计算量匹配的并行采样对比）。除量化指标外，迭代优化通过将复杂提示分解为序列修正，生成结果更具忠实度——人工评估者对该方法的偏好率达58.7%（基线为41.3%）。这些发现共同表明，迭代式自校正可作为组合式图像生成的普适性优化原则。完整结果与可视化案例详见项目网页。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image models often fail to accurately generate images from complex prompts involving multiple objects and relations. To address this, the authors propose an iterative refinement strategy where a vision-language model provides feedback to guide the image generator across multiple denoising steps, decomposing the complex prompt into sequential corrections. Experimental results show significant improvements, including a 16.9% increase in all-correct rate on the ConceptMix benchmark and a 58.7% human preference rate over a parallel sampling baseline, demonstrating the effectiveness of iterative self-correction for compositional generation.</div>
<div class="mono" style="margin-top:8px">文本到图像模型在处理涉及多个物体、关系和属性的复杂提示时常常表现不佳。为此，研究者提出了一种迭代优化策略，利用视觉语言模型提供反馈，在多步去噪过程中引导图像生成器，将复杂提示分解为顺序修正。实验结果表明该方法带来了显著提升，如在ConceptMix基准上的全正确率提高了16.9%，并在人类评估中以58.7%的偏好率优于并行采样基线，证明了迭代自校正对于组合式图像生成的广泛适用性。</div>
</details>
</div>
<div class="card">
<div class="title">MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs</div>
<div class="meta-line">Authors: Christoph Bartmann, Johannes Schimunek, Mykyta Ielanskyi, Philipp Seidl, Günter Klambauer, Sohvi Luukkonen</div>
<div class="meta-line">First: 2026-01-21T18:58:01+00:00 · Latest: 2026-01-21T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15279v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A molecule&#x27;s properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolecularIQ：通过分子图符号验证表征化学推理能力</div>
<div class="mono" style="margin-top:8px">分子的性质根本上由其分子图所编码的组成与结构决定。因此，对分子性质进行推理需要解析和理解分子图的能力。大语言模型正日益应用于化学领域，处理诸如分子名称转换、描述生成、文本引导生成以及性质或反应预测等任务。现有基准大多侧重于通用化学知识，依赖文献或存在泄漏或偏差风险的替代标签，或将评估简化为多项选择题。我们提出MolecularIQ，一个专注于符号可验证任务的分子结构推理基准。MolecularIQ支持对分子图推理进行细粒度评估，并揭示将模型失败定位到特定任务和分子结构的能力模式。这为当前化学大语言模型的优势与局限提供了可操作的见解，并指导开发能够对分子结构进行可靠推理的模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate how well Large Language Models (LLMs) can reason about molecular properties, which are fundamentally determined by a molecule&#x27;s graph structure, as existing benchmarks often rely on general knowledge, potentially biased labels, or simplified multiple-choice formats. The method introduces MolecularIQ, a benchmark composed of symbolically verifiable tasks that require parsing and understanding molecular graphs, enabling a fine-grained analysis of reasoning capabilities. Key experimental findings reveal distinct capability patterns, localizing model failures to specific tasks and molecular structures, thereby providing actionable insights into the strengths and limitations of current chemistry LLMs and guiding the development of more faithful reasoning models.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要评估大语言模型（LLMs）对分子特性的推理能力，因为分子特性根本上由其图结构决定，而现有基准测试通常依赖通用知识、可能存在偏见的标签或简化的选择题形式。方法上，研究引入了MolecularIQ基准，它由一系列可符号验证的任务组成，要求模型解析和理解分子图，从而实现对推理能力的细粒度分析。关键的实验结果揭示了不同的能力模式，将模型失败定位到特定任务和分子结构，这为当前化学大语言模型的优势和局限性提供了可操作的见解，并指导开发更具忠实推理能力的模型。</div>
</details>
</div>
<div class="card">
<div class="title">RayRoPE: Projective Ray Positional Encoding for Multi-view Attention</div>
<div class="meta-line">Authors: Yu Wu, Minsik Jeon, Jen-Hao Rick Chang, Oncel Tuzel, Shubham Tulsiani</div>
<div class="meta-line">First: 2026-01-21T18:55:51+00:00 · Latest: 2026-01-21T18:55:51+00:00</div>
<div class="meta-line">Comments: Project page: https://rayrope.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15275v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15275v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rayrope.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the &#x27;predicted&#x27; 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RayRoPE：用于多视角注意力的投影射线位置编码</div>
<div class="mono" style="margin-top:8px">本研究针对处理多视角姿态输入图像序列的多视角变换器，探索一种位置编码机制：需实现图像块的唯一编码、支持具有多频相似性的SE(3)不变注意力，并能自适应底层场景几何结构。现有多视角注意力编码方案（绝对或相对式）均无法满足上述要求，为此我们提出RayRoPE解决方案。该方法基于关联射线表示图像块位置，但创新性地采用射线上的预测点而非射线方向进行几何感知编码。为实现SE(3)不变性，RayRoPE通过计算查询帧投影坐标来获得多频相似性。针对射线预测点可能存在的精度问题，本方法还提出在不确定性条件下解析计算期望位置编码的机制。在新颖视角合成与立体深度估计任务上的实验表明，RayRoPE持续优于现有位置编码方案（如在CO3D数据集上LPIPS指标相对提升15%）。研究还证明RayRoPE可无缝融合RGB-D输入，对此类信息无法进行位置编码的替代方案产生更显著优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for positional encodings in multi-view transformers that uniquely identify image patches, enable SE(3)-invariant attention with multi-frequency similarity, and adapt to underlying scene geometry. The proposed RayRoPE method represents patch positions using associated camera rays but leverages a predicted 3D point along each ray rather than ray direction for geometry-aware encoding; it achieves SE(3) invariance by computing query-frame projective coordinates for similarity measurement and analytically computes expected encodings under prediction uncertainty. Experimental validation on novel-view synthesis and stereo depth estimation tasks demonstrates consistent improvements over alternative encoding schemes, including a 15% relative LPIPS improvement on CO3D, with additional gains when incorporating RGB-D input.</div>
<div class="mono" style="margin-top:8px">本研究旨在为多视角Transformer设计一种位置编码方法，以唯一标识图像块、实现具有多频相似性的SE(3)不变注意力并适应场景几何，从而解决先前绝对或相对编码方案的不足。所提出的RayRoPE方法基于关联的相机射线表示块位置，但利用沿射线预测的3D点进行几何感知编码；它通过计算查询帧投影坐标实现相似性以达成SE(3)不变性，并解析计算预测不确定性下的期望编码。在新视角合成和立体深度估计任务上的实验验证表明，该方法持续优于其他编码方案（如在CO3D数据集上LPIPS指标相对提升15%），并能有效融合RGB-D输入以获得更大性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?</div>
<div class="meta-line">Authors: Felix Schur, Niklas Pfister, Peng Ding, Sach Mukherjee, Jonas Peters</div>
<div class="meta-line">First: 2026-01-21T18:36:34+00:00 · Latest: 2026-01-21T18:36:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15254v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\ell_1$-regularized estimation and post-selection refitting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多实验、少重复、非配对数据与稀疏效应：因果推断是否可行？</div>
<div class="mono" style="margin-top:8px">我们研究在隐藏混杂因素下估计因果效应的问题，数据为非配对设置：在不同实验条件（环境）下观测到协变量$X$和结果$Y$，但未同时观测二者；仅观测到$X$或$Y$。在适当正则条件下，该问题可转化为工具变量（IV）回归，其中环境作为（可能高维的）工具变量。当环境数量多但每个环境观测极少时，标准双样本IV估计量无法保持一致性。我们提出一种基于工具变量-协变量样本交叉折叠分割的GMM型估计量，并证明其在环境数量增长而每个环境样本量固定时具有一致性。进一步通过$\ell_1$正则化估计与后选择重拟合方法，将本方法扩展至稀疏因果效应场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of estimating causal effects from unpaired observational data where covariates and outcomes are recorded under different experimental conditions but never jointly observed, a scenario common in many real-world studies with hidden confounding. The authors propose a GMM-type estimator that leverages cross-fold sample splitting of the instrument-covariate sample, treating the environment as a high-dimensional instrumental variable, and extend it to sparse effects via ℓ₁-regularization and refitting. Their theoretical analysis and experiments demonstrate that this method achieves consistency as the number of environments increases, even when each environment has only a few observations, overcoming limitations of standard two-sample IV estimators.</div>
<div class="mono" style="margin-top:8px">本文针对未配对观测数据中因果效应估计的挑战展开研究，该场景下协变量和结果在不同实验条件下分别记录但从未联合观测，这在存在隐藏混杂的真实世界研究中很常见。作者提出了一种基于工具变量-协变量样本交叉折叠分割的GMM型估计器，将环境视为高维工具变量，并通过ℓ₁正则化和后选择重拟合扩展至稀疏效应情形。理论分析表明，该估计器在环境数量增加时具有一致性，即使每个环境仅包含少量观测，从而克服了标准两样本工具变量方法的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism</div>
<div class="meta-line">Authors: Garrett G. Wen, Buxin Su, Natalie Collina, Zhun Deng, Weijie Su</div>
<div class="meta-line">First: 2026-01-21T18:30:42+00:00 · Latest: 2026-01-21T18:30:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15249v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors&#x27; assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions&#x27; ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于等渗机制的ML/AI会议最佳论文奖推荐方法</div>
<div class="mono" style="margin-top:8px">NeurIPS、ICML等机器学习与人工智能会议如今常收到数万篇投稿，这对维持同行评审的质量与一致性构成重大挑战。最佳论文奖的评选尤为突出——它虽是评审流程的重要环节，近年却日益引发争议。本文提出一种作者辅助机制以优化最佳论文评选。该方法采用等渗机制收集作者对自身投稿的排序评估，并据此调整原始评审分数，从而最优估计论文的真实质量。我们证明：当作者的效用函数为调整后分数的凸可加函数时，作者有动机如实报告；利用ICLR（2019-2023）和NeurIPS（2021-2023）公开评审数据，我们验证了最佳论文奖场景下该凸性假设的合理性。关键的是，在作者仅有一个提名配额（即只能提名一篇论文）的特殊情况下，我们证明即使效用函数仅为非递减可加函数，真实性依然成立——这显著放宽了既有研究所需的假设条件。为实际应用，我们扩展该机制以适应常见的作者重叠场景。仿真结果表明，本机制能显著提升获奖论文质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The surge in submissions to major ML/AI conferences like NeurIPS and ICML has strained the peer review process, especially in selecting credible best paper awards. To address this, the authors propose an author-assisted mechanism that uses the Isotonic Mechanism to elicit truthful rankings from authors about their own submissions, which are then used to adjust raw review scores for better estimation of paper quality. Theoretical analysis shows authors are incentivized to report truthfully under convex additive utility, with a key relaxation to merely nondecreasing additive utility for single-quota cases, validated on ICLR (2019-2023) and NeurIPS (2021-2023) data. Simulations confirm the mechanism significantly enhances the quality of awarded papers.</div>
<div class="mono" style="margin-top:8px">随着NeurIPS、ICML等主要机器学习与人工智能会议投稿量激增，同行评审的质量与一致性面临严峻挑战，最佳论文奖的评选尤其引发争议。为此，研究者提出一种作者辅助机制，采用等渗机制收集作者对自身投稿的私下排序，并利用这些排序对原始评审分数进行等渗调整，以更准确地估计论文的真实质量。理论分析表明，在凸可加效用函数下作者有如实报告的激励，这一条件通过对ICLR（2019-2023）和NeurIPS（2021-2023）公开评审数据的验证得到支持；对于仅有一个提名配额（即只能提名一篇论文）的作者，即使在更弱的非递减可加效用下也能保证真实性。该机制进一步扩展以适应常见的作者重叠情形，模拟实验证明其能显著提升获奖论文的质量。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-context principal component analysis</div>
<div class="meta-line">Authors: Kexin Wang, Salil Bhate, João M. Pereira, Joe Kileel, Matylda Figlerowicz, Anna Seigal</div>
<div class="meta-line">First: 2026-01-21T18:24:32+00:00 · Latest: 2026-01-21T18:24:32+00:00</div>
<div class="meta-line">Comments: 47 pages, 8 figures. Supplementary tables are provided as downloadable file</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15239v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15239v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多情境主成分分析</div>
<div class="mono" style="margin-top:8px">主成分分析（PCA）是一种捕捉数据变异解释因子的工具。当前各领域数据常采集于多重情境（例如不同疾病的个体、不同类型的细胞或跨文本的词汇）。虽然解释数据变异的因子在不同情境子集间必然存在共享，但目前缺乏系统性恢复此类因子的工具。我们开发了多情境主成分分析（MCPCA）——一个理论与算法框架，可将数据分解为跨情境子集共享的因子。应用于基因表达数据时，MCPCA揭示了跨癌症类型子集共享的变异轴，以及一个在肿瘤细胞中变异度（非均值）与肺癌进展相关的轴线。应用于语言模型的语境化词向量时，MCPCA映射出关于人性辩论的演进阶段，揭示了数十年来科学与虚构领域的对话脉络。这些轴线无法通过跨情境数据合并或局限于单一情境的分析获得。MCPCA是PCA的原则性泛化，旨在应对理解跨情境数据底层因子的挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Principal component analysis (PCA) is limited when data are collected across multiple distinct contexts, as it cannot systematically identify factors shared across subsets of these contexts. To address this, the authors develop Multi-context Principal Component Analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors that are shared across specific subsets of contexts rather than all contexts or just one. In experiments on gene expression data, MCPCA identified axes of variation shared across subsets of cancer types and an axis where variability in tumor cells, but not the mean, was linked to lung cancer progression; in analyzing contextualized word embeddings, it mapped stages of a debate on human nature, revealing a discussion between science and fiction over decades, findings not obtainable by simply pooling all data or analyzing contexts individually.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要识别跨多个上下文（如不同疾病或文本类型）数据中的共享因子，而现有方法如主成分分析无法系统性地恢复此类跨上下文因子。作者开发了多上下文主成分分析（MCPCA），这是一个将数据分解为跨上下文子集共享因子的框架，扩展了PCA以处理多上下文数据集。关键实验结果表明，在基因表达数据中，MCPCA成功识别了跨癌症类型子集共享的变异轴，包括一个通过肿瘤细胞变异性与肺癌进展相关的轴；在语言模型的上下文词嵌入中，它映射了辩论阶段，揭示了数十年间科学与虚构之间的互动，这些发现无法通过组合或隔离单个上下文来检测。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion</div>
<div class="meta-line">Authors: Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang</div>
<div class="meta-line">First: 2026-01-20T05:00:26+00:00 · Latest: 2026-01-21T18:21:39+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13599v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13599v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a &#x27;draft-then-refine&#x27; framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model&#x27;s global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散中的扩散：在半自回归扩散模型中重获全局连贯性</div>
<div class="mono" style="margin-top:8px">全局离散扩散语言模型最引人注目的特征之一是其全局双向上下文能力。然而，现有的基于块的扩散研究倾向于引入自回归先验，这虽然带来益处，却可能导致模型在宏观层面失去全局连贯性。为在保留半自回归范式优势的同时重获全局上下文理解能力，我们提出&#x27;扩散中的扩散&#x27;——一种&#x27;先草拟后精修&#x27;的框架，旨在克服块扩散模型固有的不可逆性与短视问题。该方法首先采用块扩散通过小模块快速生成草稿，随后通过具有更大双向感受野的全局双向扩散对这些草稿进行精修。我们利用快照置信度重掩码技术识别需要修改的最关键词元，并应用混合尺度训练来扩展块扩散模型的全局能力。实验结果表明，我们的方法在OpenWebText数据集上为离散扩散模型设立了新基准。仅使用基线模型26%的微调预算，便将生成困惑度从25.7降至21.9，显著缩小了与自回归模型的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the loss of global coherence in block-based diffusion language models due to autoregressive priors, aiming to restore bidirectional contextual understanding while retaining semi-autoregressive efficiency. The proposed Diffusion in Diffusion framework employs a two-stage process: first generating draft text rapidly via block diffusion with small blocks, then refining it through global bidirectional diffusion to enhance coherence, utilizing snapshot confidence remasking to pinpoint tokens needing modification and mix-scale training to improve global modeling. Experimental results on OpenWebText show the method sets a new benchmark for discrete diffusion models, reducing generative perplexity from 25.7 to 21.9 with only 26% of the fine-tuning budget of baselines, thereby narrowing the performance gap with autoregressive models.</div>
<div class="mono" style="margin-top:8px">本研究针对基于块的扩散语言模型因自回归先验而丧失全局连贯性的问题，旨在恢复双向上下文理解能力，同时保持半自回归效率。提出的Diffusion in Diffusion框架采用两阶段流程：首先通过小块块扩散快速生成草稿文本，然后通过全局双向扩散进行细化以增强连贯性，利用快照置信度重掩码识别需要修改的关键词元，并应用混合尺度训练提升全局能力。在OpenWebText数据集上的实验表明，该方法为离散扩散模型设立了新基准，仅用基线模型26%的微调预算就将生成困惑度从25.7降至21.9，显著缩小了与自回归模型的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results</div>
<div class="meta-line">Authors: Francesco Orabona, Ryan D&#x27;Orazio</div>
<div class="meta-line">First: 2025-05-26T17:00:27+00:00 · Latest: 2026-01-21T18:20:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20219v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Polyak stepsize has been proven to be a fundamental stepsize in convex optimization, giving near optimal gradient descent rates across a wide range of assumptions. The universality of the Polyak stepsize has also inspired many stochastic variants, with theoretical guarantees and strong empirical performance. Despite the many theoretical results, our understanding of the convergence properties and shortcomings of the Polyak stepsize or its variants is both incomplete and fractured across different analyses. We propose a new, unified, and simple perspective for the Polyak stepsize and its variants as gradient descent on a surrogate loss. We show that each variant is equivalent to minimize a surrogate function with stepsizes that adapt to a guaranteed local curvature. Our general surrogate loss perspective is then used to provide a unified analysis of existing variants across different assumptions. Moreover, we show a number of negative results proving that the non-convergence results in some of the upper bounds is indeed real.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Polyak步长新视角：代理函数与负面结果</div>
<div class="mono" style="margin-top:8px">Polyak步长已被证明是凸优化中的基础步长，在广泛假设下提供接近最优的梯度下降速率。其普适性也催生了多种随机变体，兼具理论保证与强实证性能。尽管已有大量理论成果，我们对Polyak步长及其变体的收敛特性与局限性的理解仍不完整，且散见于不同分析中。本文提出一种新颖、统一且简洁的视角：将Polyak步长及其变体视为对代理损失的梯度下降。我们证明每种变体都等价于最小化一个代理函数，其步长自适应于有保障的局部曲率。这一通用代理损失视角被用于统一分析现有变体在不同假设下的表现。此外，我们通过多项负面结果证实，某些上界中的非收敛结论确实存在。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the fragmented understanding of Polyak stepsize convergence properties by proposing a unified surrogate loss perspective. The method demonstrates that Polyak stepsize variants are equivalent to gradient descent on a surrogate function with curvature-adaptive stepsizes, enabling a consolidated analysis across different assumptions. Experimental findings reveal fundamental non-convergence limitations in existing upper bounds, establishing negative results that confirm theoretical shortcomings.</div>
<div class="mono" style="margin-top:8px">本研究针对凸优化中Polyak步长及其随机变体的收敛性质和局限性的理解不完整且零散的问题，提出了一个新的统一视角。作者将这些方法解释为应用于代理损失函数的梯度下降，其中步长根据有保证的局部曲率进行自适应调整。分析表明，该框架能够统一不同假设下的现有变体，并揭示了负面结果，证实了某些上界所指示的不收敛行为确实是固有的。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification</div>
<div class="meta-line">Authors: Fabi Nahian Madhurja, Rusab Sarmun, Muhammad E. H. Chowdhury, Adam Mushtak, Israa Al-Hashimi, Sohaib Bassam Zoghoul</div>
<div class="meta-line">First: 2026-01-21T18:15:47+00:00 · Latest: 2026-01-21T18:15:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15235v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model&#x27;s performance with expert radiologists, demonstrating competitive results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在二维笔画中追踪三维解剖结构：一种用于颈椎骨折识别的多阶段投影驱动方法</div>
<div class="mono" style="margin-top:8px">颈椎骨折是需精确高效检测以进行有效临床处理的关键医疗状况。本研究探讨了基于二维投影的椎骨分割在三维CT体积中实现椎骨级骨折检测的可行性，提出了一种用于自动化分析颈椎（C1-C7）的端到端流程。通过优化的二维轴向、矢状和冠状投影近似三维体积，利用YOLOv8模型从所有视图中识别感兴趣区域并组合以近似三维颈椎区域，实现了94.45%的三维mIoU。与传统三维分割方法相比，这种基于投影的定位策略在保持高性能的同时降低了计算复杂度。随后采用基于DenseNet121-Unet的多标签分割，利用基于方差和能量的投影，实现了87.86%的Dice分数。从这些二维分割掩模中战略性地近似三维椎骨掩模，能够提取单个椎骨体积。通过集成2.5D时空序列模型分析这些体积的骨折情况，该模型结合每个椎骨的原始切片和投影进行互补评估，在椎骨级和患者级分别实现了68.15和82.26的F1分数，以及91.62和83.04的ROC-AUC分数。我们进一步通过可解释性研究验证了该方法，该研究提供了突出诊断相关解剖区域的显著性图可视化，并通过与专家放射科医生比较模型性能的观察者间变异性分析，展示了具有竞争力的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Cervical spine fractures require accurate detection for clinical management, motivating an automated approach to identify fractures from 3D CT volumes using efficient 2D projections. The method employs a multi-stage pipeline: first, YOLOv8 localizes vertebrae by combining optimized axial, sagittal, and coronal projections, achieving a 3D mIoU of 94.45% for spine area approximation; then, DenseNet121-Unet performs multi-label segmentation on variance- and energy-based projections, attaining a Dice score of 87.86%; finally, an ensemble of 2.5D Spatio-Sequential models analyzes extracted vertebra volumes for fractures, yielding vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. Experimental validation includes explainability visualizations and interobserver analysis showing competitive performance against expert radiologists.</div>
<div class="mono" style="margin-top:8px">颈椎骨折的临床管理需要精确检测，这促使研究采用基于二维投影的自动化方法从三维CT体积中识别骨折，以降低计算复杂度。该方法采用多阶段流程：首先，使用YOLOv8从优化的轴向、矢状和冠状投影中定位椎骨，实现脊柱区域近似，三维mIoU达94.45%；接着，基于DenseNet121-Unet在方差和能量投影上进行多标签分割，Dice分数达87.86%；最后，通过集成2.5D时空序列模型分析提取的椎骨体积以检测骨折，获得椎骨级别和患者级别的F1分数分别为68.15和82.26，ROC-AUC分数分别为91.62和83.04。实验验证包括可解释性显著性图谱和观察者间变异性分析，显示模型与专家放射科医生相比具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">From Construction to Injection: Edit-Based Fingerprints for Large Language Models</div>
<div class="meta-line">Authors: Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo, Linlin Wang</div>
<div class="meta-line">First: 2025-09-03T08:22:04+00:00 · Latest: 2026-01-21T17:56:42+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03122v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.03122v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Establishing reliable and verifiable fingerprinting mechanisms is fundamental to controlling the unauthorized redistribution of large language models (LLMs). However, existing approaches face two major challenges: (a) ensuring imperceptibility, including resistance to statistical identification and avoidance of accidental activation during fingerprint construction, and (b) preserving both model utility and fingerprint detectability under subsequent model modifications. To address these challenges, we propose an end-to-end fingerprinting framework with two components. First, we design a rule-based code-mixing fingerprint (CF) that maps natural-query-like prompts to multi-candidate targets, reducing accidental triggering via high-complexity code-mixing formulations. Second, we introduce Multi-Candidate Editing (MCEdit), which jointly optimizes multi-candidate targets and enforces margins between target and non-target outputs to improve post-modification detectability. Extensive experiments demonstrate that our framework provides a robust and practical solution for fingerprinting LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从构建到注入：基于编辑的大型语言模型指纹方法</div>
<div class="mono" style="margin-top:8px">建立可靠且可验证的指纹机制对于控制大型语言模型（LLMs）的未经授权再分发至关重要。然而，现有方法面临两大挑战：（a）确保不可感知性，包括抵抗统计识别并避免指纹构建过程中的意外激活；（b）在后续模型修改下同时保持模型效用和指纹可检测性。为解决这些挑战，我们提出了一个包含两个组件的端到端指纹框架。首先，我们设计了一种基于规则的代码混合指纹（CF），将类自然查询的提示映射到多候选目标，通过高复杂度的代码混合公式减少意外触发。其次，我们引入了多候选编辑（MCEdit），联合优化多候选目标并强制目标与非目标输出之间的边界，以提高修改后的可检测性。大量实验表明，我们的框架为LLMs指纹识别提供了稳健且实用的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for reliable and verifiable fingerprinting to prevent unauthorized redistribution of large language models (LLMs), tackling challenges in ensuring imperceptibility and maintaining utility and detectability after model modifications. The proposed method is an end-to-end framework featuring a rule-based code-mixing fingerprint to reduce accidental activation and a Multi-Candidate Editing (MCEdit) technique that jointly optimizes multiple target outputs to enhance robustness. Experimental results show the framework provides a robust and practical fingerprinting solution for LLMs.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型（LLMs）未经授权分发的控制问题，通过建立可靠且可验证的指纹机制，应对指纹不可感知性和模型修改后鲁棒性的挑战。方法提出一个端到端框架，包含基于规则的代码混合指纹以减少意外触发，以及多候选编辑（MCEdit）技术，通过联合优化多候选目标并强制目标与非目标输出之间的边界来提升修改后的可检测性。大量实验表明，该框架为LLMs指纹识别提供了稳健且实用的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">QueStER: Query Specification for Generative keyword-based Retrieval</div>
<div class="meta-line">Authors: Arthur Satouf, Yuxuan Zong, Habiboulaye Amadou-Boubacar, Pablo Piantanida, Benjamin Piwowarski</div>
<div class="meta-line">Venue: eACL 2026</div>
<div class="meta-line">First: 2025-11-07T15:01:38+00:00 · Latest: 2026-01-21T17:37:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05301v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05301v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative retrieval (GR) differs from the traditional index-then-retrieve pipeline by storing relevance in model parameters and generating retrieval cues directly from the query, but it can be brittle out of domain and expensive to scale. We introduce QueStER (QUEry SpecificaTion for gEnerative Keyword-Based Retrieval), which bridges GR and query reformulation by learning to generate explicit keyword-based search specifications. Given a user query, a lightweight LLM produces a keyword query that is executed by a standard retriever (BM25), combining the generalization benefits of generative query rewriting with the efficiency and scalability of lexical indexing. We train the rewriting policy with reinforcement learning techniques. Across in- and out-of-domain evaluations, QueStER consistently improves over BM25 and is competitive with neural IR baselines, while maintaining strong efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QueStER：基于生成式关键词检索的查询规范方法</div>
<div class="mono" style="margin-top:8px">生成式检索（GR）通过将相关性存储在模型参数中并直接从查询生成检索线索，与传统“先索引后检索”流程不同，但其存在跨领域脆弱性和扩展成本高的问题。本文提出QueStER（基于生成式关键词检索的查询规范方法），通过学习生成显式的基于关键词的搜索规范，桥接生成式检索与查询重构。给定用户查询时，轻量级大语言模型生成可由标准检索器（BM25）执行的关键词查询，从而融合生成式查询重写的泛化优势与词法索引的效率及可扩展性。我们采用强化学习技术训练重写策略。在领域内外的评估中，QueStER持续优于BM25，并与神经信息检索基线模型表现相当，同时保持高效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the brittleness and scalability challenges of generative retrieval, this research introduces QueStER, a method that bridges generative retrieval and query reformulation. The approach uses a lightweight LLM to generate explicit keyword-based search specifications from a user query, which are then executed by a standard BM25 retriever, combining the generalization of generative rewriting with the efficiency of lexical indexing. The rewriting policy is trained with reinforcement learning. Experimental results across in- and out-of-domain evaluations show that QueStER consistently improves over BM25 and is competitive with neural IR baselines while maintaining strong efficiency.</div>
<div class="mono" style="margin-top:8px">该研究针对生成式检索的脆弱性和可扩展性问题，提出了QueStER方法，以桥接生成式检索与查询重构。该方法使用一个轻量级大语言模型，根据用户查询生成明确的关键词搜索指令，然后由标准的BM25检索器执行，从而结合了生成式重写的泛化能力与词法索引的效率；其重写策略通过强化学习进行训练。在领域内和领域外的评估实验中，QueStER一致性地超越了BM25，并与神经信息检索基线模型表现相当，同时保持了高效性。</div>
</details>
</div>
<div class="card">
<div class="title">ZENITH: Automated Gradient Norm Informed Stochastic Optimization</div>
<div class="meta-line">Authors: Dhrubo Saha</div>
<div class="meta-line">First: 2026-01-21T17:36:12+00:00 · Latest: 2026-01-21T17:36:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training deep computer vision models requires manual oversight or hyperparameter tuning of the learning rate (LR) schedule. While existing adaptive optimizers schedule the LR automatically, they suffer from computational and memory overhead, incompatibility with regularization, and suboptimal LR choices. In this work, we introduce the ZENITH (Zero-overhead Evolution using Norm-Informed Training History) optimizer, which adapts the LR using the temporal evolution of the gradient norm. Image classification experiments spanning 6 CNN architectures and 6 benchmarks demonstrate that ZENITH achieves higher test accuracy in lower wall-clock time than baselines. It also yielded superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO using the R-CNN family of models. Furthermore, its compatibility with regularization enables even better generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ZENITH：基于梯度范数信息的自动随机优化方法</div>
<div class="mono" style="margin-top:8px">训练深度计算机视觉模型通常需要人工监控或对学习率（LR）调度进行超参数调优。现有的自适应优化器虽能自动调度学习率，但存在计算与内存开销大、与正则化方法不兼容以及学习率选择欠佳等问题。本研究提出ZENITH（基于范数训练历史演化的零开销）优化器，通过梯度范数的时间演化动态调整学习率。在涵盖6种CNN架构和6个基准测试的图像分类实验中，ZENITH在更短的挂钟时间内实现了比基线方法更高的测试准确率。在使用R-CNN系列模型的MS COCO数据集上，该方法在目标检测、关键点检测和实例分割任务中也取得了更优的mAP指标。此外，其与正则化技术的兼容性进一步提升了模型的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational overhead, regularization incompatibility, and suboptimal learning rate choices of existing adaptive optimizers, this work introduces ZENITH, an optimizer that automatically schedules the learning rate based on the temporal evolution of the gradient norm. Experiments on image classification across six CNN architectures and benchmarks show that ZENITH achieves higher test accuracy in less wall-clock time than baselines; it also yields superior mAP in object detection, keypoint detection, and instance segmentation on MS COCO with R-CNN models, and its compatibility with regularization further improves generalization.</div>
<div class="mono" style="margin-top:8px">该研究针对深度视觉模型中学习率需手动调整的问题，现有自适应优化器存在计算开销大、与正则化不兼容等局限性。提出的ZENITH优化器通过梯度范数的时序演化来自适应调整学习率，无需人工设计调度方案。在六种CNN架构和基准的图像分类实验中，ZENITH在更短的挂钟时间内实现了更高的测试准确率；在MS COCO数据集上，使用R-CNN系列模型进行目标检测、关键点检测和实例分割也获得了更优的mAP，且其与正则化的兼容性进一步提升了泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Nonnegative Low-rank Matrix Recovery Can Have Spurious Local Minima</div>
<div class="meta-line">Authors: Richard Y. Zhang</div>
<div class="meta-line">First: 2025-05-06T17:43:35+00:00 · Latest: 2026-01-21T17:07:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.03717v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.03717v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-rank matrix recovery is well-known to exhibit benign nonconvexity under the restricted isometry property (RIP): every second-order critical point is globally optimal, so local methods provably recover the ground truth. Motivated by the strong empirical performance of projected gradient methods for nonnegative low-rank recovery problems, we investigate whether this benign geometry persists when the factor matrices are constrained to be elementwise nonnegative. In the simple setting of a rank-1 nonnegative ground truth, we confirm that benign nonconvexity holds in the fully-observed case with RIP constant $δ=0$. This benign nonconvexity, however, is unstable. It fails to extend to the partially-observed case with any arbitrarily small RIP constant $δ&gt;0$, and to higher-rank ground truths $r^{\star}&gt;1$, regardless of how much the search rank $r\ge r^{\star}$ is overparameterized. Together, these results undermine the standard stability-based explanation for the empirical success of nonconvex methods and suggest that fundamentally different tools are needed to analyze nonnegative low-rank recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非负低秩矩阵恢复可能存在伪局部极小值</div>
<div class="mono" style="margin-top:8px">在受限等距性质（RIP）条件下，低秩矩阵恢复已知具有良性非凸性：所有二阶临界点均为全局最优点，因此局部方法可证明恢复真实解。受投影梯度法在非负低秩恢复问题中优异实证表现的启发，本研究探讨当因子矩阵被约束为元素非负时，这种良性几何特性是否依然存在。在秩为1的非负真实解这一简单设定下，我们证实了在RIP常数δ=0的完全观测情况下良性非凸性成立。然而这种良性非凸性并不稳定：它无法推广到任意小RIP常数δ&gt;0的部分观测情况，也无法适用于更高秩的真实解r*＞1——无论搜索秩r≥r*被过参数化到何种程度。这些结果共同动摇了基于稳定性的非凸方法实证成功标准解释，表明需要从根本上不同的工具来分析非负低秩恢复问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates whether the benign nonconvex geometry observed in low-rank matrix recovery under the restricted isometry property (RIP) extends to nonnegative constraints, motivated by the strong empirical performance of projected gradient methods in such settings. By analyzing a simple rank-1 nonnegative ground truth, the authors find that benign nonconvexity holds in the fully-observed case with RIP constant δ=0, but this property is unstable: it fails under any arbitrarily small RIP constant δ&gt;0 in partially-observed cases and for higher-rank ground truths, regardless of overparameterization. These results challenge the stability-based explanation for the success of nonconvex methods and indicate a need for new analytical tools in nonnegative low-rank recovery.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在受限等距性质（RIP）下低秩矩阵恢复中观察到的良性非凸几何是否适用于非负约束，其动机是投影梯度方法在此类设置中表现出的强大实证性能。针对秩为1的非负真实矩阵，作者发现完全观测情况下（RIP常数δ=0）存在良性非凸性，但这一性质不稳定：在部分观测（任意δ&gt;0）及更高秩的真实矩阵中均不成立，无论过参数化程度如何。这些结果挑战了基于稳定性的非凸方法成功解释，并表明需要新的分析工具。</div>
</details>
</div>
<div class="card">
<div class="title">A Comparative Evaluation of Deep Learning Models for Speech Enhancement in Real-World Noisy Environments</div>
<div class="meta-line">Authors: Md Jahangir Alam Khondkar, Ajan Ahmed, Stephanie Schuckers, Masudul Haider Imtiaz</div>
<div class="meta-line">First: 2025-06-17T22:12:40+00:00 · Latest: 2026-01-21T17:05:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.15000v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.15000v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech enhancement, particularly denoising, is vital in improving the intelligibility and quality of speech signals for real-world applications, especially in noisy environments. While prior research has introduced various deep learning models for this purpose, many struggle to balance noise suppression, perceptual quality, and speaker-specific feature preservation, leaving a critical research gap in their comparative performance evaluation. This study benchmarks three state-of-the-art models Wave-U-Net, CMGAN, and U-Net, on diverse datasets such as SpEAR, VPQAD, and Clarkson datasets. These models were chosen due to their relevance in the literature and code accessibility. The evaluation reveals that U-Net achieves high noise suppression with SNR improvements of +71.96% on SpEAR, +64.83% on VPQAD, and +364.2% on the Clarkson dataset. CMGAN outperforms in perceptual quality, attaining the highest PESQ scores of 4.04 on SpEAR and 1.46 on VPQAD, making it well-suited for applications prioritizing natural and intelligible speech. Wave-U-Net balances these attributes with improvements in speaker-specific feature retention, evidenced by VeriSpeak score gains of +10.84% on SpEAR and +27.38% on VPQAD. This research indicates how advanced methods can optimize trade-offs between noise suppression, perceptual quality, and speaker recognition. The findings may contribute to advancing voice biometrics, forensic audio analysis, telecommunication, and speaker verification in challenging acoustic conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>真实噪声环境下语音增强深度学习模型的比较评估</div>
<div class="mono" style="margin-top:8px">语音增强，特别是去噪，对于提升真实应用场景（尤其是噪声环境）中语音信号的可懂度和质量至关重要。尽管已有研究提出了多种深度学习模型，但许多模型难以在噪声抑制、感知质量和说话人特征保留之间取得平衡，其比较性能评估存在关键研究空白。本研究在SpEAR、VPQAD和Clarkson等多个数据集上对Wave-U-Net、CMGAN和U-Net三种前沿模型进行了基准测试。选择这些模型基于其在文献中的相关性和代码可获取性。评估结果显示，U-Net在噪声抑制方面表现突出，在SpEAR、VPQAD和Clarkson数据集上的信噪比分别提升了+71.96%、+64.83%和+364.2%。CMGAN在感知质量上最优，在SpEAR和VPQAD数据集上分别获得最高的PESQ分数4.04和1.46，适合优先追求自然可懂语音的应用。Wave-U-Net则平衡了这些特性，在说话人特征保留方面有所改进，在SpEAR和VPQAD数据集上的VeriSpeak分数分别提升了+10.84%和+27.38%。本研究揭示了先进方法如何优化噪声抑制、感知质量与说话人识别之间的权衡。这些发现可能有助于推动声学挑战环境下的声纹识别、司法音频分析、电信通信和说话人验证技术的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need to balance noise suppression, perceptual quality, and speaker feature preservation in speech enhancement for real-world noisy environments. It conducts a comparative evaluation of three deep learning models—Wave-U-Net, CMGAN, and U-Net—on the SpEAR, VPQAD, and Clarkson datasets. Experimental results show that U-Net excels in noise suppression with significant SNR improvements, CMGAN achieves the best perceptual quality as measured by PESQ scores, and Wave-U-Net offers a balanced performance with notable gains in speaker-specific feature retention.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估深度学习模型在现实嘈杂环境中进行语音增强时，如何平衡噪声抑制、感知质量和说话人特征保留。它对Wave-U-Net、CMGAN和U-Net三种模型在SpEAR、VPQAD和Clarkson数据集上进行了基准测试。实验结果表明，U-Net在噪声抑制方面表现突出，信噪比提升显著；CMGAN在感知质量上表现最佳，获得了最高的PESQ分数；而Wave-U-Net则提供了较为均衡的性能，在说话人特征保留方面取得了明显增益。</div>
</details>
</div>
<div class="card">
<div class="title">Which Similarity-Sensitive Entropy (S-entropy)?</div>
<div class="meta-line">Authors: Phuc Nguyen, Josiah Couch, Rahul Bansal, Alexandra Morgan, Chris Tam, Miao Li, Rima Arnaout, Ramy Arnaout</div>
<div class="meta-line">First: 2025-11-05T20:39:29+00:00 · Latest: 2026-01-21T17:02:43+00:00</div>
<div class="meta-line">Comments: 21 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.03849v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.03849v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Shannon entropy is not the only entropy that is relevant to machine-learning datasets, nor possibly even the most important one. Traditional entropies such as Shannon entropy capture information represented by elements&#x27; frequencies but not the richer information encoded by their similarities and differences. Capturing the latter requires similarity-sensitive entropy (S-entropy). S-entropy can be measured using either the recently developed Leinster-Cobbold-Reeve framework (LCR) or the newer Vendi score (VS). This raises the practical question of which one to use: LCR or VS. Here we address this question conceptually, analytically, and experimentally, using 53 large and well-known imaging and tabular datasets. We find that LCR and VS values can differ by orders of magnitude and are complementary, except in limiting cases. We show that both LCR and VS results depend on how similarities are scaled, and introduce the notion of ``half-distance&#x27;&#x27; to parameterize this dependence. We prove that VS provides an upper bound on LCR for several values of the Rényi-Hill order parameter and present evidence that this bound holds for all values. We conclude that VS is preferable only when a dataset&#x27;s elements can be usefully interpreted as linear combinations of a more fundamental set of ``ur-elements&#x27;&#x27; or when the system that the dataset describes has a quantum-mechanical character. In the broader case where one simply wishes to capture the rich information encoded by elements&#x27; similarities and differences as well as their frequencies, LCR is favored; nevertheless, for certain half-distances the two methods can complement each other.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>选择哪种相似性敏感熵（S-熵）？</div>
<div class="mono" style="margin-top:8px">香农熵并非机器学习数据集中唯一相关的熵，甚至可能不是最重要的熵。传统熵（如香农熵）仅捕捉元素频率信息，而无法涵盖其相似性与差异性所编码的丰富信息。捕捉后者需要相似性敏感熵（S-熵）。S-熵可通过新近发展的Leinster-Cobbold-Reeve框架（LCR）或更新的Vendi评分（VS）进行度量，这引出了实际选择问题：LCR还是VS？本文通过概念分析、理论推导和实验验证（使用53个大型知名图像与表格数据集）探讨该问题。研究发现，除极限情况外，LCR与VS的数值可能相差数个数量级且具有互补性。我们证明两者结果均受相似性缩放方式影响，并引入“半距离”概念参数化这种依赖关系。理论证明在多个Rényi-Hill阶参数取值下，VS为LCR提供上界，并有证据表明该界对所有参数值成立。结论表明：仅当数据集元素可有效解释为更基础“本原元素”集的线性组合，或数据集描述的系统具有量子力学特征时，VS更适用；而在更普遍情况下，若需同时捕捉元素频率及其相似性/差异性编码的丰富信息，则LCR更优。但特定半距离下两种方法可互为补充。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of Shannon entropy in capturing similarity information among dataset elements, this study compares two similarity-sensitive entropy measures: the Leinster-Cobbold-Reeve (LCR) framework and the Vendi score (VS). The method involves conceptual, analytical, and experimental evaluation across 53 large imaging and tabular datasets, introducing the &#x27;half-distance&#x27; parameter to address similarity scaling. Key findings show that LCR and VS values can differ by orders of magnitude and are generally complementary, with VS providing an upper bound on LCR for several Rényi-Hill orders, and evidence suggests this bound holds for all orders. The authors conclude that VS is preferable only when dataset elements can be interpreted as linear combinations of fundamental &#x27;ur-elements&#x27; or in quantum-mechanical contexts, while LCR is favored for broadly capturing similarity and frequency information, though the two can complement each other at certain half-distances.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决传统熵（如香农熵）仅能捕捉元素频率信息，而无法涵盖相似性和差异性等更丰富信息的问题，因此比较了两种相似性敏感熵度量方法：Leinster-Cobbold-Reeve框架（LCR）和Vendi评分（VS）。方法包括概念性、分析性和实验性分析，使用了53个大型图像和表格数据集，并引入“半距离”参数来量化相似性缩放的影响。主要实验结果表明，LCR和VS的值可能相差数个数量级且通常互补，VS为多个Rényi-Hill阶参数提供了LCR的上界，证据表明这一关系适用于所有参数；在广泛捕捉相似性和频率信息时，LCR更受青睐，而VS仅适用于量子力学系统或可解释为基本元素线性组合的数据集等特定场景。</div>
</details>
</div>
<div class="card">
<div class="title">RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors</div>
<div class="meta-line">Authors: Miaomiao Cai, Zhijie Zhang, Junfeng Fang, Zhiyong Cheng, Xiang Wang, Meng Wang</div>
<div class="meta-line">Venue: WWW2026</div>
<div class="meta-line">First: 2026-01-13T16:34:17+00:00 · Latest: 2026-01-21T17:02:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08705v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.08705v3">PDF</a> · <a href="https://github.com/miaomiao-cai2/RMBRec/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.
  In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users&#x27; auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RMBRec：面向目标行为的鲁棒多行为推荐</div>
<div class="mono" style="margin-top:8px">多行为推荐在实践中面临关键挑战：辅助行为（如点击、加购）常存在噪声、弱相关性或与目标行为（如购买）语义不对齐，导致偏好学习偏差与次优性能。现有方法虽尝试融合异构信号，但本质上缺乏确保行为不一致鲁棒性的理论机制。本研究提出面向目标行为的鲁棒多行为推荐框架RMBRec，其基于信息论的鲁棒性原理，将鲁棒性阐释为最大化预测信息同时最小化其在异构行为环境中方差的联合过程。在此视角下，表征鲁棒性模块通过最大化用户辅助行为与目标行为表征间的互信息增强局部语义一致性；优化鲁棒性模块则通过最小化跨行为预测风险方差（即不变风险最小化的高效近似）确保全局稳定性。这种局部-全局协作以理论自洽的方式桥接了表征纯化与优化不变性。在三个真实数据集上的大量实验表明，RMBRec不仅在准确性上超越现有最优方法，且在多种噪声扰动下保持显著稳定性。代码已开源：https://github.com/miaomiao-cai2/RMBRec/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-behavior recommendation systems often suffer from noise and semantic misalignment between auxiliary behaviors (e.g., clicks) and the target behavior (e.g., purchases), leading to biased learning. To address this, the authors propose RMBRec, a framework that enhances robustness by maximizing predictive information while minimizing its variance across behaviors through two modules: a Representation Robustness Module that increases mutual information between user representations from different behaviors, and an Optimization Robustness Module that minimizes predictive risk variance to approximate invariant risk minimization. Experiments on three real-world datasets show that RMBRec achieves superior accuracy and maintains stability under various noise conditions compared to state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">多行为推荐系统常因辅助行为（如点击）与目标行为（如购买）之间的噪声和语义错位而导致学习偏差。为解决此问题，提出的RMBRec框架基于信息论鲁棒性原则，通过表示鲁棒性模块最大化用户在不同行为上表示间的互信息，并通过优化鲁棒性模块最小化跨行为预测风险的方差，近似实现不变风险最小化。在三个真实数据集上的实验表明，与现有先进方法相比，RMBRec在准确性上表现更优，并在各种噪声干扰下保持了显著的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Finding Kissing Numbers with Game-theoretic Reinforcement Learning</div>
<div class="meta-line">Authors: Chengdong Ma, Théo Tao Zhaowei, Pengyu Li, Minghao Liu, Haojun Chen, Zihao Mao, Yuan Cheng, Yuan Qi, Yaodong Yang</div>
<div class="meta-line">First: 2025-11-17T14:02:00+00:00 · Latest: 2026-01-21T16:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13391v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13391v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert&#x27;s 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game that can be fully parallelized at large scale, and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions, discovers over 6000 new structures in 14 and other dimensions, and establishes new records for generalized kissing configurations under various angular constraints. These results demonstrate AI&#x27;s power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于博弈论强化学习的接吻数求解</div>
<div class="mono" style="margin-top:8px">自艾萨克·牛顿于1694年首次研究接吻数问题以来，确定中心球体周围非重叠球体的最大数量始终是基础性难题。该问题是希尔伯特第18个球堆积问题的局部对应，连接几何学、数论与信息论。尽管通过格与编码取得重要进展，但8维以上高维几何的不规则性与指数级增长的组合复杂度（超过围棋复杂度）限制了现有方法的可扩展性。本研究将该问题建模为可大规模并行化的双玩家矩阵补全博弈，训练博弈论强化学习系统PackingStar以高效探索高维空间。矩阵元素表示球心向量间的成对余弦值；一方填充元素，另一方修正次优项，协同最大化矩阵规模以对应接吻数。这种合作动力学显著提升样本质量，使极大空间可处理。PackingStar复现了已有构型，并在25至31维突破所有已知人类记录——其中25维构型几何对应Leech格并暗示潜在最优性。系统在13维实现自1971年以来首次超越有理结构的突破，在14维及其他维度发现6000余个新结构，并在多种角度约束下建立广义接吻构型新记录。这些成果证明人工智能探索超越人类直觉的高维空间的能力，为接吻数问题及更广泛几何问题开辟新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Kissing Number Problem, which seeks the maximum number of non-overlapping spheres that can touch a central sphere, is a long-standing geometric challenge with connections to sphere packing and high-dimensional complexity. To address the combinatorial explosion in dimensions beyond eight, this work models the problem as a two-player matrix completion game, where entries represent pairwise cosines of sphere center vectors; one player proposes entries while another corrects suboptimal ones, jointly maximizing the matrix size through a cooperative reinforcement learning system called PackingStar. This method efficiently explores high-dimensional spaces, reproducing known configurations and surpassing all human-known records from dimensions 25 to 31, achieving the first breakthrough beyond rational structures since 1971 in dimension 13, discovering over 6000 new structures in dimension 14 and others, and setting new records for generalized kissing configurations under various angular constraints.</div>
<div class="mono" style="margin-top:8px">自牛顿时代以来，亲吻数问题作为一个长期存在的几何挑战，旨在寻找一个中心球体周围可接触的最大非重叠球体数量，其中高维情况因指数级复杂度而难以计算。为解决此问题，作者将该问题建模为一个双玩家矩阵补全游戏：一名玩家填充表示球心向量对之间余弦的矩阵条目，另一名玩家修正次优条目，共同最大化矩阵大小以寻找亲吻数；这种合作的博弈论强化学习系统称为PackingStar，能够高效并行探索高维空间。实验结果表明，PackingStar不仅复现了已知构型，还超越了25至31维的所有人类已知记录，在25维中暗示了与Leech格点对应的可能最优性，在13维实现了自1971年以来的首次突破，在14维及其他维度发现了超过6000个新结构，并为各种角度约束下的广义亲吻构型设立了新记录，证明了人工智能探索超越人类直觉的高维空间的能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</div>
<div class="meta-line">Authors: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</div>
<div class="meta-line">First: 2026-01-21T16:41:58+00:00 · Latest: 2026-01-21T16:41:58+00:00</div>
<div class="meta-line">Comments: Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15165v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15165v1">PDF</a> · <a href="https://github.com/LeapLabTHU/JustGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://nzl-thu.github.io/the-flexibility-trap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活性陷阱：为何任意顺序限制扩散语言模型的推理潜力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）突破了传统LLM严格的从左到右约束，实现了按任意顺序生成词元。直观上，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上为数学和编程等通用任务解锁了更优的推理潜力。因此，许多研究采用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但没有扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用顺序灵活性规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一发现挑战了现有dLLMs强化学习方法的前提——这些方法往往为保持顺序灵活性而投入大量复杂度（如处理组合轨迹和难解似然）。我们证明，通过主动放弃任意顺序并采用标准组相对策略优化（GRPO），能更有效地激发推理能力。我们的方法JustGRPO极简却效果显著（如在GSM8K上达89.1%准确率），同时完全保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study challenges the assumption that arbitrary token generation order in diffusion language models inherently enhances reasoning capabilities, arguing that it instead leads to a premature collapse of the solution space as models exploit flexibility to avoid high-uncertainty tokens crucial for exploration. The authors propose JustGRPO, a method that intentionally forgoes arbitrary order generation and applies standard Group Relative Policy Optimization, demonstrating its effectiveness through experiments such as achieving 89.1% accuracy on GSM8K while retaining parallel decoding ability.</div>
<div class="mono" style="margin-top:8px">本文研究了扩散大语言模型（dLLMs）的推理能力，该模型允许以任意顺序生成标记，这种灵活性理论上优于固定的自回归模型。与直觉相反，作者发现这种任意顺序生成会导致模型绕过对探索至关重要的高不确定性标记，从而过早地坍缩解空间并阻碍推理。为解决此问题，他们提出了JustGRPO方法，该方法有意放弃任意顺序生成，转而应用标准的组相对策略优化。该方法虽简约但非常有效，在GSM8K上达到了89.1%的准确率，同时保留了并行解码能力。</div>
</details>
</div>
<div class="card">
<div class="title">Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</div>
<div class="meta-line">Authors: Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-05-29T04:09:19+00:00 · Latest: 2026-01-21T16:37:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23062v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.23062v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating pre-collected offline data can substantially improve the sample efficiency of reinforcement learning (RL), but its benefits can break down when the transition dynamics in the offline dataset differ from those encountered online. Existing approaches typically mitigate this issue by penalizing or filtering offline transitions in regions with large dynamics gap. However, their dynamics-gap estimators often rely on KL divergence or mutual information, which can be ill-defined when offline and online dynamics have mismatched support. To address this challenge, we propose CompFlow, a principled framework built on the theoretical connection between flow matching and optimal transport. Specifically, we model the online dynamics as a conditional flow built upon the output distribution of a pretrained offline flow, rather than learning it directly from a Gaussian prior. This composite structure provides two advantages: (1) improved generalization when learning online dynamics under limited interaction data, and (2) a well-defined and stable estimate of the dynamics gap via the Wasserstein distance between offline and online transitions. Building on this dynamics-gap estimator, we further develop an optimistic active data collection strategy that prioritizes exploration in high-gap regions, and show theoretically that it reduces the performance gap to the optimal policy. Empirically, CompFlow consistently outperforms strong baselines across a range of RL benchmarks with shifted-dynamics data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向动态偏移数据强化学习的复合流匹配方法</div>
<div class="mono" style="margin-top:8px">利用预收集的离线数据可显著提升强化学习的样本效率，但当离线数据集中的状态转移动态与在线环境存在差异时，其优势可能失效。现有方法通常通过对动态差异较大区域的离线转移施加惩罚或过滤来缓解此问题，但其动态差异估计器常依赖KL散度或互信息，在离线与在线动态分布支撑集不匹配时可能失效。为此，我们提出CompFlow——基于流匹配与最优传输理论联系构建的原理性框架。具体而言，我们将在线动态建模为基于预训练离线流输出分布的条件流，而非直接从高斯先验学习。这种复合结构具有双重优势：(1) 在有限交互数据下学习在线动态时提升泛化能力；(2) 通过离线与在线转移的Wasserstein距离获得定义明确且稳定的动态差异估计。基于此估计器，我们进一步开发了乐观主动数据收集策略，优先探索高差异区域，并从理论上证明其能缩小与最优策略的性能差距。在多个含动态偏移数据的强化学习基准测试中，CompFlow均持续超越强基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of leveraging offline data in reinforcement learning when the transition dynamics differ from the online environment, as existing methods using KL divergence or mutual information can fail when dynamics have mismatched support. The proposed CompFlow framework models online dynamics as a conditional flow built upon a pretrained offline flow&#x27;s output distribution, utilizing the connection between flow matching and optimal transport. This composite structure improves generalization with limited online data and provides a stable dynamics-gap estimator via Wasserstein distance. Experiments across RL benchmarks with shifted dynamics show CompFlow consistently outperforms strong baselines, and the theoretical analysis demonstrates its optimistic active data collection strategy reduces the performance gap to the optimal policy.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习中离线数据与在线环境动态转移不一致时样本效率下降的问题，提出了一种解决方案。所提出的CompFlow方法将在线动态建模为基于预训练离线流输出分布的条件流，而非直接从高斯先验学习，通过流匹配建立了与最优传输的理论联系。这种复合结构在有限在线交互下提高了泛化能力，并提供了基于Wasserstein距离的稳定动态差距估计，从而支持一种优先探索高差距区域的乐观主动数据收集策略。实验结果表明，在多种动态偏移的RL基准测试中，CompFlow consistently outperforms strong baselines across various RL benchmarks with shifted-dynamics data，理论分析也表明该方法能缩小与最优策略的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-01-21T16:36:19+00:00</div>
<div class="meta-line">Comments: 80 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动梯度下降发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的梯度流动力学来探讨此问题——该任务无思维链无法解决，但存在简单迭代解法。我们证明：尽管仅通过最终答案正确性进行训练，梯度流仍会驱动模型收敛至结构化、可解释的逐顶点迭代遍历算法。我们刻画了这种涌现所需的分布特性，指出“简单示例”（需要较少推理步骤的实例）的关键作用。当训练分布中此类简单实例具有足够权重时，模型可学习泛化的遍历策略并外推至更长推理链；若权重消失，基于梯度的学习将不可行。我们通过合成数据实验及真实数学推理任务的语言模型实验验证理论结果，证明理论发现适用于实际场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to understand how Transformers trained with outcome-based Reinforcement Learning (RL) can spontaneously develop systematic reasoning steps (Chain-of-Thought) from sparse rewards, a mechanism that remains unclear. The method involves analyzing gradient flow dynamics in single-layer Transformers on a synthetic graph traversal task that necessitates Chain-of-Thought, proving that gradient flow converges to an interpretable, iterative traversal algorithm despite training only on final-answer correctness. Key experimental findings show that emergence depends on training data distribution, specifically requiring sufficient &quot;simple examples&quot; with fewer reasoning steps to enable learning a generalizable strategy that extrapolates to longer chains, whereas absence of such examples makes learning infeasible; these theoretical insights are corroborated by experiments on synthetic data and real-world language models for mathematical reasoning.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究基于结果的强化学习训练的Transformer如何自发形成系统性推理链，其机制尚不明确。作者通过分析在必须依赖中间步骤的合成图遍历任务上的梯度流动态，证明仅基于最终答案正确性的训练会使模型收敛到一个可解释的迭代算法。关键实验结果表明，这种可泛化推理策略的出现，关键依赖于训练数据分布包含足够的‘简单示例’（即所需推理步骤较少的实例）；若缺乏此类示例，学习将失败，这一发现在合成数据和真实世界数学推理任务上均得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">The Good, the Bad and the Ugly: Meta-Analysis of Watermarks, Transferable Attacks and Adversarial Defenses</div>
<div class="meta-line">Authors: Grzegorz Głuch, Berkant Turan, Sai Ganesh Nagarajan, Sebastian Pokutta</div>
<div class="meta-line">Venue: ICML 2024</div>
<div class="meta-line">First: 2024-10-11T14:44:05+00:00 · Latest: 2026-01-21T16:22:57+00:00</div>
<div class="meta-line">Comments: 47 pages, 3 figures, 4 tables, preliminary version published in ICML 2024 (Workshop on Theoretical Foundations of Foundation Models) and , see https://openreview.net/pdf?id=WMaFRiggwV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.08864v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.08864v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We formalize and analyze the trade-off between backdoor-based watermarks and adversarial defenses, framing it as an interactive protocol between a verifier and a prover. While previous works have primarily focused on this trade-off, our analysis extends it by identifying transferable attacks as a third, counterintuitive, but necessary option. Our main result shows that for all learning tasks, at least one of the three exists: a watermark, an adversarial defense, or a transferable attack. By transferable attack, we refer to an efficient algorithm that generates queries indistinguishable from the data distribution and capable of fooling all efficient defenders. Using cryptographic techniques, specifically fully homomorphic encryption, we construct a transferable attack and prove its necessity in this trade-off. Finally, we show that tasks of bounded VC-dimension allow adversarial defenses against all attackers, while a subclass allows watermarks secure against fast adversaries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>水印、可迁移攻击与对抗防御的元分析：优劣与困境</div>
<div class="mono" style="margin-top:8px">本研究形式化分析了基于后门的水印与对抗防御之间的权衡关系，将其构建为验证者与证明者间的交互协议。在延续前人研究框架的基础上，我们通过引入可迁移攻击作为第三种反直觉但必要的选项，拓展了该权衡的分析维度。核心结论表明：对于所有学习任务，水印、对抗防御或可迁移攻击三者至少存在其一。可迁移攻击指能生成与数据分布不可区分、且可欺骗所有高效防御者的查询生成算法。借助全同态加密等密码学技术，我们构建了可迁移攻击并证明其在此权衡中的必要性。最后，我们证明有限VC维任务允许构建抵御所有攻击者的对抗防御，而其中特定子类允许构建抵御快速对抗者的安全水印。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the inherent trade-offs among backdoor-based watermarks, adversarial defenses, and transferable attacks in machine learning, framing their interaction as a protocol between a verifier and a prover. The core method employs cryptographic techniques, specifically fully homomorphic encryption, to construct a provably necessary transferable attack—an efficient algorithm that generates distribution-conforming queries capable of fooling all efficient defenders. Key findings establish that for any learning task, at least one of these three elements must exist, and demonstrate that tasks with bounded VC-dimension permit adversarial defenses against all attackers, while a subclass also supports watermarks secure against computationally bounded adversaries.</div>
<div class="mono" style="margin-top:8px">本研究探讨了机器学习中基于后门的水印、对抗性防御和可迁移攻击之间的内在权衡，并将其交互形式化为验证者与证明者之间的协议。核心方法采用密码学技术，特别是全同态加密，来构建一个被证明必要的可迁移攻击——一种能生成符合数据分布查询的高效算法，可欺骗所有高效防御者。关键结果表明，对于任何学习任务，水印、对抗防御或可迁移攻击这三者中至少有一个必然存在；进一步研究刻画了有限VC维度的任务允许针对所有攻击者的对抗防御，而其一个子类则允许存在能抵御快速对手的安全水印。</div>
</details>
</div>
<div class="card">
<div class="title">Does Object Binding Naturally Emerge in Large Pretrained Vision Transformers?</div>
<div class="meta-line">Authors: Yihao Li, Saeed Salehi, Lyle Ungar, Konrad P. Kording</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-10-28T17:57:05+00:00 · Latest: 2026-01-21T16:16:08+00:00</div>
<div class="meta-line">Comments: Accepted as a Spotlight at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24709v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24709v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object binding, the brain&#x27;s ability to bind the many features that collectively represent an object into a coherent whole, is central to human cognition. It groups low-level perceptual features into high-level object representations, stores those objects efficiently and compositionally in memory, and supports human reasoning about individual object instances. While prior work often imposes object-centric attention (e.g., Slot Attention) explicitly to probe these benefits, it remains unclear whether this ability naturally emerges in pre-trained Vision Transformers (ViTs). Intuitively, they could: recognizing which patches belong to the same object should be useful for downstream prediction and thus guide attention. Motivated by the quadratic nature of self-attention, we hypothesize that ViTs represent whether two patches belong to the same object, a property we term IsSameObject. We decode IsSameObject from patch embeddings across ViT layers using a quadratic similarity probe, which reaches over 90% accuracy. Crucially, this object-binding capability emerges reliably in DINO, CLIP, and ImageNet-supervised ViTs, but is markedly weaker in MAE, suggesting that binding is not a trivial architectural artifact, but an ability acquired through specific pretraining objectives. We further discover that IsSameObject is encoded in a low-dimensional subspace on top of object features, and that this signal actively guides attention. Ablating IsSameObject from model activations degrades downstream performance and works against the learning objective, implying that emergent object binding naturally serves the pretraining objective. Our findings challenge the view that ViTs lack object binding and highlight how symbolic knowledge of &quot;which parts belong together&quot; emerges naturally in a connectionist system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模预训练视觉Transformer中是否自然涌现物体绑定能力？</div>
<div class="mono" style="margin-top:8px">物体绑定是大脑将构成物体的多种特征整合为连贯整体的核心认知能力，它将低层感知特征组合为高层物体表征，并以高效组合方式存储于记忆中，支撑人类对个体物体的推理。先前研究常通过显式施加物体中心注意力机制（如槽注意力）来验证其益处，但预训练视觉Transformer（ViT）是否自然具备此能力尚不明确。直觉上，识别哪些图像块属于同一物体对下游预测有益，可能引导注意力机制。基于自注意力的二次方特性，我们提出假设：ViT能表征两个图像块是否属于同一物体（称为IsSameObject属性）。通过二次相似性探针从ViT各层嵌入中解码IsSameObject，准确率超90%。关键发现是：DINO、CLIP和ImageNet监督训练的ViT均稳定涌现此能力，而MAE模型表现显著较弱，表明绑定能力并非架构固有特性，而是通过特定预训练目标习得。进一步研究发现，IsSameObject编码于物体特征顶部的低维子空间，并主动引导注意力机制。从模型激活中剔除IsSameObject会降低下游性能并违背学习目标，说明涌现的物体绑定能力天然服务于预训练目标。本研究挑战了“ViT缺乏物体绑定能力”的观点，揭示了连接主义系统中“部件归属关系”的符号知识如何自然涌现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether object binding, the cognitive process of grouping features into coherent object representations, naturally emerges in pre-trained Vision Transformers (ViTs) without explicit architectural biases. The authors hypothesize that ViTs encode whether two image patches belong to the same object, termed IsSameObject, and decode this property using a quadratic similarity probe applied to patch embeddings across model layers. Experimental results show the probe achieves over 90% accuracy, with this binding capability emerging reliably in ViTs trained with DINO, CLIP, and ImageNet supervision but being markedly weaker in MAE models, indicating it is learned through specific pretraining objectives rather than being an architectural artifact. Further analysis reveals IsSameObject is encoded in a low-dimensional subspace, actively guides attention, and its ablation degrades downstream performance, suggesting emergent object binding serves the pretraining objective.</div>
<div class="mono" style="margin-top:8px">本研究探讨了物体绑定——即将特征组合成连贯物体表征的认知过程——是否会在未经显式架构偏置的预训练视觉Transformer（ViT）中自然涌现。作者假设ViT编码了两个图像块是否属于同一物体（称为IsSameObject），并通过在模型各层的块嵌入上应用二次相似性探针来解码这一属性。实验结果表明，该探针准确率超过90%，且这种绑定能力在DINO、CLIP和ImageNet监督训练的ViT中可靠地涌现，但在MAE模型中明显较弱，表明它是通过特定预训练目标习得的，而非架构伪影。进一步分析揭示IsSameObject编码于一个低维子空间，主动引导注意力，且其消融会降低下游性能，这意味着涌现的物体绑定服务于预训练目标。</div>
</details>
</div>
<div class="card">
<div class="title">CLEANER: Self-Purified Trajectories Boost Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Tianshi Xu, Yuteng Chen, Meng Li</div>
<div class="meta-line">First: 2026-01-21T16:14:30+00:00 · Latest: 2026-01-21T16:14:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15141v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic Reinforcement Learning (RL) has empowered Large Language Models (LLMs) to utilize tools like Python interpreters for complex problem-solving. However, for parameter-constrained models (e.g., 4B--7B), the exploration phase is often plagued by frequent execution failures, creating noisy trajectories that hinder policy optimization. Under standard outcome-based reward settings, this noise leads to a critical credit assignment issue, where erroneous actions are inadvertently reinforced alongside successful outcomes. Existing mitigations face a dilemma: dense rewards often trigger reward hacking, while supersampling incurs prohibitive computational costs. To address these challenges, we propose CLEANER. Distinct from external filtering methods, CLEANER exploits the model&#x27;s intrinsic self-correction capabilities to eliminate error-contaminated context directly during data collection. At its core, the Similarity-Aware Adaptive Rollback (SAAR) mechanism autonomously constructs clean, purified trajectories by retrospectively replacing failures with successful self-corrections. Based on semantic similarity, SAAR adaptively regulates replacement granularity from shallow execution repairs to deep reasoning substitutions. By training on these self-purified paths, the model internalizes correct reasoning patterns rather than error-recovery loops. Empirical results on AIME24/25, GPQA, and LiveCodeBench show average accuracy gains of 6%, 3%, and 5% over baselines. Notably, CLEANER matches state-of-the-art performance using only one-third of the training steps, highlighting trajectory purification as a scalable solution for efficient agentic RL. Our models and code are available at GitHub</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLEANER：自净化轨迹提升智能体强化学习</div>
<div class="mono" style="margin-top:8px">智能体强化学习（RL）使大语言模型（LLM）能够利用Python解释器等工具解决复杂问题。然而，对于参数受限模型（如4B-7B规模），探索阶段常因频繁执行失败而产生噪声轨迹，阻碍策略优化。在标准基于结果的奖励设置下，这种噪声会导致严重的信用分配问题——错误动作会与成功结果一同被无意强化。现有缓解方法面临两难：密集奖励易引发奖励破解，而过采样则带来高昂计算成本。为此，我们提出CLEANER框架。区别于外部过滤方法，CLEANER利用模型内在自校正能力，直接在数据收集阶段消除错误污染上下文。其核心相似性感知自适应回滚（SAAR）机制通过追溯性将失败替换为成功自校正，自主构建洁净的净化轨迹。SAAR基于语义相似度自适应调节替换粒度，从浅层执行修复到深层推理替换。通过在这些自净化路径上训练，模型内化了正确推理模式而非错误恢复循环。在AIME24/25、GPQA和LiveCodeBench的实证结果显示，相比基线平均准确率分别提升6%、3%和5%。值得注意的是，CLEANER仅用三分之一训练步数即达到最优性能，表明轨迹净化是可扩展的高效智能体RL解决方案。模型与代码已在GitHub开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of noisy trajectories from frequent execution failures during exploration in agentic reinforcement learning for parameter-constrained LLMs, which causes credit assignment issues and hinders policy optimization under outcome-based rewards. The proposed method, CLEANER, introduces a Similarity-Aware Adaptive Rollback (SAAR) mechanism that leverages the model&#x27;s intrinsic self-correction capability to autonomously construct purified trajectories by retrospectively replacing failed actions with successful self-corrections, adapting replacement granularity based on semantic similarity. Experimental results on benchmarks including AIME24/25, GPQA, and LiveCodeBench demonstrate average accuracy improvements of 6%, 3%, and 5% over baselines, with CLEANER achieving state-of-the-art performance using only one-third of the training steps, highlighting its efficiency and scalability.</div>
<div class="mono" style="margin-top:8px">为解决参数受限的智能体强化学习中探索阶段因频繁执行失败产生的噪声轨迹所导致的信用分配与策略优化难题，本研究提出了CLEANER方法。该方法通过相似性感知自适应回滚机制，利用模型内在的自我纠正能力，在数据收集过程中自主构建纯净轨迹，即根据语义相似性自适应地以成功自我纠正替换失败动作。在AIME24/25、GPQA和LiveCodeBench基准测试上的实验结果表明，该方法相比基线平均准确率分别提升了6%、3%和5%，且仅需三分之一训练步数即可达到最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">Graph Recognition via Subgraph Prediction</div>
<div class="meta-line">Authors: André Eberhard, Gerhard Neumann, Pascal Friederich</div>
<div class="meta-line">First: 2026-01-21T16:07:17+00:00 · Latest: 2026-01-21T16:07:17+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite tremendous improvements in tasks such as image classification, object detection, and segmentation, the recognition of visual relationships, commonly modeled as the extraction of a graph from an image, remains a challenging task. We believe that this mainly stems from the fact that there is no canonical way to approach the visual graph recognition task. Most existing solutions are specific to a problem and cannot be transferred between different contexts out-of-the box, even though the conceptual problem remains the same. With broad applicability and simplicity in mind, in this paper we develop a method, \textbf{Gra}ph Recognition via \textbf{S}ubgraph \textbf{P}rediction (\textbf{GraSP}), for recognizing graphs in images. We show across several synthetic benchmarks and one real-world application that our method works with a set of diverse types of graphs and their drawings, and can be transferred between tasks without task-specific modifications, paving the way to a more unified framework for visual graph recognition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于子图预测的图识别</div>
<div class="mono" style="margin-top:8px">尽管在图像分类、目标检测和分割等任务上取得了巨大进展，但视觉关系的识别——通常建模为从图像中提取图结构——仍然是一项具有挑战性的任务。我们认为这主要源于视觉图识别任务缺乏规范化的处理方法。现有解决方案大多针对特定问题设计，即使概念问题相同，也无法在不同情境下直接迁移应用。基于广泛适用性和简洁性的考量，本文提出了一种名为“基于子图预测的图识别”的方法，用于识别图像中的图结构。通过在多个合成基准测试和一个实际应用中的验证，我们证明该方法能处理多种类型的图及其绘制形式，且无需针对特定任务修改即可跨任务迁移，为构建更统一的视觉图识别框架奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of a canonical and transferable approach for visual graph recognition, a challenging task distinct from other vision problems. The proposed method, Graph Recognition via Subgraph Prediction (GraSP), aims to provide a unified framework by predicting subgraphs. Experimental results across several synthetic benchmarks and a real-world application demonstrate that GraSP works with diverse graph types and drawings and can be transferred between tasks without task-specific modifications.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于视觉图识别任务缺乏一种规范且可迁移的方法，现有解决方案通常针对特定问题，难以在不同场景间直接迁移。为此，本文提出了通过子图预测进行图识别的方法（GraSP），旨在通过预测子图来识别图像中的多种图结构。在多个合成基准和一个实际应用中的实验结果表明，GraSP能够处理不同类型的图及其绘制方式，并且无需针对任务进行修改即可在不同任务间迁移，为视觉图识别提供了更统一的框架。</div>
</details>
</div>
<div class="card">
<div class="title">DeepFedNAS: A Unified Framework for Principled, Hardware-Aware, and Predictor-Free Federated Neural Architecture Search</div>
<div class="meta-line">Authors: Bostan Khan, Masoud Daneshtalab</div>
<div class="meta-line">First: 2026-01-21T16:03:25+00:00 · Latest: 2026-01-21T16:03:25+00:00</div>
<div class="meta-line">Comments: This paper significantly extends the preliminary work accepted at ESANN 2026. Source Code: https://github.com/bostankhan6/DeepFedNAS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15127v1">PDF</a> · <a href="https://github.com/bostankhan6/DeepFedNAS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Neural Architecture Search (FedNAS) aims to automate model design for privacy-preserving Federated Learning (FL) but currently faces two critical bottlenecks: unguided supernet training that yields suboptimal models, and costly multi-hour pipelines for post-training subnet discovery. We introduce DeepFedNAS, a novel, two-phase framework underpinned by a principled, multi-objective fitness function that synthesizes mathematical network design with architectural heuristics. Enabled by a re-engineered supernet, DeepFedNAS introduces Federated Pareto Optimal Supernet Training, which leverages a pre-computed Pareto-optimal cache of high-fitness architectures as an intelligent curriculum to optimize shared supernet weights. Subsequently, its Predictor-Free Search Method eliminates the need for costly accuracy surrogates by utilizing this fitness function as a direct, zero-cost proxy for accuracy, enabling on-demand subnet discovery in mere seconds. DeepFedNAS achieves state-of-the-art accuracy (e.g., up to 1.21% absolute improvement on CIFAR-100), superior parameter and communication efficiency, and a substantial ~61x speedup in total post-training search pipeline time. By reducing the pipeline from over 20 hours to approximately 20 minutes (including initial cache generation) and enabling 20-second individual subnet searches, DeepFedNAS makes hardware-aware FL deployments instantaneous and practical. The complete source code and experimental scripts are available at: https://github.com/bostankhan6/DeepFedNAS</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepFedNAS：一种面向原则性、硬件感知且无需预测器的联邦神经架构搜索统一框架</div>
<div class="mono" style="margin-top:8px">联邦神经架构搜索旨在为隐私保护的联邦学习自动化模型设计，但当前面临两大瓶颈：无引导的超网训练导致次优模型，以及训练后子网发现流程耗时数小时。本文提出DeepFedNAS——一种基于原则性多目标适应度函数的新型两阶段框架，该函数融合了数学化网络设计与架构启发式规则。通过重构的超网，DeepFedNAS引入联邦帕累托最优超网训练，利用预计算的高适应度架构帕累托最优缓存作为智能课程来优化共享超网权重。随后，其无预测器搜索方法通过将该适应度函数作为零成本的精度代理，无需昂贵精度替代模型，实现数秒内按需子网发现。DeepFedNAS在CIFAR-100上取得最高1.21%的绝对精度提升，具备更优的参数与通信效率，训练后搜索流程整体加速约61倍。通过将流程从超20小时缩减至约20分钟（含初始缓存生成），并实现20秒级单次子网搜索，本框架使硬件感知的联邦学习部署具备即时性与实用性。完整源代码与实验脚本已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Federated Neural Architecture Search (FedNAS) faces challenges of unguided supernet training leading to suboptimal models and inefficient post-training search pipelines. This work introduces DeepFedNAS, a two-phase framework that first employs Federated Pareto Optimal Supernet Training, using a pre-computed cache of high-fitness architectures as a curriculum to optimize supernet weights, and then utilizes a predictor-free search method that leverages a multi-objective fitness function as a direct accuracy proxy for rapid subnet discovery. Experiments demonstrate state-of-the-art accuracy, improved parameter and communication efficiency, and a ~61x speedup in the total search pipeline, reducing it from over 20 hours to about 20 minutes while enabling individual subnet searches in seconds.</div>
<div class="mono" style="margin-top:8px">联邦神经架构搜索（FedNAS）面临超网络训练效率低下和训练后子网络发现缓慢的挑战。为此，DeepFedNAS提出了一个两阶段框架：首先，它利用一个原则性的适应度函数和重新设计的超网络，通过预计算的高适应度架构缓存指导，实现联邦帕累托最优超网络训练；其次，它采用无预测器搜索方法，将适应度函数作为零成本精度代理进行快速子网络发现。实验结果表明，该方法实现了最先进的精度提升（例如在CIFAR-100上最高提升1.21%），具有更好的参数和通信效率，并将总搜索流程加速约61倍，从超过20小时缩短至约20分钟，同时支持秒级的单个子网络搜索。</div>
</details>
</div>
<div class="card">
<div class="title">Competitive Audio-Language Models with Data-Efficient Single-Stage Training on Public Data</div>
<div class="meta-line">Authors: Gokul Karthik Kumar, Rishabh Saraf, Ludovick Lepauloux, Abdul Muneer, Billel Mokeddem, Hakim Hacid</div>
<div class="meta-line">First: 2025-09-09T09:01:01+00:00 · Latest: 2026-01-21T16:03:15+00:00</div>
<div class="meta-line">Comments: Accepted at ASRU 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07526v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.07526v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have transformed NLP, yet their integration with audio remains underexplored despite audio&#x27;s centrality to human communication. We introduce Falcon3-Audio, a family of Audio-Language Models (ALMs) built on instruction-tuned LLMs and Whisper encoders. Using a remarkably small amount of public audio data, less than 30K hours (5K unique), Falcon3-Audio-7B matches the best reported performance among open-weight models on the MMAU benchmark, with a score of 64.14, matching R1-AQA, while distinguishing itself through superior data and parameter efficiency, single-stage training, and transparency. Notably, our smallest 1B model remains competitive with larger open models ranging from 2B to 13B parameters. Through extensive ablations, we find that common complexities such as curriculum learning, multiple audio encoders, and intricate cross-attention connectors are not required for strong performance, even compared to models trained on over 500K hours of data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于公共数据高效单阶段训练的竞争性音频语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）已变革自然语言处理领域，但音频作为人类沟通的核心载体，其与LLMs的融合仍待深入探索。本文推出Falcon3-Audio系列音频语言模型（ALMs），其基于指令调优的LLMs与Whisper编码器构建。仅使用极少量公共音频数据（不足3万小时，含5千条独立数据），Falcon3-Audio-7B即在MMAU基准测试中获得64.14分，与R1-AQA持平，达到开源权重模型的最佳报告性能，并凭借卓越的数据与参数效率、单阶段训练流程及完全透明性脱颖而出。值得注意的是，我们最小的10亿参数模型仍可与20亿至130亿参数规模的开源模型竞争。通过大量消融实验发现，即使与使用超50万小时数据训练的模型相比，课程学习、多重音频编码器、复杂交叉注意力连接器等常见复杂设计对实现强劲性能并非必需。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the underexplored integration of large language models with audio, which is central to human communication, by developing efficient audio-language models. The method introduces Falcon3-Audio, a family of models built on instruction-tuned LLMs and Whisper encoders, using a single-stage training approach with a remarkably small amount of public audio data (less than 30K hours). Key experimental results show that the 7B model matches the best open-weight performance on the MMAU benchmark with a score of 64.14, while extensive ablations reveal that common complexities like curriculum learning are unnecessary for strong performance, even compared to models trained on over 500K hours of data.</div>
<div class="mono" style="margin-top:8px">本研究针对音频与大型语言模型整合不足的问题，尽管音频在人类交流中至关重要。方法上引入了Falcon3-Audio系列音频-语言模型，通过将指令调优的大型语言模型与Whisper音频编码器结合，并采用数据高效的单一阶段训练流程，仅使用不到3万小时的公开音频数据。关键实验结果表明，70亿参数模型在MMAU基准测试中以64.14分匹配了最佳公开权重性能，而广泛的消融实验证明，即使与使用超过50万小时数据训练的模型相比，强性能也无需课程学习或多编码器等常见复杂设计。</div>
</details>
</div>
<div class="card">
<div class="title">Overcoming In-Memory Bottlenecks in Graph Foundation Models via Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Haonan Yuan, Qingyun Sun, Jiacheng Tao, Xingcheng Fu, Jianxin Li</div>
<div class="meta-line">Venue: the Web Conference 2026</div>
<div class="meta-line">First: 2026-01-21T16:02:43+00:00 · Latest: 2026-01-21T16:02:43+00:00</div>
<div class="meta-line">Comments: Accepted by the Web Conference 2026 (Research Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15124v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph Foundation Models (GFMs) have emerged as a frontier in graph learning, which are expected to deliver transferable representations across diverse tasks. However, GFMs remain constrained by in-memory bottlenecks: they attempt to encode knowledge into model parameters, which limits semantic capacity, introduces heavy lossy compression with conflicts, and entangles graph representation with the knowledge in ways that hinder efficient adaptation, undermining scalability and interpretability. In this work,we propose RAG-GFM, a Retrieval-Augmented Generation aided Graph Foundation Model that offloads knowledge from parameters and complements parameterized learning. To externalize graph knowledge, we build a dual-modal unified retrieval module, where a semantic store from prefix-structured text and a structural store from centrality-based motif. To preserve heterogeneous information, we design a dual-view alignment objective that contrasts both modalities to capture both content and relational patterns. To enable efficient downstream adaptation, we perform in-context augmentation to enrich supporting instances with retrieved texts and motifs as contextual evidence. Extensive experiments on five benchmark graph datasets demonstrate that RAG-GFM consistently outperforms 13 state-of-the-art baselines in both cross-domain node and graph classification, achieving superior effectiveness and efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于检索增强生成克服图基础模型的内存瓶颈</div>
<div class="mono" style="margin-top:8px">图基础模型已成为图学习的前沿领域，有望为多样化任务提供可迁移的表征。然而，此类模型仍受限于内存瓶颈：其试图将知识编码至模型参数中，这限制了语义容量，引入了严重的带冲突有损压缩，并以阻碍高效适配的方式将图表征与知识纠缠，从而削弱了可扩展性与可解释性。本研究提出RAG-GFM——一种检索增强生成辅助的图基础模型，通过将知识从参数中卸载来补充参数化学习。为外化图知识，我们构建了双模态统一检索模块，包含基于前缀结构文本的语义存储库和基于中心性模体的结构存储库。为保留异构信息，设计了双视图对齐目标，通过对比两种模态以捕捉内容与关系模式。为实现高效下游适配，采用上下文增强方法，以检索到的文本和模体作为语境证据来丰富支持实例。在五个基准图数据集上的大量实验表明，RAG-GFM在跨领域节点分类和图分类任务中均持续优于13个前沿基线模型，实现了卓越的效能与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the in-memory bottlenecks of Graph Foundation Models (GFMs), which encode knowledge into parameters, limiting semantic capacity, causing lossy compression, and hindering efficient adaptation. The proposed method, RAG-GFM, externalizes knowledge via a dual-modal retrieval module with semantic and structural stores, uses a dual-view alignment objective to preserve heterogeneous information, and employs in-context augmentation for downstream tasks. Experiments on five benchmark datasets show RAG-GFM outperforms 13 state-of-the-art baselines in cross-domain node and graph classification, achieving superior effectiveness and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究针对图基础模型（GFMs）因将知识编码到参数中而导致的内存瓶颈问题，如语义容量受限、有损压缩和适应困难。提出的RAG-GFM通过双模态检索模块（包含语义和结构存储）卸载知识，利用双视图对齐目标保留异构信息，并通过基于检索证据的上下文增强实现高效下游适应。在五个基准图数据集上的实验表明，RAG-GFM在跨领域节点和图分类任务中持续优于13个先进基线，实现了卓越的有效性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">PPGFlowECG: Latent Rectified Flow with Cross-Modal Encoding for PPG-Guided ECG Generation and Cardiovascular Disease Detection</div>
<div class="meta-line">Authors: Xiaocheng Fang, Jiarui Jin, Haoyu Wang, Che Liu, Jieyi Cai, Yujie Xiao, Guangkun Nie, Bo Liu, Shun Huang, Hongyan Li, Shenda Hong</div>
<div class="meta-line">First: 2025-09-24T05:54:33+00:00 · Latest: 2026-01-21T15:58:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19774v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19774v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Electrocardiography (ECG) is the clinical gold standard for cardiovascular disease (CVD) assessment, yet continuous monitoring is constrained by the need for dedicated hardware and trained personnel. Photoplethysmography (PPG) is ubiquitous in wearable devices and readily scalable, but it lacks electrophysiological specificity, limiting diagnostic reliability. While generative methods aim to translate PPG into clinically useful ECG signals, existing approaches are limited by the misalignment of physiological semantics in generative models and the complexity of modeling in high-dimensional signals. To address these limitations, we propose PPGFlowECG, a two-stage framework that aligns PPG and ECG in a shared latent space using the CardioAlign Encoder and then synthesizes ECGs with latent rectified flow. We further provide a formal analysis of this coupling, showing that the CardioAlign Encoder is necessary to guarantee stable and semantically consistent ECG synthesis under our formulation. Extensive experiments on four datasets demonstrate improved synthesis fidelity and downstream diagnostic utility. These results indicate that PPGFlowECG supports scalable, wearable-first CVD screening when standard ECG acquisition is unavailable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PPGFlowECG：基于跨模态编码的潜在修正流用于PPG引导的ECG生成与心血管疾病检测</div>
<div class="mono" style="margin-top:8px">心电图（ECG）是心血管疾病（CVD）评估的临床金标准，但连续监测受限于专用硬件和专业人员的需求。光电容积描记法（PPG）在可穿戴设备中普遍存在且易于扩展，但缺乏电生理特异性，限制了诊断可靠性。尽管生成方法旨在将PPG转换为临床可用的ECG信号，但现有方法受限于生成模型中生理语义的错位以及高维信号建模的复杂性。为解决这些局限，我们提出PPGFlowECG，这是一个两阶段框架：首先使用CardioAlign编码器将PPG和ECG对齐到共享潜在空间，然后通过潜在修正流合成ECG。我们进一步对该耦合进行了形式化分析，表明CardioAlign编码器对于保证我们框架下稳定且语义一致的ECG合成是必要的。在四个数据集上的大量实验证明了合成保真度和下游诊断效用的提升。这些结果表明，当标准ECG采集不可行时，PPGFlowECG能够支持可扩展、以可穿戴设备为先的CVD筛查。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for continuous cardiovascular disease (CVD) monitoring, which is limited by the hardware and expertise required for clinical-grade electrocardiography (ECG), while ubiquitous photoplethysmography (PPG) from wearables lacks diagnostic specificity. The method introduces PPGFlowECG, a two-stage framework that first aligns PPG and ECG signals in a shared latent space using a CardioAlign Encoder and then synthesizes ECG via latent rectified flow, with formal analysis showing this alignment is necessary for stable, semantically consistent generation. Experimental results on four datasets demonstrate that the approach improves both the fidelity of the synthesized ECG signals and their utility in downstream CVD detection tasks, indicating potential for scalable, wearable-based screening when standard ECG is unavailable.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过从普遍存在的光电容积描记（PPG）信号生成具有临床价值的心电图（ECG），以实现持续的心血管疾病（CVD）监测，以解决现有生成方法存在的生理语义错位和高维建模复杂性的局限。提出的PPGFlowECG框架采用两阶段方法：首先使用CardioAlign编码器将PPG和ECG信号在共享潜在空间中对齐，然后通过潜在整流流模型合成ECG波形；形式化分析表明该编码器对于稳定且语义一致的生成是必要的。在四个数据集上的实验表明，该方法提高了合成ECG信号的保真度及其在下游CVD检测任务中的实用性，显示了其在标准ECG采集不可行时用于可扩展、可穿戴筛查的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Cluster-Based Generalized Additive Models Informed by Random Fourier Features</div>
<div class="meta-line">Authors: Xin Huang, Jia Li, Jun Yu</div>
<div class="meta-line">First: 2025-12-22T13:15:52+00:00 · Latest: 2026-01-21T15:56:08+00:00</div>
<div class="meta-line">Comments: 33 pages, 13 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19373v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19373v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the development of learning systems, there is an ongoing need to reconcile the strong predictive performance offered by opaque black-box models with the level of transparency required for critical applications. This work introduces a methodological framework that combines spectral representation learning with transparent statistical modeling to construct a mixture of generalized additive models (GAMs). The approach utilizes random Fourier feature embeddings to uncover locally adaptive structures within the data. High-dimensional random feature representations are compressed via principal component analysis to derive a latent space that informs a Gaussian mixture model, which performs soft clustering to partition the input space into distinct regimes. Within each cluster, a local GAM captures nonlinear univariate effects through interpretable spline-based smoothers. Numerical experiments across diverse regression benchmarks demonstrate that the proposed method consistently improves upon classical global interpretable models by effectively modeling data heterogeneity. Furthermore, the mixture-of-GAMs framework achieves performance comparable to explainable boosting machine, random forest, and multilayer perceptron on certain tasks. Overall, this construction provides a principled approach for integrating representation learning with transparent statistical modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于随机傅里叶特征启发的聚类广义可加模型</div>
<div class="mono" style="margin-top:8px">在学习系统开发中，始终存在调和黑箱模型强大预测能力与关键应用所需透明度之间的矛盾。本研究提出一种方法框架，将谱表示学习与透明统计建模相结合，构建广义可加模型（GAM）的混合体。该方法利用随机傅里叶特征嵌入揭示数据中的局部自适应结构，通过主成分分析压缩高维随机特征表示，构建用于高斯混合模型的隐空间，通过软聚类将输入空间划分为不同区域。每个聚类内采用局部GAM，通过可解释的样条平滑器捕捉非线性单变量效应。多组回归基准的数值实验表明，该方法能有效建模数据异质性，持续超越经典全局可解释模型。此外，在某些任务中，混合GAM框架的性能与可解释提升机、随机森林及多层感知器相当。整体而言，该构建为表示学习与透明统计建模的融合提供了理论化路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To balance the high predictive power of black-box models with the interpretability needed for critical applications, this work proposes a framework that constructs a mixture of generalized additive models (GAMs) informed by spectral representation learning. The method uses random Fourier feature embeddings to capture data structures, compresses these features via principal component analysis to create a latent space, and then applies a Gaussian mixture model for soft clustering to partition the input space; within each cluster, a local GAM with spline-based smoothers models nonlinear univariate effects. Experiments on various regression benchmarks show the approach consistently outperforms classical global interpretable models by effectively handling data heterogeneity and achieves performance comparable to models like explainable boosting machine, random forest, and multilayer perceptron on some tasks.</div>
<div class="mono" style="margin-top:8px">为了在关键应用中平衡黑盒模型的预测能力与可解释性需求，本研究提出一个框架，将谱表示学习与透明统计建模相结合，以构建广义可加模型（GAMs）的混合体。该方法利用随机傅里叶特征嵌入来捕捉局部数据结构，通过主成分分析压缩这些高维表示以形成潜在空间，并使用高斯混合模型进行软聚类来划分输入空间；在每个聚类内，一个具有可解释样条平滑器的局部GAM建模非线性单变量效应。在多样回归基准上的实验表明，该方法通过有效捕捉数据异质性，持续优于经典的全局可解释模型，并在某些任务上实现了与可解释提升机、随机森林和多层感知器等模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">WavLink: Compact Audio--Text Embeddings with a Global Whisper Token</div>
<div class="meta-line">Authors: Gokul Karthik Kumar, Ludovick Lepauloux, Hakim Hacid</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T15:55:58+00:00 · Latest: 2026-01-21T15:55:58+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15118v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whisper has become the de-facto encoder for extracting general-purpose audio features in large audio-language models, where a 30-second clip is typically represented by 1500 frame features projected into an LLM. In contrast, audio-text embedding models like CLAP-based models have largely relied on alternative audio encoders (e.g., HTS-AT, PaSST), and have not leveraged Whisper effectively. We present WavLink, a compact audio-text embedding model that augments Whisper encoder with a learnable global token, trained jointly with a text encoder. Through a systematic study of design choices, including pretrained text encoders, loss functions, training modes, and data mixtures, we identify configurations that yield state-of-the-art retrieval performance. Our two-stage training recipe across three model sizes, combined with Matryoshka-style supervision, improves scalability, enabling 8x smaller embeddings with minimal performance drop. WavLink also demonstrates competitive performance on AIR-Bench with MCQs and zero-shot classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WavLink：基于全局Whisper令牌的紧凑音频-文本嵌入模型</div>
<div class="mono" style="margin-top:8px">Whisper已成为大型音频-语言模型中提取通用音频特征的事实编码器，通常将30秒音频片段表示为1500帧特征并投影至大语言模型。相比之下，基于CLAP的音频-文本嵌入模型主要依赖其他音频编码器（如HTS-AT、PaSST），未能有效利用Whisper。本文提出WavLink——一种紧凑的音频-文本嵌入模型，通过为Whisper编码器添加可学习的全局令牌，并与文本编码器联合训练。通过对预训练文本编码器、损失函数、训练模式及数据混合等设计选择的系统研究，我们确定了实现最先进检索性能的配置方案。跨三种模型规模的两阶段训练策略结合套娃式监督，提升了可扩展性，能以8倍压缩的嵌入维度实现性能微降。WavLink在AIR-Bench的多选题和零样本分类任务中也展现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the underutilization of Whisper as an audio encoder in audio-text embedding models, which typically rely on other encoders despite Whisper&#x27;s prominence in large audio-language models. The method introduces WavLink, a compact audio-text embedding model that enhances the Whisper encoder with a learnable global token and jointly trains it with a text encoder, exploring design choices like pretrained text encoders, loss functions, training modes, and data mixtures. Key experimental findings show that this approach achieves state-of-the-art retrieval performance, with a two-stage training recipe and Matryoshka-style supervision enabling 8x smaller embeddings with minimal performance drop, and competitive results on AIR-Bench for MCQs and zero-shot classification.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决音频-文本嵌入模型中Whisper编码器未被充分利用的问题，这类模型主要依赖HTS-AT和PaSST等其他架构。提出的WavLink方法通过为Whisper编码器添加可学习的全局标记，并与文本编码器进行联合训练，系统探索了预训练文本编码器、损失函数、训练模式和数据混合等设计选择。关键实验结果表明，所确定的配置实现了最先进的检索性能，且通过两阶段训练方案和Matryoshka式监督，能够在性能下降最小的情况下将嵌入尺寸缩小8倍，同时在AIR-Bench的多选题和零样本分类任务上保持竞争力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
