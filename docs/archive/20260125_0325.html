<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-25 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260125_0325</div>
    <div class="row"><div class="card">
<div class="title">Why Can&#x27;t I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition</div>
<div class="meta-line">Authors: Geo Ahn, Inwoong Lee, Taeoh Kim, Minho Shim, Dongyoon Wee, Jinwoo Choi</div>
<div class="meta-line">First: 2026-01-22T18:59:13+00:00 · Latest: 2026-01-22T18:59:13+00:00</div>
<div class="meta-line">Comments: The code is available at https://github.com/KHU-VLL/RCORE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16211v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16211v1">PDF</a> · <a href="https://github.com/KHU-VLL/RCORE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为何打不开抽屉？缓解零样本组合动作识别中的物体驱动捷径</div>
<div class="mono" style="margin-top:8px">本研究聚焦组合视频理解（CVU），要求模型识别动词与物体并将其组合以泛化至未见组合。我们发现现有零样本组合动作识别（ZS-CAR）模型失效的主要原因是未被重视的失效模式：物体驱动的动词捷径。通过系统分析，我们揭示该行为源于两个交织因素：组合监督的严重稀疏性与偏态分布，以及动词与物体间不对称的学习难度。随着训练进行，现有ZS-CAR模型逐渐忽略视觉证据并过度拟合共现统计，导致其无法在未见动词-物体组合中获得组合识别的优势。为此，我们提出RCORE框架，通过强制时序锚定的动词学习来应对此问题。RCORE包含：（i）组合感知增强策略——在不破坏运动线索的前提下多样化动词-物体组合；（ii）时序顺序正则化损失——通过显式建模时序结构惩罚捷径行为。在Sth-com与新构建的EK100-com两个基准测试中，RCORE显著提升未见组合准确率，降低对共现偏置的依赖，并实现持续正向的组合性能增益。本研究揭示了物体驱动捷径是ZS-CAR的关键限制因素，并证明解决该问题对实现鲁棒的组合视频理解至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the failure of existing Zero-Shot Compositional Action Recognition (ZS-CAR) models, attributing it to object-driven verb shortcuts where models ignore visual motion and overfit to spurious verb-object co-occurrences due to sparse, skewed supervision and asymmetric learning difficulty. To mitigate this, the authors propose RCORE, a framework that enforces temporally grounded verb learning through composition-aware video augmentation to diversify combinations and a temporal order regularization loss to penalize shortcut behaviors. Experiments on the Sth-com and a newly constructed EK100-com benchmark show that RCORE significantly improves accuracy on unseen compositions, reduces co-occurrence bias, and achieves consistently positive compositional gaps, demonstrating the critical role of addressing object-driven shortcuts for robust compositional video understanding.</div>
<div class="mono" style="margin-top:8px">本研究探讨了现有零样本组合动作识别模型失败的原因，将其归因于物体驱动的动词捷径，即模型忽略视觉运动线索并过度拟合训练数据中虚假的动词-物体共现关系。为缓解此问题，作者提出了RCORE框架，该框架通过组合感知的视频增强来多样化组合，并利用时序顺序正则化损失来惩罚捷径行为，从而强制进行基于时序的动词学习。在Sth-com和新构建的EK100-com基准测试上的实验表明，RCORE显著提升了未见动词-物体组合的识别准确率，降低了对共现偏差的依赖，并实现了持续为正的组合泛化差距，证明了解决物体驱动捷径对于实现鲁棒的组合视频理解至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation</div>
<div class="meta-line">Authors: Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet A. Nguyen, Dong-Hwan Jang, Inderjit S Dhillon, Ismini Lourentzou</div>
<div class="meta-line">First: 2026-01-22T18:58:55+00:00 · Latest: 2026-01-22T18:58:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16210v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PyraTok：面向视频理解与生成的语言对齐金字塔型分词器</div>
<div class="mono" style="margin-top:8px">离散视频变分自编码器是现代文生视频与视频理解系统的核心基础，但现有分词器通常仅在单一尺度下学习视觉码本，其词汇量有限且语言监督浅层，导致跨模态对齐与零样本迁移性能不佳。本文提出PyraTok——一种语言对齐的金字塔型分词器，可在多时空分辨率下学习语义结构化的离散隐变量。PyraTok基于预训练视频VAE构建，其创新性语言对齐金字塔量化模块通过共享大型二进制码本在多个深度离散化编码器特征，生成紧凑而富有表现力的视频标记序列。为紧密耦合视觉标记与语言，PyraTok联合优化多尺度文本引导量化与标记层级结构的全局自回归目标。在十项基准测试中，PyraTok实现了最先进的视频重建效果，持续提升文生视频质量，并在视频分割、时序动作定位及视频理解任务中创下零样本性能新纪录，可稳健扩展至4K/8K分辨率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing discrete video VAEs often rely on single-scale visual codebooks with limited vocabularies and weak language supervision, which hampers cross-modal alignment and zero-shot generalization. To address this, we propose PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions by building on a pretrained video VAE and a novel Language-aligned Pyramidal Quantization (LaPQ) module; this module discretizes encoder features at various depths using a shared large binary codebook, and the model is jointly optimized with multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Experimental results across ten benchmarks show that PyraTok achieves state-of-the-art video reconstruction, consistently enhances text-to-video generation quality, and establishes new state-of-the-art zero-shot performance on video segmentation, temporal action localization, and video understanding tasks, while scaling robustly to 4K/8K resolutions.</div>
<div class="mono" style="margin-top:8px">现有的离散视频变分自编码器通常依赖于词汇量有限、语言监督薄弱的单尺度视觉码本，这阻碍了跨模态对齐和零样本迁移能力。为解决此问题，研究者提出了PyraTok，一种语言对齐的金字塔形分词器，它基于预训练的视频变分自编码器，并引入了语言对齐的金字塔量化模块，通过共享的大型二进制码本在多个时空分辨率深度上离散化编码器特征，从而生成紧凑而富有表现力的令牌序列。该方法通过多尺度文本引导量化和令牌层次结构的全局自回归目标进行联合优化。在十个基准测试上的实验结果表明，PyraTok实现了最先进的视频重建效果，持续提升了文本到视频的生成质量，并在视频分割、时序动作定位和视频理解任务上创造了新的零样本性能记录，同时能够稳健地扩展到高达4K/8K的分辨率。</div>
</details>
</div>
<div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：面向文档的具身视觉语言前端系统</div>
<div class="mono" style="margin-top:8px">GutenOCR是通过对Qwen2.5-VL-3B和Qwen2.5-VL-7B进行微调得到的一系列具身OCR前端模型。这些单检查点的视觉语言模型通过统一的提示驱动接口，实现了文本读取、检测与定位功能。模型基于商业文档、科学文献及合成定位数据训练，支持整页与局部读取，可提供行级与段落级边界框，并响应条件式“X在何处？”查询。我们提出了具身OCR评估框架，实验表明在1.05万份保留的商业与科学文档上，GutenOCR-7B的复合具身OCR分数较其骨干模型Qwen2.5-VL-7B提升超一倍（0.40至0.82）。在Fox与OmniDocBench v1.5基准测试中，该方法显著提升了区域/行级OCR性能及文本检测召回率，但在页面级线性化、色彩引导OCR及公式密集版式处理方面存在权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for unified document understanding models that combine reading, detection, and grounding capabilities. The authors introduce GutenOCR, a family of vision-language models created by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on business documents, scientific articles, and synthetic grounding data, enabling prompt-based full-page and localized reading with bounding box outputs. Experimental results show that GutenOCR-7B more than doubles the composite grounded OCR score of its backbone model (from 0.40 to 0.82) on 10.5K held-out pages, substantially improves region- and line-level OCR and text-detection recall on Fox and OmniDocBench v1.5 benchmarks, while revealing trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发能够统一实现文档阅读、检测与定位能力的文档理解模型。作者提出了GutenOCR模型系列，通过对Qwen2.5-VL-3B和Qwen2.5-VL-7B在商业文档、科学论文及合成定位数据上进行微调，构建了支持基于提示的全页和局部阅读并输出边界框的视觉语言模型。实验结果表明，在10.5K份保留文档上，GutenOCR-7B将其骨干模型的综合定位OCR分数从0.40提升至0.82以上；在Fox和OmniDocBench基准测试中，该模型显著提升了区域级和行级OCR性能以及文本检测召回率，但也在页面级线性化、颜色引导OCR和公式密集布局方面显示出性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-in-Sandbox Elicits General Agentic Intelligence</div>
<div class="meta-line">Authors: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</div>
<div class="meta-line">First: 2026-01-22T18:57:09+00:00 · Latest: 2026-01-22T18:57:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://llm-in-sandbox.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16206v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16206v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://llm-in-sandbox.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#x27;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM-in-Sandbox激发通用智能体智能</div>
<div class="mono" style="margin-top:8px">我们提出LLM-in-Sandbox方法，使大语言模型能在代码沙盒（即虚拟计算机）内进行探索，从而激发非代码领域的通用智能。我们首先证明，未经额外训练的强大大语言模型展现出利用代码沙盒处理非代码任务的泛化能力。例如，大语言模型能自发访问外部资源获取新知识、利用文件系统处理长上下文、执行脚本满足格式要求。我们进一步表明，通过LLM-in-Sandbox强化学习（仅使用非智能体数据训练模型进行沙盒探索），可增强这些智能体能力。实验表明，LLM-in-Sandbox在免训练和训练后两种设置下，均实现了跨越数学、物理、化学、生物医学、长上下文理解及指令遵循的稳健泛化。最后，我们从计算和系统角度分析其效率，并将其开源为Python包以促进实际部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to elicit general agentic intelligence from large language models (LLMs) by enabling them to explore and act within a code sandbox environment, which serves as a virtual computer. The method involves allowing LLMs to use the sandbox for tasks like accessing external resources, managing files for long contexts, and executing scripts, initially without additional training, and then enhancing these capabilities through a reinforcement learning approach (LLM-in-Sandbox-RL) that trains models using only non-agentic data. Experimental results show that this approach achieves robust generalization across diverse domains including mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following, with efficiency analyses and an open-source Python package provided for deployment.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过让大型语言模型（LLM）在代码沙盒环境中探索，从而在非代码领域激发其通用的智能体能力。方法包括使用未经额外训练的强LLM来展示泛化能力，例如访问外部资源、利用文件系统处理长上下文和执行脚本，并通过基于非智能体数据训练的LLM-in-Sandbox强化学习（LLM-in-Sandbox-RL）进一步增强这些能力。实验结果表明，LLM-in-Sandbox在无需训练和经过后训练两种设置下，在数学、物理、化学、生物医学、长上下文理解和指令遵循方面均实现了稳健的泛化，并从计算和系统角度分析了其效率。</div>
</details>
</div>
<div class="card">
<div class="title">Counterfactual Training: Teaching Models Plausible and Actionable Explanations</div>
<div class="meta-line">Authors: Patrick Altmeyer, Aleksander Buszydlik, Arie van Deursen, Cynthia C. S. Liem</div>
<div class="meta-line">First: 2026-01-22T18:56:14+00:00 · Latest: 2026-01-22T18:56:14+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the 2026 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反事实训练：为模型提供合理且可操作的因果解释</div>
<div class="mono" style="margin-top:8px">我们提出了一种称为反事实训练的新型训练机制，利用反事实解释来增强模型的可解释性。反事实解释已成为对不透明机器学习模型进行事后解释的流行方法：它们揭示了事实输入需要如何改变，才能使模型产生期望的输出。为了在实际决策系统中发挥作用，反事实解释应基于底层数据具有合理性，并符合特征可变性的约束条件。因此，现有研究多集中于开发事后方法以生成满足这些要求的反事实解释。在本研究中，我们转而让模型直接对期望的最终目标负责：反事实训练在训练阶段使用反事实解释，以最小化学得表征与合理、可操作解释之间的差异。我们通过实证和理论证明，所提出的方法有助于训练出能提供内在理想反事实解释的模型，并同时提升对抗鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the explanatory capacity of machine learning models by ensuring that their counterfactual explanations are both plausible with respect to the data distribution and actionable under feature constraints, which is crucial for real-world decision-making. The proposed method, counterfactual training, integrates counterfactual explanations directly into the training phase to minimize the divergence between the model&#x27;s learned representations and these desirable explanations. Experimental results demonstrate that this approach not only yields models that inherently provide plausible and actionable counterfactuals but also improves their adversarial robustness.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升机器学习模型的解释能力，确保其反事实解释既符合数据分布的合理性，又能在特征可变性约束下具有可操作性，这对现实世界决策至关重要。所提出的方法——反事实训练——将反事实解释直接整合到模型训练阶段，以最小化学习到的表征与这些理想解释之间的差异，而非依赖事后生成技术。实验和理论结果表明，该方法不仅能使模型内在地提供合理且可操作的反事实解释，还增强了模型的对抗鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Geospatial Place Representation Learning from Large-Scale Point-of-Interest Graph Data</div>
<div class="meta-line">Authors: Mohammad Hashemi, Hossein Amiri, Andreas Zufle</div>
<div class="meta-line">First: 2025-06-25T15:10:31+00:00 · Latest: 2026-01-22T18:46:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02921v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.02921v3">PDF</a> · <a href="https://github.com/mohammadhashemii/PlaceRep">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning effective representations of urban environments requires capturing spatial structure beyond fixed administrative boundaries. Existing geospatial representation learning approaches typically aggregate Points of Interest(POI) into pre-defined administrative regions such as census units or ZIP code areas, assigning a single embedding to each region. However, POIs often form semantically meaningful groups that extend across, within, or beyond these boundaries, defining places that better reflect human activity and urban function. To address this limitation, we propose PlaceRep, a training-free geospatial representation learning method that constructs place-level representations by clustering spatially and semantically related POIs. PlaceRep summarizes large-scale POI graphs from U.S. Foursquare data to produce general-purpose urban region embeddings while automatically identifying places across multiple spatial scales. By eliminating model pre-training, PlaceRep provides a scalable and efficient solution for multi-granular geospatial analysis. Experiments using the tasks of population density estimation and housing price prediction as downstream tasks show that PlaceRep outperforms most state-of-the-art graph-based geospatial representation learning methods and achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation of PlaceRep is available at https://github.com/mohammadhashemii/PlaceRep.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大规模兴趣点图数据的免训练地理空间场所表征学习</div>
<div class="mono" style="margin-top:8px">学习有效的城市环境表征需捕捉超越固定行政边界的空间结构。现有地理空间表征学习方法通常将兴趣点聚合至人口普查单元或邮政编码区等预定义行政区域，并为每个区域分配单一嵌入向量。然而，兴趣点常形成跨越、穿透或超出这些边界的语义化群组，从而定义出更能反映人类活动与城市功能的场所。为突破此局限，我们提出PlaceRep——一种免训练的地理空间表征学习方法，通过聚类空间与语义相关的兴趣点来构建场所级表征。该方法基于美国Foursquare数据的大规模兴趣点图进行归纳，在自动识别多空间尺度场所的同时生成通用型城市区域嵌入向量。通过消除模型预训练环节，PlaceRep为多粒度地理空间分析提供了可扩展的高效解决方案。在人口密度估算和房价预测两项下游任务中的实验表明，PlaceRep优于多数基于图结构的先进地理空间表征学习方法，并在大规模兴趣点图上生成区域级表征时实现高达100倍的加速。项目代码已开源：https://github.com/mohammadhashemii/PlaceRep。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitation of existing geospatial representation methods that rely on fixed administrative boundaries and assign a single embedding to each pre-defined region, this study introduces PlaceRep, a training-free method for learning place-level representations. The approach constructs representations by clustering Points of Interest (POI) from large-scale graph data based on spatial and semantic relatedness, automatically identifying meaningful places across multiple scales without requiring model pre-training. Experimental evaluations on downstream tasks including population density estimation and housing price prediction demonstrate that PlaceRep outperforms most state-of-the-art graph-based geospatial methods and achieves up to a 100x speedup in generating region-level embeddings from large-scale POI graphs.</div>
<div class="mono" style="margin-top:8px">针对现有地理空间表征学习方法依赖固定行政边界、为每个预定义区域分配单一嵌入向量的局限，本研究提出了PlaceRep，一种无需训练的地点级表征学习方法。该方法通过基于空间和语义相关性对大规模兴趣点（POI）图数据进行聚类来构建表征，无需模型预训练即可自动识别跨多个尺度的有意义地点。在人口密度估计和房价预测等下游任务上的实验评估表明，PlaceRep的性能优于大多数最先进的基于图的地理空间方法，并且在大规模POI图上生成区域级嵌入的速度提升了高达100倍。</div>
</details>
</div>
<div class="card">
<div class="title">SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks</div>
<div class="meta-line">Authors: Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Charles McGrady, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, Yixin Liu, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, Arman Cohan</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-07-01T17:51:59+00:00 · Latest: 2026-01-22T18:32:06+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Datasets &amp; Benchmarks Track (Spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01001v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.01001v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature-grounded tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 47 foundation models and has collected over 20,000 votes from human researchers across diverse scientific domains. Our analysis of the data collected so far confirms its high quality. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on collected preference data. It measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark&#x27;s challenges and emphasize the need for more reliable automated evaluation methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SciArena：面向非可验证科学文献基础任务的开放评估平台</div>
<div class="mono" style="margin-top:8px">我们推出SciArena，一个开放协作的平台，用于评估基础模型在科学文献基础任务上的表现。与传统科学文献理解与综合基准不同，SciArena借鉴Chatbot Arena的社区投票评估模式，直接让研究社区参与模型比较。通过汇聚集体智慧，该平台以社区驱动的方式评估模型在需要基于文献的长篇回答的开放式科学任务上的性能。目前平台支持47个基础模型，并已收集来自不同科学领域研究人员的超过20,000张投票。我们对现有数据的分析证实了其高质量。我们基于模型排名榜单讨论了结果与洞见。为促进文献任务中基于模型的自动评估系统研究，我们发布了SciArena-Eval——一个基于收集的偏好数据的元评估基准，通过对比模型成对评估与人类投票来衡量模型判断答案质量的准确性。实验凸显了该基准的挑战性，并强调了对更可靠自动评估方法的需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for better evaluation of foundation models on open-ended scientific literature-grounded tasks, where traditional benchmarks often lack verifiable ground truth. The method introduces SciArena, an open platform that adapts the Chatbot Arena approach, enabling the research community to directly vote on pairwise model comparisons to generate performance rankings based on collective intelligence. Experimental results from over 20,000 human votes across 47 models confirm the high quality of the collected data, and the derived SciArena-Eval benchmark reveals significant challenges for automated evaluation methods in matching human judgment accuracy.</div>
<div class="mono" style="margin-top:8px">SciArena 的研发动机是解决现有基准难以评估大模型在开放式、基于科学文献的任务上的表现，因为这类任务通常无法通过传统可验证的基准进行准确衡量。该方法借鉴了 Chatbot Arena 的社区投票模式，构建了一个开放平台，让研究社区直接对模型生成的文献支撑的长文本回答进行比较和投票，从而利用集体智慧进行评估。基于对 47 个模型收集的超过 20,000 张人类投票的关键实验结果表明，所收集的数据质量很高，并得出了模型排名榜单；同时，基于这些偏好数据发布的 SciArena-Eval 元评估基准显示，当前自动化评估方法在判断回答质量时与人类投票的一致性较差，这凸显了对更可靠自动化评估方法的需求。</div>
</details>
</div>
<div class="card">
<div class="title">Paramanu: Compact and Competitive Monolingual Language Models for Low-Resource Morphologically Rich Indian Languages</div>
<div class="meta-line">Authors: Mitodru Niyogi, Eric Gaussier, Arnab Bhattacharya</div>
<div class="meta-line">First: 2024-01-31T17:58:10+00:00 · Latest: 2026-01-22T18:28:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.18034v3">Abs</a> · <a href="https://arxiv.org/pdf/2401.18034v3">PDF</a> · <a href="https://huggingface.co/collections/mitodru/paramanu">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multilingual large language models (LLMs) are expensive to pretrain and often suffer from imbalances across languages and datasets, English-centric bias, tokenizer oversegmentation for morphologically rich low-resource languages, and the curse of multilinguality. We introduce PARAMANU, the first family of Indian-only autoregressive language models trained from scratch on open-source language-specific data for the five most spoken Indian languages: Bengali, Hindi, Marathi, Tamil, and Telugu. All models are designed for affordability and are trained on a single GPU with a budget under $1,000, allowing under-resourced researchers to build competitive language models. To address low-resource challenges, we develop morphology-aligned, low-fertility tokenizers, propose an interpolation-based method for token position indices in RoPE based scaling to train longer sequences efficiently. We also create instruction-tuning datasets in Bangla that are translated to the other four languages. Despite their small size (108M-367M parameters), Paramanu achieves a strong performance-efficiency tradeoff and outperforms most larger multilingual models across all five languages. Our collection is available at https://huggingface.co/collections/mitodru/paramanu.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Paramanu：面向低资源形态丰富印度语言的紧凑型高性能单语模型</div>
<div class="mono" style="margin-top:8px">多语种大语言模型（LLM）预训练成本高昂，常面临语言与数据集不平衡、英语中心偏见、对形态丰富的低资源语言分词过度，以及多语性诅咒等问题。我们推出PARAMANU——首个完全基于开源单语数据从头训练的印度语系自回归语言模型家族，涵盖五种使用最广泛的印度语言：孟加拉语、印地语、马拉地语、泰米尔语和泰卢固语。所有模型均注重成本效益，可在单GPU上以低于1000美元的预算完成训练，使资源有限的研究者也能构建有竞争力的语言模型。为应对低资源挑战，我们开发了形态对齐的低生育率分词器，提出基于旋转位置编码（RoPE）缩放中位置索引插值的方法以高效训练长序列，并创建了孟加拉语指令微调数据集（同步翻译至其余四种语言）。尽管模型规模较小（1.08亿-3.67亿参数），Paramanu在性能与效率间取得了优异平衡，在全部五种语言上超越了多数规模更大的多语模型。模型集已发布于https://huggingface.co/collections/mitodru/paramanu。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high cost and imbalances of multilingual large language models, particularly for low-resource morphologically rich languages, this work introduces Paramanu, a family of monolingual autoregressive models trained from scratch for five major Indian languages. The method involves developing morphology-aligned tokenizers to reduce oversegmentation, proposing an interpolation-based technique for efficient longer sequence training with RoPE, and creating instruction-tuning datasets via translation. Experimental results show that despite their compact size of 108M to 367M parameters and training on a single GPU under $1,000, these models outperform most larger multilingual models across Bengali, Hindi, Marathi, Tamil, and Telugu, achieving a strong performance-efficiency tradeoff.</div>
<div class="mono" style="margin-top:8px">为解决多语言大语言模型成本高昂且存在语言间不平衡的问题，特别是针对低资源、形态丰富的语言，本研究提出了Paramanu，这是一个为五种主要印度语言从头开始训练的单语自回归模型系列。方法包括开发形态对齐的分词器以减少过度分割，采用基于插值的技术在RoPE中高效训练更长序列，并通过翻译创建指令微调数据集。实验结果表明，尽管模型参数量仅为1.08亿至3.67亿，且在单GPU上以低于1000美元的成本训练，这些模型在孟加拉语、印地语、马拉地语、泰米尔语和泰卢固语上均超越了多数更大的多语言模型，展现了优异的性能与效率平衡。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-01-22T18:24:00+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v1">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题探索新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时实施强化学习，使大型语言模型能够持续训练，并针对具体测试问题积累经验。这种持续学习形式具有特殊性：其目标是产生单一卓越解决方案而非平均意义上的多个良好方案，且专注于解决当前问题而非泛化至其他问题。因此，我们的学习目标和搜索子程序均优先考虑最具潜力的解决方案。我们将此方法称为“测试时训练发现法”。延续先前研究，我们聚焦于具有连续奖励的问题，并在数学、GPU内核工程、算法设计和生物学领域报告所有尝试问题的结果。该方法在几乎所有领域均创下新纪录：（1）埃尔德什最小重叠问题与自相关不等式；（2）GPUMode内核竞赛（较现有技术提速达2倍）；（3）历史AtCoder算法竞赛；（4）单细胞分析去噪问题。所有解决方案均经专家或主办方评审。与先前依赖封闭前沿模型的最佳成果不同，本研究全部结果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现。测试时训练通过Thinking Machines的Tinker API执行，每个问题成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance AI&#x27;s capability to discover novel state-of-the-art solutions for scientific problems, moving beyond prior test-time scaling methods that rely on prompting frozen LLMs. The proposed method, Test-Time Training to Discover (TTT-Discover), employs reinforcement learning at test time, allowing the LLM to continuously train with experience specific to the test problem, focusing on generating a single optimal solution rather than average performance across tasks. Key experimental results demonstrate that TTT-Discover achieves new state-of-the-art performance across diverse domains, including mathematics (Erdős&#x27; minimum overlap problem and an autocorrelation inequality), GPU kernel engineering (up to 2× faster than prior art), algorithm design (past AtCoder competitions), and biology (denoising in single-cell analysis), with all solutions validated by experts and achieved using an open model and publicly available code.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升人工智能在科学问题中发现新颖最优解的能力，超越先前依赖提示冻结大型语言模型的测试时扩展方法。所提出的方法——测试时训练发现（TTT-Discover），在测试时采用强化学习，使大型语言模型能够针对特定测试问题持续训练，专注于生成单一最优解而非平均性能。关键实验结果表明，TTT-Discover在多个领域实现了新的最优性能，包括数学（埃尔德什最小重叠问题和自相关不等式）、GPU内核工程（比先前技术快达2倍）、算法设计（过往AtCoder竞赛）和生物学（单细胞分析中的去噪问题），所有解决方案均经专家验证，并使用开放模型和公开代码实现。</div>
</details>
</div>
<div class="card">
<div class="title">Structured Hints for Sample-Efficient Lean Theorem Proving</div>
<div class="meta-line">Authors: Zachary Burton</div>
<div class="meta-line">First: 2026-01-22T18:16:46+00:00 · Latest: 2026-01-22T18:16:46+00:00</div>
<div class="meta-line">Comments: 9 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构化提示在样本高效Lean定理证明中的应用</div>
<div class="mono" style="margin-top:8px">当前最先进的神经定理证明器（如DeepSeek-Prover-V1.5）将大语言模型与强化学习相结合，通过复杂训练取得了显著成果。我们提出：这些经过高度训练的模型在推理时是否仍能从简单的结构指导中受益？我们在miniF2F基准上评估了一种轻量级干预方法——基于15种常见策略骨架的固定提示调度。该简单方法实现了21.7%的pass@16，而相同模型的标准采样仅为15.2%，在使用相同样本数（k=16）和相同最大生成长度（1024个标记）的情况下相对提升了43%。结果表明，即使经过RL训练的证明器也未能充分利用策略语言中可用的结构化先验，而简单的推理时指导仍能提供低成本、互补性的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work investigates whether state-of-the-art neural theorem provers, which are trained with sophisticated reinforcement learning, can still benefit from simple structural guidance during inference. The method introduces a lightweight intervention using a fixed prompt schedule over 15 common tactic skeletons to guide the DeepSeek-Prover-V1.5 model on the miniF2F benchmark. Experimental results show that this approach yields a 21.7% pass@16 success rate compared to 15.2% for standard sampling, representing a 43% relative improvement using the same computational budget, which indicates that even capable RL-trained provers underutilize available structural priors and that simple inference-time guidance provides a cheap, complementary performance boost.</div>
<div class="mono" style="margin-top:8px">本研究探讨了经过强化学习高度训练的先进神经定理证明器，在推理时是否仍能从简单的结构引导中受益。方法引入了一种轻量级干预，即在miniF2F基准测试上使用一个包含15个常见策略骨架的固定提示调度来引导模型。实验结果表明，该方法在相同样本数和生成长度下，取得了21.7%的pass@16成功率，而标准采样仅为15.2%，相对提升了43%，这表明即使能力强大的模型也未充分利用策略语言中的结构先验，而推理时的简单引导能提供一种廉价且互补的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</div>
<div class="meta-line">Authors: Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</div>
<div class="meta-line">First: 2026-01-22T18:09:30+00:00 · Latest: 2026-01-22T18:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16163v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16163v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#x27;s latent diffusion process, harnessing the model&#x27;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cosmos策略：面向视觉运动控制与规划的视觉模型微调</div>
<div class="mono" style="margin-top:8px">近期视频生成模型展现出捕捉复杂物理交互与时序场景演变的卓越能力。为利用其时空先验知识，机器人研究领域已采用视频模型进行策略学习，但需通过多阶段后训练及新增动作生成架构组件，引入了复杂性。本研究提出Cosmos策略，这是一种通过单阶段后训练将大型预训练视频模型（Cosmos-Predict2）适配为高效机器人策略的简洁方法：仅使用目标平台采集的机器人演示数据进行训练，无需修改模型架构。该策略通过视频模型的潜在扩散过程直接生成编码为潜在帧的机器人动作，利用模型的预训练先验与核心学习算法捕捉复杂动作分布。此外，Cosmos策略能同步生成编码为潜在帧的未来状态图像与价值函数（预期累积奖励），从而在测试阶段规划更高成功率的动作轨迹。实验评估显示，Cosmos策略在LIBERO与RoboCasa仿真基准测试中分别达到98.5%与67.1%的平均成功率，在现实世界复杂双手操作任务中获得最高平均分，其表现优于从头训练的扩散策略、基于视频模型的策略及相同演示数据微调的先进视觉-语言-动作模型。该策略还可利用策略执行数据，通过经验学习优化其世界模型与价值函数，并借助基于模型的规划在挑战性任务中实现更高成功率。相关代码、模型与训练数据已发布于https://research.nvidia.com/labs/dir/cosmos-policy/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To leverage the strong spatiotemporal priors of video generation models for robotics without introducing complex multi-stage training or architectural modifications, this work introduces Cosmos Policy, which fine-tunes a pretrained video model (Cosmos-Predict2) directly on robot demonstration data. The method encodes robot actions, future state images, and value predictions as latent frames within the model&#x27;s existing latent diffusion process, enabling it to capture complex action distributions and support test-time planning. Experiments show state-of-the-art performance on LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates) and superior results in real-world bimanual manipulation, outperforming diffusion policies trained from scratch and other fine-tuned models, with additional improvements possible through learning from rollout data to refine its world model and value function.</div>
<div class="mono" style="margin-top:8px">本研究旨在简化将强大视频生成模型应用于机器人控制的过程，避免先前工作中所需的多阶段后训练和架构修改。所提出的Cosmos Policy方法直接在机器人演示数据上微调预训练视频模型（Cosmos-Predict2），将机器人动作、未来状态图像和价值估计编码为模型现有潜在扩散过程中的潜在帧，从而利用其时空先验进行控制和规划。实验结果表明，该方法在LIBERO和RoboCasa仿真基准上取得了最先进的性能（平均成功率分别为98.5%和67.1%），并在现实世界的双手操作任务中获得了最高平均分数，优于从头训练的扩散策略、其他基于视频模型的策略以及在相同演示数据上微调的先进视觉-语言-动作模型，且通过从策略 rollout 数据中学习以改进其世界模型和价值函数，还能实现更高的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">Substrate Stability Under Persistent Disagreement: Structural Constraints for Neutral Ontological Substrates</div>
<div class="meta-line">Authors: Denise M. Case</div>
<div class="meta-line">First: 2026-01-22T17:51:02+00:00 · Latest: 2026-01-22T17:51:02+00:00</div>
<div class="meta-line">Comments: 29 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16152v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16152v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern data systems increasingly operate under conditions of persistent legal, political, and analytic disagreement. In such settings, interoperability cannot rely on shared interpretation, negotiated semantics, or centralized authority. Instead, representations must function as neutral substrates that preserve stable reference across incompatible extensions. This paper investigates the structural constraints imposed on ontological design by this requirement. Building on a neutrality framework that treats interpretive non-commitment and stability under extension as explicit design constraints, we ask what minimal ontological structure is forced if accountability relationships are to remain referable and comparable under disagreement. Minimality here is not mere parsimony: a reduction is admissible only if it does not reintroduce stability-critical distinctions as hidden roles, flags, or contextual predicates. We establish a conditional lower-bound result: any ontology capable of supporting accountability under persistent disagreement must realize at least six distinct identity-and-persistence regimes. We further show that a construction with exactly six such regimes is sufficient to satisfy the stated requirements without embedding causal or normative commitments in the substrate. The result is not a proposal for a universal ontology, but a constraint on what is possible when neutrality and stable reference are treated as non-negotiable design goals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>持续分歧下的基底稳定性：中性本体论基底的结构约束</div>
<div class="mono" style="margin-top:8px">现代数据系统日益在持续的法律、政治与分析分歧条件下运行。此类场景中，互操作性无法依赖共享解释、协商语义或集中权威，而必须通过能跨不兼容扩展保持稳定指称的中性基底实现表征。本文探究该需求对本体制设计施加的结构约束。基于将解释非承诺性与扩展稳定性作为显式设计约束的中性框架，我们追问：若问责关系需在分歧下保持可指称与可比较，何种最小本体结构是必需的？此处最小性非单纯简约：仅当不将稳定性关键区分重新引入为隐藏角色、标记或语境谓词时，简化才可接受。我们建立了条件性下界结果：任何能在持续分歧下支持问责的本体必须实现至少六种不同的身份-存续机制。进一步证明，恰好六种此类机制的构造足以满足既定需求，且不在基底中嵌入因果或规范承诺。该结果非普适本体方案，而是将中立性与稳定指称视为不可协商设计目标时的可能性约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern data systems must operate amid persistent legal, political, and analytic disagreements, where interoperability cannot depend on shared interpretation or centralized authority. This paper investigates the structural constraints for designing neutral ontological substrates that maintain stable reference across incompatible extensions, treating interpretive non-commitment and stability under extension as explicit design goals. The method builds on a neutrality framework to determine the minimal ontological structure required for preserving referable and comparable accountability relationships, avoiding reductions that hide critical distinctions as roles or predicates. Key experimental findings establish a conditional lower-bound result: any ontology supporting accountability under persistent disagreement must implement at least six distinct identity-and-persistence regimes, and a construction with exactly six regimes is shown to be sufficient without embedding causal or normative commitments.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在持续存在的法律、政治和分析分歧下设计数据系统的挑战，这些情况下无法依赖共享解释或集中权威来实现互操作性。方法基于一个中立性框架，将解释性非承诺和扩展下的稳定性作为明确的设计约束，以推导出本体论基质的最小结构要求，从而保持问责关系的稳定指称。关键的实验结果表明了一个条件性下限结果，证明任何在此类分歧下支持问责的本体必须实现至少六种不同的身份与持久性机制，并论证了恰好六种机制的构建足以满足要求，而无需在基质中嵌入因果或规范性承诺。</div>
</details>
</div>
<div class="card">
<div class="title">Pay (Cross) Attention to the Melody: Curriculum Masking for Single-Encoder Melodic Harmonization</div>
<div class="meta-line">Authors: Maximos Kaliakatsos-Papakostas, Dimos Makris, Konstantinos Soiledis, Konstantinos-Theodoros Tsamis, Vassilis Katsouros, Emilios Cambouropoulos</div>
<div class="meta-line">First: 2026-01-22T17:46:31+00:00 · Latest: 2026-01-22T17:46:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Melodic harmonization, the task of generating harmonic accompaniments for a given melody, remains a central challenge in computational music generation. Recent single encoder transformer approaches have framed harmonization as a masked sequence modeling problem, but existing training curricula inspired by discrete diffusion often result in weak (cross) attention between melody and harmony. This leads to limited exploitation of melodic cues, particularly in out-of-domain contexts. In this work, we introduce a training curriculum, FF (full-to-full), which keeps all harmony tokens masked for several training steps before progressively unmasking entire sequences during training to strengthen melody-harmony interactions. We systematically evaluate this approach against prior curricula across multiple experimental axes, including temporal quantization (quarter vs. sixteenth note), bar-level vs. time-signature conditioning, melody representation (full range vs. pitch class), and inference-time unmasking strategies. Models are trained on the HookTheory dataset and evaluated both in-domain and on a curated collection of jazz standards, using a comprehensive set of metrics that assess chord progression structure, harmony-melody alignment, and rhythmic coherence. Results demonstrate that the proposed FF curriculum consistently outperforms baselines in nearly all metrics, with particularly strong gains in out-of-domain evaluations where harmonic adaptability to novel melodic queues is crucial. We further find that quarter-note quantization, intertwining of bar tokens, and pitch-class melody representations are advantageous in the FF setting. Our findings highlight the importance of training curricula in enabling effective melody conditioning and suggest that full-to-full unmasking offers a robust strategy for single encoder harmonization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关注旋律：单编码器旋律和声生成的课程掩码策略</div>
<div class="mono" style="margin-top:8px">旋律和声生成——为给定旋律创作和声伴奏——是计算音乐生成的核心挑战。近期单编码器Transformer方法将其构建为掩码序列建模问题，但受离散扩散启发的现有训练课程常导致旋律与和声间的（交叉）注意力薄弱，限制了旋律线索的利用，尤其在域外场景中。本研究提出FF（全掩码到全揭示）训练课程：在训练初期持续掩码所有和声标记，随后逐步揭示完整序列以强化旋律-和声交互。我们通过多维度实验系统评估该方法，包括时间量化（四分音符vs十六分音符）、小节级vs拍号条件、旋律表征（全音域vs音级）及推理时揭示策略。模型在HookTheory数据集训练，并在域内及精选爵士标准曲集中评估，使用综合指标衡量和弦进行结构、和声-旋律对齐及节奏连贯性。结果表明，FF课程在几乎所有指标上均优于基线，在域外评估中提升尤为显著——其对新颖旋律线索的和声适应性至关重要。进一步发现，四分音符量化、小节标记交织及音级旋律表征在FF框架中具有优势。本研究凸显了训练课程对实现有效旋律条件化的重要性，表明全掩码到全揭示策略为单编码器和声生成提供了稳健方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to improve melody-harmony interaction in single-encoder transformer models for melodic harmonization, which often exhibit weak cross-attention, this work introduces a full-to-full (FF) training curriculum. The method initially keeps all harmony tokens masked for several steps before progressively unmasking entire sequences during training to strengthen conditioning on the melody. Experimental evaluation on the HookTheory dataset and a collection of jazz standards, using metrics for chord structure, alignment, and rhythm, shows the FF curriculum consistently outperforms prior curricula across nearly all metrics, with especially strong gains in out-of-domain settings, and identifies advantageous configurations like quarter-note quantization and pitch-class melody representations.</div>
<div class="mono" style="margin-top:8px">针对单编码器旋律和声化模型中旋律与和声交互较弱的问题，本研究提出了一种全到全（FF）训练课程。该方法在训练初期保持所有和声标记被掩码，随后逐步对整个序列进行解掩码，以强化模型对旋律条件的利用。在HookTheory数据集和爵士标准曲集上的实验评估表明，使用评估和弦结构、对齐和节奏的综合性指标，FF课程在几乎所有指标上均优于基线方法，尤其在域外评估中提升显著，并确定了四分音符量化、小节标记交织和音高类旋律表示等有利配置。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamical Mechanisms for Coordinating Long-term Working Memory Based on the Precision of Spike-timing in Cortical Neurons</div>
<div class="meta-line">Authors: Terrence J. Sejnowski</div>
<div class="meta-line">First: 2025-12-17T19:05:18+00:00 · Latest: 2026-01-22T17:40:42+00:00</div>
<div class="meta-line">Comments: 31 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15891v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.15891v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the last century, most sensorimotor studies of cortical neurons relied on average firing rates. Rate coding is efficient for fast sensorimotor processing that occurs within a few seconds. Much less is known about long-term working memory with a time scale of hours (Ericsson and Kintsch, 1995). The discovery of millisecond-precision spike initiation in cortical neurons was unexpected (Mainen and Sejnowski, 1995). Even more striking was the precision of spiking in vivo, in response to rapidly fluctuating sensory inputs, suggesting that neural circuits could preserve and manipulate sensory information through spike timing. High temporal resolution enables a broader range of neural codes. It could also support spike-timing-dependent plasticity (STDP), which is triggered by the relative timing of spikes between presynaptic and postsynaptic neurons in the millisecond range. What spike-timing mechanisms could regulate STDP in vivo? Cortical traveling waves have been observed across many frequency bands with high temporal precision. Traveling waves have wave fronts that could link spike timing to STDP. As a wave front passes through a cortical column, excitatory synapses on the dendrites of both pyramidal and basket cells are stimulated synchronously. Inhibitory basket cells form a calyx on pyramidal cell bodies, and inhibitory rebound following a strong transient hyperpolarization can trigger a backpropagating action potential, which arrives shortly after the excitatory inputs on pyramidal dendrites. STDP activated in this way could persist for hours, creating a second-tier network. This temporary network could support long-term working memory, a cognitive network riding above the long-term sensorimotor network. On their own, traveling waves and STDP have not yet yielded new insights into cortical function. Together, they could be responsible for how we think (Sejnowski, 2025).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于皮层神经元尖峰时序精度的长时工作记忆协调动力学机制</div>
<div class="mono" style="margin-top:8px">上世纪大多数关于皮层神经元的感知运动研究依赖于平均放电频率。速率编码适用于数秒内完成的快速感知运动处理。而对于时间尺度达数小时的长时工作记忆（Ericsson与Kintsch，1995），认知则相对有限。皮层神经元毫秒级精度的尖峰启动现象出人意料（Mainen与Sejnowski，1995）。更令人瞩目的是活体内响应快速波动感觉输入时尖峰放电的精确性，这表明神经回路可通过尖峰时序保存和处理感觉信息。高时间分辨率拓展了神经编码的范畴，亦可能支持尖峰时序依赖性可塑性（STDP）——该机制由突触前后神经元毫秒级尖峰相对时序触发。何种尖峰时序机制能在活体调控STDP？皮层行波已在多频段观测到其高时间精度特性。行波波前可将尖峰时序与STDP相耦合：当波前穿过皮层柱时，锥体细胞与篮状细胞树突上的兴奋性突触被同步激活。抑制性篮状细胞在锥体细胞胞体形成萼状结构，强瞬态超极化后的抑制性反弹可触发反向传播动作电位，该电位稍晚于锥体树突兴奋性输入抵达。由此激活的STDP可持续数小时，形成次级网络。该临时网络可支撑长时工作记忆——一种叠加于长时感知运动网络之上的认知网络。单独的行波与STDP尚未对皮层功能研究带来新突破，但二者结合可能揭示思维运作机制（Sejnowski，2025）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the gap in understanding long-term working memory, which operates over hours, contrasting with the well-studied fast sensorimotor processes relying on average firing rates. It proposes a mechanism where cortical traveling waves, with their precise temporal structure, synchronize excitatory inputs and trigger inhibitory rebound, leading to backpropagating action potentials that activate spike-timing-dependent plasticity (STDP) with millisecond precision. Experimental and theoretical modeling suggests this coordinated spike-timing mechanism can establish a persistent second-tier network, potentially supporting long-term working memory by forming temporary cognitive networks atop existing sensorimotor circuits.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决对长达数小时的长时工作记忆机制的理解不足，这与基于平均发放率的快速感觉运动处理形成对比。它提出了一种机制，其中皮层行波以毫秒精度的时间同步兴奋性输入，并触发抑制性反弹，导致反向传播动作电位，从而激活依赖于尖峰时序的可塑性。这一过程形成了一个可持续数小时的临时二级网络，为尖峰时序机制如何在体内调节STDP以支持思维等认知功能提供了潜在解释。</div>
</details>
</div>
<div class="card">
<div class="title">Chat-TS: Enhancing Multi-Modal Reasoning Over Time-Series and Natural Language Data</div>
<div class="meta-line">Authors: Paul Quinlan, Qingguo Li, Xiaodan Zhu</div>
<div class="meta-line">First: 2025-03-13T21:05:11+00:00 · Latest: 2026-01-22T17:37:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10883v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.10883v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are being rapidly deployed across many fields such as healthcare, finance, transportation, and energy, where time-series data are fundamental components. The current works are still limited in their ability to perform reasoning that involves both time-series and the corresponding textual content. We address this gap by introducing Chat-TS, a large language model (LLM) based framework designed to support reasoning over time series and textual data. Unlike traditional models, Chat-TS integrates time-series tokens into LLMs&#x27; vocabulary, enhancing its reasoning ability over both modalities without compromising core natural language capabilities. To support learning and evaluation, we contribute new datasets: the TS Instruct Training Dataset (pairing diverse time-series data with relevant text instructions and responses for instruction tuning), the TS Instruct Question and Answer (QA) Gold Dataset (multiple-choice questions to evaluate multimodal reasoning), and a TS Instruct Quantitative Probing Set (a small subset of TS Instruct QA reasoning tasks alongside math and decision-making questions for LLM evaluation). We design a training strategy to preserve the inherent reasoning capabilities of LLMs while augmenting them for time-series reasoning. Experiments show that Chat-TS achieves state-of-the-art performance in multimodal reasoning tasks by maintaining strong natural language proficiency while improving time-series reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Chat-TS：增强时间序列与自然语言数据的多模态推理能力</div>
<div class="mono" style="margin-top:8px">大语言模型正迅速应用于医疗、金融、交通和能源等多个领域，这些领域以时间序列数据为基础。现有研究在处理时间序列与对应文本内容的联合推理方面仍存在局限。为填补这一空白，我们提出了Chat-TS——一个基于大语言模型的框架，旨在支持对时间序列与文本数据的推理。与传统模型不同，Chat-TS将时间序列标记集成到大语言模型的词汇表中，在保持核心自然语言能力的同时，增强了对两种模态的推理能力。为支持学习与评估，我们贡献了新的数据集：TS Instruct训练数据集（将多样化时间序列数据与相关文本指令及响应配对，用于指令微调）、TS Instruct问答黄金数据集（通过多选题评估多模态推理），以及TS Instruct定量探测集（包含少量TS Instruct问答推理任务及数学与决策问题，用于大语言模型评估）。我们设计了一种训练策略，在保持大语言模型固有推理能力的同时，增强其时间序列推理能力。实验表明，Chat-TS在保持强大自然语言能力的同时提升了时间序列推理，在多模态推理任务中实现了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of large language models in performing joint reasoning over time-series data and corresponding textual content, which is crucial for fields like healthcare and finance. The proposed Chat-TS framework integrates time-series tokens into the LLM vocabulary through a novel training strategy that preserves core natural language capabilities while enhancing multimodal reasoning. Experimental results demonstrate that Chat-TS achieves state-of-the-art performance in multimodal reasoning tasks, maintaining strong natural language proficiency alongside improved time-series reasoning, as validated on newly contributed datasets including instruction-tuning and evaluation sets.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型在结合时间序列数据与相应文本内容进行联合推理方面的局限性，这在医疗和金融等领域至关重要。提出的Chat-TS框架通过将时间序列标记集成到LLM词汇表中，采用一种新颖的训练策略，在保持核心自然语言能力的同时增强多模态推理。实验结果表明，Chat-TS在多模态推理任务中实现了最先进的性能，在保持强大自然语言熟练度的同时提升了时间序列推理能力，这在新贡献的数据集（包括指令调优对和多项选择评估集）上得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">Mantis: A Foundation Model for Mechanistic Disease Forecasting</div>
<div class="meta-line">Authors: Carson Dudley, Reiden Magdaleno, Christopher Harding, Ananya Sharma, Emily Martin, Marisa Eisenberg</div>
<div class="meta-line">First: 2025-08-17T06:55:29+00:00 · Latest: 2026-01-22T17:34:42+00:00</div>
<div class="meta-line">Comments: 11 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12260v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.12260v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Infectious disease forecasting in novel outbreaks or low-resource settings is hampered by the need for large disease and covariate data sets, bespoke training, and expert tuning, all of which can hinder rapid generation of forecasts for new settings. To help address these challenges, we developed Mantis, a foundation model trained entirely on mechanistic simulations, which enables out-of-the-box forecasting across diseases, regions, and outcomes, even in settings with limited historical data. We evaluated Mantis against 48 forecasting models across six diseases with diverse modes of transmission, assessing both point forecast accuracy (mean absolute error) and probabilistic performance (weighted interval score and coverage). Despite using no real-world data during training, Mantis achieved lower mean absolute error than all models in the CDC&#x27;s COVID-19 Forecast Hub when backtested on early pandemic forecasts which it had not previously seen. Across all other diseases tested, Mantis consistently ranked in the top two models across evaluation metrics. Mantis further generalized to diseases with transmission mechanisms not represented in its training data, demonstrating that it can capture fundamental contagion dynamics rather than memorizing disease-specific patterns. These capabilities illustrate that purely simulation-based foundation models such as Mantis can provide a practical foundation for disease forecasting: general-purpose, accurate, and deployable where traditional models struggle.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mantis：一种用于机制性疾病预测的基础模型</div>
<div class="mono" style="margin-top:8px">在新发疫情或资源匮乏环境中，传染病预测面临三大障碍：需要大量疾病与协变量数据集、定制化训练及专家调参，这些均会阻碍新场景下的快速预测。为应对这些挑战，我们开发了Mantis——一个完全基于机制性模拟训练的基础模型，能够实现跨疾病、跨区域、跨预测指标的即用型预测，即使在历史数据有限的环境中也能运行。我们针对六种传播模式各异的疾病，将Mantis与48个预测模型进行比较，评估了点预测准确度（平均绝对误差）和概率预测性能（加权区间评分与覆盖度）。尽管训练过程中未使用任何真实世界数据，在对未见过的早期疫情预测进行回溯测试时，Mantis的平均绝对误差低于美国疾控中心COVID-19预测中心的所有模型。在所有其他测试疾病中，Mantis在各项评估指标中均稳居前两名。该模型进一步泛化至训练数据未涵盖传播机制的疾病，证明其能捕捉基础传染动力学而非记忆疾病特定模式。这些能力表明，纯基于模拟的基础模型（如Mantis）可为疾病预测提供实用基础：在传统模型难以应对的场景中实现通用、准确且可部署的预测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of rapid infectious disease forecasting in novel outbreaks or low-resource settings, where traditional models require extensive data and expert tuning. The authors developed Mantis, a foundation model trained entirely on mechanistic simulations, enabling out-of-the-box forecasting across diseases, regions, and outcomes without real-world training data. When evaluated against 48 models across six diseases, Mantis achieved lower mean absolute error than all models in the CDC&#x27;s COVID-19 Forecast Hub on early pandemic backtests and consistently ranked in the top two across other diseases, even generalizing to unseen transmission mechanisms.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决在新发疫情或资源有限环境中快速进行传染病预测的挑战，传统模型在这些场景下需要大量真实世界数据和专家调优。作者开发了Mantis这一基础模型，其训练完全基于机制模拟，从而能够跨疾病、区域和结果生成预测，无需为每个新应用收集真实训练数据。在针对六种疾病的48个模型评估中，Mantis在早期疫情回溯测试中的平均绝对误差低于CDC COVID-19预测中心的所有模型，并在其他疾病预测中持续排名前两位，甚至能泛化到训练数据中未包含传播机制的疾病。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Watermark in the Latent Space of Generative Models</div>
<div class="meta-line">Authors: Sylvestre-Alvise Rebuffi, Tuan Tran, Valeriu Lacatusu, Pierre Fernandez, Tomáš Souček, Nikola Jovanović, Tom Sander, Hady Elsahar, Alexandre Mourachko</div>
<div class="meta-line">First: 2026-01-22T17:34:30+00:00 · Latest: 2026-01-22T17:34:30+00:00</div>
<div class="meta-line">Comments: Code and models are available at https://github.com/facebookresearch/distseal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16140v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16140v1">PDF</a> · <a href="https://github.com/facebookresearch/distseal">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing approaches for watermarking AI-generated images often rely on post-hoc methods applied in pixel space, introducing computational overhead and potential visual artifacts. In this work, we explore latent space watermarking and introduce DistSeal, a unified approach for latent watermarking that works across both diffusion and autoregressive models. Our approach works by training post-hoc watermarking models in the latent space of generative models. We demonstrate that these latent watermarkers can be effectively distilled either into the generative model itself or into the latent decoder, enabling in-model watermarking. The resulting latent watermarks achieve competitive robustness while offering similar imperceptibility and up to 20x speedup compared to pixel-space baselines. Our experiments further reveal that distilling latent watermarkers outperforms distilling pixel-space ones, providing a solution that is both more efficient and more robust.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在生成模型潜在空间中的水印嵌入学习</div>
<div class="mono" style="margin-top:8px">现有AI生成图像水印方法多采用像素空间的后处理技术，存在计算开销大且可能引入视觉伪影的问题。本研究探索潜在空间水印技术，提出跨扩散模型与自回归模型的统一潜在水印框架DistSeal。该方法通过在生成模型潜在空间中训练后处理水印模型，证明潜在水印器可有效蒸馏至生成模型或潜在解码器中，实现模型内嵌水印。实验表明：相较于像素空间基线方法，所得潜在水印在保持竞争性鲁棒性的同时，具有相近的不可感知性，且速度提升最高达20倍。进一步研究发现，蒸馏潜在水印器的效果优于蒸馏像素空间水印器，提供了更高效、更鲁棒的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational overhead and visual artifacts of pixel-space watermarking for AI-generated images, this work proposes DistSeal, a latent space watermarking method applicable to diffusion and autoregressive models. The approach trains post-hoc watermarking models in the latent space of generative models, which can then be distilled into the generative model or its latent decoder for in-model watermarking. Experiments show that this method achieves competitive robustness and similar imperceptibility while offering up to 20x speedup over pixel-space baselines, with distillation of latent watermarkers proving more efficient and robust than distilling pixel-space ones.</div>
<div class="mono" style="margin-top:8px">为解决AI生成图像在像素空间水印方法存在的计算开销和视觉伪影问题，本研究提出了DistSeal，一种在扩散和自回归模型的潜在空间中直接嵌入水印的方法。该方法通过在潜在空间训练后处理水印模型，并将其蒸馏到生成模型或其解码器中，从而实现高效的内置水印。实验结果表明，所产生的水印具有鲁棒性和不可感知性，相比像素空间基线实现了高达20倍的加速，且潜在空间蒸馏比像素空间蒸馏方法更有效、更鲁棒。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Prompt Evaluation for Educational Applications</div>
<div class="meta-line">Authors: Langdon Holmes, Adam Coscia, Scott Crossley, Joon Suh Choi, Wesley Morris</div>
<div class="meta-line">First: 2026-01-22T17:31:25+00:00 · Latest: 2026-01-22T17:31:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16134v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16134v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become increasingly common in educational applications, there is a growing need for evidence-based methods to design and evaluate LLM prompts that produce personalized and pedagogically aligned out-puts. This study presents a generalizable, systematic approach for evaluating prompts, demonstrated through an analysis of LLM-generated follow-up questions in a structured dialogue activity. Six prompt templates were designed and tested. The templates incorporated established prompt engineering patterns, with each prompt emphasizing distinct pedagogical strategies. The prompt templates were compared through a tournament-style evaluation framework that can be adapted for other educational applications. The tournament employed the Glicko2 rating system with eight judges evaluating question pairs across three dimensions: format, dialogue support, and appropriateness for learners. Data was sourced from 120 authentic user interactions across three distinct educational deployments. Results showed that a single prompt related to strategic reading out-performed other templates with win probabilities ranging from 81% to 100% in pairwise comparisons. This prompt combined persona and context manager pat-terns and was designed to support metacognitive learning strategies such as self-directed learning. The methodology showcases how educational technology re- searchers can systematically evaluate and improve prompt designs, moving beyond ad-hoc prompt engineering toward evidence-based prompt development for educational applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>教育应用中的大语言模型提示评估</div>
<div class="mono" style="margin-top:8px">随着大语言模型在教育应用中日渐普及，亟需基于实证的方法来设计和评估能产生个性化且符合教学目标的输出提示。本研究提出一种可推广的系统化提示评估方法，通过对结构化对话活动中LLM生成的后续问题进行分析来验证。研究设计并测试了六种提示模板，这些模板融合了成熟的提示工程模式，每种提示侧重不同的教学策略。通过适用于其他教育场景的锦标赛式评估框架对提示模板进行比较，该锦标赛采用Glicko2评分系统，由八位评委从格式、对话支持度、学习者适配性三个维度对问题组进行评价。数据来源于三个独立教育场景中的120组真实用户交互记录。结果显示，涉及策略性阅读的单一提示模板在成对比较中以81%至100%的胜率优于其他模板。该提示融合了角色设定与语境管理模块，旨在支持元认知学习策略（如自主学习）。本方法为教育技术研究者展示了如何系统评估并改进提示设计，推动教育应用从临时性提示工程向基于实证的提示开发范式转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing use of large language models (LLMs) in education and the need for evidence-based prompt design, this study develops a generalizable, systematic method for evaluating LLM prompts. The method involves designing six prompt templates that incorporate established engineering patterns to emphasize distinct pedagogical strategies, and comparing them using a tournament-style evaluation framework with the Glicko2 rating system, where eight judges assessed outputs from 120 authentic user interactions across three dimensions: format, dialogue support, and learner appropriateness. Key experimental findings reveal that a single prompt template focused on strategic reading, which combined persona and context manager patterns to support metacognitive strategies, consistently outperformed others, achieving win probabilities between 81% and 100% in pairwise comparisons.</div>
<div class="mono" style="margin-top:8px">本研究动机源于大型语言模型在教育中的应用日益普遍，以及需要基于证据的提示设计方法。研究者提出了一种系统化、可推广的提示评估方法，设计了六个融入不同教学策略和成熟工程模式的提示模板，并通过采用Glicko2评分系统的锦标赛式评估框架进行比较，由八位评委基于120个真实用户交互数据，从格式、对话支持和学习者适宜性三个维度评估模型生成的后续问题。主要实验结果表明，一个结合角色扮演和上下文管理器模式、旨在支持元认知策略（如自主学习）的提示模板表现最优，在成对比较中胜出概率达81%至100%。</div>
</details>
</div>
<div class="card">
<div class="title">ViSymRe: Vision-guided Multimodal Symbolic Regression</div>
<div class="meta-line">Authors: Da Li, Junping Yin, Jin Xu, Xinxin Li, Juan Zhang</div>
<div class="meta-line">First: 2024-12-15T10:05:31+00:00 · Latest: 2026-01-22T17:29:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11139v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.11139v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSymRe：视觉引导的多模态符号回归</div>
<div class="mono" style="margin-top:8px">从观测数据集中提取简单数学表达式以描述复杂自然现象是人工智能（AI）的核心目标之一，该领域称为符号回归（SR）。传统SR模型基于遗传编程（GP）或强化学习（RL），面临效率低、过拟合等挑战。近期研究将SR与大型语言模型（LLMs）结合，通过从数百万数据集-表达式对中学习映射实现快速零样本推理。然而，由于输入与输出本质属于不同模态，此类模型常难以有效收敛。本文提出ViSymRe，一种视觉引导的多模态SR模型，引入第三种资源——表达式图——以弥合模态鸿沟。与传统多模态模型不同，ViSymRe训练从数据集中提取视觉信息（称为虚拟视觉），无需依赖表达式图的全局可用性，从而解决了视觉SR的关键挑战：推理时表达式图不可得。在多个主流基准上的评估结果表明，ViSymRe比当前最先进的仅使用数据集的基线模型具有更强竞争力。其预测的表达式不仅与数据高度拟合，且结构简洁准确，实现了SR模型追求的核心目标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency and overfitting issues in traditional symbolic regression (SR) methods like genetic programming and reinforcement learning, as well as the modality gap challenge in recent large language model-based SR approaches. The proposed ViSymRe model introduces a vision-guided multimodal framework that incorporates expression graphs as an intermediate modality to bridge the gap between input datasets and output mathematical expressions; it is trained to generate &#x27;virtual vision&#x27; from datasets without requiring expression graphs during inference. Experimental evaluations on multiple benchmarks demonstrate that ViSymRe outperforms state-of-the-art dataset-only baselines, producing expressions that are both accurate in fitting data and structurally simple.</div>
<div class="mono" style="margin-top:8px">该研究旨在改进从数据中提取数学表达式的符号回归（SR）方法，以解决遗传编程和强化学习方法效率低下，以及基于大语言模型（LLM）方法中存在的模态差距问题。提出的ViSymRe模型采用了一种视觉引导的多模态方法，在训练时利用表达式图作为中间资源来弥合输入数据集与输出表达式之间的模态差距，同时在推理时从数据集生成“虚拟视觉”，而无需实际的表达式图。在多个主流基准测试上的评估结果表明，ViSymRe的性能优于最先进的仅使用数据集的基线方法，其预测的表达式不仅与数据拟合良好，而且结构准确、形式简洁，达到了符号回归模型追求的目标。</div>
</details>
</div>
<div class="card">
<div class="title">Replicating Human Motivated Reasoning Studies with LLMs</div>
<div class="meta-line">Authors: Neeley Pate, Adiba Mahbub Proma, Hangfeng He, James N. Druckman, Daniel Molden, Gourab Ghoshal, Ehsan Hoque</div>
<div class="meta-line">First: 2026-01-22T17:29:07+00:00 · Latest: 2026-01-22T17:29:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16130v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motivated reasoning -- the idea that individuals processing information may be motivated to reach a certain conclusion, whether it be accurate or predetermined -- has been well-explored as a human phenomenon. However, it is unclear whether base LLMs mimic these motivational changes. Replicating 4 prior political motivated reasoning studies, we find that base LLM behavior does not align with expected human behavior. Furthermore, base LLM behavior across models shares some similarities, such as smaller standard deviations and inaccurate argument strength assessments. We emphasize the importance of these findings for researchers using LLMs to automate tasks such as survey data collection and argument assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型复现人类动机性推理研究</div>
<div class="mono" style="margin-top:8px">动机性推理——即个体在处理信息时可能受特定结论（无论其准确与否或是否预先确定）的驱动——作为人类心理现象已得到充分研究。然而，基础大语言模型是否会模仿这种动机性变化尚不明确。通过复现四项既有政治动机性推理研究，我们发现基础大语言模型的行为模式与预期的人类行为并不一致。此外，不同基础模型的行为表现出某些共性，例如标准差较小、论据强度评估失准等。我们强调这些发现对使用大语言模型自动化执行调查数据收集、论据评估等任务的研究者具有重要参考价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether base large language models (LLMs) exhibit motivated reasoning, a human tendency to process information in a way that favors predetermined conclusions. The researchers replicated four prior political motivated reasoning studies using base LLMs and compared the model outputs to expected human behavior. They found that base LLM behavior does not align with human motivated reasoning patterns, showing smaller standard deviations and inaccurate assessments of argument strength, which highlights critical implications for using LLMs to automate tasks like survey data collection and argument evaluation.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究基础大语言模型是否表现出动机性推理这一人类认知偏差，即个体为达成特定结论而处理信息的倾向。研究人员通过复现四项先前的政治动机性推理研究，使用基础大语言模型将其反应与已知的人类行为模式进行比较。实验结果表明，基础大语言模型的行为与预期的人类动机性推理并不一致；相反，模型表现出更小的标准差和对论证强度的不准确评估，这凸显了在自动化任务（如调查数据收集和论证评估）中使用大语言模型的显著局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Training Efficiency and Reducing Maintenance Costs via Language Specific Model Merging</div>
<div class="meta-line">Authors: Alphaeus Dmonte, Vidhi Gupta, Daniel J Perry, Mark Arehart</div>
<div class="meta-line">First: 2026-01-22T17:28:24+00:00 · Latest: 2026-01-22T17:28:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning a task-specific multilingual large language model (LLM) involves training the model on a multilingual dataset with examples in all the required languages. Updating one or more supported languages with additional data or adding support for a new language involves retraining the model, which can be computationally inefficient and creates a severe maintenance bottleneck. Recent research on merging multilingual multitask models has shown promise in terms of improved quality, but its computational and maintenance efficiency remains unstudied. In this work, we provide the first focused analysis of this merging strategy from an efficiency perspective, evaluating it across three independent tasks. We demonstrate significant efficiency gains while maintaining parity in terms of quality: this merging approach reduces the initial training time by up to 50\%. We also demonstrate that updating an individual language and re-merging as part of model maintenance reduces training costs by more than 60\%, compared to re-training the full multilingual model. We show this on both public and proprietary industry datasets confirming that the approach works well for industrial use cases in addition to academic settings already studied in previous work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语言特定模型融合提升训练效率并降低维护成本</div>
<div class="mono" style="margin-top:8px">针对特定任务的多语言大语言模型（LLM）进行微调时，需使用包含所有目标语言示例的多语言数据集进行训练。若需用额外数据更新一个或多个支持语言，或添加新语言支持，则需重新训练模型，这可能导致计算效率低下并形成严重的维护瓶颈。近期关于多语言多任务模型融合的研究在提升质量方面展现出潜力，但其计算与维护效率尚未得到充分研究。本文首次从效率角度对该融合策略进行聚焦分析，并在三个独立任务中评估其表现。研究表明，该方法在保持质量相当的同时实现了显著的效率提升：融合策略可将初始训练时间减少高达50%。此外，在模型维护过程中，更新单一语言后重新融合，相比重新训练完整多语言模型，能降低超过60%的训练成本。我们在公开数据集和行业专有数据集上均验证了该方法的有效性，证实其不仅适用于学术场景，也能很好地满足工业应用需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency and maintenance bottlenecks of retraining multilingual large language models when updating or adding languages, this work analyzes a language-specific model merging strategy from an efficiency perspective. The method involves training separate models for different languages and then merging them, evaluated across three independent tasks. Experimental results on public and proprietary datasets show this approach reduces initial training time by up to 50% and cuts training costs by over 60% for language updates while maintaining quality parity, confirming its effectiveness for industrial applications.</div>
<div class="mono" style="margin-top:8px">为解决多语言大语言模型在更新或添加语言时重新训练带来的计算效率低下和维护瓶颈问题，本研究从效率角度分析了一种语言特定模型合并策略。该方法涉及为不同语言训练独立模型后进行合并，并在三个独立任务上进行评估。在公开和专有行业数据集上的实验结果表明，该方法将初始训练时间减少了高达50%，并通过重新合并将语言更新的训练成本降低了60%以上，同时保持了与传统全模型重新训练相当的质量水平。</div>
</details>
</div>
<div class="card">
<div class="title">AudioMotionBench: Evaluating Auditory Motion Perception in Audio LLMs</div>
<div class="meta-line">Authors: Zhe Sun, Yujun Cai, Jiayu Yao, Yiwei Wang</div>
<div class="meta-line">First: 2025-11-17T11:45:41+00:00 · Latest: 2026-01-22T17:11:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13273v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13273v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Audio-Language Models (LALMs) have recently shown impressive progress in speech recognition, audio captioning, and auditory question answering. Yet, whether these models can perceive spatial dynamics, particularly the motion of sound sources, remains unclear. In this work, we uncover a systematic motion perception deficit in current ALLMs. To investigate this issue, we introduce AudioMotionBench, the first benchmark explicitly designed to evaluate auditory motion understanding. AudioMotionBench introduces a controlled question-answering benchmark designed to evaluate whether Audio-Language Models (LALMs) can infer the direction and trajectory of moving sound sources from binaural audio. Comprehensive quantitative and qualitative analyses reveal that current models struggle to reliably recognize motion cues or distinguish directional patterns. The average accuracy remains below 50\%, underscoring a fundamental limitation in auditory spatial reasoning. Our study highlights a fundamental gap between human and model auditory spatial reasoning, providing both a diagnostic tool and new insight for enhancing spatial cognition in future Audio-Language Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AudioMotionBench：评估音频大语言模型中的听觉运动感知能力</div>
<div class="mono" style="margin-top:8px">大型音频-语言模型近期在语音识别、音频描述和听觉问答方面取得显著进展，但这些模型能否感知空间动态（尤其是声源运动）仍不明确。本研究揭示了当前音频-语言模型存在系统性运动感知缺陷。为此，我们推出首个专门评估听觉运动理解的基准测试AudioMotionBench，该基准通过受控问答任务评估模型能否从双耳音频推断移动声源的方向与轨迹。综合定量与定性分析表明，现有模型难以可靠识别运动线索或区分方向模式，平均准确率低于50%，暴露出听觉空间推理的根本性局限。本研究揭示了人类与模型在听觉空间推理上的本质差距，为未来增强音频-语言模型的空间认知提供了诊断工具与新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current Large Audio-Language Models (LALMs) excel in tasks like speech recognition but their ability to perceive spatial dynamics, specifically the motion of sound sources, is unknown and potentially deficient. To systematically investigate this, the authors introduce AudioMotionBench, a controlled question-answering benchmark that uses binaural audio to evaluate models on inferring sound source direction and trajectory. Comprehensive analyses reveal that existing models struggle significantly, with average accuracy below 50%, failing to reliably recognize motion cues or distinguish directional patterns, which highlights a fundamental gap in auditory spatial reasoning compared to humans.</div>
<div class="mono" style="margin-top:8px">当前的大型音频语言模型在语音识别等任务上表现出色，但其感知空间动态（特别是声源运动）的能力尚不明确且可能存在缺陷。为系统评估该能力，研究者引入了AudioMotionBench，这是一个使用双耳音频、通过问答形式测试模型推断声源方向与轨迹能力的基准。综合定量与定性分析表明，现有模型难以可靠识别运动线索或区分方向模式，平均准确率低于50%，这揭示了模型在听觉空间推理方面相比人类能力存在根本性局限。</div>
</details>
</div>
<div class="card">
<div class="title">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-22T17:01:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayesianVLA：基于潜在动作查询的视觉语言动作模型贝叶斯分解</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但常难以泛化至新指令或复杂多任务场景。我们指出当前训练范式中存在一个关键缺陷：目标驱动的数据收集导致数据集偏差。在此类数据集中，仅凭视觉观测即可高度预测语言指令，致使指令与动作间的条件互信息趋近于零，这一现象我们称为“信息坍缩”。因此，模型退化为仅依赖视觉的策略，忽略语言约束并在分布外（OOD）场景中失效。为解决此问题，我们提出BayesianVLA，一种通过贝叶斯分解强制遵循指令的新框架。通过引入可学习的潜在动作查询，我们构建双分支架构以同时估计仅视觉先验$p(a \mid v)$和语言条件后验$π(a \mid v, \ell)$，进而优化策略以最大化动作与指令间的条件点互信息（PMI）。该目标有效惩罚视觉捷径，并奖励能显式解释语言命令的动作。无需新数据，BayesianVLA显著提升泛化能力。在SimplerEnv和RoboCasa上的大量实验证明其显著优势，包括在挑战性OOD基准SimplerEnv上提升11.3%，验证了本方法在动作中稳健扎根语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions are predictable from visual context, leading to Information Collapse where models ignore language and act as vision-only policies. To counteract this, the proposed BayesianVLA framework introduces learnable Latent Action Queries within a dual-branch architecture to separately model a vision-only action prior and a language-conditioned posterior, then optimizes the policy to maximize the conditional pointwise mutual information between actions and instructions, thereby penalizing the vision shortcut. Experiments on SimplerEnv and RoboCasa benchmarks show the method significantly improves out-of-distribution generalization without new data, achieving an 11.3% performance gain on a challenging OOD benchmark.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差导致泛化能力不足，其中语言指令可从视觉上下文预测，引发信息坍缩，使模型退化为忽略语言的纯视觉策略。为解决该问题，提出的BayesianVLA框架引入了可学习的潜在动作查询，通过双分支架构分别建模纯视觉动作先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息，从而惩罚视觉捷径。在SimplerEnv和RoboCasa基准上的实验表明，该方法无需新数据即可显著提升分布外泛化能力，在具有挑战性的OOD基准上实现了11.3%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal Climate Disinformation Detection: Integrating Vision-Language Models with External Knowledge Sources</div>
<div class="meta-line">Authors: Marzieh Adeli Shamsabad, Hamed Ghodrati</div>
<div class="meta-line">First: 2026-01-22T16:55:48+00:00 · Latest: 2026-01-22T16:55:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16108v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16108v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate disinformation has become a major challenge in today digital world, especially with the rise of misleading images and videos shared widely on social media. These false claims are often convincing and difficult to detect, which can delay actions on climate change. While vision-language models (VLMs) have been used to identify visual disinformation, they rely only on the knowledge available at the time of training. This limits their ability to reason about recent events or updates. The main goal of this paper is to overcome that limitation by combining VLMs with external knowledge. By retrieving up-to-date information such as reverse image results, online fact-checks, and trusted expert content, the system can better assess whether an image and its claim are accurate, misleading, false, or unverifiable. This approach improves the model ability to handle real-world climate disinformation and supports efforts to protect public understanding of science in a rapidly changing information landscape.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态气候虚假信息检测：融合视觉语言模型与外部知识源</div>
<div class="mono" style="margin-top:8px">气候虚假信息已成为当今数字世界的重大挑战，尤其是在社交媒体上广泛传播的误导性图像和视频日益增多的情况下。这些虚假主张往往具有说服力且难以识别，可能延缓应对气候变化的行动。虽然视觉语言模型（VLMs）已被用于识别视觉虚假信息，但它们仅依赖于训练时可获取的知识，这限制了其处理近期事件或更新的能力。本文的主要目标是通过将VLMs与外部知识相结合来克服这一局限。通过检索最新信息（如反向图像搜索结果、在线事实核查和可信专家内容），该系统能更准确地评估图像及其主张是否真实、具有误导性、虚假或无法验证。该方法提升了模型处理现实世界气候虚假信息的能力，并在快速变化的信息环境中助力维护公众对科学的理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting climate disinformation in multimodal content, which is often convincing and can hinder climate action, by overcoming the limitation of vision-language models (VLMs) that rely on static training knowledge. The proposed method integrates VLMs with external, up-to-date knowledge sources, such as reverse image search results, online fact-checks, and trusted expert content, to assess the veracity of images and associated claims. Experimental findings demonstrate that this approach enhances the model&#x27;s ability to accurately classify content as accurate, misleading, false, or unverifiable, thereby improving real-world disinformation detection and supporting public understanding of climate science.</div>
<div class="mono" style="margin-top:8px">本研究针对多模态气候虚假信息检测的挑战，这类信息往往具有说服力且可能阻碍气候行动，旨在克服视觉语言模型依赖静态训练知识的局限。方法上，通过整合外部最新知识源，如反向图像搜索结果、在线事实核查和可信专家内容，来评估图像及相关声明的真实性。实验结果表明，该方法能有效提升系统将内容准确分类为真实、误导、虚假或无法验证的能力，从而增强了现实世界虚假信息检测效果，并有助于维护公众对气候科学的理解。</div>
</details>
</div>
<div class="card">
<div class="title">TDFlow: Agentic Workflows for Test Driven Development</div>
<div class="meta-line">Authors: Kevin Han, Siddharth Maddikayala, Tim Knappe, Om Patel, Austen Liao, Amir Barati Farimani</div>
<div class="meta-line">First: 2025-10-27T18:44:59+00:00 · Latest: 2026-01-22T16:50:52+00:00</div>
<div class="meta-line">Comments: Published in the 19th Conference of the European Chapter of the Association for Computational Linguistics (EACL 2026 Main Conference)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23761v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23761v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce TDFlow, a novel test-driven agentic workflow that frames repository-scale software engineering as a test-resolution task, specifically designed to solve human-written tests. Given a set of tests, TDFlow repeatedly proposes, revises, and debugs repository-scale patches using precisely engineered sub-agents and tightly constrained tools. The workflow decomposes software engineering program repair into four components governed by respective sub-agents. This simple, forced decoupling of patch proposing, debugging, patch revision, and optional test generation (1) reduces long-context burden on any individual sub-agent, (2) focuses each sub-agent on specific, pre-defined sub-tasks, and (3) allows for specialized performance improvement on specific sub-tasks. When provided human-written tests, TDFlow attains 88.8% pass rate on SWE-Bench Lite (an absolute improvement of 27.8% over the next best system) and 94.3% on SWE-Bench Verified. Manual inspection of the 800 TDFlow runs within SWE-Bench Lite and Verified uncover only 7 instances of test hacking, which were subsequently counted as failures. Furthermore, we show that the primary obstacle to human-level software engineering performance lies within writing successful reproduction tests. We envision a human-LLM interactive system powered by TDFlow where human developers write tests solved by LLM systems. Together, these results indicate that modern LLMs, when embedded in a narrowly engineered, test-driven workflow, already achieve human-level test resolution -- with the final frontier for fully autonomous repository repair being the accurate generation of valid reproduction tests.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TDFlow：面向测试驱动开发的智能体工作流</div>
<div class="mono" style="margin-top:8px">本文提出TDFlow——一种新颖的测试驱动智能体工作流，将仓库级软件工程构建为测试解析任务，专门用于解决人工编写的测试用例。给定测试集后，TDFlow通过精密设计的子智能体与严格约束的工具，持续提出、修订和调试仓库级代码补丁。该工作流将软件工程程序修复分解为四个由对应子智能体管控的组件。这种对补丁提出、调试、补丁修订及可选测试生成的强制解耦设计具有三重优势：（1）减轻单个子智能体的长上下文负担；（2）使各子智能体专注于预定义的特定子任务；（3）支持针对特定子任务的专项性能提升。在人工编写测试的场景下，TDFlow在SWE-Bench Lite上达到88.8%通过率（较次优系统绝对提升27.8%），在SWE-Bench Verified上达到94.3%。对SWE-Bench Lite与Verified中800次TDFlow运行的手动检查仅发现7例测试篡改行为（已计入失败案例）。研究进一步表明，实现人类级软件工程性能的主要障碍在于编写有效的复现测试。我们展望基于TDFlow构建人机交互系统，由开发者编写测试而LLM系统负责解决。综合结果表明：现代大语言模型嵌入精心设计的测试驱动工作流后，已能实现人类级测试解析能力——完全自主仓库修复的最终瓶颈在于准确生成有效的复现测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of automating repository-scale software engineering by framing it as a test-resolution task, motivated by the need to effectively leverage large language models (LLMs) for complex program repair. The method introduces TDFlow, an agentic workflow that decomposes the repair process into four distinct sub-tasks—patch proposal, debugging, revision, and optional test generation—each handled by specialized sub-agents with tightly constrained tools to reduce long-context burdens and improve focus. Experimental results on SWE-Bench Lite show TDFlow achieves an 88.8% pass rate with human-written tests, a 27.8% absolute improvement over the next best system, and only 7 instances of test hacking were found in 800 runs, indicating robust performance. The findings suggest that modern LLMs in this engineered workflow can achieve human-level test resolution, with the primary remaining obstacle being the accurate generation of valid reproduction tests.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过将仓库级软件工程构建为测试解决任务，以应对自动化复杂程序修复的挑战，其动机是有效利用大语言模型（LLM）进行代码维护。方法上提出了TDFlow，一种智能体工作流，它将修复过程分解为四个子任务——补丁提议、调试、修订和可选的测试生成——每个子任务由专门设计的子智能体处理，并配合严格约束的工具，以减轻长上下文负担并提升专注度。在SWE-Bench上的实验结果表明，TDFlow在Lite数据集上达到了88.8%的通过率（相比次优系统绝对提升27.8%），在Verified数据集上达到94.3%，且测试篡改情况极少，这表明当提供人工编写的测试时，现代LLM在此工程化工作流中已能实现人类水平的测试解决能力。</div>
</details>
</div>
<div class="card">
<div class="title">Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals</div>
<div class="meta-line">Authors: Saar Cohen</div>
<div class="meta-line">First: 2026-01-22T16:42:05+00:00 · Latest: 2026-01-22T16:42:05+00:00</div>
<div class="meta-line">Comments: To Appear in the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16091v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16091v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point&#x27;s location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points&#x27; locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机到达在线非质心聚类中的延迟分配</div>
<div class="mono" style="margin-top:8px">聚类是一个基础性问题，旨在将一组元素（如智能体或数据点）划分为若干簇，使得同一簇内元素间的距离小于与其他簇元素间的距离。本文提出一种研究带延迟的在线非质心聚类的新框架：元素作为有限度量空间中的点逐个到达，需被分配至簇中，但分配不必立即执行。具体而言，每个点到达时其位置被揭示，在线算法必须不可撤销地将其分配至现有簇，或创建仅包含该点的新簇。然而，我们允许以延迟成本为代价推迟决策，而非遵循常见的到达即决策假设。这带来关键挑战：目标是最小化各簇内点间距离成本与延迟分配产生的总延迟成本。在经典最坏情况到达模型（点以任意顺序到达）中，任何算法的竞争比均无法优于点数量的亚对数级别。为突破这一强不可能性，我们聚焦于随机到达模型——点的位置随时间独立地从有限度量空间的未知固定概率分布中抽取。我们为超越最坏情况对抗提供了希望：设计出一种常数竞争算法，即随着点数增长，输出聚类与最优离线聚类的期望总成本之比受常数约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of online non-centroid clustering where immediate assignment of arriving points is not required, allowing for delayed decisions at a cost, which aims to balance clustering quality with delay penalties. The method introduces a stochastic arrival model where points are drawn independently from an unknown distribution over a finite metric space, and it devises an algorithm that makes irrevocable assignment decisions, possibly after a delay, to minimize the sum of intra-cluster distance costs and delay costs. Key experimental findings show that, in contrast to the strong impossibility results under worst-case arbitrary arrivals, the proposed algorithm achieves a constant competitive ratio, meaning the expected cost of its clustering is within a constant factor of the optimal offline clustering as the number of points increases.</div>
<div class="mono" style="margin-top:8px">本研究针对在线非质心聚类中无需立即分配到达点、允许以成本为代价延迟决策的挑战，旨在平衡聚类质量与延迟惩罚。方法引入了随机到达模型，其中点从有限度量空间上的未知分布中独立抽取，并设计了一种算法进行不可撤销的分配决策，允许推迟分配。关键实验结果表明，与最坏情况到达下的较差竞争比相比，该算法实现了常数竞争比，即随着点数增加，其聚类期望总成本被限制在最优离线成本的常数倍内。</div>
</details>
</div>
<div class="card">
<div class="title">Controlling Long-Horizon Behavior in Language Model Agents with Explicit State Dynamics</div>
<div class="meta-line">Authors: Sukesh Subaharan</div>
<div class="meta-line">First: 2026-01-22T16:34:05+00:00 · Latest: 2026-01-22T16:34:05+00:00</div>
<div class="meta-line">Comments: Supplementary materials can be found here: https://github.com/drsukeshs/agent-behavior-ext-dynamics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16087v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16087v1">PDF</a> · <a href="https://github.com/drsukeshs/agent-behavior-ext-dynamics">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) agents often exhibit abrupt shifts in tone and persona during extended interaction, reflecting the absence of explicit temporal structure governing agent-level state. While prior work emphasizes turn-local sentiment or static emotion classification, the role of explicit affective dynamics in shaping long-horizon agent behavior remains underexplored. This work investigates whether imposing dynamical structure on an external affective state can induce temporal coherence and controlled recovery in multi-turn dialogue. We introduce an agent-level affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state external to the language model and governed by first- and second-order update rules. Instantaneous affective signals are extracted using a fixed, memoryless estimator and integrated over time via exponential smoothing or momentum-based dynamics. The resulting affective state is injected back into generation without modifying model parameters. Using a fixed 25-turn dialogue protocol, we compare stateless, first-order, and second-order affective dynamics. Stateless agents fail to exhibit coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery. Second-order dynamics introduce affective inertia and hysteresis that increase with momentum, revealing a trade-off between stability and responsiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过显式状态动态控制语言模型智能体的长时程行为</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）智能体在长时间交互中常出现语气和人设的突变，这反映了缺乏管理智能体层面状态的显式时间结构。现有研究多关注轮次局部情感或静态情绪分类，而显式情感动态在塑造长时程智能体行为中的作用尚未充分探索。本研究探讨对外部情感状态施加动态结构能否在多轮对话中引发时间连贯性和受控恢复。我们引入一个智能体层面的情感子系统，该子系统在语言模型外部维持连续的效价-唤醒-支配（VAD）状态，并受一阶和二阶更新规则控制。瞬时情感信号通过固定的无记忆估计器提取，并通过指数平滑或基于动量的动态机制随时间整合。生成过程中注入最终情感状态而不修改模型参数。采用固定的25轮对话协议，我们比较了无状态、一阶和二阶情感动态机制。无状态智能体无法展现连贯轨迹或恢复能力，而状态持续性支持延迟响应和可靠恢复。二阶动态机制引入的情感惯性和滞后效应随动量增强，揭示了稳定性与响应性之间的权衡关系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the issue of abrupt shifts in tone and persona exhibited by large language model (LLM) agents during extended interactions, which stems from a lack of explicit temporal structure governing the agent&#x27;s state. The method introduces an external affective subsystem that maintains a continuous Valence-Arousal-Dominance (VAD) state, updated via first- or second-order dynamics and integrated using techniques like exponential smoothing, with the state then injected into the generation process without modifying the LLM&#x27;s parameters. Experimental results from a 25-turn dialogue protocol show that stateless agents fail to achieve coherent trajectories or recovery, while state persistence enables delayed responses and reliable recovery, and second-order dynamics introduce affective inertia and hysteresis, revealing a trade-off between stability and responsiveness.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLM）智能体在长时间交互中出现的语气和角色突然转变问题，该问题源于智能体层面状态缺乏明确的时间结构。方法上引入了一个外部情感子系统，用于维护一个连续的效价-唤醒-支配（VAD）状态，该状态通过一阶或二阶动态规则进行更新，并利用指数平滑等技术随时间整合，然后将该状态注入生成过程而不修改LLM参数。在一个固定的25轮对话协议上的实验结果表明，无状态智能体无法实现连贯的行为轨迹或恢复，而状态持续性则能实现延迟响应和可靠的恢复，二阶动态则引入了情感惯性和滞后效应，揭示了稳定性与响应性之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Probably Approximately Correct Maximum A Posteriori Inference</div>
<div class="meta-line">Authors: Matthew Shorvon, Frederik Mallmann-Trenn, David S. Watson</div>
<div class="meta-line">First: 2026-01-22T16:28:01+00:00 · Latest: 2026-01-22T16:28:01+00:00</div>
<div class="meta-line">Comments: 7 pages main text, 16 total, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>概率近似正确最大后验推断</div>
<div class="mono" style="margin-top:8px">计算分布的条件众数，即通常所说的$\mathit{最大后验}$（MAP）赋值，是概率推断中的一项基础任务。然而，MAP估计通常难以精确求解，即使在许多常见的结构约束和近似方案下仍然困难。我们提出了用于MAP推断的$\mathit{概率近似正确}$（PAC）算法，该算法在可变和固定计算预算下可提供理论证明的最优解。我们利用可从有限样本中估计的信息论度量，刻画了PAC-MAP的可处理性条件。我们的PAC-MAP求解器通过具有适当架构的概率电路高效实现。所开发的随机化策略既可作为独立的MAP推断技术使用，也可用于改进常用启发式方法，为其解提供严格的理论保证。实验在一系列基准测试中验证了本方法的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the intractability of maximum a posteriori (MAP) inference, a core problem in probabilistic reasoning that remains difficult even with common approximations. The method introduces probably approximately correct (PAC) algorithms for MAP inference, which guarantee optimal solutions within defined computational budgets, leveraging tractability conditions characterized by information-theoretic measures estimated from data. These solvers are efficiently implemented using probabilistic circuits with specific architectures. Experimental results across various benchmarks demonstrate that the proposed techniques can either serve as standalone inference methods or enhance existing heuristics by providing rigorous performance guarantees.</div>
<div class="mono" style="margin-top:8px">本研究针对最大后验概率推断这一概率推理核心问题的难解性展开，该问题即使在常见近似方案下依然困难。方法引入了近似正确概率的最大后验概率推断算法，该算法能在设定的计算预算内保证最优解，并利用从数据估计的信息论度量来刻画可处理性条件。这些求解器通过具有适当架构的概率电路高效实现。在多个基准测试中的实验结果表明，所提技术可作为独立的推断方法使用，或通过提供严格性能保证来改进现有启发式方法。</div>
</details>
</div>
<div class="card">
<div class="title">Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface</div>
<div class="meta-line">Authors: Paige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy, Gidey Gezae, Dante Conway, Abraham Glasser, Raja Kushalnagar, Christian Vogler</div>
<div class="meta-line">First: 2026-01-21T17:33:00+00:00 · Latest: 2026-01-22T16:01:25+00:00</div>
<div class="meta-line">Comments: Accepted for publication in ACM CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15209v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15209v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa&#x27;s automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered &quot;task prompter,&quot; which integrated the user&#x27;s history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>听障人士对智能个人助理的访问：语音交互方案与基于大语言模型的触控界面对比</div>
<div class="mono" style="margin-top:8px">本研究探讨了在日常交流中能够使用语音的聋哑及听障人士对智能个人助理的可访问性。由于智能个人助理无法理解包括聋人口音在内的多样化口音，导致非手语使用且依赖口语的听障群体难以有效使用。通过混合方法研究，我们利用Echo Show设备，比较了以下三种交互方式的可用性：基于英语口语的自然语言输入配合Alexa自动语音识别、由训练有素的协助者复述指令的“绿野仙踪”模拟设置，以及基于大语言模型的触控界面。触控方法通过一个由大语言模型驱动的“任务提示器”进行导航，该提示器整合用户历史与智能环境信息以推荐情境适配的指令。定量结果显示，两种英语语音条件与基于大语言模型的触控方式之间无显著差异；定性结果则显示用户对不同方法的可用性评价存在差异。最终，实现智能个人助理原生支持对聋人口音的鲁棒识别至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the accessibility gap in intelligent personal assistants (IPAs) for deaf and hard of hearing (DHH) individuals who use speech, as standard IPAs often fail to recognize diverse accents including deaf speech. The study compared three input methods on an Echo Show: standard voice input using Alexa&#x27;s automatic speech recognition, a Wizard-of-Oz condition where a facilitator re-spoke user commands, and a novel touch interface powered by a large language model (LLM) that suggested contextually appropriate commands based on user history and environment. Quantitative results found no significant performance differences between the two spoken conditions and the LLM-assisted touch method, while qualitative feedback revealed varied user preferences, underscoring the need for IPAs to natively and robustly recognize deaf-accented speech.</div>
<div class="mono" style="margin-top:8px">本研究针对使用口语的聋人和听力障碍人士在智能个人助理（IPA）中面临的可访问性差距，因为标准IPA通常无法理解包括聋人口音在内的多样化口音。该研究比较了基于语音的输入（使用Alexa的自动语音识别和由协调员复述命令的Wizard-of-Oz设置）与一种由大型语言模型（LLM）驱动的新型触摸界面的可用性，该界面根据用户历史和环境提供情境化命令建议。混合方法研究的实验结果表明，在英语语音条件和LLM辅助触摸方法之间，可用性没有显著的定量差异，但定性反馈揭示了用户偏好的多样性，强调了IPA需要原生且稳健地识别聋人口音的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Words to Describe What I&#x27;m Feeling: Exploring the Potential of AI Agents for High Subjectivity Decisions in Advance Care Planning</div>
<div class="meta-line">Authors: Kellie Yu Hui Sim, Pin Sym Foong, Chenyu Zhao, Melanie Yi Ning Quek, Swarangi Subodh Mehta, Kenny Tsu Wei Choo</div>
<div class="meta-line">First: 2025-12-12T04:39:34+00:00 · Latest: 2026-01-22T15:58:47+00:00</div>
<div class="meta-line">Comments: Accepted at CHI 2026. 34 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11276v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11276v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Loss of decisional capacity, coupled with the increasing absence of reliable human proxies, raises urgent questions about how individuals&#x27; values can be represented in Advance Care Planning (ACP). To probe this fraught design space of high-risk, high-subjectivity decision support, we built an experience prototype (\acpagent{}) and asked 15 participants in 4 workshops to train it to be their personal ACP proxy. We analysed their coping strategies and feature requests and mapped the results onto axes of agent autonomy and human control. Our findings show a surprising 86.7\% agreement with \acpagent{}, arguing for a potential new role of AI in ACP where agents act as personal advocates for individuals, building mutual intelligibility over time. We propose that the key areas of future risk that must be addressed are the moderation of users&#x27; expectations and designing accountability and oversight over agent deployment and cutoffs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>描述我内心感受的词汇：探索人工智能代理在预先护理规划中高主观性决策的潜力</div>
<div class="mono" style="margin-top:8px">决策能力的丧失，加上可靠人类代理日益稀缺，引发了关于个人价值观如何在预先护理规划中得以体现的紧迫问题。为探索这一高风险、高主观性的决策支持设计领域，我们构建了一个体验原型（ACP代理），并邀请4场工作坊的15名参与者训练其成为个人ACP代理。通过分析参与者的应对策略与功能需求，我们将结果映射至代理自主性与人类控制度的维度。研究发现，参与者对ACP代理的认同度高达86.7%，这揭示了人工智能在ACP中可能扮演的新角色——作为个人权益倡导者，通过长期互动建立双向理解。我们提出未来需重点应对的风险领域包括：调节用户预期，以及设计代理部署、终止机制的问责与监督体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of representing individuals&#x27; values in Advance Care Planning when decisional capacity is lost and reliable human proxies are unavailable. The authors developed an AI agent experience prototype (ACPagent) and conducted four workshops with 15 participants to explore how they would train it to act as a personal proxy for high-subjectivity, high-risk decisions. Analysis of coping strategies and feature requests revealed a surprising 86.7% agreement rate with the agent&#x27;s decisions, suggesting AI could serve as personal advocates that build mutual understanding over time, though future work must address expectation moderation and accountability mechanisms.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在个人丧失决策能力且缺乏可靠人类代理时，如何在预先护理计划中代表其价值观的挑战。作者开发了一个名为acpagent的体验原型，并组织了四场工作坊，让15名参与者训练其作为个人预先护理计划代理，同时分析了参与者的应对策略和功能需求，并将其映射到代理自主性与人类控制的维度上。关键的实验结果是参与者对代理决策的同意率高达86.7%，这表明人工智能可能扮演个人倡导者的新角色，通过时间建立相互理解，但未来的工作必须解决如何调节用户期望以及设计代理部署和终止的问责与监督机制。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
