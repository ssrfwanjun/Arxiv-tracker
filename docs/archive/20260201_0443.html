<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-01 04:43</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260201_0443</div>
    <div class="row"><div class="card">
<div class="title">Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</div>
<div class="meta-line">Authors: Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy</div>
<div class="meta-line">First: 2026-01-29T18:59:24+00:00 · Latest: 2026-01-29T18:59:24+00:00</div>
<div class="meta-line">Comments: 26 pages, 31 figures, 13 tables. Project Page: https://sites.google.com/view/vi-probe/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/vi-probe/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) often answer classic visual illusions &quot;correctly&quot; on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型是感知还是回忆？基于经典视觉错觉的视觉感知与记忆机制探究</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）在处理原始经典视觉错觉图像时往往能给出“正确”答案，但当错觉要素被反转后（这种视觉变化对人类显而易见），模型却依然保持原有回答。这引发了一个根本性问题：VLMs究竟是在感知视觉变化，还是仅依赖记忆模式进行回忆？尽管已有研究注意到此现象，但其内在成因尚不明确。为从现象观察转向系统化理解，本文提出VI-Probe——一个具备分级扰动与匹配视觉对照（不含错觉诱导因素）的可控视觉错觉框架，旨在分离基于视觉的感知与语言驱动的回忆机制。与以往仅关注平均准确率的研究不同，我们通过极性翻转一致性、模板固化指数及经匹配对照标准化的错觉乘数，综合衡量模型的稳定性与敏感性。跨不同模型系列的实验表明，回答的持续性源于异质性成因而非单一机制。例如，GPT-5表现出记忆覆盖特性，Claude-Opus-4.1呈现感知-记忆竞争模式，而Qwen系列则暗示其存在视觉处理局限。本研究挑战了单一成因论，并倡导采用基于探测的评估方法，同时衡量模型的知识储备与对受控视觉变化的敏感性。数据与代码公开于：https://sites.google.com/view/vi-probe/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether Vision-Language Models (VLMs) genuinely perceive visual content or primarily rely on recalling memorized patterns, motivated by their tendency to give consistent answers to classic visual illusions even when the illusion-inducing elements are inverted—a change obvious to humans. The authors introduce VI-Probe, a controlled framework that applies graded perturbations to illusion images and uses matched visual controls (without illusion inducers) to disentangle visually grounded perception from language-driven recall. Instead of relying on average accuracy, they employ metrics like Polarity-Flip Consistency and an illusion multiplier normalized against controls. Experiments across multiple model families reveal that response persistence stems from heterogeneous mechanisms: for example, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, and Qwen variants indicate visual-processing limitations, challenging the view of a single underlying cause.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究大型视觉语言模型（VLMs）是真正感知视觉内容还是主要依赖记忆模式进行回忆，其动机在于这些模型即使在视觉错觉诱导因素被明显反转的情况下，仍倾向于对经典视觉错觉给出一致答案。作者提出了VI-Probe，一个通过分级扰动错觉图像并使用匹配视觉控制来分离视觉感知与语言驱动回忆的受控框架，并采用了极性翻转一致性和针对控制组标准化的错觉乘数等度量指标。在不同模型系列上的实验结果表明，回答的持续性源于异质机制：例如，GPT-5表现出记忆覆盖，Claude-Opus-4.1显示出感知-记忆竞争，而Qwen变体则暗示了视觉处理限制，这挑战了单一原因的观点。</div>
</details>
</div>
<div class="card">
<div class="title">SINA: A Circuit Schematic Image-to-Netlist Generator Using Artificial Intelligence</div>
<div class="meta-line">Authors: Saoud Aldowaish, Yashwanth Karumanchi, Kai-Chen Chiang, Soroosh Noorzad, Morteza Fayazi</div>
<div class="meta-line">First: 2026-01-29T18:41:52+00:00 · Latest: 2026-01-29T18:41:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22114v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22114v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current methods for converting circuit schematic images into machine-readable netlists struggle with component recognition and connectivity inference. In this paper, we present SINA, an open-source, fully automated circuit schematic image-to-netlist generator. SINA integrates deep learning for accurate component detection, Connected-Component Labeling (CCL) for precise connectivity extraction, and Optical Character Recognition (OCR) for component reference designator retrieval, while employing a Vision-Language Model (VLM) for reliable reference designator assignments. In our experiments, SINA achieves 96.47% overall netlist-generation accuracy, which is 2.72x higher than state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SINA：基于人工智能的电路原理图图像至网表生成器</div>
<div class="mono" style="margin-top:8px">现有电路原理图图像转机器可读网表的方法在元件识别与连接关系推断方面存在困难。本文提出SINA——一种开源、全自动的电路原理图图像至网表生成器。该系统集成深度学习实现精确元件检测，采用连通域标记技术提取准确连接关系，结合光学字符识别获取元件参考标识符，并运用视觉语言模型实现可靠的参考标识符分配。实验表明，SINA的网表生成总体准确率达96.47%，较现有最优方法提升2.72倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing methods in accurately recognizing components and inferring connectivity from circuit schematic images, this paper introduces SINA, an open-source automated generator that combines deep learning for component detection, Connected-Component Labeling for connectivity extraction, Optical Character Recognition for retrieving reference designators, and a Vision-Language Model for assigning them reliably. Experimental results demonstrate that SINA achieves an overall netlist-generation accuracy of 96.47%, outperforming state-of-the-art approaches by a factor of 2.72.</div>
<div class="mono" style="margin-top:8px">针对现有方法在电路原理图图像中准确识别元件和推断连接性方面的不足，本文提出了SINA，一种开源的自动化生成器，可将此类图像转换为网表。该方法结合了深度学习用于元件检测、连通域标记用于连接性提取、光学字符识别用于获取元件参考标识符，并利用视觉语言模型可靠地分配这些标识符。实验结果表明，SINA实现了96.47%的整体网表生成准确率，比现有最先进方法高出2.72倍。</div>
</details>
</div>
<div class="card">
<div class="title">Causal World Modeling for Robot Control</div>
<div class="meta-line">Authors: Lin Li, Qihang Zhang, Yiming Luo, Shuai Yang, Ruilin Wang, Fei Han, Mingrui Yu, Zelin Gao, Nan Xue, Xing Zhu, Yujun Shen, Yinghao Xu</div>
<div class="meta-line">First: 2026-01-29T17:07:43+00:00 · Latest: 2026-01-29T17:07:43+00:00</div>
<div class="meta-line">Comments: Project page: https://technology.robbyant.com/lingbot-va Code: https://github.com/robbyant/lingbot-va</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21998v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21998v1">PDF</a> · <a href="https://github.com/robbyant/lingbot-va">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work highlights that video world modeling, alongside vision-language pre-training, establishes a fresh and independent foundation for robot learning. Intuitively, video world models provide the ability to imagine the near future by understanding the causality between actions and visual dynamics. Inspired by this, we introduce LingBot-VA, an autoregressive diffusion framework that learns frame prediction and policy execution simultaneously. Our model features three carefully crafted designs: (1) a shared latent space, integrating vision and action tokens, driven by a Mixture-of-Transformers (MoT) architecture, (2) a closed-loop rollout mechanism, allowing for ongoing acquisition of environmental feedback with ground-truth observations, (3) an asynchronous inference pipeline, parallelizing action prediction and motor execution to support efficient control. We evaluate our model on both simulation benchmarks and real-world scenarios, where it shows significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalizability to novel configurations. The code and model are made publicly available to facilitate the community.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向机器人控制的因果世界建模</div>
<div class="mono" style="margin-top:8px">本研究指出，视频世界建模与视觉语言预训练相结合，为机器人学习建立了全新且独立的基础。直观而言，视频世界模型通过理解动作与视觉动态之间的因果关系，提供了预测近期未来的能力。受此启发，我们提出了LingBot-VA——一种同时学习帧预测与策略执行的自回归扩散框架。该模型具备三项精心设计：(1) 基于混合变换器架构的共享潜在空间，融合视觉与动作标记；(2) 闭环推演机制，支持通过真实观测持续获取环境反馈；(3) 异步推理管线，并行执行动作预测与运动控制以实现高效操作。我们在仿真基准测试和实际场景中评估模型，结果显示其在长周期操控、训练后数据效率以及对新配置的强泛化能力方面均展现出显著潜力。代码与模型已开源以促进社区研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the potential of video world modeling and vision-language pre-training to serve as a new foundation for robot learning, as they enable the imagination of future states by understanding action-visual causality. The method introduces LingBot-VA, an autoregressive diffusion framework that jointly learns frame prediction and policy execution through a shared latent space with a Mixture-of-Transformers architecture, a closed-loop rollout mechanism for environmental feedback, and an asynchronous inference pipeline for efficient control. Experimental results on simulation benchmarks and real-world scenarios demonstrate the model&#x27;s significant promise in long-horizon manipulation, data efficiency in post-training, and strong generalization to novel configurations.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人学习框架能力提升的需求，利用视频世界建模和视觉语言预训练为控制任务构建统一基础。提出的LingBot-VA是一个自回归扩散框架，通过混合专家Transformer架构构建的视觉与动作令牌共享潜在空间、持续获取环境反馈的闭环推演机制以及支持高效控制的异步推理管道，实现了帧预测与策略执行的联合学习。在仿真基准和真实场景的实验中，该模型在长时程操作任务中表现出色，在训练后阶段展现出更高的数据效率，并能有效泛化到新配置环境。</div>
</details>
</div>
<div class="card">
<div class="title">MMFineReason: Closing the Multimodal Reasoning Gap via Open Data-Centric Methods</div>
<div class="meta-line">Authors: Honglin Lin, Zheng Liu, Yun Zhu, Chonghan Qin, Juekai Lin, Xiaoran Shang, Conghui He, Wentao Zhang, Lijun Wu</div>
<div class="meta-line">First: 2026-01-29T15:07:28+00:00 · Latest: 2026-01-29T15:07:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21821v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Vision Language Models (VLMs) have driven significant progress in visual reasoning. However, open-source VLMs still lag behind proprietary systems, largely due to the lack of high-quality reasoning data. Existing datasets offer limited coverage of challenging domains such as STEM diagrams and visual puzzles, and lack consistent, long-form Chain-of-Thought (CoT) annotations essential for eliciting strong reasoning capabilities. To bridge this gap, we introduce MMFineReason, a large-scale multimodal reasoning dataset comprising 1.8M samples and 5.1B solution tokens, featuring high-quality reasoning annotations distilled from Qwen3-VL-235B-A22B-Thinking. The dataset is established via a systematic three-stage pipeline: (1) large-scale data collection and standardization, (2) CoT rationale generation, and (3) comprehensive selection based on reasoning quality and difficulty awareness. The resulting dataset spans STEM problems, visual puzzles, games, and complex diagrams, with each sample annotated with visually grounded reasoning traces. We fine-tune Qwen3-VL-Instruct on MMFineReason to develop MMFineReason-2B/4B/8B versions. Our models establish new state-of-the-art results for their size class. Notably, MMFineReason-4B succesfully surpasses Qwen3-VL-8B-Thinking, and MMFineReason-8B even outperforms Qwen3-VL-30B-A3B-Thinking while approaching Qwen3-VL-32B-Thinking, demonstrating remarkable parameter efficiency. Crucially, we uncover a &quot;less is more&quot; phenomenon via our difficulty-aware filtering strategy: a subset of just 7\% (123K samples) achieves performance comparable to the full dataset. Notably, we reveal a synergistic effect where reasoning-oriented data composition simultaneously boosts general capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMFineReason：通过开放式数据驱动方法弥合多模态推理鸿沟</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）的最新进展推动了视觉推理领域的显著进步。然而，开源VLM仍落后于专有系统，主要原因是缺乏高质量推理数据。现有数据集对STEM图表、视觉谜题等挑战性领域覆盖有限，且缺乏能激发强大推理能力所必需的一致、长链思维（CoT）标注。为弥合这一差距，我们推出了MMFineReason——一个包含180万样本、51亿解答标记的大规模多模态推理数据集，其高质量推理标注提炼自Qwen3-VL-235B-A22B-Thinking。该数据集通过系统化的三阶段流程构建：（1）大规模数据收集与标准化，（2）CoT原理生成，（3）基于推理质量与难度感知的综合筛选。最终数据集涵盖STEM问题、视觉谜题、游戏及复杂图表，每个样本均配有视觉化推理轨迹标注。我们在MMFineReason上微调Qwen3-VL-Instruct，开发出MMFineReason-2B/4B/8B版本。这些模型在其规模类别中均取得最先进成果：MMFineReason-4B成功超越Qwen3-VL-8B-Thinking，MMFineReason-8B甚至优于Qwen3-VL-30B-A3B-Thinking并接近Qwen3-VL-32B-Thinking，展现出卓越的参数效率。关键发现是，通过难度感知过滤策略揭示了“少即是多”现象：仅7%（12.3万样本）的子集即可达到与完整数据集相当的性能。值得注意的是，我们发现了推理导向数据组合能同步提升通用能力的协同效应。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the performance gap between open-source and proprietary Vision Language Models caused by insufficient high-quality multimodal reasoning data, this work introduces MMFineReason, a large-scale dataset with 1.8 million samples and 5.1 billion solution tokens featuring detailed Chain-of-Thought annotations distilled from a powerful teacher model. The dataset is constructed through a three-stage pipeline involving data collection, rationale generation, and quality filtering across challenging domains like STEM diagrams and visual puzzles. Experimental results show that fine-tuning Qwen3-VL-Instruct on this dataset produces highly parameter-efficient models, with the 4B version surpassing Qwen3-VL-8B-Thinking and the 8B version outperforming Qwen3-VL-30B-A3B-Thinking, while a key finding reveals that a carefully filtered subset of only 7% of the data achieves comparable performance to the full dataset, demonstrating a &quot;less is more&quot; phenomenon.</div>
<div class="mono" style="margin-top:8px">为解决开源视觉语言模型因缺乏高质量、长链推理数据而落后于闭源系统的问题，本研究提出了大规模多模态推理数据集MMFineReason。该方法采用三阶段流程进行数据收集、标准化，并基于强大教师模型蒸馏生成思维链标注，随后进行质量和难度感知的筛选。实验结果表明，基于该数据集微调的模型在其参数量级上取得了最先进的性能，其中8B模型超越了30B的基线模型，并揭示了一个精心筛选的仅占总数7%的数据子集即可达到与完整数据集相当的性能，同时还能提升模型的通用能力。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Weakly-Supervised Learning and VLM Distillation: Noisy Partial Label Learning for Efficient Downstream Adaptation</div>
<div class="meta-line">Authors: Qian-Wei Wang, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2025-06-03T12:48:54+00:00 · Latest: 2026-01-29T13:56:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.03229v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.03229v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of noisy partial label learning (NPLL), each training sample is associated with a set of candidate labels annotated by multiple noisy annotators. With the emergence of high-performance pre-trained vision-language models (VLMs) such as CLIP, LLaVA, and GPT-4V, leveraging these models to replace time-consuming manual annotation and enable annotation-free training has become a promising research direction. This paper studies learning from noisy partial labels generated by pre-trained VLMs and proposes a collaborative consistency regularization (Co-Reg) framework. Unlike symmetric noise commonly assumed in traditional noisy label learning, VLM-generated noise is instance-dependent and reflects the intrinsic biases of pre-trained models, posing greater challenges. To address this issue, we jointly train two neural networks to perform collaborative label purification via a co-pseudo-labeling mechanism, while enforcing consistency regularization in both label and feature representation spaces. In addition, multiple anti-overfitting strategies are introduced, including alternating optimization of contrastive representations and pseudo-labels, as well as maintaining class prototypes in a shared feature space. The proposed method can further incorporate few-shot manually annotated labels for performance enhancement. Extensive experiments under various settings demonstrate the effectiveness of our approach and highlight the potential of integrating weakly supervised learning into the knowledge distillation of pre-trained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>桥接弱监督学习与视觉语言模型蒸馏：面向高效下游适应的噪声部分标签学习</div>
<div class="mono" style="margin-top:8px">在噪声部分标签学习（NPLL）框架下，每个训练样本关联着由多个噪声标注者标注的候选标签集合。随着CLIP、LLaVA、GPT-4V等高性能预训练视觉语言模型（VLM）的出现，利用这些模型替代耗时的人工标注并实现免标注训练已成为新兴研究方向。本文研究从预训练VLM生成的噪声部分标签中学习，提出协同一致性正则化（Co-Reg）框架。与传统噪声标签学习中常假设的对称噪声不同，VLM生成的噪声具有实例依赖性，反映了预训练模型的内在偏差，带来更大挑战。为解决该问题，我们通过协同伪标签机制联合训练两个神经网络进行标签净化，同时在标签空间和特征表示空间实施一致性正则化。此外，引入多种抗过拟合策略，包括对比表示与伪标签的交替优化，以及在共享特征空间中维护类别原型。所提方法可进一步融合少量人工标注标签以提升性能。多场景实验验证了方法的有效性，彰显了将弱监督学习整合到预训练模型知识蒸馏中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of learning from noisy partial labels generated by pre-trained vision-language models (VLMs), which offer a promising but noisy alternative to manual annotation. The authors propose a collaborative consistency regularization (Co-Reg) framework that jointly trains two neural networks to purify labels through a co-pseudo-labeling mechanism, while enforcing consistency in both label and feature spaces and employing anti-overfitting strategies like alternating optimization and class prototype maintenance. Experimental results across various settings demonstrate the method&#x27;s effectiveness in handling the instance-dependent noise from VLMs and show that performance can be further enhanced by incorporating few-shot manual annotations.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从预训练视觉语言模型（VLM）生成的噪声部分标签中学习的问题，这些标签为手动标注提供了可扩展但不完美的替代方案。所提出的方法是一种协作一致性正则化（Co-Reg）框架，它联合训练两个网络，通过协同伪标签机制进行标签净化，同时在标签和特征空间应用一致性正则化，并采用交替优化和类别原型等策略防止过拟合。在各种设置下的实验结果表明，该框架能有效处理VLM产生的实例依赖性噪声，并能整合少量干净标签以提升性能，为预训练模型的弱监督适应提供了一条可行路径。</div>
</details>
</div>
<div class="card">
<div class="title">OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</div>
<div class="meta-line">Authors: Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng</div>
<div class="meta-line">First: 2026-01-29T12:43:02+00:00 · Latest: 2026-01-29T12:43:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21639v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (\textbf{Text-centric OCR}), neglecting the identification of visual elements from visually information-dense image sources (\textbf{Vision-centric OCR}), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose \textbf{OCRVerse}, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OCRVerse：迈向端到端视觉语言模型中的全息OCR</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型的发展推动了对海量多模态数据管理及应用的需求，使得从视觉图像中提取信息的OCR技术日益普及。然而，现有OCR方法主要聚焦于从图像或扫描文档中识别文本元素（文本中心OCR），而忽视了从视觉信息密集的图像源（视觉中心OCR）——如图表、网页和科学图表——中识别视觉元素。现实中，这类视觉信息密集的图像在互联网中广泛存在，具有重要的实际应用价值，例如数据可视化和网页分析。在本技术报告中，我们提出了OCRVerse，首个以端到端方式实现文本中心OCR与视觉中心OCR统一的全息OCR方法。为此，我们构建了全面的数据工程，覆盖报纸、杂志、书籍等广泛文本中心文档，以及图表、网页和科学图表等视觉中心渲染复合体。此外，我们为OCRVerse提出了一种两阶段SFT-RL多领域训练方法：SFT直接混合跨领域数据进行训练以建立初始领域知识，而RL则针对各领域特性设计个性化奖励策略。具体而言，由于不同领域需要多样化的输出格式和预期结果，我们在RL阶段提供充分灵活性，为每个领域定制灵活的奖励信号，从而提升跨领域融合能力并避免数据冲突。实验结果表明，OCRVerse在文本中心与视觉中心数据类型上均取得具有竞争力的结果，甚至可与大规模开源及闭源模型相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for OCR technology that goes beyond traditional text-centric recognition to also handle vision-centric tasks, such as extracting information from charts, web pages, and scientific plots, which are prevalent and valuable in real-world applications. The method introduces OCRVerse, an end-to-end holistic OCR approach that unifies both text-centric and vision-centric OCR through comprehensive data engineering covering diverse document types and a two-stage SFT-RL multi-domain training strategy, where supervised fine-tuning establishes initial domain knowledge and reinforcement learning employs personalized reward signals tailored to each domain&#x27;s output formats to enhance cross-domain fusion and mitigate data conflicts. Experimental results show that OCRVerse achieves competitive performance across both text-centric and vision-centric data types, rivaling large-scale open-source and closed-source models.</div>
<div class="mono" style="margin-top:8px">该研究针对现有OCR方法主要关注文本中心识别而忽视视觉中心OCR（如图表和网页等视觉密集图像）的局限性，这些图像在互联网上广泛存在，在数据可视化等应用中具有重要价值。作者提出OCRVerse，一种端到端的整体OCR方法，通过涵盖文档和渲染复合材料的全面数据工程，以及两阶段SFT-RL多领域训练方法，统一了文本中心和视觉中心OCR；其中监督微调建立初始领域知识，强化学习则针对每个领域的特点设计个性化奖励策略，以处理多样化的输出格式并避免数据冲突。实验结果表明，OCRVerse在文本中心和视觉中心数据类型上均取得有竞争力的性能，可与大规模开源和闭源模型相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization</div>
<div class="meta-line">Authors: Songhan Jiang, Fengchun Liu, Ziyue Wang, Linghan Cai, Yongbing Zhang</div>
<div class="meta-line">First: 2026-01-29T12:21:16+00:00 · Latest: 2026-01-29T12:21:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21617v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21617v1">PDF</a> · <a href="https://github.com/cyclexfy/PathReasoner-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are advancing computational pathology with superior visual understanding capabilities. However, current systems often reduce diagnosis to directly output conclusions without verifiable evidence-linked reasoning, which severely limits clinical trust and hinders expert error rectification. To address these barriers, we construct PathReasoner, the first large-scale dataset of whole-slide image (WSI) reasoning. Unlike previous work reliant on unverified distillation, we develop a rigorous knowledge-guided generation pipeline. By leveraging medical knowledge graphs, we explicitly align structured pathological findings and clinical reasoning with diagnoses, generating over 20K high-quality instructional samples. Based on the database, we propose PathReasoner-R1, which synergizes trajectory-masked supervised fine-tuning with reasoning-oriented reinforcement learning to instill structured chain-of-thought capabilities. To ensure medical rigor, we engineer a knowledge-aware multi-granular reward function incorporating an Entity Reward mechanism strictly aligned with knowledge graphs. This effectively guides the model to optimize for logical consistency rather than mere outcome matching, thereby enhancing robustness. Extensive experiments demonstrate that PathReasoner-R1 achieves state-of-the-art performance on both PathReasoner and public benchmarks across various image scales, equipping pathology models with transparent, clinically grounded reasoning capabilities. Dataset and code are available at https://github.com/cyclexfy/PathReasoner-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PathReasoner-R1：通过知识引导的策略优化为病理视觉语言模型注入结构化推理能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型凭借卓越的视觉理解能力正推动计算病理学发展，但现有系统常将诊断简化为直接输出结论，缺乏可验证的证据链推理，严重制约临床可信度与专家纠错。为此，我们构建首个全切片图像推理大规模数据集PathReasoner。区别于依赖未经验证蒸馏的先前工作，我们开发了严格的知识引导生成流程，通过医学知识图谱显式对齐结构化病理发现、临床推理与诊断，生成超2万高质量指令样本。基于此，我们提出PathReasoner-R1，融合轨迹掩码监督微调与推理导向强化学习，以注入结构化思维链能力。为保障医学严谨性，我们设计了融合知识图谱严格对齐的实体奖励机制的多粒度奖励函数，引导模型优化逻辑一致性而非仅结果匹配，从而增强鲁棒性。大量实验表明，PathReasoner-R1在PathReasoner及多尺度公开基准测试中均达到最先进性能，为病理模型赋予透明且临床可溯的推理能力。数据集与代码发布于https://github.com/cyclexfy/PathReasoner-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current pathology vision-language models often produce direct diagnostic conclusions without verifiable reasoning, limiting clinical trust and error correction. To address this, the authors constructed PathReasoner, a large-scale dataset of whole-slide image reasoning generated via a knowledge-guided pipeline using medical knowledge graphs to align findings and reasoning with diagnoses, yielding over 20K instructional samples. They then developed PathReasoner-R1, a model trained with trajectory-masked supervised fine-tuning and reasoning-oriented reinforcement learning, optimized by a knowledge-aware multi-granular reward function including an Entity Reward mechanism for logical consistency. Experiments show PathReasoner-R1 achieves state-of-the-art performance on both the PathReasoner dataset and public benchmarks across various image scales, providing transparent, clinically grounded reasoning.</div>
<div class="mono" style="margin-top:8px">当前病理视觉语言模型常直接输出诊断结论而缺乏可验证的推理过程，这限制了临床信任和错误纠正。为解决此问题，作者构建了PathReasoner，这是一个通过知识引导流程生成的大规模全切片图像推理数据集，该流程利用医学知识图谱将结构化的病理发现和临床推理与诊断对齐。随后，他们开发了PathReasoner-R1模型，该模型通过轨迹掩码监督微调和面向推理的强化学习进行训练，并辅以包含实体奖励机制的知识感知多粒度奖励函数进行引导。实验表明，PathReasoner-R1在PathReasoner数据集和公共基准测试中均取得了最先进的性能，提供了透明且基于临床的推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models</div>
<div class="meta-line">Authors: Zijin Yang, Yu Sun, Kejiang Chen, Jiawei Zhao, Jun Jiang, Weiming Zhang, Nenghai Yu</div>
<div class="meta-line">First: 2026-01-29T12:14:32+00:00 · Latest: 2026-01-29T12:14:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21610v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21610v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital watermarking is essential for securing generated images from diffusion models. Accurate watermark evaluation is critical for algorithm development, yet existing methods have significant limitations: they lack a unified framework for both residual and semantic watermarks, provide results without interpretability, neglect comprehensive security considerations, and often use inappropriate metrics for semantic watermarks. To address these gaps, we propose WMVLM, the first unified and interpretable evaluation framework for diffusion model image watermarking via vision-language models (VLMs). We redefine quality and security metrics for each watermark type: residual watermarks are evaluated by artifact strength and erasure resistance, while semantic watermarks are assessed through latent distribution shifts. Moreover, we introduce a three-stage training strategy to progressively enable the model to achieve classification, scoring, and interpretable text generation. Experiments show WMVLM outperforms state-of-the-art VLMs with strong generalization across datasets, diffusion models, and watermarking methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WMVLM：基于视觉语言模型的扩散模型图像水印评估方法</div>
<div class="mono" style="margin-top:8px">数字水印对保护扩散模型生成的图像至关重要。准确的水印评估是算法开发的关键，但现有方法存在显著局限：缺乏针对残留水印和语义水印的统一框架、结果缺乏可解释性、忽视全面的安全性考量，且常对语义水印使用不恰当的评估指标。为解决这些问题，我们提出WMVLM——首个基于视觉语言模型（VLM）的统一可解释扩散模型图像水印评估框架。我们重新定义了各类水印的质量与安全指标：残留水印通过伪影强度和抗擦除性评估，语义水印则通过潜在分布偏移进行度量。此外，我们引入三阶段训练策略，使模型逐步实现分类、评分和可解释文本生成功能。实验表明，WMVLM在跨数据集、扩散模型和水印方法的泛化能力上均优于当前最先进的视觉语言模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of a unified, interpretable, and comprehensive evaluation framework for both residual and semantic watermarks in diffusion model-generated images, this paper proposes WMVLM, a vision-language model-based framework. The method redefines quality and security metrics for each watermark type—evaluating residual watermarks via artifact strength and erasure resistance, and semantic watermarks via latent distribution shifts—and employs a three-stage training strategy for classification, scoring, and interpretable text generation. Experimental results demonstrate that WMVLM outperforms existing state-of-the-art vision-language models and exhibits strong generalization across various datasets, diffusion models, and watermarking techniques.</div>
<div class="mono" style="margin-top:8px">针对扩散模型生成图像中残留水印和语义水印缺乏统一、可解释且全面的评估框架的问题，本文提出了WMVLM，一个基于视觉-语言模型的评估框架。该方法为每种水印类型重新定义了质量和安全度量标准——通过伪影强度和抗擦除性评估残留水印，通过潜在分布偏移评估语义水印——并采用三阶段训练策略以实现分类、评分和可解释的文本生成。实验结果表明，WMVLM优于现有的先进视觉-语言模型，并在不同数据集、扩散模型和水印方法上展现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">On the Adversarial Robustness of Large Vision-Language Models under Visual Token Compression</div>
<div class="meta-line">Authors: Xinwei Zhang, Hangcheng Liu, Li Bai, Hao Wang, Qingqing Ye, Tianwei Zhang, Haibo Hu</div>
<div class="meta-line">First: 2026-01-29T10:47:21+00:00 · Latest: 2026-01-29T10:47:21+00:00</div>
<div class="meta-line">Comments: Under Review, 20 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21531v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21531v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual token compression is widely used to accelerate large vision-language models (LVLMs) by pruning or merging visual tokens, yet its adversarial robustness remains unexplored. We show that existing encoder-based attacks can substantially overestimate the robustness of compressed LVLMs, due to an optimization-inference mismatch: perturbations are optimized on the full-token representation, while inference is performed through a token-compression bottleneck. To address this gap, we propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference without assuming access to the deployed compression mechanism or its token budget. CAGE combines (i) expected feature disruption, which concentrates distortion on tokens likely to survive across plausible budgets, and (ii) rank distortion alignment, which actively aligns token distortions with rank scores to promote the retention of highly distorted evidence. Across diverse representative plug-and-play compression mechanisms and datasets, our results show that CAGE consistently achieves lower robust accuracy than the baseline. This work highlights that robustness assessments ignoring compression can be overly optimistic, calling for compression-aware security evaluation and defenses for efficient LVLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉令牌压缩下大型视觉语言模型的对抗鲁棒性研究</div>
<div class="mono" style="margin-top:8px">视觉令牌压缩通过剪枝或合并视觉令牌来加速大型视觉语言模型，但其对抗鲁棒性尚未得到充分探索。我们发现，由于优化与推理的不匹配——扰动在完整令牌表示上优化，而推理通过令牌压缩瓶颈执行——现有基于编码器的攻击会显著高估压缩后模型的鲁棒性。为解决这一问题，我们提出压缩对齐攻击方法，该方法在不假设已知部署压缩机制或令牌预算的前提下，将扰动优化与压缩推理对齐。该方法结合了（1）预期特征破坏——将失真集中于可能在不同预算下存活的令牌，以及（2）秩失真对齐——主动将令牌失真与秩分数对齐以促进高失真证据的保留。在多种代表性即插即用压缩机制和数据集上的实验表明，该方法始终获得比基线更低的鲁棒准确率。本研究揭示忽略压缩的鲁棒性评估可能过于乐观，呼吁对高效大型视觉语言模型开展压缩感知的安全评估与防御研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the adversarial robustness of large vision-language models (LVLMs) when visual token compression is applied for efficiency, noting that prior encoder-based attacks overestimate robustness due to an optimization-inference mismatch. To address this, the authors propose the Compression-AliGnEd attack (CAGE), which aligns perturbation optimization with compression inference by using expected feature disruption to focus distortion on tokens likely to survive across budgets and rank distortion alignment to promote retention of highly distorted evidence. Experimental results across various compression mechanisms and datasets demonstrate that CAGE consistently achieves lower robust accuracy than baseline attacks, revealing that ignoring compression leads to overly optimistic robustness assessments and underscoring the need for compression-aware security evaluations.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型视觉语言模型在应用视觉令牌压缩以提高效率时的对抗鲁棒性，这一场景先前被忽视。针对现有攻击方法中存在的优化与推理不匹配问题，作者提出了压缩对齐攻击（CAGE），该方法通过预期特征破坏来聚焦于可能存活于压缩过程中的令牌，并利用秩失真对齐来促进失真证据的保留，从而使扰动优化与压缩推理对齐。在不同压缩机制和数据集上的实验表明，CAGE相比基线攻击能持续降低鲁棒准确率，揭示了忽略压缩会导致过于乐观的鲁棒性评估，并强调了进行压缩感知安全评估的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">GeoRC: A Benchmark for Geolocation Reasoning Chains</div>
<div class="meta-line">Authors: Mohit Talreja, Joshua Diao, Jim Thannikary James, Radu Casapu, Tejas Santanam, Ethan Mendes, Alan Ritter, Wei Xu, James Hays</div>
<div class="meta-line">First: 2026-01-29T05:18:40+00:00 · Latest: 2026-01-29T05:18:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21278v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21278v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) are good at recognizing the global location of a photograph -- their geolocation prediction accuracy rivals the best human experts. But many VLMs are startlingly bad at explaining which image evidence led to their prediction, even when their location prediction is correct. The reasoning chains produced by VLMs frequently hallucinate scene attributes to support their location prediction (e.g. phantom writing, imagined infrastructure, misidentified flora). In this paper, we introduce the first benchmark for geolocation reasoning chains. We focus on the global location prediction task in the popular GeoGuessr game which draws from Google Street View spanning more than 100 countries. We collaborate with expert GeoGuessr players, including the reigning world champion, to produce 800 ground truth reasoning chains for 500 query scenes. These expert reasoning chains address hundreds of different discriminative visual attributes such as license plate shape, architecture, and soil properties to name just a few. We evaluate LLM-as-a-judge and VLM-as-a-judge strategies for scoring VLM-generated reasoning chains against our expert reasoning chains and find that Qwen 3 LLM-as-a-judge correlates best with human scoring. Our benchmark reveals that while large, closed-source VLMs such as Gemini and GPT 5 rival human experts at prediction locations, they still lag behind human experts when it comes to producing auditable reasoning chains. Open weights VLMs such as Llama and Qwen catastrophically fail on our benchmark -- they perform only slightly better than a baseline in which an LLM hallucinates a reasoning chain with oracle knowledge of the photo location but no visual information at all. We believe the gap between human experts and VLMs on this task points to VLM limitations at extracting fine-grained visual attributes from high resolution images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoRC：地理定位推理链基准测试</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）擅长识别照片的全球地理位置——其地理定位预测准确度可与顶尖人类专家媲美。但许多VLMs在解释图像证据如何支持其预测方面表现惊人地差，即使其位置预测正确。VLMs生成的推理链常虚构场景属性以支持定位（如幻视文字、想象的基础设施、误判的植被）。本文首次提出地理定位推理链的基准测试，聚焦于流行游戏GeoGuessr中的全球定位任务（数据源涵盖100余国的谷歌街景）。我们联合包括卫冕世界冠军在内的GeoGuessr专家玩家，为500个查询场景构建了800条真实推理链。这些专家推理链涉及数百种判别性视觉属性，如车牌形状、建筑风格、土壤特性等。通过评估LLM-as-a-judge和VLM-as-a-judge策略对VLM生成推理链与专家链的评分，发现Qwen 3 LLM-as-a-judge与人工评分相关性最佳。基准测试表明：Gemini、GPT 5等大型闭源VLMs虽在位置预测上比肩人类专家，但在生成可审计推理链方面仍落后；Llama、Qwen等开源权重VLMs则表现崩溃——其效果仅略优于基线（即LLM在完全无视觉信息情况下仅凭照片位置先知知识虚构推理链）。我们认为该任务中人类专家与VLMs的差距，揭示了VLMs从高分辨率图像提取细粒度视觉属性的能力局限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that Vision Language Models (VLMs) achieve high geolocation prediction accuracy but often produce poor explanations, with reasoning chains that hallucinate visual evidence. To address this, the authors introduce GeoRC, a benchmark for evaluating geolocation reasoning chains, constructed by collaborating with expert GeoGuessr players to create 800 ground truth reasoning chains for 500 Google Street View scenes, focusing on discriminative visual attributes. Experimental results show that while large closed-source VLMs like Gemini and GPT rival human location prediction, they lag in producing auditable reasoning chains; open-weight VLMs perform only slightly better than a baseline with hallucinated chains, and the evaluation finds that using Qwen 3 as an LLM judge correlates best with human scoring, highlighting VLM limitations in fine-grained visual attribute extraction.</div>
<div class="mono" style="margin-top:8px">该研究的动机是观察到视觉语言模型（VLMs）在地理位置预测上准确率高，但常产生解释力差的推理链，并幻觉视觉证据。为此，作者引入了GeoRC，一个用于评估地理位置推理链的基准，通过与包括世界冠军在内的GeoGuessr专家玩家合作，为500个谷歌街景场景创建了800条基于判别性视觉属性的真实推理链。实验评估采用LLM-as-a-judge和VLM-as-a-judge策略，发现Qwen 3 LLM-as-a-judge与人类评分相关性最佳，并揭示尽管大型闭源VLMs如Gemini和GPT在位置预测上媲美人类专家，但在生成可审计推理链方面仍落后，开源权重VLMs表现仅略优于幻觉基线，凸显了VLMs在细粒度视觉属性提取上的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Retraining: Training-Free Unknown Class Filtering for Source-Free Open Set Domain Adaptation of Vision-Language Models</div>
<div class="meta-line">Authors: Yongguang Li, Jindong Li, Qi Wang, Qianli Xing, Runliang Niu, Shengsheng Wang, Menglin Yang</div>
<div class="meta-line">First: 2025-04-19T08:12:19+00:00 · Latest: 2026-01-29T04:29:06+00:00</div>
<div class="meta-line">Comments: Core methods unchanged; title updated and full-text narrative refined for clarity and logical coherence. No changes to key findings and conclusions</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.14224v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.14224v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have gained widespread attention for their strong zero-shot capabilities across numerous downstream tasks. However, these models assume that each test image&#x27;s class label is drawn from a predefined label set and lack a reliable mechanism to reject samples from emerging unknown classes when only unlabeled data are available. To address this gap, open-set domain adaptation methods retrain models to push potential unknowns away from known clusters. Yet, some unknown samples remain stably anchored to specific known classes in the VLM feature space due to semantic relevance, which is termed as Semantic Affinity Anchoring (SAA). Forcibly repelling these samples unavoidably distorts the native geometry of VLMs and degrades performance. Meanwhile, existing score-based unknown detectors use simplistic thresholds and suffer from threshold sensitivity, resulting in sub-optimal performance. To address aforementioned issues, we propose VLM-OpenXpert, which comprises two training-free, plug-and-play inference modules. SUFF performs SVD on high-confidence unknowns to extract a low-rank &quot;unknown subspace&quot;. Each sample&#x27;s projection onto this subspace is weighted and softly removed from its feature, suppressing unknown components while preserving semantics. BGAT corrects score skewness via a Box-Cox transform, then fits a bimodal Gaussian mixture to adaptively estimate the optimal threshold balancing known-class recognition and unknown-class rejection. Experiments on 9 benchmarks and three backbones (CLIP, SigLIP, ALIGN) under source-free OSDA settings show that our training-free pipeline matches or outperforms retraining-heavy state-of-the-art methods, establishing a powerful lightweight inference calibration paradigm for open-set VLM deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越重训练：面向视觉语言模型无源开放集域适应的免训练未知类过滤方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型因其在众多下游任务中强大的零样本能力而广受关注。然而，这些模型假设每个测试图像的类别标签均来自预定义的标签集，且在仅有未标注数据时缺乏可靠机制来拒斥来自新兴未知类别的样本。为填补这一空白，开放集域适应方法通过重训练模型将潜在未知样本推离已知类别簇。但由于语义相关性，部分未知样本在VLM特征空间中仍稳定锚定于特定已知类别，这种现象称为语义亲和锚定。强行排斥这些样本不可避免地会扭曲VLM的固有几何结构并降低性能。同时，现有基于分数的未知检测器使用简单阈值且受阈值敏感性影响，导致次优性能。针对上述问题，我们提出VLM-OpenXpert框架，包含两个免训练的即插即用推理模块：SUFF通过对高置信度未知样本进行奇异值分解提取低秩“未知子空间”，通过加权投影并软性移除特征中的未知成分，在保留语义的同时抑制未知分量；BGAT通过Box-Cox变换校正分数偏度，再拟合双峰高斯混合模型自适应估计最优阈值以平衡已知类识别与未知类拒斥。在9个基准数据集和三种骨干网络（CLIP、SigLIP、ALIGN）上的无源开放集域适应实验表明，我们的免训练流程达到或超越了依赖重训练的先进方法，为开放集VLM部署建立了强大的轻量级推理校准范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of open-set domain adaptation for vision-language models, where existing methods either retrain models to separate unknown classes, risking distortion of the feature space due to semantic affinity anchoring, or rely on simplistic thresholds for unknown detection, leading to suboptimal performance. The proposed VLM-OpenXpert introduces two training-free modules: SUFF uses singular value decomposition on high-confidence unknown samples to extract an unknown subspace and softly removes its projection from features to suppress unknown components while preserving semantics, and BGAT applies a Box-Cox transform to correct score skewness and fits a bimodal Gaussian mixture to adaptively estimate an optimal threshold for balancing known-class recognition and unknown-class rejection. Experiments across nine benchmarks with three backbones under source-free settings demonstrate that this lightweight inference pipeline matches or surpasses retraining-heavy state-of-the-art methods, establishing an effective calibration paradigm for open-set deployment.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在源无关开放集域适应中的未知类过滤问题，现有方法要么通过重训练模型可能因语义亲和锚定而扭曲原生特征几何，要么依赖基于分数的检测器使用简单阈值。提出的VLM-OpenXpert包含两个免训练模块：SUFF对高置信度未知样本进行奇异值分解以提取未知子空间，并软性移除其特征投影以抑制未知成分；BGAT通过Box-Cox变换校正分数偏斜，并拟合双峰高斯混合模型自适应估计最优阈值以平衡已知类识别与未知类拒绝。在九个基准和三种骨干网络上的实验表明，该轻量级推理流程在性能上匹配或超越了需重训练的先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models</div>
<div class="meta-line">Authors: Wenbo Xu, Wei Lu, Xiangyang Luo, Jiantao Zhou</div>
<div class="meta-line">First: 2026-01-28T09:44:31+00:00 · Latest: 2026-01-29T03:56:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20433v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20433v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deepfake detection is a widely researched topic that is crucial for combating the spread of malicious content, with existing methods mainly modeling the problem as classification or spatial localization. The rapid advancements in generative models impose new demands on Deepfake detection. In this paper, we propose multimodal alignment and reinforcement for explainable Deepfake detection via vision-language models, termed MARE, which aims to enhance the accuracy and reliability of Vision-Language Models (VLMs) in Deepfake detection and reasoning. Specifically, MARE designs comprehensive reward functions, incorporating reinforcement learning from human feedback (RLHF), to incentivize the generation of text-spatially aligned reasoning content that adheres to human preferences. Besides, MARE introduces a forgery disentanglement module to capture intrinsic forgery traces from high-level facial semantics, thereby improving its authenticity detection capability. We conduct thorough evaluations on the reasoning content generated by MARE. Both quantitative and qualitative experimental results demonstrate that MARE achieves state-of-the-art performance in terms of accuracy and reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARE：基于视觉语言模型的多模态对齐与强化可解释深度伪造检测</div>
<div class="mono" style="margin-top:8px">深度伪造检测是广泛研究的关键课题，现有方法主要将问题建模为分类或空间定位任务。生成模型的快速发展对检测技术提出了新要求。本文提出基于视觉语言模型的多模态对齐与强化可解释深度伪造检测框架MARE，旨在提升视觉语言模型在深度伪造检测与推理中的准确性与可靠性。具体而言，MARE设计了融合人类反馈强化学习的综合奖励函数，激励生成符合人类偏好且文本-空间对齐的推理内容。此外，MARE引入伪造解耦模块，从高层面部语义中捕捉本质伪造痕迹，从而提升真实性检测能力。我们对MARE生成的推理内容进行了全面评估，定量与定性实验结果均表明，MARE在准确性与可靠性方面达到了最先进的性能水平。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for more accurate and interpretable deepfake detection as generative models advance, moving beyond simple classification or localization. The proposed MARE method enhances vision-language models by using reinforcement learning from human feedback with tailored reward functions to produce text-spatially aligned explanations, and incorporates a forgery disentanglement module to separate intrinsic forgery traces from facial semantics. Experiments show that MARE achieves state-of-the-art performance in both accuracy and reliability for explainable deepfake detection.</div>
<div class="mono" style="margin-top:8px">为应对生成模型进步对深度伪造检测在准确性和可解释性方面提出的新要求，本文提出了MARE方法，通过多模态对齐和强化学习来增强视觉-语言模型。该方法利用基于人类反馈的强化学习设计综合奖励函数，以生成文本-空间对齐的推理内容，并引入伪造解缠模块从高层面部语义中分离内在伪造痕迹。实验评估表明，MARE在可解释的深度伪造检测中实现了最先进的准确性和可靠性性能。</div>
</details>
</div>
<div class="card">
<div class="title">MuSLR: Multimodal Symbolic Logical Reasoning</div>
<div class="meta-line">Authors: Jundong Xu, Hao Fei, Yuhui Zhang, Liangming Pan, Qijun Huang, Qian Liu, Preslav Nakov, Min-Yen Kan, William Yang Wang, Mong-Li Lee, Wynne Hsu</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-09-30T06:42:20+00:00 · Latest: 2026-01-29T03:43:05+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25851v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25851v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://llm-symbol.github.io/MuSLR">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal symbolic logical reasoning, which aims to deduce new facts from multimodal input via formal logic, is critical in high-stakes applications such as autonomous driving and medical diagnosis, as its rigorous, deterministic reasoning helps prevent serious consequences. To evaluate such capabilities of current state-of-the-art vision language models (VLMs), we introduce the first benchmark MuSLR for multimodal symbolic logical reasoning grounded in formal logical rules. MuSLR comprises 1,093 instances across 7 domains, including 35 atomic symbolic logic and 976 logical combinations, with reasoning depths ranging from 2 to 9. We evaluate 7 state-of-the-art VLMs on MuSLR and find that they all struggle with multimodal symbolic reasoning, with the best model, GPT-4.1, achieving only 46.8%. Thus, we propose LogiCAM, a modular framework that applies formal logical rules to multimodal inputs, boosting GPT-4.1&#x27;s Chain-of-Thought performance by 14.13%, and delivering even larger gains on complex logics such as first-order logic. We also conduct a comprehensive error analysis, showing that around 70% of failures stem from logical misalignment between modalities, offering key insights to guide future improvements. All data and code are publicly available at https://llm-symbol.github.io/MuSLR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MuSLR：多模态符号逻辑推理</div>
<div class="mono" style="margin-top:8px">多模态符号逻辑推理旨在通过形式逻辑从多模态输入中推导新事实，在自动驾驶、医疗诊断等高风险应用中至关重要，因其严谨、确定性的推理有助于避免严重后果。为评估当前先进视觉语言模型在此方面的能力，我们推出了首个基于形式逻辑规则的多模态符号逻辑推理基准MuSLR。该基准涵盖7个领域的1,093个实例，包含35个原子符号逻辑和976个逻辑组合，推理深度为2至9层。我们对7个先进视觉语言模型进行测试，发现它们均难以处理多模态符号推理，表现最佳的GPT-4.1模型仅达到46.8%准确率。为此，我们提出LogiCAM模块化框架，将形式逻辑规则应用于多模态输入，使GPT-4.1的思维链性能提升14.13%，在一阶逻辑等复杂逻辑上提升更为显著。通过全面错误分析，我们发现约70%的失败源于模态间的逻辑错位，这为未来改进提供了关键方向。所有数据与代码已公开于https://llm-symbol.github.io/MuSLR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for rigorous multimodal symbolic logical reasoning in high-stakes applications like autonomous driving, where deterministic deduction from multimodal inputs is crucial to prevent errors. To assess this capability in vision-language models, the authors introduce the MuSLR benchmark, comprising 1,093 instances across seven domains with reasoning depths up to nine, and evaluate seven state-of-the-art models, finding that even the best-performing GPT-4.1 achieves only 46.8% accuracy. They then propose LogiCAM, a modular framework that applies formal logical rules to multimodal inputs, which improves GPT-4.1&#x27;s Chain-of-Thought performance by 14.13%, with larger gains on complex logics like first-order logic, and an error analysis reveals that about 70% of failures are due to logical misalignment between modalities.</div>
<div class="mono" style="margin-top:8px">本研究针对自动驾驶和医疗诊断等高风险应用中需要严谨的多模态符号逻辑推理的问题，其确定性推理对于防止严重后果至关重要。作者引入了首个评估该能力的基准MuSLR，包含7个领域共1093个实例，推理深度达9层，并发现7种最先进的视觉语言模型在此任务上表现均不佳，其中GPT-4.1仅达到46.8%的准确率。为此，他们提出了LogiCAM框架，该模块化方法将形式逻辑规则应用于多模态输入，将GPT-4.1的思维链性能提升了14.13%，在如一阶逻辑等复杂逻辑上提升更为显著，同时错误分析表明约70%的失败源于模态间的逻辑不一致。</div>
</details>
</div>
<div class="card">
<div class="title">Causality-guided Prompt Learning for Vision-language Models via Visual Granulation</div>
<div class="meta-line">Authors: Mengyu Gao, Qiulei Dong</div>
<div class="meta-line">First: 2025-09-04T01:40:41+00:00 · Latest: 2026-01-29T03:10:49+00:00</div>
<div class="meta-line">Comments: Updated version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03803v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.03803v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt learning has recently attracted much attention for adapting pre-trained vision-language models (e.g., CLIP) to downstream recognition tasks. However, most of the existing CLIP-based prompt learning methods only show a limited ability for handling fine-grained datasets. To address this issue, we propose a causality-guided text prompt learning method via visual granulation for CLIP, called CaPL, where the explored visual granulation technique could construct sets of visual granules for the text prompt to capture subtle discrepancies among different fine-grained classes through casual inference. The CaPL method contains the following two modules: (1) An attribute disentanglement module is proposed to decompose visual features into non-individualized attributes (shared by some classes) and individualized attributes (specific to single classes) using a Brownian Bridge Diffusion Model; (2) A granule learning module is proposed to construct visual granules by integrating the aforementioned attributes for recognition under two causal inference strategies. Thanks to the learned visual granules, more discriminative text prompt is expected to be learned. Extensive experimental results on 15 datasets demonstrate that our CaPL method significantly outperforms the state-of-the-art prompt learning methods, especially on fine-grained datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉粒化的因果引导提示学习用于视觉语言模型</div>
<div class="mono" style="margin-top:8px">提示学习近期在将预训练视觉语言模型（如CLIP）适配至下游识别任务中受到广泛关注。然而，现有大多数基于CLIP的提示学习方法在处理细粒度数据集时表现有限。为此，我们提出一种基于视觉粒化的因果引导文本提示学习方法（简称CaPL），该方法通过视觉粒化技术构建视觉粒集，借助因果推理捕捉不同细粒度类别间的细微差异。CaPL包含两个模块：（1）属性解耦模块：利用布朗桥扩散模型将视觉特征分解为跨类共享的非个体化属性与类特定的个体化属性；（2）粒化学习模块：通过整合上述属性构建视觉粒，并基于两种因果推理策略进行识别。借助习得的视觉粒，可学习更具判别力的文本提示。在15个数据集上的大量实验表明，CaPL方法显著优于当前最先进的提示学习方法，尤其在细粒度数据集上表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance the fine-grained recognition capability of CLIP-based prompt learning methods, which often struggle with subtle class distinctions, this study introduces CaPL, a causality-guided prompt learning approach via visual granulation. The method employs an attribute disentanglement module using a Brownian Bridge Diffusion Model to separate visual features into shared and class-specific attributes, and a granule learning module that constructs visual granules by integrating these attributes under causal inference strategies to guide more discriminative text prompt learning. Experiments on 15 datasets show that CaPL significantly outperforms existing prompt learning methods, particularly on fine-grained classification tasks.</div>
<div class="mono" style="margin-top:8px">针对现有基于CLIP的提示学习方法在细粒度数据集上识别能力有限的问题，本研究提出了CaPL，一种通过视觉粒化进行因果引导的提示学习方法。该方法首先使用布朗桥扩散模型将视觉特征解耦为共享属性和个体属性，然后通过因果推理策略整合这些属性以构建视觉颗粒，从而捕捉细粒度类别间的细微差异，学习更具判别性的文本提示。在15个数据集上的实验结果表明，CaPL显著优于现有的提示学习方法，尤其在细粒度任务上表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Thinker: A vision-language foundation model for embodied intelligence</div>
<div class="meta-line">Authors: Baiyu Pan, Daqin Luo, Junpeng Yang, Jiyuan Wang, Yixuan Zhang, Hailin Shi, Jichao Jiao</div>
<div class="meta-line">Venue: IROS 2025</div>
<div class="meta-line">First: 2026-01-29T02:52:08+00:00 · Latest: 2026-01-29T02:52:08+00:00</div>
<div class="meta-line">Comments: IROS 2025, 4 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21199v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When large vision-language models are applied to the field of robotics, they encounter problems that are simple for humans yet error-prone for models. Such issues include confusion between third-person and first-person perspectives and a tendency to overlook information in video endings during temporal reasoning. To address these challenges, we propose Thinker, a large vision-language foundation model designed for embodied intelligence. We tackle the aforementioned issues from two perspectives. Firstly, we construct a large-scale dataset tailored for robotic perception and reasoning, encompassing ego-view videos, visual grounding, spatial understanding, and chain-of-thought data. Secondly, we introduce a simple yet effective approach that substantially enhances the model&#x27;s capacity for video comprehension by jointly incorporating key frames and full video sequences as inputs. Our model achieves state-of-the-art results on two of the most commonly used benchmark datasets in the field of task planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Thinker：面向具身智能的视觉语言基础模型</div>
<div class="mono" style="margin-top:8px">当大型视觉语言模型应用于机器人领域时，会遇到对人类简单但对模型易错的问题，例如第三人称与第一人称视角混淆、时序推理中易忽略视频末尾信息等。为此，我们提出Thinker——专为具身智能设计的大型视觉语言基础模型。我们从两方面应对上述挑战：首先，构建了面向机器人感知与推理的大规模数据集，包含第一视角视频、视觉定位、空间理解和思维链数据；其次，提出一种简洁有效的方法，通过联合关键帧与完整视频序列作为输入，显著提升模型的视频理解能力。该模型在任务规划领域两个常用基准数据集上取得了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses critical limitations of large vision-language models in robotics, where models struggle with perspective confusion and temporal reasoning by overlooking video endings. The proposed Thinker model tackles these issues by constructing a large-scale dataset for robotic perception and reasoning, and introducing a method that jointly processes key frames and full video sequences to enhance video comprehension. Experimental results demonstrate state-of-the-art performance on two benchmark datasets for task planning.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决大型视觉语言模型在机器人应用中遇到的困难，例如视角混淆和视频时序信息遗漏。方法上提出了Thinker基础模型，构建了针对机器人感知的大规模数据集，并采用结合关键帧与完整视频序列的双输入策略以增强视频理解能力。实验结果表明，Thinker在两个常用任务规划基准测试中取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">FRISM: Fine-Grained Reasoning Injection via Subspace-Level Model Merging for Vision-Language Models</div>
<div class="meta-line">Authors: Chenyu Huang, Peng Ye, Xudong Tan, Jinhan Mu, Shenghe Zheng, Li Shen, Tao Chen</div>
<div class="meta-line">First: 2026-01-29T02:36:19+00:00 · Latest: 2026-01-29T02:36:19+00:00</div>
<div class="meta-line">Comments: 23 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21187v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently enhancing the reasoning capabilities of Vision-Language Models (VLMs) by merging them with Large Reasoning Models (LRMs) has emerged as a promising direction. However, existing methods typically operate at a coarse-grained layer level, which often leads to a trade-off between injecting reasoning capabilities and preserving visual capabilities. To address this limitation, we propose {FRISM} (Fine-grained Reasoning Injection via Subspace-level model Merging), a fine-grained reasoning injection framework based on subspace-level model merging. Observing that reasoning capabilities are encoded in distinct subspaces, FRISM decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace through learning to realize fine-grained reasoning injection. Furthermore, we introduce a label-free self-distillation learning strategy with a dual-objective optimization using common vision-language perception datasets. Extensive experiments demonstrate that FRISM effectively improves reasoning capabilities without compromising the model&#x27;s original visual capabilities by consistently achieving state-of-the-art performance across diverse visual reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FRISM：基于子空间级模型融合的视觉语言模型细粒度推理能力注入</div>
<div class="mono" style="margin-top:8px">通过将视觉语言模型（VLM）与大型推理模型（LRM）融合来高效增强其推理能力已成为重要研究方向。现有方法通常在粗粒度的层级进行操作，往往导致推理能力注入与视觉能力保留之间的权衡困境。为突破此局限，我们提出FRISM（基于子空间级模型融合的细粒度推理注入框架）。通过观测发现推理能力编码于特定子空间，FRISM采用奇异值分解（SVD）解析LRM任务向量，并通过自适应学习调整各子空间的缩放系数，实现细粒度推理注入。此外，我们引入基于通用视觉语言感知数据集的双目标优化无标签自蒸馏学习策略。大量实验表明，FRISM能在不影响模型原始视觉能力的前提下有效提升推理性能，在多种视觉推理基准测试中持续取得最先进的表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the trade-off between injecting reasoning capabilities and preserving visual capabilities in Vision-Language Models (VLMs) when merging them with Large Reasoning Models (LRMs), this work proposes FRISM, a fine-grained reasoning injection framework. The method decomposes LRM task vectors via Singular Value Decomposition (SVD) and adaptively tunes the scaling coefficients of each subspace, combined with a label-free self-distillation learning strategy using common vision-language datasets. Experimental results show that FRISM consistently achieves state-of-the-art performance across diverse visual reasoning benchmarks while effectively preserving the model&#x27;s original visual capabilities.</div>
<div class="mono" style="margin-top:8px">为了提升视觉语言模型的推理能力而不损害其视觉感知能力，本研究针对现有粗粒度模型融合方法常导致推理与视觉能力权衡的问题，提出了细粒度推理注入框架FRISM。该方法通过子空间层级的模型融合实现细粒度推理注入：利用奇异值分解对大型推理模型的任务向量进行分解，并自适应调整各子空间的缩放系数，同时采用无标签自蒸馏学习策略进行双目标优化。大量实验表明，FRISM在多种视觉推理基准测试中均取得了最先进的性能，有效增强了模型的推理能力，同时保持了其原有的视觉能力。</div>
</details>
</div>
<div class="card">
<div class="title">LaTo: Landmark-tokenized Diffusion Transformer for Fine-grained Human Face Editing</div>
<div class="meta-line">Authors: Zhenghao Zhang, Ziying Zhang, Junchao Liao, Xiangyu Meng, Qiang Hu, Siyu Zhu, Xiaoyun Zhang, Long Qin, Weizhi Wang</div>
<div class="meta-line">First: 2025-09-30T03:40:27+00:00 · Latest: 2026-01-29T02:18:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25731v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.25731v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent multimodal models for instruction-based face editing enable semantic manipulation but still struggle with precise attribute control and identity preservation. Structural facial representations such as landmarks are effective for intermediate supervision, yet most existing methods treat them as rigid geometric constraints, which can degrade identity when conditional landmarks deviate significantly from the source (e.g., large expression or pose changes, inaccurate landmark estimates). To address these limitations, we propose LaTo, a landmark-tokenized diffusion transformer for fine-grained, identity-preserving face editing. Our key innovations include: (1) a landmark tokenizer that directly quantizes raw landmark coordinates into discrete facial tokens, obviating the need for dense pixel-wise correspondence; (2) a location-mapped positional encoding and a landmark-aware classifier-free guidance that jointly facilitate flexible yet decoupled interactions among instruction, geometry, and appearance, enabling strong identity preservation; and (3) a landmark predictor that leverages vision-language models to infer target landmarks from instructions and source images, whose structured chain-of-thought improves estimation accuracy and interactive control. To mitigate data scarcity, we curate HFL-150K, to our knowledge the largest benchmark for this task, containing over 150K real face pairs with fine-grained instructions. Extensive experiments show that LaTo outperforms state-of-the-art methods by 7.8% in identity preservation and 4.6% in semantic consistency. Code and dataset will be made publicly available upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LaTo：基于地标标记化的扩散Transformer用于细粒度人脸编辑</div>
<div class="mono" style="margin-top:8px">当前基于指令的人脸编辑多模态模型虽能实现语义操控，但在精确属性控制与身份保持方面仍存在局限。面部地标等结构化表征虽能提供有效的中间监督，但现有方法多将其视为刚性几何约束，当条件地标与源图像显著偏离（如大幅表情/姿态变化、地标估计不准）时易损害身份一致性。为此，我们提出LaTo——一种地标标记化的扩散Transformer，实现细粒度且保持身份的人脸编辑。核心创新包括：（1）地标标记器将原始坐标直接量化为离散面部标记，无需密集像素级对应；（2）位置映射的位置编码与地标感知的无分类器引导机制协同促进指令、几何与外观间灵活解耦的交互，实现强身份保持；（3）利用视觉语言模型从指令和源图像推断目标地标的预测器，其结构化思维链提升估计精度与交互控制。为缓解数据稀缺，我们构建了迄今最大规模基准数据集HFL-150K，包含15万对带细粒度指令的真实人脸数据。大量实验表明，LaTo在身份保持与语义一致性上分别以7.8%和4.6%的优势超越现有最优方法。代码与数据集将在论文录用后公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing multimodal models for instruction-based face editing, which struggle with precise attribute control and identity preservation, especially when conditional landmarks deviate significantly from the source due to large expression or pose changes. The proposed method, LaTo, introduces a landmark-tokenized diffusion transformer featuring a landmark tokenizer that quantizes raw coordinates into discrete facial tokens, a location-mapped positional encoding and landmark-aware classifier-free guidance for flexible interactions, and a landmark predictor leveraging vision-language models for accurate target inference. Key experimental results demonstrate that LaTo outperforms state-of-the-art methods by 7.8% in identity preservation and 4.6% in semantic consistency, validated on a newly curated large-scale benchmark, HFL-150K.</div>
<div class="mono" style="margin-top:8px">为解决基于指令的人脸编辑中精确属性控制和身份保持的难题，本文提出了LaTo，一种基于地标标记化的扩散Transformer模型。该方法通过地标标记器将原始坐标离散化，利用位置映射的位置编码和地标感知的无分类器引导实现几何与外观的灵活交互，并整合了基于视觉语言模型的地标预测器以准确推断目标。在新构建的HFL-150K数据集上的实验表明，LaTo优于现有最先进方法，在身份保持和语义一致性上分别提升了7.8%和4.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Mitigating Modality Bias in Vision-Language Models for Temporal Action Localization</div>
<div class="meta-line">Authors: Jiaqi Li, Guangming Wang, Shuntian Zheng, Minzhe Ni, Xiaoman Lu, Guanghui Ye, Yu Guan</div>
<div class="meta-line">First: 2026-01-28T22:03:46+00:00 · Latest: 2026-01-28T22:03:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21078v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporal Action Localization (TAL) requires identifying both the boundaries and categories of actions in untrimmed videos. While vision-language models (VLMs) offer rich semantics to complement visual evidence, existing approaches tend to overemphasize linguistic priors at the expense of visual performance, leading to a pronounced modality bias. We propose ActionVLM, a vision-language aggregation framework that systematically mitigates modality bias in TAL. Our key insight is to preserve vision as the dominant signal while adaptively exploiting language only when beneficial. To this end, we introduce (i) a debiasing reweighting module that estimates the language advantage-the incremental benefit of language over vision-only predictions-and dynamically reweights language modality accordingly, and (ii) a residual aggregation strategy that treats language as a complementary refinement rather than the primary driver. This combination alleviates modality bias, reduces overconfidence from linguistic priors, and strengthens temporal reasoning. Experiments on THUMOS14 show that our model outperforms state-of-the-art by up to 3.2% mAP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向时序动作定位的视觉语言模型模态偏差缓解研究</div>
<div class="mono" style="margin-top:8px">时序动作定位（TAL）需在未剪辑视频中同时识别动作边界与类别。尽管视觉语言模型（VLMs）能提供丰富语义以补充视觉证据，现有方法往往过度强调语言先验而牺牲视觉性能，导致显著的模态偏差。本文提出ActionVLM——一种系统缓解TAL中模态偏差的视觉语言聚合框架。核心思路是保持视觉信号的主导地位，仅当语言有益时才自适应地利用语言信息。为此，我们引入：（1）去偏重加权模块，通过评估语言优势（即语言相对于纯视觉预测的增量收益）动态调整语言模态权重；（2）残差聚合策略，将语言视为补充性优化而非主要驱动因素。该组合有效缓解模态偏差，降低语言先验导致的过度自信，并增强时序推理能力。在THUMOS14数据集上的实验表明，本模型以最高3.2%的mAP优势超越现有最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the modality bias problem in vision-language models for temporal action localization, where existing methods overly rely on linguistic priors and underutilize visual evidence, degrading performance. The proposed ActionVLM framework mitigates this bias by preserving vision as the dominant signal and adaptively leveraging language through a debiasing reweighting module that estimates the language advantage and a residual aggregation strategy that treats language as a complementary refinement. Experiments on THUMOS14 demonstrate that this approach reduces overconfidence from linguistic priors, strengthens temporal reasoning, and achieves state-of-the-art performance with an improvement of up to 3.2% mAP.</div>
<div class="mono" style="margin-top:8px">本研究针对时序动作定位中视觉-语言模型的模态偏差问题，即现有方法过度依赖语言先验而忽视视觉证据，导致性能下降。提出的ActionVLM框架通过保持视觉为主导信号，并仅在有益时自适应利用语言来缓解此偏差，具体包括一个估计语言优势的去偏重加权模块和一个将语言视为补充优化的残差聚合策略。在THUMOS14上的实验表明，该方法减少了语言先验的过度自信，增强了时序推理能力，并以高达3.2%的mAP提升实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Thinking in Frames: How Visual Context and Test-Time Scaling Empower Video Reasoning</div>
<div class="meta-line">Authors: Chengzu Li, Zanyi Wang, Jiaang Li, Yi Xu, Han Zhou, Huanyu Zhang, Ruichuan An, Dengyang Jiang, Zhaochong An, Ivan Vulić, Serge Belongie, Anna Korhonen</div>
<div class="meta-line">First: 2026-01-28T20:57:55+00:00 · Latest: 2026-01-28T20:57:55+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 3 tables (26 pages, 13 figures, 6 tables including references and appendices)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21037v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.21037v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models have excelled at textual reasoning, but they often struggle with fine-grained spatial understanding and continuous action planning, failing to simulate the dynamics required for complex visual reasoning. In this work, we formulate visual reasoning by means of video generation models, positing that generated frames can act as intermediate reasoning steps between initial states and solutions. We evaluate their capacity in two distinct regimes: Maze Navigation for sequential discrete planning with low visual change and Tangram Puzzle for continuous manipulation with high visual change. Our experiments reveal three critical insights: (1) Robust Zero-Shot Generalization: In both tasks, the model demonstrates strong performance on unseen data distributions without specific finetuning. (2) Visual Context: The model effectively uses visual context as explicit control, such as agent icons and tangram shapes, enabling it to maintain high visual consistency and adapt its planning capability robustly to unseen patterns. (3) Visual Test-Time Scaling: We observe a test-time scaling law in sequential planning; increasing the generated video length (visual inference budget) empowers better zero-shot generalization to spatially and temporally complex paths. These findings suggest that video generation is not merely a media tool, but a scalable, generalizable paradigm for visual reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>框架思维：视觉上下文与测试时缩放如何赋能视频推理</div>
<div class="mono" style="margin-top:8px">视觉语言模型在文本推理方面表现出色，但在细粒度空间理解和连续动作规划方面往往存在困难，难以模拟复杂视觉推理所需的动态过程。本研究通过视频生成模型构建视觉推理框架，提出生成的帧可作为初始状态与解决方案之间的中间推理步骤。我们在两种不同场景中评估其能力：视觉变化较小的序列离散规划任务“迷宫导航”，以及视觉变化较大的连续操作任务“七巧板拼图”。实验揭示了三个关键发现：（1）鲁棒的零样本泛化能力：模型在两项任务中均对未见数据分布展现出强劲性能，无需特定微调。（2）视觉上下文：模型能有效利用视觉上下文（如智能体图标和七巧板形状）作为显式控制，保持高度视觉一致性，并将规划能力稳健地适应于未见模式。（3）视觉测试时缩放：我们在序列规划中观察到测试时缩放定律；增加生成视频长度（视觉推理预算）能显著提升对时空复杂路径的零样本泛化能力。这些发现表明，视频生成不仅是媒体工具，更是一种可扩展、可泛化的视觉推理范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models often lack fine-grained spatial understanding and continuous action planning for complex visual reasoning. This work proposes using video generation models as a reasoning paradigm, where generated frames serve as intermediate steps between initial states and solutions. The method is evaluated on Maze Navigation for discrete planning and Tangram Puzzle for continuous manipulation. Key findings include robust zero-shot generalization to unseen data, effective use of visual context like agent icons for control, and a test-time scaling law where longer generated videos improve generalization to complex spatial-temporal paths.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在文本推理上表现出色，但在细粒度空间理解和连续动作规划方面存在不足，难以模拟复杂视觉推理所需的动态过程。为此，本研究通过视频生成模型构建视觉推理框架，将生成的帧作为初始状态到解决方案的中间推理步骤，并在迷宫导航（序列离散规划）和七巧板拼图（连续操作）两个任务上进行评估。主要实验结果包括：模型在未见数据分布上展现出强大的零样本泛化能力；能有效利用视觉上下文（如代理图标和形状）作为显式控制，保持高视觉一致性并适应新图案；以及观察到测试时缩放定律，即增加生成视频长度（视觉推理预算）能提升对时空复杂路径的零样本泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">CMOOD: Concept-based Multi-label OOD Detection</div>
<div class="meta-line">Authors: Zhendong Liu, Yi Nian, Yuehan Qin, Henry Peng Zou, Li Li, Xiyang Hu, Yue Zhao</div>
<div class="meta-line">First: 2024-11-15T08:15:48+00:00 · Latest: 2026-01-28T20:40:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.13578v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.13578v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can models effectively detect out-of-distribution (OOD) samples in complex, multi-label settings without extensive retraining? Existing OOD detection methods struggle to capture the intricate semantic relationships and label co-occurrences inherent in multi-label settings, often requiring large amounts of training data and failing to generalize to unseen label combinations. While large language models have revolutionized zero-shot OOD detection, they primarily focus on single-label scenarios, leaving a critical gap in handling real-world tasks where samples can be associated with multiple interdependent labels. To address these challenges, we introduce COOD, a novel zero-shot multi-label OOD detection framework. COOD leverages pre-trained vision-language models, enhancing them with a concept-based label expansion strategy and a new scoring function. By enriching the semantic space with both positive and negative concepts for each label, our approach models complex label dependencies, precisely differentiating OOD samples without the need for additional training. Extensive experiments demonstrate that our method significantly outperforms existing approaches, achieving approximately 95% average AUROC on both VOC and COCO datasets, while maintaining robust performance across varying numbers of labels and different types of OOD samples.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CMOOD：基于概念的多标签分布外检测</div>
<div class="mono" style="margin-top:8px">模型如何在无需大量重新训练的情况下，有效检测复杂多标签场景中的分布外样本？现有OOD检测方法难以捕捉多标签场景中固有的复杂语义关系和标签共现模式，通常需要大量训练数据且无法泛化至未见过的标签组合。尽管大语言模型已革新零样本OOD检测领域，但其主要聚焦单标签场景，在处理现实世界中样本可能关联多个相互依赖标签的任务时存在关键空白。为此，我们提出COOD——一种新颖的零样本多标签OOD检测框架。该框架利用预训练视觉语言模型，通过基于概念的标签扩展策略和新型评分函数进行增强。通过为每个标签注入正向与负向概念以丰富语义空间，我们的方法能够建模复杂标签依赖关系，在无需额外训练的前提下精准区分OOD样本。大量实验表明，本方法显著优于现有方案，在VOC和COCO数据集上平均AUROC达到约95%，并在不同标签数量和OOD样本类型下保持稳健性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting out-of-distribution (OOD) samples in multi-label settings, where existing methods fail to capture complex label dependencies and require extensive retraining. The proposed CMOOD framework introduces a zero-shot approach using pre-trained vision-language models, enhanced by a concept-based label expansion strategy and a novel scoring function to model label co-occurrences with both positive and negative concepts. Experimental results show that CMOOD significantly outperforms prior methods, achieving about 95% average AUROC on VOC and COCO datasets while maintaining robustness across varying label counts and OOD sample types.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多标签场景下的分布外样本检测难题，现有方法难以捕捉复杂的标签依赖关系且通常需要大量重新训练。提出的CMOOD框架采用零样本方法，利用预训练的视觉-语言模型，通过基于概念的标签扩展策略和新的评分函数，为每个标签引入正负概念以丰富语义空间并建模标签关系。实验结果表明，CMOOD在VOC和COCO数据集上平均AUROC达到约95%，显著优于现有方法，并在不同标签数量和OOD样本类型下保持鲁棒性能。</div>
</details>
</div>
<div class="card">
<div class="title">BlindSight: Harnessing Sparsity for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Tharun Adithya Srikrishnan, Deval Shah, Timothy Hein, Ahmed Hasssan, Stephen Youn, Steven K. Reinhardt</div>
<div class="meta-line">First: 2025-07-11T23:15:30+00:00 · Latest: 2026-01-28T18:45:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.09071v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.09071v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) enable joint processing of text and images. However, incorporating vision data significantly increases the prompt length, resulting in a longer time to first token (TTFT). This bottleneck can be alleviated by leveraging the inherent sparsity in the attention computation. Analyzing these attention patterns in VLMs when processing a series of images, we observe the absence of inter-image attention in a substantial portion of layers. Based on this, we propose BlindSight: an approach to optimize multi-image VLM inference using an input-template-aware attention sparsity mask with no runtime overhead. We utilize a dataset to derive a prompt-agnostic categorization for attention heads: Dense, Sink, Intra-Image, and Intra-Image+Sink. We develop a Triton-based GPU kernel to leverage this sparsity. BlindSight achieves a 1.8-3.2x speedup in the attention computation (prompt length 36K-300K). BlindSight generalizes across VLMs (Qwen2-VL, Qwen2.5-VL, Gemma 3), with only a 0.78% absolute accuracy degradation on average on multi-image comprehension benchmarks. Finally, we advocate for the design of efficient VLMs that combine BlindSight-inspired sparse and dense layers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BlindSight：利用稀疏性实现高效视觉语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLM）支持文本与图像的联合处理。然而，引入视觉数据会显著增加提示长度，导致首词元生成时间（TTFT）延长。通过利用注意力计算中固有的稀疏性可缓解此瓶颈。分析VLM处理系列图像时的注意力模式，我们发现在大部分层中不存在图像间注意力。基于此，我们提出BlindSight：一种利用输入模板感知的注意力稀疏掩码优化多图像VLM推理的方法，且无需运行时开销。我们通过数据集推导出注意力头的提示无关分类：稠密型、汇点型、图像内型、图像内+汇点型。开发了基于Triton的GPU内核以利用此稀疏性。BlindSight在注意力计算中实现1.8-3.2倍加速（提示长度36K-300K）。该方法可泛化至多种VLM（Qwen2-VL、Qwen2.5-VL、Gemma 3），在多图像理解基准测试中平均仅产生0.78%的绝对准确率下降。最后，我们倡导设计融合BlindSight启发的稀疏层与稠密层的高效VLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the significant increase in time to first token (TTFT) caused by long visual prompts in large vision-language models (VLMs). The method, BlindSight, analyzes attention patterns across multiple images and identifies that many layers lack inter-image attention; it then introduces a prompt-agnostic categorization of attention heads and a specialized GPU kernel to apply an input-template-aware sparsity mask with zero runtime overhead. Experimental results show that BlindSight achieves a 1.8-3.2x speedup in attention computation for prompts of 36K to 300K tokens, generalizes across models like Qwen2-VL and Gemma 3, and incurs only a 0.78% average accuracy drop on multi-image benchmarks.</div>
<div class="mono" style="margin-top:8px">该研究针对大型视觉语言模型（VLM）因长视觉提示导致推理瓶颈、增加首词生成时间（TTFT）的问题。提出的方法BlindSight通过分析处理多图像时各层的注意力模式，发现许多层缺乏图像间注意力；据此设计了一种输入模板感知的注意力稀疏掩码，将注意力头分类为密集、汇点、图像内或图像内+汇点，并利用自定义的Triton GPU内核来利用这种稀疏性，且无运行时开销。实验结果表明，BlindSight在36K至300K令牌的提示上实现了注意力计算1.8-3.2倍的加速，可泛化至Qwen2-VL和Gemma 3等模型，在多图像理解基准测试中平均准确率仅下降0.78%，从而倡导未来VLM设计应结合此类稀疏与密集层。</div>
</details>
</div>
<div class="card">
<div class="title">Open-Vocabulary Functional 3D Human-Scene Interaction Generation</div>
<div class="meta-line">Authors: Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron, Michael J. Black, Yan Zhang</div>
<div class="meta-line">First: 2026-01-28T18:34:25+00:00 · Latest: 2026-01-28T18:34:25+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20835v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &quot;sitting on a sofa&#x27;&#x27;, while supporting fine-grained functional human-scene interactions, e.g., &quot;increasing the room temperature&#x27;&#x27;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放词汇功能性三维人-场景交互生成</div>
<div class="mono" style="margin-top:8px">生成与三维场景进行功能性交互的三维人体，仍然是具身智能、机器人学和交互式内容创作领域的一个开放性问题。核心挑战在于同时理解三维场景中功能元素的语义，以及实现功能感知交互所需的三维人体姿态。现有方法通常缺乏对物体功能性和相应人-场景接触的显式推理，导致生成不自然或功能错误的交互。本文提出FunHSI，一种免训练的功能驱动框架，能够根据开放词汇任务提示生成功能正确的人-场景交互。给定任务提示后，FunHSI通过功能感知接触推理识别场景功能元素，重建其三维几何结构，并通过接触图建模高层级交互。随后利用视觉-语言模型合成执行任务的图像化人体，并估计提议的三维身体与手部姿态。最后通过分阶段优化细化提议的三维身体构型，确保物理合理性与功能正确性。与现有方法相比，FunHSI不仅能合成更合理的通用三维交互（如“坐在沙发上”），还支持细粒度功能性人-场景交互（如“调高室温”）。大量实验表明，FunHSI能在多样化的室内外场景中持续生成功能正确且物理合理的人-场景交互。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to generate 3D humans that functionally interact with 3D scenes, addressing the lack of explicit reasoning over object functionality and human-scene contact in existing methods. The proposed FunHSI framework is training-free and functionality-driven: given an open-vocabulary task prompt, it performs functionality-aware contact reasoning to identify functional scene elements, reconstructs their 3D geometry, models interactions via a contact graph, uses vision-language models to synthesize a human image and estimate 3D poses, and refines the configuration through stage-wise optimization. Experiments show that FunHSI consistently generates functionally correct and physically plausible interactions, such as &quot;sitting on a sofa&quot; and &quot;increasing the room temperature&quot;, across diverse indoor and outdoor scenes.</div>
<div class="mono" style="margin-top:8px">该研究旨在生成与3D场景进行功能交互的3D人体，以解决现有方法因缺乏对物体功能和人与场景接触的显式推理而常产生不合理交互的问题。提出的FunHSI框架无需训练且以功能驱动；它首先从开放词汇任务提示中识别功能场景元素并通过接触图建模交互，然后利用视觉语言模型合成执行任务的人体图像并估计3D姿态，最后通过分阶段优化确保物理合理性。实验结果表明，FunHSI能在多样室内外场景中一致生成功能正确且物理合理的交互，在“坐在沙发上”等一般任务和“调高室温”等细粒度任务上均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering</div>
<div class="meta-line">Authors: Chaodong Tong, Qi Zhang, Chen Li, Lei Jiang, Yanbing Liu</div>
<div class="meta-line">First: 2026-01-01T09:19:39+00:00 · Latest: 2026-01-28T16:05:20+00:00</div>
<div class="meta-line">Comments: 21 pages, 13 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00269v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00269v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Faithfulness hallucinations in VQA occur when vision-language models produce fluent yet visually ungrounded answers, severely undermining their reliability in safety-critical applications. Existing detection methods mainly fall into two categories: external verification approaches relying on auxiliary models or knowledge bases, and uncertainty-driven approaches using repeated sampling or uncertainty estimates. The former suffer from high computational overhead and are limited by external resource quality, while the latter capture only limited facets of model uncertainty and fail to sufficiently explore the rich internal signals associated with the diverse failure modes. Both paradigms thus have inherent limitations in efficiency, robustness, and detection performance. To address these challenges, we propose FaithSCAN: a lightweight network that detects hallucinations by exploiting rich internal signals of VLMs, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features. These signals are fused via branch-wise evidence encoding and uncertainty-aware attention. We also extend the LLM-as-a-Judge paradigm to VQA hallucination and propose a low-cost strategy to automatically generate model-dependent supervision signals, enabling supervised training without costly human labels while maintaining high detection accuracy. Experiments on multiple VQA benchmarks show that FaithSCAN significantly outperforms existing methods in both effectiveness and efficiency. In-depth analysis shows hallucinations arise from systematic internal state variations in visual perception, cross-modal reasoning, and language decoding. Different internal signals provide complementary diagnostic cues, and hallucination patterns vary across VLM architectures, offering new insights into the underlying causes of multimodal hallucinations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FaithSCAN：面向可信视觉问答的模型驱动单轮幻觉检测</div>
<div class="mono" style="margin-top:8px">视觉问答中的忠实性幻觉指视觉语言模型生成流畅但缺乏视觉依据的答案，严重削弱其在安全关键应用中的可靠性。现有检测方法主要分为两类：依赖辅助模型或知识库的外部验证方法，以及采用重复采样或不确定性估计的不确定性驱动方法。前者计算开销大且受限于外部资源质量，后者仅能捕捉模型不确定性的有限维度，未能充分挖掘与多样化失效模式相关的丰富内部信号。这两类范式在效率、鲁棒性和检测性能上均存在固有局限。为应对这些挑战，我们提出FaithSCAN：一种通过利用视觉语言模型的丰富内部信号（包括词元级解码不确定性、中间视觉表征和跨模态对齐特征）进行幻觉检测的轻量级网络。这些信号通过分支证据编码和不确定性感知注意力机制进行融合。我们还将“大语言模型即评判”范式扩展至视觉问答幻觉检测，提出一种低成本策略自动生成模型相关的监督信号，在无需昂贵人工标注的情况下实现监督训练，同时保持高检测精度。在多个视觉问答基准上的实验表明，FaithSCAN在效果和效率上均显著优于现有方法。深入分析揭示幻觉源于视觉感知、跨模态推理和语言解码等系统内部状态变化，不同内部信号提供互补的诊断线索，且幻觉模式随视觉语言模型架构变化，为理解多模态幻觉的成因提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the problem of faithfulness hallucinations in visual question answering, where models generate plausible but visually ungrounded answers, compromising reliability in critical applications. The proposed FaithSCAN method introduces a lightweight network that detects hallucinations by fusing internal signals from vision-language models, including token-level decoding uncertainty, intermediate visual representations, and cross-modal alignment features, using branch-wise evidence encoding and uncertainty-aware attention; it also employs a low-cost strategy to automatically generate supervision signals for training without human labels. Experimental results on multiple VQA benchmarks demonstrate that FaithSCAN significantly surpasses existing methods in both detection effectiveness and efficiency, with in-depth analysis revealing that hallucinations stem from systematic variations in visual perception, cross-modal reasoning, and language decoding, and that different internal signals provide complementary diagnostic cues across model architectures.</div>
<div class="mono" style="margin-top:8px">视觉问答（VQA）中的忠实性幻觉问题，即模型生成流畅但缺乏视觉依据的答案，严重影响了其在安全关键应用中的可靠性。针对现有依赖外部验证或不确定性估计的检测方法在效率、鲁棒性和性能上的固有局限，本研究提出了FaithSCAN，一个轻量级网络，它通过分支证据编码和不确定性感知注意力，融合了视觉语言模型丰富的内部信号，包括令牌级解码不确定性、中间视觉表示和跨模态对齐特征。该工作还扩展了“LLM即法官”范式至VQA幻觉检测，提出了一种低成本的、模型依赖的监督信号自动生成策略，从而无需昂贵的人工标注即可进行监督训练。在多个VQA基准测试上的实验表明，FaithSCAN在检测效果和效率上均显著优于现有方法；深入分析揭示，幻觉源于视觉感知、跨模态推理和语言解码中的系统性内部状态变化，且不同内部信号提供了互补的诊断线索，幻觉模式因VLM架构而异。</div>
</details>
</div>
<div class="card">
<div class="title">bi-modal textual prompt learning for vision-language models in remote sensing</div>
<div class="meta-line">Authors: Pankhi Kashyap, Mainak Singha, Biplab Banerjee</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-28T14:58:14+00:00 · Latest: 2026-01-28T14:58:14+00:00</div>
<div class="meta-line">Comments: Accepted in ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20675v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20675v1">PDF</a> · <a href="https://github.com/ipankhi/BiMoRS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prompt learning (PL) has emerged as an effective strategy to adapt vision-language models (VLMs), such as CLIP, for downstream tasks under limited supervision. While PL has demonstrated strong generalization on natural image datasets, its transferability to remote sensing (RS) imagery remains underexplored. RS data present unique challenges, including multi-label scenes, high intra-class variability, and diverse spatial resolutions, that hinder the direct applicability of existing PL methods. In particular, current prompt-based approaches often struggle to identify dominant semantic cues and fail to generalize to novel classes in RS scenarios. To address these challenges, we propose BiMoRS, a lightweight bi-modal prompt learning framework tailored for RS tasks. BiMoRS employs a frozen image captioning model (e.g., BLIP-2) to extract textual semantic summaries from RS images. These captions are tokenized using a BERT tokenizer and fused with high-level visual features from the CLIP encoder. A lightweight cross-attention module then conditions a learnable query prompt on the fused textual-visual representation, yielding contextualized prompts without altering the CLIP backbone. We evaluate BiMoRS on four RS datasets across three domain generalization (DG) tasks and observe consistent performance gains, outperforming strong baselines by up to 2% on average. Codes are available at https://github.com/ipankhi/BiMoRS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向遥感视觉语言模型的双模态文本提示学习</div>
<div class="mono" style="margin-top:8px">提示学习已成为在有限监督下适配视觉语言模型（如CLIP）至下游任务的有效策略。尽管提示学习在自然图像数据集上展现出强大的泛化能力，但其对遥感影像的迁移性仍待深入探索。遥感数据具有多标签场景、高类内变异性和多样空间分辨率等独特挑战，阻碍了现有提示学习方法的直接应用。现有基于提示的方法常难以识别主导语义线索，且在遥感新类别场景中泛化能力不足。为此，我们提出BiMoRS——一个专为遥感任务设计的轻量级双模态提示学习框架。BiMoRS采用冻结的图像描述模型（如BLIP-2）从遥感图像中提取文本语义摘要，通过BERT分词器进行标记化处理，并与CLIP编码器的高级视觉特征融合。轻量级交叉注意力模块基于融合的文本-视觉表征对可学习查询提示进行条件化处理，在不改变CLIP主干网络的前提下生成情境化提示。我们在四个遥感数据集上针对三个领域泛化任务评估BiMoRS，均观察到稳定的性能提升，平均优于基线方法达2%。代码已开源：https://github.com/ipankhi/BiMoRS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limited transferability of prompt learning from natural images to remote sensing (RS) data, which is challenged by multi-label scenes, high intra-class variability, and diverse spatial resolutions. The proposed method, BiMoRS, is a lightweight bi-modal prompt learning framework that uses a frozen image captioning model to extract textual semantic summaries from RS images, tokenizes them with BERT, and fuses them with CLIP&#x27;s visual features via a cross-attention module to generate contextualized prompts without modifying the CLIP backbone. Experimental results on four RS datasets across three domain generalization tasks show that BiMoRS consistently outperforms strong baselines by up to 2% on average.</div>
<div class="mono" style="margin-top:8px">本研究针对提示学习从自然图像到遥感数据的有限可迁移性问题，该领域面临多标签场景、高类内变异性和多样空间分辨率等挑战。所提出的方法BiMoRS是一个轻量级双模态提示学习框架，它使用冻结的图像描述模型从遥感图像中提取文本语义摘要，通过BERT进行标记化，并将其与CLIP的视觉特征融合；一个交叉注意力模块随后基于这种融合表示条件化可学习的查询提示，且不修改CLIP主干网络。在四个遥感数据集上的三个领域泛化任务实验结果表明，该方法性能持续提升，平均优于强基线达2%。</div>
</details>
</div>
<div class="card">
<div class="title">DeepSeek-OCR 2: Visual Causal Flow</div>
<div class="meta-line">Authors: Haoran Wei, Yaofeng Sun, Yukun Li</div>
<div class="meta-line">First: 2026-01-28T12:46:07+00:00 · Latest: 2026-01-28T12:46:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20552v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20552v1">PDF</a> · <a href="http://github.com/deepseek-ai/DeepSeek-OCR-2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present DeepSeek-OCR 2 to investigate the feasibility of a novel encoder-DeepEncoder V2-capable of dynamically reordering visual tokens upon image semantics. Conventional vision-language models (VLMs) invariably process visual tokens in a rigid raster-scan order (top-left to bottom-right) with fixed positional encoding when fed into LLMs. However, this contradicts human visual perception, which follows flexible yet semantically coherent scanning patterns driven by inherent logical structures. Particularly for images with complex layouts, human vision exhibits causally-informed sequential processing. Inspired by this cognitive mechanism, DeepEncoder V2 is designed to endow the encoder with causal reasoning capabilities, enabling it to intelligently reorder visual tokens prior to LLM-based content interpretation. This work explores a novel paradigm: whether 2D image understanding can be effectively achieved through two-cascaded 1D causal reasoning structures, thereby offering a new architectural approach with the potential to achieve genuine 2D reasoning. Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR-2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepSeek-OCR 2：视觉因果流</div>
<div class="mono" style="margin-top:8px">本文提出DeepSeek-OCR 2，旨在探索一种新型编码器DeepEncoder V2的可行性，该编码器能够根据图像语义动态重排视觉标记。传统视觉语言模型在处理图像时，始终以固定的光栅扫描顺序（左上至右下）和位置编码将视觉标记输入大语言模型，这与人类视觉感知方式相悖——人类视觉遵循由内在逻辑结构驱动的灵活且语义连贯的扫描模式。尤其对于复杂布局的图像，人类视觉展现出基于因果关系的序列处理能力。受此认知机制启发，DeepEncoder V2被设计为赋予编码器因果推理能力，使其在基于大语言模型的内容解读前能智能重排视觉标记。本研究探索了一种新范式：是否可通过两级级联的一维因果推理结构有效实现二维图像理解，从而提供一种有望实现真正二维推理的全新架构方案。代码与模型权重已公开于http://github.com/deepseek-ai/DeepSeek-OCR-2。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of conventional vision-language models that process visual tokens in a fixed raster-scan order, which contradicts the flexible, semantically-driven scanning patterns of human visual perception. The proposed method, DeepSeek-OCR 2, introduces a novel encoder called DeepEncoder V2, which is designed to dynamically reorder visual tokens based on image semantics using causal reasoning capabilities before feeding them to a large language model for interpretation. Experimental findings demonstrate the feasibility of this approach, showing that effective 2D image understanding can be achieved through two cascaded 1D causal reasoning structures, offering a new architectural paradigm for genuine 2D reasoning.</div>
<div class="mono" style="margin-top:8px">本研究针对传统视觉语言模型以固定的光栅扫描顺序处理视觉标记的局限性，这与人类视觉感知中灵活、语义驱动的扫描模式相矛盾。所提出的方法DeepSeek-OCR 2引入了一种名为DeepEncoder V2的新型编码器，该编码器被赋予因果推理能力，能够在将视觉标记输入大型语言模型之前，根据图像语义对其进行动态重排序。实验结果表明，该方法能够实现对图像（尤其是复杂布局图像）的智能、因果信息驱动的顺序处理，并探索了通过级联的一维因果推理结构来实现二维图像理解的新架构范式。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing Open-source World Models</div>
<div class="meta-line">Authors: Robbyant Team, Zelin Gao, Qiuyu Wang, Yanhong Zeng, Jiapeng Zhu, Ka Leong Cheng, Yixuan Li, Hanlin Wang, Yinghao Xu, Shuailei Ma, Yihang Chen, Jie Liu, Yansong Cheng, Yao Yao, Jiayi Zhu, Yihao Meng, Kecheng Zheng, Qingyan Bai, Jingye Chen, Zehong Shen, Yue Yu, Xing Zhu, Yujun Shen, Hao Ouyang</div>
<div class="meta-line">First: 2026-01-28T12:37:01+00:00 · Latest: 2026-01-28T12:37:01+00:00</div>
<div class="meta-line">Comments: Project page: https://technology.robbyant.com/lingbot-world; Code: https://github.com/robbyant/lingbot-world</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20540v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20540v1">PDF</a> · <a href="https://github.com/robbyant/lingbot-world">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present LingBot-World, an open-sourced world simulator stemming from video generation. Positioned as a top-tier world model, LingBot-World offers the following features. (1) It maintains high fidelity and robust dynamics in a broad spectrum of environments, including realism, scientific contexts, cartoon styles, and beyond. (2) It enables a minute-level horizon while preserving contextual consistency over time, which is also known as &quot;long-term memory&quot;. (3) It supports real-time interactivity, achieving a latency of under 1 second when producing 16 frames per second. We provide public access to the code and model in an effort to narrow the divide between open-source and closed-source technologies. We believe our release will empower the community with practical applications across areas like content creation, gaming, and robot learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推进开源世界模型发展</div>
<div class="mono" style="margin-top:8px">我们推出LingBot-World——一个源于视频生成的开源世界模拟器。作为顶级世界模型，LingBot-World具备以下特性：(1) 在广泛环境中保持高保真度与强健动态表现，涵盖写实场景、科学情境、卡通风格等多元领域；(2) 实现分钟级时序预测的同时保持长期上下文一致性，即具备“长期记忆”能力；(3) 支持实时交互，在以每秒16帧生成时延迟低于1秒。我们公开代码与模型，旨在缩小开源与闭源技术间的差距。相信本次发布将赋能社区在内容创作、游戏开发、机器人学习等领域的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to bridge the gap between proprietary and open-source world models by developing a high-fidelity, interactive simulator for diverse applications. The method, named LingBot-World, is built upon a video generation framework to create a world model that maintains robust dynamics across varied environments, ensures long-term contextual consistency over minute-level horizons, and supports real-time interactivity with low latency. Key experimental results demonstrate the model&#x27;s capability to generate consistent, high-fidelity simulations in realistic, scientific, and cartoon-style settings while achieving a latency under one second for 16-frame-per-second output.</div>
<div class="mono" style="margin-top:8px">为弥合开源与闭源世界模型之间的差距，本研究推出了LingBot-World，一个基于视频生成的开源世界模拟器。该方法致力于在多样化环境中实现高保真度的动态模拟，确保分钟级时间跨度的长期上下文一致性，并支持实时交互。主要实验结果表明，该模型在写实、科学和卡通风格等场景中均保持鲁棒性能，能够维持长时间的记忆，并以每秒16帧的速率实现低于1秒的生成延迟，从而为内容创作、游戏和机器人学习等领域提供了实用工具。</div>
</details>
</div>
<div class="card">
<div class="title">AnomalyVFM -- Transforming Vision Foundation Models into Zero-Shot Anomaly Detectors</div>
<div class="meta-line">Authors: Matic Fučka, Vitjan Zavrtanik, Danijel Skočaj</div>
<div class="meta-line">First: 2026-01-28T12:02:58+00:00 · Latest: 2026-01-28T12:02:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20524v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20524v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://maticfuc.github.io/anomaly_vfm/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Zero-shot anomaly detection aims to detect and localise abnormal regions in the image without access to any in-domain training images. While recent approaches leverage vision-language models (VLMs), such as CLIP, to transfer high-level concept knowledge, methods based on purely vision foundation models (VFMs), like DINOv2, have lagged behind in performance. We argue that this gap stems from two practical issues: (i) limited diversity in existing auxiliary anomaly detection datasets and (ii) overly shallow VFM adaptation strategies. To address both challenges, we propose AnomalyVFM, a general and effective framework that turns any pretrained VFM into a strong zero-shot anomaly detector. Our approach combines a robust three-stage synthetic dataset generation scheme with a parameter-efficient adaptation mechanism, utilising low-rank feature adapters and a confidence-weighted pixel loss. Together, these components enable modern VFMs to substantially outperform current state-of-the-art methods. More specifically, with RADIO as a backbone, AnomalyVFM achieves an average image-level AUROC of 94.1% across 9 diverse datasets, surpassing previous methods by significant 3.3 percentage points. Project Page: https://maticfuc.github.io/anomaly_vfm/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnomalyVFM——将视觉基础模型转化为零样本异常检测器</div>
<div class="mono" style="margin-top:8px">零样本异常检测旨在无需任何域内训练图像的情况下检测并定位图像中的异常区域。尽管现有方法利用视觉语言模型（如CLIP）传递高层概念知识，但基于纯视觉基础模型（如DINOv2）的方法在性能上仍显滞后。我们认为这一差距源于两个实际问题：（一）现有辅助异常检测数据集多样性有限；（二）视觉基础模型适应策略过于浅层。为应对这两项挑战，我们提出AnomalyVFM——一个通用且高效的框架，可将任何预训练的视觉基础模型转化为强大的零样本异常检测器。该方法结合了鲁棒的三阶段合成数据集生成方案与参数高效的适应机制，利用低秩特征适配器和置信度加权像素损失。这些组件共同使现代视觉基础模型显著超越当前最优方法。具体而言，以RADIO为骨干网络，AnomalyVFM在9个多样化数据集上实现了94.1%的平均图像级AUROC，较先前方法提升3.3个百分点。项目页面：https://maticfuc.github.io/anomaly_vfm/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the performance gap in zero-shot anomaly detection between vision-language models and vision foundation models (VFMs), attributing it to limited dataset diversity and shallow adaptation strategies. The proposed AnomalyVFM framework introduces a three-stage synthetic dataset generation process and a parameter-efficient adaptation mechanism using low-rank feature adapters with a confidence-weighted pixel loss. Experimental results demonstrate that AnomalyVFM substantially outperforms prior methods, achieving an average image-level AUROC of 94.1% across nine datasets with the RADIO backbone, representing a 3.3 percentage point improvement over previous state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型与视觉基础模型在零样本异常检测中的性能差距问题，将其归因于现有辅助数据集多样性不足和模型适应策略过于浅层。提出的AnomalyVFM框架通过三阶段合成数据集生成方案，结合使用低秩特征适配器和置信度加权像素损失的参数高效适应机制，将任何预训练的视觉基础模型转化为异常检测器。实验结果表明，以RADIO为骨干网络时，AnomalyVFM在九个数据集上平均图像级AUROC达到94.1%，比先前最优方法提升3.3个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">Mixing Importance with Diversity: Joint Optimization for KV Cache Compression in Large Vision-Language Models</div>
<div class="meta-line">Authors: Xuyang Liu, Xiyan Gui, Yuchao Zhang, Linfeng Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-23T16:17:47+00:00 · Latest: 2026-01-28T10:49:58+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026. Our code is available at https://github.com/xuyang-liu16/MixKV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.20707v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.20707v2">PDF</a> · <a href="https://github.com/xuyang-liu16/MixKV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent large vision-language models (LVLMs) demonstrate remarkable capabilities in processing extended multi-modal sequences, yet the resulting key-value (KV) cache expansion creates a critical memory bottleneck that fundamentally limits deployment scalability. While existing KV cache compression methods focus on retaining high-importance KV pairs to minimize storage, they often overlook the modality-specific semantic redundancy patterns that emerge distinctively in multi-modal KV caches. In this work, we first analyze how, beyond simple importance, the KV cache in LVLMs exhibits varying levels of redundancy across attention heads. We show that relying solely on importance can only cover a subset of the full KV cache information distribution, leading to potential loss of semantic coverage. To address this, we propose MixKV, a novel method that mixes importance with diversity for optimized KV cache compression in LVLMs. MixKV adapts to head-wise semantic redundancy, selectively balancing diversity and importance when compressing KV pairs. Extensive experiments demonstrate that MixKV consistently enhances existing methods across multiple LVLMs. Under extreme compression (budget=64), MixKV improves baseline methods by an average of 5.1% across five multi-modal understanding benchmarks and achieves remarkable gains of 8.0% and 9.0% for SnapKV and AdaKV on GUI grounding tasks, all while maintaining comparable inference efficiency. Furthermore, MixKV extends seamlessly to LLMs with comparable performance gains. Our code is available at https://github.com/xuyang-liu16/MixKV.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合重要性与多样性：大型视觉语言模型KV缓存压缩的联合优化</div>
<div class="mono" style="margin-top:8px">近期的大型视觉语言模型在处理长序列多模态数据时展现出卓越能力，但其产生的键值缓存扩展形成了关键的内存瓶颈，从根本上限制了部署可扩展性。现有KV缓存压缩方法主要聚焦于保留高重要性键值对以最小化存储，却常忽视多模态KV缓存中特有的模态语义冗余模式。本研究首先分析了LVLMs中KV缓存如何在注意力头间呈现不同程度的冗余性，并证明仅依赖重要性指标只能覆盖部分KV缓存信息分布，可能导致语义覆盖损失。为此，我们提出MixKV——一种融合重要性与多样性的新型KV缓存压缩方法。MixKV能自适应头级语义冗余，在压缩KV对时选择性平衡多样性与重要性。大量实验表明，MixKV在多种LVLMs上持续提升现有方法性能：在极端压缩条件下，MixKV在五项多模态理解基准上将基线方法平均提升5.1%，在GUI定位任务中为SnapKV和AdaKV分别带来8.0%和9.0%的显著增益，同时保持相当的推理效率。此外，MixKV可无缝扩展至LLMs并取得可比性能提升。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the critical memory bottleneck caused by key-value (KV) cache expansion in large vision-language models (LVLMs) during long multi-modal sequence processing. The proposed MixKV method introduces a novel compression approach that adapts to head-wise semantic redundancy by jointly optimizing for both importance and diversity when selecting KV pairs, rather than relying solely on importance metrics as prior methods do. Experimental results show that under extreme compression (budget=64), MixKV improves baseline methods by an average of 5.1% across five multi-modal understanding benchmarks and achieves gains of 8.0% and 9.0% for specific methods on GUI grounding tasks, while maintaining comparable inference efficiency and demonstrating seamless extension to LLMs.</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在处理长多模态序列时，其键值（KV）缓存的内存快速膨胀构成了关键的部署瓶颈。为解决此问题，本研究提出了MixKV压缩方法，该方法超越了仅选择高重要性KV对的思路，通过联合优化重要性和多样性，以适应不同注意力头中观察到的语义冗余模式。实验结果表明，在极端压缩设置下，MixKV持续提升了现有基线方法的性能，在五个多模态基准测试中平均性能提高5.1%，在GUI定位任务上最高获得9.0%的性能增益，同时保持了推理效率，并证明其可泛化至大型语言模型。</div>
</details>
</div>
<div class="card">
<div class="title">Let&#x27;s Roll a BiFTA: Bi-refinement for Fine-grained Text-visual Alignment in Vision-Language Models</div>
<div class="meta-line">Authors: Yuhao Sun, Chengyi Cai, Jiacheng Zhang, Zesheng Ye, Xingliang Yuan, Feng Liu</div>
<div class="meta-line">First: 2026-01-28T09:24:14+00:00 · Latest: 2026-01-28T09:24:14+00:00</div>
<div class="meta-line">Comments: 25 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20419v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20419v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent research has shown that aligning fine-grained text descriptions with localized image patches can significantly improve the zero-shot performance of pre-trained vision-language models (e.g., CLIP). However, we find that both fine-grained text descriptions and localized image patches often contain redundant information, making text-visual alignment less effective. In this paper, we tackle this issue from two perspectives: \emph{View Refinement} and \emph{Description refinement}, termed as \textit{\textbf{Bi}-refinement for \textbf{F}ine-grained \textbf{T}ext-visual \textbf{A}lignment} (BiFTA). \emph{View refinement} removes redundant image patches with high \emph{Intersection over Union} (IoU) ratios, resulting in more distinctive visual samples. \emph{Description refinement} removes redundant text descriptions with high pairwise cosine similarity, ensuring greater diversity in the remaining descriptions. BiFTA achieves superior zero-shot performance on 6 benchmark datasets for both ViT-based and ResNet-based CLIP, justifying the necessity to remove redundant information in visual-text alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BiFTA：视觉语言模型中细粒度文本-视觉对齐的双向精炼</div>
<div class="mono" style="margin-top:8px">近期研究表明，将细粒度文本描述与局部图像块对齐可显著提升预训练视觉语言模型（如CLIP）的零样本性能。然而，我们发现细粒度文本描述和局部图像块常包含冗余信息，降低了文本-视觉对齐效果。本文从两个角度解决该问题：\emph{视图精炼}与\emph{描述精炼}，统称为\textit{\textbf{细粒度文本-视觉对齐双向精炼}}（BiFTA）。\emph{视图精炼}通过移除具有高\emph{交并比}（IoU）的冗余图像块，获得更具区分度的视觉样本；\emph{描述精炼}通过移除具有高余弦相似度的冗余文本描述，确保剩余描述的多样性。BiFTA在6个基准数据集上为基于ViT和ResNet的CLIP模型均实现了更优的零样本性能，证实了在视觉-文本对齐中消除冗余信息的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the issue of redundant information in fine-grained text descriptions and localized image patches, which hampers effective text-visual alignment in pre-trained vision-language models like CLIP. The proposed method, BiFTA, tackles this from two perspectives: View Refinement removes redundant image patches with high Intersection over Union ratios to create more distinctive visual samples, and Description Refinement eliminates redundant text descriptions with high pairwise cosine similarity to ensure greater diversity. Experimental results demonstrate that BiFTA achieves superior zero-shot performance on six benchmark datasets for both ViT-based and ResNet-based CLIP models, justifying the necessity of removing such redundancies.</div>
<div class="mono" style="margin-top:8px">该研究针对细粒度文本描述和局部图像块常包含冗余信息，从而降低如CLIP等视觉语言模型中文本-视觉对齐效果的问题。提出的BiFTA方法通过两种精炼来解决：视图精炼基于高交并比去除冗余图像块以生成更具区分性的视觉样本，描述精炼利用高余弦相似度去除冗余文本描述以确保更大的多样性。实验结果表明，BiFTA在六个基准数据集上为基于ViT和ResNet的CLIP均实现了卓越的零样本性能，证明了在视觉-文本对齐中去除冗余信息的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">NLPrompt: Noise-Label Prompt Learning for Vision-Language Models</div>
<div class="meta-line">Authors: Bikang Pan, Qun Li, Xiaoying Tang, Wei Huang, Zhen Fang, Feng Liu, Jingya Wang, Jingyi Yu, Ye Shi</div>
<div class="meta-line">First: 2024-12-02T08:25:09+00:00 · Latest: 2026-01-28T08:33:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.01256v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.01256v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of vision-language foundation models, such as CLIP, has revolutionized image-text representation, enabling a broad range of applications via prompt learning. Despite its promise, real-world datasets often contain noisy labels that can degrade prompt learning performance. In this paper, we demonstrate that using mean absolute error (MAE) loss in prompt learning, named PromptMAE, significantly enhances robustness against noisy labels while maintaining high accuracy. Though MAE is straightforward and recognized for its robustness, it is rarely used in noisy-label learning due to its slow convergence and poor performance outside prompt learning scenarios. To elucidate the robustness of PromptMAE, we leverage feature learning theory to show that MAE can suppress the influence of noisy samples, thereby improving the signal-to-noise ratio and enhancing overall robustness. Additionally, we introduce PromptOT, a prompt-based optimal transport data purification method to enhance the robustness further. PromptOT employs text features in vision-language models as prototypes to construct an optimal transportation matrix. This matrix effectively partitions datasets into clean and noisy subsets, allowing for the application of cross-entropy loss to the clean subset and MAE loss to the noisy subset. Our Noise-Label Prompt Learning method, named NLPrompt, offers a simple and efficient approach that leverages the expressive representations and precise alignment capabilities of vision-language models for robust prompt learning. We validate NLPrompt through extensive experiments across various noise settings, demonstrating significant performance improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NLPrompt：面向视觉语言模型的噪声标签提示学习</div>
<div class="mono" style="margin-top:8px">视觉语言基础模型（如CLIP）的出现革新了图文表示，通过提示学习实现了广泛应用。然而，现实数据集常含噪声标签，可能损害提示学习性能。本文提出在提示学习中使用平均绝对误差（MAE）损失（称为PromptMAE），能在保持高精度的同时显著增强对噪声标签的鲁棒性。尽管MAE因其鲁棒性被认可，但由于收敛慢且在非提示学习场景中表现不佳，鲜少用于噪声标签学习。为阐明PromptMAE的鲁棒性，我们借助特征学习理论证明MAE能抑制噪声样本的影响，从而提高信噪比并增强整体鲁棒性。此外，我们提出PromptOT——一种基于提示的最优传输数据净化方法，以进一步提升鲁棒性。PromptOT利用视觉语言模型中的文本特征作为原型构建最优传输矩阵，有效将数据集划分为干净与噪声子集，从而对干净子集应用交叉熵损失，对噪声子集应用MAE损失。我们提出的噪声标签提示学习方法NLPrompt，提供了一种简洁高效的方案，利用视觉语言模型的强表征能力与精准对齐特性实现鲁棒提示学习。通过多种噪声设置下的广泛实验，我们验证了NLPrompt能带来显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the degradation of prompt learning performance in vision-language models like CLIP due to noisy labels in real-world datasets. It proposes NLPrompt, a method combining PromptMAE—which uses mean absolute error loss to suppress noisy sample influence and improve robustness—with PromptOT, a prompt-based optimal transport technique that purifies data by partitioning datasets into clean and noisy subsets for tailored loss application. Experiments across various noise settings show that NLPrompt significantly enhances robustness and maintains high accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对现实数据集中噪声标签导致视觉语言模型（如CLIP）提示学习性能下降的问题，提出了NLPrompt方法。该方法结合了PromptMAE（使用平均绝对误差损失抑制噪声样本影响以提升鲁棒性）和PromptOT（一种基于提示的最优传输数据净化技术，通过划分数据集为干净和噪声子集并应用相应损失函数）。在不同噪声设置下的实验表明，NLPrompt显著提高了鲁棒性并保持了高准确率。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
