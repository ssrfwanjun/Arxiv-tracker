<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 04:49</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0449</div>
    <div class="row"><div class="card">
<div class="title">LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</div>
<div class="meta-line">Authors: Said Taghadouini, Adrien Cavaillès, Baptiste Aubertin</div>
<div class="meta-line">First: 2026-01-20T18:58:32+00:00 · Latest: 2026-01-20T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14251v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LightOnOCR：一款10亿参数端到端多语言视觉语言模型，实现最先进OCR性能</div>
<div class="mono" style="margin-top:8px">我们推出\textbf{LightOnOCR-2-1B}，这是一个拥有10亿参数的端到端多语言视觉语言模型，能够直接将文档图像（如PDF）转换为整洁、自然排序的文本，无需依赖脆弱的OCR流程。该模型通过大规模高质量蒸馏混合数据训练，广泛涵盖扫描文档、法语文档和科学PDF，在OlmOCR-Bench上取得最先进成果，同时体积比先前最佳模型缩小9倍且速度显著提升。我们进一步扩展输出格式以预测嵌入式图像的归一化边界框，通过恢复策略在预训练中引入定位能力，并利用基于IoU奖励的RLVR进行优化。最后，我们通过检查点平均和任务算术融合提升模型鲁棒性。模型检查点按Apache 2.0协议发布，数据集及\textbf{LightOnOCR-bbox-bench}评估工具按各自许可公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to overcome the brittleness of traditional OCR pipelines by developing an end-to-end multilingual vision-language model that directly extracts clean, naturally ordered text from document images like PDFs. The method introduces LightOnOCR-2-1B, a 1-billion-parameter model trained on a large-scale, high-quality distillation dataset with strong coverage of scans, French documents, and scientific PDFs; it extends output to predict normalized bounding boxes for embedded images through a resume strategy for localization pretraining and RLVR refinement with IoU-based rewards, while also employing checkpoint averaging and task-arithmetic merging for robustness. Key experimental results show that the model achieves state-of-the-art performance on OlmOCR-Bench while being 9 times smaller and substantially faster than prior best models, and it is released alongside a new evaluation benchmark, LightOnOCR-bbox-bench.</div>
<div class="mono" style="margin-top:8px">该研究旨在克服传统OCR流程的脆弱性，开发一个端到端的多语言视觉-语言模型，直接从PDF等文档图像中提取干净、自然排序的文本。方法提出了LightOnOCR-2-1B，这是一个10亿参数的模型，基于大规模高质量蒸馏数据集训练，该数据集广泛覆盖扫描件、法语文档和科学PDF；模型扩展了输出格式以预测嵌入图像的归一化边界框，通过恢复策略进行定位预训练，并使用基于IoU奖励的RLVR进行细化，同时采用检查点平均和任务算术合并来提升鲁棒性。主要实验结果表明，该模型在OlmOCR-Bench上达到了最先进的性能，同时比先前最佳模型小9倍且速度显著更快，并在新发布的LightOnOCR-bbox-bench上评估了其图像定位能力。</div>
</details>
</div>
<div class="card">
<div class="title">IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</div>
<div class="meta-line">Authors: Liang Shi, Wei Li, Kevin M Beussman, Lin Chen, Yun Fu</div>
<div class="meta-line">First: 2026-01-20T17:45:24+00:00 · Latest: 2026-01-20T17:45:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14188v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM&#x27;s efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IIR-VLM：面向大型视觉语言模型的上下文实例级识别</div>
<div class="mono" style="margin-top:8px">实例级识别（ILR）关注于区分不同个体实例，行人重识别是其典型应用。尽管现代视觉语言模型（VLM）具备出色的视觉感知能力，但其在ILR任务上的表现不尽如人意，常显著落后于领域专用ILR模型。这一局限阻碍了VLM在许多实际场景中的应用，例如在需要识别熟悉人物或物体以实现有效视觉理解的场景中。现有方案通常依赖实例专用数据集逐个学习识别实例，不仅需承担高昂的数据收集与训练成本，且在细粒度区分上存在困难。本研究提出IIR-VLM——一种增强上下文实例级识别能力的VLM。我们集成预训练的ILR专家模型作为辅助视觉编码器，为学习多样化实例提供专业化特征，使VLM能够以单样本方式在上下文中学习新实例。进一步，IIR-VLM利用该知识实现实例感知的视觉理解。我们在现有实例个性化基准测试中验证了IIR-VLM的有效性，最后通过涵盖行人、人脸、宠物及通用物体等多类别、多难度层级的挑战性新基准测试，展示了其卓越的ILR性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the unsatisfactory performance of modern Vision-Language Models (VLMs) on instance-level recognition (ILR) tasks, such as person re-identification, where they significantly underperform domain-specific models, limiting their practical application. The proposed method, IIR-VLM, enhances VLMs by integrating pre-trained ILR expert models as auxiliary visual encoders to provide specialized features, enabling the VLM to learn new instances in-context in a one-shot manner and perform instance-aware visual understanding. Key experimental results demonstrate the model&#x27;s efficacy on existing instance personalization benchmarks and its superior ILR performance on a new challenging benchmark that assesses capabilities across varying difficulty and diverse categories including persons, faces, pets, and general objects.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，现代视觉-语言模型在实例级识别任务（如行人重识别）上表现不佳，显著落后于领域专用模型，这限制了需要识别熟悉个体或物体的实际应用。所提出的方法IIR-VLM通过集成预训练的实例级识别专家模型作为辅助视觉编码器来提供专业化特征，从而增强视觉-语言模型，使其能够以单样本上下文学习的方式学习新实例，并利用这些知识进行实例感知的视觉理解。关键实验结果验证了该模型在现有实例个性化基准上的有效性，以及在一个新的具有挑战性的基准上的优越实例级识别性能，该基准评估了跨不同难度和多样类别（包括人物、人脸、宠物和一般物体）的识别能力。</div>
</details>
</div>
<div class="card">
<div class="title">TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers</div>
<div class="meta-line">Authors: Bin Yu, Shijie Lian, Xiaopeng Lin, Yuliang Wei, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Xinming Wang, Bailing Wang, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-20T16:30:07+00:00 · Latest: 2026-01-20T16:30:07+00:00</div>
<div class="meta-line">Comments: GitHub: https://github.com/ZGC-EmbodyAI/TwinBrainVLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14133v1">PDF</a> · <a href="https://github.com/ZGC-EmbodyAI/TwinBrainVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to &quot;catastrophic forgetting&quot; of the model&#x27;s open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen &quot;Left Brain&quot;, which retains robust general visual reasoning, with a trainable &quot;Right Brain&quot;, specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TwinBrainVLA：通过非对称混合Transformer架构释放通用视觉语言模型在具身任务中的潜力</div>
<div class="mono" style="margin-top:8px">标准视觉-语言-动作模型通常通过微调单一视觉语言模型主干来适配机器人控制任务，但这种方法在保持高层语义理解与学习低层精细感知运动技能之间存在根本矛盾，常导致模型对开放世界能力的灾难性遗忘。为解决该问题，我们提出TwinBrainVLA——一种协调通用视觉语言模型与专用具身感知模型的新型架构。该架构通过非对称混合Transformer机制，将保持通用视觉推理能力的冻结“左脑”与专攻具身感知的可训练“右脑”相协同，使右脑能动态查询左脑的语义知识并与本体感知状态融合，为流匹配动作专家生成精确连续控制提供丰富条件。在SimplerEnv和RoboCasa基准测试中的实验表明，TwinBrainVLA在保持预训练模型全面视觉理解能力的同时，其操作性能显著优于现有先进基线，为构建兼具高层语义理解与低层物理操作能力的通用机器人提供了新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Standard Vision-Language-Action models face a conflict between preserving general semantic knowledge and learning fine-grained robotic control, often causing catastrophic forgetting. To resolve this, TwinBrainVLA introduces an asymmetric architecture coordinating a frozen generalist VLM for semantic understanding and a trainable specialist VLM for embodied perception via an Asymmetric Mixture-of-Transformers mechanism, which dynamically fuses semantic knowledge with proprioceptive states to condition a Flow-Matching Action Expert for control generation. Experiments on SimplerEnv and RoboCasa benchmarks show the model achieves superior manipulation performance while explicitly preserving the pre-trained VLM&#x27;s visual understanding capabilities.</div>
<div class="mono" style="margin-top:8px">标准的视觉-语言-动作模型在保持预训练获得的高级语义理解与学习机器人控制所需的低级感知运动技能之间存在冲突，常导致灾难性遗忘。为解决此问题，TwinBrainVLA提出了一种非对称架构，协调一个冻结的通用视觉语言模型（左脑）保持通用语义理解，以及一个可训练的专用视觉语言模型（右脑）负责具身本体感知，两者通过非对称混合Transformer机制连接，使右脑能动态查询左脑的语义知识并与本体状态融合，从而为流匹配动作专家生成精确的连续控制提供条件。在SimplerEnv和RoboCasa基准测试上的实验表明，该模型在实现优于最先进基线的操作性能的同时，明确保留了预训练视觉语言模型的全面视觉理解能力。</div>
</details>
</div>
<div class="card">
<div class="title">DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning</div>
<div class="meta-line">Authors: Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra Dagtas, Dilara Ilhan Erdil, Gulsum Gencoglan, Burak Temelkuran</div>
<div class="meta-line">First: 2026-01-20T15:44:57+00:00 · Latest: 2026-01-20T15:44:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14084v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14084v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DermaBench：面向皮肤病学视觉问答与推理的临床专家标注基准数据集</div>
<div class="mono" style="margin-top:8px">视觉语言模型在医疗应用中的重要性日益凸显，但其在皮肤病学领域的评估仍受限于主要关注皮损识别等图像级分类任务的数据集。此类数据集虽对识别任务具有价值，却无法全面评估多模态模型的视觉理解、语言关联及临床推理能力。需通过视觉问答基准来评估模型如何解读皮肤病学图像、对细粒度形态特征进行推理，并生成具有临床意义的描述。我们基于多样化皮肤病图像数据集构建了临床专家标注的皮肤病学视觉问答基准DermaBench。该数据集涵盖570名独特患者的656张临床图像，覆盖Fitzpatrick皮肤分型I-VI级。通过包含22个主要问题的分层标注框架，皮肤科专家对每张图像的诊断、解剖部位、皮损形态、分布、表面特征、颜色及图像质量进行标注，同时提供开放式叙述性描述与总结，共产生约14,474条视觉问答式标注。为遵循上游许可协议，DermaBench以纯元数据形式发布，可通过哈佛Dataverse公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to move beyond simple image classification in evaluating vision-language models for dermatology, as existing datasets fail to assess comprehensive visual understanding and clinical reasoning. The method involves creating DermaBench, a clinician-annotated visual question answering benchmark built on the Diverse Dermatology Images dataset, using a hierarchical schema with 22 main question types to capture diagnosis, morphology, and other clinical details. Key experimental findings include the dataset&#x27;s composition of 656 images from 570 patients across Fitzpatrick skin types I-VI, yielding approximately 14,474 VQA-style annotations to support evaluation of multimodal reasoning in dermatology.</div>
<div class="mono" style="margin-top:8px">该研究的动机是，需要超越简单的图像分类，以评估视觉语言模型在皮肤病学中的全面视觉理解、语言基础和临床推理能力。其方法是在多样化皮肤病图像数据集基础上，通过采用包含22个主要问题类型的分层标注模式来创建临床医生标注的视觉问答基准DermaBench，以捕捉诊断和形态学细节。主要的实验成果是创建了一个公开可用的基准数据集，包含656张临床图像和约14,474个专家标注，涵盖多种皮肤类型，旨在评估模型在细粒度皮肤病学推理任务上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology</div>
<div class="meta-line">Authors: Kaiyu Wu, Pucheng Han, Hualong Zhang, Naigeng Wu, Keze Wang</div>
<div class="meta-line">First: 2026-01-20T15:00:15+00:00 · Latest: 2026-01-20T15:00:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14044v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14044v1">PDF</a> · <a href="https://github.com/Marcowky/Weather-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model&#x27;s reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Weather-R1：面向气象学多模态推理的逻辑一致性强化微调</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型在推理能力上不断进步，但其在气象学领域的应用仍受限于领域鸿沟与推理忠实性差距。具体而言，主流强化微调方法可能引发自相矛盾推理，即模型推理过程与其最终结论相悖，这在气象等高风险领域是不可接受的。为应对这些挑战，我们构建了WeatherQA——一个新颖的气象学多模态推理基准数据集，并提出逻辑一致性强化微调方法，通过引入逻辑一致性奖励机制解决自相矛盾问题。此外，我们首次推出具备逻辑忠实性的气象推理视觉语言模型Weather-R1。实验表明，Weather-R1在WeatherQA基准上的性能较基线提升9.8个百分点，优于监督微调与强化微调方法，甚至超越原始Qwen2.5-VL-32B模型。这些结果凸显了逻辑一致性强化微调的有效性及Weather-R1的优越性。我们的基准数据集与代码已开源：https://github.com/Marcowky/Weather-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses two critical gaps in applying Vision Language Models (VLMs) to meteorology: a domain-specific knowledge gap and a reasoning faithfulness gap where standard Reinforcement Fine-Tuning (RFT) can cause self-contradictory reasoning. To tackle this, the authors first construct WeatherQA, a novel multimodal reasoning benchmark for meteorology, and then propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), a method that introduces a logical consistency reward to eliminate contradictory reasoning. The resulting model, Weather-R1, demonstrates a 9.8 percentage point improvement over the baseline on WeatherQA, outperforming both Supervised Fine-Tuning and standard RFT, and even surpassing the performance of the larger original Qwen2.5-VL-32B model.</div>
<div class="mono" style="margin-top:8px">本研究旨在将视觉语言模型应用于高风险的气象推理领域，现有模型在此存在领域差距和推理忠实性差距，特别是存在自我矛盾推理问题，即推理过程与最终答案相矛盾。为解决这一问题，作者构建了WeatherQA基准数据集，并提出了逻辑一致强化微调方法，该方法在强化微调中引入逻辑一致性奖励以解决矛盾。实验结果表明，由此得到的Weather-R1模型在WeatherQA上的性能比基线提高了9.8个百分点，优于监督微调和标准强化微调，甚至超越了原始的Qwen2.5-VL-32B模型。</div>
</details>
</div>
<div class="card">
<div class="title">HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs</div>
<div class="meta-line">Authors: Yuezhe Yang, Hao Wang, Yige Peng, Jinman Kim, Lei Bi</div>
<div class="meta-line">First: 2026-01-20T12:48:09+00:00 · Latest: 2026-01-20T12:48:09+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13919v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13919v1">PDF</a> · <a href="https://github.com/Bean-Young/HyperWalker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \textbf{HyperWalker}, a \textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HyperWalker：基于动态超图的深度诊断框架，用于跨电子健康记录与X射线的医学视觉语言模型多跳临床建模</div>
<div class="mono" style="margin-top:8px">自动化临床诊断仍是医疗AI的核心挑战，通常需要模型整合多模态数据并在复杂的病例特定情境中进行推理。尽管近期方法通过医学视觉语言模型（VLMs）推进了医学报告生成（MRG）和视觉问答（VQA），但这些方法主要采用样本隔离的推理范式，即独立处理病例而无法访问纵向电子健康记录（EHR）或结构相关的患者案例。该范式将推理局限于图像衍生信息，忽略了外部补充医学证据对提升诊断准确性的潜力。为突破此限制，我们提出**HyperWalker**——一种通过动态超图和测试时训练重构临床推理的**深度诊断**框架。首先，我们构建名为**iBrochure**的动态超图，以建模EHR数据的结构异质性与多模态临床信息间的高阶隐式关联。在此超图中，强化学习智能体**Walker**通过导航识别最优诊断路径。为确保测试样本中多样临床特征的全面覆盖，我们引入**徘徊机制**——一种多跳正交检索策略，通过迭代选择反映不同临床属性的互补邻域病例。在MIMIC数据集上的MRG实验及EHRXQA的医学VQA实验表明，HyperWalker实现了最先进的性能。代码发布于：https://github.com/Bean-Young/HyperWalker</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automated clinical diagnosis requires integrating multimodal data and reasoning across complex contexts, but existing medical vision-language models often process cases independently without leveraging longitudinal electronic health records (EHRs) or related patient examples, limiting diagnostic accuracy. To address this, HyperWalker introduces a deep diagnosis framework that constructs a dynamic hypergraph, iBrochure, to model heterogeneous EHR data and high-order associations, and employs a reinforcement learning agent, Walker, to navigate and identify optimal diagnostic paths; it also incorporates a linger mechanism for multi-hop orthogonal retrieval to iteratively select clinically complementary neighborhood cases. Experiments on medical report generation with MIMIC and visual question answering on EHRXQA show that HyperWalker achieves state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">自动化临床诊断通常需要整合多模态数据并在复杂情境中进行推理，但现有的医学视觉语言模型通常孤立处理病例，未能利用纵向电子健康记录（EHR）或相关患者示例，从而限制了诊断准确性。为解决这一问题，HyperWalker提出了一个深度诊断框架，该框架构建了一个动态超图iBrochure，以建模EHR数据的结构异质性和多模态临床信息间的高阶关联；随后，一个强化学习智能体Walker在此超图中导航以识别最优诊断路径，并辅以一种徘徊机制，迭代检索具有临床互补性的邻域病例。在MIMIC上的医学报告生成和EHRXQA上的医学视觉问答实验表明，HyperWalker实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重访多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前视觉表征学习仍呈两极分化：视觉语言模型（如CLIP）擅长全局语义对齐但缺乏空间精度，而自监督方法（如MAE、DINO）能捕捉精细局部结构却难以处理高层语义上下文。我们认为这些范式本质互补，可通过密集空间监督整合为原则性多任务框架。本文提出MTV——一个多任务视觉预训练框架，通过视觉语言对比、自监督和密集空间目标联合优化共享主干网络。为减少人工标注需求，我们利用高性能“专家”模型（如Depth Anything V2和OWLv2）大规模合成结构化密集伪标签。除框架外，我们系统探究了多任务视觉学习机制，分析：（1）各目标的边际增益，（2）任务协同与干扰效应，（3）不同数据与模型规模下的扩展规律。实验表明MTV实现“两全其美”性能，在保持全局语义理解的同时显著提升细粒度空间推理能力。研究指出：基于高质量伪监督的多任务学习是构建通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the complementary limitations of current visual representation learning paradigms: vision-language models like CLIP capture global semantics but lack spatial precision, while self-supervised methods like MAE and DINO capture local structures but struggle with high-level semantics. The authors propose MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone with vision-language contrastive, self-supervised, and dense spatial objectives, using high-capacity expert models to generate dense pseudo-labels and avoid manual annotation. Experimental analysis shows that MTV achieves a &#x27;best-of-both-worlds&#x27; performance, significantly improving fine-grained spatial reasoning without harming global semantic understanding, and provides systematic insights into task synergies, interference, and scaling behavior.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉表示学习中的分野问题：视觉语言模型（如CLIP）擅长全局语义对齐但缺乏空间精度，而自监督方法（如MAE、DINO）能捕捉局部细节却难以理解高层语义。作者提出MTV多任务视觉预训练框架，通过联合优化共享主干网络，整合视觉语言对比、自监督和密集空间监督目标，并利用高性能专家模型生成密集伪标签以避免人工标注。实验结果表明，MTV实现了“两者兼优”的性能，在保持全局语义理解的同时显著提升了细粒度空间推理能力，系统分析揭示了各目标的边际收益、任务协同效应及在不同数据与模型规模下的扩展规律。</div>
</details>
</div>
<div class="card">
<div class="title">DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes</div>
<div class="meta-line">Authors: Aisha Al-Mohannadi, Ayisha Firoz, Yin Yang, Muhammad Imran, Ferda Ofli</div>
<div class="meta-line">First: 2026-01-20T10:50:46+00:00 · Latest: 2026-01-20T10:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13839v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DisasterVQA：面向灾害场景的视觉问答基准数据集</div>
<div class="mono" style="margin-top:8px">社交媒体图像为自然灾害和人为灾害期间提供了低延迟的态势信息源，支持快速损害评估与应急响应。尽管视觉问答在通用领域表现出色，但其对灾害响应所需的复杂安全关键推理的适用性尚不明确。本文提出DisasterVQA——专为危机情境感知与推理设计的基准数据集，包含1,395张真实灾害图像和4,405组专家标注的问答对，涵盖洪水、野火、地震等多元灾害事件。基于FEMA ESF和OCHA MIRA等人道主义框架，数据集涵盖二元选择、多项选择及开放式问题，涉及态势感知与行动决策任务。通过对七种前沿视觉语言模型的基准测试，发现模型在不同问题类型、灾害类别、地域及人道任务间存在性能差异：虽在二元问题上准确率高，但在细粒度定量推理、物体计数和情境敏感理解方面表现欠佳，对代表性不足的灾害场景尤为明显。DisasterVQA为开发更鲁棒且具操作意义的灾害响应视觉语言模型提供了具有挑战性的实用基准。数据集已公开于https://zenodo.org/records/18267770。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to leverage social media imagery for rapid situational awareness during disasters, but existing Visual Question Answering (VQA) systems are not well-evaluated for the complex reasoning required in such safety-critical contexts. The method involves creating DisasterVQA, a benchmark dataset with 1,395 real-world disaster images and 4,405 expert-curated question-answer pairs grounded in humanitarian frameworks, covering diverse events and question types to test perception and reasoning. Experimental results from benchmarking seven state-of-the-art vision-language models show high accuracy on binary questions but significant struggles with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, especially for underrepresented disaster scenarios, highlighting the dataset&#x27;s challenge and practical utility.</div>
<div class="mono" style="margin-top:8px">为评估视觉问答（VQA）在灾害响应中支持复杂推理的潜力，其中社交媒体图像提供低延迟的态势信息，本研究引入了基于FEMA ESF和OCHA MIRA等人道主义框架的基准数据集DisasterVQA。该方法通过整理1,395张真实灾害场景图像和4,405个专家构建的问答对，涵盖洪水、野火等多种事件，包含针对态势感知和操作决策的二元、多项选择和开放式问题。对七个最先进的视觉语言模型进行实验基准测试显示，它们在二元问题上准确率较高，但在细粒度定量推理、物体计数和上下文敏感解释方面存在显著困难，尤其对于代表性不足的灾害场景，这凸显了该数据集对开发更鲁棒模型的挑战性。</div>
</details>
</div>
<div class="card">
<div class="title">CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction</div>
<div class="meta-line">Authors: Xianghui Xie, Bowen Wen, Yan Chang, Hesam Rabeti, Jiefeng Li, Ye Yuan, Gerard Pons-Moll, Stan Birchfield</div>
<div class="meta-line">First: 2025-12-12T19:11:11+00:00 · Latest: 2026-01-20T10:16:00+00:00</div>
<div class="meta-line">Comments: 14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11988v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11988v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nvlabs.github.io/CARI4D/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARI4D：类别无关的人-物交互四维重建</div>
<div class="mono" style="margin-top:8px">从RGB相机等泛在传感器准确捕捉人-物交互对于人类行为理解、游戏和机器人学习应用至关重要。然而，由于物体与人体信息未知、深度歧义、遮挡及复杂运动等因素，从单目RGB视图推断四维交互极具挑战，这阻碍了三维与时间一致性的重建。现有方法通过假设真实物体模板或限定物体类别范围来简化设置。我们提出首个类别无关方法CARI4D，可从单目RGB视频重建具有空间与时间一致性的公制尺度四维人-物交互。为此，我们设计了一种姿态假设选择算法，鲁棒整合基础模型的独立预测结果，通过可学习的渲染-比对范式进行联合优化以确保空间、时间与像素对齐，最终推理复杂接触关系以进一步满足物理约束的精细化重建。实验表明，本方法在分布内数据集的重建误差降低38%，在未见数据集上降低36%。模型可泛化至训练类别之外，实现零样本应用于真实网络视频。代码与预训练模型将公开释放。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of reconstructing 4D human-object interactions from monocular RGB videos, which is difficult due to unknown object and human information, depth ambiguity, occlusion, and complex motion. The proposed method, CARI4D, introduces a category-agnostic approach that integrates predictions from foundation models using a pose hypothesis selection algorithm and jointly refines them through a learned render-and-compare paradigm to ensure spatial, temporal, and pixel alignment, while also reasoning about intricate contacts under physical constraints. Experimental results demonstrate that CARI4D outperforms prior methods by 38% on in-distribution datasets and 36% on unseen datasets in terms of reconstruction error, and it generalizes zero-shot to in-the-wild internet videos beyond training categories.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从单目RGB视频重建4D人-物交互的挑战，该任务因未知的物体/人体信息、深度模糊和遮挡而变得复杂。提出的CARI4D方法采用了一种类别无关的框架，通过姿态假设选择算法整合基础模型的预测，并利用学习的渲染-比较范式进行联合优化，以确保空间、时间和像素对齐，同时推理物理接触。实验结果表明，该方法在分布内数据集上的重建误差比先前技术降低了38%，在未见数据集上降低了36%，并且能够零样本泛化到真实世界的网络视频中。</div>
</details>
</div>
<div class="card">
<div class="title">PREGEN: Uncovering Latent Thoughts in Composed Video Retrieval</div>
<div class="meta-line">Authors: Gabriele Serussi, David Vainshtein, Jonathan Kouchly, Dotan Di Castro, Chaim Baskin</div>
<div class="meta-line">First: 2026-01-20T09:57:04+00:00 · Latest: 2026-01-20T09:57:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13797v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13797v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Composed Video Retrieval (CoVR) aims to retrieve a video based on a query video and a modifying text. Current CoVR methods fail to fully exploit modern Vision-Language Models (VLMs), either using outdated architectures or requiring computationally expensive fine-tuning and slow caption generation. We introduce PREGEN (PRE GENeration extraction), an efficient and powerful CoVR framework that overcomes these limitations. Our approach uniquely pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for any VLM fine-tuning. We feed the query video and modifying text into the VLM and extract the hidden state of the final token from each layer. A simple encoder is then trained on these pooled representations, creating a semantically rich and compact embedding for retrieval. PREGEN significantly advances the state of the art, surpassing all prior methods on standard CoVR benchmarks with substantial gains in Recall@1 of +27.23 and +69.59. Our method demonstrates robustness across different VLM backbones and exhibits strong zero-shot generalization to more complex textual modifications, highlighting its effectiveness and semantic capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PREGEN：揭示组合视频检索中的潜在思维</div>
<div class="mono" style="margin-top:8px">组合视频检索（CoVR）旨在根据查询视频和修改文本检索目标视频。现有CoVR方法未能充分利用现代视觉语言模型（VLM），或采用过时架构，或需计算成本高昂的微调与缓慢的标题生成。我们提出PREGEN（预生成提取）——一种高效强大的CoVR框架，通过冻结预训练VLM与轻量编码模型的独特组合，无需任何VLM微调。该方法将查询视频与修改文本输入VLM，提取各网络层最终词元的隐藏状态，并通过简单编码器训练这些池化表征，生成语义丰富且紧凑的检索嵌入。PREGEN在标准CoVR基准测试中显著超越所有现有方法，Recall@1指标分别提升+27.23和+69.59。该方法在不同VLM骨干网络中均表现稳健，并对复杂文本修改展现出强大的零样本泛化能力，凸显了其语义理解效能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of current Composed Video Retrieval (CoVR) methods, which either use outdated architectures or require computationally expensive fine-tuning of Vision-Language Models (VLMs) for caption generation. The proposed method, PREGEN, introduces an efficient framework that pairs a frozen, pre-trained VLM with a lightweight encoding model, eliminating the need for VLM fine-tuning. It extracts the hidden states of the final token from each VLM layer after processing the query video and modifying text, then trains a simple encoder on these pooled representations to create a compact retrieval embedding. Experiments show PREGEN significantly advances the state of the art, achieving substantial gains of +27.23 and +69.59 in Recall@1 on standard benchmarks, while demonstrating robustness across VLM backbones and strong zero-shot generalization to complex textual modifications.</div>
<div class="mono" style="margin-top:8px">组合视频检索（CoVR）旨在通过查询视频和修改文本来检索目标视频，但现有方法因架构过时或微调成本高昂而未能充分利用现代视觉语言模型（VLM）。为此，我们提出了PREGEN框架，它将一个冻结的预训练VLM与一个轻量级编码器结合；该方法在处理查询视频和文本时，提取VLM所有层中最后一个标记的隐藏状态，然后在这些池化表示上训练编码器以生成用于检索的紧凑嵌入。实验表明，PREGEN显著提升了当前最优性能，在标准CoVR基准测试中Recall@1指标分别大幅提升了+27.23和+69.59，同时在不同VLM骨干网络上表现出鲁棒性，并对复杂的文本修改具备强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Long Video Understanding with Audiovisual Entity Cohesion and Agentic Search</div>
<div class="meta-line">Authors: Xinlei Yin, Xiulian Peng, Xiao Li, Zhiwei Xiong, Yan Lu</div>
<div class="meta-line">First: 2026-01-20T08:23:29+00:00 · Latest: 2026-01-20T08:23:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13719v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13719v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long video understanding presents significant challenges for vision-language models due to extremely long context windows. Existing solutions relying on naive chunking strategies with retrieval-augmented generation, typically suffer from information fragmentation and a loss of global coherence. We present HAVEN, a unified framework for long-video understanding that enables coherent and comprehensive reasoning by integrating audiovisual entity cohesion and hierarchical video indexing with agentic search. First, we preserve semantic consistency by integrating entity-level representations across visual and auditory streams, while organizing content into a structured hierarchy spanning global summary, scene, segment, and entity levels. Then we employ an agentic search mechanism to enable dynamic retrieval and reasoning across these layers, facilitating coherent narrative reconstruction and fine-grained entity tracking. Extensive experiments demonstrate that our method achieves good temporal coherence, entity consistency, and retrieval efficiency, establishing a new state-of-the-art with an overall accuracy of 84.1% on LVBench. Notably, it achieves outstanding performance in the challenging reasoning category, reaching 80.1%. These results highlight the effectiveness of structured, multimodal reasoning for comprehensive and context-consistent understanding of long-form videos.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视听实体连贯性与智能搜索的层次化长视频理解</div>
<div class="mono" style="margin-top:8px">长视频理解因极长的上下文窗口对视觉-语言模型构成重大挑战。现有基于简单分块策略与检索增强生成的方法通常存在信息碎片化和全局连贯性缺失的问题。我们提出HAVEN统一框架，通过整合视听实体连贯性、层次化视频索引与智能搜索，实现连贯全面的长视频推理。首先，我们通过融合视觉与听觉流中的实体级表征保持语义一致性，并将内容组织为涵盖全局摘要、场景、片段和实体层级的结构化层次。随后采用智能搜索机制实现跨层动态检索与推理，促进连贯叙事重建与细粒度实体追踪。大量实验表明，该方法在时间连贯性、实体一致性和检索效率方面表现优异，在LVBench基准上以84.1%的整体准确率创下新标杆，尤其在挑战性推理类别中达到80.1%的突出性能。这些结果凸显了结构化多模态推理对实现全面且上下文一致的长视频理解的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of information fragmentation and loss of global coherence in long video understanding, which arises from naive chunking strategies in existing vision-language models. The proposed HAVEN framework integrates audiovisual entity cohesion to preserve semantic consistency and organizes video content into a hierarchical structure (global, scene, segment, entity). It employs an agentic search mechanism for dynamic retrieval and reasoning across these layers, enabling coherent narrative reconstruction and fine-grained entity tracking. Experimental results on LVBench demonstrate state-of-the-art performance with 84.1% overall accuracy and 80.1% in the challenging reasoning category, highlighting the method&#x27;s effectiveness in achieving temporal coherence, entity consistency, and retrieval efficiency.</div>
<div class="mono" style="margin-top:8px">为解决长视频理解中因简单分块和检索增强生成导致的信息碎片化与全局连贯性丧失问题，本文提出了HAVEN框架，该框架通过整合视听实体内聚、分层视频索引与智能体搜索来实现统一处理。该方法通过对齐视听流中的实体级表示以保持语义一致性，并将视频内容组织为从全局摘要到实体层的结构化层次，进而采用智能体搜索机制进行动态跨层检索与推理，以支持连贯的叙事重建和细粒度实体追踪。实验结果表明，该方法在时间连贯性、实体一致性和检索效率方面表现优异，在LVBench基准上以84.1%的总准确率创造了新的最优性能，尤其在具有挑战性的推理类别中达到了80.1%，证明了结构化多模态推理对于全面、上下文一致的长视频理解的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-space Contrastive Guidance for Efficient Hallucination Mitigation in LVLMs</div>
<div class="meta-line">Authors: Yujin Jo, Sangyoon Bae, Taesup Kim</div>
<div class="meta-line">First: 2026-01-20T08:04:18+00:00 · Latest: 2026-01-20T08:04:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hallucinations in large vision-language models (LVLMs) often arise when language priors dominate over visual evidence, causing object misidentification and visually inconsistent descriptions. We address this issue by framing hallucination mitigation as contrastive guidance, steering generation toward visually grounded and semantically faithful text. This approach regulates the model&#x27;s internal behavior by reducing over-dependence on language priors and contrasting visually grounded with language-only representations. We propose Attention-space Contrastive Guidance (ACG), a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in a single forward computation. This integration enables computationally efficient guidance directly embedded in the model&#x27;s representation contextualization. To correct approximation bias introduced by the single-pass formulation, we further apply an orthogonalized correction that removes components aligned with the language-only path, selectively amplifying visual contributions. Experiments on the CHAIR and POPE benchmarks show that ACG achieves state-of-the-art faithfulness and caption quality while significantly reducing computational cost. Our method establishes a principled and efficient alternative, reducing latency by up to 2x compared to prior contrastive decoding methods that require multiple forward passes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意力空间对比引导：高效缓解大型视觉语言模型幻觉</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLM）中的幻觉问题常源于语言先验对视觉证据的压制，导致物体误识别和视觉不一致描述。本研究将幻觉缓解构建为对比引导框架，通过引导生成过程偏向视觉基础化且语义忠实的文本来解决该问题。该方法通过降低对语言先验的过度依赖，并对比视觉基础表征与纯语言表征来调控模型内部行为。我们提出注意力空间对比引导（ACG）——一种单次前向计算机制，在自注意力层内同步构建视觉-语言与纯语言注意力路径。这种集成实现了直接嵌入模型表征上下文化的计算高效引导。为修正单次前向计算引入的近似偏差，我们进一步采用正交化校正技术，消除与纯语言路径对齐的分量，从而选择性增强视觉贡献。在CHAIR和POPEBenchmark上的实验表明，ACG在显著降低计算成本的同时，实现了最先进的忠实度与描述质量。相比需要多次前向计算的对比解码方法，本方法建立了原理清晰的高效替代方案，将延迟降低最高达2倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address hallucinations in large vision-language models where language priors override visual evidence, this research frames mitigation as a contrastive guidance problem, aiming to steer generation toward visually grounded text. The proposed Attention-space Contrastive Guidance (ACG) is a single-pass mechanism that operates within self-attention layers to construct both vision-language and language-only attention paths in one forward computation, integrating efficient guidance directly into representation contextualization; an orthogonalized correction is applied to remove approximation bias and amplify visual contributions. Experiments on CHAIR and POPE benchmarks demonstrate that ACG achieves state-of-the-art faithfulness and caption quality while reducing computational latency by up to 2x compared to prior multi-pass contrastive decoding methods.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型中语言先验主导视觉证据导致的幻觉问题，本研究将缓解策略构建为对比引导，使生成内容更基于视觉依据。提出的注意力空间对比引导方法在自注意力层内运行，通过单次前向计算同时构建视觉-语言和纯语言注意力路径，并采用正交化校正消除语言对齐成分以减少近似偏差。在CHAIR和POPE基准测试中，该方法在实现最先进的忠实度和描述质量的同时，将延迟降低至先前多轮对比解码方法的二分之一。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning or Pattern Matching? Probing Large Vision-Language Models with Visual Puzzles</div>
<div class="meta-line">Authors: Maria Lymperaiou, Vasileios Karampinis, Giorgos Filandrianos, Angelos Vlachos, Chrysoula Zerva, Athanasios Voulodimos</div>
<div class="meta-line">First: 2026-01-20T08:02:04+00:00 · Latest: 2026-01-20T08:02:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13705v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Puzzles have long served as compact and revealing probes of human cognition, isolating abstraction, rule discovery, and systematic reasoning with minimal reliance on prior knowledge. Leveraging these properties, visual puzzles have recently emerged as a powerful diagnostic tool for evaluating the reasoning abilities of Large Vision-Language Models (LVLMs), offering controlled, verifiable alternatives to open-ended multimodal benchmarks. This survey provides a unified perspective of visual puzzle reasoning in LVLMs. We frame visual puzzles through a common abstraction and organize existing benchmarks by the reasoning mechanisms they target (inductive, analogical, algorithmic, deductive, and geometric/spatial), thereby linking puzzle design to the cognitive operations required for solving. Synthesizing empirical evidence across these categories, we identify consistent limitations in current models, including brittle generalization, tight entanglement between perception and reasoning, and a persistent gap between fluent explanations and faithful execution. By framing visual puzzles as diagnostic instruments rather than task formats, this survey elaborates on the state of LVLM reasoning and outlines key directions for future benchmarks and reasoning-aware multimodal systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理还是模式匹配？用视觉谜题探究大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">谜题历来是检验人类认知的简洁有效工具，能以最小先验知识依赖分离出抽象、规则发现和系统推理能力。基于此特性，视觉谜题近期已成为评估大型视觉语言模型推理能力的重要诊断工具，为开放式多模态基准提供了可控可验证的替代方案。本综述为LVLM的视觉谜题推理研究提供统一视角：通过共同抽象框架界定视觉谜题，并依据目标推理机制（归纳、类比、算法、演绎及几何/空间推理）对现有基准进行分类，从而建立谜题设计与所需认知操作的关联。综合各类实证证据，我们指出当前模型存在普遍局限：脆弱的泛化能力、感知与推理的紧密纠缠、流畅解释与忠实执行间的持续差距。通过将视觉谜题定位为诊断工具而非任务形式，本综述系统阐述LVLM推理研究现状，并为未来基准测试与推理感知型多模态系统指明关键发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey investigates whether large vision-language models (LVLMs) perform genuine reasoning or merely pattern matching by using visual puzzles as diagnostic tools, which isolate abstract reasoning with minimal prior knowledge. It organizes existing benchmarks by the cognitive operations they target—inductive, analogical, algorithmic, deductive, and geometric/spatial reasoning—to link puzzle design to required reasoning mechanisms. Empirical synthesis reveals consistent model limitations, including brittle generalization, tight perception-reasoning entanglement, and a gap between fluent explanations and faithful execution, highlighting the need for future benchmarks and reasoning-aware systems.</div>
<div class="mono" style="margin-top:8px">本综述通过视觉谜题作为诊断工具，探究大型视觉语言模型（LVLM）是进行真正的推理还是仅进行模式匹配，这些谜题以最少先验知识隔离了抽象和规则发现等认知过程。作者提出了一个统一框架，将现有视觉谜题基准按目标推理机制（归纳、类比、算法、演绎和几何/空间）进行分类，将谜题设计与所需认知操作联系起来。实证综合分析揭示了模型的一致局限性，包括脆弱的泛化能力、感知与推理的紧密纠缠，以及流畅解释与忠实执行之间的差距，强调了未来基准和推理感知多模态系统的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">CARPE: Context-Aware Image Representation Prioritization via Ensemble for Large Vision-Language Models</div>
<div class="meta-line">Authors: Donghee Lee, Rui Cai, Zhe Zhao</div>
<div class="meta-line">First: 2026-01-20T05:44:33+00:00 · Latest: 2026-01-20T05:44:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13622v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13622v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have pushed them closer to becoming general-purpose assistants. Despite their strong performance, LVLMs still struggle with vision-centric tasks such as image classification, underperforming compared to their base vision encoders, which are often CLIP-based models. To address this limitation, we propose Context-Aware Image Representation Prioritization via Ensemble (CARPE), a novel, model-agnostic framework which introduces vision-integration layers and a context-aware ensemble strategy to identify when to prioritize image representations or rely on the reasoning capabilities of the language model. This design enhances the model&#x27;s ability to adaptively weight visual and textual modalities and enables the model to capture various aspects of image representations, leading to consistent improvements in generalization across classification and vision-language benchmarks. Extensive experiments demonstrate that CARPE not only improves performance on image classification benchmarks but also enhances results across various vision-language benchmarks. Finally, CARPE is designed to be effectively integrated with most open-source LVLMs that consist of a vision encoder and a language model, ensuring its adaptability across diverse architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CARPE：基于集成的情境感知图像表示优先级排序框架用于大型视觉语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLM）的最新进展使其更接近成为通用助手。尽管性能强大，LVLM在图像分类等视觉中心任务上仍存在困难，表现常逊于其基于CLIP的视觉编码器。为突破此局限，我们提出CARPE——一种新颖的模型无关框架，通过引入视觉集成层和情境感知集成策略，动态判断何时优先采用图像表示或依赖语言模型的推理能力。该设计增强了模型对视觉与文本模态的自适应加权能力，使其能捕捉图像表示的多维特征，从而在分类任务和视觉语言基准测试中实现泛化性能的持续提升。大量实验表明，CARPE不仅能提升图像分类基准性能，还能增强各类视觉语言基准测试结果。该框架可与多数开源LVLM（包含视觉编码器和语言模型）有效集成，确保其在不同架构间的广泛适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that large vision-language models (LVLMs) underperform their base vision encoders on vision-centric tasks like image classification. To address this, the authors propose CARPE, a model-agnostic framework that introduces vision-integration layers and a context-aware ensemble strategy to adaptively prioritize image representations or rely on the language model&#x27;s reasoning. Experimental results show that CARPE consistently improves generalization, enhancing performance on both image classification and various vision-language benchmarks.</div>
<div class="mono" style="margin-top:8px">该研究的动机是观察到大型视觉语言模型在图像分类等以视觉为中心的任务上表现不如其基础视觉编码器。为此，作者提出了CARPE，这是一个与模型无关的框架，它引入了视觉集成层和一种上下文感知的集成策略，以自适应地优先处理图像表征或依赖语言模型的推理能力。实验结果表明，CARPE能持续提升泛化能力，在图像分类和各种视觉语言基准测试中都增强了性能。</div>
</details>
</div>
<div class="card">
<div class="title">ChartVerse: Scaling Chart Reasoning via Reliable Programmatic Synthesis from Scratch</div>
<div class="meta-line">Authors: Zheng Liu, Honglin Lin, Chonghan Qin, Xiaoyang Wang, Xin Gao, Yu Li, Mengzhang Cai, Yun Zhu, Zhanping Zhong, Qizhi Pei, Zhuoshi Pan, Xiaoran Shang, Bin Cui, Conghui He, Wentao Zhang, Lijun Wu</div>
<div class="meta-line">First: 2026-01-20T05:11:44+00:00 · Latest: 2026-01-20T05:11:44+00:00</div>
<div class="meta-line">Comments: 29 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13606v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13606v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chart reasoning is a critical capability for Vision Language Models (VLMs). However, the development of open-source models is severely hindered by the lack of high-quality training data. Existing datasets suffer from a dual challenge: synthetic charts are often simplistic and repetitive, while the associated QA pairs are prone to hallucinations and lack the reasoning depth required for complex tasks. To bridge this gap, we propose ChartVerse, a scalable framework designed to synthesize complex charts and reliable reasoning data from scratch. (1) To address the bottleneck of simple patterns, we first introduce Rollout Posterior Entropy (RPE), a novel metric that quantifies chart complexity. Guided by RPE, we develop complexity-aware chart coder to autonomously synthesize diverse, high-complexity charts via executable programs. (2) To guarantee reasoning rigor, we develop truth-anchored inverse QA synthesis. Diverging from standard generation, we adopt an answer-first paradigm: we extract deterministic answers directly from the source code, generate questions conditional on these anchors, and enforce strict consistency verification. To further elevate difficulty and reasoning depth, we filter samples based on model fail-rate and distill high-quality Chain-of-Thought (CoT) reasoning. We curate ChartVerse-SFT-600K and ChartVerse-RL-40K using Qwen3-VL-30B-A3B-Thinking as the teacher. Experimental results demonstrate that ChartVerse-8B achieves state-of-the-art performance, notably surpassing its teacher and rivaling the stronger Qwen3-VL-32B-Thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ChartVerse：通过从零开始的可靠程序化合成实现图表推理的规模化</div>
<div class="mono" style="margin-top:8px">图表推理是视觉语言模型（VLMs）的关键能力，但高质量训练数据的缺乏严重阻碍了开源模型的发展。现有数据集面临双重挑战：合成图表往往过于简单且重复，而相关的问答对容易出现幻觉，缺乏复杂任务所需的推理深度。为弥补这一差距，我们提出了ChartVerse，一个可扩展的框架，旨在从零开始合成复杂图表和可靠的推理数据。（1）为解决简单模式的瓶颈，我们首先引入Rollout Posterior Entropy（RPE），一种量化图表复杂度的新指标。在RPE指导下，我们开发了复杂度感知图表编码器，通过可执行程序自主合成多样化、高复杂度的图表。（2）为保证推理严谨性，我们开发了基于真实锚点的逆向问答合成方法。不同于标准生成，我们采用答案优先范式：直接从源代码提取确定性答案，基于这些锚点生成问题，并执行严格的一致性验证。为进一步提升难度和推理深度，我们根据模型失败率筛选样本，并提炼高质量的思维链（CoT）推理。我们使用Qwen3-VL-30B-A3B-Thinking作为教师模型，构建了ChartVerse-SFT-600K和ChartVerse-RL-40K数据集。实验结果表明，ChartVerse-8B实现了最先进的性能，显著超越其教师模型，并与更强的Qwen3-VL-32B-Thinking相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The development of open-source Vision Language Models for chart reasoning is hindered by the lack of high-quality, complex training data, as existing datasets often contain simplistic charts and hallucinated or shallow QA pairs. To address this, the ChartVerse framework synthesizes complex charts and reliable reasoning data from scratch by introducing Rollout Posterior Entropy to quantify and guide the generation of diverse, high-complexity charts via executable programs, and employing a truth-anchored inverse QA synthesis method that extracts deterministic answers from source code first, then generates verified questions, and further filters for difficulty and distills Chain-of-Thought reasoning. Experimental results show that the resulting ChartVerse-8B model achieves state-of-the-art performance, surpassing its teacher model and rivaling a significantly larger model.</div>
<div class="mono" style="margin-top:8px">开源视觉语言模型在图表推理方面的发展受到高质量、复杂训练数据缺乏的阻碍，现有数据集通常包含过于简单的图表以及存在幻觉或推理深度不足的问答对。为解决这一问题，ChartVerse框架从零开始合成复杂图表和可靠的推理数据，其方法包括引入Rollout Posterior Entropy度量来量化并指导通过可执行程序生成多样化的高复杂度图表，以及采用以真实答案为锚点的逆向问答合成方法，即先从源代码提取确定性答案再生成经过严格验证的问题，并进一步基于模型失败率筛选样本和蒸馏高质量思维链推理。实验结果表明，由此训练出的ChartVerse-8B模型取得了最先进的性能，超越了其教师模型，并与更强的32B模型相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">Analyzing VLM-Based Approaches for Anomaly Classification and Segmentation</div>
<div class="meta-line">Authors: Mohit Kakda, Mirudula Shri Muthukumaran, Uttapreksha Patel, Lawrence Swaminathan Xavier Prince</div>
<div class="meta-line">First: 2026-01-19T22:55:30+00:00 · Latest: 2026-01-19T22:55:30+00:00</div>
<div class="meta-line">Comments: 10 pages,4 images</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13440v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13440v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs), particularly CLIP, have revolutionized anomaly detection by enabling zero-shot and few-shot defect identification without extensive labeled datasets. By learning aligned representations of images and text, VLMs facilitate anomaly classification and segmentation through natural language descriptions of normal and abnormal states, eliminating traditional requirements for task-specific training or defect examples. This project presents a comprehensive analysis of VLM-based approaches for anomaly classification (AC) and anomaly segmentation (AS). We systematically investigate key architectural paradigms including sliding window-based dense feature extraction (WinCLIP), multi-stage feature alignment with learnable projections (AprilLab framework), and compositional prompt ensemble strategies. Our analysis evaluates these methods across critical dimensions: feature extraction mechanisms, text-visual alignment strategies, prompt engineering techniques, zero-shot versus few-shot trade-offs, computational efficiency, and cross-domain generalization. Through rigorous experimentation on benchmarks such as MVTec AD and VisA, we compare classification accuracy, segmentation precision, and inference efficiency. The primary contribution is a foundational understanding of how and why VLMs succeed in anomaly detection, synthesizing practical insights for method selection and identifying current limitations. This work aims to facilitate informed adoption of VLM-based methods in industrial quality control and guide future research directions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的异常分类与分割方法分析</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM），特别是CLIP，通过实现无需大量标注数据的零样本和少样本缺陷识别，彻底改变了异常检测领域。通过学**图像与文本的对齐表征，VLM能够借助对正常与异常状态的自然语言描述，促进异常分类与分割，从而消除了传统方法对任务特定训练或缺陷样本的需求。本项目对基于VLM的异常分类（AC）与异常分割（AS）方法进行了全面分析。我们系统研究了关键架构范式，包括基于滑动窗口的密集特征提取（WinCLIP）、带可学习投影的多阶段特征对齐（AprilLab框架）以及组合式提示集成策略。分析从多个关键维度评估这些方法：特征提取机制、文本-视觉对齐策略、提示工程技术、零样本与少样本的权衡、计算效率以及跨域泛化能力。通过在MVTec AD和VisA等基准数据集上的严格实验，我们比较了分类准确率、分割精度和推理效率。主要贡献在于建立了对VLM在异常检测中成功原因与机制的基础性理解，综合了方法选择的实践见解并指出了当前局限。本工作旨在促进基于VLM的方法在工业质量控制中的明智应用，并为未来研究方向提供指引。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to systematically understand how Vision-Language Models (VLMs) like CLIP enable zero-shot and few-shot anomaly detection without extensive labeled data, which is crucial for industrial quality control. The method involves a comprehensive analysis of key VLM-based architectural paradigms, including WinCLIP for dense feature extraction, the AprilLab framework for multi-stage feature alignment with learnable projections, and compositional prompt ensemble strategies. Experimental evaluation on benchmarks such as MVTec AD and VisA demonstrates comparative performance in classification accuracy, segmentation precision, and inference efficiency, providing foundational insights into why these methods work and practical guidance for method selection while highlighting current limitations.</div>
<div class="mono" style="margin-top:8px">本研究旨在深入理解视觉语言模型（如CLIP）如何实现无需大量标注数据的零样本和少样本异常检测。方法上，对基于VLM的关键架构范式进行了全面分析，包括基于滑动窗口的密集特征提取（WinCLIP）、具有可学习投影的多阶段特征对齐（AprilLab）以及组合提示集成策略，并从特征提取、对齐策略和计算效率等多个维度进行评估。在MVTec AD和VisA等基准数据集上的实验结果表明了不同方法在分类精度、分割准确性和推理效率上的对比，为理解VLM的成功机制提供了基础性见解，并为工业质量控制中的方法选择提供了实用指导。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning with Pixel-level Precision: QVLM Architecture and SQuID Dataset for Quantitative Geospatial Analytics</div>
<div class="meta-line">Authors: Peter A. Massih, Eric Cosatto</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-01-19T21:14:34+00:00 · Latest: 2026-01-19T21:14:34+00:00</div>
<div class="meta-line">Comments: Submitted to CVPR 2026. Introduces the QVLM architecture and the SQuID dataset for quantitative geospatial reasoning. Dataset DOI: 10.57967/hf/7565</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13401v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Vision-Language Models (VLMs) fail at quantitative spatial reasoning because their architectures destroy pixel-level information required for counting and measurements. Vision encoders compress images through patch embeddings, reducing spatial indexing and losing the precise pixel-level tracking required for accurate counting. We present two contributions to address this fundamental limitation. First, we introduce SQuID (Satellite Quantitative Intelligence Dataset), a benchmark of 2,000 satellite image Question-Answer pairs with both numerical range and categorical answers, designed to evaluate quantitative spatial reasoning. The dataset spans three difficulty tiers with annotations automatically generated from human labels and their learned variability. Second, we propose QVLM (Quantitative Vision-Language Model), a code-generation architecture that maintains pixel precision by decoupling language understanding from visual analysis. Instead of encoding images into embeddings, QVLM generates executable code that first calls a segmentation model to obtain pixel-level masks, then operates directly on these masks, preserving spatial indexing throughout the reasoning process. Our experiments show that QVLM using GPT-5 as coder achieves 42.0% accuracy on SQuID compared to 28.1% for a VLM prompted with image-question pairs. Our work reveals that, for quantitative spatial reasoning, architectural decoupling enables better accuracy on quantitative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素级精度推理：面向定量地理空间分析的QVLM架构与SQuID数据集</div>
<div class="mono" style="margin-top:8px">现有视觉语言模型在定量空间推理任务中表现不佳，因其架构会破坏计数与测量所需的像素级信息。视觉编码器通过图像块嵌入压缩图像，削弱了空间索引能力，丢失了精确计数所需的像素级追踪。我们提出两项核心贡献以解决这一根本局限：首先，我们发布SQuID（卫星定量智能数据集），这是一个包含2000个卫星图像问答对的基准数据集，涵盖数值区间与分类答案，专门用于评估定量空间推理能力。数据集包含三个难度层级，其标注通过人工标签及其学习变异性自动生成。其次，我们提出QVLM（定量视觉语言模型），这是一种通过解耦语言理解与视觉分析来保持像素精度的代码生成架构。QVLM不将图像编码为嵌入向量，而是生成可执行代码：先调用分割模型获取像素级掩码，随后直接基于这些保持空间索引的掩码进行推理操作。实验表明，采用GPT-5作为代码生成器的QVLM在SQuID数据集上达到42.0%准确率，显著优于直接输入图像-问题对的视觉语言模型（28.1%）。本研究揭示：对于定量空间推理任务，架构解耦能有效提升定量分析精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current Vision-Language Models (VLMs) are limited in quantitative spatial reasoning because their patch-based vision encoders destroy the precise pixel-level information needed for counting and measurement tasks. To address this, the authors introduce the SQuID dataset, a benchmark of 2,000 satellite image QA pairs for evaluating quantitative reasoning, and propose the QVLM architecture, which decouples language understanding from visual analysis by generating executable code that calls a segmentation model to obtain and operate directly on pixel-level masks, thereby preserving spatial indexing. Experimental results demonstrate that QVLM, using GPT-5 as the coder, achieves 42.0% accuracy on SQuID, significantly outperforming a standard VLM baseline which scores 28.1%, confirming that architectural decoupling improves performance on quantitative geospatial tasks.</div>
<div class="mono" style="margin-top:8px">当前的视觉语言模型（VLMs）在定量空间推理方面存在局限，因为其视觉编码器通过图像块嵌入破坏了像素级信息，而这是计数和测量任务所必需的。为解决这一问题，作者提出了SQuID数据集，这是一个包含2000个卫星图像问答对的基准，用于评估定量推理能力，并设计了QVLM架构，该架构通过生成可执行代码将语言理解与视觉分析解耦，代码首先调用分割模型获取像素级掩码，然后直接对这些掩码进行空间操作。实验结果表明，使用GPT-5作为编码器的QVLM在SQuID上达到了42.0%的准确率，显著优于仅获得28.1%准确率的传统VLM基线，这证明了解耦的架构能够保持空间索引，从而提升定量任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Organ-Aware Attention Improves CT Triage and Classification</div>
<div class="meta-line">Authors: Lavsen Dahal, Yubraj Bhandari, Geoffrey D. Rubin, Joseph Y. Lo</div>
<div class="meta-line">First: 2026-01-19T20:37:45+00:00 · Latest: 2026-01-19T20:37:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13385v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13385v1">PDF</a> · <a href="https://github.com/lavsendahal/oracle-ct">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There is an urgent need for triage and classification of high-volume medical imaging modalities such as computed tomography (CT), which can improve patient care and mitigate radiologist burnout. Study-level CT triage requires calibrated predictions with localized evidence; however, off-the-shelf Vision Language Models (VLM) struggle with 3D anatomy, protocol shifts, and noisy report supervision. This study used the two largest publicly available chest CT datasets: CT-RATE and RADCHEST-CT (held-out external test set). Our carefully tuned supervised baseline (instantiated as a simple Global Average Pooling head) establishes a new supervised state of the art, surpassing all reported linear-probe VLMs. Building on this baseline, we present ORACLE-CT, an encoder-agnostic, organ-aware head that pairs Organ-Masked Attention (mask-restricted, per-organ pooling that yields spatial evidence) with Organ-Scalar Fusion (lightweight fusion of normalized volume and mean-HU cues). In the chest setting, ORACLE-CT masked attention model achieves AUROC 0.86 on CT-RATE; in the abdomen setting, on MERLIN (30 findings), our supervised baseline exceeds a reproduced zero-shot VLM baseline obtained by running publicly released weights through our pipeline, and adding masked attention plus scalar fusion further improves performance to AUROC 0.85. Together, these results deliver state-of-the-art supervised classification performance across both chest and abdomen CT under a unified evaluation protocol. The source code is available at https://github.com/lavsendahal/oracle-ct.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>器官感知注意力机制提升CT分诊与分类效能</div>
<div class="mono" style="margin-top:8px">对计算机断层扫描（CT）等高通量医学影像进行分诊与分类具有迫切需求，此举可优化患者诊疗并缓解放射科医师职业倦怠。研究级CT分诊需具备局部证据支持的校准预测，但现有视觉语言模型（VLM）在处理三维解剖结构、协议差异及噪声报告监督方面存在局限。本研究采用两个最大公开胸部CT数据集：CT-RATE与RADCHEST-CT（留作外部测试集）。经精细调优的监督基线模型（采用全局平均池化头实现）创造了新的监督学习最优性能，超越所有已报道的线性探测VLM。基于此基线，我们提出ORACLE-CT——一种编码器无关的器官感知头部模块，其融合了器官掩码注意力（通过掩码限制的逐器官池化生成空间证据）与器官标量融合（标准化体积与平均HU特征的轻量级融合）。在胸部场景中，ORACLE-CT掩码注意力模型在CT-RATE上达到AUROC 0.86；在腹部场景中，于MERLIN数据集（30种病变）上，我们的监督基线超越了通过公开权重复现的零样本VLM基线，加入掩码注意力与标量融合后性能进一步提升至AUROC 0.85。这些成果在统一评估协议下，实现了胸腹部CT监督分类性能的双重突破。源代码已发布于https://github.com/lavsendahal/oracle-ct。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the urgent need for automated triage and classification of high-volume CT scans to improve patient care and reduce radiologist workload, tackling challenges such as 3D anatomy, protocol shifts, and noisy report supervision that hinder off-the-shelf Vision Language Models. The method introduces ORACLE-CT, an encoder-agnostic, organ-aware head that combines Organ-Masked Attention for spatial evidence localization and Organ-Scalar Fusion for integrating normalized volume and mean Hounsfield unit cues, building upon a carefully tuned supervised baseline with a Global Average Pooling head. Experimental results show that ORACLE-CT achieves an AUROC of 0.86 on the chest CT-RATE dataset and, in abdominal CT on MERLIN with 30 findings, the supervised baseline outperforms a reproduced zero-shot VLM baseline, with further improvements to AUROC 0.85 when adding masked attention and scalar fusion, establishing state-of-the-art supervised classification performance across chest and abdomen CT under a unified protocol.</div>
<div class="mono" style="margin-top:8px">本研究旨在应对高负荷CT扫描自动分诊和分类的需求，以改善患者护理并减轻放射科医生负担，解决了现成视觉语言模型在3D解剖结构、协议变化和噪声报告监督方面的挑战。方法上提出了ORACLE-CT，一种与编码器无关的器官感知头部，结合了用于局部空间证据的器官掩码注意力和整合归一化体积及平均亨氏单位线索的器官标量融合，基于精心调优的监督基线构建。实验结果表明，ORACLE-CT在胸部CT-RATE数据集上达到0.86的AUROC，在腹部CT的MERLIN数据集（含30种发现）上超越了复现的零样本视觉语言模型基线，AUROC提升至0.85，在统一评估协议下实现了胸腹部CT分类的先进监督性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Semantic Decoupling-Based Two-Stage Rainy-Day Attack for Revealing Weather Robustness Deficiencies in Vision-Language Models</div>
<div class="meta-line">Authors: Chengyin Hu, Xiang Chen, Zhe Jia, Weiwen Shi, Fengyu Zhang, Jiujiang Guo, Yiwei Wei</div>
<div class="meta-line">First: 2026-01-19T17:16:30+00:00 · Latest: 2026-01-19T17:16:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13238v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are trained on image-text pairs collected under canonical visual conditions and achieve strong performance on multimodal tasks. However, their robustness to real-world weather conditions, and the stability of cross-modal semantic alignment under such structured perturbations, remain insufficiently studied. In this paper, we focus on rainy scenarios and introduce the first adversarial framework that exploits realistic weather to attack VLMs, using a two-stage, parameterized perturbation model based on semantic decoupling to analyze rain-induced shifts in decision-making. In Stage 1, we model the global effects of rainfall by applying a low-dimensional global modulation to condition the embedding space and gradually weaken the original semantic decision boundaries. In Stage 2, we introduce structured rain variations by explicitly modeling multi-scale raindrop appearance and rainfall-induced illumination changes, and optimize the resulting non-differentiable weather space to induce stable semantic shifts. Operating in a non-pixel parameter space, our framework generates perturbations that are both physically grounded and interpretable. Experiments across multiple tasks show that even physically plausible, highly constrained weather perturbations can induce substantial semantic misalignment in mainstream VLMs, posing potential safety and reliability risks in real-world deployment. Ablations further confirm that illumination modeling and multi-scale raindrop structures are key drivers of these semantic shifts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语义解耦的两阶段雨天攻击：揭示视觉语言模型在天气鲁棒性上的缺陷</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在标准视觉条件下收集的图像-文本对上进行训练，在多模态任务中表现出色。然而，其对真实世界天气条件的鲁棒性，以及在这种结构化扰动下跨模态语义对齐的稳定性，仍未得到充分研究。本文聚焦雨天场景，首次提出利用真实天气攻击VLMs的对抗性框架，采用基于语义解耦的两阶段参数化扰动模型，分析降雨引发的决策偏移。第一阶段通过低维全局调制建模降雨的全局效应，以调整嵌入空间并逐步削弱原始语义决策边界。第二阶段通过显式建模多尺度雨滴外观和降雨引发的光照变化，引入结构化雨效变化，并优化由此产生的不可微天气空间以诱导稳定的语义偏移。该框架在非像素参数空间中运行，生成的扰动既符合物理规律又可解释。多任务实验表明，即使物理上合理且高度受限的天气扰动，也能在主流VLMs中引发显著的语义错位，为实际部署带来潜在的安全与可靠性风险。消融实验进一步证实，光照建模和多尺度雨滴结构是这些语义偏移的关键驱动因素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) are typically trained on clean data, but their robustness to real-world weather perturbations like rain remains unclear. To investigate this, the authors propose a two-stage adversarial attack framework that first applies a global modulation to weaken semantic decision boundaries and then introduces structured, multi-scale raindrop and illumination variations to induce semantic misalignment. Experiments demonstrate that even physically plausible rain perturbations can cause significant performance degradation in mainstream VLMs, with ablations confirming the critical role of illumination and multi-scale raindrop modeling in driving these semantic shifts.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究视觉语言模型在真实天气扰动下的鲁棒性，其动机在于缺乏对降雨等结构化天气条件如何影响跨模态语义对齐的研究。该方法提出了一种两阶段对抗攻击框架：首先，通过全局调制在嵌入空间中削弱原始语义决策边界；其次，通过显式建模多尺度雨滴外观和降雨引起的照明变化，并在不可微的天气空间中进行优化，以引入结构化扰动并诱导语义偏移。实验结果表明，即使是物理上合理且高度受限的雨天扰动，也会导致主流视觉语言模型出现显著的语义错位，消融实验进一步证实照明建模和多尺度雨滴结构是引发这些语义偏移的关键因素。</div>
</details>
</div>
<div class="card">
<div class="title">TVWorld: Foundations for Remote-Control TV Agents</div>
<div class="meta-line">Authors: Zhantao Ma, Quanfeng Lu, Shuai Zhong, Dahai Yu, Ping Luo, Michael K. Ng</div>
<div class="meta-line">First: 2026-01-19T15:24:32+00:00 · Latest: 2026-01-19T15:24:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13142v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \textbf{TVWorld-N} for topology-aware navigation and \textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TVWorld：面向遥控电视智能体的基础框架</div>
<div class="mono" style="margin-top:8px">近期的大规模视觉语言模型在设备控制方面展现出巨大潜力，但现有研究主要集中于点选式交互，而日常电视使用中常见的遥控交互模式尚未得到充分探索。为填补这一空白，我们提出了TVWorld——一种基于离线图结构的真实电视导航抽象，支持可复现且无需部署的评估。在此基础上，我们构建了两个互补的基准测试体系：面向拓扑感知导航的TVWorld-N与面向焦点感知定位的TVWorld-G，全面评估电视使用能力。这些基准揭示了现有智能体的关键局限：在基于焦点的长程电视导航中拓扑感知能力不足。基于此发现，我们提出了拓扑感知训练框架，将拓扑感知能力注入大规模视觉语言模型，并由此开发出专精于电视导航的基础模型TVTheseus。该模型在TVWorld-N上达到68.3%的成功率，超越了Gemini 3 Flash等强闭源基线，创造了当前最优性能。进一步的分析为开发高效电视使用智能体提供了重要洞见。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the gap that while large vision-language models (LVLMs) show promise for device control, existing work focuses on point-and-click interaction, leaving the common remote-control (RC) paradigm for TV navigation underexplored. To address this, the authors introduce TVWorld, an offline graph-based abstraction of real-world TV navigation, and derive two benchmarks—TVWorld-N for topology-aware navigation and TVWorld-G for focus-aware grounding—which reveal a key limitation in existing agents: insufficient topology awareness for long-horizon tasks. They then propose a Topology-Aware Training framework to inject this awareness into LVLMs, developing TVTheseus, a foundation model that achieves a 68.3% success rate on TVWorld-N, outperforming strong closed-source baselines like Gemini 3 Flash and setting a new state-of-the-art.</div>
<div class="mono" style="margin-top:8px">本研究针对电视导航中远程控制交互这一尚未充分探索的问题，其与常见的点按交互模式不同。作者提出了TVWorld，一种基于图结构的真实电视界面离线抽象，并从中衍生出两个基准测试（用于拓扑感知导航的TVWorld-N和用于焦点感知定位的TVWorld-G）来评估智能体。这些基准测试揭示了现有的大视觉语言模型在长程导航中缺乏足够的拓扑感知能力。为此，作者提出了一个拓扑感知训练框架，并开发了专门的基础模型TVTheseus。该模型在TVWorld-N上取得了68.3%的成功率，超越了Gemini 3 Flash等强大的闭源基线，创造了新的最优性能。</div>
</details>
</div>
<div class="card">
<div class="title">CLIP-Guided Adaptable Self-Supervised Learning for Human-Centric Visual Tasks</div>
<div class="meta-line">Authors: Mingshuang Luo, Ruibing Hou, Bo Chao, Hong Chang, Zimo Liu, Yaowei Wang, Shiguang Shan</div>
<div class="meta-line">First: 2026-01-19T15:19:28+00:00 · Latest: 2026-01-19T15:19:28+00:00</div>
<div class="meta-line">Comments: Accepted by TMM (IEEE Transactions on Multimedia), 16 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human-centric visual analysis plays a pivotal role in diverse applications, including surveillance, healthcare, and human-computer interaction. With the emergence of large-scale unlabeled human image datasets, there is an increasing need for a general unsupervised pre-training model capable of supporting diverse human-centric downstream tasks. To achieve this goal, we propose CLASP (CLIP-guided Adaptable Self-suPervised learning), a novel framework designed for unsupervised pre-training in human-centric visual tasks. CLASP leverages the powerful vision-language model CLIP to generate both low-level (e.g., body parts) and high-level (e.g., attributes) semantic pseudo-labels. These multi-level semantic cues are then integrated into the learned visual representations, enriching their expressiveness and generalizability. Recognizing that different downstream tasks demand varying levels of semantic granularity, CLASP incorporates a Prompt-Controlled Mixture-of-Experts (MoE) module. MoE dynamically adapts feature extraction based on task-specific prompts, mitigating potential feature conflicts and enhancing transferability. Furthermore, CLASP employs a multi-task pre-training strategy, where part- and attribute-level pseudo-labels derived from CLIP guide the representation learning process. Extensive experiments across multiple benchmarks demonstrate that CLASP consistently outperforms existing unsupervised pre-training methods, advancing the field of human-centric visual analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向人本视觉任务的CLIP引导可适应自监督学习</div>
<div class="mono" style="margin-top:8px">人本视觉分析在监控、医疗和人机交互等多样化应用中具有关键作用。随着大规模无标注人体图像数据集的出现，亟需能够支持多样化人本下游任务的通用无监督预训练模型。为此，我们提出CLASP（CLIP引导的可适应自监督学习）——专为人本视觉任务设计的无监督预训练新框架。CLASP利用强大的视觉语言模型CLIP生成低层级（如身体部位）与高层级（如属性）语义伪标签，并将这些多层级语义线索整合到学习到的视觉表征中，增强其表达力与泛化能力。针对不同下游任务对语义粒度的差异化需求，CLASP引入提示控制专家混合模块，该模块能根据任务特定提示动态调整特征提取，缓解潜在特征冲突并提升迁移性能。此外，CLASP采用多任务预训练策略，通过CLIP衍生的部位级与属性级伪标签指导表征学习过程。在多个基准测试上的广泛实验表明，CLASP持续优于现有无监督预训练方法，推动了人本视觉分析领域的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a general unsupervised pre-training model to support diverse human-centric visual tasks using large-scale unlabeled image datasets. The method, named CLASP, leverages the CLIP vision-language model to generate multi-level semantic pseudo-labels (e.g., body parts and attributes) and integrates them into visual representations; it also employs a Prompt-Controlled Mixture-of-Experts module to dynamically adapt feature extraction based on task-specific prompts. Experimental results on multiple benchmarks show that CLASP consistently outperforms existing unsupervised pre-training methods, advancing human-centric visual analysis.</div>
<div class="mono" style="margin-top:8px">本研究旨在利用大规模无标签人体图像数据集，为监控、医疗和人机交互等应用开发一个通用的无监督预训练模型，以支持多样化的人体中心视觉任务。为此，论文提出了CLASP框架，该方法利用CLIP视觉语言模型生成从低级（如身体部位）到高级（如属性）的语义伪标签，并通过多任务预训练策略和提示控制的混合专家模块，根据任务提示动态调整特征提取，以缓解特征冲突并增强可迁移性。主要实验结果表明，CLASP在多个基准测试中持续优于现有的无监督预训练方法，推动了人体中心视觉分析领域的发展。</div>
</details>
</div>
<div class="card">
<div class="title">GaussExplorer: 3D Gaussian Splatting for Embodied Exploration and Reasoning</div>
<div class="meta-line">Authors: Kim Yu-Ji, Dahye Lee, Kim Jun-Seong, GeonU Kim, Nam Hyeon-Woo, Yongjin Kwon, Yu-Chiang Frank Wang, Jaesung Choe, Tae-Hyun Oh</div>
<div class="meta-line">First: 2026-01-19T15:17:58+00:00 · Latest: 2026-01-19T15:17:58+00:00</div>
<div class="meta-line">Comments: Project page: https://gaussexplorer.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13132v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://gaussexplorer.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present GaussExplorer, a framework for embodied exploration and reasoning built on 3D Gaussian Splatting (3DGS). While prior approaches to language-embedded 3DGS have made meaningful progress in aligning simple text queries with Gaussian embeddings, they are generally optimized for relatively simple queries and struggle to interpret more complex, compositional language queries. Alternative studies based on object-centric RGB-D structured memories provide spatial grounding but are constrained by pre-fixed viewpoints. To address these issues, GaussExplorer introduces Vision-Language Models (VLMs) on top of 3DGS to enable question-driven exploration and reasoning within 3D scenes. We first identify pre-captured images that are most correlated with the query question, and subsequently adjust them into novel viewpoints to more accurately capture visual information for better reasoning by VLMs. Experiments show that ours outperforms existing methods on several benchmarks, demonstrating the effectiveness of integrating VLM-based reasoning with 3DGS for embodied tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GaussExplorer：基于3D高斯泼溅的具身探索与推理框架</div>
<div class="mono" style="margin-top:8px">本文提出GaussExplorer——一个基于3D高斯泼溅（3DGS）的具身探索与推理框架。现有语言嵌入式3DGS方法虽在简单文本查询与高斯嵌入对齐方面取得进展，但通常仅针对简单查询优化，难以解析复杂的组合式语言查询。基于物体中心RGB-D结构化记忆的替代研究虽能提供空间基础，却受限于预设视角。为解决这些问题，GaussExplorer在3DGS基础上引入视觉语言模型（VLM），实现三维场景中基于问题的探索与推理。我们首先识别与查询问题最相关的预采集图像，随后将其调整至新视角，以更精准捕捉视觉信息供VLM进行推理。实验表明，本方法在多个基准测试中优于现有方案，验证了VLM推理与3DGS在具身任务中融合的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in prior language-grounded 3D Gaussian Splatting (3DGS) methods, which handle simple text queries but struggle with complex, compositional language, and in object-centric RGB-D memories, which are constrained by fixed viewpoints. The proposed GaussExplorer framework integrates Vision-Language Models (VLMs) with 3DGS to enable question-driven embodied exploration and reasoning. It identifies pre-captured images relevant to a query and synthesizes novel viewpoints from them to provide richer visual information for VLM-based reasoning. Experimental results demonstrate that this approach outperforms existing methods on multiple benchmarks, validating the effectiveness of combining VLM reasoning with 3DGS for embodied tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有语言嵌入3D高斯泼溅（3DGS）方法只能处理简单文本查询而难以应对复杂组合查询的局限，以及基于物体中心RGB-D结构记忆的方法受限于固定视角的问题。提出的GaussExplorer框架将视觉语言模型（VLM）与3DGS相结合，实现了基于问题的具身探索与推理；该方法首先识别与查询最相关的预捕获图像，随后将其调整至新视角以获取更丰富的视觉信息供VLM进行推理分析。实验结果表明，该方法在多个基准测试上优于现有方法，验证了将VLM推理与3DGS结合用于具身任务的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-VLM Fusion Framework for Autonomous Maritime Port Inspection using a Heterogeneous UAV-USV System</div>
<div class="meta-line">Authors: Muhayy Ud Din, Waseem Akram, Ahsan B. Bakht, Irfan Hussain</div>
<div class="meta-line">First: 2026-01-19T14:36:50+00:00 · Latest: 2026-01-19T14:36:50+00:00</div>
<div class="meta-line">Comments: submitted in AEJ</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13096v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13096v1">PDF</a> · <a href="https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Maritime port inspection plays a critical role in ensuring safety, regulatory compliance, and operational efficiency in complex maritime environments. However, existing inspection methods often rely on manual operations and conventional computer vision techniques that lack scalability and contextual understanding. This study introduces a novel integrated engineering framework that utilizes the synergy between Large Language Models (LLMs) and Vision Language Models (VLMs) to enable autonomous maritime port inspection using cooperative aerial and surface robotic platforms. The proposed framework replaces traditional state-machine mission planners with LLM-driven symbolic planning and improved perception pipelines through VLM-based semantic inspection, enabling context-aware and adaptive monitoring. The LLM module translates natural language mission instructions into executable symbolic plans with dependency graphs that encode operational constraints and ensure safe UAV-USV coordination. Meanwhile, the VLM module performs real-time semantic inspection and compliance assessment, generating structured reports with contextual reasoning. The framework was validated using the extended MBZIRC Maritime Simulator with realistic port infrastructure and further assessed through real-world robotic inspection trials. The lightweight on-board design ensures suitability for resource-constrained maritime platforms, advancing the development of intelligent, autonomous inspection systems. Project resources (code and videos) can be found here: https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于异构无人机-无人船系统的自主海事港口检测LLM-VLM融合框架</div>
<div class="mono" style="margin-top:8px">海事港口检测在复杂海洋环境中对确保安全、法规遵从和运营效率至关重要。然而，现有检测方法多依赖人工操作和传统计算机视觉技术，缺乏可扩展性和上下文理解能力。本研究提出一种新颖的集成工程框架，利用大语言模型（LLM）与视觉语言模型（VLM）的协同效应，通过协作式空中与水面机器人平台实现自主海事港口检测。该框架以LLM驱动的符号化规划替代传统状态机任务规划器，并通过基于VLM的语义检测改进感知流程，实现情境感知与自适应监测。LLM模块将自然语言任务指令转化为包含依赖图的可执行符号化计划，编码操作约束并确保无人机-无人船安全协同；VLM模块则执行实时语义检测与合规性评估，生成具有上下文推理的结构化报告。该框架在扩展版MBZIRC海事模拟器中通过真实港口基础设施验证，并经由实际机器人检测试验进一步评估。轻量化机载设计确保其适用于资源受限的海事平台，推动了智能自主检测系统的发展。项目资源（代码与视频）请访问：https://github.com/Muhayyuddin/llm-vlm-fusion-port-inspection</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Maritime port inspection is crucial for safety and efficiency but often relies on manual methods and conventional computer vision lacking scalability. This study proposes an integrated framework that fuses Large Language Models (LLMs) for symbolic mission planning and Vision Language Models (VLMs) for semantic perception to enable autonomous inspection using cooperative UAV-USV systems. The LLM translates natural language instructions into executable plans with dependency graphs for safe coordination, while the VLM performs real-time semantic inspection and generates structured reports. Validation in a realistic port simulator and real-world trials demonstrated the framework&#x27;s effectiveness, with a lightweight design suitable for resource-constrained platforms.</div>
<div class="mono" style="margin-top:8px">海上港口检查对于安全和效率至关重要，但通常依赖于缺乏可扩展性和上下文理解的手动方法与传统计算机视觉技术。为解决这一问题，本研究提出一个集成框架，融合大型语言模型（LLM）进行符号化任务规划与视觉语言模型（VLM）进行语义检查，以实现使用协同无人机-无人船系统的自主检查。LLM将自然语言指令转换为包含依赖图的可执行计划以确保安全协调，而VLM则执行实时语义检查与合规性评估以生成结构化报告。通过使用真实感海上模拟器和实际机器人试验进行的实验验证表明该框架有效，其轻量级机载设计适合资源受限平台。</div>
</details>
</div>
<div class="card">
<div class="title">Crafting Adversarial Inputs for Large Vision-Language Models Using Black-Box Optimization</div>
<div class="meta-line">Authors: Jiwei Guan, Haibo Jin, Haohan Wang</div>
<div class="meta-line">First: 2026-01-05T02:49:33+00:00 · Latest: 2026-01-19T13:14:16+00:00</div>
<div class="meta-line">Comments: EACL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01747v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.01747v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Vision-Language Models (LVLMs) have shown groundbreaking capabilities across diverse multimodal tasks. However, these models remain vulnerable to adversarial jailbreak attacks, where adversaries craft subtle perturbations to bypass safety mechanisms and trigger harmful outputs. Existing white-box attacks methods require full model accessibility, suffer from computing costs and exhibit insufficient adversarial transferability, making them impractical for real-world, black-box settings. To address these limitations, we propose a black-box jailbreak attack on LVLMs via Zeroth-Order optimization using Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). ZO-SPSA provides three key advantages: (i) gradient-free approximation by input-output interactions without requiring model knowledge, (ii) model-agnostic optimization without the surrogate model and (iii) lower resource requirements with reduced GPU memory consumption. We evaluate ZO-SPSA on three LVLMs, including InstructBLIP, LLaVA and MiniGPT-4, achieving the highest jailbreak success rate of 83.0% on InstructBLIP, while maintaining imperceptible perturbations comparable to white-box methods. Moreover, adversarial examples generated from MiniGPT-4 exhibit strong transferability to other LVLMs, with ASR reaching 64.18%. These findings underscore the real-world feasibility of black-box jailbreaks and expose critical weaknesses in the safety mechanisms of current LVLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于黑盒优化的大规模视觉语言模型对抗输入生成</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）的最新进展在多模态任务中展现出突破性能力，但这些模型仍易受对抗性越狱攻击的影响——攻击者通过精心设计的细微扰动绕过安全机制，触发有害输出。现有白盒攻击方法需完全访问模型，存在计算成本高、对抗迁移性不足等问题，难以应用于现实黑盒场景。为克服这些局限，我们提出通过采用同步扰动随机逼近的零阶优化方法（ZO-SPSA）对LVLMs实施黑盒越狱攻击。ZO-SPSA具备三大优势：（1）通过输入输出交互实现无需模型知识的无梯度逼近；（2）无需代理模型的模型无关优化；（3）降低资源需求，减少GPU内存消耗。我们在InstructBLIP、LLaVA和MiniGPT-4三个LVLM上评估ZO-SPSA，在InstructBLIP上实现83.0%的最高越狱成功率，同时保持与白盒方法相当的不可感知扰动。此外，从MiniGPT-4生成的对抗样本对其他LVLMs表现出强迁移性，攻击成功率（ASR）达64.18%。这些发现揭示了黑盒越狱在现实场景中的可行性，并暴露了当前LVLMs安全机制的关键弱点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the vulnerability of Large Vision-Language Models (LVLMs) to adversarial jailbreak attacks and the impracticality of existing white-box methods in real-world settings, this paper proposes a black-box attack method using Zeroth-Order optimization via Simultaneous Perturbation Stochastic Approximation (ZO-SPSA). This gradient-free approach crafts adversarial inputs by interacting with model inputs and outputs, requiring no internal model knowledge, being model-agnostic, and consuming fewer resources. Experimental evaluation on models including InstructBLIP, LLaVA, and MiniGPT-4 shows the method achieves a high jailbreak success rate of 83.0% on InstructBLIP with imperceptible perturbations, and adversarial examples from MiniGPT-4 demonstrate strong transferability, with an attack success rate reaching 64.18% on other LVLMs, highlighting critical safety weaknesses.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型易受对抗性越狱攻击的脆弱性，以及现有白盒方法在现实黑盒场景中不切实际的问题，本研究提出了一种基于零阶优化和同步扰动随机逼近的黑盒攻击方法。这种无需梯度、与模型无关的方法通过输入输出交互来近似梯度，无需了解模型内部知识，从而降低了计算资源需求。在InstructBLIP、LLaVA和MiniGPT-4上的实验评估表明，该方法在InstructBLIP上实现了83.0%的高越狱成功率且扰动难以察觉，同时从MiniGPT-4生成的对抗样本表现出强迁移性，在其他模型上的攻击成功率可达64.18%，揭示了当前大型视觉语言模型安全机制的关键缺陷。</div>
</details>
</div>
<div class="card">
<div class="title">Proxy Robustness in Vision Language Models is Effortlessly Transferable</div>
<div class="meta-line">Authors: Xiaowei Fu, Fuxiang Huang, Lei Zhang</div>
<div class="meta-line">First: 2026-01-19T09:23:11+00:00 · Latest: 2026-01-19T09:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12865v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12865v1">PDF</a> · <a href="http://github.com/fxw13/HPT-GPD">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As a pivotal technique for improving the defense of deep models, adversarial robustness transfer via distillation has demonstrated remarkable success in conventional image classification tasks. However, this paradigm encounters critical challenges when applied to vision-language models (VLM) (e.g., CLIP): constructing adversarially robust teacher for large-scale multi-modal models demands prohibitively high computational resources. We bridge this gap by revealing an interesting phenomenon: vanilla CLIP (without adversarial training) exhibits intrinsic defensive capabilities against adversarial examples generated by another CLIP with different architectures. We formally define this as proxy adversarial robustness, and naturally propose a Heterogeneous Proxy Transfer (HPT) framework that establishes cross-architectural robustness distillation channels between CLIP variants, effortlessly enabling the VLM robustness transfer from proxy to target models. Yet, such proxy transfer paradigm easily induces severe overfitting, leading to a sharp degradation in zero-shot natural generalization. To resolve that, we design Generalization-Pivot Decoupling (GPD) by leveraging the difference in learning rate scheduling. This decouples the proxy transfer process into a generalization-anchored warm-up that maintains generalization and a generalization-pulled HPT that promotes adversarial robustness, to achieve an equilibrium between natural generalization and adversarial robustness. Extensive experiments on 15 zero-shot datasets demonstrate the effectiveness of our HPT-GPD method. The code is available at the website of github.com/fxw13/HPT-GPD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中的代理鲁棒性可轻松迁移</div>
<div class="mono" style="margin-top:8px">作为提升深度模型防御能力的关键技术，通过蒸馏实现的对抗鲁棒性迁移在传统图像分类任务中已展现出显著成效。然而，当应用于视觉语言模型（如CLIP）时，该范式面临严峻挑战：为大规模多模态模型构建对抗鲁棒教师模型需要极高的计算资源。我们通过揭示一个有趣现象填补了这一空白：未经对抗训练的原始CLIP模型对由不同架构CLIP生成的对抗样本展现出内在防御能力。我们将其正式定义为代理对抗鲁棒性，并自然提出异构代理迁移框架，在CLIP变体间建立跨架构鲁棒性蒸馏通道，轻松实现从代理模型到目标模型的视觉语言模型鲁棒性迁移。但此类代理迁移范式易引发严重过拟合，导致零样本自然泛化能力急剧下降。为此，我们利用学习率调度差异设计泛化枢纽解耦机制，将代理迁移过程解耦为保持泛化能力的泛化锚定预热阶段和提升对抗鲁棒性的泛化牵引异构代理迁移阶段，以实现自然泛化与对抗鲁棒性的平衡。在15个零样本数据集上的大量实验验证了该方法的有效性。代码发布于github.com/fxw13/HPT-GPD。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the prohibitive computational cost of adversarially training large vision-language models (VLMs) like CLIP for robustness. The method introduces a Heterogeneous Proxy Transfer (HPT) framework, which leverages the finding that a standard CLIP model can defend against adversarial examples generated by a different CLIP architecture, enabling robustness distillation without adversarial training. To prevent overfitting and preserve zero-shot generalization, a Generalization-Pivot Decoupling (GPD) technique decouples the transfer process. Experiments across 15 zero-shot datasets confirm the method successfully transfers proxy robustness while maintaining natural accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对对抗性训练大型视觉语言模型（如CLIP）以进行鲁棒性迁移时计算成本过高的问题，提出了一种异构代理迁移框架。该框架基于一个发现：未经对抗训练的标准CLIP模型能够防御来自不同架构CLIP生成的对抗样本，从而无需对抗训练即可实现高效的鲁棒性蒸馏。为防止过拟合并保持零样本泛化能力，方法引入了泛化锚定解耦策略，将迁移过程解耦。在15个零样本数据集上的实验表明，该方法能有效平衡对抗鲁棒性与自然泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Left-Right Symmetry Breaking in CLIP-style Vision-Language Models Trained on Synthetic Spatial-Relation Data</div>
<div class="meta-line">Authors: Takaki Yamamoto, Chihiro Noguchi, Toshihiro Tanizawa</div>
<div class="meta-line">First: 2026-01-19T08:16:11+00:00 · Latest: 2026-01-19T08:16:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12809v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial understanding remains a key challenge in vision-language models. Yet it is still unclear whether such understanding is truly acquired, and if so, through what mechanisms. We present a controllable 1D image-text testbed to probe how left-right relational understanding emerges in Transformer-based vision and text encoders trained with a CLIP-style contrastive objective. We train lightweight Transformer-based vision and text encoders end-to-end on paired descriptions of one- and two-object scenes and evaluate generalization to unseen object pairs while systematically varying label and layout diversity. We find that contrastive training learns left-right relations and that label diversity, more than layout diversity, is the primary driver of generalization in this setting. To gain the mechanistic understanding, we perform an attention decomposition and show that interactions between positional and token embeddings induce a horizontal attention gradient that breaks left-right symmetry in the encoders; ablating this contribution substantially reduces left-right discrimination. Our results provide a mechanistic insight of when and how CLIP-style models acquire relational competence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于合成空间关系数据训练的CLIP风格视觉语言模型中的左右对称性破缺</div>
<div class="mono" style="margin-top:8px">空间理解仍是视觉语言模型的核心挑战，但其是否真正被习得及具体机制尚不明确。本研究构建可控一维图文测试平台，探究基于Transformer的视觉与文本编码器在CLIP风格对比目标训练下如何形成左右关系理解。通过端到端训练轻量级Transformer编码器处理单/双物体场景的配对描述，并在系统调整标签与布局多样性的条件下评估模型对未见物体组合的泛化能力。实验发现对比训练能有效学习左右关系，且标签多样性（而非布局多样性）是此场景下泛化的主要驱动力。机制分析表明，位置嵌入与词元嵌入的交互会诱导水平注意力梯度，从而打破编码器的左右对称性；消除该作用会显著削弱左右判别能力。本研究从机制层面揭示了CLIP风格模型在何种条件下以及如何获得关系理解能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how vision-language models acquire spatial relational understanding, specifically left-right relations, using a controlled 1D image-text testbed. The researchers trained lightweight Transformer-based vision and text encoders end-to-end with a CLIP-style contrastive objective on synthetic descriptions of one- and two-object scenes, evaluating generalization to unseen object pairs while varying label and layout diversity. Key experimental findings show that contrastive training successfully learns left-right relations, with label diversity being the primary driver of generalization rather than layout diversity. Mechanistic analysis through attention decomposition revealed that interactions between positional and token embeddings create a horizontal attention gradient that breaks left-right symmetry in the encoders, and ablating this component substantially reduces left-right discrimination.</div>
<div class="mono" style="margin-top:8px">为探究视觉-语言模型中空间关系理解的涌现机制，本研究引入了一个可控的一维图像-文本测试平台。它使用CLIP风格的对比目标，在合成的一物体和两物体场景描述上训练轻量级Transformer编码器，并通过改变标签和布局多样性来评估对未见物体对的泛化能力。关键实验结果表明，对比训练成功学习了左右关系，且标签多样性是泛化的主要驱动力。通过注意力分解的机制分析表明，位置嵌入与词元嵌入之间的相互作用产生了一个水平注意力梯度，从而打破了编码器的左右对称性；消除这一贡献会显著损害左右辨别能力，这为理解关系能力如何获得提供了机制性见解。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey on Vision-Language-Action Models for Embodied AI</div>
<div class="meta-line">Authors: Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King</div>
<div class="meta-line">First: 2024-05-23T01:43:54+00:00 · Latest: 2026-01-19T07:34:10+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/yueen-ma/Awesome-VLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.14093v6">Abs</a> · <a href="https://arxiv.org/pdf/2405.14093v6">PDF</a> · <a href="https://github.com/yueen-ma/Awesome-VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied AI is widely recognized as a cornerstone of artificial general intelligence because it involves controlling embodied agents to perform tasks in the physical world. Building on the success of large language models and vision-language models, a new category of multimodal models -- referred to as vision-language-action models (VLAs) -- has emerged to address language-conditioned robotic tasks in embodied AI by leveraging their distinct ability to generate actions. The recent proliferation of VLAs necessitates a comprehensive survey to capture the rapidly evolving landscape. To this end, we present the first survey on VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized into three major lines of research. The first line focuses on individual components of VLAs. The second line is dedicated to developing VLA-based control policies adept at predicting low-level actions. The third line comprises high-level task planners capable of decomposing long-horizon tasks into a sequence of subtasks, thereby guiding VLAs to follow more general user instructions. Furthermore, we provide an extensive summary of relevant resources, including datasets, simulators, and benchmarks. Finally, we discuss the challenges facing VLAs and outline promising future directions in embodied AI. A curated repository associated with this survey is available at: https://github.com/yueen-ma/Awesome-VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具身人工智能中视觉-语言-动作模型综述</div>
<div class="mono" style="margin-top:8px">具身人工智能被广泛认为是通用人工智能的基石，其核心在于控制具身代理在物理世界中执行任务。依托大语言模型和视觉-语言模型的成功，一类新型多模态模型——视觉-语言-动作模型应运而生，凭借其生成动作的独特能力，致力于解决具身人工智能中语言条件化的机器人任务。近期VLA模型的激增亟需系统性综述以把握快速发展的技术格局。为此，我们首次提出面向具身人工智能的VLA模型综述。本工作建立了细致的VLA分类体系，归纳为三大研究方向：首类聚焦VLA的独立组件构建；次类致力于开发擅长预测底层动作的VLA控制策略；第三类涵盖能够将长周期任务分解为子任务序列的高层任务规划器，从而引导VLA执行更通用的用户指令。此外，我们系统梳理了相关资源，包括数据集、仿真平台与评测基准。最后，探讨了VLA面临的挑战并展望了具身人工智能的未来发展方向。本综述关联的精选资源库详见：https://github.com/yueen-ma/Awesome-VLA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this survey stems from the emergence of vision-language-action models (VLAs) as a key approach for embodied AI, which aims to control agents in the physical world and is considered foundational for artificial general intelligence. The method involves presenting the first comprehensive survey on VLAs, organizing them into a three-part taxonomy: research on individual VLA components, policies for predicting low-level actions, and high-level task planners that decompose long-horizon instructions. The main findings include a systematic categorization of the field, a summary of key datasets, simulators, and benchmarks, and the identification of current challenges and future directions for VLA development in embodied AI.</div>
<div class="mono" style="margin-top:8px">本综述的动机源于具身人工智能被广泛认为是通用人工智能的基石，以及近期在大型语言模型和视觉语言模型基础上兴起的视觉-语言-动作模型，该模型利用其生成动作的独特能力来处理具身AI中的语言条件机器人任务。方法上，本文首次对VLA进行了全面综述，详细阐述了一个分为三个研究方向的分类体系：VLA的独立组件、用于预测低级动作的VLA控制策略，以及能将长周期任务分解为子任务序列的高级任务规划器。主要成果包括对相关数据集、模拟器和基准测试的广泛总结，并讨论了当前面临的挑战和未来方向，同时提供了一个精选的资源库。</div>
</details>
</div>
<div class="card">
<div class="title">VIRO: Robust and Efficient Neuro-Symbolic Reasoning with Verification for Referring Expression Comprehension</div>
<div class="meta-line">Authors: Hyejin Park, Junhyuk Kwon, Suha Kwak, Jungseul Ok</div>
<div class="meta-line">First: 2026-01-19T07:21:19+00:00 · Latest: 2026-01-19T07:21:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12781v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.12781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Referring Expression Comprehension (REC) aims to localize the image region corresponding to a natural-language query. Recent neuro-symbolic REC approaches leverage large language models (LLMs) and vision-language models (VLMs) to perform compositional reasoning, decomposing queries 4 structured programs and executing them step-by-step. While such approaches achieve interpretable reasoning and strong zero-shot generalization, they assume that intermediate reasoning steps are accurate. However, this assumption causes cascading errors: false detections and invalid relations propagate through the reasoning chain, yielding high-confidence false positives even when no target is present in the image. To address this limitation, we introduce Verification-Integrated Reasoning Operators (VIRO), a neuro-symbolic framework that embeds lightweight operator-level verifiers within reasoning steps. Each operator executes and validates its output, such as object existence or spatial relationship, thereby allowing the system to robustly handle no-target cases when verification conditions are not met. Our framework achieves state-of-the-art performance, reaching 61.1% balanced accuracy across target-present and no-target settings, and demonstrates generalization to real-world egocentric data. Furthermore, VIRO shows superior computational efficiency in terms of throughput, high reliability with a program failure rate of less than 0.3%, and scalability through decoupled program generation from execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIRO：基于验证的鲁棒高效神经符号推理用于指代表达理解</div>
<div class="mono" style="margin-top:8px">指代表达理解（REC）旨在定位与自然语言查询对应的图像区域。近期的神经符号REC方法利用大语言模型（LLM）和视觉语言模型（VLM）进行组合推理，将查询分解为结构化程序并逐步执行。尽管此类方法实现了可解释的推理和强大的零样本泛化能力，但其假设中间推理步骤是准确的。然而，这一假设会导致级联错误：错误检测和无效关系在推理链中传播，即使图像中不存在目标也会产生高置信度的误报。为解决这一局限，我们提出了验证集成推理算子（VIRO），这是一个在推理步骤中嵌入轻量级算子级验证器的神经符号框架。每个算子执行并验证其输出（如对象存在性或空间关系），从而使系统能在验证条件未满足时鲁棒地处理无目标情况。我们的框架实现了最先进的性能，在目标存在和无目标场景下达到61.1%的平衡准确率，并展示了对真实世界第一视角数据的泛化能力。此外，VIRO在吞吐量方面展现出卓越的计算效率，程序故障率低于0.3%的高可靠性，以及通过解耦程序生成与执行实现的扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the vulnerability of neuro-symbolic Referring Expression Comprehension (REC) systems to cascading errors from inaccurate intermediate reasoning steps, which can lead to high-confidence false positives even when no target object is present. To mitigate this, the authors propose VIRO, a framework that integrates lightweight operator-level verifiers within the reasoning chain to validate each step&#x27;s output, such as object existence or spatial relationships, thereby enabling robust handling of no-target cases. Experimental results show that VIRO achieves state-of-the-art performance with 61.1% balanced accuracy across target-present and no-target settings, demonstrates generalization to real-world egocentric data, offers superior computational efficiency and throughput, maintains high reliability with a program failure rate below 0.3%, and exhibits scalability through decoupled program generation and execution.</div>
<div class="mono" style="margin-top:8px">该研究针对神经符号指代表达理解（REC）系统中因中间推理步骤不准确而导致的级联错误问题，这种错误会在目标对象不存在时产生高置信度的误判。提出的方法VIRO在推理链中集成了轻量级的操作级验证器，用于验证每个步骤的输出（如对象存在性或空间关系），从而鲁棒地处理无目标情况。实验结果表明，该方法在目标存在和无目标设置下达到了61.1%的平衡准确率，实现了最先进的性能，同时具有较高的计算效率、低于0.3%的程序失败率，并能有效泛化到真实世界的自我中心数据。</div>
</details>
</div>
<div class="card">
<div class="title">STRIDE-QA: Visual Question Answering Dataset for Spatiotemporal Reasoning in Urban Driving Scenes</div>
<div class="meta-line">Authors: Keishi Ishihara, Kento Sasaki, Tsubasa Takahashi, Daiki Shiono, Yu Yamaguchi</div>
<div class="meta-line">Venue: AAAI 2026 Oral</div>
<div class="meta-line">First: 2025-08-14T07:57:06+00:00 · Latest: 2026-01-19T04:44:59+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026 (Oral). project page: https://turingmotors.github.io/stride-qa/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10427v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.10427v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://turingmotors.github.io/stride-qa/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have been applied to autonomous driving to support decision-making in complex real-world scenarios. However, their training on static, web-sourced image-text pairs fundamentally limits the precise spatiotemporal reasoning required to understand and predict dynamic traffic scenes. We address this critical gap with STRIDE-QA, a large-scale visual question answering (VQA) dataset for physically grounded reasoning from an ego-centric perspective. Constructed from 100 hours of multi-sensor driving data in Tokyo, capturing diverse and challenging conditions, STRIDE-QA is the largest VQA dataset for spatiotemporal reasoning in urban driving, offering 16M QA pairs over 270K frames. Grounded by dense, automatically generated annotations including 3D bounding boxes, segmentation masks, and multi-object tracks, the dataset uniquely supports both object-centric and ego-centric reasoning through three novel QA tasks that require spatial localization and temporal prediction. Our benchmarks demonstrate that existing VLMs struggle significantly, with near-zero scores on prediction consistency. In contrast, VLMs fine-tuned on STRIDE-QA exhibit dramatic performance gains, achieving 55% success in spatial localization and 28% consistency in future motion prediction, compared to near-zero scores from general-purpose VLMs. Therefore, STRIDE-QA establishes a comprehensive foundation for developing more reliable VLMs for safety-critical autonomous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STRIDE-QA：面向城市驾驶场景时空推理的视觉问答数据集</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）已应用于自动驾驶领域，以支持复杂现实场景的决策。然而，其基于静态网络图像-文本对的训练方式，从根本上限制了理解和预测动态交通场景所需的精确时空推理能力。我们通过STRIDE-QA填补了这一关键空白——这是一个基于第一视角物理推理的大规模视觉问答（VQA）数据集。该数据集构建于东京100小时多传感器驾驶数据之上，涵盖多样且具有挑战性的路况，是城市驾驶时空推理领域规模最大的VQA数据集，包含27万帧图像上的1600万组问答对。通过自动生成的密集标注（包括3D边界框、分割掩码和多目标轨迹）进行数据锚定，本数据集通过三项需要空间定位与时间预测的新型问答任务，独特地支持以对象为中心和以自我为中心的推理。基准测试表明，现有VLMs表现严重不足，预测一致性得分接近零。相比之下，基于STRIDE-QA微调的VLMs性能显著提升：空间定位任务成功率可达55%，未来运动预测一致性达28%，而通用VLMs在此类任务中得分接近零。因此，STRIDE-QA为开发面向安全关键型自动驾驶系统的更可靠VLMs奠定了全面基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) for autonomous driving are limited by training on static web data, which lacks the spatiotemporal reasoning needed for dynamic traffic scenes. To address this, the authors introduce STRIDE-QA, a large-scale visual question answering dataset constructed from 100 hours of multi-sensor driving data in Tokyo, featuring 16 million QA pairs over 270,000 frames with dense 3D bounding boxes, segmentation masks, and object tracks. Experimental benchmarks show that existing VLMs perform near zero on prediction consistency, while models fine-tuned on STRIDE-QA achieve 55% accuracy in spatial localization and 28% consistency in future motion prediction.</div>
<div class="mono" style="margin-top:8px">用于自动驾驶的视觉语言模型因训练数据来自静态网络图像而受限，难以对动态交通场景进行精确的时空推理。为解决这一问题，研究者提出了STRIDE-QA，这是一个大规模视觉问答数据集，基于在东京采集的100小时多传感器驾驶数据构建，包含270,000帧图像上的1600万个问答对，并配有自动生成的密集标注，如3D边界框和物体轨迹。该数据集通过需要空间定位和时间预测的新颖任务，支持以物体为中心和以自我为中心的推理。实验基准表明，现有视觉语言模型表现不佳，在预测一致性上得分近乎为零，而在STRIDE-QA上微调的模型则取得显著提升，在空间定位任务上达到55%的成功率，在未来运动预测上达到28%的一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Source-Free Domain Adaptation</div>
<div class="meta-line">Authors: Song Tang, Wenxin Su, Mao Ye, Boyu Wang, Xiatian Zhu</div>
<div class="meta-line">First: 2024-03-12T12:40:08+00:00 · Latest: 2026-01-19T03:29:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.07601v4">Abs</a> · <a href="https://arxiv.org/pdf/2403.07601v4">PDF</a> · <a href="https://github.com/tntek/CausalDA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the pursuit of transferring a source model to a target domain without access to the source training data, Source-Free Domain Adaptation (SFDA) has been extensively explored across various scenarios, including Closed-set, Open-set, Partial-set, and Generalized settings. Existing methods, focusing on specific scenarios, not only address a limited subset of challenges but also necessitate prior knowledge of the target domain, significantly limiting their practical utility and deployability. In light of these considerations, we introduce a more practical yet challenging problem, termed unified SFDA, which comprehensively incorporates all specific scenarios in a unified manner. In this paper, we propose a novel approach latent Causal factors discovery for unified SFDA (CausalDA). In contrast to previous alternatives that emphasize learning the statistical description of reality, we formulate CausalDA from a causality perspective. The objective is to uncover potential causality between latent variables and model decisions, enhancing the reliability and robustness of the learned model against domain shifts. To integrate extensive world knowledge, we leverage a pre-trained vision-language model such as CLIP. This aids in the formation and discovery of latent causal factors in the absence of supervision in the variation of distribution and semantics, coupled with a newly designed information bottleneck with theoretical guarantees. Extensive experiments demonstrate that CausalDA can achieve new state-of-the-art results in distinct SFDA settings, as well as source-free out-of-distribution generalization. Our code and data are available at https://github.com/tntek/CausalDA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统一无源域适应</div>
<div class="mono" style="margin-top:8px">在无需访问源训练数据的情况下将源模型迁移至目标域的研究中，无源域适应（SFDA）已在闭集、开集、部分集和广义设置等多种场景中得到广泛探索。现有方法聚焦于特定场景，不仅仅能应对有限的挑战子集，还需预先了解目标域知识，这严重限制了其实际应用与部署能力。基于这些考量，我们提出了一个更实用且更具挑战性的问题——统一SFDA，它以统一方式全面整合了所有特定场景。本文提出了一种新颖方法：基于潜在因果因子发现的统一SFDA（CausalDA）。与先前强调学习现实统计描述的方案不同，我们从因果关系的角度构建CausalDA，旨在揭示潜在变量与模型决策间的潜在因果关系，从而增强所学模型针对域偏移的可靠性与鲁棒性。为整合广泛的世界知识，我们利用CLIP等预训练视觉-语言模型，在分布与语义变化缺乏监督的情况下，辅助潜在因果因子的构建与发现，并结合新设计的具有理论保证的信息瓶颈。大量实验表明，CausalDA能在不同SFDA设置及无源分布外泛化任务中取得新的最优结果。代码与数据公开于https://github.com/tntek/CausalDA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the practical limitations of existing Source-Free Domain Adaptation (SFDA) methods, which are typically designed for specific scenarios like closed-set or open-set adaptation and require prior knowledge of the target domain, hindering real-world deployment. To overcome this, the authors propose a unified SFDA framework and a novel method called CausalDA, which formulates the problem from a causality perspective to uncover latent causal factors influencing model decisions, thereby enhancing robustness against domain shifts; this is achieved by leveraging a pre-trained vision-language model like CLIP for world knowledge and a new information bottleneck with theoretical guarantees. Experimental results show that CausalDA achieves state-of-the-art performance across various SFDA settings and in source-free out-of-distribution generalization tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对现有无源域自适应方法通常专为闭集、开集等特定场景设计且需预知目标域知识的局限性，这阻碍了实际部署。为此，论文提出了一个统一的无源域自适应问题，涵盖所有场景，并提出了CausalDA方法，该方法从因果性角度出发，揭示影响模型决策的潜在因果因素，而非仅依赖统计相关性；它利用CLIP等预训练视觉-语言模型融入世界知识，并采用具有理论保证的新型信息瓶颈来处理无监督下的分布和语义偏移。实验结果表明，CausalDA在多种无源域自适应设置及无源分布外泛化任务中均取得了最先进的性能。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
