<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-05 06:28</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260205_0628</div>
    <div class="row"><div class="card">
<div class="title">Multi-Agent Pathfinding Under Team-Connected Communication Constraint via Adaptive Path Expansion and Dynamic Leading</div>
<div class="meta-line">Authors: Hoang-Dung Bui, Erion Plaku, Gregoy J. Stein</div>
<div class="meta-line">First: 2025-01-06T05:21:18+00:00 · Latest: 2026-02-03T18:36:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.02770v5">Abs</a> · <a href="https://arxiv.org/pdf/2501.02770v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a novel planning framework to handle a multi-agent pathfinding problem under team-connected communication constraint, where all agents must have a connected communication channel to the rest of the team during their entire movements. Standard multi-agent path finding approaches (e.g., priority-based search) have potential in this domain but fail when neighboring configurations at start and goal differ. Their single-expansion approach -- computing each agent&#x27;s path from the start to the goal in just a single expansion -- cannot reliably handle planning under communication constraints for agents as their neighbors change during navigating. Similarly, leader-follower approaches (e.g., platooning) are effective at maintaining team communication, but fixing the leader at the outset of planning can cause planning to become stuck in dense-clutter environments, limiting their practical utility. To overcome this limitation, we propose a novel two-level multi-agent pathfinding framework that integrates two techniques: adaptive path expansion to expand agent paths to their goals in multiple stages; and dynamic leading technique that enables the reselection of the leading agent during each agent path expansion whenever progress cannot be made. Simulation experiments show the efficiency of our planners, which can handle up to 25 agents across five environment types under a limited communication range constraint and up to 11-12 agents on three environment types under line-of-sight communication constraint, exceeding 90% success-rate where baselines routinely fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应路径扩展与动态引导的团队连通通信约束下多智能体路径规划</div>
<div class="mono" style="margin-top:8px">本文提出一种新颖的规划框架，用于解决团队连通通信约束下的多智能体路径规划问题，要求所有智能体在整个移动过程中保持与团队其他成员的通信链路连通。传统多智能体路径规划方法（如基于优先级的搜索）在此领域有一定潜力，但在起点与目标点邻域配置不同时会失效。其单次扩展方法——仅通过单次扩展为每个智能体计算从起点到目标的路径——无法可靠处理通信约束下的规划，因为智能体在导航过程中邻域关系会动态变化。类似地，领导者-跟随者方法（如队列行进）能有效维持团队通信，但若在规划初期固定领导者，易在密集杂乱环境中陷入规划僵局，限制其实用性。为克服这些局限，我们提出一种新颖的两层多智能体路径规划框架，整合两项技术：自适应路径扩展——分多阶段扩展智能体至目标的路径；动态引导技术——在每次路径扩展无法推进时重新选择引导智能体。仿真实验表明，本规划器在有限通信距离约束下可处理五类环境中最多25个智能体，在视线通信约束下可处理三类环境中最多11-12个智能体，成功率超过90%，而基线方法普遍失效。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the multi-agent pathfinding problem under a team-connected communication constraint, where agents must maintain a connected communication channel throughout their movements. The proposed method introduces a two-level planning framework that combines adaptive path expansion, which expands agent paths to goals in multiple stages, with dynamic leading, allowing for reselection of the leading agent during expansion when progress stalls. Experimental results demonstrate that the framework efficiently handles up to 25 agents under limited communication range and up to 11-12 agents under line-of-sight constraints across various environments, achieving over 90% success rates where baseline methods often fail.</div>
<div class="mono" style="margin-top:8px">本研究针对团队连通通信约束下的多智能体路径规划问题，要求智能体在整个移动过程中保持与团队的通信连接。所提出的方法采用了一个双层规划框架，结合了自适应路径扩展（分阶段将智能体路径扩展至目标）和动态引导技术（在扩展过程中当进展受阻时重新选择引导智能体）。实验结果表明，该框架在有限通信范围下能高效处理多达25个智能体，在视线通信约束下能处理多达11-12个智能体，在多种环境类型中成功率超过90%，而基线方法通常无法胜任。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Reachability for Safe Control in Unknown Environments</div>
<div class="meta-line">Authors: Xinhang Ma, Junlin Wu, Yiannis Kantaros, Yevgeniy Vorobeychik</div>
<div class="meta-line">First: 2026-02-03T18:01:38+00:00 · Latest: 2026-02-03T18:01:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03799v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03799v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing provably safe control is a core problem in trustworthy autonomy. However, most prior work in this regard assumes either that the system dynamics are known or deterministic, or that the state and action space are finite, significantly limiting application scope. We address this limitation by developing a probabilistic verification framework for unknown dynamical systems which combines conformal prediction with reachability analysis. In particular, we use conformal prediction to obtain valid uncertainty intervals for the unknown dynamics at each time step, with reachability then verifying whether safety is maintained within the conformal uncertainty bounds. Next, we develop an algorithmic approach for training control policies that optimize nominal reward while also maximizing the planning horizon with sound probabilistic safety guarantees. We evaluate the proposed approach in seven safe control settings spanning four domains -- cartpole, lane following, drone control, and safe navigation -- for both affine and nonlinear safety specifications. Our experiments show that the policies we learn achieve the strongest provable safety guarantees while still maintaining high average reward.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未知环境下安全控制的保形可达性分析</div>
<div class="mono" style="margin-top:8px">设计可证明的安全控制是可信自主系统的核心问题。然而，现有研究大多假设系统动力学已知或确定，或状态与动作空间有限，这严重限制了应用范围。我们通过开发结合保形预测与可达性分析的未知动态系统概率验证框架来解决这一局限。具体而言，我们利用保形预测获取每个时间步未知动态的有效不确定区间，再通过可达性分析验证在保形不确定边界内是否保持安全性。随后，我们提出一种算法框架来训练控制策略，在优化名义奖励的同时，通过可靠的概率安全保证最大化规划时域。我们在涵盖倒立摆、车道保持、无人机控制和安全导航四个领域的七种安全控制场景中评估所提方法，覆盖线性和非线性安全约束。实验表明，我们学习的策略在保持高平均奖励的同时，实现了最强的可证明安全保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable provably safe control for unknown dynamical systems without restrictive assumptions on dynamics or state-action spaces, this work introduces a probabilistic verification framework that integrates conformal prediction with reachability analysis. The method uses conformal prediction to derive statistically valid uncertainty intervals for the unknown dynamics at each time step and then performs reachability analysis to verify safety within these probabilistic bounds; it further trains control policies to optimize nominal reward while maximizing the planning horizon under these safety guarantees. Experimental evaluation across seven safe control settings in four domains—cartpole, lane following, drone control, and safe navigation—for both affine and nonlinear safety specifications demonstrates that the learned policies achieve the strongest provable safety guarantees while maintaining high average reward.</div>
<div class="mono" style="margin-top:8px">本研究针对现有安全控制方法需要已知或确定性动力学、或有限状态-动作空间的局限性，这限制了其实际应用范围。作者开发了一个概率验证框架，将保形预测与可达性分析相结合：保形预测为未知系统动力学在每个时间步提供统计有效的置信区间，可达性分析则验证在这些不确定性范围内是否能保持安全性。他们还设计了一种算法来训练控制策略，在优化名义奖励的同时，最大化具有形式化概率安全保证的规划范围。在四个领域（倒立摆、车道跟随、无人机控制和安全导航）的七个安全控制设置中，针对仿射和非线性安全规范进行的实验评估表明，所学习的策略在保持高平均奖励的同时，实现了最强的可证明安全保证。</div>
</details>
</div>
<div class="card">
<div class="title">BridgeV2W: Bridging Video Generation Models to Embodied World Models via Embodiment Masks</div>
<div class="meta-line">Authors: Yixiang Chen, Peiyan Li, Jiabing Yang, Keji He, Xiangnan Wu, Yuan Xu, Kai Wang, Jing Liu, Nianfeng Liu, Yan Huang, Liang Wang</div>
<div class="meta-line">First: 2026-02-03T17:56:28+00:00 · Latest: 2026-02-03T17:56:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03793v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03793v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://BridgeV2W.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied world models have emerged as a promising paradigm in robotics, most of which leverage large-scale Internet videos or pretrained video generation models to enrich visual and motion priors. However, they still face key challenges: a misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. To this end, we present BridgeV2W, which converts coordinate-space actions into pixel-aligned embodiment masks rendered from the URDF and camera parameters. These masks are then injected into a pretrained video generation model via a ControlNet-style pathway, which aligns the action control signals with predicted videos, adds view-specific conditioning to accommodate camera viewpoints, and yields a unified world model architecture across embodiments. To mitigate overfitting to static backgrounds, BridgeV2W further introduces a flow-based motion loss that focuses on learning dynamic and task-relevant regions. Experiments on single-arm (DROID) and dual-arm (AgiBot-G1) datasets, covering diverse and challenging conditions with unseen viewpoints and scenes, show that BridgeV2W improves video generation quality compared to prior state-of-the-art methods. We further demonstrate the potential of BridgeV2W on downstream real-world tasks, including policy evaluation and goal-conditioned planning. More results can be found on our project website at https://BridgeV2W.github.io .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BridgeV2W：通过具身掩码将视频生成模型桥接至具身世界模型</div>
<div class="mono" style="margin-top:8px">具身世界模型已成为机器人学中颇具前景的研究范式，现有方法多利用大规模互联网视频或预训练视频生成模型来丰富视觉与运动先验。然而仍存在关键挑战：坐标空间动作与像素空间视频的对齐偏差、对相机视角的敏感性，以及跨具身形态的非统一架构。为此，我们提出BridgeV2W，通过URDF模型和相机参数将坐标空间动作转换为像素对齐的具身掩码，并借助ControlNet式通路注入预训练视频生成模型。该方法实现了动作控制信号与生成视频的对齐，通过视角特定条件适应相机位姿，并构建了跨具身形态的统一世界模型架构。为缓解静态背景过拟合问题，BridgeV2W进一步引入基于光流的运动损失函数，专注于学习动态及任务相关区域。在涵盖未知视角与场景的多样化复杂条件下，基于单臂（DROID）和双臂（AgiBot-G1）数据集的实验表明，BridgeV2W相比现有先进方法提升了视频生成质量。我们进一步展示了该方法在下游现实任务（包括策略评估与目标条件规划）中的应用潜力。更多结果详见项目网站https://BridgeV2W.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses key challenges in embodied world models, including misalignment between coordinate-space actions and pixel-space videos, sensitivity to camera viewpoint, and non-unified architectures across embodiments. The proposed method, BridgeV2W, converts coordinate-space actions into pixel-aligned embodiment masks using URDF and camera parameters, then injects these masks into a pretrained video generation model via a ControlNet-style pathway to align actions with videos, condition on viewpoints, and unify the architecture; it also introduces a flow-based motion loss to focus on dynamic regions and mitigate overfitting to static backgrounds. Experimental results on single-arm and dual-arm datasets under diverse conditions with unseen viewpoints and scenes show that BridgeV2W improves video generation quality over prior state-of-the-art methods and demonstrates potential in downstream real-world tasks like policy evaluation and goal-conditioned planning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身世界模型中的关键挑战，包括坐标空间动作与像素空间视频之间的错位、对相机视角的敏感性以及不同机器人实体架构不统一的问题。提出的BridgeV2W方法通过URDF和相机参数将坐标空间动作转换为像素对齐的实体掩码，然后通过ControlNet风格的路径将这些掩码注入预训练的视频生成模型，以对齐动作与视频预测并适应不同视角。该方法进一步引入了基于光流的运动损失，专注于学习动态且与任务相关的区域，减轻对静态背景的过拟合。在单臂和双臂机器人数据集上的实验结果表明，在包含未见视角和场景的多样化挑战条件下，BridgeV2W相比先前最先进方法提高了视频生成质量，并在策略评估和目标条件规划等下游现实任务中展现出潜力。</div>
</details>
</div>
<div class="card">
<div class="title">QVLA: Not All Channels Are Equal in Vision-Language-Action Model&#x27;s Quantization</div>
<div class="meta-line">Authors: Yuhao Xu, Yantai Yang, Zhenyang Fan, Yufan Liu, Yuming Li, Bing Li, Zhipeng Zhang</div>
<div class="meta-line">First: 2026-02-03T17:43:45+00:00 · Latest: 2026-02-03T17:43:45+00:00</div>
<div class="meta-line">Comments: ICLR2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03782v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advent of Vision-Language-Action (VLA) models represents a significant leap for embodied intelligence, yet their immense computational demands critically hinder deployment on resource-constrained robotic platforms. Intuitively, low-bit quantization is a prevalent and preferred technique for large-scale model compression. However, we find that a systematic analysis of VLA model&#x27;s quantization is fundamentally lacking. We argue that naively applying uniform-bit quantization from Large Language Models (LLMs) to robotics is flawed, as these methods prioritize passive data fidelity while ignoring how minor action deviations compound into catastrophic task failures. To bridge this gap, we introduce QVLA, the first action-centric quantization framework specifically designed for embodied control. In a sharp departure from the rigid, uniform-bit quantization of LLM-based methods, QVLA introduces a highly granular, channel-wise bit allocation strategy. Its core mechanism is to directly measure the final action-space sensitivity when quantizing each individual channel to various bit-widths. This process yields a precise, per-channel importance metric that guides a global optimization, which elegantly unifies quantization and pruning (0-bit) into a single, cohesive framework. Extensive evaluations on different baselines demonstrate the superiority of our approach. In the LIBERO, the quantization version of OpenVLA-OFT with our method requires only 29.2% of the original model&#x27;s VRAM while maintaining 98.9% of its original performance and achieving a 1.49x speedup. This translates to a 22.6% performance improvement over the LLM-derived method SmoothQuant. Our work establishes a new, principled foundation for compressing VLA models in robotics, paving the way for deploying powerful, large-scale models on real-world hardware. Code will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QVLA：视觉-语言-动作模型量化中并非所有通道都同等重要</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型的出现标志着具身智能的重大飞跃，但其巨大的计算需求严重阻碍了在资源受限机器人平台上的部署。直观而言，低位量化是大规模模型压缩的常用优选技术。然而，我们发现目前对VLA模型量化的系统性分析存在根本性缺失。我们认为，将大型语言模型（LLM）的均匀位宽量化方法简单移植到机器人领域存在缺陷，因为这些方法优先考虑被动数据保真度，却忽略了微小动作偏差如何累积成灾难性任务失败。为填补这一空白，我们提出了首个面向具身控制设计的、以动作为中心的量化框架QVLA。与基于LLM方法的刚性均匀位宽量化截然不同，QVLA引入了高度细粒度的通道级位宽分配策略。其核心机制是直接测量量化每个独立通道至不同位宽时对最终动作空间的敏感度。该过程生成精确的逐通道重要性度量，指导全局优化，将量化与剪枝（0位）优雅统一为连贯框架。在不同基线上的广泛实验证明了本方法的优越性：在LIBERO基准中，采用本方法的OpenVLA-OFT量化版本仅需原模型29.2%的显存，在保持98.9%原始性能的同时实现1.49倍加速，相较LLM衍生的SmoothQuant方法性能提升22.6%。本研究为机器人领域VLA模型压缩建立了新的理论基础，为在现实硬件上部署强大、大规模模型铺平了道路。代码即将开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational challenges of deploying Vision-Language-Action (VLA) models on resource-constrained robots, noting that standard uniform-bit quantization from Large Language Models is inadequate as it ignores how small action errors can lead to task failure. The proposed method, QVLA, introduces an action-centric quantization framework that employs a channel-wise bit allocation strategy, measuring each channel&#x27;s sensitivity in the action space to guide a global optimization that unifies quantization and pruning. Experimental results on the LIBERO benchmark show that the quantized OpenVLA-OFT model using QVLA requires only 29.2% of the original VRAM while maintaining 98.9% of its performance and achieving a 1.49x speedup, outperforming the LLM-based SmoothQuant method by 22.6% in performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在资源受限的机器人平台上部署计算密集型的视觉-语言-动作（VLA）模型的挑战，其中直接应用来自大语言模型的均匀位宽量化方法会忽略微小动作偏差导致任务灾难性失败的问题。所提出的方法QVLA引入了一个以动作为中心的量化框架，通过直接测量每个通道对最终动作空间误差的敏感性，采用细粒度的通道级位宽分配策略，将量化和剪枝统一到一个优化框架中。在LIBERO基准测试上的实验结果表明，使用QVLA量化的OpenVLA-OFT模型仅需原模型29.2%的显存，同时保持其98.9%的原始性能并实现1.49倍加速，其性能比源自大语言模型的SmoothQuant方法高出22.6%。</div>
</details>
</div>
<div class="card">
<div class="title">A Scene Graph Backed Approach to Open Set Semantic Mapping</div>
<div class="meta-line">Authors: Martin Günther, Felix Igelbrink, Oscar Lima, Lennart Niecksch, Marian Renz, Martin Atzmueller</div>
<div class="meta-line">First: 2026-02-03T17:41:51+00:00 · Latest: 2026-02-03T17:41:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03781v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Open Set Semantic Mapping and 3D Semantic Scene Graphs (3DSSGs) are established paradigms in robotic perception, deploying them effectively to support high-level reasoning in large-scale, real-world environments remains a significant challenge. Most existing approaches decouple perception from representation, treating the scene graph as a derivative layer generated post hoc. This limits both consistency and scalability. In contrast, we propose a mapping architecture where the 3DSSG serves as the foundational backend, acting as the primary knowledge representation for the entire mapping process.
  Our approach leverages prior work on incremental scene graph prediction to infer and update the graph structure in real-time as the environment is explored. This ensures that the map remains topologically consistent and computationally efficient, even during extended operations in large-scale settings. By maintaining an explicit, spatially grounded representation that supports both flat and hierarchical topologies, we bridge the gap between sub-symbolic raw sensor data and high-level symbolic reasoning. Consequently, this provides a stable, verifiable structure that knowledge-driven frameworks, ranging from knowledge graphs and ontologies to Large Language Models (LLMs), can directly exploit, enabling agents to operate with enhanced interpretability, trustworthiness, and alignment to human concepts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于场景图的开集语义建图方法</div>
<div class="mono" style="margin-top:8px">尽管开集语义建图与三维语义场景图（3DSSG）已成为机器人感知领域的成熟范式，但如何有效部署它们以支持大规模真实环境中的高层推理仍面临重大挑战。现有方法大多将感知与表征解耦，将场景图视为事后生成的衍生层，这限制了系统的一致性与可扩展性。与此不同，我们提出一种以3DSSG作为基础后端支撑的建图架构，使其成为整个建图过程的核心知识表征。该方法借鉴增量式场景图预测的已有成果，在探索环境时实时推断并更新图结构，确保地图在大规模场景的长期运行中保持拓扑一致性及计算效率。通过维护支持平面与分层拓扑的显式空间锚定表征，我们弥合了亚符号原始传感器数据与高层符号推理之间的鸿沟。由此形成的稳定可验证结构，可供从知识图谱、本体论到大型语言模型（LLM）等知识驱动框架直接利用，使智能体在运作时具备更强的可解释性、可信度及与人类概念的契合度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of decoupled perception and representation in open set semantic mapping, which hinders consistency and scalability for high-level reasoning in large-scale environments, this work proposes a novel architecture that uses a 3D Semantic Scene Graph (3DSSG) as the foundational backend knowledge representation. The method leverages incremental scene graph prediction to infer and update the graph structure in real-time during exploration, ensuring topological consistency and computational efficiency. Experimental results demonstrate that this approach provides a stable, spatially grounded representation that effectively bridges raw sensor data with symbolic reasoning, enabling enhanced interpretability and direct exploitation by knowledge-driven frameworks like knowledge graphs and Large Language Models.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在大规模真实世界环境中部署开放集语义地图和3D语义场景图以支持高级机器人推理的挑战，现有方法将场景图作为事后衍生物处理，限制了系统的一致性和可扩展性。该方法提出了一种以3D语义场景图作为基础后端和主要知识表示的建图架构，利用增量式场景图预测技术在环境探索过程中实时推断和更新图结构。实验结果表明，该方法在大规模场景的长期操作中保持了拓扑一致性和计算效率，提供了一个稳定的、空间接地的表示，能够桥接原始传感器数据与符号推理，从而增强系统的可解释性以及与人类概念的契合度。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Learning POMDPs Beyond Full-Rank Actions and State Observability</div>
<div class="meta-line">Authors: Seiji Shaw, Travis Manderson, Chad Kessens, Nicholas Roy</div>
<div class="meta-line">First: 2026-01-26T20:06:41+00:00 · Latest: 2026-02-03T17:03:44+00:00</div>
<div class="meta-line">Comments: Update abstract</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18930v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.18930v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We are interested in enabling autonomous agents to learn and reason about systems with hidden states, such as locking mechanisms. We cast this problem as learning the parameters of a discrete Partially Observable Markov Decision Process (POMDP). The agent begins with knowledge of the POMDP&#x27;s actions and observation spaces, but not its state space, transitions, or observation models. These properties must be constructed from a sequence of actions and observations. Spectral approaches to learning models of partially observable domains, such as Predictive State Representations (PSRs), learn representations of state that are sufficient to predict future outcomes. PSR models, however, do not have explicit transition and observation system models that can be used with different reward functions to solve different planning problems. Under a mild set of rankness assumptions on the products of transition and observation matrices, we show how PSRs learn POMDP matrices up to a similarity transform, and this transform may be estimated via tensor decomposition methods. Our method learns observation matrices and transition matrices up to a partition of states, where the states in a single partition have the same observation distributions corresponding to actions whose transition matrices are full-rank. Our experiments suggest that explicit observation and transition likelihoods can be leveraged to generate new plans for different goals and reward functions after the model has been learned. We also show that learning a POMDP beyond a partition of states is impossible from sequential data by constructing two POMDPs that agree on all observation distributions but differ in their transition dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向超越满秩动作与状态可观测性的POMDP学习研究</div>
<div class="mono" style="margin-top:8px">本研究致力于使自主智能体能够学习和推理具有隐藏状态的系统（如锁定机制）。我们将该问题建模为离散部分可观测马尔可夫决策过程（POMDP）的参数学习问题。智能体初始已知POMDP的动作空间与观测空间，但未知其状态空间、转移模型及观测模型，这些属性需通过动作-观测序列构建。针对部分可观测领域模型学习的光谱方法（如预测状态表示PSR）可学习足以预测未来结果的状态表示，但PSR模型缺乏可用于不同奖励函数以解决不同规划问题的显式转移与观测系统模型。在转移矩阵与观测矩阵乘积满足温和秩假设条件下，我们证明PSR可通过相似变换学习POMDP矩阵，且该变换可通过张量分解方法估计。本方法学习的观测矩阵与转移矩阵存在状态划分的模糊性——同一划分内的状态具有相同观测分布，对应转移矩阵满秩的动作。实验表明，学习模型后可利用显式观测与转移似然为不同目标与奖励函数生成新规划。同时通过构造两个观测分布完全一致但转移动态相异的POMDP，证明仅凭序列数据无法学习超越状态划分层级的POMDP模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable autonomous agents to learn and reason about systems with hidden states, such as locking mechanisms, by learning the parameters of a discrete Partially Observable Markov Decision Process (POMDP) from sequences of actions and observations. The method extends spectral learning approaches like Predictive State Representations (PSRs) by showing that, under mild rank assumptions, PSRs can recover POMDP transition and observation matrices up to a similarity transform, which can be estimated via tensor decomposition, though states with identical observation distributions under full-rank action transitions remain partitioned. Experimental results demonstrate that the learned explicit observation and transition models can be leveraged to generate new plans for different goals and reward functions, while theoretical analysis shows that learning a POMDP beyond such state partitions is impossible from sequential data alone, as evidenced by constructed POMDPs with identical observation distributions but different transition dynamics.</div>
<div class="mono" style="margin-top:8px">本研究旨在使自主智能体能够学习和推理具有隐藏状态的系统（如锁定机制），其方法是从动作和观察序列中学习离散部分可观察马尔可夫决策过程（POMDP）的参数。该方法扩展了谱学习方法，如预测状态表示（PSR），通过在温和的秩假设下证明PSR可以学习POMDP矩阵至一个相似变换，并通过张量分解估计该变换，从而恢复观测矩阵和转移矩阵至一个状态分区。实验结果表明，学习到的显式似然可用于为不同目标和奖励函数生成新计划，而理论分析表明，仅从序列数据中学习超越状态分区的POMDP是不可能的，这通过构建具有相同观测分布但不同转移动态的POMDP得以证明。</div>
</details>
</div>
<div class="card">
<div class="title">LAGEA: Language Guided Embodied Agents for Robotic Manipulation</div>
<div class="meta-line">Authors: Abdul Monaf Chowdhury, Akm Moshiur Rahman Mazumder, Rabeya Akter, Safaeid Hossain Arib</div>
<div class="meta-line">First: 2025-09-27T07:00:13+00:00 · Latest: 2026-02-03T16:15:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23155v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23155v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic manipulation benefits from foundation models that describe goals, but today&#x27;s agents still lack a principled way to learn from their own mistakes. We ask whether natural language can serve as feedback, an error-reasoning signal that helps embodied agents diagnose what went wrong and correct course. We introduce LAGEA (Language Guided Embodied Agents), a framework that turns episodic, schema-constrained reflections from a vision language model (VLM) into temporally grounded guidance for reinforcement learning. LAGEA summarizes each attempt in concise language, localizes the decisive moments in the trajectory, aligns feedback with visual state in a shared representation, and converts goal progress and feedback agreement into bounded, step-wise shaping rewards whose influence is modulated by an adaptive, failure-aware coefficient. This design yields dense signals early when exploration needs direction and gracefully recedes as competence grows. On the Meta-World MT10 and Robotic Fetch embodied manipulation benchmark, LAGEA improves average success over the state-of-the-art (SOTA) methods by 9.0% on random goals, 5.3% on fixed goals, and 17% on fetch tasks, while converging faster. These results support our hypothesis: language, when structured and grounded in time, is an effective mechanism for teaching robots to self-reflect on mistakes and make better choices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAGEA：语言引导的具身机器人操作智能体</div>
<div class="mono" style="margin-top:8px">机器人操作任务受益于描述目标的基础模型，但现有智能体仍缺乏从自身错误中学习的系统化方法。本研究探讨自然语言能否作为反馈信号，通过错误归因帮助具身智能体诊断失误并调整策略。我们提出LAGEA（语言引导具身智能体）框架，将视觉语言模型生成的阶段性、模式化反思转化为基于时间维度的强化学习指导。LAGEA通过三个核心机制实现：1）用精炼语言总结每次尝试；2）定位轨迹中的关键决策时刻；3）在共享表征中对齐视觉状态与语言反馈，并将目标进度与反馈一致性转化为有界的逐步塑形奖励，其影响强度由自适应故障感知系数动态调节。该设计在探索阶段提供密集引导信号，随智能体能力提升逐步淡出。在Meta-World MT10和Robotic Fetch具身操作基准测试中，LAGEA相比前沿方法在随机目标、固定目标和抓取任务上分别提升9.0%、5.3%和17%的平均成功率，且收敛速度更快。实验结果验证了核心假设：经结构化处理并锚定时间维度的语言反馈，能有效引导机器人通过自我反思改进决策。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of current robotic manipulation agents in learning from their own mistakes by investigating whether natural language can serve as structured feedback for error diagnosis and correction. The proposed LAGEA framework utilizes a vision language model to generate episodic, schema-constrained reflections on task attempts, which are then temporally grounded and converted into bounded, step-wise shaping rewards for reinforcement learning, with an adaptive coefficient that modulates feedback influence based on failure rates. Experimental results on the Meta-World MT10 and Robotic Fetch benchmarks demonstrate that LAGEA improves average success rates over state-of-the-art methods by 9.0% on random goals, 5.3% on fixed goals, and 17% on fetch tasks, while also achieving faster convergence, validating the effectiveness of language-guided self-reflection.</div>
<div class="mono" style="margin-top:8px">为了解决具身智能体缺乏从自身错误中学习的系统化机制的问题，本研究探讨了自然语言是否可以作为有效的反馈信号，用于错误诊断和行为修正。提出的LAGEA框架利用视觉语言模型对操作尝试生成符合特定模式的阶段性反思；随后定位轨迹中的关键决策时刻，将反馈与视觉状态对齐，并将这些信息转化为由自适应、失败感知系数调制的有界、逐步塑形奖励，以指导强化学习。在Meta-World MT10和Robotic Fetch基准测试上的实验结果表明，与最先进方法相比，LAGEA在随机目标上的平均成功率提高了9.0%，在固定目标上提高了5.3%，在抓取任务上提高了17%，同时收敛速度更快，这证明了基于时间的、结构化的语言反馈能有效实现机器人的自我反思和决策优化。</div>
</details>
</div>
<div class="card">
<div class="title">Input-to-State Safe Backstepping: Robust Safety-Critical Control with Unmatched Uncertainties</div>
<div class="meta-line">Authors: Max H. Cohen, Pio Ong, Aaron D. Ames</div>
<div class="meta-line">First: 2026-02-03T16:09:36+00:00 · Latest: 2026-02-03T16:09:36+00:00</div>
<div class="meta-line">Comments: To appear at the 2026 American Control Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03691v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03691v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Guaranteeing safety in the presence of unmatched disturbances -- uncertainties that cannot be directly canceled by the control input -- remains a key challenge in nonlinear control. This paper presents a constructive approach to safety-critical control of nonlinear systems with unmatched disturbances. We first present a generalization of the input-to-state safety (ISSf) framework for systems with these uncertainties using the recently developed notion of an Optimal Decay CBF, which provides more flexibility for satisfying the associated Lyapunov-like conditions for safety. From there, we outline a procedure for constructing ISSf-CBFs for two relevant classes of systems with unmatched uncertainties: i) strict-feedback systems; ii) dual-relative-degree systems, which are similar to differentially flat systems. Our theoretical results are illustrated via numerical simulations of an inverted pendulum and planar quadrotor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>输入到状态安全反步法：具有未匹配不确定性的鲁棒安全关键控制</div>
<div class="mono" style="margin-top:8px">在存在未匹配扰动（即无法通过控制输入直接抵消的不确定性）的情况下确保安全性，仍是非线性控制领域的核心挑战。本文提出了一种针对具有未匹配扰动的非线性系统的安全关键控制构造性方法。我们首先利用最新发展的最优衰减控制屏障函数概念，为这类不确定性系统推广了输入到状态安全框架，该框架为满足安全相关的类李雅普诺夫条件提供了更大灵活性。在此基础上，我们为两类具有未匹配不确定性的系统构建ISSf-CBF的流程：i)严格反馈系统；ii)对偶相对阶系统（类微分平坦系统）。通过倒立摆和平面四旋翼飞行器的数值仿真验证了理论结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of ensuring safety for nonlinear systems subject to unmatched disturbances, which cannot be directly canceled by control inputs. The method introduces a generalization of the input-to-state safety (ISSf) framework using Optimal Decay Control Barrier Functions (CBFs) to provide more flexible satisfaction of Lyapunov-like safety conditions. It then constructs ISSf-CBFs for two system classes: strict-feedback systems and dual-relative-degree systems. Numerical simulations on an inverted pendulum and a planar quadrotor demonstrate the effectiveness of the proposed approach in maintaining safety under unmatched uncertainties.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决非线性系统在无法被控制输入直接抵消的非匹配扰动下确保安全性的挑战。该方法通过使用最优衰减控制屏障函数（CBF）推广了输入到状态安全（ISSf）框架，为满足相关的李雅普诺夫类安全条件提供了更大灵活性，并概述了为严格反馈和双相对阶系统构建ISSf-CBF的构造性流程。在倒立摆和平面四旋翼飞行器上的数值仿真验证了所提方法在非匹配不确定性下维持安全性的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">When Should Agents Coordinate in Differentiable Sequential Decision Problems?</div>
<div class="meta-line">Authors: Caleb Probine, Su Ann Low, David Fridovich-Keil, Ufuk Topcu</div>
<div class="meta-line">First: 2026-02-03T15:55:16+00:00 · Latest: 2026-02-03T15:55:16+00:00</div>
<div class="meta-line">Comments: 15 content pages, 2 pages for references, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03674v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03674v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-robot teams must coordinate to operate effectively. When a team operates in an uncoordinated manner, and agents choose actions that are only individually optimal, the team&#x27;s outcome can suffer. However, in many domains, coordination requires costly communication. We explore the value of coordination in a broad class of differentiable motion-planning problems. In particular, we model coordinated behavior as a spectrum: at one extreme, agents jointly optimize a common team objective, and at the other, agents make unilaterally optimal decisions given their individual decision variables, i.e., they operate at Nash equilibria. We then demonstrate that reasoning about coordination in differentiable motion-planning problems reduces to reasoning about the second-order properties of agents&#x27; objectives, and we provide algorithms that use this second-order reasoning to determine at which times a team of agents should coordinate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在可微分序列决策问题中，智能体应在何时协调？</div>
<div class="mono" style="margin-top:8px">多机器人团队必须协调才能高效运作。当团队以非协调方式运作，且智能体仅选择个体最优行动时，团队整体表现可能受损。然而在许多领域，协调需要昂贵的通信成本。我们在广泛的可微分运动规划问题中探讨协调的价值。具体而言，我们将协调行为建模为一个连续谱：一端是智能体共同优化团队目标，另一端是智能体基于个体决策变量做出单边最优决策（即处于纳什均衡状态）。我们证明，在可微分运动规划问题中分析协调问题可转化为分析智能体目标函数的二阶特性，并提出了利用这种二阶推理来确定多智能体团队应在何时进行协调的算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the trade-off between coordination benefits and communication costs in multi-robot teams, where uncoordinated, individually optimal actions can degrade overall performance. The authors model coordination as a spectrum between joint team optimization and Nash equilibrium behavior, and they demonstrate that analyzing coordination in differentiable motion-planning problems can be reduced to examining the second-order properties of the agents&#x27; objectives. They provide algorithms that use this second-order reasoning to determine the specific times when agents should coordinate, with experimental results validating the approach&#x27;s ability to identify critical coordination points.</div>
<div class="mono" style="margin-top:8px">本研究针对多机器人团队中协调收益与通信成本之间的权衡问题，即无协调的个体最优行动会损害整体性能。方法将协调建模为从团队联合优化到纳什均衡行为的一个谱系，并将确定何时协调的问题转化为分析可微分运动规划问题中智能体目标函数的二阶特性。主要的实验结果是开发了利用这种二阶推理的算法，以确定团队中智能体最需要进行协调的具体时刻。</div>
</details>
</div>
<div class="card">
<div class="title">MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction</div>
<div class="meta-line">Authors: Jung Min Lee, Dohyeok Lee, Seokhun Ju, Taehyun Cho, Jin Woo Koo, Li Zhao, Sangwoo Hong, Jungwoo Lee</div>
<div class="meta-line">First: 2026-02-03T15:51:25+00:00 · Latest: 2026-02-03T15:51:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03668v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning \emph{latent actions} from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining. To make VLA pretraining effective, latent actions should contain information about the underlying agent&#x27;s actions despite the absence of ground-truth labels. We propose \textbf{M}ulti-\textbf{V}iew\textbf{P}oint \textbf{L}atent \textbf{A}ction \textbf{M}odel (\textbf{MVP-LAM}), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos. MVP-LAM trains latent actions with a \emph{cross-viewpoint reconstruction} objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues. On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation. Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MVP-LAM：通过跨视角重建学习以动作为中心的潜在动作</div>
<div class="mono" style="margin-top:8px">从多样化的人类视频中学习潜在动作，能够使机器人学习突破特定具身机器人数据集的限制，这些潜在动作近期已被用作视觉-语言-动作模型预训练的伪动作标签。为使VLA预训练有效，潜在动作应包含智能体底层动作信息，尽管缺乏真实标签。我们提出多视角潜在动作模型，该模型从时间同步的多视角视频中学习与真实动作高度相关的离散潜在动作。MVP-LAM通过跨视角重建目标训练潜在动作，使得从某一视角推断的潜在动作必须能解释另一视角的未来状态，从而减少对视角特定线索的依赖。在Bridge V2数据集上，MVP-LAM生成更具动作中心性的潜在动作，与真实动作的互信息更高，动作预测能力更强，包括在分布外评估中。最后，使用MVP-LAM潜在动作预训练的VLA模型，在SIMPLER和LIBERO-Long基准测试中提升了下游操作性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to learn latent actions from diverse human videos to scale robot learning beyond embodiment-specific datasets, addressing the need for action-centric pseudo-labels in vision-language-action (VLA) model pretraining. The method introduces MVP-LAM, which learns discrete latent actions from time-synchronized multi-view videos via a cross-viewpoint reconstruction objective, ensuring that latent actions inferred from one view can predict the future in another view to reduce viewpoint-specific biases. Experimental results on Bridge V2 show that MVP-LAM produces latent actions with higher mutual information with ground-truth actions and better action prediction, including out-of-distribution settings, and pretraining VLAs with these latent actions improves downstream manipulation performance on SIMPLER and LIBERO-Long benchmarks.</div>
<div class="mono" style="margin-top:8px">该研究旨在从多样化的人类视频中学习潜在动作，以扩展机器人学习至特定具身数据集之外，解决视觉-语言-动作（VLA）模型预训练中对动作中心伪标签的需求。方法MVP-LAM通过跨视角重建目标，从时间同步的多视角视频中学习离散潜在动作，即从一个视角推断的潜在动作必须能预测另一视角的未来状态，以减少对视角特定线索的依赖。在Bridge V2上的实验表明，MVP-LAM生成的潜在动作与真实动作具有更高的互信息，并提升了动作预测性能，包括在分布外评估中；使用这些潜在动作预训练VLA模型，进一步提高了在SIMPLER和LIBERO-Long基准上的下游操作性能。</div>
</details>
</div>
<div class="card">
<div class="title">Self-CriTeach: LLM Self-Teaching and Self-Critiquing for Improving Robotic Planning via Automated Domain Generation</div>
<div class="meta-line">Authors: Jinbang Huang, Zhiyuan Li, Yuanzhao Hu, Zhanguang Zhang, Mark Coates, Xingyue Quan, Yingxue Zhang</div>
<div class="meta-line">First: 2025-09-25T20:29:40+00:00 · Latest: 2026-02-03T15:50:10+00:00</div>
<div class="meta-line">Comments: 31 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21543v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21543v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have recently shown strong promise for robotic task planning, particularly through automatic planning domain generation. Planning domains are brittle under imperfect logical states and perception noise; prior approaches largely treat generated planning domains as plan utilities, overlooking their potential as scalable sources of reasoning supervision and structured reward signals. At the same time, reasoning LLMs depend on chain-of-thought (CoT) supervision that is expensive to collect for robotic tasks, and reinforcement learning (RL) faces challenges in reward engineering. We propose Self-CriTeach, an LLM self-teaching and self-critiquing framework in which an LLM autonomously generates symbolic planning domains that serve a dual role: (i) enabling large-scale generation of robotic planning problem-plan pairs, and (ii) providing structured reward functions. First, the self-written domains enable large-scale generation of symbolic task plans, which are automatically transformed into extended CoT trajectories for supervised fine-tuning. Second, the self-written domains are reused as structured reward functions, providing dense feedback for reinforcement learning without manual reward engineering. This unified training pipeline yields a planning-enhanced LLM with higher planning success rates, stronger cross-task generalization, reduced inference cost, and improved robustness to imperfect logical states.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Self-CriTeach：通过自动领域生成改进机器人规划的LLM自教自学与自我批判框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）近期在机器人任务规划领域展现出巨大潜力，尤其通过自动规划领域生成技术。规划领域在非完美逻辑状态和感知噪声下具有脆弱性；现有方法多将生成的规划领域视为规划工具，忽视了其作为可扩展推理监督源和结构化奖励信号的潜力。同时，推理型LLMs依赖思维链监督，而机器人任务的思维链标注成本高昂，强化学习也面临奖励工程设计的挑战。本文提出Self-CriTeach框架，通过LLM自主生成符号化规划领域实现双重功能：（1）支持大规模生成机器人规划问题-方案对；（2）提供结构化奖励函数。首先，自生成领域支持大规模符号化任务规划生成，并自动转化为扩展思维链轨迹用于监督微调。其次，自生成领域复用为结构化奖励函数，无需人工奖励工程即可为强化学习提供密集反馈。该统一训练流程最终产出具有更高规划成功率、更强跨任务泛化能力、更低推理成本及更优逻辑状态鲁棒性的规划增强型LLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the brittleness of LLM-generated planning domains for robotics under imperfect states and the high cost of obtaining chain-of-thought supervision. The proposed Self-CriTeach framework enables an LLM to autonomously generate symbolic planning domains, which serve a dual purpose: first, they facilitate the large-scale generation of problem-plan pairs that are automatically transformed into extended CoT trajectories for supervised fine-tuning; second, the same domains are reused as structured reward functions to provide dense feedback for reinforcement learning, eliminating manual reward engineering. Experimental results demonstrate that this unified training pipeline produces a planning-enhanced LLM with higher planning success rates, stronger cross-task generalization, lower inference cost, and improved robustness to imperfect logical states.</div>
<div class="mono" style="margin-top:8px">该研究针对大语言模型生成的规划领域在逻辑状态不完善和感知噪声下的脆弱性问题，同时应对机器人任务中思维链监督的高成本和强化学习中奖励工程设计的挑战。方法提出了Self-CriTeach框架，其中大语言模型自主生成符号规划领域，发挥双重作用：一方面支持大规模生成问题-规划对，用于产生扩展思维链轨迹以进行监督微调；另一方面作为结构化奖励函数，为强化学习提供密集反馈而无需人工设计奖励。主要实验结果表明，这一统一训练流程产出的规划增强型大语言模型具有更高的规划成功率、更强的跨任务泛化能力、更低的推理成本以及对不完善逻辑状态的更好鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Variance-Reduced Model Predictive Path Integral via Quadratic Model Approximation</div>
<div class="meta-line">Authors: Fabian Schramm, Franki Nguimatsia Tiofack, Nicolas Perrin-Gilbert, Marc Toussaint, Justin Carpentier</div>
<div class="meta-line">First: 2026-02-03T15:23:17+00:00 · Latest: 2026-02-03T15:23:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03639v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sampling-based controllers, such as Model Predictive Path Integral (MPPI) methods, offer substantial flexibility but often suffer from high variance and low sample efficiency. To address these challenges, we introduce a hybrid variance-reduced MPPI framework that integrates a prior model into the sampling process. Our key insight is to decompose the objective function into a known approximate model and a residual term. Since the residual captures only the discrepancy between the model and the objective, it typically exhibits a smaller magnitude and lower variance than the original objective. Although this principle applies to general modeling choices, we demonstrate that adopting a quadratic approximation enables the derivation of a closed-form, model-guided prior that effectively concentrates samples in informative regions. Crucially, the framework is agnostic to the source of geometric information, allowing the quadratic model to be constructed from exact derivatives, structural approximations (e.g., Gauss- or Quasi-Newton), or gradient-free randomized smoothing. We validate the approach on standard optimization benchmarks, a nonlinear, underactuated cart-pole control task, and a contact-rich manipulation problem with non-smooth dynamics. Across these domains, we achieve faster convergence and superior performance in low-sample regimes compared to standard MPPI. These results suggest that the method can make sample-based control strategies more practical in scenarios where obtaining samples is expensive or limited.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于二次模型近似的方差缩减模型预测路径积分方法</div>
<div class="mono" style="margin-top:8px">基于采样的控制器（如模型预测路径积分方法）虽具备高度灵活性，但常面临方差高、样本效率低的问题。为应对这些挑战，本文提出一种混合方差缩减MPPI框架，将先验模型融入采样过程。核心思路是将目标函数分解为已知近似模型与残差项：残差仅捕获模型与目标的差异，通常比原始目标量级更小、方差更低。尽管该原理适用于一般建模选择，我们证明采用二次近似可推导出闭式模型引导先验，有效将样本集中在信息丰富区域。该框架不依赖几何信息来源，允许通过精确导数、结构近似（如高斯或拟牛顿法）或无梯度随机平滑构建二次模型。我们在标准优化基准测试、非线性欠驱动倒立摆控制任务及非光滑动力学的密集接触操作问题上验证了该方法。实验表明，相比标准MPPI，本方法在低样本条件下实现了更快收敛与更优性能，证明其能提升采样控制策略在样本获取成本高或受限场景中的实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Sampling-based controllers like Model Predictive Path Integral (MPPI) are flexible but often exhibit high variance and low sample efficiency. To mitigate these issues, this work proposes a hybrid variance-reduced MPPI framework that incorporates a prior model into sampling by decomposing the objective into an approximate model and a residual term, where the residual&#x27;s lower magnitude and variance improve efficiency. The method specifically employs a quadratic model approximation to derive a closed-form, model-guided prior that concentrates samples in informative regions, and it is compatible with various geometric information sources including exact derivatives, structural approximations, or gradient-free smoothing. Experimental validation on optimization benchmarks, a nonlinear cart-pole control task, and a contact-rich manipulation problem demonstrates faster convergence and superior performance in low-sample regimes compared to standard MPPI, indicating enhanced practicality for sample-expensive applications.</div>
<div class="mono" style="margin-top:8px">基于采样的控制器，如模型预测路径积分（MPPI），具有灵活性但常面临高方差和低样本效率的问题。为应对这些挑战，本研究提出了一种混合方差缩减MPPI框架，通过将目标函数分解为一个近似模型和一个残差项，将先验模型整合到采样过程中，其中残差项因捕获模型与目标间的差异而具有更小的幅值和方差，从而提升效率。具体而言，采用二次近似能够推导出封闭形式的模型引导先验，有效将样本集中在信息丰富的区域，且该框架对几何信息的来源（如精确导数或无梯度随机平滑）保持不可知。在标准优化基准、非线性欠驱动倒立摆控制任务以及非光滑动力学的接触式操作问题上的实验验证表明，相比标准MPPI，该方法在低样本条件下实现了更快的收敛速度和更优的性能，使得在采样成本高昂或受限的场景中，基于采样的控制策略更具实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Mapping the Unseen: Unified Promptable Panoptic Mapping with Dynamic Labeling using Foundation Models</div>
<div class="meta-line">Authors: Mohamad Al Mdfaa, Raghad Salameh, Geesara Kulathunga, Sergey Zagoruyko, Gonzalo Ferrer</div>
<div class="meta-line">Venue: Robotics, vol. 15, no. 2, article 31, 2026</div>
<div class="meta-line">First: 2024-05-03T15:08:39+00:00 · Latest: 2026-02-03T15:20:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.02162v5">Abs</a> · <a href="https://arxiv.org/pdf/2405.02162v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Panoptic maps enable robots to reason about both geometry and semantics. However, open-vocabulary models repeatedly produce closely related labels that split panoptic entities and degrade volumetric consistency. The proposed UPPM advances open-world scene understanding by leveraging foundation models to introduce a panoptic Dynamic Descriptor that reconciles open-vocabulary labels with unified category structure and geometric size priors. The fusion for such dynamic descriptors is performed within a multi-resolution multi-TSDF map using language-guided open-vocabulary panoptic segmentation and semantic retrieval, resulting in a persistent and promptable panoptic map without additional model training. Based on our evaluation experiments, UPPM shows the best overall performance in terms of the map reconstruction accuracy and the panoptic segmentation quality. The ablation study investigates the contribution for each component of UPPM (custom NMS, blurry-frame filtering, and unified semantics) to the overall system performance. Consequently, UPPM preserves open-vocabulary interpretability while delivering strong geometric and panoptic accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>映射未见：利用基础模型动态标注的统一可提示全景映射</div>
<div class="mono" style="margin-top:8px">全景地图使机器人能够同时推理几何与语义信息。然而，开放词汇模型反复生成高度相关的标签，导致全景实体分裂并破坏体积一致性。所提出的UPPM通过利用基础模型引入全景动态描述符，将开放词汇标签与统一类别结构及几何尺寸先验相协调，从而推进开放世界场景理解。此类动态描述符的融合在多分辨率多TSDF地图中执行，结合语言引导的开放词汇全景分割与语义检索，无需额外模型训练即可生成持久且可提示的全景地图。根据评估实验，UPPM在地图重建精度和全景分割质量方面均表现出最佳综合性能。消融研究探讨了UPPM各组件（定制非极大值抑制、模糊帧过滤和统一语义）对整体系统性能的贡献。因此，UPPM在保持开放词汇可解释性的同时，实现了强大的几何与全景精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the issue of open-vocabulary models producing fragmented and inconsistent labels that degrade volumetric map quality, this research introduces the Unified Promptable Panoptic Mapping (UPPM) method. It leverages foundation models to create a panoptic Dynamic Descriptor that reconciles diverse labels using unified category and size priors, integrated via language-guided segmentation and semantic retrieval within a multi-resolution map without requiring retraining. Experimental results demonstrate that UPPM achieves superior performance in map reconstruction accuracy and panoptic segmentation quality, with ablation studies confirming the contributions of its custom components to the overall system.</div>
<div class="mono" style="margin-top:8px">为解决开放词汇模型生成不一致、碎片化标签从而降低体素地图一致性的问题，本研究提出了UPPM，一个统一的可提示全景建图框架。该方法引入了一种全景动态描述符，将开放词汇标签与统一的类别结构和几何尺寸先验相结合，利用基础模型在无需额外训练的多分辨率多TSDF地图中进行语言引导的分割和语义检索。实验结果表明，UPPM在建图精度和全景分割质量上均取得了最佳整体性能，消融研究证实了其定制非极大值抑制、模糊帧过滤和统一语义等组件对系统性能的贡献，在保持开放词汇可解释性的同时实现了强大的几何和全景精度。</div>
</details>
</div>
<div class="card">
<div class="title">Self-supervised Physics-Informed Manipulation of Deformable Linear Objects with Non-negligible Dynamics</div>
<div class="meta-line">Authors: Youyuan Long, Gokhan Solak, Sara Zeynalpour, Heng Zhang, Arash Ajoudani</div>
<div class="meta-line">First: 2026-02-03T15:14:09+00:00 · Latest: 2026-02-03T15:14:09+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Robotics. Video: https://youtu.be/lgX2J-00TRM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03623v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03623v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We address dynamic manipulation of deformable linear objects by presenting SPiD, a physics-informed self-supervised learning framework that couples an accurate deformable object model with an augmented self-supervised training strategy. On the modeling side, we extend a mass-spring model to more accurately capture object dynamics while remaining lightweight enough for high-throughput rollouts during self-supervised learning. On the learning side, we train a neural controller using a task-oriented cost, enabling end-to-end optimization through interaction with the differentiable object model. In addition, we propose a self-supervised DAgger variant that detects distribution shift during deployment and performs offline self-correction to further enhance robustness without expert supervision. We evaluate our method primarily on the rope stabilization task, where a robot must bring a swinging rope to rest as quickly and smoothly as possible. Extensive experiments in both simulation and the real world demonstrate that the proposed controller achieves fast and smooth rope stabilization, generalizing across unseen initial states, rope lengths, masses, non-uniform mass distributions, and external disturbances. Additionally, we develop an affordable markerless rope perception method and demonstrate that our controller maintains performance with noisy and low-frequency state updates. Furthermore, we demonstrate the generality of the framework by extending it to the rope trajectory tracking task. Overall, SPiD offers a data-efficient, robust, and physically grounded framework for dynamic manipulation of deformable linear objects, featuring strong sim-to-real generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物理信息自监督学习的动态可变形线性物体操控方法</div>
<div class="mono" style="margin-top:8px">本文提出SPiD框架，通过耦合精确的可变形物体模型与增强型自监督训练策略，实现动态可变形线性物体的操控。建模方面，我们扩展质量-弹簧模型以更准确捕捉物体动力学特性，同时保持轻量化以满足自监督学习中的高通量推演需求。学习方面，采用任务导向代价函数训练神经控制器，通过可微分物体模型的交互实现端到端优化。此外，提出自监督DAgger变体方法，能在部署时检测分布偏移并执行离线自校正，无需专家监督即可增强鲁棒性。我们主要在绳索稳定任务中评估方法——要求机器人尽可能快速平稳地使摆动绳索静止。大量仿真与实物实验表明，所提控制器能实现快速平滑的绳索稳定，对未见初始状态、绳长、质量、非均匀质量分布及外部干扰均具泛化能力。同时开发了低成本无标记绳索感知方法，验证了控制器在噪声和低频状态更新下的性能保持。进一步将框架扩展至绳索轨迹跟踪任务，展示了其通用性。总体而言，SPiD为动态可变形线性物体操控提供了数据高效、鲁棒且物理基础扎实的框架，具备优异的仿真到现实泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of dynamic manipulation of deformable linear objects, where non-negligible dynamics complicate control. The method, SPiD, is a physics-informed self-supervised learning framework that couples an enhanced, lightweight mass-spring model for accurate dynamics simulation with a neural controller trained via a task-oriented cost and a self-supervised DAgger variant for offline self-correction. Experimental results on rope stabilization show the controller achieves fast and smooth stabilization, generalizing to unseen conditions like varying rope properties and disturbances, and maintains performance with noisy perception; the framework also proves effective for trajectory tracking.</div>
<div class="mono" style="margin-top:8px">本研究旨在实现对具有显著动态特性的可变形线性物体（如绳索）进行鲁棒的动态操控。所提出的方法SPiD是一个自监督学习框架，它集成了一个可微分的、扩展的质量-弹簧模型以进行精确且计算高效的物理仿真，并结合了一个使用任务导向损失进行端到端训练的神经控制器。一个关键创新是一种自监督的DAgger变体，它能在部署过程中检测并纠正分布偏移，从而在没有专家数据的情况下增强鲁棒性。在绳索稳定任务上的实验评估表明，该控制器能实现快速平滑的稳定，在仿真和真实环境中均能有效泛化到未见过的初始状态、变化的物理属性和外部干扰，即使在使用经济型无标记感知系统提供的噪声大、频率低的状态估计时也是如此。</div>
</details>
</div>
<div class="card">
<div class="title">SEMNAV: Enhancing Visual Semantic Navigation in Robotics through Semantic Segmentation</div>
<div class="meta-line">Authors: Rafael Flor-Rodríguez, Carlos Gutiérrez-Álvarez, Francisco Javier Acevedo-Rodríguez, Sergio Lafuente-Arroyo, Roberto J. López-Sastre</div>
<div class="meta-line">First: 2025-06-02T08:19:41+00:00 · Latest: 2026-02-03T14:58:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.01418v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.01418v2">PDF</a> · <a href="https://github.com/gramuah/semnav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Semantic Navigation (VSN) is a fundamental problem in robotics, where an agent must navigate toward a target object in an unknown environment, mainly using visual information. Most state-of-the-art VSN models are trained in simulation environments, where rendered scenes of the real world are used, at best. These approaches typically rely on raw RGB data from the virtual scenes, which limits their ability to generalize to real-world environments due to domain adaptation issues. To tackle this problem, in this work, we propose SEMNAV, a novel approach that leverages semantic segmentation as the main visual input representation of the environment to enhance the agent&#x27;s perception and decision-making capabilities. By explicitly incorporating this type of high-level semantic information, our model learns robust navigation policies that improve generalization across unseen environments, both in simulated and real world settings. We also introduce the SEMNAV dataset, a newly curated dataset designed for training semantic segmentation-aware navigation models like SEMNAV. Our approach is evaluated extensively in both simulated environments and with real-world robotic platforms. Experimental results demonstrate that SEMNAV outperforms existing state-of-the-art VSN models, achieving higher success rates in the Habitat 2.0 simulation environment, using the HM3D dataset. Furthermore, our real-world experiments highlight the effectiveness of semantic segmentation in mitigating the sim-to-real gap, making our model a promising solution for practical VSN-based robotic applications. The code and datasets are accessible at https://github.com/gramuah/semnav</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SEMNAV：通过语义分割增强机器人视觉语义导航能力</div>
<div class="mono" style="margin-top:8px">视觉语义导航是机器人领域的基础问题，要求智能体在未知环境中主要依赖视觉信息导航至目标物体。当前最先进的VSN模型多在仿真环境中训练，最多仅使用真实场景的渲染图像。这些方法通常依赖虚拟场景的原始RGB数据，由于领域适应问题，其向真实环境的泛化能力受限。为解决此问题，本研究提出SEMNAV——一种创新方法，利用语义分割作为环境的主要视觉输入表征，以增强智能体的感知与决策能力。通过显式融合此类高层语义信息，我们的模型能学习到鲁棒的导航策略，在仿真与真实场景中均展现出对未知环境的优异泛化性能。我们还发布了SEMNAV数据集，这是专为训练语义分割感知导航模型而构建的新数据集。我们在仿真环境与真实机器人平台上进行了全面评估，实验结果表明：在Habitat 2.0仿真环境中使用HM3D数据集时，SEMNAV以更高成功率超越现有最先进VSN模型。真实环境实验进一步验证了语义分割在缩小仿真-现实差距方面的有效性，使本模型成为实用化VSN机器人应用的潜力解决方案。代码与数据集可通过https://github.com/gramuah/semnav获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual Semantic Navigation (VSN) models trained in simulation often struggle to generalize to real-world environments due to domain adaptation issues from relying on raw RGB inputs. To address this, the authors propose SEMNAV, a method that uses semantic segmentation as the primary visual input to enhance perception and decision-making, and introduce the SEMNAV dataset for training such models. Experiments in Habitat 2.0 simulation with the HM3D dataset show SEMNAV achieves higher success rates than state-of-the-art VSN models, and real-world robotic tests confirm its effectiveness in reducing the sim-to-real gap.</div>
<div class="mono" style="margin-top:8px">基于仿真训练的视觉语义导航模型通常依赖原始RGB输入，存在领域适应问题，难以泛化到真实环境。为解决此问题，本文提出SEMNAV方法，该方法利用语义分割图作为主要视觉输入，以增强智能体的感知和决策能力，学习更鲁棒的导航策略。在Habitat 2.0仿真环境使用HM3D数据集的实验表明，SEMNAV的成功率优于现有先进模型；真实世界机器人实验进一步验证了语义分割在缩小仿真与现实差距方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Human-in-the-Loop Failure Recovery with Adaptive Task Allocation</div>
<div class="meta-line">Authors: Lorena Maria Genua, Nikita Boguslavskii, Zhi Li</div>
<div class="meta-line">First: 2026-02-03T14:55:48+00:00 · Latest: 2026-02-03T14:55:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03603v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03603v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since the recent Covid-19 pandemic, mobile manipulators and humanoid assistive robots with higher levels of autonomy have increasingly been adopted for patient care and living assistance. Despite advancements in autonomy, these robots often struggle to perform reliably in dynamic and unstructured environments and require human intervention to recover from failures. Effective human-robot collaboration is essential to enable robots to receive assistance from the most competent operator, in order to reduce their workload and minimize disruptions in task execution. In this paper, we propose an adaptive method for allocating robotic failures to human operators (ARFA). Our proposed approach models the capabilities of human operators, and continuously updates these beliefs based on their actual performance for failure recovery. For every failure to be resolved, a reward function calculates expected outcomes based on operator capabilities and historical data, task urgency, and current workload distribution. The failure is then assigned to the operator with the highest expected reward. Our simulations and user studies show that ARFA outperforms random allocation, significantly reducing robot idle time, improving overall system performance, and leading to a more distributed workload among operators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应任务分配的人机协同故障恢复</div>
<div class="mono" style="margin-top:8px">自近期新冠疫情以来，具有更高自主性的移动机械臂和人形辅助机器人已日益广泛应用于患者护理与生活辅助领域。尽管自主性有所提升，这些机器人在动态和非结构化环境中仍难以可靠运行，常需人工干预以从故障中恢复。有效的人机协作至关重要，它能使机器人获得最熟练操作员的协助，从而减轻操作员负担并最小化任务执行中的中断。本文提出一种自适应机器人故障分配至操作员的方法（ARFA）。该方法建模操作员的能力，并依据其实际故障恢复表现持续更新评估。针对每个待解决的故障，奖励函数基于操作员能力与历史数据、任务紧迫性及当前工作负荷分布计算预期结果，随后将故障分配给预期奖励最高的操作员。仿真与用户研究表明，ARFA优于随机分配，能显著减少机器人闲置时间，提升整体系统性能，并使操作员间工作负荷分布更均衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve human-robot collaboration for assistive robots, which often fail in unstructured environments and require human intervention for recovery. The proposed method, Adaptive Robotic Failure Allocation (ARFA), dynamically models operator capabilities and assigns each failure to the most suitable human operator based on a reward function that considers capability, task urgency, and workload. Experimental results from simulations and user studies demonstrate that ARFA significantly reduces robot idle time, enhances system performance, and achieves a more balanced workload distribution compared to random allocation.</div>
<div class="mono" style="margin-top:8px">为提升医疗等动态环境中人机协作的效率，解决机器人因故障常需人工干预的问题，本文提出了ARFA，一种将机器人故障自适应分配给人类操作员的方法。该方法对操作员能力进行建模，根据实际表现更新评估，并通过一个综合考虑能力、任务紧迫性和工作负载的奖励函数，将故障分配给预期收益最高的操作员。仿真和用户研究表明，与随机分配相比，ARFA显著减少了机器人闲置时间，提高了整体系统性能，并实现了更均衡的工作负载分配。</div>
</details>
</div>
<div class="card">
<div class="title">Spiking Neural Networks for Continuous Control via End-to-End Model-Based Learning</div>
<div class="meta-line">Authors: Justus Huebotter, Pablo Lanillos, Marcel van Gerven, Serge Thill</div>
<div class="meta-line">First: 2025-09-03T12:05:58+00:00 · Latest: 2026-02-03T14:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05356v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.05356v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent progress in training spiking neural networks (SNNs) for classification, their application to continuous motor control remains limited. Here, we demonstrate that fully spiking architectures can be trained end-to-end to control robotic arms with multiple degrees of freedom in continuous environments. Our predictive-control framework combines Leaky Integrate-and-Fire dynamics with surrogate gradients, jointly optimizing a forward model for dynamics prediction and a policy network for goal-directed action. We evaluate this approach on both a planar 2D reaching task and a simulated 6-DOF Franka Emika Panda robot with torque control. In direct comparison to non-spiking recurrent baselines trained under the same predictive-control pipeline, the proposed SNN achieves comparable task performance while using substantially fewer parameters. An extensive ablation study highlights the role of initialization, learnable time constants, adaptive thresholds, and latent-space compression as key contributors to stable training and effective control. Together, these findings establish spiking neural networks as a viable and scalable substrate for high-dimensional continuous control, while emphasizing the importance of principled architectural and training design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于端到端模型学习的脉冲神经网络连续控制方法</div>
<div class="mono" style="margin-top:8px">尽管脉冲神经网络在分类任务训练方面取得进展，但其在连续运动控制中的应用仍有限。本研究证明，完全脉冲架构可通过端到端训练实现多自由度机械臂在连续环境中的控制。我们提出的预测控制框架将漏积分发放动力学与代理梯度相结合，联合优化用于动力学预测的前向模型和面向目标动作的策略网络。该方法在平面二维到达任务和具有扭矩控制的模拟六自由度Franka Emika Panda机器人上均得到验证。与相同预测控制流程训练的非脉冲循环基线模型直接对比，所提出的脉冲神经网络在显著减少参数量的同时实现了可比的任务性能。深入的消融实验表明，初始化策略、可学习时间常数、自适应阈值和潜空间压缩是稳定训练与有效控制的关键因素。这些发现共同确立了脉冲神经网络作为高维连续控制可行且可扩展基底的潜力，同时强调了体系结构与训练设计的原理性重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limited application of spiking neural networks (SNNs) in continuous motor control by developing an end-to-end model-based learning framework. The method integrates Leaky Integrate-and-Fire dynamics with surrogate gradients to jointly optimize a forward dynamics model and a policy network for robotic control. Experimental results on 2D reaching and 6-DOF robotic arm tasks show that the fully spiking architecture achieves performance comparable to non-spiking recurrent baselines while using significantly fewer parameters, with ablation studies identifying key design elements like learnable time constants and adaptive thresholds for stable training.</div>
<div class="mono" style="margin-top:8px">本研究针对脉冲神经网络在连续运动控制中应用有限的问题，开发了一种端到端的基于模型的学习框架。该方法将漏积分发放动力学与替代梯度相结合，联合优化用于机器人控制的前向动力学模型和策略网络。在二维到达和六自由度机械臂任务上的实验结果表明，全脉冲架构在显著减少参数量的同时，达到了与非脉冲循环基线相当的性能，消融研究进一步明确了实现稳定训练的关键设计因素。</div>
</details>
</div>
<div class="card">
<div class="title">Driving on Registers</div>
<div class="meta-line">Authors: Ellington Kirby, Alexandre Boulch, Yihong Xu, Yuan Yin, Gilles Puy, Éloi Zablocki, Andrei Bursuc, Spyros Gidaris, Renaud Marlet, Florent Bartoccioni, Anh-Quan Cao, Nermin Samet, Tuan-Hung VU, Matthieu Cord</div>
<div class="meta-line">First: 2026-01-08T16:28:24+00:00 · Latest: 2026-02-03T14:21:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05083v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05083v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present DrivoR, a simple and efficient transformer-based architecture for end-to-end autonomous driving. Our approach builds on pretrained Vision Transformers (ViTs) and introduces camera-aware register tokens that compress multi-camera features into a compact scene representation, significantly reducing downstream computation without sacrificing accuracy. These tokens drive two lightweight transformer decoders that generate and then score candidate trajectories. The scoring decoder learns to mimic an oracle and predicts interpretable sub-scores representing aspects such as safety, comfort, and efficiency, enabling behavior-conditioned driving at inference. Despite its minimal design, DrivoR outperforms or matches strong contemporary baselines across NAVSIM-v1, NAVSIM-v2, and the photorealistic closed-loop HUGSIM benchmark. Our results show that a pure-transformer architecture, combined with targeted token compression, is sufficient for accurate, efficient, and adaptive end-to-end driving. Code and checkpoints will be made available via the project page.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于寄存器的自动驾驶</div>
<div class="mono" style="margin-top:8px">我们提出DrivoR，一种简洁高效的基于Transformer的端到端自动驾驶架构。该方法基于预训练的视觉Transformer（ViT），引入相机感知寄存器令牌，将多摄像头特征压缩为紧凑的场景表示，在保持精度的同时显著降低下游计算量。这些令牌驱动两个轻量级Transformer解码器，首先生成候选轨迹，随后进行评分。评分解码器学习模拟最优策略，预测可解释的子分数，代表安全性、舒适性和效率等方面，实现推理过程中的行为条件化驾驶。尽管设计极简，DrivoR在NAVSIM-v1、NAVSIM-v2及逼真闭环仿真基准HUGSIM上均优于或匹配当前主流基线模型。结果表明，纯Transformer架构结合定向令牌压缩技术，足以实现精准、高效且自适应的端到端驾驶。代码与模型权重将通过项目页面公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the computational inefficiency of existing end-to-end autonomous driving systems by proposing DrivoR, a transformer-based architecture that compresses multi-camera inputs. The method builds on pretrained Vision Transformers and introduces camera-aware register tokens to create compact scene representations, which feed into two lightweight decoders for trajectory generation and scoring. Experimental results demonstrate that DrivoR outperforms or matches contemporary baselines on NAVSIM-v1, NAVSIM-v2, and HUGSIM benchmarks while maintaining accuracy with reduced computation, showing that targeted token compression enables efficient and adaptive driving behavior.</div>
<div class="mono" style="margin-top:8px">该研究旨在开发一种高效的端到端自动驾驶模型，以降低计算需求同时保持高性能。方法提出了DrivoR，这是一种基于Transformer的架构，使用预训练的视觉Transformer（ViT）和相机感知寄存器令牌将多相机特征压缩为紧凑的场景表示，然后通过两个轻量级Transformer解码器生成和评分候选轨迹；评分解码器模拟预言机来预测安全性、舒适性和效率等可解释的子分数，从而实现行为条件驾驶。实验结果表明，DrivoR在NAVSIM-v1、NAVSIM-v2和HUGSIM基准测试中优于或匹配现有强基线，证明纯Transformer设计结合令牌压缩能够实现准确、高效和自适应的驾驶。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Player, Multi-Strategy Quantum Game Model for Interaction-Aware Decision-Making in Autonomous Driving</div>
<div class="meta-line">Authors: Karim Essalmi, Fernando Garrido, Fawzi Nashashibi</div>
<div class="meta-line">First: 2026-02-03T14:14:11+00:00 · Latest: 2026-02-03T14:14:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03571v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03571v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although significant progress has been made in decision-making for automated driving, challenges remain for deployment in the real world. One challenge lies in addressing interaction-awareness. Most existing approaches oversimplify interactions between the ego vehicle and surrounding agents, and often neglect interactions among the agents themselves. A common solution is to model these interactions using classical game theory. However, its formulation assumes rational players, whereas human behavior is frequently uncertain or irrational. To address these challenges, we propose the Quantum Game Decision-Making (QGDM) model, a novel framework that combines classical game theory with quantum mechanics principles (such as superposition, entanglement, and interference) to tackle multi-player, multi-strategy decision-making problems. To the best of our knowledge, this is one of the first studies to apply quantum game theory to decision-making for automated driving. QGDM runs in real time on a standard computer, without requiring quantum hardware. We evaluate QGDM in simulation across various scenarios, including roundabouts, merging, and highways, and compare its performance with multiple baseline methods. Results show that QGDM significantly improves success rates and reduces collision rates compared to classical approaches, particularly in scenarios with high interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向自动驾驶交互感知决策的多参与者多策略量子博弈模型</div>
<div class="mono" style="margin-top:8px">尽管自动驾驶决策已取得显著进展，但在实际部署中仍面临挑战，其中之一便是交互感知问题。现有方法大多过度简化自车与周围交通参与者之间的交互，且常忽略其他参与者彼此间的相互作用。经典博弈论是常用的建模工具，但其假设参与者完全理性，而人类行为常具有不确定性或非理性特征。为应对这些挑战，我们提出量子博弈决策模型——一种融合经典博弈论与量子力学原理（如叠加、纠缠和干涉）的新型框架，用于解决多参与者、多策略的决策问题。据我们所知，这是将量子博弈论应用于自动驾驶决策的首批研究之一。该模型可在标准计算机上实时运行，无需量子硬件支持。我们在环岛、汇流、高速公路等多种仿真场景中对模型进行评估，并与多种基线方法进行性能对比。结果表明，相较于经典方法，量子博弈决策模型显著提升了成功率并降低了碰撞率，尤其在高度交互的场景中表现突出。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of classical game theory in modeling uncertain or irrational human behavior for interaction-aware autonomous driving, this study proposes the Quantum Game Decision-Making (QGDM) model. The method integrates principles from quantum mechanics, such as superposition and entanglement, into a multi-player, multi-strategy game framework to better capture complex interactions between the ego vehicle and surrounding agents, and it operates in real-time on standard computers without quantum hardware. Experimental evaluation in simulated scenarios like roundabouts and highway merging demonstrates that QGDM significantly increases success rates and reduces collision rates compared to classical baseline methods, especially in high-interaction situations.</div>
<div class="mono" style="margin-top:8px">该研究针对自动驾驶中交互感知决策的挑战，现有方法常过度简化自车与其他交通参与者之间的交互并忽略参与者间的动态，而经典博弈论假设的理性行为与人类的不确定性不符。提出的量子博弈决策模型将经典博弈论与量子力学原理如叠加、纠缠和干涉相结合，以处理多参与者、多策略场景，可在标准计算机上实时运行而无需量子硬件。在环岛、汇入和高速公路等模拟场景中的实验评估表明，相比经典基线方法，该模型显著提高了成功率并降低了碰撞率，尤其在高度交互的场景中效果更佳。</div>
</details>
</div>
<div class="card">
<div class="title">Formal Evidence Generation for Assurance Cases for Robotic Software Models</div>
<div class="meta-line">Authors: Fang Yan, Simon Foster, Ana Cavalcanti, Ibrahim Habli, James Baxter</div>
<div class="meta-line">First: 2026-02-03T14:01:30+00:00 · Latest: 2026-02-03T14:01:30+00:00</div>
<div class="meta-line">Comments: This is a preprint. The paper is currently under review at Software and Systems Modeling</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03550v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03550v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotics and Autonomous Systems are increasingly deployed in safety-critical domains, so that demonstrating their safety is essential. Assurance Cases (ACs) provide structured arguments supported by evidence, but generating and maintaining this evidence is labour-intensive, error-prone, and difficult to keep consistent as systems evolve. We present a model-based approach to systematically generating AC evidence by embedding formal verification into the assurance workflow. The approach addresses three challenges: systematically deriving formal assertions from natural language requirements using templates, orchestrating multiple formal verification tools to handle diverse property types, and integrating formal evidence production into the workflow. Leveraging RoboChart, a domain-specific modelling language with formal semantics, we combine model checking and theorem proving in our approach. Structured requirements are automatically transformed into formal assertions using predefined templates, and verification results are automatically integrated as evidence. Case studies demonstrate the effectiveness of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向机器人软件模型保证案例的形式化证据生成</div>
<div class="mono" style="margin-top:8px">机器人与自主系统日益部署于安全关键领域，因此证明其安全性至关重要。保证案例通过结构化论证提供证据支持，但证据的生成和维护具有劳动密集、易出错、且难以随系统演进保持一致的挑战。本文提出一种基于模型的方法，通过将形式化验证嵌入保证工作流来系统化生成保证案例证据。该方法应对三大挑战：利用模板从自然语言需求系统化推导形式化断言；协调多种形式化验证工具以处理不同属性类型；将形式化证据生成整合至工作流。基于具备形式语义的领域特定建模语言RoboChart，本方法结合了模型检测与定理证明技术。结构化需求通过预定义模板自动转换为形式化断言，验证结果自动整合为证据。案例研究验证了本方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to demonstrate safety for robotics and autonomous systems in critical domains, where constructing and maintaining evidence for Assurance Cases is labor-intensive and error-prone. The method introduces a model-based approach that systematically generates formal evidence by embedding verification into the assurance workflow, using RoboChart for modeling, templates to derive formal assertions from natural language requirements, and orchestrating multiple verification tools like model checking and theorem proving. Experimental case studies show the approach effectively automates evidence generation and integration, addressing consistency and efficiency challenges.</div>
<div class="mono" style="margin-top:8px">该研究的动机是解决安全关键机器人系统中，为保障案例生成和维护证据时存在的工作量大、易出错且难以保持一致性的问题。方法上提出了一种基于模型的方法，通过使用具有形式语义的领域特定建模语言RoboChart，将形式化验证系统性地嵌入保障工作流，利用模板从结构化需求自动生成形式化断言，并协调模型检测和定理证明等多种验证工具。案例研究表明，该方法能有效自动生成并整合形式化证据。</div>
</details>
</div>
<div class="card">
<div class="title">AffordanceGrasp-R1:Leveraging Reasoning-Based Affordance Segmentation with Reinforcement Learning for Robotic Grasping</div>
<div class="meta-line">Authors: Dingyi Zhou, Mu He, Zhuowei Fang, Xiangtong Yao, Yinlong Liu, Alois Knoll, Hu Cao</div>
<div class="meta-line">First: 2026-02-03T14:00:56+00:00 · Latest: 2026-02-03T14:00:56+00:00</div>
<div class="meta-line">Comments: Preprint version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03547v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03547v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce AffordanceGrasp-R1, a reasoning-driven affordance segmentation framework for robotic grasping that combines a chain-of-thought (CoT) cold-start strategy with reinforcement learning to enhance deduction and spatial grounding. In addition, we redesign the grasping pipeline to be more context-aware by generating grasp candidates from the global scene point cloud and subsequently filtering them using instruction-conditioned affordance masks. Extensive experiments demonstrate that AffordanceGrasp-R1 consistently outperforms state-of-the-art (SOTA) methods on benchmark datasets, and real-world robotic grasping evaluations further validate its robustness and generalization under complex language-conditioned manipulation scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AffordanceGrasp-R1：基于推理的可用性分割与强化学习融合的机器人抓取方法</div>
<div class="mono" style="margin-top:8px">本文提出AffordanceGrasp-R1，一种基于推理的机器人抓取可用性分割框架，通过结合思维链冷启动策略与强化学习来增强推理与空间定位能力。同时，我们重新设计了抓取流程以提升上下文感知能力：从全局场景点云生成抓取候选，再通过指令条件化的可用性掩码进行筛选。大量实验表明，AffordanceGrasp-R1在基准数据集上持续优于现有最优方法，真实场景的机器人抓取评估进一步验证了其在复杂语言条件操作场景下的鲁棒性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to improve robotic grasping in complex, language-conditioned scenarios by enhancing the system&#x27;s reasoning and spatial understanding. The proposed AffordanceGrasp-R1 method integrates a chain-of-thought reasoning strategy for cold-start initialization with reinforcement learning, and redesigns the grasping pipeline to generate candidates from a global scene point cloud before filtering them with instruction-conditioned affordance masks. Experimental results show that the framework consistently surpasses state-of-the-art methods on benchmarks and demonstrates robust generalization in real-world robotic grasping tasks.</div>
<div class="mono" style="margin-top:8px">为提升复杂语言条件场景下的机器人抓取能力，本研究提出了AffordanceGrasp-R1框架，该框架将基于推理的思维链启动策略与强化学习相结合，以增强推理和空间定位能力。该方法采用重新设计的、具有上下文感知的流程，首先生成全局场景点云的抓取候选，随后使用指令条件的可供性掩码对其进行筛选。大量实验表明，该框架在基准数据集上持续优于现有最优方法，并且在真实世界机器人抓取评估中进一步验证了其在复杂语言条件操作场景下的鲁棒性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Investigating the Influence of Spatial Ability in Augmented Reality-assisted Robot Programming</div>
<div class="meta-line">Authors: Nicolas Leins, Jana Gonnermann-Müller, Malte Teichmann, Sebastian Pokutta</div>
<div class="meta-line">First: 2026-02-03T13:58:54+00:00 · Latest: 2026-02-03T13:58:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03544v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03544v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Augmented Reality (AR) offers promising opportunities to enhance learning, but its mechanisms and effects are not yet fully understood. As learning becomes increasingly personalized, considering individual learner characteristics becomes more important. This study investigates the moderating effect of spatial ability on learning experience with AR in the context of robot programming. A between-subjects experiment ($N=71$) compared conventional robot programming to an AR-assisted approach using a head-mounted display. Participants&#x27; spatial ability was assessed using the Mental Rotation Test. The learning experience was measured through the System Usability Scale (SUS) and cognitive load. The results indicate that AR support does not significantly improve the learning experience compared to the conventional approach. However, AR appears to have a compensatory effect on the influence of spatial ability. In the control group, spatial ability was significantly positively associated with SUS scores and negatively associated with extraneous cognitive load, indicating that higher spatial ability predicts a better learning experience. In the AR condition, these relationships were not observable, suggesting that AR mitigated the disadvantage typically experienced by learners with lower spatial abilities. These findings suggest that AR can serve a compensatory function by reducing the influence of learner characteristics. Future research should further explore this compensatory role of AR to guide the design of personalized learning environments that address diverse learner needs and reduce barriers for learners with varying cognitive profiles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究空间能力在增强现实辅助机器人编程中的影响</div>
<div class="mono" style="margin-top:8px">增强现实（AR）为促进学习提供了广阔前景，但其作用机制与效果尚未被完全理解。随着学习日益个性化，考量学习者个体特征变得愈发重要。本研究探讨了在机器人编程情境中，空间能力对AR学习体验的调节作用。通过一项被试间实验（N=71），对比了传统机器人编程与采用头戴式显示器的AR辅助方法。使用心理旋转测试评估参与者的空间能力，并通过系统可用性量表（SUS）与认知负荷测量学习体验。结果表明，与传统方法相比，AR支持并未显著改善学习体验。然而，AR似乎对空间能力的影响具有补偿效应：在对照组中，空间能力与SUS得分呈显著正相关，与外部认知负荷呈负相关，表明较高空间能力可预测更好的学习体验；而在AR条件下，这些关联均未显现，说明AR缓解了低空间能力学习者通常面临的不利条件。这些发现表明，AR可通过减弱学习者特征的影响发挥补偿功能。未来研究应进一步探索AR的补偿作用，以指导设计满足多样化学习需求、降低不同认知特征学习者障碍的个性化学习环境。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how individual differences in spatial ability moderate the learning experience in Augmented Reality (AR)-assisted robot programming, motivated by the need to understand AR&#x27;s mechanisms for personalized learning. A between-subjects experiment with 71 participants compared a conventional programming method to an AR-assisted approach using a head-mounted display, measuring spatial ability via the Mental Rotation Test and learning experience via the System Usability Scale (SUS) and cognitive load. Results showed that AR support did not significantly improve the overall learning experience compared to the conventional method. However, AR exhibited a compensatory effect: in the control group, higher spatial ability predicted better SUS scores and lower extraneous cognitive load, whereas in the AR condition, these relationships disappeared, indicating that AR mitigated the disadvantage typically faced by learners with lower spatial abilities.</div>
<div class="mono" style="margin-top:8px">本研究探讨增强现实（AR）能否通过调节个体空间能力的影响，实现机器人编程学习的个性化。研究进行了一项组间实验（N=71），比较了传统编程方法与使用头戴式显示器的AR辅助方法，通过心理旋转测试评估空间能力，并通过系统可用性量表（SUS）和认知负荷衡量学习体验。结果表明，与传统方法相比，AR并未显著改善整体学习体验，但具有补偿效应：在对照组中，较高的空间能力预示着更好的SUS得分和更低的外部认知负荷；而在AR条件下，这种关系消失，表明AR缓解了空间能力较低学习者的劣势。</div>
</details>
</div>
<div class="card">
<div class="title">CMR: Contractive Mapping Embeddings for Robust Humanoid Locomotion on Unstructured Terrains</div>
<div class="meta-line">Authors: Qixin Zeng, Hongyin Zhang, Shangke Lyu, Junxi Jin, Donglin Wang, Chao Huang</div>
<div class="meta-line">First: 2026-02-03T13:30:18+00:00 · Latest: 2026-02-03T13:30:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03511v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03511v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust disturbance rejection remains a longstanding challenge in humanoid locomotion, particularly on unstructured terrains where sensing is unreliable and model mismatch is pronounced. While perception information, such as height map, enhances terrain awareness, sensor noise and sim-to-real gaps can destabilize policies in practice. In this work, we provide theoretical analysis that bounds the return gap under observation noise, when the induced latent dynamics are contractive. Furthermore, we present Contractive Mapping for Robustness (CMR) framework that maps high-dimensional, disturbance-prone observations into a latent space, where local perturbations are attenuated over time. Specifically, this approach couples contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while explicitly controlling sensitivity. Notably, the formulation can be incorporated into modern deep reinforcement learning pipelines as an auxiliary loss term with minimal additional technical effort required. Further, our extensive humanoid experiments show that CMR potently outperforms other locomotion algorithms under increased noise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CMR：基于压缩映射嵌入的仿人机器人非结构化地形鲁棒运动控制</div>
<div class="mono" style="margin-top:8px">鲁棒的扰动抑制始终是仿人机器人运动控制中的长期挑战，尤其在感知不可靠且模型失配显著的非结构化地形上。虽然高度图等感知信息增强了地形认知，但传感器噪声与仿真到现实的差距在实践中易导致策略失稳。本研究通过理论分析证明：当潜在动力学具有压缩性时，观测噪声引起的回报差距存在明确上界。进而提出鲁棒性压缩映射框架，将高维易受扰动的观测映射至潜在空间，使局部扰动随时间衰减。该方法通过对比表征学习与Lipschitz正则化的耦合，在显式控制敏感度的同时保持任务相关几何结构。该框架可作为辅助损失项无缝集成至现代深度强化学习流程，无需过多额外技术负担。大量仿人机器人实验表明，在增强噪声环境下CMR显著优于其他运动控制算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Robust humanoid locomotion on unstructured terrains is challenged by unreliable sensing and model mismatch, which can destabilize policies that rely on noisy perception data. To address this, the authors propose the Contractive Mapping for Robustness (CMR) framework, which maps high-dimensional, noisy observations into a contractive latent space where local perturbations are attenuated over time; this is achieved by coupling contrastive representation learning with Lipschitz regularization to preserve task-relevant geometry while controlling sensitivity, and can be integrated as an auxiliary loss in deep reinforcement learning pipelines. Experimental results on humanoid locomotion demonstrate that CMR significantly outperforms other algorithms under conditions of increased sensor noise.</div>
<div class="mono" style="margin-top:8px">在非结构化地形上，人形机器人的鲁棒运动因不可靠的感知和模型失配而面临挑战，依赖噪声感知数据的策略容易失稳。为此，本研究提出了收缩映射鲁棒性（CMR）框架，将高维、易受干扰的观测映射到一个收缩的潜在空间，使局部扰动随时间衰减；该方法通过结合对比表示学习和Lipschitz正则化，在控制敏感性的同时保持任务相关的几何结构，并可作为辅助损失集成到深度强化学习流程中。在人体运动实验中，CMR在噪声增强条件下显著优于其他运动算法。</div>
</details>
</div>
<div class="card">
<div class="title">OptiPMB: Enhancing 3D Multi-Object Tracking with Optimized Poisson Multi-Bernoulli Filtering</div>
<div class="meta-line">Authors: Guanhua Ding, Yuxuan Xia, Runwei Guan, Qinchen Wu, Tao Huang, Weiping Ding, Jinping Sun, Guoqiang Mao</div>
<div class="meta-line">First: 2025-03-17T09:24:26+00:00 · Latest: 2026-02-03T13:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.12968v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.12968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate 3D multi-object tracking (MOT) is crucial for autonomous driving, as it enables robust perception, navigation, and planning in complex environments. While deep learning-based solutions have demonstrated impressive 3D MOT performance, model-based approaches remain appealing for their simplicity, interpretability, and data efficiency. Conventional model-based trackers typically rely on random vector-based Bayesian filters within the tracking-by-detection (TBD) framework but face limitations due to heuristic data association and track management schemes. In contrast, random finite set (RFS)-based Bayesian filtering handles object birth, survival, and death in a theoretically sound manner, facilitating interpretability and parameter tuning. In this paper, we present OptiPMB, a novel RFS-based 3D MOT method that employs an optimized Poisson multi-Bernoulli (PMB) filter while incorporating several key innovative designs within the TBD framework. Specifically, we propose a measurement-driven hybrid adaptive birth model for improved track initialization, employ adaptive detection probability parameters to effectively maintain tracks for occluded objects, and optimize density pruning and track extraction modules to further enhance overall tracking performance. Extensive evaluations on nuScenes and KITTI datasets show that OptiPMB achieves superior tracking accuracy compared with state-of-the-art methods, thereby establishing a new benchmark for model-based 3D MOT and offering valuable insights for future research on RFS-based trackers in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiPMB：基于优化泊松多伯努利滤波的3D多目标跟踪增强方法</div>
<div class="mono" style="margin-top:8px">精确的3D多目标跟踪对自动驾驶至关重要，它能在复杂环境中实现鲁棒的感知、导航与路径规划。尽管基于深度学习的方案已展现出卓越的3D多目标跟踪性能，但基于模型的方法因其简洁性、可解释性和数据高效性仍具吸引力。传统基于模型的跟踪器通常在检测跟踪框架内依赖随机向量贝叶斯滤波器，但受限于启发式数据关联与轨迹管理机制。相比之下，基于随机有限集的贝叶斯滤波以理论完备的方式处理目标新生、存续与消亡，提升了可解释性与参数调优效率。本文提出OptiPMB——一种基于随机有限集的新型3D多目标跟踪方法，其在检测跟踪框架中采用优化泊松多伯努利滤波器，并融合多项关键创新设计：提出测量驱动的混合自适应新生模型以改进轨迹初始化，采用自适应检测概率参数有效维持被遮挡目标的轨迹，并优化密度剪枝与轨迹提取模块以全面提升跟踪性能。在nuScenes和KITTI数据集上的大量实验表明，OptiPMB相比前沿方法实现了更优的跟踪精度，为基于模型的3D多目标跟踪树立了新基准，并为自动驾驶领域基于随机有限集的跟踪器研究提供了重要参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to improve 3D multi-object tracking (MOT) for autonomous driving by addressing the limitations of conventional model-based trackers, which rely on heuristic data association. The proposed method, OptiPMB, introduces an optimized Poisson multi-Bernoulli filter within a tracking-by-detection framework, incorporating a measurement-driven hybrid adaptive birth model for track initialization, adaptive detection probability for handling occlusions, and optimized density pruning and track extraction. Experimental results on nuScenes and KITTI datasets demonstrate that OptiPMB achieves superior tracking accuracy compared to state-of-the-art methods, setting a new benchmark for model-based 3D MOT.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升自动驾驶中三维多目标跟踪的准确性，其中基于模型的方法相比深度学习方法具有可解释性和数据效率的优势。所提出的OptiPMB方法在检测跟踪框架内采用优化的泊松多伯努利滤波器，关键创新包括用于初始化的测量驱动混合自适应新生模型、处理遮挡的自适应检测概率参数，以及优化的密度剪枝和轨迹提取模块。在nuScenes和KITTI数据集上的广泛评估表明，OptiPMB实现了优于现有方法的跟踪精度，为基于模型的三维多目标跟踪设立了新基准。</div>
</details>
</div>
<div class="card">
<div class="title">Material-informed Gaussian Splatting for 3D World Reconstruction in a Digital Twin</div>
<div class="meta-line">Authors: Andy Huynh, João Malheiro Silva, Holger Caesar, Tong Duy Son</div>
<div class="meta-line">First: 2025-11-25T14:25:19+00:00 · Latest: 2026-02-03T12:40:10+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures. Accepted to IEEE Intelligent Vehicles Symposium (IV) 2026. Revised version (v3) presents camera-ready publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.20348v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.20348v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D reconstruction for Digital Twins often relies on LiDAR-based methods, which provide accurate geometry but lack the semantics and textures naturally captured by cameras. Traditional LiDAR-camera fusion approaches require complex calibration and still struggle with certain materials like glass, which are visible in images but poorly represented in point clouds. We propose a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts Gaussian representations to mesh surfaces with projected material labels, and assigns physics-based material properties for accurate sensor simulation in modern graphics engines and simulators. This approach combines photorealistic reconstruction with physics-based material assignment, providing sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware complexity and calibration requirements. We validate our camera-only method using an internal dataset from an instrumented test vehicle, leveraging LiDAR as ground truth for reflectivity validation alongside image similarity metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向数字孪生三维世界重建的材料感知高斯泼溅技术</div>
<div class="mono" style="margin-top:8px">数字孪生的三维重建通常依赖基于激光雷达的方法，这类方法能提供精确几何结构，但缺乏相机自然捕捉的语义与纹理信息。传统的激光雷达-相机融合方案需要复杂标定，且对玻璃等特殊材质处理困难——这类材质在图像中可见却在点云中表征不足。我们提出一种纯相机重建流程：通过多视角图像基于三维高斯泼溅技术重建场景，利用视觉模型提取语义化材料掩膜，将高斯表征转换为带投影材料标签的网格表面，并为现代图形引擎与仿真器分配基于物理的材料属性以实现精确传感器模拟。该方法融合了逼真重建与物理材料赋值，在保持与激光雷达-相机融合方案相当的传感器仿真保真度同时，消除了硬件复杂度与标定需求。我们使用装备传感器的测试车辆内部数据集验证了纯相机方案，以激光雷达作为反射率验证基准，并结合图像相似度指标进行评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of LiDAR-based 3D reconstruction for Digital Twins, which captures accurate geometry but lacks semantics and textures, and where traditional LiDAR-camera fusion struggles with materials like glass and requires complex calibration. The proposed method is a camera-only pipeline that reconstructs scenes using 3D Gaussian Splatting from multi-view images, extracts semantic material masks via vision models, converts the Gaussian representations to meshes with projected material labels, and assigns physics-based material properties for sensor simulation. Experimental validation on an internal dataset from a test vehicle, using LiDAR as ground truth for reflectivity, shows the approach achieves sensor simulation fidelity comparable to LiDAR-camera fusion while eliminating hardware and calibration complexity.</div>
<div class="mono" style="margin-top:8px">本研究针对数字孪生中激光雷达三维重建的局限性，该方法虽能获取精确几何但缺乏语义纹理，且难以处理玻璃等特殊材质。所提出的方法采用纯摄像头流程，通过多视角图像进行三维高斯泼溅重建，利用视觉模型提取语义材质掩码，将高斯表示转换为带有投影材质标签的网格表面，并为传感器仿真分配基于物理的材质属性。在配备仪器的测试车辆内部数据集上的实验验证表明，该方法在消除硬件复杂性和校准需求的同时，达到了与激光雷达-摄像头融合相当的传感器仿真保真度，并使用激光雷达作为反射率验证的基准。</div>
</details>
</div>
<div class="card">
<div class="title">HetroD: A High-Fidelity Drone Dataset and Benchmark for Autonomous Driving in Heterogeneous Traffic</div>
<div class="meta-line">Authors: Yu-Hsiang Chen, Wei-Jer Chang, Christian Kotulla, Thomas Keutgens, Steffen Runde, Tobias Moers, Christoph Klas, Wei Zhan, Masayoshi Tomizuka, Yi-Ting Chen</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-03T12:12:47+00:00 · Latest: 2026-02-03T12:12:47+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Robotics and Automation (ICRA) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03447v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03447v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hetroddata.github.io/HetroD/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present HetroD, a dataset and benchmark for developing autonomous driving systems in heterogeneous environments. HetroD targets the critical challenge of navi- gating real-world heterogeneous traffic dominated by vulner- able road users (VRUs), including pedestrians, cyclists, and motorcyclists that interact with vehicles. These mixed agent types exhibit complex behaviors such as hook turns, lane splitting, and informal right-of-way negotiation. Such behaviors pose significant challenges for autonomous vehicles but remain underrepresented in existing datasets focused on structured, lane-disciplined traffic. To bridge the gap, we collect a large- scale drone-based dataset to provide a holistic observation of traffic scenes with centimeter-accurate annotations, HD maps, and traffic signal states. We further develop a modular toolkit for extracting per-agent scenarios to support downstream task development. In total, the dataset comprises over 65.4k high- fidelity agent trajectories, 70% of which are from VRUs. HetroD supports modeling of VRU behaviors in dense, het- erogeneous traffic and provides standardized benchmarks for forecasting, planning, and simulation tasks. Evaluation results reveal that state-of-the-art prediction and planning models struggle with the challenges presented by our dataset: they fail to predict lateral VRU movements, cannot handle unstructured maneuvers, and exhibit limited performance in dense and multi-agent scenarios, highlighting the need for more robust approaches to heterogeneous traffic. See our project page for more examples: https://hetroddata.github.io/HetroD/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HetroD：面向异构交通自动驾驶的高保真无人机数据集与基准</div>
<div class="mono" style="margin-top:8px">本文提出HetroD数据集与基准，旨在支持异构环境下的自动驾驶系统开发。该数据集聚焦以弱势道路使用者（包括行人、自行车与摩托车骑手）为主导的真实世界异构交通场景，这些混合交通参与者常表现出钩形转弯、车道分割、非正式路权协商等复杂行为，对自动驾驶系统构成严峻挑战，却在现有以结构化车道纪律交通为主的数据集中严重缺失。为填补这一空白，我们通过大规模无人机采集构建了具备厘米级精度标注、高清地图与交通信号状态的全景交通场景数据集，并开发模块化工具包以提取单智能体场景支持下游任务开发。数据集共包含超过6.54万条高保真轨迹，其中70%来自弱势道路使用者。HetroD支持密集异构交通中弱势道路使用者行为建模，并为预测、规划与仿真任务提供标准化基准。评估结果表明，当前最先进的预测与规划模型在本数据集提出的挑战面前表现不佳：无法预测弱势道路使用者的横向移动、难以处理非结构化机动行为，且在密集多智能体场景中性能有限，凸显了异构交通场景需要更鲁棒的解决方案。更多示例请访问项目页面：https://hetroddata.github.io/HetroD/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of datasets representing complex, heterogeneous traffic dominated by vulnerable road users (VRUs) like pedestrians and cyclists, whose behaviors such as hook turns and lane splitting challenge autonomous vehicles. The authors introduce HetroD, a large-scale dataset collected via drones, providing holistic scene observations with centimeter-accurate annotations, HD maps, and traffic signal states, along with a toolkit for extracting per-agent scenarios. Experimental benchmarking shows that current state-of-the-art prediction and planning models fail to adequately predict lateral VRU movements, handle unstructured maneuvers, or perform well in dense, multi-agent scenarios, underscoring the dataset&#x27;s utility for developing more robust autonomous systems.</div>
<div class="mono" style="margin-top:8px">本研究针对现有自动驾驶数据集缺乏对以行人、骑行者等弱势道路使用者为主的复杂异构交通场景的覆盖问题，这些参与者的转弯、穿插等行为对自动驾驶系统构成挑战。作者提出了HetroD数据集，通过无人机采集大规模数据，提供包含厘米级精确标注、高精地图和交通信号状态的全局场景观测，并开发了用于提取个体场景的工具包。实验基准测试表明，当前最先进的预测与规划模型难以准确预测弱势道路使用者的横向运动、处理非结构化机动行为，且在密集多智能体场景中性能有限，凸显了该数据集对开发更鲁棒的异构交通处理方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">CRL-VLA: Continual Vision-Language-Action Learning</div>
<div class="meta-line">Authors: Qixin Zeng, Shuo Zhang, Hongyin Zhang, Renjie Wang, Han Zhao, Libang Zhao, Runze Li, Donglin Wang, Chao Huang</div>
<div class="meta-line">First: 2026-02-03T12:09:53+00:00 · Latest: 2026-02-03T12:09:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03445v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifelong learning is critical for embodied agents in open-world environments, where reinforcement learning fine-tuning has emerged as an important paradigm to enable Vision-Language-Action (VLA) models to master dexterous manipulation through environmental interaction. Thus, Continual Reinforcement Learning (CRL) is a promising pathway for deploying VLA models in lifelong robotic scenarios, yet balancing stability (retaining old skills) and plasticity (learning new ones) remains a formidable challenge for existing methods. We introduce CRL-VLA, a framework for continual post-training of VLA models with rigorous theoretical bounds. We derive a unified performance bound linking the stability-plasticity trade-off to goal-conditioned advantage magnitude, scaled by policy divergence. CRL-VLA resolves this dilemma via asymmetric regulation: constraining advantage magnitudes on prior tasks while enabling controlled growth on new tasks. This is realized through a simple but effective dual-critic architecture with novel Goal-Conditioned Value Formulation (GCVF), where a frozen critic anchors semantic consistency and a trainable estimator drives adaptation. Experiments on the LIBERO benchmark demonstrate that CRL-VLA effectively harmonizes these conflicting objectives, outperforming baselines in both anti-forgetting and forward adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRL-VLA：持续视觉-语言-动作学习</div>
<div class="mono" style="margin-top:8px">在开放世界环境中，终身学习对于具身智能体至关重要，其中强化学习微调已成为重要范式，使视觉-语言-动作模型能通过环境交互掌握灵巧操作。因此，持续强化学习是将VLA模型部署于终身机器人场景的可行路径，但平衡稳定性（保留旧技能）与可塑性（学习新技能）仍是现有方法的重大挑战。我们提出CRL-VLA框架，通过严格理论边界实现VLA模型的持续后训练。我们推导出统一性能边界，将稳定性-可塑性权衡与目标条件优势幅度相关联，并通过策略散度进行缩放。CRL-VLA通过非对称调节解决该困境：约束先前任务的优势幅度，同时允许新任务受控增长。这通过简单有效的双评论家架构实现，其中冻结评论家锚定语义一致性，可训练估计器驱动适应。在LIBERO基准测试中，CRL-VLA有效协调了这些冲突目标，在抗遗忘与前向适应方面均优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable Vision-Language-Action (VLA) models for lifelong robotic deployment, where balancing the retention of old skills (stability) and the acquisition of new ones (plasticity) is a key challenge, this work proposes CRL-VLA, a continual reinforcement learning framework with theoretical guarantees. The method introduces a dual-critic architecture with a novel Goal-Conditioned Value Formulation (GCVF), using a frozen critic to preserve semantic consistency on prior tasks and a trainable critic to drive adaptation on new tasks, thereby asymmetrically regulating advantage magnitudes to manage the stability-plasticity trade-off. Experimental results on the LIBERO benchmark show that CRL-VLA outperforms baseline methods in both mitigating catastrophic forgetting and facilitating forward adaptation.</div>
<div class="mono" style="margin-top:8px">该研究针对具身智能体终身学习中的挑战，即需要持续强化学习（CRL）来使视觉-语言-动作（VLA）模型适应新任务而不遗忘旧技能。提出的CRL-VLA框架推导了一个将稳定性-可塑性权衡与目标条件优势幅度及策略散度联系起来的理论界，并通过具有新颖目标条件价值公式（GCVF）的双评论家架构实现非对称调控，以约束旧任务同时允许新任务上的受控学习。在LIBERO基准上的实验结果表明，CRL-VLA在保持旧技能和适应新任务方面均优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Model-based Optimal Control for Rigid-Soft Underactuated Systems</div>
<div class="meta-line">Authors: Daniele Caradonna, Nikhil Nair, Anup Teejo Mathew, Daniel Feliu Talegón, Imran Afgan, Egidio Falotico, Cosimo Della Santina, Federico Renda</div>
<div class="meta-line">First: 2026-02-03T11:59:36+00:00 · Latest: 2026-02-03T11:59:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03435v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03435v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continuum soft robots are inherently underactuated and subject to intrinsic input constraints, making dynamic control particularly challenging, especially in hybrid rigid-soft robots. While most existing methods focus on quasi-static behaviors, dynamic tasks such as swing-up require accurate exploitation of continuum dynamics. This has led to studies on simple low-order template systems that often fail to capture the complexity of real continuum deformations. Model-based optimal control offers a systematic solution; however, its application to rigid-soft robots is often limited by the computational cost and inaccuracy of numerical differentiation for high-dimensional models. Building on recent advances in the Geometric Variable Strain model that enable analytical derivatives, this work investigates three optimal control strategies for underactuated soft systems-Direct Collocation, Differential Dynamic Programming, and Nonlinear Model Predictive Control-to perform dynamic swing-up tasks. To address stiff continuum dynamics and constrained actuation, implicit integration schemes and warm-start strategies are employed to improve numerical robustness and computational efficiency. The methods are evaluated in simulation on three Rigid-Soft and high-order soft benchmark systems-the Soft Cart-Pole, the Soft Pendubot, and the Soft Furuta Pendulum- highlighting their performance and computational trade-offs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模型的刚柔欠驱动系统最优控制</div>
<div class="mono" style="margin-top:8px">连续体软体机器人本质上是欠驱动的，并受到固有输入约束，这使得动态控制尤为困难，在刚柔混合机器人中尤其如此。现有方法多关注准静态行为，而动态任务（如摆起）需要精确利用连续体动力学特性。这导致了对简单低阶模板系统的研究，但这些系统往往难以捕捉真实连续体变形的复杂性。基于模型的最优控制提供了系统性解决方案，但其在刚柔机器人中的应用常受限于高维模型数值微分的计算成本与精度不足。基于近期可实现解析导数的几何变应变模型进展，本研究针对欠驱动软体系统探究了三种最优控制策略——直接配点法、微分动态规划和非线性模型预测控制——以执行动态摆起任务。为应对刚性连续体动力学和受限驱动问题，采用隐式积分方案与热启动策略以提升数值鲁棒性和计算效率。通过软体倒立摆、软体双摆和软体古村摆三个刚柔及高阶软体基准系统的仿真实验，评估了各方法的性能与计算权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of dynamic control for underactuated continuum soft robots, particularly in hybrid rigid-soft systems where existing quasi-static methods and simplified template models are insufficient for tasks like swing-up. The study leverages a Geometric Variable Strain model that provides analytical derivatives to efficiently implement three model-based optimal control strategies—Direct Collocation, Differential Dynamic Programming, and Nonlinear Model Predictive Control—enhanced with implicit integration and warm-start strategies for numerical robustness under stiff dynamics and actuation constraints. Simulation experiments on three benchmark systems (Soft Cart-Pole, Soft Pendubot, and Soft Furuta Pendulum) demonstrate the successful execution of dynamic swing-up tasks while revealing performance and computational trade-offs among the methods.</div>
<div class="mono" style="margin-top:8px">本研究针对欠驱动连续体软体机器人，特别是刚柔混合系统的动态控制挑战，现有准静态方法和简化模板模型无法胜任摆动等任务。该方法基于几何可变应变模型实现解析导数，研究了三种基于模型的最优控制策略——直接配点法、微分动态规划和非线性模型预测控制，并采用隐式积分和热启动策略处理刚性动力学和驱动约束。在三个基准系统（软体车杆、软体双摆、软体倒立摆）上的仿真实验验证了这些策略在执行动态摆动任务时的性能和计算权衡。</div>
</details>
</div>
<div class="card">
<div class="title">ProAct: A Benchmark and Multimodal Framework for Structure-Aware Proactive Response</div>
<div class="meta-line">Authors: Xiaomeng Zhu, Fengming Zhu, Weijie Zhou, Ye Tian, Zhenlin Hu, Yufei Huang, Yuchun Guo, Xinyu Wu, Zhengyou Zhang, Fangzhen Lin, Xuantang Xiong</div>
<div class="meta-line">First: 2026-02-03T11:52:19+00:00 · Latest: 2026-02-03T11:52:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03430v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03430v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While passive agents merely follow instructions, proactive agents align with higher-level objectives, such as assistance and safety by continuously monitoring the environment to determine when and how to act. However, developing proactive agents is hindered by the lack of specialized resources. To address this, we introduce ProAct-75, a benchmark designed to train and evaluate proactive agents across diverse domains, including assistance, maintenance, and safety monitoring. Spanning 75 tasks, our dataset features 91,581 step-level annotations enriched with explicit task graphs. These graphs encode step dependencies and parallel execution possibilities, providing the structural grounding necessary for complex decision-making. Building on this benchmark, we propose ProAct-Helper, a reference baseline powered by a Multimodal Large Language Model (MLLM) that grounds decision-making in state detection, and leveraging task graphs to enable entropy-driven heuristic search for action selection, allowing agents to execute parallel threads independently rather than mirroring the human&#x27;s next step. Extensive experiments demonstrate that ProAct-Helper outperforms strong closed-source models, improving trigger detection mF1 by 6.21%, saving 0.25 more steps in online one-step decision, and increasing the rate of parallel actions by 15.58%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProAct：面向结构感知主动响应的基准与多模态框架</div>
<div class="mono" style="margin-top:8px">被动型智能体仅遵循指令，而主动型智能体通过持续监测环境以判断行动时机与方式，实现辅助、安全等高层目标。然而，专业资源的缺乏阻碍了主动型智能体的发展。为此，我们推出ProAct-75基准，用于跨辅助、维护、安全监控等多领域训练与评估主动型智能体。该数据集涵盖75项任务，包含91,581个步骤级标注，并辅以显式任务图谱。这些图谱编码了步骤依赖关系与并行执行可能性，为复杂决策提供必要的结构化基础。基于此基准，我们提出ProAct-Helper参考基线——采用多模态大语言模型（MLLM），将决策建立在状态检测基础上，并利用任务图谱实现基于熵驱动的启发式搜索进行动作选择，使智能体能独立执行并行线程而非简单模仿人类下一步动作。大量实验表明，ProAct-Helper优于强闭源模型：触发检测mF1提升6.21%，在线单步决策中多节省0.25个步骤，并行动作率提高15.58%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the scarcity of specialized resources for developing proactive agents that align with high-level objectives like assistance and safety by continuously monitoring environments. It introduces ProAct-75, a benchmark with 75 tasks and 91,581 step-level annotations enriched with explicit task graphs to encode dependencies and parallel execution possibilities, providing structural grounding for decision-making. The proposed ProAct-Helper baseline uses a Multimodal Large Language Model to ground decisions in state detection and leverages task graphs for entropy-driven heuristic search, enabling independent parallel action execution. Experiments show that ProAct-Helper outperforms strong closed-source models, improving trigger detection mF1 by 6.21%, saving 0.25 more steps in online decision-making, and increasing the parallel action rate by 15.58%.</div>
<div class="mono" style="margin-top:8px">主动智能体通过持续监控环境来决定何时及如何行动，以实现协助、安全等高层目标，但其发展受限于专业资源的缺乏。为此，研究者提出了ProAct-75基准，包含75个任务和91,581个步骤级标注，并辅以明确的任务图来编码步骤依赖和并行执行可能性，为复杂决策提供结构化基础。基于此基准，他们设计了ProAct-Helper基线框架，采用多模态大语言模型，将决策建立在状态检测之上，并利用任务图进行熵驱动启发式搜索，使智能体能够独立执行并行线程而非简单模仿人类下一步。大量实验表明，ProAct-Helper优于强闭源模型，将触发检测mF1提升6.21%，在线单步决策中多节省0.25个步骤，并将并行动作率提高15.58%。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-based Initialization of Trajectory Optimization for Path-following Problems of Redundant Manipulators</div>
<div class="meta-line">Authors: Minsung Yoon, Mincheul Kang, Daehyung Park, Sung-Eui Yoon</div>
<div class="meta-line">Venue: ICRA 2023</div>
<div class="meta-line">First: 2026-02-03T11:44:20+00:00 · Latest: 2026-02-03T11:44:20+00:00</div>
<div class="meta-line">Comments: Accepted to ICRA 2023. &lt;a href=&quot;https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA23_RLITG/&quot; rel=&quot;external noopener nofollow&quot; class=&quot;link-external link-https&quot;&gt;Project Page&lt;/a&gt;</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03418v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03418v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sgvr.kaist.ac.kr/~msyoon/papers/ICRA23_RLITG/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Trajectory optimization (TO) is an efficient tool to generate a redundant manipulator&#x27;s joint trajectory following a 6-dimensional Cartesian path. The optimization performance largely depends on the quality of initial trajectories. However, the selection of a high-quality initial trajectory is non-trivial and requires a considerable time budget due to the extremely large space of the solution trajectories and the lack of prior knowledge about task constraints in configuration space. To alleviate the issue, we present a learning-based initial trajectory generation method that generates high-quality initial trajectories in a short time budget by adopting example-guided reinforcement learning. In addition, we suggest a null-space projected imitation reward to consider null-space constraints by efficiently learning kinematically feasible motion captured in expert demonstrations. Our statistical evaluation in simulation shows the improved optimality, efficiency, and applicability of TO when we plug in our method&#x27;s output, compared with three other baselines. We also show the performance improvement and feasibility via real-world experiments with a seven-degree-of-freedom manipulator.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习的冗余机械臂路径跟随问题轨迹优化初始化方法</div>
<div class="mono" style="margin-top:8px">轨迹优化是生成冗余机械臂沿六维笛卡尔路径运动关节轨迹的有效工具，其优化性能很大程度上取决于初始轨迹的质量。然而，由于解轨迹空间极大且在构型空间中缺乏任务约束的先验知识，选择高质量初始轨迹非常困难且耗时。为缓解此问题，我们提出一种基于学习的初始轨迹生成方法，通过采用示例引导的强化学习在短时间内生成高质量初始轨迹。此外，我们提出零空间投影模仿奖励机制，通过高效学习专家演示中捕获的运动学可行运动来考虑零空间约束。仿真统计评估表明，与三种基线方法相比，采用本方法输出的轨迹优化在最优性、效率和适用性上均有提升。我们通过七自由度机械臂的真实实验进一步验证了性能改进与可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Trajectory optimization for redundant manipulators is sensitive to initial trajectory quality, which is difficult to select efficiently due to the vast solution space and unknown task constraints. To address this, the authors propose a learning-based method that uses example-guided reinforcement learning with a novel null-space projected imitation reward to quickly generate high-quality initial trajectories by learning from expert demonstrations. Experimental results in simulation demonstrate superior optimality, efficiency, and applicability compared to baselines, and real-world tests with a 7-DOF manipulator confirm performance improvements and feasibility.</div>
<div class="mono" style="margin-top:8px">本研究针对冗余机械臂轨迹优化中高质量初始轨迹生成的难题，该问题因解空间巨大且缺乏先验知识而影响路径跟随效率。方法采用示例引导的强化学习，结合零空间投影模仿奖励，通过利用专家示范快速生成满足运动学约束的可行初始轨迹。仿真和7自由度机械臂的真实实验结果表明，与基线方法相比，该方法在最优性、效率和适用性方面均有提升。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
