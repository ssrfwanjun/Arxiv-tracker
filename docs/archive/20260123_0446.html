<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 04:46</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0446</div>
    <div class="row"><div class="card">
<div class="title">Towards Understanding Best Practices for Quantization of Vision-Language Models</div>
<div class="meta-line">Authors: Gautom Das, Vincent La, Ethan Lau, Abhinav Shrivastava, Matthew Gwilliam</div>
<div class="meta-line">First: 2026-01-21T18:59:51+00:00 · Latest: 2026-01-21T18:59:51+00:00</div>
<div class="meta-line">Comments: 15 pages, 12 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15287v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15287v1">PDF</a> · <a href="https://github.com/gautomdas/mmq">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型量化最佳实践探究</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在多种任务中展现出卓越性能，但顶尖系统需要配备大容量内存的高速GPU。为降低系统内存占用与延迟，实践者通常将学习参数量化为半精度。当前研究日益关注如何在更激进的比特位宽下保持模型性能，部分工作已尝试将这些策略应用于视觉Transformer等其他模型。本研究系统探讨了包括前沿GPTQ与AWQ在内的多种量化方法如何有效应用于由视觉模型、语言模型及其连接器构成的多模态流程。我们分析了比特位宽、量化方法及量化应用环节对图像描述、检索与问答任务性能的影响。实验表明：尽管参数规模差异显著，ViT与LLM对模型性能具有相当的重要性；LLM的低比特量化能以更低的权重比特数（bpw）实现高精度。这些发现为多模态大语言模型的高效部署提供了实用见解，并凸显了探索多模态模型组件敏感度的重要价值。代码已开源：https://github.com/gautomdas/mmq。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to reduce the memory and latency of large vision-language models (VLMs) for efficient deployment, as current systems demand high-performance GPUs with substantial memory. The study systematically applies various quantization methods, including GPTQ and AWQ, to different components of multimodal pipelines—vision models, language models, and connectors—to assess their impact on tasks like captioning, retrieval, and question answering. Key findings show that vision transformers (ViT) and large language models (LLM) are similarly critical for performance despite parameter size differences, and that lower-bit quantization of LLMs can achieve high accuracy with reduced bits per weight, offering practical guidance for optimizing VLM efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型视觉语言模型（VLM）的高内存和延迟需求，通过探索有效的量化策略以实现高效部署。该研究系统地将多种量化方法（包括GPTQ和AWQ）应用于多模态管道的不同组件——视觉模型、语言模型和连接器——并评估它们对图像描述、检索和问答等任务的影响。关键发现表明，尽管参数规模不同，视觉变换器（ViT）和大型语言模型（LLM）对性能同样重要，且对LLM进行低位量化可以在显著降低每权重比特数的同时保持高精度。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Refinement Improves Compositional Image Generation</div>
<div class="meta-line">Authors: Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak</div>
<div class="meta-line">First: 2026-01-21T18:59:40+00:00 · Latest: 2026-01-21T18:59:40+00:00</div>
<div class="meta-line">Comments: Project webpage: https://iterative-img-gen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15286v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iterative-img-gen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代优化提升组合式图像生成质量</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型已取得显著进展，但在处理需要同时满足多对象、多关系、多属性的复杂提示时仍面临挑战。现有的推理时策略（如结合验证器的并行采样或单纯增加去噪步数）虽能提升提示对齐度，但在需要满足多重约束的复杂组合场景中仍显不足。受大语言模型中思维链推理成功的启发，我们提出一种迭代式测试时策略：T2I模型在视觉语言模型作为循环评判者的反馈指导下，通过多步骤渐进优化生成结果。该方法无需外部工具或先验知识，可灵活适配各类图像生成器与视觉语言模型。实验表明，该方法在多个基准测试中持续提升图像生成效果：在ConceptMix（k=7）的全正确率提升16.9%，在T2I-CompBench（3D空间类别）提升13.8%，在Visual Jenga场景解构任务中较计算量匹配的并行采样提升12.5%。除量化指标外，迭代优化通过将复杂提示分解为序列化修正，生成结果更具忠实度——人工评估者对本方法的偏好率达58.7%，显著高于并行基线（41.3%）。这些发现共同表明，迭代式自校正可作为组合式图像生成的普适性优化原则。完整结果与可视化案例详见项目网页。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image models often fail to accurately generate images from complex prompts involving multiple objects and relations. To address this, the authors propose an iterative refinement strategy where a vision-language model provides feedback to guide the image generator across multiple denoising steps, decomposing the prompt into sequential corrections. Experimental results show significant improvements, including a 16.9% increase in all-correct rate on the ConceptMix benchmark and a 58.7% human preference rate over a parallel sampling baseline, demonstrating the effectiveness of iterative self-correction for compositional generation.</div>
<div class="mono" style="margin-top:8px">文本到图像模型在处理涉及多个对象和关系的复杂提示时常常表现不佳。为此，研究者提出了一种迭代优化策略，利用视觉语言模型提供反馈，在多步去噪过程中引导图像生成器，将复杂提示分解为顺序修正。实验结果表明该方法带来了显著提升，如在ConceptMix基准测试上的全正确率提高了16.9%，并在人类评估中以58.7%的偏好率优于并行采样基线，证明了迭代自校正对于组合式图像生成的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu</div>
<div class="meta-line">First: 2026-01-21T17:56:59+00:00 · Latest: 2026-01-21T17:56:59+00:00</div>
<div class="meta-line">Comments: Website: https://progresslm.github.io/ProgressLM/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15224v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://progresslm.github.io/ProgressLM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PROGRESSLM：迈向视觉语言模型中的进程推理</div>
<div class="mono" style="margin-top:8px">任务进程估计需要对长时程动态进行推理，而非识别静态视觉内容。尽管现代视觉语言模型擅长描述可见内容，但其能否通过局部观察推断任务进展程度尚不明确。为此，我们提出Progress-Bench基准，用于系统评估VLM的进程推理能力。除基准测试外，我们进一步通过免训练提示和基于ProgressLM-45K数据集的训练方法，探索了受人类启发的两阶段进程推理范式。对14个VLM的实验表明，多数模型尚未具备任务进程估计能力，存在对演示模态和视角变化的敏感性，以及对不可回答情况的处理缺陷。强制结构化进程推理的免训练提示仅能带来有限且模型依赖的改进，而基于训练的ProgressLM-3B即使在小规模模型上也能实现稳定提升——尽管其训练任务集与评估任务完全不相交。进一步分析揭示了典型错误模式，并阐明了进程推理成功或失败的条件与原因。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of current Vision-Language Models (VLMs) in reasoning about long-horizon task progress from partial observations, moving beyond static scene description. The authors introduce Progress-Bench, a systematic benchmark for evaluating progress reasoning, and explore a two-stage reasoning paradigm through both training-free prompting and a training-based approach using a curated dataset called ProgressLM-45K. Experimental evaluation of 14 VLMs reveals that most models struggle with progress estimation, showing sensitivity to demonstration modality and viewpoint changes, and poor performance on unanswerable cases. While structured prompting yields limited, model-dependent improvements, the training-based ProgressLM-3B model achieves consistent gains even at a small scale and when trained on tasks disjoint from evaluation, with further analysis identifying characteristic error patterns.</div>
<div class="mono" style="margin-top:8px">本研究针对当前视觉语言模型在从部分观察中推理任务进度方面的局限性，该任务需要理解长期动态而非静态视觉内容。作者提出了Progress-Bench基准来评估进度推理能力，并探索了无需训练的提示方法和基于45K数据集的训练方法ProgressLM。对14个视觉语言模型的实验评估表明，大多数模型在进度估计上表现不佳，对演示模态和视角变化敏感，且难以处理不可回答的情况。虽然提示方法带来的改进有限，但经过训练的ProgressLM-3B模型即使在训练任务与评估任务完全不同的情况下仍能取得一致性的提升，进一步分析揭示了进度推理中的典型错误模式。</div>
</details>
</div>
<div class="card">
<div class="title">CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation</div>
<div class="meta-line">Authors: V. Kovalev, A. Kuvshinov, A. Buzovkin, D. Pokidov, D. Timonin</div>
<div class="meta-line">First: 2025-12-23T13:44:41+00:00 · Latest: 2026-01-21T16:42:28+00:00</div>
<div class="meta-line">Comments: 37 pages, 42 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20362v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20362v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping. We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free and model-agnostic framework for multimodal image generation. CRAFT transforms a user prompt into a set of explicit, dependency-structured visual constraints, verifies generated images using a vision-language model, and performs targeted prompt updates only when specific constraints are violated. This iterative process includes an explicit stopping criterion, resulting in an interpretable and controllable inference-time refinement loop. Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRAFT：面向多模态文本到图像生成的持续推理与智能反馈调优</div>
<div class="mono" style="margin-top:8px">近期研究表明，推理时的反思与修正能提升文本到图像生成质量且无需重新训练。然而，现有方法多依赖隐式整体评价或无约束提示词改写，导致其行为难以解释、控制或可靠终止。相比之下，大语言模型已从基于验证、定向修正和提前终止的显式结构化“思考”中获益。本文提出CRAFT（持续推理与智能反馈调优），这是一个免训练且模型无关的多模态图像生成框架。CRAFT将用户提示转化为显式的依赖结构化视觉约束集合，通过视觉语言模型验证生成图像，仅在特定约束被违反时执行定向提示更新。该迭代过程包含显式终止准则，形成可解释、可控的推理时优化循环。在多种模型架构和挑战性基准测试中，CRAFT持续提升组合准确性、文本渲染能力和偏好评估表现，对轻量级生成器的提升尤为显著。重要的是，这些改进仅带来可忽略的推理时间开销，使小型或经济型模型能接近昂贵系统的生成质量。我们的结果表明，显式结构化、约束驱动的推理时思考是提升多模态生成模型可靠性的关键要素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of interpretability and control in existing inference-time refinement methods for text-to-image generation, this work introduces CRAFT, a training-free framework that structures the reasoning process. The method transforms a user prompt into explicit, dependency-structured visual constraints, iteratively verifies generated images with a vision-language model, and performs targeted prompt updates only when specific constraints are violated, incorporating an explicit stopping criterion. Experiments across multiple model families and benchmarks show that CRAFT consistently improves compositional accuracy, text rendering, and preference scores, especially for lightweight generators, with minimal inference-time overhead, enabling smaller models to approach the quality of larger systems.</div>
<div class="mono" style="margin-top:8px">针对现有文本到图像生成推理时优化方法缺乏可解释性和可控性的问题，本研究提出了CRAFT这一免训练框架，将推理过程结构化。该方法将用户提示转换为显式的、依赖关系结构的视觉约束，使用视觉语言模型验证生成图像，仅在特定约束被违反时进行针对性提示更新，并包含明确的停止准则。在多个模型系列和基准测试上的实验表明，CRAFT能持续提升组合准确性、文本渲染效果和偏好评分，对轻量级生成器的提升尤为显著，同时仅带来可忽略的推理时间开销。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free and Interpretable Hateful Video Detection via Multi-stage Adversarial Reasoning</div>
<div class="meta-line">Authors: Shuonan Yang, Yuchen Zhang, Zeyu Fu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-21T15:52:26+00:00 · Latest: 2026-01-21T15:52:26+00:00</div>
<div class="meta-line">Comments: Accepted at ICASSP 2026. \c{opyright} 2026 IEEE. This is the author accepted manuscript. The final published version will be available via IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15115v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15115v1">PDF</a> · <a href="https://github.com/Multimodal-Intelligence-Lab-MIL/MARS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hateful videos pose serious risks by amplifying discrimination, inciting violence, and undermining online safety. Existing training-based hateful video detection methods are constrained by limited training data and lack of interpretability, while directly prompting large vision-language models often struggle to deliver reliable hate detection. To address these challenges, this paper introduces MARS, a training-free Multi-stage Adversarial ReaSoning framework that enables reliable and interpretable hateful content detection. MARS begins with the objective description of video content, establishing a neutral foundation for subsequent analysis. Building on this, it develops evidence-based reasoning that supports potential hateful interpretations, while in parallel incorporating counter-evidence reasoning to capture plausible non-hateful perspectives. Finally, these perspectives are synthesized into a conclusive and explainable decision. Extensive evaluation on two real-world datasets shows that MARS achieves up to 10% improvement under certain backbones and settings compared to other training-free approaches and outperforms state-of-the-art training-based methods on one dataset. In addition, MARS produces human-understandable justifications, thereby supporting compliance oversight and enhancing the transparency of content moderation workflows. The code is available at https://github.com/Multimodal-Intelligence-Lab-MIL/MARS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多阶段对抗推理的无训练可解释仇恨视频检测</div>
<div class="mono" style="margin-top:8px">仇恨视频通过放大歧视、煽动暴力和破坏在线安全构成严重风险。现有基于训练的仇恨视频检测方法受限于有限的训练数据和可解释性不足，而直接提示大型视觉语言模型往往难以实现可靠的仇恨检测。为应对这些挑战，本文提出MARS——一种无需训练的多阶段对抗推理框架，能够实现可靠且可解释的仇恨内容检测。MARS首先对视频内容进行客观描述，为后续分析建立中立基础。在此基础上，构建支持潜在仇恨解读的证据推理，同时并行整合反证据推理以捕捉合理的非仇恨视角。最后，将这些视角综合为可解释的结论性决策。在两个真实数据集上的广泛评估表明，在某些骨干网络和设置下，MARS相比其他无训练方法提升达10%，并在一个数据集上优于最先进的基于训练的方法。此外，MARS生成人类可理解的判定依据，从而支持合规监管并提升内容审核工作流程的透明度。代码发布于https://github.com/Multimodal-Intelligence-Lab-MIL/MARS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hateful videos amplify discrimination and incite violence, but existing detection methods are limited by scarce training data and lack interpretability, while direct prompting of large vision-language models proves unreliable. To address this, the authors propose MARS, a training-free Multi-stage Adversarial Reasoning framework that first generates a neutral objective description of video content, then conducts parallel reasoning to develop both evidence supporting hateful interpretations and counter-evidence for non-hateful perspectives, finally synthesizing these into an explainable decision. Experimental results on two real-world datasets show MARS achieves up to a 10% improvement over other training-free methods under certain backbones and settings, outperforms state-of-the-art training-based methods on one dataset, and generates human-understandable justifications for enhanced transparency in content moderation.</div>
<div class="mono" style="margin-top:8px">仇恨视频会加剧歧视并煽动暴力，但现有检测方法因训练数据稀缺和可解释性差而受限，直接提示大型视觉语言模型也往往不可靠。为此，本文提出MARS，一种无需训练的多阶段对抗推理框架：首先生成视频内容的中性描述，随后基于证据并行构建支持仇恨与非仇恨观点的推理链，最后综合得出可解释的判定。在两个真实数据集上的实验表明，MARS在某些设置下比其他免训练方法性能提升最高达10%，并在一个数据集上超越了最先进的基于训练的方法，同时能生成人类可理解的判定依据，提升了内容审核流程的透明度。</div>
</details>
</div>
<div class="card">
<div class="title">Unified Multi-Dataset Training for TBPS</div>
<div class="meta-line">Authors: Nilanjana Chatterjee, Sidharatha Garg, A V Subramanyam, Brejesh Lall</div>
<div class="meta-line">First: 2026-01-21T13:26:28+00:00 · Latest: 2026-01-21T13:26:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14978v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-Based Person Search (TBPS) has seen significant progress with vision-language models (VLMs), yet it remains constrained by limited training data and the fact that VLMs are not inherently pre-trained for pedestrian-centric recognition. Existing TBPS methods therefore rely on dataset-centric fine-tuning to handle distribution shift, resulting in multiple independently trained models for different datasets. While synthetic data can increase the scale needed to fine-tune VLMs, it does not eliminate dataset-specific adaptation. This motivates a fundamental question: can we train a single unified TBPS model across multiple datasets? We show that naive joint training over all datasets remains sub-optimal because current training paradigms do not scale to a large number of unique person identities and are vulnerable to noisy image-text pairs. To address these challenges, we propose Scale-TBPS with two contributions: (i) a noise-aware unified dataset curation strategy that cohesively merges diverse TBPS datasets; and (ii) a scalable discriminative identity learning framework that remains effective under a large number of unique identities. Extensive experiments on CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926 demonstrate that a single Scale-TBPS model outperforms dataset-centric optimized models and naive joint training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向TBPS的统一多数据集训练</div>
<div class="mono" style="margin-top:8px">基于文本的行人检索（TBPS）在视觉-语言模型（VLMs）的推动下取得了显著进展，但仍受限于训练数据不足以及VLMs本身并非针对行人中心识别进行预训练。现有TBPS方法依赖以数据集为中心的微调来处理分布偏移，导致针对不同数据集需训练多个独立模型。虽然合成数据可扩大微调VLMs所需的规模，但无法消除数据集特定的适应性需求。这引出一个根本性问题：能否跨多个数据集训练一个统一的TBPS模型？我们发现，对所有数据集进行简单联合训练仍非最优解，因为当前训练范式难以扩展到大量独立行人身份，且易受噪声图文对影响。为解决这些挑战，我们提出Scale-TBPS方法，包含两项贡献：（i）噪声感知的统一数据集构建策略，能有机整合多样化的TBPS数据集；（ii）可扩展的判别性身份学习框架，在大量独立身份下仍保持高效。在CUHK-PEDES、ICFG-PEDES、RSTPReid、IIITD-20K和UFine6926数据集上的大量实验表明，单一Scale-TBPS模型性能优于以数据集为中心的优化模型及简单联合训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of Text-Based Person Search (TBPS), where vision-language models (VLMs) are constrained by limited pedestrian-specific training data and require dataset-specific fine-tuning, leading to multiple independent models. To enable a single unified model across datasets, the method introduces Scale-TBPS, which includes a noise-aware unified dataset curation strategy to merge diverse TBPS datasets cohesively and a scalable discriminative identity learning framework that handles a large number of unique identities effectively. Experimental results on five datasets (CUHK-PEDES, ICFG-PEDES, RSTPReid, IIITD-20K, and UFine6926) show that the single Scale-TBPS model outperforms both dataset-centric optimized models and naive joint training approaches.</div>
<div class="mono" style="margin-top:8px">本研究针对基于文本的行人检索（TBPS）的局限性展开，该领域依赖的视觉语言模型并非针对行人识别预训练，且需要针对不同数据集进行微调，导致多个独立模型并存。为实现跨数据集的统一模型，方法提出了Scale-TBPS，包括一个噪声感知的统一数据集整理策略以融合多样数据集，以及一个可扩展的判别性身份学习框架以有效处理大量独特身份。在CUHK-PEDES、ICFG-PEDES等五个数据集上的实验结果表明，Scale-TBPS的性能优于针对单个数据集优化的模型和简单的联合训练方法。</div>
</details>
</div>
<div class="card">
<div class="title">GAIA: A Global, Multi-modal, Multi-scale Vision-Language Dataset for Remote Sensing Image Analysis</div>
<div class="meta-line">Authors: Angelos Zavras, Dimitrios Michail, Xiao Xiang Zhu, Begüm Demir, Ioannis Papoutsis</div>
<div class="meta-line">First: 2025-02-13T18:52:14+00:00 · Latest: 2026-01-21T12:51:46+00:00</div>
<div class="meta-line">Comments: 26 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.09598v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.09598v2">PDF</a> · <a href="https://github.com/Orion-AI-Lab/GAIA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing Vision-Language Models (VLMs) are predominantly trained on web-scraped, noisy image-text data, exhibiting limited exposure to the specialized domain of RS. This deficiency results in poor performance on RS-specific tasks, as commonly used datasets often lack detailed, scientifically accurate textual descriptions and instead emphasize solely on attributes like date and location. To bridge this critical gap, we introduce GAIA, a novel dataset designed for multi-scale, multi-sensor, and multi-modal RS image analysis. GAIA comprises of 201,005 meticulously curated RS image-text pairs, representing a diverse range of RS modalities associated to different spatial resolutions. Unlike existing vision-language datasets in RS, GAIA specifically focuses on capturing a diverse range of RS applications, providing unique information about environmental changes, natural disasters, and various other dynamic phenomena. The dataset provides a spatially and temporally balanced distribution, spanning across the globe, covering the last 25 years with a balanced temporal distribution of observations. GAIA&#x27;s construction involved a two-stage process: (1) targeted web-scraping of images and accompanying text from reputable RS-related sources, and (2) generation of five high-quality, scientifically grounded synthetic captions for each image using carefully crafted prompts that leverage the advanced vision-language capabilities of GPT-4o. Our extensive experiments, including fine-tuning of CLIP and BLIP2 models, demonstrate that GAIA significantly improves performance on RS image classification, cross-modal retrieval and image captioning tasks. We make our dataset, automated processing framework and fine-tuned model weights publicly available on our project&#x27;s GitHub repository: https://github.com/Orion-AI-Lab/GAIA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GAIA：面向遥感图像分析的全球多模态多尺度视觉语言数据集</div>
<div class="mono" style="margin-top:8px">现有视觉语言模型主要基于网络爬取的噪声图像-文本数据训练，对遥感专业领域接触有限，导致在遥感特定任务上表现不佳。常用数据集常缺乏科学准确的详细文本描述，仅强调日期、位置等属性。为填补这一空白，我们提出GAIA——一个专为多尺度、多传感器、多模态遥感图像分析设计的新数据集。GAIA包含201,005个精心筛选的遥感图像-文本对，涵盖不同空间分辨率的多种遥感模态。与现有遥感视觉语言数据集不同，GAIA聚焦于捕捉多样化的遥感应用场景，提供环境变化、自然灾害等动态现象的独特信息。数据集具备时空平衡分布，覆盖全球范围及过去25年，观测时间分布均衡。GAIA构建采用两阶段流程：(1)从权威遥感来源定向爬取图像及配套文本；(2)通过精心设计的提示词调用GPT-4o的先进视觉语言能力，为每幅图像生成五条高质量科学合成描述。基于CLIP和BLIP2模型的微调实验表明，GAIA显著提升了遥感图像分类、跨模态检索与图像描述任务的性能。数据集、自动化处理框架及微调模型权重已公开于项目GitHub仓库：https://github.com/Orion-AI-Lab/GAIA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of existing Vision-Language Models (VLMs) in the remote sensing (RS) domain, which are trained on noisy web data and lack detailed, scientifically accurate textual descriptions. To bridge this gap, the authors introduce GAIA, a novel multi-scale, multi-sensor, and multi-modal RS dataset constructed through a two-stage process: targeted web-scraping from reputable RS sources followed by the generation of five high-quality synthetic captions per image using GPT-4o. Experimental results from fine-tuning CLIP and BLIP2 models show that GAIA significantly enhances performance on RS-specific tasks, including image classification, cross-modal retrieval, and image captioning.</div>
<div class="mono" style="margin-top:8px">针对通用视觉语言模型在遥感任务上因网络数据噪声大和领域特定描述不足而性能有限的问题，本研究提出了GAIA数据集，这是一个包含201,005个精心策划的遥感图像-文本对的多尺度、多传感器、多模态数据集。该数据集通过从权威来源进行针对性网络爬取，并利用GPT-4o结合科学设计的提示词为每张图像生成五条高质量合成描述来构建，确保了全球空间覆盖和25年的时间跨度。对CLIP和BLIP2模型进行微调的实验表明，GAIA显著提升了遥感图像分类、跨模态检索和图像描述任务的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis</div>
<div class="meta-line">Authors: Keita Takeda, Tomoya Sakai</div>
<div class="meta-line">Venue: ISBI short</div>
<div class="meta-line">First: 2026-01-21T08:53:40+00:00 · Latest: 2026-01-21T08:53:40+00:00</div>
<div class="meta-line">Comments: A short version paper of this research has been accepted for The IEEE International Symposium on Biomedical Imaging (ISBI) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14774v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14774v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates the feature representations produced by publicly available open source medical vision-language models (VLMs). While medical VLMs are expected to capture diagnostically relevant features, their learned representations remain underexplored, and standard evaluations like classification accuracy do not fully reveal if they acquire truly discriminative, lesion-specific features. Understanding these representations is crucial for revealing medical image structures and improving downstream tasks in medical image analysis. This study aims to investigate the feature distributions learned by medical VLMs and evaluate the impact of medical specialization. We analyze the feature distribution of multiple image modalities extracted by some representative medical VLMs across lesion classification datasets on multiple modalities. These distributions were compared them with non-medical VLMs to assess the domain-specific medical training. Our experiments showed that medical VLMs can extract discriminative features that are effective for medical classification tasks. Moreover, it was found that non-medical VLMs with recent improvement with contextual enrichment such as LLM2CLIP produce more refined feature representations. Our results imply that enhancing text encoder is more crucial than training intensively on medical images when developing medical VLMs. Notably, non-medical models are particularly vulnerable to biases introduced by overlaied text strings on images. These findings underscore the need for careful consideration on model selection according to downstream tasks besides potential risks in inference due to background biases such as textual information in images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>医学视觉语言模型的专业化是否增强判别力？：基于特征分布分析的全面探究</div>
<div class="mono" style="margin-top:8px">本研究探讨了公开开源医学视觉语言模型（VLMs）生成的特征表示。尽管医学VLMs预期能捕捉诊断相关特征，但其学习到的表征尚未被充分探索，且分类准确率等标准评估无法完全揭示它们是否真正获得了具有判别性的病灶特异性特征。理解这些表征对于揭示医学图像结构及改进医学图像分析的下游任务至关重要。本研究旨在探究医学VLMs学习到的特征分布，并评估医学专业化的影响。我们分析了多个代表性医学VLMs在多模态病灶分类数据集上提取的多模态图像特征分布，并与非医学VLMs进行比较，以评估领域特异性医学训练的效果。实验表明，医学VLMs能够提取对医学分类任务有效的判别性特征。此外，研究发现，经过上下文增强（如LLM2CLIP）改进的非医学VLMs能生成更精细的特征表示。结果表明，在开发医学VLMs时，增强文本编码器比密集训练医学图像更为关键。值得注意的是，非医学模型尤其容易受到图像上叠加文本字符串引入的偏差影响。这些发现强调，除了图像中文本信息等背景偏差可能带来的推理风险外，还需根据下游任务谨慎选择模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether medical specialization in vision-language models (VLMs) enhances their ability to learn discriminative features for medical image analysis, as standard evaluations like classification accuracy may not fully reveal the acquisition of lesion-specific features. The method involves a comprehensive feature distribution analysis, comparing the representations extracted by several representative medical VLMs against non-medical VLMs, including advanced models like LLM2CLIP, across multiple medical image modalities and lesion classification datasets. Key experimental findings show that while medical VLMs can extract effective discriminative features, non-medical VLMs with enhanced text encoders and contextual enrichment can produce more refined feature representations, suggesting that improving the text encoder is more crucial than intensive medical image training. Additionally, non-medical models were found to be particularly vulnerable to biases from overlaid text on images, highlighting the importance of model selection based on downstream tasks and awareness of background biases.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究医学视觉-语言模型（VLMs）的专业化是否能增强其学习诊断任务判别性特征的能力，因为标准的准确率指标可能无法完全揭示所学表征的质量。方法是对比分析多个开源医学VLMs与通用领域VLMs的特征分布，在多模态病灶分类数据集上评估领域特定训练的影响。关键实验结果表明，虽然医学VLMs能提取对分类有效的特征，但近期具有增强文本编码器（如LLM2CLIP）的通用领域VLMs产生了更精细的表征，这表明改进文本理解比密集的医学图像训练更为关键。此外，非医学模型更容易受到图像上叠加文本引入的偏差影响，这凸显了根据下游任务谨慎选择模型以及关注背景偏差风险的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">A Training-Free Guess What Vision Language Model from Snippets to Open-Vocabulary Object Detection</div>
<div class="meta-line">Authors: Guiying Zhu, Bowen Yang, Yin Zhuang, Tong Zhang, Guanqun Wang, Zhihao Che, He Chen, Lianlin Li</div>
<div class="meta-line">First: 2026-01-17T05:14:42+00:00 · Latest: 2026-01-21T08:41:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11910v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11910v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-Vocabulary Object Detection (OVOD) aims to develop the capability to detect anything. Although myriads of large-scale pre-training efforts have built versatile foundation models that exhibit impressive zero-shot capabilities to facilitate OVOD, the necessity of creating a universal understanding for any object cognition according to already pretrained foundation models is usually overlooked. Therefore, in this paper, a training-free Guess What Vision Language Model, called GW-VLM, is proposed to form a universal understanding paradigm based on our carefully designed Multi-Scale Visual Language Searching (MS-VLS) coupled with Contextual Concept Prompt (CCP) for OVOD. This approach can engage a pre-trained Vision Language Model (VLM) and a Large Language Model (LLM) in the game of &quot;guess what&quot;. Wherein, MS-VLS leverages multi-scale visual-language soft-alignment for VLM to generate snippets from the results of class-agnostic object detection, while CCP can form the concept of flow referring to MS-VLS and then make LLM understand snippets for OVOD. Finally, the extensive experiments are carried out on natural and remote sensing datasets, including COCO val, Pascal VOC, DIOR, and NWPU-10, and the results indicate that our proposed GW-VLM can achieve superior OVOD performance compared to the-state-of-the-art methods without any training step.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种免训练的猜图视觉语言模型：从片段到开放词汇目标检测</div>
<div class="mono" style="margin-top:8px">开放词汇目标检测旨在发展检测任意对象的能力。尽管大规模预训练已构建出展现出色零样本能力的通用基础模型以促进OVOD，但根据已有预训练基础模型建立对任意对象认知的通用理解常被忽视。为此，本文提出一种免训练的猜图视觉语言模型GW-VLM，通过精心设计的多尺度视觉语言搜索与上下文概念提示构建通用理解范式。该方法使预训练视觉语言模型与大型语言模型参与“猜图”游戏：MS-VLS利用多尺度视觉语言软对齐从类别无关检测结果生成片段，CCP则基于MS-VLS形成概念流以帮助LLM理解片段。最终在自然与遥感数据集上的实验表明，GW-VLM无需训练即可实现优于现有方法的OVOD性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the overlooked need for universal object understanding in Open-Vocabulary Object Detection (OVOD) by proposing GW-VLM, a training-free method that leverages pre-trained Vision Language Models (VLMs) and Large Language Models (LLMs) in a &quot;guess what&quot; framework. It employs Multi-Scale Visual Language Searching (MS-VLS) to generate object snippets from class-agnostic detections and Contextual Concept Prompt (CCP) to enable LLMs to interpret these snippets for detection. Experiments on COCO val, Pascal VOC, DIOR, and NWPU-10 datasets show that GW-VLM achieves state-of-the-art OVOD performance without requiring any training.</div>
<div class="mono" style="margin-top:8px">该研究针对现有基础模型在开放词汇目标检测（OVOD）中普遍对象理解能力不足的问题，提出了一种无需训练的“猜猜看”视觉语言模型（GW-VLM）。该方法通过多尺度视觉语言搜索（MS-VLS）机制，利用预训练的视觉语言模型和大型语言模型，从类别无关的检测结果中生成对象片段，并结合上下文概念提示（CCP）使语言模型理解这些片段以实现检测。在COCO val、Pascal VOC、DIOR和NWPU-10数据集上的实验结果表明，GW-VLM无需任何训练步骤，即可实现优于最先进方法的OVOD性能。</div>
</details>
</div>
<div class="card">
<div class="title">Render-of-Thought: Rendering Textual Chain-of-Thought as Images for Visual Latent Reasoning</div>
<div class="meta-line">Authors: Yifan Wang, Shiyu Li, Peiming Li, Xiaochen Yang, Yang Tang, Zheng Wei</div>
<div class="meta-line">First: 2026-01-21T08:09:25+00:00 · Latest: 2026-01-21T08:09:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14750v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14750v1">PDF</a> · <a href="https://github.com/TencentBAC/RoT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought (CoT) prompting has achieved remarkable success in unlocking the reasoning capabilities of Large Language Models (LLMs). Although CoT prompting enhances reasoning, its verbosity imposes substantial computational overhead. Recent works often focus exclusively on outcome alignment and lack supervision on the intermediate reasoning process. These deficiencies obscure the analyzability of the latent reasoning chain. To address these challenges, we introduce Render-of-Thought (RoT), the first framework to reify the reasoning chain by rendering textual steps into images, making the latent rationale explicit and traceable. Specifically, we leverage the vision encoders of existing Vision Language Models (VLMs) as semantic anchors to align the vision embeddings with the textual space. This design ensures plug-and-play implementation without incurring additional pre-training overhead. Extensive experiments on mathematical and logical reasoning benchmarks demonstrate that our method achieves 3-4x token compression and substantial inference acceleration compared to explicit CoT. Furthermore, it maintains competitive performance against other methods, validating the feasibility of this paradigm. Our code is available at https://github.com/TencentBAC/RoT</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维渲染：将文本思维链转化为图像以实现视觉潜在推理</div>
<div class="mono" style="margin-top:8px">思维链提示在释放大语言模型的推理能力方面取得了显著成功。尽管思维链提示增强了推理能力，但其冗长性带来了巨大的计算开销。近期研究往往仅关注结果对齐，缺乏对中间推理过程的监督，这些缺陷使得潜在推理链的可分析性变得模糊。为解决这些问题，我们提出了思维渲染框架——首个通过将文本步骤渲染为图像来具象化推理链的框架，使潜在逻辑变得显式且可追溯。具体而言，我们利用现有视觉语言模型的视觉编码器作为语义锚点，将视觉嵌入与文本空间对齐。该设计确保了即插即用的实现方式，无需额外预训练开销。在数学与逻辑推理基准上的大量实验表明，相较于显式思维链，我们的方法实现了3-4倍的标记压缩和显著的推理加速，同时保持与其他方法相当的竞争力，验证了该范式的可行性。代码已开源：https://github.com/TencentBAC/RoT</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational inefficiency and lack of intermediate supervision in traditional Chain-of-Thought (CoT) prompting for Large Language Models. The proposed Render-of-Thought (RoT) framework converts verbose textual reasoning steps into images, using vision encoders from existing Vision Language Models to align visual embeddings with textual semantics without requiring additional pre-training. Experimental results on mathematical and logical reasoning benchmarks show that RoT achieves 3-4x token compression and significant inference speedup compared to explicit CoT, while maintaining competitive task performance.</div>
<div class="mono" style="margin-top:8px">本研究针对传统思维链提示在大型语言模型中存在的计算效率低下和中间推理过程缺乏监督的问题，该方法通常产生冗长文本且推理过程不透明。提出的Render-of-Thought框架将文本推理步骤转化为图像，利用现有视觉语言模型的视觉编码器将视觉嵌入与文本语义对齐，实现了即插即用的部署而无需额外预训练。在数学和逻辑推理基准测试上的实验表明，该方法相比显式思维链实现了3-4倍的令牌压缩和显著的推理加速，同时保持了有竞争力的性能。</div>
</details>
</div>
<div class="card">
<div class="title">DeepMoLM: Leveraging Visual and Geometric Structural Information for Molecule-Text Modeling</div>
<div class="meta-line">Authors: Jing Lan, Hexiao Ding, Hongzhao Chen, Yufeng Jiang, Nga-Chun Ng, Gwing Kei Yip, Gerald W. Y. Cheng, Yunlin Mao, Jing Cai, Liang-ting Lin, Jung Sun Yoo</div>
<div class="meta-line">First: 2026-01-21T07:41:59+00:00 · Latest: 2026-01-21T07:41:59+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14732v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14732v1">PDF</a> · <a href="https://github.com/1anj/DeepMoLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI models for drug discovery and chemical literature mining must interpret molecular images and generate outputs consistent with 3D geometry and stereochemistry. Most molecular language models rely on strings or graphs, while vision-language models often miss stereochemical details and struggle to map continuous 3D structures into discrete tokens. We propose DeepMoLM: Deep Molecular Language M odeling, a dual-view framework that grounds high-resolution molecular images in geometric invariants derived from molecular conformations. DeepMoLM preserves high-frequency evidence from 1024 $\times$ 1024 inputs, encodes conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints, and fuses visual and geometric streams with cross-attention, enabling physically grounded generation without atom coordinates. DeepMoLM improves PubChem captioning with a 12.3% relative METEOR gain over the strongest generalist baseline while staying competitive with specialist methods. It produces valid numeric outputs for all property queries and attains MAE 13.64 g/mol on Molecular Weight and 37.89 on Complexity in the specialist setting. On ChEBI-20 description generation from images, it exceeds generalist baselines and matches state-of-the-art vision-language models. Code is available at https://github.com/1anj/DeepMoLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeepMoLM：利用视觉与几何结构信息进行分子-文本建模</div>
<div class="mono" style="margin-top:8px">用于药物发现和化学文献挖掘的AI模型必须能解读分子图像，并生成符合三维几何与立体化学的输出。现有分子语言模型多依赖字符串或图结构，而视觉语言模型常忽略立体化学细节，且难以将连续三维结构映射为离散标记。我们提出DeepMoLM（深度分子语言建模）——一种双视角框架，将高分辨率分子图像锚定于分子构象衍生的几何不变量中。该模型保留1024×1024输入的高频细节，将构象邻域编码为离散的扩展三维指纹，并通过交叉注意力融合视觉与几何流，实现无需原子坐标的物理基础生成。在PubChem描述生成任务中，DeepMoLM相比最强通用基线获得12.3%的相对METEOR提升，同时保持与专业方法的竞争力。其对所有性质查询均能生成有效数值输出，在专业设定下分子量预测MAE为13.64 g/mol，复杂度预测MAE为37.89。在ChEBI-20图像描述生成任务中，其表现超越通用基线并与前沿视觉语言模型相当。代码发布于https://github.com/1anj/DeepMoLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for AI models in drug discovery and chemical literature mining to accurately interpret molecular images while preserving 3D geometric and stereochemical information, as existing methods often rely on simplified representations that miss these critical details. The proposed DeepMoLM framework integrates high-resolution molecular images with geometric invariants from molecular conformations, encoding conformer neighborhoods as discrete Extended 3-Dimensional Fingerprints and fusing visual and geometric streams via cross-attention to enable physically grounded generation without direct atom coordinates. Experimental results show that DeepMoLM achieves a 12.3% relative improvement in METEOR score over the strongest generalist baseline for PubChem captioning, produces valid numeric outputs for all property queries with MAEs of 13.64 g/mol on Molecular Weight and 37.89 on Complexity, and matches state-of-the-art vision-language models on ChEBI-20 description generation.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决药物发现和化学文献挖掘中AI模型需要解释分子图像并准确反映三维几何与立体化学信息的需求，因为现有方法通常依赖简化表示而忽略了这些关键细节。提出的DeepMoLM框架将高分辨率分子图像与分子构象的几何不变量相结合，将构象邻域编码为离散的扩展三维指纹，并通过交叉注意力融合视觉和几何数据，从而在不依赖原子坐标的情况下实现基于物理的文本生成。实验结果表明，DeepMoLM在PubChem描述生成任务上相比最强通用基线取得了12.3%的相对METEOR分数提升，对所有属性查询均能生成有效数值输出（分子量平均绝对误差为13.64 g/mol），并在ChEBI-20图像描述生成任务上达到了与最先进视觉语言模型相当的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Can Synthetic Images Serve as Effective and Efficient Class Prototypes?</div>
<div class="meta-line">Authors: Dianxing Shi, Dingjie Fu, Yuqiao Liu, Jun Wang</div>
<div class="meta-line">First: 2025-12-19T01:39:43+00:00 · Latest: 2026-01-21T07:00:03+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17160v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17160v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown strong performance in zero-shot image classification tasks. However, existing methods, including Contrastive Language-Image Pre-training (CLIP), all rely on annotated text-to-image pairs for aligning visual and textual modalities. This dependency introduces substantial cost and accuracy requirement in preparing high-quality datasets. At the same time, processing data from two modes also requires dual-tower encoders for most models, which also hinders their lightweight. To address these limitations, we introduce a ``Contrastive Language-Image Pre-training via Large-Language-Model-based Generation (LGCLIP)&quot; framework. LGCLIP leverages a Large Language Model (LLM) to generate class-specific prompts that guide a diffusion model in synthesizing reference images. Afterwards these generated images serve as visual prototypes, and the visual features of real images are extracted and compared with the visual features of these prototypes to achieve comparative prediction. By optimizing prompt generation through the LLM and employing only a visual encoder, LGCLIP remains lightweight and efficient. Crucially, our framework requires only class labels as input during whole experimental procedure, eliminating the need for manually annotated image-text pairs and extra pre-processing. Experimental results validate the feasibility and efficiency of LGCLIP, demonstrating great performance in zero-shot classification tasks and establishing a novel paradigm for classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合成图像能否作为有效且高效的类别原型？</div>
<div class="mono" style="margin-top:8px">视觉-语言模型在零样本图像分类任务中表现出色，但现有方法（包括对比语言-图像预训练）均依赖标注的文本-图像对来实现视觉与文本模态的对齐。这种依赖性在准备高质量数据集时带来了高昂成本和精度要求。同时，双模态数据处理通常需要双塔编码器，也限制了模型的轻量化。为突破这些局限，我们提出了“基于大语言模型生成的对比语言-图像预训练”框架。该框架利用大语言模型生成类别特定的提示词，引导扩散模型合成参考图像。这些生成图像作为视觉原型，通过提取真实图像的视觉特征并与原型特征进行比对来实现分类预测。通过大语言模型优化提示生成并仅使用视觉编码器，该框架保持了轻量化与高效性。关键优势在于整个实验过程仅需类别标签作为输入，无需人工标注的图文对和额外预处理。实验结果验证了该框架的可行性与高效性，在零样本分类任务中表现优异，为分类任务建立了新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the high cost and accuracy requirements of preparing annotated text-image pairs for Vision-Language Models (VLMs) like CLIP, and the computational burden of their dual-tower encoders. The proposed method, LGCLIP, uses a Large Language Model to generate class-specific prompts, which guide a diffusion model to synthesize reference images that serve as visual prototypes; classification is then performed by comparing the visual features of real images to these prototypes using only a visual encoder. Experimental results demonstrate that LGCLIP achieves strong performance in zero-shot classification tasks, validating its feasibility and efficiency while requiring only class labels as input.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有视觉语言模型（如CLIP）依赖标注图文对进行零样本分类所导致的高成本及双编码器复杂性问题。为此，作者提出了LGCLIP框架，该框架利用大语言模型生成类别特定的提示词，进而引导扩散模型合成视觉原型；分类时仅使用单一视觉编码器，通过比较真实图像与这些原型的视觉特征来实现。实验结果表明，LGCLIP在零样本分类任务中取得了优异性能，且无需人工标注的图文对，运行高效。</div>
</details>
</div>
<div class="card">
<div class="title">AutoDriDM: An Explainable Benchmark for Decision-Making of Vision-Language Models in Autonomous Driving</div>
<div class="meta-line">Authors: Zecong Tang, Zixu Wang, Yifei Wang, Weitong Lian, Tianjian Gao, Haoran Li, Tengju Ru, Lingyi Meng, Zhejun Cui, Yichen Zhu, Qi Kang, Kaixuan Wang, Yu Zhang</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2026-01-21T06:29:09+00:00 · Latest: 2026-01-21T06:29:09+00:00</div>
<div class="meta-line">Comments: 23 pages. Submitted to ACL ARR 2026 January</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14702v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14702v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous driving is a highly challenging domain that requires reliable perception and safe decision-making in complex scenarios. Recent vision-language models (VLMs) demonstrate reasoning and generalization abilities, opening new possibilities for autonomous driving; however, existing benchmarks and metrics overemphasize perceptual competence and fail to adequately assess decision-making processes. In this work, we present AutoDriDM, a decision-centric, progressive benchmark with 6,650 questions across three dimensions - Object, Scene, and Decision. We evaluate mainstream VLMs to delineate the perception-to-decision capability boundary in autonomous driving, and our correlation analysis reveals weak alignment between perception and decision-making performance. We further conduct explainability analyses of models&#x27; reasoning processes, identifying key failure modes such as logical reasoning errors, and introduce an analyzer model to automate large-scale annotation. AutoDriDM bridges the gap between perception-centered and decision-centered evaluation, providing guidance toward safer and more reliable VLMs for real-world autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoDriDM：自动驾驶中视觉语言模型决策能力的可解释性基准</div>
<div class="mono" style="margin-top:8px">自动驾驶是极具挑战性的领域，需要在复杂场景中实现可靠感知与安全决策。近期视觉语言模型展现出推理与泛化能力，为自动驾驶开辟了新可能；然而现有基准与指标过度强调感知能力，未能充分评估决策过程。本研究提出AutoDriDM——一个以决策为核心、包含6,650道问题的渐进式基准，涵盖对象、场景、决策三个维度。我们通过评估主流视觉语言模型，界定了自动驾驶中感知到决策的能力边界，相关性分析显示感知与决策性能存在弱关联性。进一步对模型推理过程进行可解释性分析，识别出逻辑推理错误等关键失效模式，并引入分析器模型实现大规模自动标注。AutoDriDM填补了以感知为中心和以决策为中心的评估体系间的空白，为构建更安全可靠的现实世界自动驾驶视觉语言模型提供指引。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the gap in current autonomous driving benchmarks, which overly focus on perception while neglecting the assessment of decision-making processes, despite the potential of vision-language models (VLMs) to enhance reasoning in this domain. The authors introduce AutoDriDM, a decision-centric benchmark comprising 6,650 questions across Object, Scene, and Decision dimensions, to evaluate mainstream VLMs and analyze their reasoning through explainability methods, including an automated analyzer model for large-scale annotation. Key experimental findings reveal a weak correlation between perception and decision-making performance in VLMs and identify critical failure modes such as logical reasoning errors, thereby providing guidance for developing safer and more reliable models for real-world autonomous driving applications.</div>
<div class="mono" style="margin-top:8px">本研究针对当前自动驾驶基准测试过度关注感知能力而忽视决策过程评估的不足，尽管视觉语言模型在复杂驾驶场景中展现出增强推理的潜力。作者提出了AutoDriDM，一个以决策为中心的基准测试，包含对象、场景和决策三个维度的6650个问题，用于评估主流视觉语言模型，并通过可解释性分析（包括用于大规模标注的自动化分析模型）来剖析其推理过程。关键实验结果表明，视觉语言模型的感知与决策性能之间相关性较弱，逻辑推理错误被确定为主要失败模式，从而为开发更安全可靠的现实世界自动驾驶模型提供了指导。</div>
</details>
</div>
<div class="card">
<div class="title">T2T-VICL: Unlocking the Boundaries of Cross-Task Visual In-Context Learning via Implicit Text-Driven VLMs</div>
<div class="meta-line">Authors: Shao-Jun Xia, Huixin Zhang, Zhengzhong Tu</div>
<div class="meta-line">First: 2025-11-20T07:02:06+00:00 · Latest: 2026-01-21T06:18:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16107v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16107v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In large language models (LLM), in-context learning (ICL) refers to performing new tasks by conditioning on small demonstrations provided in the input context. Recent advances in visual in-context learning (VICL) demonstrate promising capabilities for solving downstream tasks by unified vision-language models (VLMs). When the visual prompt and the target images originate from different visual tasks, can VLMs still enable VICL? In the paper, we propose a fully collaborative pipeline, i.e. T2T-VICL, for VLMs to investigate the potential of cross-task VICL. Fundamentally, we design a mechanism to generate and select text prompts that best implicitly describe the differences between two distinct low-level vision tasks, and construct the first cross-task VICL dataset. Building upon this, we propose a novel inference framework that combines perceptual score-based reasoning with traditional evaluation metrics to perform cross-task VICL. Our approach achieves top-tier results across twelve cross-task scenarios and second-tier performance in nine additional scenarios, unlocking the boundaries of cross-task VICL within VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T2T-VICL：通过隐式文本驱动视觉语言模型突破跨任务视觉上下文学习的边界</div>
<div class="mono" style="margin-top:8px">在大型语言模型（LLM）中，上下文学习（ICL）指通过输入上下文中提供的少量示例来执行新任务。视觉上下文学习（VICL）的最新进展表明，统一视觉语言模型（VLM）在解决下游任务方面展现出巨大潜力。当视觉提示与目标图像源自不同视觉任务时，VLM是否仍能实现VICL？本文提出一个全协作流程T2T-VICL，用于探索VLM在跨任务VICL中的潜力。核心机制是设计生成并筛选能最有效隐式描述两种低层视觉任务差异的文本提示，并构建首个跨任务VICL数据集。在此基础上，提出一种融合感知评分推理与传统评估指标的新型推断框架，以执行跨任务VICL。该方法在十二个跨任务场景中取得顶尖性能，在另外九个场景中位列第二梯队，成功突破了VLM在跨任务VICL中的能力边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses whether vision-language models can perform visual in-context learning when demonstration and target images come from different low-level vision tasks. The authors propose T2T-VICL, a pipeline that generates and selects text prompts that implicitly describe task differences, constructs a cross-task dataset, and introduces an inference framework combining perceptual scoring with traditional metrics. Their method achieves top-tier results in twelve cross-task scenarios and second-tier performance in nine others, demonstrating expanded cross-task in-context learning capabilities.</div>
<div class="mono" style="margin-top:8px">本研究探讨了当演示图像与目标图像来自不同低级视觉任务时，视觉语言模型是否仍能进行视觉上下文学习。作者提出了T2T-VICL流程，通过生成和选择能隐式描述任务差异的文本提示，构建了首个跨任务数据集，并引入了一个结合感知评分推理与传统指标的推断框架。该方法在十二个跨任务场景中取得顶级性能，在另外九个场景中取得次优性能，显著突破了视觉语言模型在跨任务视觉上下文学习中的边界。</div>
</details>
</div>
<div class="card">
<div class="title">LRR-Bench: Left, Right or Rotate? Vision-Language models Still Struggle With Spatial Understanding Tasks</div>
<div class="meta-line">Authors: Fei Kong</div>
<div class="meta-line">First: 2025-07-27T08:31:24+00:00 · Latest: 2026-01-21T05:06:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20174v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.20174v2">PDF</a> · <a href="https://github.com/kong13661/LRR-Bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world applications, such as autonomous driving and humanoid robot manipulation, require precise spatial perception. However, it remains underexplored how Vision-Language Models (VLMs) recognize spatial relationships and perceive spatial movement. In this work, we introduce a spatial evaluation pipeline and construct a corresponding benchmark. Specifically, we categorize spatial understanding into two main types: absolute spatial understanding, which involves querying the absolute spatial position (e.g., left, right) of an object within an image, and 3D spatial understanding, which includes movement and rotation. Notably, our dataset is entirely synthetic, enabling the generation of test samples at a low cost while also preventing dataset contamination. We conduct experiments on multiple state-of-the-art VLMs and observe that there is significant room for improvement in their spatial understanding abilities. Explicitly, in our experiments, humans achieve near-perfect performance on all tasks, whereas current VLMs attain human-level performance only on the two simplest tasks. For the remaining tasks, the performance of VLMs is distinctly lower than that of humans. In fact, the best-performing Vision-Language Models even achieve near-zero scores on multiple tasks. The dataset and code are available on https://github.com/kong13661/LRR-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LRR-Bench：左、右还是旋转？视觉语言模型在空间理解任务中仍面临挑战</div>
<div class="mono" style="margin-top:8px">自动驾驶和人形机器人操控等实际应用需要精确的空间感知能力。然而，视觉语言模型如何识别空间关系与感知空间运动仍缺乏深入探索。本研究提出一套空间评估流程并构建了相应基准。具体而言，我们将空间理解分为两类：绝对空间理解（涉及查询图像中物体的绝对空间位置，如左、右）和三维空间理解（包含移动与旋转）。值得注意的是，本数据集完全通过合成生成，能以低成本创建测试样本，同时避免数据集污染。我们在多个前沿视觉语言模型上进行实验，发现其空间理解能力仍有显著提升空间。实验数据显示，人类在所有任务中均接近完美表现，而当前视觉语言模型仅在两项最简单任务上达到人类水平。对于其余任务，模型表现明显低于人类。事实上，性能最优的视觉语言模型在多项任务中得分甚至接近零分。数据集与代码已发布于 https://github.com/kong13661/LRR-Bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for precise spatial perception in applications like autonomous driving and robot manipulation, this work investigates the spatial understanding capabilities of Vision-Language Models (VLMs). The method involves creating a synthetic benchmark, LRR-Bench, which categorizes spatial understanding into absolute position queries and 3D movement/rotation tasks, enabling low-cost, contamination-free evaluation. Key experimental findings reveal a significant performance gap: while humans achieve near-perfect scores, current VLMs only match human performance on the simplest tasks and show near-zero accuracy on several more complex spatial reasoning challenges.</div>
<div class="mono" style="margin-top:8px">受自动驾驶和机器人操控等应用对精确空间感知需求的驱动，本研究探究了视觉语言模型（VLMs）的空间理解能力。方法上，研究构建了一个合成基准测试LRR-Bench，将空间理解分为绝对位置（如左/右）和三维运动/旋转两类任务，实现了低成本且无数据污染的评估。关键实验结果表明存在显著的性能差距：人类在各项任务上接近完美表现，而当前VLMs仅在最简单的任务上达到人类水平，在多个更复杂的空间推理任务上得分甚至接近零。</div>
</details>
</div>
<div class="card">
<div class="title">Forest-Chat: Adapting Vision-Language Agents for Interactive Forest Change Analysis</div>
<div class="meta-line">Authors: James Brock, Ce Zhang, Nantheera Anantrasirichai</div>
<div class="meta-line">First: 2026-01-21T04:23:33+00:00 · Latest: 2026-01-21T04:23:33+00:00</div>
<div class="meta-line">Comments: 22 pages, 8 figures, 7 tables, Submitted to Ecological Informatics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14637v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14637v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increasing availability of high-resolution satellite imagery, together with advances in deep learning, creates new opportunities for enhancing forest monitoring workflows. Two central challenges in this domain are pixel-level change detection and semantic change interpretation, particularly for complex forest dynamics. While large language models (LLMs) are increasingly adopted for data exploration, their integration with vision-language models (VLMs) for remote sensing image change interpretation (RSICI) remains underexplored, especially beyond urban environments. We introduce Forest-Chat, an LLM-driven agent designed for integrated forest change analysis. The proposed framework enables natural language querying and supports multiple RSICI tasks, including change detection, change captioning, object counting, deforestation percentage estimation, and change reasoning. Forest-Chat builds upon a multi-level change interpretation (MCI) vision-language backbone with LLM-based orchestration, and incorporates zero-shot change detection via a foundation change detection model together with an interactive point-prompt interface to support fine-grained user guidance. To facilitate adaptation and evaluation in forest environments, we introduce the Forest-Change dataset, comprising bi-temporal satellite imagery, pixel-level change masks, and multi-granularity semantic change captions generated through a combination of human annotation and rule-based methods. Experimental results demonstrate that Forest-Chat achieves strong performance on Forest-Change and on LEVIR-MCI-Trees, a tree-focused subset of LEVIR-MCI, for joint change detection and captioning, highlighting the potential of interactive, LLM-driven RSICI systems to improve accessibility, interpretability, and analytical efficiency in forest change analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Forest-Chat：面向交互式森林变化分析的视觉-语言智能体适配</div>
<div class="mono" style="margin-top:8px">高分辨率卫星影像的日益普及与深度学习的进步，为增强森林监测工作流程创造了新机遇。该领域的两大核心挑战是像素级变化检测与语义变化解译，尤其在复杂森林动态场景中。尽管大语言模型（LLMs）在数据探索中的应用日益广泛，但其与视觉-语言模型（VLMs）在遥感影像变化解译（RSICI）中的融合仍待深入探索，特别是在非城市环境领域。本文提出Forest-Chat——一个面向集成式森林变化分析的LLM驱动智能体。该框架支持自然语言查询，可执行多种RSICI任务，包括变化检测、变化描述、目标计数、森林砍伐比例估算及变化归因分析。Forest-Chat构建于具有LLM编排能力的多层次变化解译（MCI）视觉-语言骨干网络之上，通过基础变化检测模型实现零样本变化检测，并结合交互式点提示界面以支持细粒度用户引导。为促进森林环境中的适配与评估，我们构建了Forest-Change数据集，包含双时相卫星影像、像素级变化掩码，以及通过人工标注与规则方法生成的多粒度语义变化描述。实验结果表明，Forest-Chat在Forest-Change数据集及以树木为核心的LEVIR-MCI子集LEVIR-MCI-Trees上，均实现了优异的变化检测与描述联合性能，彰显了交互式LLM驱动RSICI系统在提升森林变化分析的可访问性、可解释性与分析效率方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for enhanced forest monitoring by tackling pixel-level change detection and semantic interpretation of complex forest dynamics, where the integration of large language models (LLMs) with vision-language models (VLMs) for remote sensing image change interpretation remains underexplored. The method introduces Forest-Chat, an LLM-driven agent that uses a multi-level change interpretation vision-language backbone with LLM orchestration, incorporating zero-shot change detection via a foundation model and an interactive point-prompt interface for fine-grained user guidance. Experimental results on the introduced Forest-Change dataset and a tree-focused subset of LEVIR-MCI show strong performance in joint change detection and captioning, demonstrating the system&#x27;s potential to improve accessibility, interpretability, and efficiency in forest change analysis.</div>
<div class="mono" style="margin-top:8px">该研究针对森林监测中像素级变化检测和复杂森林动态语义解释的需求，其中将大语言模型（LLM）与视觉语言模型（VLM）集成用于遥感图像变化解释（RSICI）的探索不足。方法提出了Forest-Chat，这是一个基于LLM驱动的智能体，采用多级变化解释的视觉语言骨干网络与LLM编排，结合了基于基础模型的零样本变化检测和交互式点提示界面以支持细粒度用户引导。在引入的Forest-Change数据集和以树木为重点的LEVIR-MCI子集上的实验结果表明，其在联合变化检测和描述任务中表现强劲，凸显了该系统在提升森林变化分析的可访问性、可解释性和效率方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Consistent Taxonomic Classification through Hierarchical Reasoning</div>
<div class="meta-line">Authors: Zhenghong Li, Kecheng Zheng, Haibin Ling</div>
<div class="meta-line">First: 2026-01-21T03:00:00+00:00 · Latest: 2026-01-21T03:00:00+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14610v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14610v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) excel at visual understanding, they often fail to grasp hierarchical knowledge. This leads to common errors where VLMs misclassify coarser taxonomic levels even when correctly identifying the most specific level (leaf level). Existing approaches largely overlook this issue by failing to model hierarchical reasoning. To address this gap, we propose VL-Taxon, a two-stage, hierarchy-based reasoning framework designed to improve both leaf-level accuracy and hierarchical consistency in taxonomic classification. The first stage employs a top-down process to enhance leaf-level classification accuracy. The second stage then leverages this accurate leaf-level output to ensure consistency throughout the entire taxonomic hierarchy. Each stage is initially trained with supervised fine-tuning to instill taxonomy knowledge, followed by reinforcement learning to refine the model&#x27;s reasoning and generalization capabilities. Extensive experiments reveal a remarkable result: our VL-Taxon framework, implemented on the Qwen2.5-VL-7B model, outperforms its original 72B counterpart by over 10% in both leaf-level and hierarchical consistency accuracy on average on the iNaturalist-2021 dataset. Notably, this significant gain was achieved by fine-tuning on just a small subset of data, without relying on any examples generated by other VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过层次化推理实现一致性的分类学分类</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言模型在视觉理解方面表现出色，但它们往往难以掌握层次化知识。这导致常见错误：即使正确识别了最具体的层级（叶层级），VLM仍会错误分类更粗粒度的分类学层级。现有方法大多忽视了这一问题，未能对层次化推理进行建模。为填补这一空白，我们提出了VL-Taxon——一个基于层次的两阶段推理框架，旨在提升分类学分类中的叶层级准确性和层次一致性。第一阶段采用自上而下的流程来提升叶层级分类准确率；第二阶段则利用准确的叶层级输出来确保整个分类学层次的一致性。每个阶段首先通过监督微调注入分类学知识，再通过强化学习优化模型的推理与泛化能力。大量实验显示：在iNaturalist-2021数据集上，基于Qwen2.5-VL-7B模型实现的VL-Taxon框架，其叶层级与层次一致性准确率平均超越原版72B模型超过10%。值得注意的是，这一显著提升仅通过对少量数据子集进行微调实现，且未依赖任何其他VLM生成的示例。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) often lack hierarchical reasoning, leading to inconsistent taxonomic classifications where they correctly identify specific species but misclassify broader categories. To address this, the authors propose VL-Taxon, a two-stage framework that first improves leaf-level classification accuracy via a top-down process and then uses this output to enforce consistency across the entire taxonomic hierarchy. Each stage is trained with supervised fine-tuning followed by reinforcement learning. Experiments on the iNaturalist-2021 dataset show that VL-Taxon, implemented on the Qwen2.5-VL-7B model, outperforms the original 72B model by over 10% in both leaf-level and hierarchical consistency accuracy, achieving this gain with fine-tuning on only a small data subset and without using examples from other VLMs.</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）通常缺乏层次推理能力，导致分类学分类不一致，即模型能正确识别具体物种却误判更广泛的类别。为解决此问题，研究者提出了VL-Taxon，一个两阶段框架：第一阶段通过自上而下的过程提升叶节点分类精度，第二阶段利用精确的叶节点输出确保整个分类学层次的一致性。在iNaturalist-2021数据集上的实验表明，基于Qwen2.5-VL-7B模型实现的VL-Taxon，其叶节点准确率和层次一致性平均比原版72B模型高出10%以上，且仅通过小规模数据微调实现，未依赖其他VLM生成的示例。</div>
</details>
</div>
<div class="card">
<div class="title">3D Space as a Scratchpad for Editable Text-to-Image Generation</div>
<div class="meta-line">Authors: Oindrila Saha, Vojtech Krs, Radomir Mech, Subhransu Maji, Matheus Gadelha, Kevin Blackburn-Matzen</div>
<div class="meta-line">First: 2026-01-21T02:40:19+00:00 · Latest: 2026-01-21T02:40:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14602v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14602v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://oindrilasaha.github.io/3DScratchpad/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in large language models (LLMs) has shown that reasoning improves when intermediate thoughts are externalized into explicit workspaces, such as chain-of-thought traces or tool-augmented reasoning. Yet, visual language models (VLMs) lack an analogous mechanism for spatial reasoning, limiting their ability to generate images that accurately reflect geometric relations, object identities, and compositional intent. We introduce the concept of a spatial scratchpad -- a 3D reasoning substrate that bridges linguistic intent and image synthesis. Given a text prompt, our framework parses subjects and background elements, instantiates them as editable 3D meshes, and employs agentic scene planning for placement, orientation, and viewpoint selection. The resulting 3D arrangement is rendered back into the image domain with identity-preserving cues, enabling the VLM to generate spatially consistent and visually coherent outputs. Unlike prior 2D layout-based methods, our approach supports intuitive 3D edits that propagate reliably into final images. Empirically, it achieves a 32% improvement in text alignment on GenAI-Bench, demonstrating the benefit of explicit 3D reasoning for precise, controllable image generation. Our results highlight a new paradigm for vision-language models that deliberate not only in language, but also in space. Code and visualizations at https://oindrilasaha.github.io/3DScratchpad/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三维空间作为可编辑文本到图像生成的草稿板</div>
<div class="mono" style="margin-top:8px">近期大语言模型的研究表明，将中间思考过程外化至显式工作空间（如思维链轨迹或工具增强推理）能提升推理能力。然而，视觉语言模型缺乏类似的空间推理机制，限制了其生成准确反映几何关系、物体识别与构图意图的图像。我们提出空间草稿板概念——一种连接语言意图与图像合成的三维推理基底。给定文本提示，本框架解析主体与背景元素，将其实例化为可编辑三维网格，并采用智能场景规划进行布局、朝向与视角选择。生成的三维布局通过保持识别特征的线索渲染回图像域，使视觉语言模型能生成空间一致且视觉连贯的输出。与先前基于二维布局的方法不同，本方法支持直观的三维编辑，并能可靠传递至最终图像。实验表明，该方法在GenAI-Bench上实现文本对齐度32%的提升，证实了显式三维推理对精确可控图像生成的益处。我们的成果为视觉语言模型开辟了新范式，使其不仅在语言层面，更能在空间维度进行推演。代码与可视化结果详见：https://oindrilasaha.github.io/3DScratchpad/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that while large language models benefit from externalizing intermediate reasoning steps, visual language models lack a similar mechanism for spatial reasoning, which limits their ability to generate images with accurate geometric relations and object composition. To address this, the method introduces a spatial scratchpad, a 3D reasoning substrate that parses text prompts into subjects and background elements, instantiates them as editable 3D meshes, and uses agentic scene planning for placement, orientation, and viewpoint selection before rendering back to the image domain with identity-preserving cues. Experimental results show a 32% improvement in text alignment on GenAI-Bench, demonstrating that explicit 3D reasoning enables more precise and controllable image generation compared to prior 2D layout-based approaches.</div>
<div class="mono" style="margin-top:8px">该研究的动机是观察到，尽管大型语言模型通过外化中间推理步骤而受益，但视觉语言模型缺乏类似的空间推理机制，这限制了其生成准确反映几何关系和构图意图的图像的能力。该方法引入了空间草稿板这一3D推理基底，将文本提示解析为主题和背景元素，将它们实例化为可编辑的3D网格，并利用智能体场景规划进行放置、方向和视角选择，最后通过保持身份线索渲染回图像域。关键实验结果表明，在GenAI-Bench基准上，文本对齐度相比先前方法提高了32%，证明了显式3D推理能够实现更精确、可控的图像生成，并支持可靠的编辑传播。</div>
</details>
</div>
<div class="card">
<div class="title">Coding the Visual World: From Image to Simulation Using Vision Language Models</div>
<div class="meta-line">Authors: Sagi Eppel</div>
<div class="meta-line">First: 2026-01-08T19:49:05+00:00 · Latest: 2026-01-20T21:37:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05344v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05344v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to construct mental models of the world is a central aspect of understanding. Similarly, visual understanding can be viewed as the ability to construct a representative model of the system depicted in an image. This work explores the capacity of Vision Language Models (VLMs) to recognize and simulate the systems and mechanisms depicted in images using the Im2Sim methodology. The VLM is given a natural image of a real-world system (e.g., cities, clouds, vegetation) and is tasked with describing the system and writing code that simulates and generates it. This generative code is then executed to produce a synthetic image, which is compared against the original. This approach is tested on various complex emergent systems, ranging from physical systems (waves, lights, clouds) to vegetation, cities, materials, and geological formations. Through analysis of the models and images generated by the VLMs, we examine their understanding of the systems in images. The results show that leading VLMs (GPT, Gemini) have the ability to understand and model complex, multi-component systems across multiple layers of abstraction and a wide range of domains. At the same time, the VLMs exhibit limited ability to replicate fine details and low-level arrangements of patterns in the image. These findings reveal an interesting asymmetry: VLMs combine high-level, deep visual understanding of images with limited perception of fine details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>编码视觉世界：基于视觉语言模型从图像到仿真的实现</div>
<div class="mono" style="margin-top:8px">构建世界的心智模型是理解能力的核心体现。类似地，视觉理解可视为构建图像所描绘系统的表征模型的能力。本研究通过Im2Sim方法，探索视觉语言模型识别并仿真图像中系统与机制的能力。模型接收真实世界系统（如城市、云层、植被）的自然图像，需描述该系统并编写能仿真生成该系统的代码。执行此生成代码后得到合成图像，再与原图进行对比。该方法在多种复杂涌现系统上进行了测试，涵盖物理系统（波浪、光线、云层）至植被、城市、材料及地质构造。通过分析模型生成的代码与图像，我们检验了其对图像系统的理解程度。结果表明，领先的视觉语言模型（GPT、Gemini）具备跨多抽象层级与广泛领域理解并建模复杂多组件系统的能力。同时，这些模型在复现图像精细细节与底层模式排列方面能力有限。这些发现揭示了一种有趣的不对称性：视觉语言模型兼具对图像的高层次深度理解能力与对细节的有限感知能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether Vision Language Models (VLMs) can achieve a deeper, model-based understanding of images by not just describing them but also generating executable code to simulate the depicted systems. The proposed Im2Sim method prompts a VLM to analyze a natural image of a complex real-world system, produce a descriptive explanation, and then write a program that, when run, synthesizes a comparable image; the fidelity of this simulation is evaluated by comparing the synthetic output to the original. Experiments across diverse domains like physical phenomena, vegetation, and urban structures show that leading VLMs (e.g., GPT, Gemini) can successfully capture and model high-level system components and their abstract relationships, yet they struggle to accurately replicate fine-grained details and low-level pattern arrangements, revealing an asymmetry between strong conceptual understanding and weaker perception of minutiae.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究视觉语言模型（VLMs）能否通过构建对图像中描绘系统的可执行模拟，来实现更深层次的、基于模型的理解。所提出的Im2Sim方法提示VLM分析自然图像、描述其展现的系统，并生成能模拟和视觉重建该系统的代码；通过将生成的合成图像与原始图像进行比较来评估。在物理现象、植被和城市等多个领域的实验表明，领先的VLM（如GPT、Gemini）能够在高层次、抽象层面理解和建模复杂系统，但难以复现精细的细节和低层级的模式排列，这揭示了其高层次概念把握与细节感知保真度之间的不对称性。</div>
</details>
</div>
<div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-20T21:26:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：面向文档的具身视觉语言前端系统</div>
<div class="mono" style="margin-top:8px">GutenOCR是通过微调Qwen2.5-VL-3B和Qwen2.5-VL-7B获得的一系列具身OCR前端模型。所得的单检查点视觉语言模型通过统一的提示驱动接口，实现文本读取、检测与定位功能。基于商业文档、科学文献及合成定位数据训练，该模型支持整页与局部读取，提供行级与段落级边界框，并能响应“X在哪里？”的条件查询。我们提出了具身OCR评估框架，实验表明在1.05万份保留的商业与科学文档上，GutenOCR-7B的复合具身OCR分数较其骨干网络Qwen2.5-VL-7B提升超一倍（0.40至0.82）。在Fox与OmniDocBench v1.5基准测试中，本方法显著提升了区域/行级OCR性能及文本检测召回率，但在页面级线性化、色彩引导OCR及公式密集版式处理方面存在权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for unified document understanding models that combine reading, detection, and grounding capabilities. The method involves fine-tuning Qwen2.5-VL vision-language models (3B and 7B parameters) on business documents, scientific articles, and synthetic grounding data to create GutenOCR, which offers a prompt-based interface for full-page and localized reading with bounding box outputs. Experimental results show that GutenOCR-7B more than doubles the composite grounded OCR score of its backbone model (from 0.40 to 0.82) on 10.5K held-out pages, substantially improves region- and line-level OCR and text-detection recall on Fox and OmniDocBench benchmarks, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发能够统一结合文本识别、检测与空间定位能力的文档理解模型。方法上，通过对Qwen2.5-VL视觉语言模型（30亿和70亿参数）在商业文档、科学论文及合成定位数据上进行微调，构建了GutenOCR系列模型，其通过提示接口支持整页/局部阅读及空间查询。主要实验结果表明，GutenOCR-7B在10.5万张保留业务与科学文档上的综合定位OCR分数相比其骨干模型提升一倍以上（0.40至0.82），在Fox和OmniDocBench基准测试中显著提升了区域/行级OCR性能与文本检测召回率，但也揭示了在页面级线性化、颜色引导OCR及公式密集布局处理方面的性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Large-Scale Label Quality Assessment for Medical Segmentation via a Vision-Language Judge and Synthetic Data</div>
<div class="meta-line">Authors: Yixiong Chen, Zongwei Zhou, Wenxuan Li, Alan Yuille</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-20T19:09:12+00:00 · Latest: 2026-01-20T19:09:12+00:00</div>
<div class="meta-line">Comments: ISBI 2026 accepted</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14406v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14406v1">PDF</a> · <a href="https://github.com/Schuture/SegAE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale medical segmentation datasets often combine manual and pseudo-labels of uneven quality, which can compromise training and evaluation. Low-quality labels may hamper performance and make the model training less robust. To address this issue, we propose SegAE (Segmentation Assessment Engine), a lightweight vision-language model (VLM) that automatically predicts label quality across 142 anatomical structures. Trained on over four million image-label pairs with quality scores, SegAE achieves a high correlation coefficient of 0.902 with ground-truth Dice similarity and evaluates a 3D mask in 0.06s. SegAE shows several practical benefits: (I) Our analysis reveals widespread low-quality labeling across public datasets; (II) SegAE improves data efficiency and training performance in active and semi-supervised learning, reducing dataset annotation cost by one-third and quality-checking time by 70% per label. This tool provides a simple and effective solution for quality control in large-scale medical segmentation datasets. The dataset, model weights, and codes are released at https://github.com/Schuture/SegAE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言判据与合成数据的大规模医学分割标签质量评估</div>
<div class="mono" style="margin-top:8px">大规模医学分割数据集常包含质量不均的人工标注与伪标签，可能影响训练与评估。低质量标签会损害模型性能并降低训练鲁棒性。为此，我们提出SegAE（分割评估引擎），一种轻量级视觉语言模型，可自动预测142个解剖结构的标签质量。该模型在超过四百万带质量评分的图像-标签对上训练，与真实Dice相似度的相关系数达0.902，评估单个3D掩码仅需0.06秒。SegAE具有多重实用价值：（一）分析揭示公共数据集普遍存在低质量标注；（二）在主动学习与半监督学习中，SegAE能提升数据效率与训练性能，将数据集标注成本降低三分之一，单标签质检时间减少70%。该工具为大规模医学分割数据集的质量控制提供了简洁高效的解决方案。数据集、模型权重及代码已发布于https://github.com/Schuture/SegAE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large-scale medical segmentation datasets often contain labels of varying quality from manual and pseudo-labeling sources, which can undermine model training and evaluation. To address this, the authors propose SegAE, a lightweight vision-language model trained on over four million image-label pairs with quality scores to automatically assess label quality across 142 anatomical structures. Experimental results show SegAE achieves a 0.902 correlation with ground-truth Dice scores, processes a 3D mask in 0.06 seconds, reveals widespread low-quality labeling in public datasets, and improves data efficiency—reducing annotation costs by one-third and quality-checking time by 70% per label in active and semi-supervised learning scenarios.</div>
<div class="mono" style="margin-top:8px">大规模医学分割数据集常包含质量不均的标签，这会损害模型训练与评估。为此，研究者提出了SegAE，一个轻量级的视觉语言模型，通过在超过四百万个图像-标签对上训练，自动评估142个解剖结构的标签质量。实验结果表明，SegAE与真实Dice分数的相关系数达0.902，能识别公共数据集中普遍存在的低质量标签，并在主动学习应用中，将标注成本降低三分之一，每个标签的质量检查时间减少70%。</div>
</details>
</div>
<div class="card">
<div class="title">LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</div>
<div class="meta-line">Authors: Said Taghadouini, Adrien Cavaillès, Baptiste Aubertin</div>
<div class="meta-line">First: 2026-01-20T18:58:32+00:00 · Latest: 2026-01-20T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14251v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LightOnOCR：一款10亿参数端到端多语言视觉语言模型，实现最先进OCR性能</div>
<div class="mono" style="margin-top:8px">我们推出\textbf{LightOnOCR-2-1B}，这是一个拥有10亿参数的端到端多语言视觉语言模型，能够直接将文档图像（如PDF）转换为整洁、自然排序的文本，无需依赖脆弱的OCR流程。该模型通过大规模高质量蒸馏混合数据训练，广泛涵盖扫描文档、法语文档和科学PDF，在OlmOCR-Bench上取得最先进成果，同时体积比先前最佳模型缩小9倍且速度显著提升。我们进一步扩展输出格式以预测嵌入图像的归一化边界框，通过恢复策略在预训练阶段引入定位能力，并利用基于IoU奖励的RLVR进行优化。最后，我们通过检查点平均和任务算术融合提升模型鲁棒性。模型检查点按Apache 2.0协议发布，数据集及\textbf{LightOnOCR-bbox-bench}评估工具将依据各自许可公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to overcome the brittleness of traditional OCR pipelines by developing an end-to-end multilingual vision-language model that directly extracts clean, naturally ordered text from document images. The method introduces LightOnOCR-2-1B, a 1-billion-parameter model trained on a large-scale, high-quality distillation dataset with strong coverage of scans, French documents, and scientific PDFs; it extends output to predict normalized bounding boxes for embedded images through a resume strategy for localization pretraining and RLVR refinement with IoU-based rewards, while also employing checkpoint averaging and task-arithmetic merging for robustness. Key experimental results show that the model achieves state-of-the-art performance on OlmOCR-Bench while being 9 times smaller and substantially faster than prior best models, and it is released alongside a new evaluation benchmark for bounding box prediction.</div>
<div class="mono" style="margin-top:8px">该研究旨在克服传统OCR流程的脆弱性，通过开发一个端到端的多语言视觉-语言模型，直接从PDF等文档图像中提取干净、自然排序的文本。方法上提出了LightOnOCR-2-1B，这是一个10亿参数的模型，基于大规模高质量蒸馏数据集训练，涵盖扫描件、法语文档和科学PDF；模型扩展输出以预测嵌入图像的归一化边界框，通过恢复策略进行定位预训练，并使用基于IoU奖励的RLVR进行细化，同时采用检查点平均和任务算术合并来提升鲁棒性。主要实验结果表明，该模型在OlmOCR-Bench上取得了最先进的性能，同时比先前最佳模型小9倍且速度显著更快，并进一步引入了用于边界框预测的新评估基准LightOnOCR-bbox-bench。</div>
</details>
</div>
<div class="card">
<div class="title">IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models</div>
<div class="meta-line">Authors: Liang Shi, Wei Li, Kevin M Beussman, Lin Chen, Yun Fu</div>
<div class="meta-line">First: 2026-01-20T17:45:24+00:00 · Latest: 2026-01-20T17:45:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14188v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instance-level recognition (ILR) concerns distinguishing individual instances from one another, with person re-identification as a prominent example. Despite the impressive visual perception capabilities of modern VLMs, we find their performance on ILR unsatisfactory, often dramatically underperforming domain-specific ILR models. This limitation hinders many practical application of VLMs, e.g. where recognizing familiar people and objects is crucial for effective visual understanding. Existing solutions typically learn to recognize instances one at a time using instance-specific datasets, which not only incur substantial data collection and training costs but also struggle with fine-grained discrimination. In this work, we propose IIR-VLM, a VLM enhanced for In-context Instance-level Recognition. We integrate pre-trained ILR expert models as auxiliary visual encoders to provide specialized features for learning diverse instances, which enables VLMs to learn new instances in-context in a one-shot manner. Further, IIR-VLM leverages this knowledge for instance-aware visual understanding. We validate IIR-VLM&#x27;s efficacy on existing instance personalization benchmarks. Finally, we demonstrate its superior ILR performance on a challenging new benchmark, which assesses ILR capabilities across varying difficulty and diverse categories, with person, face, pet and general objects as the instances at task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IIR-VLM：面向大型视觉语言模型的上下文实例级识别</div>
<div class="mono" style="margin-top:8px">实例级识别（ILR）关注于区分不同个体实例，行人重识别是其典型应用。尽管现代视觉语言模型（VLM）具备出色的视觉感知能力，但其在ILR任务上的表现不尽如人意，常显著落后于领域专用ILR模型。这一局限阻碍了VLM在许多实际场景中的应用，例如在需要识别熟悉人物或物体以实现有效视觉理解的场景中。现有方案通常依赖实例专用数据集逐个学习识别实例，不仅需承担高昂的数据收集与训练成本，且在细粒度区分上存在困难。本研究提出IIR-VLM——一种增强上下文实例级识别能力的VLM。我们集成预训练的ILR专家模型作为辅助视觉编码器，为学习多样化实例提供专业化特征，使VLM能够以单样本方式在上下文中学习新实例。进一步，IIR-VLM利用该知识实现实例感知的视觉理解。我们在现有实例个性化基准测试中验证了IIR-VLM的有效性，并最终在涵盖人员、人脸、宠物及通用物体等多类别、多难度层级的挑战性新基准上，展示了其卓越的ILR性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the unsatisfactory performance of modern vision-language models (VLMs) on instance-level recognition (ILR) tasks, such as person re-identification, where they significantly underperform domain-specific models, limiting practical applications requiring fine-grained visual understanding. The proposed method, IIR-VLM, enhances VLMs by integrating pre-trained ILR expert models as auxiliary visual encoders to provide specialized features, enabling the model to learn new instances in-context in a one-shot manner and leverage this knowledge for instance-aware understanding. Experimental validation on existing personalization benchmarks and a new challenging benchmark covering diverse categories like persons, faces, pets, and objects demonstrates IIR-VLM&#x27;s superior ILR performance across varying difficulty levels.</div>
<div class="mono" style="margin-top:8px">本研究动机源于现代视觉-语言模型（VLM）在实例级识别（ILR）任务（如行人重识别）上表现不佳，显著落后于领域专用模型，这限制了需要细粒度视觉理解的实际应用。所提出的IIR-VLM方法通过集成预训练的ILR专家模型作为辅助视觉编码器来提供专业化特征，从而增强VLM，使得模型能够以单样本上下文学习的方式学习新实例，并利用此知识进行实例感知理解。在现有个性化基准和一个涵盖人物、人脸、宠物和物体等多样类别的新挑战性基准上的实验验证表明，IIR-VLM在不同难度级别上均表现出优越的ILR性能。</div>
</details>
</div>
<div class="card">
<div class="title">TwinBrainVLA: Unleashing the Potential of Generalist VLMs for Embodied Tasks via Asymmetric Mixture-of-Transformers</div>
<div class="meta-line">Authors: Bin Yu, Shijie Lian, Xiaopeng Lin, Yuliang Wei, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Xinming Wang, Bailing Wang, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-20T16:30:07+00:00 · Latest: 2026-01-20T16:30:07+00:00</div>
<div class="meta-line">Comments: GitHub: https://github.com/ZGC-EmbodyAI/TwinBrainVLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14133v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14133v1">PDF</a> · <a href="https://github.com/ZGC-EmbodyAI/TwinBrainVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard Vision-Language-Action (VLA) models typically fine-tune a monolithic Vision-Language Model (VLM) backbone explicitly for robotic control. However, this approach creates a critical tension between maintaining high-level general semantic understanding and learning low-level, fine-grained sensorimotor skills, often leading to &quot;catastrophic forgetting&quot; of the model&#x27;s open-world capabilities. To resolve this conflict, we introduce TwinBrainVLA, a novel architecture that coordinates a generalist VLM retaining universal semantic understanding and a specialist VLM dedicated to embodied proprioception for joint robotic control. TwinBrainVLA synergizes a frozen &quot;Left Brain&quot;, which retains robust general visual reasoning, with a trainable &quot;Right Brain&quot;, specialized for embodied perception, via a novel Asymmetric Mixture-of-Transformers (AsyMoT) mechanism. This design allows the Right Brain to dynamically query semantic knowledge from the frozen Left Brain and fuse it with proprioceptive states, providing rich conditioning for a Flow-Matching Action Expert to generate precise continuous controls. Extensive experiments on SimplerEnv and RoboCasa benchmarks demonstrate that TwinBrainVLA achieves superior manipulation performance compared to state-of-the-art baselines while explicitly preserving the comprehensive visual understanding capabilities of the pre-trained VLM, offering a promising direction for building general-purpose robots that simultaneously achieve high-level semantic understanding and low-level physical dexterity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TwinBrainVLA：通过非对称混合变换器释放通用视觉语言模型在具身任务中的潜力</div>
<div class="mono" style="margin-top:8px">标准视觉-语言-动作模型通常通过微调单一视觉语言模型主干来适配机器人控制，但这种方法在保持高层语义理解与学习低层精细感知运动技能之间存在矛盾，常导致模型开放世界能力的灾难性遗忘。为解决此问题，我们提出TwinBrainVLA——一种协调通用视觉语言模型与专用具身本体感知模型的双脑架构。该架构通过新型非对称混合变换器机制，将保持通用视觉推理能力的冻结“左脑”与专攻具身感知的可训练“右脑”相协同，使右脑能动态查询左脑的语义知识并与本体状态融合，为流匹配动作专家生成精确连续控制提供丰富条件。在SimplerEnv和RoboCasa基准测试中，TwinBrainVLA在保持预训练模型完整视觉理解能力的同时，实现了优于现有方法的操作性能，为构建兼具高层语义理解与低层物理操控能力的通用机器人提供了新方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Standard Vision-Language-Action models face a conflict between preserving high-level semantic understanding from pre-training and learning low-level robotic control, often leading to catastrophic forgetting. To resolve this, TwinBrainVLA introduces an asymmetric architecture coordinating a frozen generalist VLM (the &quot;Left Brain&quot;) for semantic knowledge and a trainable specialist VLM (the &quot;Right Brain&quot;) for embodied proprioception, linked via an Asymmetric Mixture-of-Transformers mechanism that allows dynamic querying and fusion of information to condition a Flow-Matching Action Expert. Experiments on SimplerEnv and RoboCasa benchmarks show the model achieves state-of-the-art manipulation performance while explicitly preserving the pre-trained VLM&#x27;s comprehensive visual understanding.</div>
<div class="mono" style="margin-top:8px">为解决视觉-语言-动作模型中保持通用语义理解与学习细粒度感知运动技能之间的冲突（常导致灾难性遗忘），本研究提出了TwinBrainVLA。该方法采用一种非对称架构，协调一个冻结的通用视觉语言模型（“左脑”）用于语义知识，和一个可训练的专用视觉语言模型（“右脑”）用于具身本体感知，两者通过非对称混合Transformer机制连接，从而动态查询并融合语义与本体感知信息，以指导一个流匹配动作专家生成控制。在SimplerEnv和RoboCasa基准上的实验结果表明，TwinBrainVLA相比最先进的基线方法实现了更优的操作性能，同时明确保留了预训练视觉语言模型的全面视觉理解能力。</div>
</details>
</div>
<div class="card">
<div class="title">DermaBench: A Clinician-Annotated Benchmark Dataset for Dermatology Visual Question Answering and Reasoning</div>
<div class="meta-line">Authors: Abdurrahim Yilmaz, Ozan Erdem, Ece Gokyayla, Ayda Acar, Burc Bugra Dagtas, Dilara Ilhan Erdil, Gulsum Gencoglan, Burak Temelkuran</div>
<div class="meta-line">First: 2026-01-20T15:44:57+00:00 · Latest: 2026-01-20T15:44:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14084v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14084v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) are increasingly important in medical applications; however, their evaluation in dermatology remains limited by datasets that focus primarily on image-level classification tasks such as lesion recognition. While valuable for recognition, such datasets cannot assess the full visual understanding, language grounding, and clinical reasoning capabilities of multimodal models. Visual question answering (VQA) benchmarks are required to evaluate how models interpret dermatological images, reason over fine-grained morphology, and generate clinically meaningful descriptions. We introduce DermaBench, a clinician-annotated dermatology VQA benchmark built on the Diverse Dermatology Images (DDI) dataset. DermaBench comprises 656 clinical images from 570 unique patients spanning Fitzpatrick skin types I-VI. Using a hierarchical annotation schema with 22 main questions (single-choice, multi-choice, and open-ended), expert dermatologists annotated each image for diagnosis, anatomic site, lesion morphology, distribution, surface features, color, and image quality, together with open-ended narrative descriptions and summaries, yielding approximately 14.474 VQA-style annotations. DermaBench is released as a metadata-only dataset to respect upstream licensing and is publicly available at Harvard Dataverse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DermaBench：面向皮肤病学视觉问答与推理的临床专家标注基准数据集</div>
<div class="mono" style="margin-top:8px">视觉语言模型在医疗应用中日渐重要，但其在皮肤病学领域的评估仍受限于主要关注图像级分类任务（如皮损识别）的数据集。此类数据集虽对识别任务有价值，却无法全面评估多模态模型的视觉理解、语言基础与临床推理能力。需通过视觉问答基准来评估模型如何解读皮肤病学图像、对细粒度形态特征进行推理，并生成具有临床意义的描述。我们基于多样化皮肤病图像数据集构建了临床专家标注的皮肤病学视觉问答基准DermaBench。该数据集涵盖570名独特患者的656张临床图像，覆盖Fitzpatrick皮肤分型I-VI级。通过包含22个主要问题（单选、多选及开放式）的分层标注框架，皮肤科专家对每张图像的诊断、解剖部位、皮损形态、分布、表面特征、颜色及图像质量进行标注，同时提供开放式叙述性描述与总结，共产生约14,474条视觉问答式标注。为遵循上游许可协议，DermaBench以纯元数据形式发布，可通过哈佛Dataverse公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limited evaluation of vision-language models in dermatology, where existing datasets focus on image classification and fail to assess comprehensive visual understanding and clinical reasoning. To enable rigorous assessment, the authors introduce DermaBench, a clinician-annotated visual question answering benchmark built on the Diverse Dermatology Images dataset, featuring 656 clinical images annotated by experts using a hierarchical schema with 22 main question types covering diagnosis, morphology, and narrative descriptions. The resulting dataset contains approximately 14,474 VQA-style annotations across diverse skin types, providing a metadata-only resource for evaluating multimodal reasoning in dermatology.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决皮肤病学中视觉语言模型评估局限于图像分类的问题，现有数据集无法全面评估视觉理解和临床推理能力。方法上，基于多样化皮肤病图像数据集，构建了临床医生标注的视觉问答基准DermaBench，采用包含22个主要问题类型的分层标注模式，以捕捉诊断、形态学等临床细节。主要实验成果是数据集包含来自570名患者的656张图像，覆盖菲茨帕特里克皮肤类型I-VI，产生了约14,474个标注，可用于评估细粒度推理和描述生成能力。</div>
</details>
</div>
<div class="card">
<div class="title">Weather-R1: Logically Consistent Reinforcement Fine-Tuning for Multimodal Reasoning in Meteorology</div>
<div class="meta-line">Authors: Kaiyu Wu, Pucheng Han, Hualong Zhang, Naigeng Wu, Keze Wang</div>
<div class="meta-line">First: 2026-01-20T15:00:15+00:00 · Latest: 2026-01-20T15:00:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14044v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14044v1">PDF</a> · <a href="https://github.com/Marcowky/Weather-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision Language Models (VLMs) show advancing reasoning capabilities, their application in meteorology is constrained by a domain gap and a reasoning faithfulness gap. Specifically, mainstream Reinforcement Fine-Tuning (RFT) can induce Self-Contradictory Reasoning (Self-Contra), where the model&#x27;s reasoning contradicts its final answer, which is unacceptable in such a high-stakes domain. To address these challenges, we construct WeatherQA, a novel multimodal reasoning benchmark in meteorology. We also propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), which resolves Self-Contra by introducing a logical consistency reward. Furthermore, we introduce Weather-R1, the first reasoning VLM with logical faithfulness in meteorology, to the best of our knowledge. Experiments demonstrate that Weather-R1 improves performance on WeatherQA by 9.8 percentage points over the baseline, outperforming Supervised Fine-Tuning and RFT, and even surpassing the original Qwen2.5-VL-32B. These results highlight the effectiveness of our LoCo-RFT and the superiority of Weather-R1. Our benchmark and code are available at https://github.com/Marcowky/Weather-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Weather-R1：面向气象学多模态推理的逻辑一致性强化微调</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）展现出日益增强的推理能力，但其在气象学领域的应用仍受限于领域鸿沟与推理可信度鸿沟。具体而言，主流强化微调（RFT）可能引发自相矛盾推理（Self-Contra），即模型的推理过程与其最终答案相矛盾，这在气象等高风险领域是不可接受的。为应对这些挑战，我们构建了WeatherQA——一个新颖的气象学多模态推理基准数据集，并提出逻辑一致性强化微调（LoCo-RFT），通过引入逻辑一致性奖励机制解决自相矛盾问题。此外，我们推出了Weather-R1，据我们所知，这是首个具备逻辑可信度的气象学推理视觉语言模型。实验表明，Weather-R1在WeatherQA基准上的性能较基线提升9.8个百分点，优于监督微调和传统RFT方法，甚至超越了原始Qwen2.5-VL-32B模型。这些结果凸显了LoCo-RFT方法的有效性及Weather-R1模型的优越性。我们的基准数据集与代码已开源：https://github.com/Marcowky/Weather-R1。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses two critical gaps in applying Vision Language Models (VLMs) to meteorology: a domain gap and a reasoning faithfulness gap, where standard Reinforcement Fine-Tuning (RFT) can lead to Self-Contradictory Reasoning (Self-Contra). To tackle this, the authors first construct a new multimodal reasoning benchmark called WeatherQA and propose Logically Consistent Reinforcement Fine-Tuning (LoCo-RFT), a method that introduces a logical consistency reward to resolve contradictions. The resulting model, Weather-R1, demonstrates a 9.8 percentage point improvement over the baseline on WeatherQA, outperforming both Supervised Fine-Tuning and standard RFT, and even surpassing the original Qwen2.5-VL-32B model, validating the effectiveness of the proposed approach.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉语言模型在气象学应用中存在的领域差距和推理忠实度差距，特别是标准强化微调引发的自相矛盾推理问题，旨在提升其可靠性。方法包括构建WeatherQA基准数据集，并提出逻辑一致强化微调，通过引入逻辑一致性奖励来确保推理的忠实性。实验结果表明，所开发的Weather-R1模型在WeatherQA上的性能比基线提高了9.8个百分点，优于监督微调和标准强化微调，甚至超过了原始的Qwen2.5-VL-32B模型。</div>
</details>
</div>
<div class="card">
<div class="title">CityCube: Benchmarking Cross-view Spatial Reasoning on Vision-Language Models in Urban Environments</div>
<div class="meta-line">Authors: Haotian Xu, Yue Hu, Zhengqiu Zhu, Chen Gao, Ziyou Wang, Junreng Rao, Wenhao Lu, Weishi Li, Quanjun Yin, Yong Li</div>
<div class="meta-line">First: 2026-01-20T13:44:02+00:00 · Latest: 2026-01-20T13:44:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14339v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-view spatial reasoning is essential for embodied AI, underpinning spatial understanding, mental simulation and planning in complex environments. Existing benchmarks primarily emphasize indoor or street settings, overlooking the unique challenges of open-ended urban spaces characterized by rich semantics, complex geometries, and view variations. To address this, we introduce CityCube, a systematic benchmark designed to probe cross-view reasoning capabilities of current VLMs in urban settings. CityCube integrates four viewpoint dynamics to mimic camera movements and spans a wide spectrum of perspectives from multiple platforms, e.g., vehicles, drones and satellites. For a comprehensive assessment, it features 5,022 meticulously annotated multi-view QA pairs categorized into five cognitive dimensions and three spatial relation expressions. A comprehensive evaluation of 33 VLMs reveals a significant performance disparity with humans: even large-scale models struggle to exceed 54.1% accuracy, remaining 34.2% below human performance. By contrast, small-scale fine-tuned VLMs achieve over 60.0% accuracy, highlighting the necessity of our benchmark. Further analyses indicate the task correlations and fundamental cognitive disparity between VLMs and human-like reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CityCube：城市环境中视觉语言模型跨视角空间推理能力基准测试</div>
<div class="mono" style="margin-top:8px">跨视角空间推理对具身智能至关重要，支撑着复杂环境中的空间理解、心理模拟与规划。现有基准主要关注室内或街道场景，忽视了开放城市空间特有的挑战——丰富的语义、复杂的几何结构及视角变化。为此，我们提出CityCube，一个系统性基准测试，旨在探究当前视觉语言模型在城市环境中的跨视角推理能力。CityCube整合了四种视角动态以模拟相机运动，并涵盖车辆、无人机、卫星等多平台的广泛视角。该基准包含5,022个精细标注的多视角问答对，划分为五个认知维度与三种空间关系表达形式，用于全面评估。对33个视觉语言模型的综合测试显示，其与人类表现存在显著差距：即使大规模模型准确率最高仅达54.1%，仍低于人类水平34.2%。相比之下，经微调的小规模模型准确率超过60.0%，凸显了本基准的必要性。进一步分析揭示了任务间的关联性，以及视觉语言模型与类人推理间的根本认知差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of benchmarks for cross-view spatial reasoning in complex, open-ended urban environments, which are characterized by rich semantics and varied geometries. The authors introduce CityCube, a systematic benchmark comprising 5,022 annotated multi-view question-answer pairs from diverse platforms like vehicles and drones, categorized into five cognitive dimensions and three spatial relation expressions to evaluate vision-language models (VLMs). Experimental evaluation of 33 VLMs reveals a significant gap: even large-scale models achieve only up to 54.1% accuracy, falling 34.2% short of human performance, while small-scale fine-tuned models can exceed 60.0%, underscoring the benchmark&#x27;s utility and highlighting fundamental cognitive disparities between model and human reasoning.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于评估开放城市环境中的跨视角空间推理能力，现有基准主要关注室内或街道场景，忽视了城市空间特有的丰富语义、复杂几何和视角变化的挑战。方法上提出了CityCube，这是一个系统性基准，包含5,022个标注的多视角问答对，整合了四种视角动态以模拟车辆、无人机和卫星等平台的相机运动，并分为五个认知维度和三种空间关系表达。通过对33个视觉语言模型的综合评估，主要实验结果表明存在显著的性能差距：大规模模型准确率难以超过54.1%，比人类表现低34.2%，而小规模微调模型则能达到60.0%以上的准确率，这凸显了该基准的必要性，并揭示了任务相关性以及模型与人类推理之间的根本认知差异。</div>
</details>
</div>
<div class="card">
<div class="title">HyperWalker: Dynamic Hypergraph-Based Deep Diagnosis for Multi-Hop Clinical Modeling across EHR and X-Ray in Medical VLMs</div>
<div class="meta-line">Authors: Yuezhe Yang, Hao Wang, Yige Peng, Jinman Kim, Lei Bi</div>
<div class="meta-line">First: 2026-01-20T12:48:09+00:00 · Latest: 2026-01-20T12:48:09+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13919v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13919v1">PDF</a> · <a href="https://github.com/Bean-Young/HyperWalker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated clinical diagnosis remains a core challenge in medical AI, which usually requires models to integrate multi-modal data and reason across complex, case-specific contexts. Although recent methods have advanced medical report generation (MRG) and visual question answering (VQA) with medical vision-language models (VLMs), these methods, however, predominantly operate under a sample-isolated inference paradigm, as such processing cases independently without access to longitudinal electronic health records (EHRs) or structurally related patient examples. This paradigm limits reasoning to image-derived information alone, which ignores external complementary medical evidence for potentially more accurate diagnosis. To overcome this limitation, we propose \textbf{HyperWalker}, a \textit{Deep Diagnosis} framework that reformulates clinical reasoning via dynamic hypergraphs and test-time training. First, we construct a dynamic hypergraph, termed \textbf{iBrochure}, to model the structural heterogeneity of EHR data and implicit high-order associations among multimodal clinical information. Within this hypergraph, a reinforcement learning agent, \textbf{Walker}, navigates to and identifies optimal diagnostic paths. To ensure comprehensive coverage of diverse clinical characteristics in test samples, we incorporate a \textit{linger mechanism}, a multi-hop orthogonal retrieval strategy that iteratively selects clinically complementary neighborhood cases reflecting distinct clinical attributes. Experiments on MRG with MIMIC and medical VQA on EHRXQA demonstrate that HyperWalker achieves state-of-the-art performance. Code is available at: https://github.com/Bean-Young/HyperWalker</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HyperWalker：基于动态超图的深度诊断框架，用于跨电子健康记录与X射线的医学视觉语言模型多跳临床建模</div>
<div class="mono" style="margin-top:8px">自动化临床诊断仍是医疗AI的核心挑战，通常需要模型整合多模态数据并在复杂的病例特定情境中进行推理。尽管近期方法通过医学视觉语言模型（VLMs）推进了医学报告生成（MRG）和视觉问答（VQA），但这些方法主要采用样本隔离的推理范式，即独立处理病例而无法访问纵向电子健康记录（EHR）或结构相关的患者案例。该范式将推理局限于图像衍生信息，忽略了外部互补医学证据对提升诊断准确性的潜力。为突破此限制，我们提出**HyperWalker**——一种通过动态超图和测试时训练重构临床推理的**深度诊断**框架。首先，我们构建名为**iBrochure**的动态超图，以建模EHR数据的结构异质性与多模态临床信息间的高阶隐式关联。在该超图中，强化学习智能体**Walker**通过导航识别最优诊断路径。为确保测试样本中多样临床特征的全面覆盖，我们引入**徘徊机制**——一种多跳正交检索策略，通过迭代选择反映不同临床属性的互补邻域病例。在MIMIC数据集上的MRG实验及EHRXQA的医学VQA实验表明，HyperWalker实现了最先进的性能。代码发布于：https://github.com/Bean-Young/HyperWalker</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automated clinical diagnosis requires integrating multimodal data and reasoning across complex contexts, but existing medical vision-language models often process cases independently without leveraging longitudinal electronic health records or related patient examples, limiting diagnostic accuracy. To address this, HyperWalker introduces a deep diagnosis framework that constructs a dynamic hypergraph called iBrochure to model heterogeneous EHR data and high-order associations, then uses a reinforcement learning agent named Walker to navigate and identify optimal diagnostic paths, enhanced by a linger mechanism for multi-hop orthogonal retrieval of clinically complementary cases. Experiments on medical report generation with MIMIC and visual question answering on EHRXQA show that HyperWalker achieves state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">自动化临床诊断通常需要整合多模态数据并在复杂情境中进行推理，但现有的医学视觉语言模型通常孤立处理病例，未能利用纵向电子健康记录（EHR）或相关患者示例，从而限制了诊断准确性。为解决这一问题，HyperWalker提出了一个深度诊断框架，该框架构建了一个动态超图iBrochure来建模异构的EHR数据和高阶关联，并采用强化学习智能体Walker来导航并识别最优诊断路径；同时，它引入了徘徊机制，通过多跳正交检索迭代选择具有临床互补性的邻域病例。在MIMIC上的医学报告生成和EHRXQA上的医学视觉问答实验表明，HyperWalker实现了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting Multi-Task Visual Representation Learning</div>
<div class="meta-line">Authors: Shangzhe Di, Zhonghua Zhai, Weidi Xie</div>
<div class="meta-line">First: 2026-01-20T11:59:19+00:00 · Latest: 2026-01-20T11:59:19+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Becomebright/MTV</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13886v1">PDF</a> · <a href="https://github.com/Becomebright/MTV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current visual representation learning remains bifurcated: vision-language models (e.g., CLIP) excel at global semantic alignment but lack spatial precision, while self-supervised methods (e.g., MAE, DINO) capture intricate local structures yet struggle with high-level semantic context. We argue that these paradigms are fundamentally complementary and can be integrated into a principled multi-task framework, further enhanced by dense spatial supervision. We introduce MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives. To mitigate the need for manual annotations, we leverage high-capacity &quot;expert&quot; models -- such as Depth Anything V2 and OWLv2 -- to synthesize dense, structured pseudo-labels at scale. Beyond the framework, we provide a systematic investigation into the mechanics of multi-task visual learning, analyzing: (i) the marginal gain of each objective, (ii) task synergies versus interference, and (iii) scaling behavior across varying data and model scales. Our results demonstrate that MTV achieves &quot;best-of-both-worlds&quot; performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding. Our findings suggest that multi-task learning, fueled by high-quality pseudo-supervision, is a scalable path toward more general visual encoders.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重访多任务视觉表征学习</div>
<div class="mono" style="margin-top:8px">当前视觉表征学习仍呈两极分化：视觉语言模型（如CLIP）擅长全局语义对齐但缺乏空间精度，而自监督方法（如MAE、DINO）能捕捉精细局部结构却难以理解高层语义语境。我们认为这些范式本质互补，可通过密集空间监督整合为原则性多任务框架。本文提出MTV——一种多任务视觉预训练框架，通过视觉语言对比、自监督和密集空间目标联合优化共享主干网络。为减少人工标注需求，我们利用高性能“专家”模型（如Depth Anything V2和OWLv2）大规模合成结构化密集伪标签。除框架外，我们系统探究了多任务视觉学习机制，分析：（1）各目标的边际增益，（2）任务协同与干扰效应，（3）不同数据与模型规模下的扩展规律。实验表明MTV实现“两全其美”性能，在保持全局语义理解的同时显著提升细粒度空间推理能力。研究证明：基于高质量伪监督的多任务学习是构建通用视觉编码器的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the complementary limitations of current visual representation learning paradigms: vision-language models like CLIP capture global semantics but lack spatial precision, while self-supervised methods like MAE capture local structures but struggle with high-level context. The method introduces MTV, a multi-task visual pretraining framework that jointly optimizes a shared backbone across vision-language contrastive, self-supervised, and dense spatial objectives, using high-capacity expert models to generate dense pseudo-labels and avoid manual annotation. Key experimental findings show that MTV achieves best-of-both-worlds performance, significantly enhancing fine-grained spatial reasoning without compromising global semantic understanding, and the systematic analysis reveals the marginal gains, synergies, and scaling behavior of the integrated objectives.</div>
<div class="mono" style="margin-top:8px">本研究针对当前视觉表示学习的分化问题：视觉-语言模型（如CLIP）擅长全局语义对齐但缺乏空间精度，而自监督方法（如MAE）能捕捉局部细节却难以理解高级语义。作者提出MTV，一个多任务视觉预训练框架，通过联合优化共享主干网络，结合视觉-语言对比、自监督和密集空间目标，并利用高性能专家模型生成密集伪标签以避免人工标注。实验结果表明，MTV实现了“两全其美”的性能，在显著提升细粒度空间推理能力的同时不损害全局语义理解，系统分析进一步揭示了各目标的边际收益、任务协同效应及在不同数据与模型规模下的扩展规律。</div>
</details>
</div>
<div class="card">
<div class="title">DisasterVQA: A Visual Question Answering Benchmark Dataset for Disaster Scenes</div>
<div class="meta-line">Authors: Aisha Al-Mohannadi, Ayisha Firoz, Yin Yang, Muhammad Imran, Ferda Ofli</div>
<div class="meta-line">First: 2026-01-20T10:50:46+00:00 · Latest: 2026-01-20T10:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13839v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.13839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Social media imagery provides a low-latency source of situational information during natural and human-induced disasters, enabling rapid damage assessment and response. While Visual Question Answering (VQA) has shown strong performance in general-purpose domains, its suitability for the complex and safety-critical reasoning required in disaster response remains unclear. We introduce DisasterVQA, a benchmark dataset designed for perception and reasoning in crisis contexts. DisasterVQA consists of 1,395 real-world images and 4,405 expert-curated question-answer pairs spanning diverse events such as floods, wildfires, and earthquakes. Grounded in humanitarian frameworks including FEMA ESF and OCHA MIRA, the dataset includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making tasks. We benchmark seven state-of-the-art vision-language models and find performance variability across question types, disaster categories, regions, and humanitarian tasks. Although models achieve high accuracy on binary questions, they struggle with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios. DisasterVQA provides a challenging and practical benchmark to guide the development of more robust and operationally meaningful vision-language models for disaster response. The dataset is publicly available at https://zenodo.org/records/18267770.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DisasterVQA：面向灾害场景的视觉问答基准数据集</div>
<div class="mono" style="margin-top:8px">社交媒体图像为自然灾害和人为灾害期间提供了低延迟的态势信息源，支持快速损害评估与应急响应。尽管视觉问答（VQA）在通用领域表现出色，但其对灾害响应所需的复杂安全关键型推理的适用性仍不明确。本文提出DisasterVQA——一个专为危机情境感知与推理设计的基准数据集。该数据集包含1,395张真实灾害图像及4,405组专家标注的问答对，涵盖洪水、野火、地震等多种灾害事件。基于FEMA ESF和OCHA MIRA等人道主义框架，数据集包含二元选择、多项选择及开放式问题，覆盖态势感知与操作决策任务。我们对七种前沿视觉语言模型进行基准测试，发现模型在不同问题类型、灾害类别、地域及人道任务中表现存在差异。虽然模型在二元问题上准确率较高，但在细粒度定量推理、物体计数和情境敏感解释方面表现欠佳，尤其对代表性不足的灾害场景。DisasterVQA为开发更鲁棒且具操作意义的灾害响应视觉语言模型提供了具有挑战性的实用基准。数据集已公开于https://zenodo.org/records/18267770。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To evaluate the suitability of Visual Question Answering (VQA) for the complex reasoning required in disaster response, this work introduces DisasterVQA, a benchmark dataset of 1,395 real-world disaster images and 4,405 expert-curated question-answer pairs grounded in humanitarian frameworks. The method involves benchmarking seven state-of-the-art vision-language models on this dataset, which includes binary, multiple-choice, and open-ended questions covering situational awareness and operational decision-making. Key experimental findings reveal that while models achieve high accuracy on binary questions, they struggle significantly with fine-grained quantitative reasoning, object counting, and context-sensitive interpretation, particularly for underrepresented disaster scenarios, highlighting a performance gap for practical application.</div>
<div class="mono" style="margin-top:8px">为评估视觉问答（VQA）技术在灾害响应所需复杂推理任务中的适用性，本研究引入了DisasterVQA基准数据集，包含1,395张真实灾害场景图像和4,405个基于人道主义框架（如FEMA ESF和OCHA MIRA）构建的专家标注问答对。研究方法是对七种先进的视觉语言模型在该数据集上进行基准测试，数据集涵盖二元选择、多项选择和开放式问题，涉及态势感知和行动决策任务。主要实验结果表明，模型在二元问题上准确率较高，但在细粒度定量推理、物体计数和上下文敏感解释方面表现不佳，尤其对代表性不足的灾害类型，这揭示了现有模型在实际应用中的性能局限。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
