<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-19 06:24</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260119_0624</div>
    <div class="row"><div class="card">
<div class="title">See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection</div>
<div class="meta-line">Authors: Amir Mallak, Erfan Aasi, Shiva Sreeram, Tsun-Hsuan Wang, Daniela Rus, Alaa Maalouf</div>
<div class="meta-line">First: 2026-01-15T18:58:33+00:00 · Latest: 2026-01-15T18:58:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少看多开：基于基础模型随机补丁选择的端到端自动驾驶泛化方法</div>
<div class="mono" style="margin-top:8px">端到端自动驾驶的最新进展表明，基于基础模型提取的补丁对齐特征训练的策略在分布外泛化方面表现更优。我们假设，由于自注意力机制，每个补丁特征都以不同方式和强度隐式嵌入了所有其他补丁的信息，导致这些描述符高度冗余。我们通过主成分分析和跨补丁相似性量化了此类（BLIP2）特征的冗余度：90%的方差可由17/64个主成分捕获，且强跨令牌相关性普遍存在。基于此类重叠信息训练会导致策略过拟合虚假相关性，损害分布外鲁棒性。我们提出随机补丁选择方法——一种简单有效的策略学习框架，能提升模型的鲁棒性、泛化能力和效率。SPS对每帧图像随机掩蔽部分补丁描述符（不输入策略模型），同时保持剩余补丁的空间布局。策略因此获得同一场景的不同随机但完整的视图：每个随机补丁子集都构成对世界的不同但合理的连贯投影。策略决策将基于对特定令牌存活性保持不变的特性。大量实验证实，在所有分布外场景中，我们的方法均优于当前最优技术：平均提升6.2%，闭环仿真中最高提升20.4%，同时推理速度提升2.4倍。我们通过9个系统的掩蔽率与补丁特征重组消融实验（其中8个超越先前最优技术）验证了方法的有效性。最后，我们证明学习到的策略无需调整即可迁移至真实物理车辆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the issue of feature redundancy in foundation model-based autonomous driving, which leads to overfitting and poor out-of-distribution (OOD) generalization, this paper introduces Stochastic Patch Selection (SPS). The method randomly masks a fraction of patch descriptors at each frame while preserving spatial layout, forcing the policy to rely on invariant features from different stochastic views of the same scene. Experiments demonstrate that SPS outperforms prior state-of-the-art methods across all OOD scenarios, achieving an average 6.2% improvement and up to 20.4% in closed-loop simulation, while being 2.4 times faster, and the learned policy successfully transfers to a real-world car without fine-tuning.</div>
<div class="mono" style="margin-top:8px">针对基于基础模型的自动驾驶中特征冗余导致过拟合和分布外泛化能力差的问题，本文提出了随机补丁选择方法。该方法在每帧随机掩蔽一部分补丁描述符并保持空间布局，迫使策略从随机但连贯的场景视图中学习并关注不变特征。实验表明，该方法在分布外场景中优于现有技术，平均性能提升6.2%，闭环仿真中最高提升20.4%，同时速度提高2.4倍，且学习到的策略无需调整即可成功迁移到真实物理车辆上。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Euclidean Distance Field Properties for Fast and Safe 3D planning with a modified Lazy Theta*</div>
<div class="meta-line">Authors: Jose A. Cobano, L. Merino, F. Caballero</div>
<div class="meta-line">Venue: Published in Robotics and Autonomous Systems (RAS), 2025</div>
<div class="meta-line">First: 2025-05-29T21:51:02+00:00 · Latest: 2026-01-15T17:24:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24024v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.24024v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the FS-Planner, a fast graph-search planner based on a modified Lazy Theta* algorithm that exploits the analytical properties of Euclidean Distance Fields (EDFs). We introduce a new cost function that integrates an EDF-based term proven to satisfy the triangle inequality, enabling efficient parent selection and reducing computation time while generating safe paths with smaller heading variations. We also derive an analytic approximation of the EDF integral along a segment and analyze the influence of the line-of-sight limit on the approximation error, motivating the use of a bounded visibility range. Furthermore, we propose a gradient-based neighbour-selection mechanism that decreases the number of explored nodes and improves computational performance without degrading safety or path quality. The FS-Planner produces safe paths with small heading changes without requiring the use of post-processing methods. Extensive experiments and comparisons in challenging 3D indoor simulation environments, complemented by tests in real-world outdoor environments, are used to evaluate and validate the FS-Planner. The results show consistent improvements in computation time, exploration efficiency, safety, and smoothness in a geometric sense compared with baseline heuristic planners, while maintaining sub-optimality within acceptable bounds. Finally, the proposed EDF-based cost formulation is orthogonal to the underlying search method and can be incorporated into other planning paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用欧几里得距离场特性实现基于改进Lazy Theta*算法的快速安全三维规划</div>
<div class="mono" style="margin-top:8px">本文提出FS-Planner，这是一种基于改进Lazy Theta*算法的快速图搜索规划器，通过利用欧几里得距离场的解析特性实现高效规划。我们设计了一种新型成本函数，其中整合了满足三角不等式的EDF项，从而在生成航向变化较小的安全路径时，能高效选择父节点并减少计算时间。同时推导了线段上EDF积分的解析近似方法，分析了视线限制对近似误差的影响，论证了有限可视范围的应用价值。此外，提出基于梯度的邻域选择机制，在保证路径安全性与质量的前提下减少探索节点数量、提升计算性能。FS-Planner无需后处理即可生成航向变化平缓的安全路径。通过在复杂三维室内仿真环境的大量实验对比，以及真实室外场景测试，验证了FS-Planner相较基准启发式规划器在计算时间、探索效率、安全性与几何平滑度方面的持续改进，同时将次优性控制在可接受范围内。最后，所提出的EDF成本函数框架与底层搜索方法正交，可扩展应用于其他规划范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the need for fast and safe 3D path planning by developing a graph-search planner that leverages the analytical properties of Euclidean Distance Fields (EDFs). The proposed FS-Planner modifies the Lazy Theta* algorithm with a new EDF-based cost function proven to satisfy the triangle inequality, an analytic approximation for the EDF integral along a segment, and a gradient-based neighbor-selection mechanism to reduce node exploration. Experimental evaluation in simulated 3D indoor and real-world outdoor environments demonstrates that the planner achieves consistent improvements in computation time, exploration efficiency, path safety, and geometric smoothness compared to baseline methods, while maintaining acceptable sub-optimality bounds.</div>
<div class="mono" style="margin-top:8px">本研究针对快速安全的3D路径规划需求，开发了一种高效利用欧几里得距离场特性的方法。提出的FS-Planner通过改进Lazy Theta*算法，引入了一种被证明满足三角不等式的EDF成本函数以实现高效父节点选择，并结合基于梯度的邻居选择机制来减少节点探索。在3D室内仿真和真实室外环境中的实验评估表明，与基线方法相比，该规划器在计算时间、探索效率、安全性和路径平滑度方面均取得持续改进，同时在不需后处理的情况下保持了可接受的次优性。</div>
</details>
</div>
<div class="card">
<div class="title">RGS-SLAM: Robust Gaussian Splatting SLAM with One-Shot Dense Initialization</div>
<div class="meta-line">Authors: Wei-Tse Cheng, Yen-Jen Chiou, Yuan-Fu Yang</div>
<div class="meta-line">First: 2025-12-28T03:45:57+00:00 · Latest: 2026-01-15T15:14:21+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00705v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.00705v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://breeze1124.github.io/rgs-slam-project-page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce RGS-SLAM, a robust Gaussian-splatting SLAM framework that replaces the residual-driven densification stage of GS-SLAM with a training-free correspondence-to-Gaussian initialization. Instead of progressively adding Gaussians as residuals reveal missing geometry, RGS-SLAM performs a one-shot triangulation of dense multi-view correspondences derived from DINOv3 descriptors refined through a confidence-aware inlier classifier, generating a well-distributed and structure-aware Gaussian seed prior to optimization. This initialization stabilizes early mapping and accelerates convergence by roughly 20\%, yielding higher rendering fidelity in texture-rich and cluttered scenes while remaining fully compatible with existing GS-SLAM pipelines. Evaluated on the TUM RGB-D and Replica datasets, RGS-SLAM achieves competitive or superior localization and reconstruction accuracy compared with state-of-the-art Gaussian and point-based SLAM systems, sustaining real-time mapping performance at up to 925 FPS. Additional details and resources are available at this URL: https://breeze1124.github.io/rgs-slam-project-page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RGS-SLAM：基于一次性密集初始化的鲁棒高斯溅射SLAM</div>
<div class="mono" style="margin-top:8px">本文提出RGS-SLAM，一种鲁棒的高斯溅射SLAM框架，通过免训练的对应点-高斯初始化替代了GS-SLAM中残差驱动的致密化阶段。该方法摒弃了依赖残差逐步添加高斯粒子的传统方式，转而利用经置信度感知内点分类器优化的DINOv3描述符，从密集多视角对应点中一次性完成三角化，在优化前生成分布均匀且结构感知的高斯种子。该初始化策略提升了早期建图的稳定性，收敛速度加快约20%，在纹理丰富和杂乱场景中实现更高渲染保真度，同时完全兼容现有GS-SLAM流程。在TUM RGB-D和Replica数据集上的评估表明，RGS-SLAM在定位与重建精度上达到或超越了当前最先进的高斯及点云SLAM系统，最高可维持925 FPS的实时建图性能。更多细节与资源详见：https://breeze1124.github.io/rgs-slam-project-page/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the instability and slow convergence in Gaussian Splatting SLAM systems caused by progressive densification. The proposed RGS-SLAM framework replaces residual-driven densification with a training-free, one-shot dense initialization method that triangulates dense multi-view correspondences derived from DINOv3 descriptors, refined through a confidence-aware inlier classifier to create a structure-aware Gaussian seed prior to optimization. Experimental results on TUM RGB-D and Replica datasets demonstrate that this approach stabilizes early mapping, accelerates convergence by approximately 20%, achieves competitive or superior localization and reconstruction accuracy compared to state-of-the-art systems, and maintains real-time performance up to 925 FPS while improving rendering fidelity in texture-rich and cluttered scenes.</div>
<div class="mono" style="margin-top:8px">本研究针对高斯泼溅SLAM系统的不稳定性和收敛速度慢的问题，提出了RGS-SLAM框架，用一次性、无需训练的高密度初始化方法取代了传统的残差驱动致密化过程。其核心方法是通过DINOv3描述子生成密集多视角对应点，利用置信度感知的内点分类器进行精炼，并通过一次性三角测量在优化前创建分布良好的高斯种子地图。在TUM RGB-D和Replica数据集上的实验结果表明，该方法稳定了早期建图，加速收敛约20%，在定位和重建精度上达到或超越了当前最先进的高斯与点云SLAM系统，同时保持高达925 FPS的实时性能，并在纹理丰富和杂乱场景中提高了渲染保真度。</div>
</details>
</div>
<div class="card">
<div class="title">SurgGoal: Rethinking Surgical Planning Evaluation via Goal-Satisfiability</div>
<div class="meta-line">Authors: Ruochen Li, Kun Yuan, Yufei Xia, Yue Zhou, Qingyu Lu, Weihang Li, Youxiang Zhu, Nassir Navab</div>
<div class="meta-line">First: 2026-01-15T14:47:26+00:00 · Latest: 2026-01-15T14:47:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10455v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10455v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Surgical planning integrates visual perception, long-horizon reasoning, and procedural knowledge, yet it remains unclear whether current evaluation protocols reliably assess vision-language models (VLMs) in safety-critical settings. Motivated by a goal-oriented view of surgical planning, we define planning correctness via phase-goal satisfiability, where plan validity is determined by expert-defined surgical rules. Based on this definition, we introduce a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans containing order and content errors. Using this benchmark, we show that sequence similarity metrics systematically misjudge planning quality, penalizing valid plans while failing to identify invalid ones. We therefore adopt a rule-based goal-satisfiability metric as a high-precision meta-evaluation reference to assess Video-LLMs under progressively constrained settings, revealing failures due to perception errors and under-constrained reasoning. Structural knowledge consistently improves performance, whereas semantic guidance alone is unreliable and benefits larger models only when combined with structural constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SurgGoal：基于目标可满足性的手术规划评估方法重构</div>
<div class="mono" style="margin-top:8px">手术规划融合了视觉感知、长程推理与流程知识，但现有评估方案能否在安全关键场景中可靠评估视觉语言模型仍不明确。本研究从目标导向视角出发，通过阶段目标可满足性定义规划正确性，即规划有效性由专家定义的手术规则判定。基于此定义，我们构建了包含有效流程变体及含顺序/内容错误无效方案的多中心元评估基准。实验表明，序列相似度指标系统性误判规划质量：既惩罚有效方案，又无法识别无效方案。因此，我们采用基于规则的目标可满足性指标作为高精度元评估基准，在渐进约束条件下评估视频大语言模型，揭示了感知错误与欠约束推理导致的失效案例。结构化知识能持续提升性能，而纯语义引导不可靠，仅当与结构化约束结合时才对更大模型产生增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for reliable evaluation of vision-language models in safety-critical surgical planning, this research redefines planning correctness through phase-goal satisfiability based on expert-defined surgical rules. The method introduces a multicentric meta-evaluation benchmark with valid procedural variations and invalid plans, then uses a rule-based goal-satisfiability metric as a high-precision reference to assess Video-LLMs under constrained settings. Key findings show that sequence similarity metrics systematically misjudge planning quality, while the proposed evaluation reveals model failures due to perception errors and under-constrained reasoning, with structural knowledge consistently improving performance whereas semantic guidance alone proves unreliable.</div>
<div class="mono" style="margin-top:8px">本研究出于在安全关键的手术规划中可靠评估视觉语言模型的需求，通过基于专家定义手术规则的阶段目标可满足性重新定义了规划正确性。该方法引入了一个包含有效程序变体和带有顺序与内容错误的无效计划的多中心元评估基准，并采用基于规则的目标可满足性指标作为高精度参考，在逐步受限的设置下评估视频大语言模型。关键实验结果表明，序列相似性指标会系统性地误判规划质量，而所提出的评估揭示了模型因感知错误和约束不足的推理而失败，其中结构化知识能持续提升性能，而单独的语义指导则不可靠。</div>
</details>
</div>
<div class="card">
<div class="title">Sampling-Based Constrained Motion Planning with Products of Experts</div>
<div class="meta-line">Authors: Amirreza Razmjoo, Teng Xue, Suhan Shetty, Sylvain Calinon</div>
<div class="meta-line">First: 2024-12-23T10:39:59+00:00 · Latest: 2026-01-15T14:31:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.17462v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.17462v2">PDF</a> · <a href="https://github.com/idiap/smpc_poe">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach to enhance the performance of sampling-based Model Predictive Control (MPC) in constrained optimization by leveraging products of experts. Our methodology divides the main problem into two components: one focused on optimality and the other on feasibility. By combining the solutions from each component, represented as distributions, we apply products of experts to implement a project-then-sample strategy. In this strategy, the optimality distribution is projected into the feasible area, allowing for more efficient sampling. This approach contrasts with the traditional sample-then-project and naive sample-then-reject method, leading to more diverse exploration and reducing the accumulation of samples on the boundaries. We demonstrate an effective implementation of this principle using a tensor train-based distribution model, which is characterized by its non-parametric nature, ease of combination with other distributions at the task level, and straightforward sampling technique. We adapt existing tensor train models to suit this purpose and validate the efficacy of our approach through experiments in various tasks, including obstacle avoidance, non-prehensile manipulation, and tasks involving staying in a restricted volume. Our experimental results demonstrate that the proposed method consistently outperforms known baselines, providing strong empirical support for its effectiveness. Sample codes for this project are available at https://github.com/idiap/smpc_poe.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于采样的约束运动规划与专家乘积法</div>
<div class="mono" style="margin-top:8px">本文提出一种新方法，通过利用专家乘积提升基于采样的模型预测控制在约束优化中的性能。该方法将主问题分解为两个部分：一部分关注最优性，另一部分关注可行性。通过结合表示为分布的各个组件解，我们应用专家乘积实施“先投影后采样”策略。在此策略中，最优性分布被投影至可行区域，从而实现更高效的采样。与传统“先采样后投影”及朴素“先采样后拒绝”方法相比，该方法能促进更广泛的探索并减少边界样本积累。我们使用基于张量链的分布模型有效实现了这一原理，该模型具有非参数特性、易于在任务层面与其他分布结合，且采样技术简便。我们调整现有张量链模型以适应此目的，并通过避障、非抓取操作及受限空间停留等多项任务的实验验证了方法的有效性。实验结果表明，所提方法持续优于已知基线，为其效能提供了有力实证支持。项目示例代码发布于 https://github.com/idiap/smpc_poe。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To improve the performance of sampling-based Model Predictive Control (MPC) in constrained optimization, this work introduces a method that leverages products of experts to separate optimality and feasibility concerns. The approach employs a project-then-sample strategy, where an optimality distribution is projected into the feasible region before sampling, using a tensor train-based non-parametric model for efficient combination and sampling. Experimental validation across tasks such as obstacle avoidance and non-prehensile manipulation shows that the method outperforms traditional sample-then-project and rejection sampling baselines by enabling more diverse exploration and reducing boundary sample accumulation.</div>
<div class="mono" style="margin-top:8px">为提高基于采样的模型预测控制在约束优化中的性能，本研究提出了一种利用专家乘积来分离最优性与可行性问题的方法。该方法采用“先投影后采样”策略，将最优性分布投影至可行区域后再进行采样，并使用基于张量链的非参数模型实现高效组合与采样。在避障、非抓取操作等任务上的实验验证表明，该方法通过促进更多样化的探索并减少边界样本积累，性能优于传统的先采样后投影及简单拒绝采样基线。</div>
</details>
</div>
<div class="card">
<div class="title">Singularity-Free Guiding Vector Field over Bézier&#x27;s Curves Applied to Rovers Path Planning and Path Following</div>
<div class="meta-line">Authors: Alfredo González-Calvin, Lía García-Pérez, Juan Jiménez</div>
<div class="meta-line">Venue: Journal of Field Robotics, 2025. 42:2720-27-39</div>
<div class="meta-line">First: 2024-12-17T15:56:40+00:00 · Latest: 2026-01-15T13:47:35+00:00</div>
<div class="meta-line">Comments: Final version, accepted for publication. 26 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.13033v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.13033v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a guidance algorithm for solving the problem of following parametric paths, as well as a curvature-varying speed setpoint for land-based car-type wheeled mobile robots (WMRs). The guidance algorithm relies on Singularity-Free Guiding Vector Fields SF-GVF. This novel GVF approach expands the desired robot path and the Guiding vector field to a higher dimensional space, in which an angular control function can be found to ensure global asymptotic convergence to the desired parametric path while avoiding field singularities. In SF-GVF, paths should follow a parametric definition. This feature makes using Bezier&#x27;s curves attractive to define the robot&#x27;s desired patch. The curvature-varying speed setpoint, combined with the guidance algorithm, eases the convergence to the path when physical restrictions exist, such as minimal turning radius or maximal lateral acceleration. We provide theoretical results, simulations, and outdoor experiments using a WMR platform assembled with off-the-shelf components.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝塞尔曲线的无奇异性引导矢量场在漫游车路径规划与路径跟踪中的应用</div>
<div class="mono" style="margin-top:8px">本文提出一种用于解决参数化路径跟踪问题的引导算法，以及适用于陆地轮式移动机器人的曲率变化速度设定点。该引导算法基于无奇异性引导矢量场。这种新型GVF方法将期望机器人路径和引导矢量场扩展至高维空间，通过角控制函数确保对目标参数化路径的全局渐近收敛，同时避免场奇异性。在SF-GVF中，路径需遵循参数化定义，这使得采用贝塞尔曲线定义机器人期望路径具有优势。曲率变化速度设定点与引导算法相结合，可在存在最小转弯半径或最大横向加速度等物理约束时，促进路径收敛。我们通过理论分析、仿真实验以及采用现成组件组装的WMR平台户外实验验证了该方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the path-following problem for car-like wheeled mobile robots by developing a guidance algorithm that ensures global convergence to parametric paths without singularities. The method introduces a Singularity-Free Guiding Vector Field (SF-GVF), which embeds the desired path and vector field into a higher-dimensional space to define an angular control function, enabling singularity avoidance; Bézier curves are utilized for parametric path representation, and a curvature-varying speed setpoint is integrated to accommodate physical constraints like minimal turning radius. Experimental validation through simulations and outdoor tests with an off-the-shelf robot platform demonstrates effective path following and convergence under real-world restrictions.</div>
<div class="mono" style="margin-top:8px">本研究针对轮式移动机器人的路径跟踪问题，提出了一种确保全局收敛且避免奇点的引导算法。该方法采用无奇点引导向量场（SF-GVF），将路径和向量场提升到高维空间，通过角控制函数实现渐近收敛而无奇异性，并利用贝塞尔曲线定义参数化路径，结合曲率变化的速度设定点以适应最小转弯半径和最大横向加速度等物理限制。通过仿真和户外实验验证，基于现成组件搭建的机器人平台在真实约束下实现了有效的路径跟踪。</div>
</details>
</div>
<div class="card">
<div class="title">Bootstrap Off-policy with World Model</div>
<div class="meta-line">Authors: Guojian Zhan, Likun Wang, Xiangteng Zhang, Jiaxin Gao, Masayoshi Tomizuka, Shengbo Eben Li</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-01T06:33:04+00:00 · Latest: 2026-01-15T13:38:44+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00423v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00423v3">PDF</a> · <a href="https://github.com/molumitu/BOOM_MBRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy&#x27;s actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner&#x27;s non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner&#x27;s action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于世界模型的引导式离轨策略</div>
<div class="mono" style="margin-top:8px">在线规划在强化学习中已被证明能有效提升样本效率和最终性能。然而，将规划用于环境交互不可避免地导致收集数据与策略实际行为之间的偏差，从而损害模型学习和策略改进。为此，我们提出BOOM（基于世界模型的引导式离轨策略框架），该框架通过引导循环紧密整合规划与离轨学习：策略初始化规划器，规划器通过行为对齐优化动作以引导策略更新。该循环由联合学习的世界模型支持，使规划器能模拟未来轨迹并提供价值目标以促进策略改进。BOOM的核心是无似然对齐损失函数，它利用规划器的非参数化动作分布引导策略，并结合软价值加权机制——该机制优先考虑高回报行为，并缓解回放缓冲区中规划器动作质量的波动性。在DeepMind Control Suite和Humanoid-Bench等高维环境上的实验表明，BOOM在训练稳定性和最终性能方面均达到最先进水平。代码发布于https://github.com/molumitu/BOOM_MBRL。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the performance degradation in reinforcement learning when using online planning for environment interaction, which creates a mismatch between collected data and actual policy behaviors. The proposed BOOM framework integrates planning and off-policy learning through a bootstrap loop where the policy initializes the planner and the planner refines actions to align behaviors, supported by a jointly learned world model for trajectory simulation and value targets. Experiments on DeepMind Control Suite and Humanoid-Bench demonstrate that BOOM achieves state-of-the-art results in training stability and final performance.</div>
<div class="mono" style="margin-top:8px">强化学习中的在线规划能提高样本效率，但会导致收集数据与策略行为之间的不匹配，损害模型学习和策略改进。为解决此问题，研究者提出了BOOM框架，通过一个引导循环紧密集成规划与离线策略学习：策略初始化规划器，规划器则通过行为对齐来优化动作以引导策略，并辅以一个联合学习的世界模型用于轨迹模拟和价值目标。其核心是一个无似然对齐损失，利用规划器的非参数动作分布来引导策略，并结合软价值加权机制，优先考虑高回报行为并减少回放缓冲区中动作质量的波动。在DeepMind Control Suite和Humanoid-Bench上的实验表明，BOOM在训练稳定性和最终性能方面均达到了最先进水平。</div>
</details>
</div>
<div class="card">
<div class="title">Online identification of nonlinear time-varying systems with uncertain information</div>
<div class="meta-line">Authors: He Ren, Gaowei Yan, Hang Liu, Lifeng Cao, Zhijun Zhao, Gang Dang</div>
<div class="meta-line">First: 2026-01-15T13:33:48+00:00 · Latest: 2026-01-15T13:33:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10379v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10379v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital twins (DTs), serving as the core enablers for real-time monitoring and predictive maintenance of complex cyber-physical systems, impose critical requirements on their virtual models: high predictive accuracy, strong interpretability, and online adaptive capability. However, existing techniques struggle to meet these demands simultaneously: Bayesian methods excel in uncertainty quantification but lack model interpretability, while interpretable symbolic identification methods (e.g., SINDy) are constrained by their offline, batch-processing nature, which make real-time updates challenging. To bridge this semantic and computational gap, this paper proposes a novel Bayesian Regression-based Symbolic Learning (BRSL) framework. The framework formulates online symbolic discovery as a unified probabilistic state-space model. By incorporating sparse horseshoe priors, model selection is transformed into a Bayesian inference task, enabling simultaneous system identification and uncertainty quantification. Furthermore, we derive an online recursive algorithm with a forgetting factor and establish precise recursive conditions that guarantee the well-posedness of the posterior distribution. These conditions also function as real-time monitors for data utility, enhancing algorithmic robustness. Additionally, a rigorous convergence analysis is provided, demonstrating the convergence of parameter estimates under persistent excitation conditions. Case studies validate the effectiveness of the proposed framework in achieving interpretable, probabilistic prediction and online learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不确定信息下非线性时变系统的在线辨识</div>
<div class="mono" style="margin-top:8px">数字孪生作为复杂信息物理系统实时监测与预测性维护的核心使能技术，对其虚拟模型提出了关键要求：高预测精度、强可解释性与在线自适应能力。然而，现有技术难以同时满足这些需求：贝叶斯方法擅长不确定性量化但缺乏模型可解释性，而可解释的符号辨识方法（如SINDy）受限于其离线批处理特性，难以实现实时更新。为弥合这一语义与计算鸿沟，本文提出一种基于贝叶斯回归的符号学习新框架。该框架将在线符号发现构建为统一的概率状态空间模型，通过引入稀疏马蹄铁先验，将模型选择转化为贝叶斯推断任务，实现系统辨识与不确定性量化的同步处理。进一步，我们推导出带遗忘因子的在线递归算法，并建立保证后验分布适定性的精确递归条件，这些条件同时可作为数据效用的实时监测器以增强算法鲁棒性。此外，严格的收敛性分析证明了参数估计在持续激励条件下的收敛性。案例研究验证了所提框架在实现可解释概率预测与在线学习方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of developing digital twin models that simultaneously achieve high predictive accuracy, strong interpretability, and online adaptability, as existing methods like Bayesian inference or symbolic regression (e.g., SINDy) typically excel in only one aspect. The proposed solution is a Bayesian Regression-based Symbolic Learning (BRSL) framework, which formulates online symbolic discovery as a probabilistic state-space model using sparse horseshoe priors to enable simultaneous system identification and uncertainty quantification, and derives an online recursive algorithm with a forgetting factor and real-time data utility monitoring. Experimental case studies demonstrate the framework&#x27;s effectiveness in achieving interpretable, probabilistic prediction and successful online learning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决数字孪生模型开发中同时实现高预测精度、强可解释性和在线自适应能力的挑战，因为现有方法如贝叶斯技术或符号识别（例如SINDy）无法满足所有这些需求。提出的解决方案是一种基于贝叶斯回归的符号学习（BRSL）框架，该框架将在线符号发现表述为概率状态空间模型，利用稀疏马蹄铁先验实现系统识别和不确定性量化的同步进行，并推导出带有遗忘因子和实时数据效用监测器的在线递归算法。案例研究验证了该框架成功实现了可解释的概率预测和在线学习，并在持续激励条件下保证了参数估计的收敛性。</div>
</details>
</div>
<div class="card">
<div class="title">UrbanNav: Learning Language-Guided Urban Navigation from Web-Scale Human Trajectories</div>
<div class="meta-line">Authors: Yanghong Mei, Yirong Yang, Longteng Guo, Qunbo Wang, Ming-Ming Yu, Xingjian He, Wenjun Wu, Jing Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-10T12:54:04+00:00 · Latest: 2026-01-15T13:22:05+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures, accepted to AAAI 2026. Project page:https://github.com/CASIA-IVA-Lab/UrbanNav</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09607v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09607v2">PDF</a> · <a href="https://github.com/CASIA-IVA-Lab/UrbanNav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Navigating complex urban environments using natural language instructions poses significant challenges for embodied agents, including noisy language instructions, ambiguous spatial references, diverse landmarks, and dynamic street scenes. Current visual navigation methods are typically limited to simulated or off-street environments, and often rely on precise goal formats, such as specific coordinates or images. This limits their effectiveness for autonomous agents like last-mile delivery robots navigating unfamiliar cities. To address these limitations, we introduce UrbanNav, a scalable framework that trains embodied agents to follow free-form language instructions in diverse urban settings. Leveraging web-scale city walking videos, we develop an scalable annotation pipeline that aligns human navigation trajectories with language instructions grounded in real-world landmarks. UrbanNav encompasses over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets, capturing a wide range of urban scenarios. Our model learns robust navigation policies to tackle complex urban scenarios, demonstrating superior spatial reasoning, robustness to noisy instructions, and generalization to unseen urban settings. Experimental results show that UrbanNav significantly outperforms existing methods, highlighting the potential of large-scale web video data to enable language-guided, real-world urban navigation for embodied agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UrbanNav：基于网络规模人类轨迹的语言引导城市导航学习</div>
<div class="mono" style="margin-top:8px">利用自然语言指令在复杂城市环境中导航对具身智能体提出了重大挑战，包括噪声语言指令、模糊空间参照、多样化地标和动态街景。现有视觉导航方法通常局限于模拟或非街道环境，且依赖精确目标格式（如特定坐标或图像），限制了其在陌生城市执行最后一公里配送的自主智能体中的实用性。为此，我们提出UrbanNav——一个可扩展框架，通过训练具身智能体在多样化城市场景中遵循自由形式语言指令。借助网络规模的城市步行视频，我们开发了可扩展的标注流程，将人类导航轨迹与基于真实世界地标的语言指令对齐。UrbanNav包含超过1500小时的导航数据和300万条指令-轨迹-地标三元组，覆盖广泛城市场景。我们的模型通过学习鲁棒的导航策略处理复杂城市情境，展现出卓越的空间推理能力、对噪声指令的鲁棒性以及对未见城市场景的泛化能力。实验结果表明UrbanNav显著优于现有方法，凸显了大规模网络视频数据在实现具身智能体语言引导现实城市导航方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling embodied agents to navigate complex urban environments using free-form natural language instructions, which is hindered by noisy instructions, ambiguous references, and dynamic scenes. The authors introduce UrbanNav, a framework that trains agents by leveraging web-scale city walking videos through a scalable annotation pipeline to align human trajectories with language instructions grounded in real-world landmarks, resulting in a dataset of over 1,500 hours of navigation data and 3 million instruction-trajectory-landmark triplets. Experiments demonstrate that the model learns robust navigation policies, showing superior spatial reasoning, robustness to instruction noise, and generalization to unseen urban settings, significantly outperforming existing methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决具身智能体在复杂城市环境中使用自由形式自然语言指令进行导航的挑战，该挑战受限于嘈杂的语言指令、模糊的空间参照，以及现有方法对精确目标格式和模拟环境的依赖。提出的UrbanNav框架通过利用网络规模的城市步行视频，开发了一个可扩展的标注流程，将人类导航轨迹与基于真实世界地标的语言指令对齐，从而构建了一个包含超过1500小时导航数据和300万个指令-轨迹-地标三元组的数据集。实验结果表明，该模型学习到了鲁棒的导航策略，在空间推理、对噪声指令的鲁棒性以及向未见城市环境的泛化能力方面表现优异，显著超越了现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">FastStair: Learning to Run Up Stairs with Humanoid Robots</div>
<div class="meta-line">Authors: Yan Liu, Tao Yu, Haolin Song, Hongbo Zhu, Nianzong Hu, Yuzhi Hao, Xiuyong Yao, Xizhe Zang, Hua Chen, Jie Zhao</div>
<div class="meta-line">First: 2026-01-15T13:14:59+00:00 · Latest: 2026-01-15T13:14:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10365v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10365v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Running up stairs is effortless for humans but remains extremely challenging for humanoid robots due to the simultaneous requirements of high agility and strict stability. Model-free reinforcement learning (RL) can generate dynamic locomotion, yet implicit stability rewards and heavy reliance on task-specific reward shaping tend to result in unsafe behaviors, especially on stairs; conversely, model-based foothold planners encode contact feasibility and stability structure, but enforcing their hard constraints often induces conservative motion that limits speed. We present FastStair, a planner-guided, multi-stage learning framework that reconciles these complementary strengths to achieve fast and stable stair ascent. FastStair integrates a parallel model-based foothold planner into the RL training loop to bias exploration toward dynamically feasible contacts and to pretrain a safety-focused base policy. To mitigate planner-induced conservatism and the discrepancy between low- and high-speed action distributions, the base policy was fine-tuned into speed-specialized experts and then integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across the full commanded-speed range. We deploy the resulting controller on the Oli humanoid robot, achieving stable stair ascent at commanded speeds up to 1.65 m/s and traversing a 33-step spiral staircase (17 cm rise per step) in 12 s, demonstrating robust high-speed performance on long staircases. Notably, the proposed approach served as the champion solution in the Canton Tower Robot Run Up Competition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FastStair：人形机器人学习跑楼梯</div>
<div class="mono" style="margin-top:8px">跑楼梯对人类而言轻而易举，但对人形机器人却极具挑战，因其需同时满足高敏捷性与严格稳定性。无模型强化学习可生成动态步态，但隐式稳定性奖励及对任务特定奖励设计的重度依赖易导致不安全行为（尤其在楼梯上）；反之，基于模型的落脚点规划器虽能编码接触可行性与稳定性结构，但其硬约束常引发限制速度的保守运动。本文提出FastStair——一个规划器引导的多阶段学习框架，通过融合二者互补优势实现快速稳定的上楼梯运动。该框架将并行模型落脚点规划器集成至强化学习训练循环，以引导探索朝向动态可行的接触，并预训练注重安全的基础策略。为缓解规划器导致的保守性及高低速动作分布差异，基础策略被微调为速度专精专家，再通过低秩自适应技术整合，实现全指令速度范围内的平滑操作。我们将最终控制器部署于Oli人形机器人，在指令速度达1.65 m/s时仍能稳定上楼梯，仅用12秒穿越33级螺旋楼梯（每级高17厘米），在长楼梯场景中展现了鲁棒的高速性能。值得注意的是，本方法已成为广州塔机器人登塔比赛的冠军解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Running up stairs is challenging for humanoid robots because it demands both agility and stability, which are often in tension: model-free reinforcement learning can produce dynamic motion but risks unsafe behaviors, while model-based planners ensure stability but lead to overly conservative and slow gaits. To address this, the authors introduce FastStair, a multi-stage learning framework that integrates a parallel model-based foothold planner into the RL training loop to guide exploration toward feasible contacts and pretrain a safety-focused base policy; this policy is then fine-tuned into speed-specialized experts and integrated via Low-Rank Adaptation (LoRA) to enable smooth operation across a wide speed range. Experimental deployment on the Oli humanoid robot achieved stable stair ascent at commanded speeds up to 1.65 m/s, traversing a 33-step spiral staircase in 12 seconds, and the approach won the Canton Tower Robot Run Up Competition, demonstrating robust high-speed performance on long staircases.</div>
<div class="mono" style="margin-top:8px">人形机器人上楼梯极具挑战性，因为它同时要求高敏捷性和严格稳定性，而模型自由强化学习能生成动态运动但可能导致不安全行为，基于模型的立足点规划器虽能确保稳定性却往往过于保守且速度受限。为此，FastStair提出一个规划器引导的多阶段学习框架，将并行模型立足点规划器集成到强化学习训练中，以引导探索朝向可行接触并预训练一个注重安全的基础策略；该基础策略随后被微调为速度专精专家，并通过低秩自适应（LoRA）进行整合，从而实现整个指令速度范围内的平滑操作。在Oli人形机器人上的实验部署实现了高达1.65米/秒的稳定上楼梯速度，在12秒内穿越了33级螺旋楼梯（每级高17厘米），并在广州塔机器人登楼比赛中夺冠，展示了在长楼梯上稳健的高速性能。</div>
</details>
</div>
<div class="card">
<div class="title">CHORAL: Traversal-Aware Planning for Safe and Efficient Heterogeneous Multi-Robot Routing</div>
<div class="meta-line">Authors: David Morilla-Cabello, Eduardo Montijano</div>
<div class="meta-line">First: 2026-01-15T12:34:22+00:00 · Latest: 2026-01-15T12:34:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10340v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monitoring large, unknown, and complex environments with autonomous robots poses significant navigation challenges, where deploying teams of heterogeneous robots with complementary capabilities can substantially improve both mission performance and feasibility. However, effectively modeling how different robotic platforms interact with the environment requires rich, semantic scene understanding. Despite this, existing approaches often assume homogeneous robot teams or focus on discrete task compatibility rather than continuous routing. Consequently, scene understanding is not fully integrated into routing decisions, limiting their ability to adapt to the environment and to leverage each robot&#x27;s strengths. In this paper, we propose an integrated semantic-aware framework for coordinating heterogeneous robots. Starting from a reconnaissance flight, we build a metric-semantic map using open-vocabulary vision models and use it to identify regions requiring closer inspection and capability-aware paths for each platform to reach them. These are then incorporated into a heterogeneous vehicle routing formulation that jointly assigns inspection tasks and computes robot trajectories. Experiments in simulation and in a real inspection mission with three robotic platforms demonstrate the effectiveness of our approach in planning safer and more efficient routes by explicitly accounting for each platform&#x27;s navigation capabilities. We release our framework, CHORAL, as open source to support reproducibility and deployment of diverse robot teams.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHORAL：面向安全高效异构多机器人路径规划的遍历感知规划方法</div>
<div class="mono" style="margin-top:8px">利用自主机器人监测广阔、未知且复杂的环境面临显著的导航挑战，部署具有互补能力的异构机器人团队可大幅提升任务执行效能与可行性。然而，有效建模不同机器人平台与环境交互需要丰富的语义场景理解。现有方法常假设机器人团队同质化，或仅关注离散任务兼容性而非连续路径规划，导致场景理解未能充分融入路径决策，限制了系统适应环境及发挥各机器人优势的能力。本文提出一种集成语义感知的异构机器人协同框架：通过侦察飞行构建基于开放词汇视觉模型的度量-语义地图，识别需精细探查的区域，并为各平台规划能力感知路径；进而将其融入异构车辆路径规划模型，联合分配巡检任务并计算机器人轨迹。在仿真及包含三种机器人平台的实际巡检任务中，实验证明该方法通过显式考量各平台导航能力，能规划出更安全高效的路径。我们将框架CHORAL开源发布，以支持异构机器人团队的可复现研究与应用部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing multi-robot routing methods that often assume homogeneous teams or lack integrated scene understanding, this research introduces CHORAL, a framework for safe and efficient heterogeneous multi-robot routing. The method begins with a reconnaissance flight to build a metric-semantic map using open-vocabulary vision models, which identifies regions for closer inspection and generates capability-aware paths for different robotic platforms; these elements are then integrated into a heterogeneous vehicle routing formulation that jointly assigns tasks and computes trajectories. Experimental results from both simulation and a real-world inspection mission with three robotic platforms demonstrate that the approach successfully plans safer and more efficient routes by explicitly accounting for each platform&#x27;s navigation capabilities.</div>
<div class="mono" style="margin-top:8px">为安全高效地监测复杂环境，本研究针对现有多机器人路径规划方法通常假设团队同质或缺乏集成场景理解的局限性，提出了CHORAL框架。该方法首先利用开放词汇视觉模型从侦察飞行中构建度量-语义地图，识别需要检查的区域，并为异构平台生成能力感知路径。这些元素随后被整合到一个异构车辆路径规划公式中，共同分配检查任务并计算轨迹。在仿真和包含三个机器人平台的真实检查任务中的实验结果表明，该方法通过显式考虑每个机器人的导航能力，规划出了更安全、更高效的路线。</div>
</details>
</div>
<div class="card">
<div class="title">Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics</div>
<div class="meta-line">Authors: Dongyoung Kim, Sumin Park, Huiwon Jang, Jinwoo Shin, Jaehyung Kim, Younggyo Seo</div>
<div class="meta-line">First: 2025-05-29T16:41:12+00:00 · Latest: 2026-01-15T11:24:14+00:00</div>
<div class="meta-line">Comments: 29 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.00070v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.00070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have recently shown great promise in advancing robotics by combining embodied reasoning with robot control. A common approach involves training on embodied reasoning tasks related to robot control using Supervised Fine-Tuning (SFT). However, SFT datasets are often heuristically constructed and not explicitly optimized for improving robot control. Furthermore, SFT often leads to issues such as catastrophic forgetting and reduced generalization performance. To address these limitations, we introduce Robot-R1, a novel framework that leverages reinforcement learning to enhance embodied reasoning specifically for robot control. Robot-R1 learns to predict the next keypoint state required for task completion, conditioned on the current scene image and environment metadata derived from expert demonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples reasoning-based responses and reinforces those that lead to more accurate predictions. To rigorously evaluate Robot-R1, we also introduce a new benchmark that demands the diverse embodied reasoning capabilities for the task. Our experiments show that models trained with Robot-R1 outperform SFT methods on embodied reasoning tasks. Despite having only 7B parameters, Robot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action control, such as spatial and movement reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Robot-R1：基于强化学习的机器人具身推理增强框架</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（LVLMs）近期通过结合具身推理与机器人控制，在推动机器人技术发展方面展现出巨大潜力。常见方法采用监督微调（SFT）对机器人控制相关的具身推理任务进行训练，但SFT数据集通常基于启发式构建，未针对机器人控制优化，且易导致灾难性遗忘与泛化性能下降。为突破这些局限，我们提出Robot-R1——一种利用强化学习专门增强机器人控制具身推理能力的新型框架。该框架基于专家示范的当前场景图像与环境元数据，学习预测任务完成所需的下一个关键点状态。受DeepSeek-R1学习方法启发，Robot-R1对基于推理的响应进行采样，并强化那些能产生更准确预测的响应。为系统评估Robot-R1，我们同时提出了一个需要多样化具身推理能力的新基准测试。实验表明，采用Robot-R1训练的模型在具身推理任务上优于SFT方法。尽管仅拥有70亿参数，Robot-R1在空间与运动推理等底层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of Supervised Fine-Tuning (SFT) for training Large Vision-Language Models (LVLMs) on embodied reasoning tasks, as SFT datasets are not optimized for robot control and can cause catastrophic forgetting and poor generalization. The proposed method, Robot-R1, is a reinforcement learning framework that learns to predict the next keypoint state for task completion from current scene images and environment metadata, sampling and reinforcing reasoning-based responses that lead to accurate predictions. Experimental results on a new benchmark show that Robot-R1 outperforms SFT methods on embodied reasoning tasks and, despite having only 7B parameters, surpasses GPT-4o in low-level action control reasoning such as spatial and movement reasoning.</div>
<div class="mono" style="margin-top:8px">为解决监督微调（SFT）在机器人控制中存在的启发式数据集构建、灾难性遗忘和泛化性能下降等局限，本文提出了Robot-R1框架，利用强化学习来增强具身推理能力。该方法基于专家演示的当前场景图像和环境元数据来预测下一个关键点状态，并通过强化那些能带来更准确预测的推理响应进行学习。在新基准上的实验结果表明，Robot-R1在具身推理任务上优于SFT方法，并且尽管仅有70亿参数，其在空间和运动推理等低层动作控制相关任务上甚至超越了GPT-4o。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Quadrotor Control From Visual Features Using Differentiable Simulation</div>
<div class="meta-line">Authors: Johannes Heeg, Yunlong Song, Davide Scaramuzza</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2024-10-21T13:06:06+00:00 · Latest: 2026-01-15T10:42:31+00:00</div>
<div class="meta-line">Comments: Accepted for presentation at the IEEE International Conference on Robotics and Automation (ICRA) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.15979v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.15979v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The sample inefficiency of reinforcement learning (RL) remains a significant challenge in robotics. RL requires large-scale simulation and can still cause long training times, slowing research and innovation. This issue is particularly pronounced in vision-based control tasks where reliable state estimates are not accessible. Differentiable simulation offers an alternative by enabling gradient back-propagation through the dynamics model, providing low-variance analytical policy gradients and, hence, higher sample efficiency. However, its usage for real-world robotic tasks has yet been limited. This work demonstrates the great potential of differentiable simulation for learning quadrotor control. We show that training in differentiable simulation significantly outperforms model-free RL in terms of both sample efficiency and training time, allowing a policy to learn to recover a quadrotor in seconds when providing vehicle states and in minutes when relying solely on visual features. The key to our success is two-fold. First, the use of a simple surrogate model for gradient computation greatly accelerates training without sacrificing control performance. Second, combining state representation learning with policy learning enhances convergence speed in tasks where only visual features are observable. These findings highlight the potential of differentiable simulation for real-world robotics and offer a compelling alternative to conventional RL approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可微分模拟从视觉特征学习四旋翼飞行器控制</div>
<div class="mono" style="margin-top:8px">强化学习（RL）的样本低效性仍是机器人学领域的重大挑战。RL需要大规模仿真且训练周期长，制约了研究创新。该问题在无法获取可靠状态估计的视觉控制任务中尤为突出。可微分模拟通过动力学模型的梯度反向传播提供了替代方案，能产生低方差解析策略梯度，从而提升样本效率，但其在现实机器人任务中的应用仍有限。本研究证明了可微分模拟在学习四旋翼控制方面的巨大潜力：在可微分模拟中的训练，其样本效率与训练时间均显著优于无模型RL——当提供飞行器状态时策略可在数秒内学会恢复飞行姿态，仅依赖视觉特征时也仅需数分钟。成功的关键在于两方面：首先，采用简化代理模型计算梯度，在保持控制性能的同时大幅加速训练；其次，在仅能观测视觉特征的任务中，将状态表征学习与策略学习结合提升了收敛速度。这些发现凸显了可微分模拟在现实机器人任务中的潜力，为传统RL方法提供了具有竞争力的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the sample inefficiency and long training times of reinforcement learning (RL) in robotics, particularly for vision-based control where state estimates are unavailable. The method employs differentiable simulation, which enables gradient back-propagation through the dynamics model to provide low-variance policy gradients, and combines it with a simple surrogate model for efficient gradient computation and state representation learning for visual tasks. Experimental results show that training in this differentiable simulator significantly outperforms model-free RL in sample efficiency and speed, allowing a quadrotor policy to learn recovery in seconds with vehicle states and in minutes using only visual features.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习在机器人学中样本效率低、训练时间长的问题，特别是在状态估计不可靠的视觉控制任务中。该方法采用可微分仿真，通过动力学模型实现梯度反向传播以提供低方差策略梯度，并结合简单的代理模型进行高效梯度计算以及针对视觉任务的状态表征学习。实验结果表明，在该可微分仿真框架中的训练在样本效率和速度上显著优于无模型强化学习，使得四旋翼控制策略能在数秒内学会使用机体状态进行恢复，并在仅使用视觉特征时在数分钟内完成学习。</div>
</details>
</div>
<div class="card">
<div class="title">The impact of tactile sensor configurations on grasp learning efficiency -- a comparative evaluation in simulation</div>
<div class="meta-line">Authors: Eszter Birtalan, Miklós Koller</div>
<div class="meta-line">First: 2026-01-15T10:38:14+00:00 · Latest: 2026-01-15T10:38:14+00:00</div>
<div class="meta-line">Comments: 13 pages, 6 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10268v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10268v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tactile sensors are breaking into the field of robotics to provide direct information related to contact surfaces, including contact events, slip events and even texture identification. These events are especially important for robotic hand designs, including prosthetics, as they can greatly improve grasp stability. Most presently published robotic hand designs, however, implement them in vastly different densities and layouts on the hand surface, often reserving the majority of the available space. We used simulations to evaluate 6 different tactile sensor configurations with different densities and layouts, based on their impact on reinforcement learning. Our two-setup system allows for robust results that are not dependent on the use of a given physics simulator, robotic hand model or machine learning algorithm. Our results show setup-specific, as well as generalized effects across the 6 sensorized simulations, and we identify one configuration as consistently yielding the best performance across both setups. These results could help future research aimed at robotic hand designs, including prostheses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触觉传感器配置对抓取学习效率的影响——基于仿真的对比评估</div>
<div class="mono" style="margin-top:8px">触觉传感器正进入机器人领域，为接触表面提供直接信息，包括接触事件、滑动事件乃至纹理识别。这些事件对机器人手设计（包括假肢）尤为重要，能显著提升抓取稳定性。然而，当前多数已发布的机器人手设计在手掌表面采用差异巨大的密度与布局方案，常占据大部分可用空间。本研究通过仿真评估了6种不同密度与布局的触觉传感器配置对强化学习的影响。我们的双实验系统确保了结果鲁棒性，不依赖于特定物理模拟器、机器人手模型或机器学习算法。结果显示，6组传感器仿真实验既存在特定配置效应，也呈现普适性规律，其中一种配置在两组实验中均表现最优。该结论可为未来机器人手（含假肢）设计研究提供参考。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how different tactile sensor configurations affect grasp learning efficiency in robotic hands, motivated by the need to optimize sensor placement for improved grasp stability in applications like prosthetics. The authors conducted a comparative evaluation in simulation, testing six sensor layouts with varying densities and arrangements using a two-setup system to ensure robustness across different physics simulators, hand models, and machine learning algorithms. Experimental results revealed both setup-specific and generalized effects, with one configuration consistently achieving the best performance across both setups, offering guidance for future robotic hand designs.</div>
<div class="mono" style="margin-top:8px">本研究探讨了机器人手上不同触觉传感器密度和布局对抓握学习效率的影响，动机在于优化传感器配置以提升如假肢等应用的抓握稳定性。方法基于仿真比较评估了六种不同的触觉传感器配置，采用双设置系统确保结果在不同物理模拟器、手部模型和强化学习算法中的鲁棒性。主要实验结果表明了设置特定和跨设置的通用性能效应，并识别出一种在两种设置中均表现最佳的配置，为未来机器人手设计提供了参考。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory</div>
<div class="meta-line">Authors: Johann Licher, Max Bartholdt, Henrik Krauss, Tim-Lukas Habich, Thomas Seel, Moritz Schappler</div>
<div class="meta-line">First: 2025-08-18T07:24:36+00:00 · Latest: 2026-01-15T09:58:58+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Robotics, 20 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12681v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12681v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic control of soft continuum robots (SCRs) holds great potential for expanding their applications, but remains a challenging problem due to the high computational demands of accurate dynamic models. While data-driven approaches like Koopman-operator-based methods have been proposed, they typically lack adaptability and cannot reconstruct the full robot shape, limiting their applicability. This work introduces a real-time-capable nonlinear model-predictive control (MPC) framework for SCRs based on a domain-decoupled physics-informed neural network (DD-PINN) with adaptable bending stiffness. The DD-PINN serves as a surrogate for the dynamic Cosserat rod model with a speed-up factor of 44000. It is also used within an unscented Kalman filter for estimating the model states and bending compliance from end-effector position measurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the GPU. In simulation, it demonstrates accurate tracking of dynamic trajectories and setpoint control with end-effector position errors below 3 mm (2.3% of the actuator&#x27;s length). In real-world experiments, the controller achieves similar accuracy and accelerations up to 3.55 m/s2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Cosserat杆理论的物理信息神经网络实现软体连续体机器人自适应模型预测控制</div>
<div class="mono" style="margin-top:8px">软体连续体机器人的动态控制具有广阔应用前景，但精确动力学模型的高计算需求使其成为难题。现有数据驱动方法（如基于Koopman算子的方法）通常缺乏适应性且无法重构完整机器人形态。本研究提出一种基于可调弯曲刚度域解耦物理信息神经网络的实时非线性模型预测控制框架，该神经网络作为动态Cosserat杆模型的替代模型，速度提升达44000倍，并集成无迹卡尔曼滤波器通过末端执行器位置测量估计模型状态与弯曲柔度。在GPU上以70Hz运行的进化非线性模型预测控制器，在仿真中实现动态轨迹精确跟踪（末端位置误差小于3毫米，占执行器长度2.3%），实物实验达到同等精度及3.55m/s²的加速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of dynamic control for soft continuum robots, where accurate real-time modeling is computationally demanding and existing data-driven methods lack adaptability and full shape reconstruction. The authors propose a nonlinear model-predictive control framework that employs a domain-decoupled physics-informed neural network as a surrogate for the dynamic Cosserat rod model, achieving a 44000x speed-up, and integrate it with an unscented Kalman filter for state and stiffness estimation. Experimental results show the controller operates at 70 Hz, achieving end-effector position errors below 3 mm (2.3% of actuator length) in simulation and similar accuracy with accelerations up to 3.55 m/s² in real-world tests.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决软体连续机器人的动态控制难题，该问题因精确动态模型计算量大而充满挑战，且现有数据驱动方法缺乏适应性和完整形状重建能力。作者提出了一种非线性模型预测控制框架，采用基于Cosserat杆理论的域解耦物理信息神经网络作为快速代理模型，实现了44000倍的加速，并结合无迹卡尔曼滤波器进行状态和刚度估计。实验结果表明，该控制器以70 Hz频率运行，在仿真中实现了末端执行器误差低于3毫米的精确轨迹跟踪，实际测试也达到了相近精度，加速度最高达3.55 m/s²。</div>
</details>
</div>
<div class="card">
<div class="title">Proactive Local-Minima-Free Robot Navigation: Blending Motion Prediction with Safe Control</div>
<div class="meta-line">Authors: Yifan Xue, Ze Zhang, Knut Åkesson, Nadia Figueroa</div>
<div class="meta-line">First: 2026-01-15T09:46:03+00:00 · Latest: 2026-01-15T09:46:03+00:00</div>
<div class="meta-line">Comments: Co-first authors: Yifan Xue and Ze Zhang</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work addresses the challenge of safe and efficient mobile robot navigation in complex dynamic environments with concave moving obstacles. Reactive safe controllers like Control Barrier Functions (CBFs) design obstacle avoidance strategies based only on the current states of the obstacles, risking future collisions. To alleviate this problem, we use Gaussian processes to learn barrier functions online from multimodal motion predictions of obstacles generated by neural networks trained with energy-based learning. The learned barrier functions are then fed into quadratic programs using modulated CBFs (MCBFs), a local-minimum-free version of CBFs, to achieve safe and efficient navigation. The proposed framework makes two key contributions. First, it develops a prediction-to-barrier function online learning pipeline. Second, it introduces an autonomous parameter tuning algorithm that adapts MCBFs to deforming, prediction-based barrier functions. The framework is evaluated in both simulations and real-world experiments, consistently outperforming baselines and demonstrating superior safety and efficiency in crowded dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动无局部极小值机器人导航：融合运动预测与安全控制</div>
<div class="mono" style="margin-top:8px">本研究致力于解决在具有凹形移动障碍物的复杂动态环境中实现移动机器人安全高效导航的挑战。基于控制屏障函数（CBFs）等反应式安全控制器仅根据障碍物当前状态设计避障策略，存在未来碰撞风险。为缓解此问题，我们采用高斯过程在线学习屏障函数，其输入为通过基于能量的学习训练的神经网络生成的多模态障碍物运动预测。随后将学习得到的屏障函数输入至采用调制控制屏障函数（MCBFs）的二次规划中——MCBFs是CBFs的无局部极小值版本，以实现安全高效导航。该框架作出两项关键贡献：其一，构建了从预测到屏障函数的在线学习流程；其二，提出了自主参数调优算法，使MCBFs能自适应基于预测的可变形屏障函数。通过仿真与真实实验验证，该框架在拥挤动态环境中始终优于基线方法，展现出卓越的安全性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research tackles the challenge of enabling mobile robots to navigate safely and efficiently in complex dynamic environments containing concave moving obstacles, where purely reactive controllers risk future collisions by relying only on current obstacle states. The method integrates multimodal neural network motion predictions of obstacles with online Gaussian process learning to construct predictive barrier functions, which are then incorporated into a local-minima-free safe control framework using modulated control barrier functions (MCBFs) within quadratic programs; an autonomous parameter tuning algorithm further adapts the MCBFs to these learned, deforming barriers. Experimental evaluations in simulations and real-world tests show the framework consistently outperforms baseline methods, achieving superior safety and navigation efficiency in crowded dynamic settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决移动机器人在动态复杂环境中安全高效导航的挑战，其中纯反应式控制器因缺乏前瞻性而存在碰撞风险。该方法将基于神经网络的多模态障碍物运动预测与通过高斯过程在线学习屏障函数相结合，并将学习到的函数用于调制控制屏障函数（MCBF）框架，以构建一个无局部最优的安全控制器。仿真和真实世界实验结果表明，该框架在拥挤动态环境中 consistently 优于基线方法，实现了更高的安全性和导航效率。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Framework for Kinematic Simulation of Rigid Foldable Structures</div>
<div class="meta-line">Authors: Dongwook Kwak, Geonhee Cho, Jiook Chung, Jinkyu Yang</div>
<div class="meta-line">First: 2026-01-15T09:38:42+00:00 · Latest: 2026-01-15T09:38:42+00:00</div>
<div class="meta-line">Comments: 34 pages (20 pages main text), 11 figures (7 in main text, 4 in appendix)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10225v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Origami-inspired structures with rigid panels now span thick, kirigami, and multi-sheet realizations, making unified kinematic analysis essential. Yet a general method that consolidates their loop constraints has been lacking. We present an automated approach that generates the Pfaffian constraint matrix for arbitrary rigid foldable structures (RFS). From a minimally extended data schema, the tool constructs the facet-hinge graph, extracts a minimum cycle basis that captures all constraints, and assembles a velocity-level constraint matrix via screw theory that encodes coupled rotation and translation loop closure. The framework computes and visualizes deploy and fold motions across diverse RFS while eliminating tedious and error-prone constraint calculations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>刚性可折叠结构运动学模拟的统一框架</div>
<div class="mono" style="margin-top:8px">受折纸启发的刚性板结构现已涵盖厚板、剪纸及多片层实现形式，统一的运动学分析至关重要。然而，整合其环路约束的通用方法长期缺失。本文提出一种自动化方法，可为任意刚性可折叠结构生成普法夫约束矩阵。该工具基于最小化扩展数据架构，构建面-铰链图，提取捕获所有约束的最小环基，并通过旋量理论组装速度级约束矩阵，以编码耦合旋转与平移的环路闭合。该框架能计算并可视化各类刚性可折叠结构的展开与折叠运动，同时消除了繁琐易错的约束计算过程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a unified kinematic analysis method for diverse rigid foldable structures, including thick, kirigami, and multi-sheet realizations, as existing approaches lack a general way to consolidate their loop constraints. The method introduces an automated framework that generates a Pfaffian constraint matrix from a minimal data schema by constructing a facet-hinge graph, extracting a minimum cycle basis, and assembling a velocity-level constraint matrix via screw theory to encode coupled rotation and translation loop closure. Key experimental results demonstrate that the framework successfully computes and visualizes deployment and folding motions across various rigid foldable structures, effectively eliminating tedious and error-prone manual constraint calculations.</div>
<div class="mono" style="margin-top:8px">本研究旨在为厚板、剪纸和多层片等多样化的刚性可折叠结构提供统一的运动学分析方法，因为此前缺乏一种整合其环路约束的通用方法。该方法提出了一个自动化框架，通过从最小化数据模式构建面-铰链图，提取捕获所有约束的最小环基，并利用螺旋理论组装编码耦合旋转与平移环路闭合的速度级约束矩阵，从而为任意结构生成普法夫约束矩阵。关键实验结果表明，该框架成功计算并可视化了多种刚性可折叠结构的展开与折叠运动，有效消除了繁琐且易出错的手动约束计算。</div>
</details>
</div>
<div class="card">
<div class="title">Terrain-Adaptive Mobile 3D Printing with Hierarchical Control</div>
<div class="meta-line">Authors: Shuangshan Nors Li, J. Nathan Kutz</div>
<div class="meta-line">First: 2026-01-15T09:15:06+00:00 · Latest: 2026-01-15T09:15:06+00:00</div>
<div class="meta-line">Comments: Submitted to the 43rd International Symposium on Automation and Robotics in Construction (ISARC 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile 3D printing on unstructured terrain remains challenging due to the conflict between platform mobility and deposition precision. Existing gantry-based systems achieve high accuracy but lack mobility, while mobile platforms struggle to maintain print quality on uneven ground. We present a framework that tightly integrates AI-driven disturbance prediction with multi-modal sensor fusion and hierarchical hardware control, forming a closed-loop perception-learning-actuation system. The AI module learns terrain-to-perturbation mappings from IMU, vision, and depth sensors, enabling proactive compensation rather than reactive correction. This intelligence is embedded into a three-layer control architecture: path planning, predictive chassis-manipulator coordination, and precision hardware execution. Through outdoor experiments on terrain with slopes and surface irregularities, we demonstrate sub-centimeter printing accuracy while maintaining full platform mobility. This AI-hardware integration establishes a practical foundation for autonomous construction in unstructured environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分层控制的地形自适应移动式3D打印技术</div>
<div class="mono" style="margin-top:8px">非结构化地形上的移动式3D打印因平台移动性与沉积精度的矛盾而面临挑战。现有龙门式系统精度高但缺乏移动性，移动平台则难以在崎岖地面保持打印质量。本研究提出一种将AI驱动的扰动预测与多模态传感器融合、分层硬件控制深度集成的框架，形成感知-学习-执行的闭环系统。AI模块通过IMU、视觉与深度传感器学习地形-扰动映射关系，实现主动补偿而非被动校正。该智能系统嵌入三层控制架构：路径规划层、预测性底盘-机械臂协调层、精密硬件执行层。通过在斜坡与不规则表面的户外实验，系统在保持平台全向移动能力的同时实现了亚厘米级打印精度。这种AI-硬件深度融合技术为无结构环境下的自主建造奠定了实践基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mobile 3D printing on unstructured terrain is hindered by the trade-off between platform mobility and deposition precision, as gantry systems lack mobility and mobile platforms lose accuracy on uneven ground. To address this, the authors propose a closed-loop system integrating AI-driven disturbance prediction from multi-modal sensor data (IMU, vision, depth) with a three-layer hierarchical control architecture for path planning, predictive chassis-manipulator coordination, and precision hardware execution. Experimental results from outdoor tests on sloped and irregular terrain demonstrate that the framework achieves sub-centimeter printing accuracy while preserving full platform mobility, establishing a foundation for autonomous construction in unstructured environments.</div>
<div class="mono" style="margin-top:8px">在非结构化地形上进行移动式3D打印面临平台移动性与打印精度之间的冲突。为解决该问题，本研究提出了一个闭环系统，该系统将基于多模态传感器（IMU、视觉、深度）数据的AI扰动预测，与一个包含路径规划、预测性底盘-机械臂协调和精密硬件执行的三层分层控制架构相结合。在具有坡度和表面不规则地形的户外实验中，该框架在保持平台完全移动性的同时，实现了亚厘米级的打印精度。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Querying for Reward Learning from Human Feedback</div>
<div class="meta-line">Authors: Yashwanthi Anand, Nnamdi Nwagwu, Kevin Sabbe, Naomi T. Fitter, Sandhya Saisubramanian</div>
<div class="meta-line">First: 2024-12-11T00:02:48+00:00 · Latest: 2026-01-15T09:01:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.07990v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.07990v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from human feedback is a popular approach to train robots to adapt to user preferences and improve safety. Existing approaches typically consider a single querying (interaction) format when seeking human feedback and do not leverage multiple modes of user interaction with a robot. We examine how to learn a penalty function associated with unsafe behaviors using multiple forms of human feedback, by optimizing both the query state and feedback format. Our proposed adaptive feedback selection is an iterative, two-phase approach which first selects critical states for querying, and then uses information gain to select a feedback format for querying across the sampled critical states. The feedback format selection also accounts for the cost and probability of receiving feedback in a certain format. Our experiments in simulation demonstrate the sample efficiency of our approach in learning to avoid undesirable behaviors. The results of our user study with a physical robot highlight the practicality and effectiveness of adaptive feedback selection in seeking informative, user-aligned feedback that accelerate learning. Experiment videos, code and appendices are found on our website: https://tinyurl.com/AFS-learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于自适应查询的人机反馈奖励学习</div>
<div class="mono" style="margin-top:8px">从人类反馈中学习是训练机器人适应用户偏好并提升安全性的常用方法。现有方法在获取人类反馈时通常仅考虑单一查询（交互）形式，未能充分利用用户与机器人的多种交互模式。本研究探讨如何通过优化查询状态与反馈形式，利用多种人类反馈学习与不安全行为相关的惩罚函数。我们提出的自适应反馈选择是一种迭代式两阶段方法：首先筛选关键状态进行查询，随后基于信息增益在采样的关键状态中选择反馈形式。反馈形式的选择同时考虑了特定格式反馈的获取成本与概率。仿真实验表明，该方法在学习规避不良行为时具有较高的样本效率。基于实体机器人的用户研究结果进一步验证了自适应反馈选择在获取信息丰富、符合用户意图的反馈以加速学习方面的实用性与有效性。实验视频、代码及附录详见项目网站：https://tinyurl.com/AFS-learning。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing methods for learning from human feedback often rely on a single interaction format, failing to leverage diverse modes of user input. To address this, the study proposes an adaptive feedback selection method that iteratively chooses critical states for querying and then selects the most informative feedback format—such as rankings or corrections—based on expected information gain, while also considering the cost and likelihood of receiving feedback. Experimental simulations and a user study with a physical robot demonstrate that this approach learns to avoid undesirable behaviors more sample-efficiently and accelerates learning by obtaining more informative, user-aligned feedback.</div>
<div class="mono" style="margin-top:8px">现有基于人类反馈的学习方法通常依赖单一的交互形式，难以充分利用多样化的用户输入。为此，本研究提出了一种自适应反馈选择方法，通过优化查询状态和反馈形式来学习与不安全机器人行为相关的惩罚函数。这种迭代式两阶段方法首先选择关键状态进行查询，然后基于信息增益选择反馈形式，同时考虑特定反馈形式的成本和接收概率。仿真实验结果表明，该方法在学习避免不良行为方面具有样本高效性；与实体机器人进行的用户研究进一步证实了其在实际应用中的可行性与有效性，能够获取信息丰富且符合用户偏好的反馈，从而加速学习过程。</div>
</details>
</div>
<div class="card">
<div class="title">RAG-3DSG: Enhancing 3D Scene Graphs with Re-Shot Guided Retrieval-Augmented Generation</div>
<div class="meta-line">Authors: Yue Chang, Rufeng Chen, Zhaofan Zhang, Yi Chen, Sihong Xie</div>
<div class="meta-line">First: 2026-01-15T08:15:01+00:00 · Latest: 2026-01-15T08:15:01+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10168v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10168v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary 3D Scene Graph (3DSG) generation can enhance various downstream tasks in robotics, such as manipulation and navigation, by leveraging structured semantic representations. A 3DSG is constructed from multiple images of a scene, where objects are represented as nodes and relationships as edges. However, existing works for open-vocabulary 3DSG generation suffer from both low object-level recognition accuracy and speed, mainly due to constrained viewpoints, occlusions, and redundant surface density. To address these challenges, we propose RAG-3DSG to mitigate aggregation noise through re-shot guided uncertainty estimation and support object-level Retrieval-Augmented Generation (RAG) via reliable low-uncertainty objects. Furthermore, we propose a dynamic downsample-mapping strategy to accelerate cross-image object aggregation with adaptive granularity. Experiments on Replica dataset demonstrate that RAG-3DSG significantly improves node captioning accuracy in 3DSG generation while reducing the mapping time by two-thirds compared to the vanilla version.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAG-3DSG：通过重拍引导的检索增强生成优化三维场景图</div>
<div class="mono" style="margin-top:8px">开放词汇三维场景图生成可通过结构化语义表征提升机器人操作、导航等下游任务性能。三维场景图由场景多视角图像构建，以节点表示对象、边表示关系。然而，现有方法因视角受限、遮挡及表面冗余密度等问题，存在对象识别准确率低、速度慢的缺陷。为此，我们提出RAG-3DSG：通过重拍引导的不确定性估计缓解聚合噪声，并借助低不确定性可靠对象实现对象级检索增强生成。此外，我们提出动态降采样映射策略，以自适应粒度加速跨图像对象聚合。在Replica数据集上的实验表明，RAG-3DSG显著提升三维场景图节点标注准确率，同时将映射时间缩减至基准版本的三分之一。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve open-vocabulary 3D Scene Graph (3DSG) generation for robotics applications, as existing methods suffer from low accuracy and speed due to limited viewpoints, occlusions, and redundant data. The proposed RAG-3DSG method introduces a re-shot guided uncertainty estimation to reduce aggregation noise and employs object-level Retrieval-Augmented Generation (RAG) using reliable low-uncertainty objects, alongside a dynamic downsample-mapping strategy for faster cross-image aggregation. Experimental results on the Replica dataset show that this approach significantly enhances node captioning accuracy and reduces mapping time by two-thirds compared to the baseline.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升用于机器人应用的开放词汇3D场景图生成技术，该技术因视角受限、遮挡和数据冗余导致精度和速度较低。所提出的RAG-3DSG方法通过重拍引导的不确定性估计来减少聚合噪声，并利用可靠的低不确定性对象进行对象级检索增强生成，同时采用动态下采样映射策略以加速跨图像对象聚合。在Replica数据集上的实验结果表明，该方法显著提高了节点标注的准确性，并将建图时间相比原始版本减少了三分之二。</div>
</details>
</div>
<div class="card">
<div class="title">CoCoPlan: Adaptive Coordination and Communication for Multi-robot Systems in Dynamic and Unknown Environments</div>
<div class="meta-line">Authors: Xintong Zhang, Junfeng Chen, Yuxiao Zhu, Bing Luo, Meng Guo</div>
<div class="meta-line">First: 2026-01-15T06:50:21+00:00 · Latest: 2026-01-15T06:50:21+00:00</div>
<div class="meta-line">Comments: 8 pages, 8 figures, published to RA-L</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10116v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10116v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-robot systems can greatly enhance efficiency through coordination and collaboration, yet in practice, full-time communication is rarely available and interactions are constrained to close-range exchanges. Existing methods either maintain all-time connectivity, rely on fixed schedules, or adopt pairwise protocols, but none adapt effectively to dynamic spatio-temporal task distributions under limited communication, resulting in suboptimal coordination. To address this gap, we propose CoCoPlan, a unified framework that co-optimizes collaborative task planning and team-wise intermittent communication. Our approach integrates a branch-and-bound architecture that jointly encodes task assignments and communication events, an adaptive objective function that balances task efficiency against communication latency, and a communication event optimization module that strategically determines when, where and how the global connectivity should be re-established. Extensive experiments demonstrate that it outperforms state-of-the-art methods by achieving a 22.4% higher task completion rate, reducing communication overhead by 58.6%, and improving the scalability by supporting up to 100 robots in dynamic environments. Hardware experiments include the complex 2D office environment and large-scale 3D disaster-response scenario.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoCoPlan：动态未知环境下多机器人系统的自适应协同与通信框架</div>
<div class="mono" style="margin-top:8px">多机器人系统通过协同合作可显著提升效率，但实践中难以实现全时段通信，交互通常局限于近距离交换。现有方法或维持全时连接、依赖固定调度、采用成对协议，均无法在有限通信条件下有效适应动态时空任务分布，导致协同效率欠佳。为此，我们提出CoCoPlan——一个协同优化协作任务规划与团队间歇通信的统一框架。该框架融合了以下组件：联合编码任务分配与通信事件的分支定界架构、平衡任务效率与通信延迟的自适应目标函数，以及策略性决定何时何地以何种方式重建全局连接的通信事件优化模块。大量实验表明，本方法在动态环境中任务完成率提升22.4%，通信开销降低58.6%，并通过支持多达100台机器人显著提升可扩展性，性能优于现有先进方法。硬件实验涵盖复杂二维办公环境与大规模三维灾难响应场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-robot systems often operate with limited, intermittent communication, but existing coordination methods fail to adapt effectively to dynamic task distributions under such constraints. To solve this, CoCoPlan co-optimizes task planning and team-wise intermittent communication through a branch-and-bound architecture that jointly encodes assignments and communication events, an adaptive objective balancing efficiency and latency, and a module for strategically re-establishing global connectivity. Experiments show it achieves a 22.4% higher task completion rate, reduces communication overhead by 58.6%, and scales to support 100 robots in dynamic environments, as validated in 2D office and 3D disaster-response scenarios.</div>
<div class="mono" style="margin-top:8px">多机器人系统在动态未知环境中常受限于间歇性通信，现有协调方法难以适应此种约束下的时空任务分布，导致性能不佳。为此，CoCoPlan提出了一个统一框架，通过分支定界架构联合编码任务分配与通信事件、平衡任务效率与通信延迟的自适应目标函数，以及战略性规划全局连接事件的优化模块，共同优化协同任务规划与团队间歇通信。实验结果表明，该方法在动态环境中任务完成率提升22.4%，通信开销降低58.6%，并可扩展至100台机器人，已在2D办公室和3D灾难响应场景中得到硬件验证。</div>
</details>
</div>
<div class="card">
<div class="title">Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making</div>
<div class="meta-line">Authors: Jua Han, Jaeyoon Seo, Jungbin Min, Jean Oh, Jihie Kim</div>
<div class="meta-line">First: 2026-01-09T05:04:15+00:00 · Latest: 2026-01-15T05:09:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05529v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05529v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how &quot;rare&quot; errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全未找到（404）：基于大语言模型的机器人决策隐藏风险</div>
<div class="mono" style="margin-top:8px">在安全关键场景中，AI系统的一次失误可能危及生命。随着大语言模型（LLMs）成为机器人决策的核心组成部分，风险的物理维度随之扩大——单条错误指令可直接威胁人身安全。本文针对亟需系统评估LLMs在微小误差即导致灾难性后果场景中的性能展开研究。通过对火灾疏散场景的定性评估，我们识别出基于LLM决策的关键失效案例。基于此，我们设计了七项定量评估任务，分为三类：完整信息任务、不完整信息任务、安全导向空间推理（SOSR）任务。完整信息任务采用ASCII地图以最小化解读歧义，将空间推理与视觉处理分离；不完整信息任务要求模型推断缺失语境，检验空间连续性与幻觉现象；SOSR任务使用自然语言评估危及生命情境下的安全决策能力。我们对各类LLMs和视觉语言模型（VLMs）进行了基准测试。除整体性能外，我们深入分析了1%故障率的影响，揭示“罕见”错误如何升级为灾难性后果。结果暴露严重漏洞：多个模型在ASCII导航任务中成功率为0%；在模拟消防演练中，模型竟指示机器人朝危险区域而非紧急出口移动。研究得出警示性结论：当前LLMs尚未具备直接部署于安全关键系统的条件。99%的准确率在机器人领域具有危险误导性——这意味着每百次执行就可能发生一次灾难性伤害。我们证明即使最先进模型也无法保障安全，对其绝对依赖将产生不可接受的风险。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the severe physical risks posed by even minor errors when Large Language Models (LLMs) are used for robotics decision-making in safety-critical contexts like fire evacuations. The method involves a qualitative evaluation followed by a quantitative assessment across seven tasks categorized into Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR), using ASCII maps and natural language to benchmark various LLMs and Vision-Language Models (VLMs). Key experimental findings reveal critical vulnerabilities, including models achieving a 0% success rate in ASCII navigation tasks and, in a simulated fire drill, instructing robots to move toward hazards instead of exits, demonstrating that a 99% accuracy rate is dangerously insufficient and current models cannot guarantee safety for direct deployment.</div>
<div class="mono" style="margin-top:8px">本研究源于大型语言模型（LLM）在火灾疏散等安全关键场景中用于机器人决策时，即使微小错误也可能导致严重后果的风险。研究方法采用系统性评估框架，设计了七项定量任务，分为完全信息、不完全信息和安全导向空间推理（SOSR）三类，使用ASCII地图和自然语言对多种LLM和视觉语言模型（VLM）进行基准测试。主要实验结果揭示了严重漏洞，包括模型在ASCII导航任务中成功率降至0%，以及在模拟消防演练中指示机器人移向危险区域而非紧急出口，表明99%的准确率具有误导性，意味着每百次执行就可能发生一次灾难性失败。</div>
</details>
</div>
<div class="card">
<div class="title">CoinFT: A Coin-Sized, Capacitive 6-Axis Force Torque Sensor for Robotic Applications</div>
<div class="meta-line">Authors: Hojung Choi, Jun En Low, Tae Myung Huh, Seongheon Hong, Gabriela A. Uribe, Kenneth A. W. Hoffmann, Julia Di, Tony G. Chen, Andrew A. Stanley, Mark R. Cutkosky</div>
<div class="meta-line">First: 2025-03-25T00:12:48+00:00 · Latest: 2026-01-15T04:10:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.19225v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.19225v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://coin-ft.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce CoinFT, a capacitive 6-axis force/torque (F/T) sensor that is compact, light, low-cost, and robust with an average root-mean-squared error of 0.16N for force and 1.08mNm for moment when the input ranges from 0~14N and 0~5N in normal and shear directions, respectively. CoinFT is a stack of two rigid PCBs with comb-shaped electrodes connected by an array of silicone rubber pillars. The microcontroller interrogates the electrodes in different subsets in order to enhance sensitivity for measuring 6-axis F/T. The combination of features of CoinFT enables various contact-rich robot interactions across different embodiment domains including drones, robot end-effectors, and wearable haptic devices. We demonstrate the utility of CoinFT through two representative applications: a multi-axial contact-probing experiment in which a CoinFT mounted beneath a hemispherical fingertip measures 6-axes of force and torque representative of manipulation scenarios, and an attitude-based force-control task on a drone. The design, fabrication, and firmware of CoinFT are open-sourced at https://coin-ft.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoinFT：一种用于机器人应用的硬币尺寸电容式六维力扭矩传感器</div>
<div class="mono" style="margin-top:8px">我们推出CoinFT，一种紧凑、轻量、低成本且坚固的电容式六维力/扭矩传感器，在法向和剪切方向输入范围分别为0~14N和0~5N时，其平均均方根误差为力0.16N、力矩1.08mNm。CoinFT由两片带有梳状电极的刚性PCB堆叠而成，通过硅橡胶柱阵列连接。微控制器通过分组合询问电极以提升六维力/扭矩测量的灵敏度。CoinFT的特性组合使其能广泛应用于无人机、机器人末端执行器和可穿戴触觉设备等不同形态领域的密集接触式机器人交互。我们通过两个代表性应用展示CoinFT的实用性：在多轴接触探测实验中，安装在半球形指尖底部的CoinFT测量了代表操控场景的六维力与扭矩；以及在无人机上进行的基于姿态的力控任务。CoinFT的设计、制造和固件已在https://coin-ft.github.io/开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a compact, low-cost, and robust six-axis force/torque sensor for diverse robotic applications. The method employs a capacitive sensing design where two rigid PCBs with comb-shaped electrodes are connected by silicone rubber pillars, and a microcontroller interrogates electrode subsets to enhance sensitivity for six-axis measurements. Experimental results show the sensor achieves an average root-mean-squared error of 0.16 N for force and 1.08 mNm for torque within specified ranges, and its utility is demonstrated in contact-probing experiments and drone force-control tasks.</div>
<div class="mono" style="margin-top:8px">为实现紧凑、低成本且鲁棒的力/力矩传感以支持丰富的机器人接触交互，本研究提出了CoinFT，一种硬币大小的电容式六维力传感器。其方法采用由硅橡胶柱连接的两层带有梳状电极的刚性PCB板堆叠结构，通过微控制器查询不同的电极子集来增强测量六维力和力矩的灵敏度。实验结果表明，该传感器在指定量程内实现了平均0.16N的力误差和1.08mNm的力矩误差，其应用潜力在机器人指尖的多轴接触探测和无人机基于姿态的力控制等任务中得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">UEOF: A Benchmark Dataset for Underwater Event-Based Optical Flow</div>
<div class="meta-line">Authors: Nick Truong, Pritam P. Karmokar, William J. Beksi</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2026-01-15T04:10:14+00:00 · Latest: 2026-01-15T04:10:14+00:00</div>
<div class="meta-line">Comments: To be presented at the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) Workshop on Event-Based Vision in the Era of Generative AI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10054v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10054v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://robotic-vision-lab.github.io/ueof">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Underwater imaging is fundamentally challenging due to wavelength-dependent light attenuation, strong scattering from suspended particles, turbidity-induced blur, and non-uniform illumination. These effects impair standard cameras and make ground-truth motion nearly impossible to obtain. On the other hand, event cameras offer microsecond resolution and high dynamic range. Nonetheless, progress on investigating event cameras for underwater environments has been limited due to the lack of datasets that pair realistic underwater optics with accurate optical flow. To address this problem, we introduce the first synthetic underwater benchmark dataset for event-based optical flow derived from physically-based ray-traced RGBD sequences. Using a modern video-to-event pipeline applied to rendered underwater videos, we produce realistic event data streams with dense ground-truth flow, depth, and camera motion. Moreover, we benchmark state-of-the-art learning-based and model-based optical flow prediction methods to understand how underwater light transport affects event formation and motion estimation accuracy. Our dataset establishes a new baseline for future development and evaluation of underwater event-based perception algorithms. The source code and dataset for this project are publicly available at https://robotic-vision-lab.github.io/ueof.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UEOF：水下事件光流基准数据集</div>
<div class="mono" style="margin-top:8px">水下成像因波长依赖性光衰减、悬浮颗粒强散射、浑浊导致的模糊及非均匀照明而极具挑战，这些效应不仅损害标准相机性能，也使真实运动数据难以获取。相比之下，事件相机具备微秒级分辨率与高动态范围优势。然而，由于缺乏结合真实水下光学特性与精确光流的数据集，水下事件相机研究进展有限。为此，我们首次提出基于物理光线追踪RGBD序列生成的合成水下事件光流基准数据集。通过将现代视频-事件转换流程应用于渲染的水下视频，我们生成了包含密集真实光流、深度与相机运动的逼真事件数据流。此外，我们评估了前沿的基于学习与模型的光流预测方法，以探究水下光传输如何影响事件形成与运动估计精度。本数据集为未来水下事件感知算法的开发与评估建立了新基准。项目源代码与数据集已公开于https://robotic-vision-lab.github.io/ueof。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Underwater imaging suffers from severe degradation due to light attenuation, scattering, and turbidity, which hinder conventional cameras and make acquiring accurate motion ground truth difficult. To enable research on event cameras in such environments, this work introduces UEOF, a synthetic benchmark dataset generated by applying a video-to-event pipeline to physically-based ray-traced underwater RGBD sequences, providing realistic event streams with dense ground-truth optical flow, depth, and camera motion. Experimental benchmarking of state-of-the-art learning-based and model-based optical flow methods on this dataset reveals how underwater light transport impacts event formation and motion estimation accuracy, establishing a new baseline for developing underwater event-based perception algorithms.</div>
<div class="mono" style="margin-top:8px">水下成像面临波长依赖的光衰减、悬浮颗粒散射和浑浊模糊等挑战，这限制了标准相机的性能并使真实运动数据难以获取，而事件相机虽具有微秒级分辨率和动态范围，却因缺乏结合真实水下光学与准确光流的数据集而进展有限。为此，研究者提出了首个合成水下事件光流基准数据集UEOF，通过将视频转事件流程应用于基于物理光线追踪的RGBD序列，生成了具有密集真实光流、深度和相机运动的逼真事件数据流。对先进学习型和模型型光流预测方法的基准测试揭示了水下光传输如何影响事件形成和运动估计精度，为未来水下事件感知算法的开发与评估建立了新基线。</div>
</details>
</div>
<div class="card">
<div class="title">In-the-Wild Compliant Manipulation with UMI-FT</div>
<div class="meta-line">Authors: Hojung Choi, Yifan Hou, Chuer Pan, Seongheon Hong, Austin Patel, Xiaomeng Xu, Mark R. Cutkosky, Shuran Song</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-01-15T02:00:03+00:00 · Latest: 2026-01-15T02:00:03+00:00</div>
<div class="meta-line">Comments: submitted to ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09988v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://umi-ft.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many manipulation tasks require careful force modulation. With insufficient force the task may fail, while excessive force could cause damage. The high cost, bulky size and fragility of commercial force/torque (F/T) sensors have limited large-scale, force-aware policy learning. We introduce UMI-FT, a handheld data-collection platform that mounts compact, six-axis force/torque sensors on each finger, enabling finger-level wrench measurements alongside RGB, depth, and pose. Using the multimodal data collected from this device, we train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. In evaluations on three contact-rich, force-sensitive tasks (whiteboard wiping, skewering zucchini, and lightbulb insertion), UMI-FT enables policies that reliably regulate external contact forces and internal grasp forces, outperforming baselines that lack compliance or force sensing. UMI-FT offers a scalable path to learning compliant manipulation from in-the-wild demonstrations. We open-source the hardware and software to facilitate broader adoption at:https://umi-ft.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于UMI-FT的野外环境顺应性操控</div>
<div class="mono" style="margin-top:8px">多数操控任务需精细的力调节：力不足则任务失败，力过大易造成损伤。商用六维力/力矩传感器因成本高、体积大、易损坏，限制了大规模力感知策略学习。本文提出UMI-FT手持数据采集平台，在每根手指搭载紧凑型六维力/力矩传感器，实现指端力矩测量，并同步采集RGB、深度与位姿数据。利用该设备采集的多模态数据，我们训练出能预测位置目标、抓握力及刚度的自适应顺应策略，可在标准顺应控制器上执行。在擦白板、串西葫芦、装灯泡三项接触密集且力敏感的任务评估中，UMI-FT策略能可靠调节外部接触力与内部抓握力，性能优于无顺应性或无力感知的基线方法。UMI-FT为从野外演示中学习顺应性操控提供了可扩展路径。我们开源硬件与软件以促进广泛应用：https://umi-ft.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenges of learning force-aware manipulation policies due to the high cost and fragility of commercial force/torque sensors, this research introduces UMI-FT, a handheld data-collection platform equipped with compact six-axis force/torque sensors on each finger. The method involves collecting multimodal data including finger-level wrench measurements, RGB, depth, and pose, which is then used to train an adaptive compliance policy that predicts position targets, grasp force, and stiffness for execution on standard compliance controllers. Experimental results on three contact-rich tasks—whiteboard wiping, skewering zucchini, and lightbulb insertion—demonstrate that policies enabled by UMI-FT reliably regulate both external contact and internal grasp forces, outperforming baselines lacking compliance or force sensing.</div>
<div class="mono" style="margin-top:8px">为解决商用六维力/力矩传感器成本高、体积大且易损坏，从而限制大规模力感知策略学习的问题，本研究提出了UMI-FT，一种在每个手指上安装紧凑型六维力/力矩传感器的手持式数据采集平台。该方法通过采集包括手指级力矩测量、RGB图像、深度和位姿在内的多模态数据，训练一个自适应柔顺策略，该策略可预测位置目标、抓握力和刚度，并在标准柔顺控制器上执行。在三个接触密集且对力敏感的任务（白板擦拭、西葫芦串刺和灯泡安装）上的评估结果表明，UMI-FT所启用的策略能可靠地调节外部接触力和内部抓握力，其性能优于缺乏柔顺控制或力感知的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Monocular Depth Refinement via Neural Radiance Fields</div>
<div class="meta-line">Authors: Arun Muthukkumar</div>
<div class="meta-line">First: 2026-01-07T12:32:39+00:00 · Latest: 2026-01-15T01:46:55+00:00</div>
<div class="meta-line">Comments: IEEE 8th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03869v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03869v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular depth estimation has applications in many fields, such as autonomous navigation and extended reality, making it an essential computer vision task. However, current methods often produce smooth depth maps that lack the fine geometric detail needed for accurate scene understanding. We propose MDENeRF, an iterative framework that refines monocular depth estimates using depth information from Neural Radiance Fields (NeRFs). MDENeRF consists of three components: (1) an initial monocular estimate for global structure, (2) a NeRF trained on perturbed viewpoints, with per-pixel uncertainty, and (3) Bayesian fusion of the noisy monocular and NeRF depths. We derive NeRF uncertainty from the volume rendering process to iteratively inject high-frequency fine details. Meanwhile, our monocular prior maintains global structure. We demonstrate improvements on key metrics and experiments using indoor scenes from the SUN RGB-D dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经辐射场的贝叶斯单目深度优化方法</div>
<div class="mono" style="margin-top:8px">单目深度估计在自动驾驶导航和扩展现实等领域具有广泛应用，是计算机视觉的重要任务。然而现有方法常生成平滑的深度图，缺乏精确场景理解所需的精细几何细节。我们提出MDENeRF迭代框架，利用神经辐射场（NeRF）的深度信息优化单目深度估计。该框架包含三个组件：（1）用于全局结构的初始单目估计；（2）基于扰动视角训练并具有逐像素不确定性的NeRF；（3）对含噪单目深度与NeRF深度进行贝叶斯融合。我们从体渲染过程中推导NeRF不确定性，以迭代方式注入高频细节，同时通过单目先验保持全局结构。基于SUN RGB-D数据集的室内场景实验表明，该方法在关键指标上均取得显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Monocular depth estimation is crucial for applications like autonomous navigation, but existing methods often yield overly smooth depth maps lacking fine geometric details. To address this, the authors propose MDENeRF, an iterative framework that refines monocular depth by fusing an initial monocular estimate with depth information from a Neural Radiance Field (NeRF) trained on perturbed viewpoints, using a Bayesian approach that incorporates per-pixel uncertainty derived from the NeRF&#x27;s volume rendering. Experimental results on indoor scenes from the SUN RGB-D dataset demonstrate that the method effectively injects high-frequency details while preserving global structure, leading to measurable improvements in key depth estimation metrics.</div>
<div class="mono" style="margin-top:8px">单目深度估计常产生过于平滑的深度图，缺乏自动驾驶等应用所需的精细几何细节。为此，研究者提出MDENeRF迭代框架，通过融合神经辐射场（NeRF）的深度信息来优化初始单目估计。该方法将全局单目先验与扰动视角训练的NeRF相结合，利用从体渲染过程推导的逐像素不确定性进行贝叶斯融合，从而注入高频细节。在SUN RGB-D室内数据集上的实验表明，该方法在关键深度估计指标上取得了可量化的提升。</div>
</details>
</div>
<div class="card">
<div class="title">OT-Drive: Out-of-Distribution Off-Road Traversable Area Segmentation via Optimal Transport</div>
<div class="meta-line">Authors: Zhihua Zhao, Guoqiang Li, Chen Min, Kangping Lu</div>
<div class="meta-line">First: 2026-01-15T00:23:45+00:00 · Latest: 2026-01-15T00:23:45+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures, 6 tables. This work has been submitted to the IEEE for possible publication. Code will be released upon acceptance</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09952v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09952v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable traversable area segmentation in unstructured environments is critical for planning and decision-making in autonomous driving. However, existing data-driven approaches often suffer from degraded segmentation performance in out-of-distribution (OOD) scenarios, consequently impairing downstream driving tasks. To address this issue, we propose OT-Drive, an Optimal Transport--driven multi-modal fusion framework. The proposed method formulates RGB and surface normal fusion as a distribution transport problem. Specifically, we design a novel Scene Anchor Generator (SAG) to decompose scene information into the joint distribution of weather, time-of-day, and road type, thereby constructing semantic anchors that can generalize to unseen scenarios. Subsequently, we design an innovative Optimal Transport-based multi-modal fusion module (OT Fusion) to transport RGB and surface normal features onto the manifold defined by the semantic anchors, enabling robust traversable area segmentation under OOD scenarios. Experimental results demonstrate that our method achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset transfer tasks, surpassing baselines by 13.99%.These results indicate that the proposed model can attain strong OOD generalization with only limited training data, substantially enhancing its practicality and efficiency for real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OT-Drive：基于最优传输的分布外越野可通行区域分割方法</div>
<div class="mono" style="margin-top:8px">在非结构化环境中实现可靠的可通行区域分割对自动驾驶的规划与决策至关重要。然而，现有数据驱动方法在分布外场景中常出现分割性能下降，进而影响下游驾驶任务。为此，我们提出OT-Drive——一种基于最优传输的多模态融合框架。该方法将RGB与表面法线融合建模为分布传输问题：首先设计场景锚点生成器，将场景信息解耦为天气、时段与道路类型的联合分布，构建可泛化至未见场景的语义锚点；继而设计基于最优传输的多模态融合模块，将RGB与表面法线特征映射至语义锚点定义的流形上，实现分布外场景下的鲁棒可通行区域分割。实验表明，本方法在ORFD分布外场景达到95.16% mIoU（较现有方法提升6.35%），在跨数据集迁移任务达到89.79% mIoU（较基线提升13.99%）。结果表明，仅需有限训练数据即可实现强分布外泛化能力，显著提升实际部署的实用性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of degraded traversable area segmentation performance in out-of-distribution (OOD) off-road scenarios, which impairs autonomous driving reliability. The proposed OT-Drive method formulates multi-modal fusion as an optimal transport problem, introducing a Scene Anchor Generator to decompose scene information into semantic anchors for generalization and an Optimal Transport-based fusion module to align RGB and surface normal features with these anchors. Experiments show the model achieves 95.16% mIoU on ORFD OOD scenarios, outperforming prior methods by 6.35%, and 89.79% mIoU on cross-dataset tasks, surpassing baselines by 13.99%, demonstrating strong OOD generalization with limited training data.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自动驾驶在非结构化环境中，可通行区域分割模型在分布外场景下性能下降的问题。所提出的OT-Drive方法将多模态融合构建为一个最优传输问题，通过设计场景锚点生成器分解场景信息为语义锚点，并利用基于最优传输的融合模块将RGB和表面法线特征对齐到这些锚点定义的流形上。实验结果表明，该方法在分布外测试集上取得了95.16%的mIoU，优于先前方法6.35%，在跨数据集迁移任务上达到89.79%的mIoU，超越基线13.99%，证明了其在有限训练数据下具备强大的分布外泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">A Taxonomy for Evaluating Generalist Robot Manipulation Policies</div>
<div class="meta-line">Authors: Jensen Gao, Suneel Belkhale, Sudeep Dasari, Ashwin Balakrishna, Dhruv Shah, Dorsa Sadigh</div>
<div class="meta-line">First: 2025-03-03T07:03:00+00:00 · Latest: 2026-01-15T00:06:44+00:00</div>
<div class="meta-line">Comments: IEEE Robotics and Automation Letters (RA-L)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.01238v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.01238v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning for robot manipulation promises to unlock generalization to novel tasks and environments. But how should we measure the progress of these policies towards generalization? Evaluating and quantifying generalization is the Wild West of modern robotics, with each work proposing and measuring different types of generalization in their own, often difficult to reproduce settings. In this work, our goal is (1) to outline the forms of generalization we believe are important for robot manipulation in a comprehensive and fine-grained manner, and (2) to provide reproducible guidelines for measuring these notions of generalization. We first propose STAR-Gen, a taxonomy of generalization for robot manipulation structured around visual, semantic, and behavioral generalization. Next, we instantiate STAR-Gen with two case studies on real-world benchmarking: one based on open-source models and the Bridge V2 dataset, and another based on the bimanual ALOHA 2 platform that covers more dexterous and longer horizon tasks. Our case studies reveal many interesting insights: for example, we observe that open-source vision-language-action models often struggle with semantic generalization, despite pre-training on internet-scale language datasets. We provide videos and other supplementary material at our website stargen-taxonomy.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用机器人操作策略评估分类体系</div>
<div class="mono" style="margin-top:8px">机器人操作的机器学习有望实现对新颖任务和环境的泛化能力。然而，我们应如何衡量这些策略在泛化方面的进展？评估和量化泛化是现代机器人学中尚未规范的领域，每项研究都以各自难以复现的方式提出和测量不同类型的泛化。本研究的目标是：(1) 以全面细粒度的方式，阐明我们认为对机器人操作至关重要的泛化形式；(2) 为测量这些泛化概念提供可复现的指导原则。我们首先提出STAR-Gen——一个围绕视觉、语义和行为泛化构建的机器人操作泛化分类体系。随后，通过两个真实世界基准测试案例对STAR-Gen进行实例化：一个基于开源模型和Bridge V2数据集，另一个基于覆盖更灵巧、更长周期任务的双臂ALOHA 2平台。案例研究揭示了诸多重要发现：例如，开源视觉-语言-动作模型尽管经过互联网规模语言数据集的预训练，仍常面临语义泛化挑战。相关视频及补充材料详见网站stargen-taxonomy.github.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for standardized evaluation metrics to measure the generalization capabilities of robot manipulation policies, which currently lack consistent and reproducible benchmarks. The method introduces STAR-Gen, a taxonomy categorizing generalization into visual, semantic, and behavioral dimensions, and applies it through two real-world case studies using the Bridge V2 dataset and the ALOHA 2 platform. Key experimental findings reveal that open-source vision-language-action models often underperform in semantic generalization despite extensive pre-training, highlighting specific weaknesses in adapting to novel task semantics.</div>
<div class="mono" style="margin-top:8px">针对机器人操作策略泛化能力评估缺乏标准化的问题，本研究提出了STAR-Gen分类法，围绕视觉、语义和行为泛化进行结构化定义。该方法通过在Bridge V2数据集上的开源模型案例研究，以及使用双手ALOHA 2平台进行灵巧长时程任务的案例研究，提供了可复现的评估方案。主要实验发现表明，尽管经过大规模语言数据预训练，开源视觉-语言-动作模型在语义泛化方面仍普遍表现不佳，这揭示了一个关键的改进方向。</div>
</details>
</div>
<div class="card">
<div class="title">SyncTwin: Fast Digital Twin Construction and Synchronization for Safe Robotic Grasping</div>
<div class="meta-line">Authors: Ruopeng Huang, Boyu Yang, Wenlong Gui, Jeremy Morgan, Erdem Biyik, Jiachen Li</div>
<div class="meta-line">First: 2026-01-14T23:03:43+00:00 · Latest: 2026-01-14T23:03:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09920v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate and safe grasping under dynamic and visually occluded conditions remains a core challenge in real-world robotic manipulation. We present SyncTwin, a digital twin framework that unifies fast 3D scene reconstruction and real-to-sim synchronization for robust and safety-aware grasping in such environments. In the offline stage, we employ VGGT to rapidly reconstruct object-level 3D assets from RGB images, forming a reusable geometry library for simulation. During execution, SyncTwin continuously synchronizes the digital twin by tracking real-world object states via point cloud segmentation updates and aligning them through colored-ICP registration. The updated twin enables motion planners to compute collision-free and dynamically feasible trajectories in simulation, which are safely executed on the real robot through a closed real-to-sim-to-real loop. Experiments in dynamic and occluded scenes show that SyncTwin improves grasp accuracy and motion safety, demonstrating the effectiveness of digital-twin synchronization for real-world robotic execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SyncTwin：面向安全机器人抓取的快速数字孪生构建与同步</div>
<div class="mono" style="margin-top:8px">在动态且视觉遮挡条件下实现精准安全的抓取，仍是现实世界机器人操作的核心挑战。本文提出SyncTwin——一种融合快速三维场景重建与实景-仿真同步的数字孪生框架，旨在为此类环境提供鲁棒且安全感知的抓取方案。离线阶段采用VGGT从RGB图像快速重建物体级三维资产，构建可复用的仿真几何库；在线执行时，SyncTwin通过点云分割更新追踪真实物体状态，并借助彩色ICP配准实现对齐，持续同步数字孪生体。更新后的孪生模型支持运动规划器在仿真中计算无碰撞且动态可行的轨迹，通过实景-仿真-实景闭环在真实机器人上安全执行。动态遮挡场景实验表明，SyncTwin显著提升了抓取精度与运动安全性，验证了数字孪生同步技术对现实机器人执行任务的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of achieving accurate and safe robotic grasping in dynamic and visually occluded real-world environments. The proposed method, SyncTwin, constructs a digital twin framework that first uses VGGT for fast offline 3D asset reconstruction from RGB images, then continuously synchronizes the twin during execution via point cloud segmentation and colored-ICP registration to track real-world object states. Experimental results in dynamic and occluded scenes demonstrate that the framework improves grasp accuracy and motion safety, validating the effectiveness of the synchronized digital twin for real-world robotic execution.</div>
<div class="mono" style="margin-top:8px">为解决动态和视觉遮挡条件下机器人抓取准确性与安全性的核心挑战，本研究提出了SyncTwin数字孪生框架，该框架将快速三维场景重建与虚实同步相统一。其方法包括利用VGGT从RGB图像快速重建物体级三维资产的离线阶段，以及通过点云分割更新和彩色ICP配准持续同步数字孪生以跟踪真实物体状态的在线执行阶段。在动态和遮挡场景中的实验表明，SyncTwin提高了抓取精度和运动安全性，验证了数字孪生同步对于现实世界机器人执行的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">How Human Motion Prediction Quality Shapes Social Robot Navigation Performance in Constrained Spaces</div>
<div class="meta-line">Authors: Andrew Stratton, Phani Teja Singamaneni, Pranav Goyal, Rachid Alami, Christoforos Mavrogiannis</div>
<div class="meta-line">First: 2026-01-14T20:34:34+00:00 · Latest: 2026-01-14T20:34:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09856v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09856v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motivated by the vision of integrating mobile robots closer to humans in warehouses, hospitals, manufacturing plants, and the home, we focus on robot navigation in dynamic and spatially constrained environments. Ensuring human safety, comfort, and efficiency in such settings requires that robots are endowed with a model of how humans move around them. Human motion prediction around robots is especially challenging due to the stochasticity of human behavior, differences in user preferences, and data scarcity. In this work, we perform a methodical investigation of the effects of human motion prediction quality on robot navigation performance, as well as human productivity and impressions. We design a scenario involving robot navigation among two human subjects in a constrained workspace and instantiate it in a user study ($N=80$) involving two different robot platforms, conducted across two sites from different world regions. Key findings include evidence that: 1) the widely adopted average displacement error is not a reliable predictor of robot navigation performance and human impressions; 2) the common assumption of human cooperation breaks down in constrained environments, with users often not reciprocating robot cooperation, and causing performance degradations; 3) more efficient robot navigation often comes at the expense of human efficiency and comfort.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类运动预测质量如何影响社交机器人在受限空间中的导航性能</div>
<div class="mono" style="margin-top:8px">为实现移动机器人在仓库、医院、工厂及家庭等场景中更紧密地融入人类环境，本研究聚焦于动态且空间受限环境下的机器人导航。为确保此类环境中的人类安全、舒适与效率，机器人需具备对人类移动行为的预测模型。由于人类行为的随机性、用户偏好差异及数据稀缺性，机器人对人类运动的预测尤为困难。本研究系统探究了人类运动预测质量对机器人导航性能、人类工作效率及主观印象的影响。我们设计了机器人在受限工作空间中与两名人类协作的导航场景，并通过跨两大洲两个实验场地、包含两种机器人平台、涉及80名参与者的用户研究进行验证。主要发现包括：1）广泛采用的平均位移误差指标无法可靠预测机器人导航性能与人类主观印象；2）在受限环境中，人类会协作的普遍假设往往不成立，用户常未对机器人的协作行为作出回应，导致性能下降；3）机器人导航效率的提升常以牺牲人类效率与舒适度为代价。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to integrate mobile robots safely and efficiently into dynamic, constrained human environments like warehouses and hospitals, this work systematically investigates how the quality of human motion prediction impacts robot navigation performance and human outcomes. The method involves designing a constrained workspace scenario where a robot navigates among two human subjects, and instantiating it in a multi-site user study (N=80) with two different robot platforms. Key experimental findings reveal that the standard average displacement error metric is unreliable for predicting navigation performance or human impressions, the assumption of human cooperation often fails in constrained spaces leading to performance degradation, and that gains in robot navigation efficiency frequently come at the cost of reduced human efficiency and comfort.</div>
<div class="mono" style="margin-top:8px">本研究旨在将移动机器人安全高效地集成到仓库、医院等动态受限的人类环境中，因此系统性地探究了人体运动预测质量对机器人导航的影响。方法上，设计了一个机器人在受限工作空间中与两名人类受试者导航的场景，并通过一项在两个不同地区站点开展、涉及两种机器人平台的用户研究（N=80）来具体实施。关键的实验结果包括：广泛采用的平均位移误差指标并不能可靠预测机器人导航性能或人类主观印象；在受限环境中，人类会合作的常见假设往往不成立，用户通常不会回应机器人的合作行为，从而导致性能下降；机器人导航效率的提升常常以牺牲人类的效率和舒适度为代价。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
