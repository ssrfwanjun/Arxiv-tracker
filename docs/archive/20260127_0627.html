<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-27 06:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260127_0627</div>
    <div class="row"><div class="card">
<div class="title">AnyView: Synthesizing Any Novel View in Dynamic Scenes</div>
<div class="meta-line">Authors: Basile Van Hoorick, Dian Chen, Shun Iwase, Pavel Tokmakov, Muhammad Zubair Irshad, Igor Vasiljevic, Swati Gupta, Fangzhou Cheng, Sergey Zakharov, Vitor Campagnolo Guizilini</div>
<div class="meta-line">First: 2026-01-23T18:59:58+00:00 · Latest: 2026-01-23T18:59:58+00:00</div>
<div class="meta-line">Comments: Project webpage: https://tri-ml.github.io/AnyView/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16982v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16982v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tri-ml.github.io/AnyView/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyView：动态场景中任意新视角的合成</div>
<div class="mono" style="margin-top:8px">现代生成式视频模型在生成逼真的高质量输出方面表现出色，但在高度动态的真实世界环境中难以保持多视角和时空一致性。本文提出\textbf{AnyView}——一种基于扩散的视频生成框架，用于\emph{动态视角合成}，仅需最小归纳偏置或几何假设。我们利用多监督层级的数据源（包括单目2D、静态多视角3D和动态多视角4D数据集），训练出通用的时空隐式表示，能够从任意相机位置和轨迹生成零样本新视角视频。在标准基准测试中，AnyView取得了与当前最优方法竞争的结果。我们还提出\textbf{AnyViewBench}——一个专为多样化真实场景中\emph{极端}动态视角合成设计的新基准。在这一更具挑战性的设定下，多数基线方法因需要视角间显著重叠而性能急剧下降，而AnyView在\emph{任意}视角提示下仍能生成真实、合理且时空一致的视频。结果、数据、代码和模型可通过以下链接查看：https://tri-ml.github.io/AnyView/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of current generative video models in maintaining multi-view and spatiotemporal consistency for dynamic scenes. The method introduces AnyView, a diffusion-based framework that trains a generalist spatiotemporal implicit representation using diverse data sources, including monocular, multi-view static, and multi-view dynamic datasets, to enable zero-shot novel video synthesis from arbitrary camera trajectories. Experimental results show that AnyView achieves competitive performance on standard benchmarks and, on the proposed challenging AnyViewBench for extreme dynamic view synthesis, significantly outperforms baselines by generating realistic and consistent videos from any viewpoint where others degrade due to requiring significant viewpoint overlap.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有生成式视频模型难以在动态场景中保持多视角和时空一致性。方法上提出了AnyView，这是一个基于扩散的框架，它利用不同监督级别的多源数据（包括单目、多视角静态和多视角动态数据集）训练一个通用的时空隐式表示，以实现从任意相机轨迹进行零样本新视角视频合成。实验结果表明，在标准基准测试中取得了有竞争力的性能，并在针对极端动态视角合成的新基准AnyViewBench上表现出色，其中AnyView能保持真实且一致的输出，而基线方法因依赖视角重叠而性能大幅下降。</div>
</details>
</div>
<div class="card">
<div class="title">MapAnything: Universal Feed-Forward Metric 3D Reconstruction</div>
<div class="meta-line">Authors: Nikhil Keetha, Norman Müller, Johannes Schönberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bulò, Christian Richardt, Deva Ramanan, Sebastian Scherer, Peter Kontschieder</div>
<div class="meta-line">First: 2025-09-16T18:00:14+00:00 · Latest: 2026-01-23T18:59:33+00:00</div>
<div class="meta-line">Comments: 3DV 2026. Project Page: https://map-anything.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.13414v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.13414v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://map-anything.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapAnything：通用前馈式度量三维重建</div>
<div class="mono" style="margin-top:8px">本文提出MapAnything，一种基于Transformer的统一前馈模型。该模型可接收单张或多张图像，以及相机内参、位姿、深度或部分重建结果等可选几何输入，直接回归出度量化的三维场景几何与相机参数。MapAnything采用多视图场景几何的分解表示形式——即深度图集合、局部射线图、相机位姿及度量尺度因子，从而将局部重建有效升级至全局一致的度量坐标系。通过跨数据集的标准化监督训练与灵活的输入增强策略，MapAnything能在单次前馈计算中处理广泛的三维视觉任务，包括未标定的运动恢复结构、已标定的多视图立体视觉、单目深度估计、相机定位、深度补全等。大量实验分析与模型消融研究表明，MapAnything在保持高效联合训练特性的同时，其性能优于或媲美专用前馈模型，为构建通用三维重建主干网络开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a universal model for diverse 3D vision tasks, which are typically addressed by separate specialized methods. The method introduces MapAnything, a unified transformer-based feed-forward model that processes images along with optional geometric inputs like camera parameters or partial reconstructions, and directly regresses metric 3D geometry and cameras using a factored representation of multi-view scene geometry. Experimental results show that MapEither outperforms or matches specialized feed-forward models across tasks including uncalibrated structure-from-motion, multi-view stereo, and monocular depth estimation, while demonstrating more efficient joint training behavior.</div>
<div class="mono" style="margin-top:8px">本研究旨在为通常由不同专用模型处理的多种3D视觉任务开发一个通用模型。该方法引入了MapAnything，这是一个基于Transformer的统一前馈模型，它处理图像及可选的几何输入，并使用多视图场景几何的分解表示直接回归度量3D几何和相机。实验结果表明，在未标定运动恢复结构、多视图立体视觉和单目深度估计等任务上，MapAnything的性能优于或匹配专用前馈模型，同时表现出更高效的联合训练特性。</div>
</details>
</div>
<div class="card">
<div class="title">Q-learning with Adjoint Matching</div>
<div class="meta-line">Authors: Qiyang Li, Sergey Levine</div>
<div class="meta-line">First: 2026-01-20T18:45:34+00:00 · Latest: 2026-01-23T18:40:14+00:00</div>
<div class="meta-line">Comments: 32 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14234v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14234v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic&#x27;s action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伴随匹配Q学习</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于时序差分的新型强化学习算法——伴随匹配Q学习（QAM），旨在解决连续动作强化学习中长期存在的难题：如何高效优化具有表达力的扩散或流匹配策略，使其与参数化Q函数相适应。有效优化需利用评论家的一阶信息，但对流或扩散策略而言，通过多步去噪过程进行基于梯度的反向传播优化存在数值不稳定性。现有方法要么仅使用值函数而丢弃梯度信息，要么依赖近似手段牺牲策略表达性或引入偏差。QAM通过采用生成建模领域新近提出的伴随匹配技术，巧妙规避了这些挑战。该技术将评论家的动作梯度转化为逐步目标函数，避免了不稳定的反向传播，同时在最优解处提供无偏且高表达力的策略。结合时序差分备份进行评论家学习，QAM在离线及离线到在线强化学习的困难稀疏奖励任务中，均持续超越现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of efficiently optimizing expressive diffusion or flow-matching policies in continuous-action reinforcement learning, where direct gradient-based optimization through multi-step denoising is numerically unstable. The proposed method, Q-learning with Adjoint Matching (QAM), leverages adjoint matching to transform the critic&#x27;s action gradient into a step-wise objective, avoiding unstable backpropagation while maintaining an unbiased and expressive policy. Experimental results show that QAM consistently outperforms prior methods on hard, sparse reward tasks in both offline and offline-to-online RL settings.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决连续动作强化学习中高效优化表达性扩散或流匹配策略的长期挑战，其中通过多步去噪过程的直接梯度优化存在数值不稳定性。所提出的方法——伴随匹配Q学习（QAM）——利用伴随匹配技术将评论者的动作梯度转化为逐步目标函数，从而避免了不稳定的反向传播，同时保持了策略的无偏性和表达性。实验结果表明，结合时序差分更新的QAM，在离线和离线到在线强化学习的困难、稀疏奖励任务上，性能持续优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Where to Touch, How to Contact: Hierarchical RL-MPC Framework for Geometry-Aware Long-Horizon Dexterous Manipulation</div>
<div class="meta-line">Authors: Zhixian Xie, Yu Xiang, Michael Posa, Wanxin Jin</div>
<div class="meta-line">First: 2026-01-16T01:20:15+00:00 · Latest: 2026-01-23T18:00:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10930v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10930v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A key challenge in contact-rich dexterous manipulation is the need to jointly reason over geometry, kinematic constraints, and intricate, nonsmooth contact dynamics. End-to-end visuomotor policies bypass this structure, but often require large amounts of data, transfer poorly from simulation to reality, and generalize weakly across tasks/embodiments. We address those limitations by leveraging a simple insight: dexterous manipulation is inherently hierarchical - at a high level, a robot decides where to touch (geometry) and move the object (kinematics); at a low level it determines how to realize that plan through contact dynamics. Building on this insight, we propose a hierarchical RL--MPC framework in which a high-level reinforcement learning (RL) policy predicts a contact intention, a novel object-centric interface that specifies (i) an object-surface contact location and (ii) a post-contact object-level subgoal pose. Conditioned on this contact intention, a low-level contact-implicit model predictive control (MPC) optimizes local contact modes and replans with contact dynamics to generate robot actions that robustly drive the object toward each subgoal. We evaluate the framework on non-prehensile tasks, including geometry-generalized pushing and object 3D reorientation. It achieves near-100% success with substantially reduced data (10x less than end-to-end baselines), highly robust performance, and zero-shot sim-to-real transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>触达何处，如何接触：面向几何感知长时程灵巧操作的层次化RL-MPC框架</div>
<div class="mono" style="margin-top:8px">接触密集型灵巧操作的核心挑战在于需同时处理几何结构、运动学约束及复杂非光滑接触动力学。端到端视觉运动策略虽绕开此结构，但通常需大量数据、仿真到现实的迁移性差，且跨任务/实体泛化能力弱。我们基于一个简明洞见应对这些局限：灵巧操作本质是层次化的——高层决策触达位置（几何）与物体移动（运动学）；底层通过接触动力学实现该规划。基于此，我们提出一种层次化RL-MPC框架：高层强化学习策略预测接触意图（一种以物体为中心的新型接口，指定物体表面接触位置及接触后物体层级子目标位姿）；底层基于该意图，通过接触隐式模型预测控制优化局部接触模式，并借助接触动力学重新规划，生成稳健驱动物体朝向各子目标的机器人动作。该框架在非抓取任务（包括几何泛化推动与物体三维重定向）中验证，以极低数据量（较端到端基线减少10倍）实现近100%成功率，具备高度鲁棒性及零样本仿真到现实迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of contact-rich dexterous manipulation, where joint reasoning over geometry, kinematics, and nonsmooth contact dynamics is required, and end-to-end visuomotor policies suffer from high data needs, poor sim-to-real transfer, and weak generalization. The proposed method is a hierarchical RL-MPC framework: a high-level RL policy predicts a contact intention specifying an object-surface contact location and a post-contact object subgoal pose, while a low-level contact-implicit model predictive control optimizes local contact modes and replans with contact dynamics to generate robot actions. Experimental results on non-prehensile tasks like geometry-generalized pushing and object 3D reorientation show near-100% success rates, a tenfold reduction in data compared to end-to-end baselines, high robustness, and zero-shot sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">本研究针对灵巧操作中需联合推理几何、运动学和复杂接触动力学的挑战，因为端到端的视觉运动策略通常需要大量数据且泛化能力差。提出的方法是一个分层RL-MPC框架：高层强化学习策略预测接触意图——指定物体表面接触位置和接触后的子目标位姿；低层基于接触隐式模型预测控制优化局部接触模式以生成鲁棒的机器人动作。在非抓取任务（如推动和三维重定向）上的实验结果表明，该方法实现了接近100%的成功率，所需数据比端到端基线少10倍，具有高鲁棒性，并能完成零模拟到真实的迁移。</div>
</details>
</div>
<div class="card">
<div class="title">HEIGHT: Heterogeneous Interaction Graph Transformer for Robot Navigation in Crowded and Constrained Environments</div>
<div class="meta-line">Authors: Shuijing Liu, Haochen Xia, Fatemeh Cheraghi Pouria, Kaiwen Hong, Neeloy Chakraborty, Zichao Hu, Joydeep Biswas, Katherine Driggs-Campbell</div>
<div class="meta-line">First: 2024-11-19T00:56:35+00:00 · Latest: 2026-01-23T17:24:19+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE Transactions of Automation Science and Engineering (T-ASE)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.12150v4">Abs</a> · <a href="https://arxiv.org/pdf/2411.12150v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/crowdnav-height/home">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of robot navigation in dense and interactive crowds with static constraints such as corridors and furniture. Previous methods fail to consider all types of spatial and temporal interactions among agents and obstacles, leading to unsafe and inefficient robot paths. In this article, we leverage a graph-based representation of crowded and constrained scenarios and propose a structured framework to learn robot navigation policies with deep reinforcement learning. We first split the representations of different inputs and propose a heterogeneous spatio-temporal graph to model distinct interactions among humans, robots, and obstacles. Based on the heterogeneous spatio-temporal graph, we propose HEIGHT, a novel navigation policy network architecture with different components to capture heterogeneous interactions through space and time. HEIGHT utilizes attention mechanisms to prioritize important interactions and a recurrent network to track changes in the dynamic scene over time, encouraging the robot to avoid collisions adaptively. Through extensive simulation and real-world experiments, we demonstrate that HEIGHT outperforms state-of-the-art baselines in terms of success, navigation time, and generalization to domain shifts in challenging navigation scenarios. More information is available at https://sites.google.com/view/crowdnav-height/home.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HEIGHT：面向拥挤受限环境下机器人导航的异质交互图变换器</div>
<div class="mono" style="margin-top:8px">本研究针对存在走廊、家具等静态约束的密集交互人群环境中的机器人导航问题展开。现有方法未能全面考虑智能体与障碍物间的时空交互关系，导致机器人路径规划存在安全风险与效率瓶颈。本文提出基于图结构的拥挤受限场景表征框架，采用深度强化学习构建结构化导航策略。首先对不同输入特征进行解耦表征，构建异质时空图以建模人、机器人、障碍物间的差异化交互关系。基于该图结构，我们提出HEIGHT导航策略网络架构，通过多组件设计捕获时空维度的异质交互特征。HEIGHT采用注意力机制动态评估交互重要性，结合循环神经网络追踪动态场景时序变化，实现机器人自适应避障。大量仿真与实物实验表明，在挑战性导航场景中，HEIGHT在成功率、导航耗时及领域迁移泛化能力方面均优于现有先进基线模型。更多信息详见：https://sites.google.com/view/crowdnav-height/home。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses robot navigation in dense crowds with static constraints, where prior methods inadequately model spatial and temporal interactions, resulting in unsafe and inefficient paths. The authors propose HEIGHT, a framework that employs a heterogeneous spatio-temporal graph to separately represent interactions among humans, robots, and obstacles, and integrates this with deep reinforcement learning; the policy network uses attention mechanisms to prioritize key interactions and a recurrent component to track dynamic changes over time. Experimental results from simulations and real-world tests show that HEIGHT surpasses state-of-the-art baselines in success rate, navigation time, and generalization to domain shifts in challenging scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人如何在密集、交互式人群及静态约束环境中导航的问题，先前方法未能充分考虑各类空间与时间交互，导致路径不安全且低效。作者提出HEIGHT框架，将场景建模为异质时空图以区分人类、机器人和障碍物之间的交互，并采用深度强化学习策略网络，结合注意力机制和循环组件来优先处理重要交互并适应动态变化。通过大量仿真和真实实验验证，HEIGHT在成功率、导航时间和对挑战性场景领域迁移的泛化能力上均优于现有先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">GPA-VGGT:Adapting VGGT to Large scale Localization by self-Supervised learning with Geometry and Physics Aware loss</div>
<div class="meta-line">Authors: Yangfan Xu, Lilian Zhang, Xiaofeng He, Pengdong Wu, Wenqi Wu, Jun Mao</div>
<div class="meta-line">First: 2026-01-23T16:46:59+00:00 · Latest: 2026-01-23T16:46:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16885v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16885v1">PDF</a> · <a href="https://github.com/X-yangfan/GPA-VGGT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GPA-VGGT：通过几何与物理感知损失的自监督学习将VGGT适配于大规模定位</div>
<div class="mono" style="margin-top:8px">基于Transformer的通用视觉几何框架在相机姿态估计和三维场景理解中展现出优异性能。视觉几何基础Transformer（VGGT）模型的最新进展在相机姿态估计与三维重建领域潜力显著，但这些模型通常依赖标注真值进行训练，在适应无标注及未见场景时面临挑战。本文提出一种自监督框架，利用无标注数据训练VGGT，从而增强其在大规模环境中的定位能力。为实现这一目标，我们将传统的成对关系扩展为序列级几何约束以进行自监督学习。具体而言，在每个序列中采样多帧源图像，并将其几何投影至不同目标帧，从而提升时序特征一致性。我们将物理光度一致性与几何约束构建为联合优化损失函数，以规避对硬标签的依赖。通过该方法训练模型，不仅局部与全局跨视图注意力层，相机与深度头部也能有效捕捉潜在的多视图几何关系。实验表明，模型在数百次迭代内即可收敛，并实现大规模定位性能的显著提升。代码发布于https://github.com/X-yangfan/GPA-VGGT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Visual Geometry Grounded Transformer (VGGT) models, which typically require ground truth labels for training, making adaptation to unlabeled, large-scale scenes challenging. The proposed method, GPA-VGGT, introduces a self-supervised framework that extends pairwise geometric constraints to sequence-wise constraints by sampling multiple source frames and projecting them onto different target frames to enhance temporal feature consistency; it formulates a joint optimization loss combining physical photometric consistency and geometric constraints to eliminate the need for hard labels. Experimental results show that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization performance, with both the attention layers and the camera/depth heads effectively capturing multi-view geometry.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉几何基础Transformer模型通常依赖真实标签进行训练、难以适应无标签大规模场景的问题，提出了一种自监督框架GPA-VGGT。该方法将传统的成对几何关系扩展为序列级约束，通过采样多个源帧并将其几何投影到不同目标帧来提升时序特征一致性，并构建了结合物理光度一致性与几何约束的联合优化损失函数，使模型的注意力层和相机/深度头能够在无标签情况下学习多视图几何。实验结果表明，该模型在数百次迭代内即可收敛，并在大规模定位任务中取得了显著性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">A Multimodal Data Collection Framework for Dialogue-Driven Assistive Robotics to Clarify Ambiguities: A Wizard-of-Oz Pilot Study</div>
<div class="meta-line">Authors: Guangping Liu, Nicholas Hawkins, Billy Madden, Tipu Sultan, Flavio Esposito, Madi Babaiasl</div>
<div class="meta-line">First: 2026-01-23T16:22:21+00:00 · Latest: 2026-01-23T16:22:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16870v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16870v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrated control of wheelchairs and wheelchair-mounted robotic arms (WMRAs) has strong potential to increase independence for users with severe motor limitations, yet existing interfaces often lack the flexibility needed for intuitive assistive interaction. Although data-driven AI methods show promise, progress is limited by the lack of multimodal datasets that capture natural Human-Robot Interaction (HRI), particularly conversational ambiguity in dialogue-driven control. To address this gap, we propose a multimodal data collection framework that employs a dialogue-based interaction protocol and a two-room Wizard-of-Oz (WoZ) setup to simulate robot autonomy while eliciting natural user behavior. The framework records five synchronized modalities: RGB-D video, conversational audio, inertial measurement unit (IMU) signals, end-effector Cartesian pose, and whole-body joint states across five assistive tasks. Using this framework, we collected a pilot dataset of 53 trials from five participants and validated its quality through motion smoothness analysis and user feedback. The results show that the framework effectively captures diverse ambiguity types and supports natural dialogue-driven interaction, demonstrating its suitability for scaling to a larger dataset for learning, benchmarking, and evaluation of ambiguity-aware assistive control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向对话驱动辅助机器人歧义澄清的多模态数据采集框架：一项Wizard-of-Oz试点研究</div>
<div class="mono" style="margin-top:8px">轮椅与轮椅搭载机械臂的集成控制具有显著提升重度运动功能障碍用户独立性的潜力，但现有交互界面常缺乏直观辅助交互所需的灵活性。尽管数据驱动的AI方法前景广阔，但缺乏捕捉自然人机交互（特别是对话驱动控制中的会话歧义）的多模态数据集限制了研究进展。为此，我们提出一种采用基于对话的交互协议和双房间Wizard-of-Oz实验设置的多模态数据采集框架，在模拟机器人自主性的同时激发用户自然行为。该框架同步记录五种模态数据：RGB-D视频、会话音频、惯性测量单元信号、末端执行器笛卡尔位姿以及覆盖五项辅助任务的全身关节状态。基于此框架，我们采集了五名参与者共计53次试验的试点数据集，并通过运动平滑度分析与用户反馈验证了数据质量。结果表明，该框架能有效捕获多种歧义类型并支持自然的对话驱动交互，证明其具备扩展为大规模数据集的潜力，适用于歧义感知辅助控制的学习、基准测试与评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To advance dialogue-driven assistive robotics, this research addresses the lack of multimodal datasets capturing natural human-robot interaction, especially conversational ambiguities, which limits data-driven AI methods for integrated wheelchair and robotic arm control. The authors propose a data collection framework using a dialogue-based protocol and a two-room Wizard-of-Oz setup to simulate autonomy, recording five synchronized modalities including RGB-D video, audio, IMU signals, end-effector pose, and joint states across five assistive tasks. In a pilot study with five participants and 53 trials, motion smoothness analysis and user feedback validated the framework&#x27;s effectiveness in capturing diverse ambiguity types and supporting natural dialogue-driven interaction, demonstrating its suitability for scaling to larger datasets for learning and evaluation.</div>
<div class="mono" style="margin-top:8px">为推进对话驱动的辅助机器人技术，本研究针对缺乏捕捉自然人机交互（特别是对话歧义）的多模态数据集问题，该问题限制了数据驱动AI方法在集成轮椅和机械臂控制中的应用。作者提出了一种数据收集框架，采用基于对话的交互协议和双房间 Wizard-of-Oz 设置来模拟自主性，在五项辅助任务中同步记录五种模态，包括RGB-D视频、对话音频、IMU信号、末端执行器位姿和全身关节状态。在一项涉及五名参与者和53次试验的试点研究中，通过运动平滑度分析和用户反馈验证了该框架能有效捕捉多种歧义类型并支持自然的对话驱动交互，证明了其适用于扩展至更大规模数据集以进行学习和评估。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting Deep Reinforcement Learning with Semantic Knowledge for Robotic Manipulators</div>
<div class="meta-line">Authors: Lucía Güitta-López, Vincenzo Suriani, Jaime Boal, Álvaro J. López-López, Daniele Nardi</div>
<div class="meta-line">Venue: Robotics, published 24 June 2025</div>
<div class="meta-line">First: 2026-01-23T16:14:28+00:00 · Latest: 2026-01-23T16:14:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16866v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16866v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Reinforcement Learning (DRL) is a powerful framework for solving complex sequential decision-making problems, particularly in robotic control. However, its practical deployment is often hindered by the substantial amount of experience required for learning, which results in high computational and time costs. In this work, we propose a novel integration of DRL with semantic knowledge in the form of Knowledge Graph Embeddings (KGEs), aiming to enhance learning efficiency by providing contextual information to the agent. Our architecture combines KGEs with visual observations, enabling the agent to exploit environmental knowledge during training. Experimental validation with robotic manipulators in environments featuring both fixed and randomized target attributes demonstrates that our method achieves up to {60}{\%} reduction in learning time and improves task accuracy by approximately 15 percentage points, without increasing training time or computational complexity. These results highlight the potential of semantic knowledge to reduce sample complexity and improve the effectiveness of DRL in robotic applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>融合语义知识提升机器人操作臂深度强化学习性能</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）是解决复杂序列决策问题的强大框架，尤其在机器人控制领域。然而，其实际应用常因学习所需的大量经验而受限，导致高昂的计算与时间成本。本研究提出一种将DRL与知识图谱嵌入（KGEs）形式语义知识相结合的新方法，旨在通过为智能体提供上下文信息来提升学习效率。该架构将KGEs与视觉观测融合，使智能体能在训练中利用环境知识。在包含固定与随机目标属性的环境中对机器人操作臂进行实验验证，结果表明：该方法可减少高达60%的学习时间，任务准确率提升约15个百分点，且未增加训练时间或计算复杂度。这些发现凸显了语义知识在降低样本复杂度、提升DRL机器人应用效能方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deep Reinforcement Learning (DRL) for robotic control faces challenges due to high sample complexity, leading to substantial computational and time costs during training. To address this, the authors propose a novel method that integrates semantic knowledge, specifically Knowledge Graph Embeddings (KGEs), with visual observations to provide contextual information to the DRL agent, thereby enhancing learning efficiency. Experimental results with robotic manipulators in environments with fixed and randomized target attributes show that this approach reduces learning time by up to 60% and improves task accuracy by approximately 15 percentage points, without increasing training time or computational complexity.</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）在机器人控制中应用时，因样本复杂度高而面临训练计算和时间成本巨大的挑战。为此，本研究提出一种新方法，将知识图谱嵌入（KGEs）形式的语义知识与视觉观察相结合，为智能体提供环境上下文信息以提高学习效率。在具有固定和随机目标属性的机器人操作器环境中进行的实验验证表明，该方法能将学习时间减少高达60%，并将任务准确率提升约15个百分点，且未增加训练时间或计算复杂度。</div>
</details>
</div>
<div class="card">
<div class="title">DAVOS: An Autonomous Vehicle Operating System in the Vehicle Computing Era</div>
<div class="meta-line">Authors: Yuxin Wang, Yuankai He, Boyang Tian, Lichen Xian, Weisong Shi</div>
<div class="meta-line">First: 2026-01-08T16:17:48+00:00 · Latest: 2026-01-23T16:11:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05072v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.05072v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vehicle computing represents a fundamental shift in how autonomous vehicles are designed and deployed, transforming them from isolated transportation systems into mobile computing platforms that support both safety-critical, real-time driving and data-centric services. In this setting, vehicles simultaneously support real-time driving pipelines and a growing set of data-driven applications, placing increased responsibility on the vehicle operating system to coordinate computation, data movement, storage, and access. These demands highlight recurring system considerations related to predictable execution, data and execution protection, efficient handling of high-rate sensor data, and long-term system evolvability, commonly summarized as Safety, Security, Efficiency, and Extensibility (SSEE). Existing vehicle operating systems and runtimes address these concerns in isolation, resulting in fragmented software stacks that limit coordination between autonomy workloads and vehicle data services. This paper presents DAVOS, the Dependable Autonomous Vehicle Operating System, a unified vehicle operating system architecture designed for the vehicle computing context. DAVOS provides a cohesive operating system foundation that supports both real-time autonomy and extensible vehicle computing within a single system framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DAVOS：车辆计算时代的自动驾驶汽车操作系统</div>
<div class="mono" style="margin-top:8px">车辆计算标志着自动驾驶汽车设计与部署的根本性转变，将其从孤立的交通系统转变为支持安全关键型实时驾驶与数据驱动服务的移动计算平台。在此背景下，车辆需同时支持实时驾驶流水线和日益增长的数据驱动应用，这对车辆操作系统在协调计算、数据移动、存储和访问方面提出了更高要求。这些需求凸显出与可预测执行、数据与执行保护、高速率传感器数据高效处理以及长期系统可演进性相关的系统性考量，通常概括为安全性、防护性、高效性与可扩展性（SSEE）。现有车辆操作系统和运行时环境孤立地处理这些问题，导致软件栈碎片化，限制了自动驾驶任务与车辆数据服务间的协调。本文提出DAVOS（可靠自动驾驶汽车操作系统），这是一种为车辆计算场景设计的统一车辆操作系统架构。DAVOS在单一系统框架内提供统一的操作系统基础，同时支持实时自动驾驶和可扩展的车辆计算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The emergence of vehicle computing, which transforms autonomous vehicles into mobile computing platforms, necessitates an operating system that can simultaneously support safety-critical real-time driving and diverse data services, addressing the core requirements of Safety, Security, Efficiency, and Extensibility (SSEE). To overcome the fragmentation of existing solutions, this paper introduces DAVOS, a unified operating system architecture designed to provide a cohesive foundation that integrates real-time autonomy pipelines with extensible vehicle computing within a single framework.</div>
<div class="mono" style="margin-top:8px">该研究的动机是自动驾驶车辆正转变为移动计算平台，需同时处理安全关键的实时驾驶和多样化的数据服务，这要求一个统一的操作系统来解决安全性、安全性、效率和可扩展性（SSEE）问题。方法提出了DAVOS（可靠自动驾驶车辆操作系统），它提供了一个将实时自主管道与可扩展车辆计算集成在单一框架内的统一架构。主要实验结果表明，DAVOS能有效协调计算、数据移动、存储和访问，克服了现有系统的碎片化问题，支持确定性执行和数据驱动应用。</div>
</details>
</div>
<div class="card">
<div class="title">Collision-Free Humanoid Traversal in Cluttered Indoor Scenes</div>
<div class="meta-line">Authors: Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu, Yunrui Lian, Jilong Wang, Qingtao Liu, Xuesong Shi, Li Yi</div>
<div class="meta-line">First: 2026-01-22T15:08:53+00:00 · Latest: 2026-01-23T14:57:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16035v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16035v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://axian12138.github.io/CAT/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>杂乱室内场景中的无碰撞人形机器人穿越</div>
<div class="mono" style="margin-top:8px">本研究探讨了人形机器人在杂乱室内场景（如跨越地面散落物体、蹲伏通过低矮障碍或挤过狭窄通道）中实现无碰撞穿越的问题。为实现这一目标，机器人需将感知到的具有多样空间布局与几何形态的周围障碍物映射至相应的穿越技能。然而，由于缺乏能有效捕捉避障过程中人形与障碍物关系的表征，直接学习此类映射较为困难。为此，我们提出人形势场（HumanoidPF），将此类关系编码为无碰撞运动方向，显著促进了基于强化学习的穿越技能学习。我们还发现，HumanoidPF作为一种感知表征展现出可忽略的仿真到现实差距。为进一步实现跨多样复杂杂乱室内场景的泛化穿越能力，我们提出一种混合场景生成方法，融合真实三维室内场景片段与程序化合成的障碍物。我们成功将策略迁移至现实世界，并开发了遥操作系统，用户仅需点击即可指挥人形机器人在杂乱室内场景中穿越。通过大量仿真与现实实验验证了方法的有效性。演示与代码详见项目网站：https://axian12138.github.io/CAT/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling humanoid robots to navigate cluttered indoor environments by hurdling, crouching, or squeezing through obstacles. The core method is the Humanoid Potential Field (HumanoidPF), a novel representation that encodes humanoid-obstacle relationships as collision-free motion directions to facilitate reinforcement learning of traversal skills. Experimental results demonstrate that policies learned with HumanoidPF successfully transfer to a real humanoid with a negligible sim-to-real gap, and a hybrid scene generation method enables generalization across diverse cluttered scenes, validated by a functional teleoperation system requiring only a single user click.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决人形机器人在杂乱室内环境中导航的挑战，例如跨越地面物体、蹲伏通过低矮障碍或挤过狭窄通道。其核心方法是提出人形势场（HumanoidPF），该表示将人形与障碍物的关系编码为无碰撞运动方向，从而促进基于强化学习的穿越技能学习。实验结果表明，基于HumanoidPF学习的策略能够以极小的仿真到现实差距成功迁移到真实人形机器人上，并且一种结合真实场景与程序化障碍的混合场景生成方法使技能能泛化到多样化的杂乱场景中，最终实现了一个可通过单次点击进行穿越指令的遥操作系统。</div>
</details>
</div>
<div class="card">
<div class="title">An Efficient Insect-inspired Approach for Visual Point-goal Navigation</div>
<div class="meta-line">Authors: Lu Yihe, Barbara Webb</div>
<div class="meta-line">First: 2026-01-23T14:57:04+00:00 · Latest: 2026-01-23T14:57:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16806v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16806v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work we develop a novel insect-inspired agent for visual point-goal navigation. This combines abstracted models of two insect brain structures that have been implicated, respectively, in associative learning and path integration. We draw an analogy between the formal benchmark of the Habitat point-goal navigation task and the ability of insects to learn and refine visually guided paths around obstacles between a discovered food location and their nest. We demonstrate that the simple insect-inspired agent exhibits performance comparable to recent SOTA models at many orders of magnitude less computational cost. Testing in a more realistic simulated environment shows the approach is robust to perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种高效的昆虫启发的视觉点目标导航方法</div>
<div class="mono" style="margin-top:8px">本研究开发了一种新颖的昆虫启发式智能体，用于视觉点目标导航。该方法融合了昆虫大脑中两个分别与联想学习和路径整合相关的抽象结构模型。我们将Habitat点目标导航任务的基准测试与昆虫在已发现食物位置与巢穴间学习并优化避障视觉引导路径的能力进行类比。结果表明，这种简单的昆虫启发式智能体在计算成本降低多个数量级的情况下，性能与近期SOTA模型相当。在更真实的模拟环境测试中，该方法展现出对扰动的强鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop a computationally efficient agent for visual point-goal navigation by drawing inspiration from insect brains, which excel at learning and navigating between locations like food and nest. The method abstracts models of two insect brain structures involved in associative learning and path integration, framing the Habitat navigation benchmark as analogous to an insect&#x27;s obstacle-avoiding route refinement. Experimental results show this simple agent achieves performance comparable to recent state-of-the-art models while requiring many orders of magnitude less computation, and it remains robust to perturbations in more realistic simulated environments.</div>
<div class="mono" style="margin-top:8px">该研究受昆虫在发现食物位置与巢穴之间学习并优化避障视觉路径的能力启发，旨在开发高效的视觉导航智能体。方法通过抽象昆虫大脑中分别负责联想学习和路径整合的两个结构模型，构建了一种新型智能体，并将其应用于Habitat点目标导航基准任务。实验结果表明，这种简单的仿昆虫智能体在计算成本降低多个数量级的情况下，性能与近期最先进模型相当，且在更真实的模拟环境中表现出对扰动的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">A Feature Extraction Pipeline for Enhancing Lightweight Neural Networks in sEMG-based Joint Torque Estimation</div>
<div class="meta-line">Authors: Kartik Chari, Raid Dokhan, Anas Homsi, Niklas Kueper, Elsa Andrea Kirchner</div>
<div class="meta-line">First: 2026-01-23T12:59:39+00:00 · Latest: 2026-01-23T12:59:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16712v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot-assisted rehabilitation offers an effective approach, wherein exoskeletons adapt to users&#x27; needs and provide personalized assistance. However, to deliver such assistance, accurate prediction of the user&#x27;s joint torques is essential. In this work, we propose a feature extraction pipeline using 8-channel surface electromyography (sEMG) signals to predict elbow and shoulder joint torques. For preliminary evaluation, this pipeline was integrated into two neural network models: the Multilayer Perceptron (MLP) and the Temporal Convolutional Network (TCN). Data were collected from a single subject performing elbow and shoulder movements under three load conditions (0 kg, 1.10 kg, and 1.85 kg) using three motion-capture cameras. Reference torques were estimated from center-of-mass kinematics under the assumption of static equilibrium. Our offline analyses showed that, with our feature extraction pipeline, MLP model achieved mean RMSE of 0.963 N m, 1.403 N m, and 1.434 N m (over five seeds) for elbow, front-shoulder, and side-shoulder joints, respectively, which were comparable to the TCN performance. These results demonstrate that the proposed feature extraction pipeline enables a simple MLP to achieve performance comparable to that of a network designed explicitly for temporal dependencies. This finding is particularly relevant for applications with limited training data, a common scenario patient care.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向基于表面肌电信号的关节力矩估计的轻量化神经网络特征提取流程</div>
<div class="mono" style="margin-top:8px">机器人辅助康复提供了一种有效方法，其中外骨骼可根据用户需求调整并提供个性化辅助。然而，为实现此类辅助，准确预测用户的关节力矩至关重要。本研究提出一种利用8通道表面肌电信号预测肘关节与肩关节力矩的特征提取流程。在初步评估中，该流程被集成至多层感知机与时序卷积网络两种神经网络模型。数据采集自单名受试者在三种负载条件下执行肘肩部动作，并通过三台运动捕捉相机记录。参考力矩基于静态平衡假设通过质心运动学估算得出。离线分析表明：采用本特征提取流程后，MLP模型在肘关节、前肩关节与侧肩关节的均方根误差均值分别为0.963牛·米、1.403牛·米与1.434牛·米，其时序性能与TCN模型相当。这些结果证明，所提特征提取流程能使简单的MLP达到专为时序依赖设计的网络性能，该发现在训练数据有限的临床应用场景中具有特殊意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to improve the accuracy of joint torque estimation for personalized robot-assisted rehabilitation, where precise prediction is essential for adaptive exoskeleton control. The method introduces a feature extraction pipeline for 8-channel surface electromyography (sEMG) signals, which is integrated into two neural network models—a Multilayer Perceptron (MLP) and a Temporal Convolutional Network (TCN)—to predict elbow and shoulder torques from data collected under varying load conditions. Experimental results from offline analysis show that with this pipeline, the simpler MLP model achieved mean RMSE values of 0.963 N m, 1.403 N m, and 1.434 N m for the elbow, front-shoulder, and side-shoulder joints, respectively, performing comparably to the temporally-aware TCN, thereby demonstrating the pipeline&#x27;s effectiveness in enhancing lightweight models especially in data-limited scenarios like patient care.</div>
<div class="mono" style="margin-top:8px">本研究旨在提高关节扭矩估计的准确性，以实现个性化的机器人辅助康复，其中精确预测对于外骨骼自适应辅助至关重要。方法提出了一种针对8通道表面肌电信号的特征提取流程，并将其与多层感知机和时序卷积网络两种神经网络模型集成，利用单名受试者在不同负载下执行肘部和肩部运动的动作捕捉数据进行评估。离线实验结果表明，通过该流程增强的多层感知机模型，在肘关节、前肩关节和侧肩关节上的平均均方根误差分别为0.963牛·米、1.403牛·米和1.434牛·米，与时序卷积网络性能相当，这表明特征提取使简单模型能够达到专为时序依赖设计的网络性能，这对于患者护理等数据有限的场景尤为有利。</div>
</details>
</div>
<div class="card">
<div class="title">Creating a biologically more accurate spider robot to study active vibration sensing</div>
<div class="meta-line">Authors: Siyuan Sun, Eugene H. Lin, Nathan Brown, Hsin-Yi Hung, Andrew Gordus, Jochen Mueller, Chen Li</div>
<div class="meta-line">First: 2026-01-23T12:10:20+00:00 · Latest: 2026-01-23T12:10:20+00:00</div>
<div class="meta-line">Comments: 8 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16691v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16691v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Orb-weaving spiders detect prey on a web using vibration sensors at leg joints. They often dynamically crouch their legs during prey sensing, likely an active sensing strategy. However, how leg crouching enhances sensing is poorly understood, because measuring system vibrations in behaving animals is difficult. We use robophysical modeling to study this problem. Our previous spider robot had only four legs, simplified leg morphology, and a shallow crouching range of motion. Here, we developed a new spider robot, with eight legs, each with four joints that better approximated spider leg morphology. Leg exoskeletons were 3-D printed and joint stiffness was tuned using integrated silicone molding with variable materials and geometry. Tendon-driven actuation allowed a motor in the body to crouch all eight legs deeply as spiders do, while accelerometers at leg joints record leg vibrations. Experiments showed that our new spider robot reproduced key vibration features observed in the previous robot while improving biological accuracy. Our new robot provides a biologically more accurate robophysical model for studying how leg behaviors modulate vibration sensing on a web.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>构建生物拟真度更高的蜘蛛机器人以研究主动振动感知机制</div>
<div class="mono" style="margin-top:8px">圆网蜘蛛通过腿关节处的振动传感器探测网上猎物，常在感知过程中动态蜷缩腿部，这可能是其主动感知策略。然而，由于难以在活体动物行为中测量系统振动，腿部蜷缩如何增强感知尚不明确。本研究采用机器人物理建模方法探究该问题。先前开发的蜘蛛机器人仅具四足、腿部形态简化且蜷缩运动范围有限。本文研制了一款新型蜘蛛机器人，配备八条各含四个关节的拟真腿部结构。腿部外骨骼通过3D打印制造，关节刚度则采用可变材料与几何形状的集成硅胶模塑技术进行调节。肌腱驱动机构使体内单个电机能如真实蜘蛛般深度蜷缩全部八足，同时腿关节处的加速度计记录振动数据。实验表明，新机器人不仅复现了先前模型的关键振动特征，更显著提升了生物拟真度。该机器人为研究蜘蛛腿部行为如何调控网上振动感知提供了生物拟真度更高的机器人物理模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To investigate how orb-weaving spiders use leg crouching as an active strategy to enhance vibration-based prey detection on webs—a process difficult to measure in live animals—this study employs robophysical modeling. The researchers developed a new eight-legged robot with four-joint limbs that better approximate spider leg morphology, using 3D-printed exoskeletons and silicone-molded joints with tunable stiffness, along with tendon-driven actuation to enable deep leg crouching and accelerometers to record vibrations. Experimental results demonstrated that this robot reproduced key vibration features from prior models while achieving greater biological accuracy, providing an improved platform for studying how leg behaviors modulate vibration sensing.</div>
<div class="mono" style="margin-top:8px">为研究圆网蜘蛛如何通过主动蜷腿行为增强猎物检测能力——这一行为因活体振动测量困难而未被充分理解，本研究采用机器人物理建模方法。研究人员开发了一种新的八足蜘蛛机器人，每条腿具有四个关节以更好地模拟生物形态，使用3D打印外骨骼和硅胶模塑可调刚度关节，结合肌腱驱动实现深度蜷腿，并通过加速度计记录振动。实验结果表明，该机器人再现了先前模型的关键振动特征，同时提高了生物准确性，为研究腿部行为如何调节网上振动感知提供了更精确的平台。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Reinforcement and Model Predictive Control Switching for Safe Human-Robot Cooperative Navigation</div>
<div class="meta-line">Authors: Ning Liu, Sen Shen, Zheng Li, Matthew D&#x27;Souza, Jen Jen Chung, Thomas Braunl</div>
<div class="meta-line">First: 2026-01-23T12:02:18+00:00 · Latest: 2026-01-23T12:02:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16686v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16686v1">PDF</a> · <a href="https://github.com/21ning/ARMS.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses the challenge of human-guided navigation for mobile collaborative robots under simultaneous proximity regulation and safety constraints. We introduce Adaptive Reinforcement and Model Predictive Control Switching (ARMS), a hybrid learning-control framework that integrates a reinforcement learning follower trained with Proximal Policy Optimization (PPO) and an analytical one-step Model Predictive Control (MPC) formulated as a quadratic program safety filter. To enable robust perception under partial observability and non-stationary human motion, ARMS employs a decoupled sensing architecture with a Long Short-Term Memory (LSTM) temporal encoder for the human-robot relative state and a spatial encoder for 360-degree LiDAR scans. The core contribution is a learned adaptive neural switcher that performs context-aware soft action fusion between the two controllers, favoring conservative, constraint-aware QP-based control in low-risk regions while progressively shifting control authority to the learned follower in highly cluttered or constrained scenarios where maneuverability is critical, and reverting to the follower action when the QP becomes infeasible. Extensive evaluations against Pure Pursuit, Dynamic Window Approach (DWA), and an RL-only baseline demonstrate that ARMS achieves an 82.5 percent success rate in highly cluttered environments, outperforming DWA and RL-only approaches by 7.1 percent and 3.1 percent, respectively, while reducing average computational latency by 33 percent to 5.2 milliseconds compared to a multi-step MPC baseline. Additional simulation transfer in Gazebo and initial real-world deployment results further indicate the practicality and robustness of ARMS for safe and efficient human-robot collaboration. Source code and a demonstration video are available at https://github.com/21ning/ARMS.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向安全人机协作导航的自适应强化学习与模型预测控制切换方法</div>
<div class="mono" style="margin-top:8px">本文针对移动协作机器人在同时满足邻近度调节与安全约束条件下的人导导航难题，提出自适应强化学习与模型预测控制切换框架。该混合学习控制框架整合了基于近端策略优化训练的强化学习跟随器，以及构建为二次规划安全滤波器的解析式单步模型预测控制器。为在部分可观测与非平稳人体运动条件下实现鲁棒感知，本框架采用解耦传感架构：利用长短期记忆时序编码器处理人机相对状态，并采用空间编码器处理360度激光雷达扫描数据。核心贡献在于通过学习的自适应神经切换器，实现两控制器间的上下文感知软动作融合——在低风险区域优先采用基于二次规划的保守约束感知控制，而在机动性至关重要的高度杂乱或受限场景中，逐步将控制权转移至学习型跟随器；当二次规划不可行时则切换回跟随器动作。与纯追踪法、动态窗口法及纯强化学习基线的对比实验表明，本框架在高度杂乱环境中达成82.5%的成功率，较动态窗口法和纯强化学习方法分别提升7.1%和3.1%，同时相比多步模型预测控制基线将平均计算延迟降低33%至5.2毫秒。Gazebo仿真迁移及初步实际部署结果进一步验证了该框架在安全高效人机协作中的实用性与鲁棒性。源代码与演示视频详见：https://github.com/21ning/ARMS.git。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enable safe and efficient human-guided navigation for mobile collaborative robots, which must operate under proximity and safety constraints. The proposed Adaptive Reinforcement and Model Predictive Control Switching (ARMS) framework integrates a reinforcement learning follower trained with Proximal Policy Optimization and a one-step Model Predictive Control safety filter, with a learned neural switcher that contextually fuses their actions. Experimental results show ARMS achieves an 82.5% success rate in highly cluttered environments, outperforming baseline methods by 3.1% to 7.1%, while reducing computational latency by 33% to 5.2 milliseconds compared to a multi-step MPC approach.</div>
<div class="mono" style="margin-top:8px">本研究旨在实现移动协作机器人在接近性和安全性约束下安全高效的人导导航。所提出的方法——自适应强化学习与模型预测控制切换（ARMS）——是一个混合框架，它结合了使用近端策略优化训练的强化学习跟随器和一个单步模型预测控制安全滤波器，并通过一个学习的神经切换器根据上下文融合两者的动作。实验结果表明，在高度杂乱的环境中，ARMS实现了82.5%的成功率，优于基线方法，并且与多步模型预测控制方法相比，计算延迟降低了33%，仿真和初步实际部署测试进一步证实了其实用性。</div>
</details>
</div>
<div class="card">
<div class="title">XR$^3$: An Extended Reality Platform for Social-Physical Human-Robot Interaction</div>
<div class="meta-line">Authors: Chao Wang, Anna Belardinelli, Michael Gienger</div>
<div class="meta-line">First: 2026-01-18T13:11:41+00:00 · Latest: 2026-01-23T11:54:41+00:00</div>
<div class="meta-line">Comments: 7 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12395v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.12395v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Social-physical human-robot interaction (spHRI) is difficult to study: building and programming robots that integrate multiple interaction modalities is costly and slow, while VR-based prototypes often lack physical contact, breaking users&#x27; visuo-tactile expectations. We present XR$^3$, a co-located dual-VR-headset platform for HRI research in which an attendee and a hidden operator share the same physical space while experiencing different virtual embodiments. The attendee sees an expressive virtual robot that interacts face-to-face in a shared virtual environment. In real time, the robot&#x27;s upper-body motion, head and gaze behavior, and facial expressions are mapped from the operator&#x27;s tracked limbs and face signals. Because the operator is co-present and calibrated in the same coordinate frame, the operator can also touch the attendee, enabling perceived robot touch synchronized with the robot&#x27;s visible hands. Finger and hand motion is mapped to the robot avatar using inverse kinematics to support precise contact. Beyond motion retargeting, XR$^3$ supports social retargeting of multiple nonverbal cues that can be experimentally varied while keeping physical interaction constant. We detail the system design and calibration, and demonstrate the platform in a touch-based Wizard-of-Oz study, lowering the barrier to prototyping and evaluating embodied, contact-based robot behaviors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XR$^3$：面向社会-物理人机交互的扩展现实平台</div>
<div class="mono" style="margin-top:8px">社会-物理人机交互（spHRI）的研究面临挑战：构建集成多模态交互的机器人成本高、周期长，而基于VR的原型常缺乏物理接触，破坏用户的视觉-触觉预期。本文提出XR$^3$——一种共定位双VR头显平台，支持参与者与隐藏操作员共享物理空间但体验不同虚拟化身。参与者可与具表现力的虚拟机器人在共享虚拟环境中面对面交互。机器人的上半身运动、头部注视行为及面部表情实时映射自操作员的肢体追踪与面部信号。由于操作员共处同一校准坐标系，其可直接触碰参与者，实现机器人可见手部动作与感知触觉的同步。通过逆运动学将手指与手部动作映射至机器人化身，以支持精确接触。除运动重定向外，XR$^3$支持多通道非语言线索的社会重定向，可在保持物理交互恒定的条件下进行实验调控。本文详述系统设计与校准流程，并通过基于触觉的“绿野仙踪”研究展示该平台如何降低具身化接触式机器人行为的原型开发与评估门槛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges in studying social-physical human-robot interaction (spHRI), where building physical robots is costly and VR prototypes often lack realistic physical contact. The authors introduce XR³, a platform using co-located dual VR headsets where an attendee interacts with a virtual robot whose upper-body motion, gaze, and facial expressions are mapped in real-time from a hidden operator&#x27;s tracked movements; the shared physical space allows the operator to touch the attendee, enabling synchronized robot touch via inverse kinematics for precise contact. In a touch-based Wizard-of-Oz study, the platform demonstrated effective prototyping of embodied, contact-based robot behaviors, successfully integrating multiple nonverbal cues while maintaining physical interaction.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决社交-物理人机交互研究中的挑战，即构建实体机器人成本高昂，而虚拟现实原型又常缺乏真实的物理接触。作者提出了XR³平台，该平台使用两个共置的VR头显：参与者与一个虚拟机器人互动，其上半身运动、视线和面部表情由隐藏的人类操作者的追踪动作实时映射生成；操作者共享同一物理空间，还能触摸参与者，通过逆运动学实现同步的机器人触摸感知。在一项基于触摸的“绿野仙踪”式研究中，该平台有效演示了具身化、基于接触的机器人行为原型，成功整合了多种非语言线索和物理交互。</div>
</details>
</div>
<div class="card">
<div class="title">Sim-to-Real Transfer via a Style-Identified Cycle Consistent Generative Adversarial Network: Zero-Shot Deployment on Robotic Manipulators through Visual Domain Adaptation</div>
<div class="meta-line">Authors: Lucía Güitta-López, Lionel Güitta-López, Jaime Boal, Álvaro Jesús López-López</div>
<div class="meta-line">Venue: Engineering Applications of Artificial Intelligence, volume 159, published Jan.2026</div>
<div class="meta-line">First: 2026-01-23T11:48:15+00:00 · Latest: 2026-01-23T11:48:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16677v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16677v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The sample efficiency challenge in Deep Reinforcement Learning (DRL) compromises its industrial adoption due to the high cost and time demands of real-world training. Virtual environments offer a cost-effective alternative for training DRL agents, but the transfer of learned policies to real setups is hindered by the sim-to-real gap. Achieving zero-shot transfer, where agents perform directly in real environments without additional tuning, is particularly desirable for its efficiency and practical value. This work proposes a novel domain adaptation approach relying on a Style-Identified Cycle Consistent Generative Adversarial Network (StyleID-CycleGAN or SICGAN), an original Cycle Consistent Generative Adversarial Network (CycleGAN) based model. SICGAN translates raw virtual observations into real-synthetic images, creating a hybrid domain for training DRL agents that combines virtual dynamics with real-like visual inputs. Following virtual training, the agent can be directly deployed, bypassing the need for real-world training. The pipeline is validated with two distinct industrial robots in the approaching phase of a pick-and-place operation. In virtual environments agents achieve success rates of 90 to 100\%, and real-world deployment confirms robust zero-shot transfer (i.e., without additional training in the physical environment) with accuracies above 95\% for most workspace regions. We use augmented reality targets to improve the evaluation process efficiency, and experimentally demonstrate that the agent successfully generalizes to real objects of varying colors and shapes, including LEGO\textsuperscript{\textregistered}~cubes and a mug. These results establish the proposed pipeline as an efficient, scalable solution to the sim-to-real problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于风格识别循环一致生成对抗网络的仿真到现实迁移：通过视觉域适应实现机器人操作器的零样本部署</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）的样本效率问题因其在现实训练中的高成本与时间需求而限制了工业应用。虚拟环境为训练DRL智能体提供了经济高效的替代方案，但学习策略向现实场景的迁移受到仿真-现实差异的阻碍。零样本迁移——即智能体无需额外调优即可直接在现实环境中执行任务——因其效率与实用价值而备受期待。本研究提出一种基于风格识别循环一致生成对抗网络（StyleID-CycleGAN或SICGAN）的新型域适应方法，该模型是在原始循环一致生成对抗网络（CycleGAN）基础上的创新架构。SICGAN将原始虚拟观测数据转换为真实-合成图像，构建出融合虚拟动力学与类真实视觉输入的混合域，用于训练DRL智能体。经过虚拟训练后，智能体可直接部署，无需进行现实世界训练。该流程在拾放操作的接近阶段通过两种不同工业机器人得到验证：虚拟环境中智能体成功率可达90%至100%，现实部署证实了稳健的零样本迁移（即无需在物理环境中额外训练），在多数工作区域精度超过95%。本研究采用增强现实目标提升评估效率，并通过实验证明智能体能成功泛化至不同颜色与形状的真实物体（包括乐高®立方体和马克杯）。这些成果确立了该流程作为解决仿真-现实问题的高效可扩展方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deep Reinforcement Learning (DRL) faces a sample efficiency challenge that hinders industrial adoption due to the high cost of real-world training, motivating the need for effective sim-to-real transfer. The method introduces a Style-Identified Cycle Consistent Generative Adversarial Network (SICGAN) to translate virtual observations into real-synthetic images, creating a hybrid domain for training DRL agents that combines virtual dynamics with realistic visuals, enabling zero-shot deployment without real-world tuning. Experimental validation on industrial robots in pick-and-place tasks shows virtual success rates of 90-100% and real-world zero-shot transfer accuracies above 95% in most regions, with generalization to objects of varying colors and shapes.</div>
<div class="mono" style="margin-top:8px">深度强化学习（DRL）存在样本效率挑战，由于真实世界训练成本高昂而阻碍工业应用，这促使利用虚拟环境进行训练，但仿真到真实的差距阻碍了策略迁移。为实现零样本迁移，本研究提出一种基于风格识别循环一致生成对抗网络（SICGAN）的域适应方法，将原始虚拟观测转换为真实-合成图像，从而在融合虚拟动力学与类真实视觉的混合域中训练DRL智能体。在两个工业机器人执行抓放接近任务的实验验证中，虚拟训练成功率可达90-100%，真实世界部署实现了鲁棒的零样本迁移，在大部分工作区域准确率超过95%，并能成功泛化到不同颜色和形状的物体。</div>
</details>
</div>
<div class="card">
<div class="title">ReViP: Reducing False Completion in Vision-Language-Action Models with Vision-Proprioception Rebalance</div>
<div class="meta-line">Authors: Zhuohao Li, Yinghao Li, Jian-Jian Jiang, Lang Zhou, Tianyu Zhang, Wei-Shi Zheng</div>
<div class="meta-line">First: 2026-01-23T11:31:07+00:00 · Latest: 2026-01-23T11:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16667v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16667v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have advanced robotic manipulation by combining vision, language, and proprioception to predict actions. However, previous methods fuse proprioceptive signals directly with VLM-encoded vision-language features, resulting in state-dominant bias and false completions despite visible execution failures. We attribute this to modality imbalance, where policies over-rely on internal state while underusing visual evidence. To address this, we present ReViP, a novel VLA framework with Vision-Proprioception Rebalance to enhance visual grounding and robustness under perturbations. The key insight is to introduce auxiliary task-aware environment priors to adaptively modulate the coupling between semantic perception and proprioceptive dynamics. Specifically, we use an external VLM as a task-stage observer to extract real-time task-centric visual cues from visual observations, which drive a Vision-Proprioception Feature-wise Linear Modulation to enhance environmental awareness and reduce state-driven errors. Moreover, to evaluate false completion, we propose the first False-Completion Benchmark Suite built on LIBERO with controlled settings such as Object-Drop. Extensive experiments show that ReViP effectively reduces false-completion rates and improves success rates over strong VLA baselines on our suite, with gains extending to LIBERO, RoboTwin 2.0, and real-world evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReViP：通过视觉-本体感知再平衡减少视觉-语言-动作模型中的误完成</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型通过融合视觉、语言和本体感知来预测动作，推动了机器人操作的发展。然而，现有方法将本体感知信号直接与VLM编码的视觉-语言特征融合，导致状态主导偏差和误完成现象，即使执行失败已显而易见。我们将此归因于模态失衡——策略过度依赖内部状态而未能充分利用视觉证据。为此，我们提出ReViP，一种具有视觉-本体感知再平衡机制的新型VLA框架，旨在增强扰动下的视觉基础与鲁棒性。其核心思想是引入辅助任务感知的环境先验，自适应调节语义感知与本体感知动态的耦合。具体而言，我们使用外部VLM作为任务阶段观察器，从视觉观测中提取实时任务中心视觉线索，驱动视觉-本体感知特征级线性调制，以增强环境感知并减少状态驱动误差。此外，为评估误完成现象，我们基于LIBERO构建了首个误完成基准测试套件，包含物体掉落等受控设置。大量实验表明，ReViP在我们的测试套件上有效降低了误完成率并提升了成功率，其优势延伸至LIBERO、RoboTwin 2.0及真实场景评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models for robotic manipulation often suffer from false completions, where actions are incorrectly predicted as successful despite visible failures, due to an over-reliance on proprioceptive state over visual evidence. To address this modality imbalance, the proposed ReViP framework introduces task-aware environment priors, using an external VLM as a task-stage observer to extract real-time visual cues that modulate the fusion of semantic and proprioceptive features via Feature-wise Linear Modulation, thereby enhancing visual grounding. Experimental evaluation on a new False-Completion Benchmark Suite based on LIBERO, along with tests on RoboTwin 2.0 and real-world setups, demonstrates that ReViP significantly reduces false-completion rates and improves task success compared to existing VLA baselines.</div>
<div class="mono" style="margin-top:8px">用于机器人操作的视觉-语言-动作模型常因过度依赖本体感知状态而忽视视觉证据，导致出现虚假完成问题，即在执行明显失败时仍错误预测动作成功。为解决这种模态不平衡，提出的ReViP框架引入了任务感知的环境先验，利用外部VLM作为任务阶段观察器提取实时视觉线索，通过特征线性调制来调节视觉与本体感知的融合，从而增强视觉基础。在基于LIBERO构建的新型虚假完成基准测试套件上的实验评估，以及在RoboTwin 2.0和真实环境中的测试表明，与现有VLA基线相比，ReViP显著降低了虚假完成率并提高了任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">A Unified Calibration Framework for High-Accuracy Articulated Robot Kinematics</div>
<div class="meta-line">Authors: Philip Tobuschat, Simon Duenser, Markus Bambach, Ivo Aschwanden</div>
<div class="meta-line">First: 2026-01-23T10:51:51+00:00 · Latest: 2026-01-23T10:51:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16638v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16638v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Researchers have identified various sources of tool positioning errors for articulated industrial robots and have proposed dedicated compensation strategies. However, these typically require individual, specialized experiments with separate models and identification procedures. This article presents a unified approach to the static calibration of industrial robots that identifies a robot model, including geometric and non-geometric effects (compliant bending, thermal deformation, gear transmission errors), using only a single, straightforward experiment for data collection. The model augments the kinematic chain with virtual joints for each modeled effect and realizes the identification using Gauss-Newton optimization with analytic gradients. Fisher information spectra show that the estimation is well-conditioned and the parameterization near-minimal, whereas systematic temporal cross-validation and model ablations demonstrate robustness of the model identification. The resulting model is very accurate and its identification robust, achieving a mean position error of 26.8 $μm$ on a KUKA KR30 industrial robot compared to 102.3 $μm$ for purely geometric calibration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高精度关节机器人运动学统一标定框架</div>
<div class="mono" style="margin-top:8px">研究人员已识别出关节式工业机器人工具定位误差的多种来源，并提出了专门的补偿策略。然而，这些策略通常需要各自独立的专业实验、模型及辨识流程。本文提出了一种工业机器人静态标定的统一方法，仅通过一次简单的数据采集实验，即可辨识出包含几何与非几何效应（柔性弯曲、热变形、齿轮传动误差）的机器人模型。该模型通过为每种效应添加虚拟关节来扩展运动学链，并利用解析梯度的高斯-牛顿优化实现参数辨识。费舍尔信息谱表明估计条件良好且参数化接近最小化，而系统性的时间交叉验证与模型消融实验证明了模型辨识的鲁棒性。所得模型精度极高且辨识稳健，在KUKA KR30工业机器人上实现了平均位置误差26.8微米，而纯几何标定误差为102.3微米。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the complexity of separately calibrating various error sources in articulated industrial robots, this work proposes a unified calibration framework that simultaneously identifies geometric and non-geometric parameters, including compliant bending, thermal deformation, and gear transmission errors. The method augments the kinematic chain with virtual joints for each effect and employs Gauss-Newton optimization with analytic gradients for parameter identification, requiring only a single straightforward experiment for data collection. Experimental validation on a KUKA KR30 robot demonstrates the framework&#x27;s robustness and high accuracy, reducing the mean position error to 26.8 μm from 102.3 μm achieved by purely geometric calibration.</div>
<div class="mono" style="margin-top:8px">针对现有工业机器人标定方法需对不同误差源进行独立实验和建模的局限性，本研究提出了一种统一标定框架，可同时辨识几何与非几何误差。该方法通过在运动链中引入代表柔性弯曲、热变形等误差的虚拟关节，并利用解析梯度的高斯-牛顿优化算法，仅需一次简单实验的数据即可完成参数辨识。在KUKA KR30机器人上的实验验证表明，该框架具有强鲁棒性和高精度，将平均位置误差从纯几何标定的102.3 μm降低至26.8 μm。</div>
</details>
</div>
<div class="card">
<div class="title">HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV</div>
<div class="meta-line">Authors: Faryal Batool, Iana Zhura, Valerii Serpiva, Roohan Ahmed Khan, Ivan Valuev, Issatay Tokmurziyev, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-21T13:22:22+00:00 · Latest: 2026-01-23T10:01:25+00:00</div>
<div class="meta-line">Comments: This paper has been accepted at HRI, Late Breaking Report, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14973v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14973v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11 based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanDiffusion：基于视觉的扩散轨迹规划器，具备面向搜救无人机的人类条件化目标</div>
<div class="mono" style="margin-top:8px">紧急场景下可靠的人机协作需要自主系统能够检测人类、推断导航目标并在动态环境中安全运行。本文提出HumanDiffusion，一种轻量级的图像条件化扩散规划器，可直接从RGB图像生成人类感知的导航轨迹。该系统结合基于YOLO-11的人类检测与扩散驱动的轨迹生成，使四旋翼无人机能够接近目标人员并提供医疗援助，无需依赖先验地图或计算密集的规划流程。轨迹在像素空间中进行预测，确保运动平滑并在人类周围保持稳定的安全距离。我们在仿真和真实室内模拟灾难场景中对HumanDiffusion进行了评估。在300个样本的测试集上，该模型在像素空间轨迹重建中实现了0.02的均方误差。真实世界实验显示，在存在部分遮挡的事故响应与搜索定位任务中，整体任务成功率达到80%。这些结果表明，人类条件化扩散规划为时间紧迫的辅助场景中的人类感知无人机导航提供了一种实用且鲁棒的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enable reliable human-robot collaboration in emergencies by developing an autonomous UAV system for search and rescue that can detect humans, infer navigation goals, and operate safely. The proposed HumanDiffusion method is a lightweight, image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB images, combining YOLO-11 for human detection with diffusion-driven trajectory generation in pixel space to ensure smooth motion and safety margins without prior maps. Experimental evaluation in simulation and real-world indoor mock-disaster scenarios showed a mean squared error of 0.02 in pixel-space trajectory reconstruction on a 300-sample test set and an 80% overall mission success rate in tasks with partial occlusions, indicating the method&#x27;s practicality and robustness for time-critical assistance.</div>
<div class="mono" style="margin-top:8px">为了在紧急情况下实现可靠的人机协作，本研究旨在开发一种用于搜救的自主无人机系统，使其能够检测人类、推断目标并安全导航。所提出的HumanDiffusion方法是一种轻量级的、基于图像的扩散规划器，它通过结合YOLO-11进行人体检测和在像素空间中进行扩散驱动的轨迹生成，直接从RGB图像生成人类感知的导航轨迹，无需先验地图。在模拟和真实室内模拟灾难场景中的实验评估显示，在300个样本的测试集上，像素空间轨迹重建的平均平方误差为0.02，并且在涉及事故响应和存在遮挡的搜索任务中，整体任务成功率达到80%。</div>
</details>
</div>
<div class="card">
<div class="title">VL-LN Bench: Towards Long-horizon Goal-oriented Navigation with Active Dialogs</div>
<div class="meta-line">Authors: Wensi Huang, Shaohao Zhu, Meng Wei, Jinming Xu, Xihui Liu, Hanqing Wang, Tai Wang, Feng Zhao, Jiangmiao Pang</div>
<div class="meta-line">First: 2025-12-26T19:00:12+00:00 · Latest: 2026-01-23T09:50:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22342v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.22342v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://0309hws.github.io/VL-LN.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In most existing embodied navigation tasks, instructions are well-defined and unambiguous, such as instruction following and object searching. Under this idealized setting, agents are required solely to produce effective navigation outputs conditioned on vision and language inputs. However, real-world navigation instructions are often vague and ambiguous, requiring the agent to resolve uncertainty and infer user intent through active dialog. To address this gap, we propose Interactive Instance Goal Navigation (IIGN), a task that requires agents not only to generate navigation actions but also to produce language outputs via active dialog, thereby aligning more closely with practical settings. IIGN extends Instance Goal Navigation (IGN) by allowing agents to freely consult an oracle in natural language while navigating. Building on this task, we present the Vision Language-Language Navigation (VL-LN) benchmark, which provides a large-scale, automatically generated dataset and a comprehensive evaluation protocol for training and assessing dialog-enabled navigation models. VL-LN comprises over 41k long-horizon dialog-augmented trajectories for training and an automatic evaluation protocol with an oracle capable of responding to agent queries. Using this benchmark, we train a navigation model equipped with dialog capabilities and show that it achieves significant improvements over the baselines. Extensive experiments and analyses further demonstrate the effectiveness and reliability of VL-LN for advancing research on dialog-enabled embodied navigation. Code and dataset: https://0309hws.github.io/VL-LN.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VL-LN基准：面向长时程目标导航与主动对话的研究</div>
<div class="mono" style="margin-top:8px">现有具身导航任务中的指令通常明确且无歧义，如指令跟随与物体搜寻。在此理想化设定下，智能体仅需基于视觉与语言输入生成有效导航动作。然而，现实世界的导航指令常存在模糊性与歧义，要求智能体通过主动对话消除不确定性并推断用户意图。为填补这一空白，我们提出交互式实例目标导航任务，要求智能体不仅生成导航动作，还需通过主动对话产生语言输出，从而更贴近实际场景。该任务扩展了实例目标导航框架，允许智能体在导航过程中以自然语言自由咨询先知系统。基于此任务，我们构建了视觉语言-语言导航基准，提供大规模自动生成数据集及完整评估协议，用于训练和评估支持对话的导航模型。该基准包含超过4.1万条长时程对话增强轨迹用于训练，以及配备自动应答能力的评估协议。利用该基准，我们训练了具备对话功能的导航模型，实验表明其性能显著超越基线方法。大量实验与分析进一步验证了该基准在推动具身对话导航研究方面的有效性与可靠性。代码与数据集：https://0309hws.github.io/VL-LN.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the gap between idealized navigation tasks with unambiguous instructions and real-world scenarios where instructions are often vague, this paper introduces the Interactive Instance Goal Navigation (IIGN) task, which requires agents to navigate while actively engaging in dialog to resolve uncertainty and infer user intent. The authors propose the Vision Language-Language Navigation (VL-LN) benchmark, featuring a large-scale, automatically generated dataset of over 41k dialog-augmented trajectories and an evaluation protocol with an oracle for responding to queries. Experimental results show that a dialog-enabled navigation model trained on this benchmark achieves significant improvements over baselines, demonstrating the benchmark&#x27;s effectiveness for advancing research in dialog-enabled embodied navigation.</div>
<div class="mono" style="margin-top:8px">针对现有具身导航任务中指令明确而现实场景中指令常模糊不清的差距，本文提出了交互式实例目标导航（IIGN）任务，要求智能体在导航过程中通过主动对话来消除不确定性并推断用户意图。方法上构建了视觉语言-语言导航（VL-LN）基准，包含超过41k条自动生成的对话增强轨迹数据集以及一个带有应答查询的预言机的评估协议。实验结果表明，基于该基准训练的具备对话能力的导航模型相比基线有显著提升，验证了该基准在推动对话式具身导航研究方面的有效性和可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving</div>
<div class="meta-line">Authors: Lei Wan, Prabesh Gupta, Andreas Eich, Marcel Kettelgerdes, Hannan Ejaz Keen, Michael Klöppel-Gersdorf, Alexey Vinel</div>
<div class="meta-line">First: 2025-05-11T13:41:37+00:00 · Latest: 2026-01-23T09:48:28+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures, submitted to IEEE VNC</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.06980v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.06980v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable perception remains a key challenge for Connected Automated Vehicles (CAVs) in complex real-world environments, where varying lighting conditions and adverse weather degrade sensing performance. While existing multi-sensor solutions improve local robustness, they remain constrained by limited sensing range, line-of-sight occlusions, and sensor failures on individual vehicles. This paper introduces VALISENS, a validated cooperative perception system that extends multi-sensor fusion beyond a single vehicle through Vehicle-to-Everything (V2X)-enabled collaboration between Connected Automated Vehicles (CAVs) and intelligent infrastructure. VALISENS integrates onboard and roadside LiDARs, radars, RGB cameras, and thermal cameras within a unified multi-agent perception framework. Thermal cameras enhances the detection of Vulnerable Road Users (VRUs) under challenging lighting conditions, while roadside sensors reduce occlusions and expand the effective perception range. In addition, an integrated sensor monitoring module continuously assesses sensor health and detects anomalies before system degradation occurs. The proposed system is implemented and evaluated in a dedicated real-world testbed. Experimental results show that VALISENS improves pedestrian situational awareness by up to 18% compared with vehicle-only sensing, while the sensor monitoring module achieves over 97% accuracy, demonstrating its effectiveness and its potential to support future Cooperative Intelligent Transport Systems (C-ITS) applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VALISENS：一种经过验证的创新型多传感器协同自动驾驶系统</div>
<div class="mono" style="margin-top:8px">在复杂的现实环境中，可靠感知仍是网联自动驾驶车辆面临的关键挑战，多变的光照条件和恶劣天气会降低传感性能。现有多传感器方案虽提升了局部鲁棒性，但仍受限于感知范围有限、视线遮挡及单车传感器故障。本文提出VALISENS——一种经过验证的协同感知系统，通过网联自动驾驶车辆与智能基础设施间的车联网协同，将多传感器融合扩展至单车之外。该系统在统一的多智能体感知框架内集成车载与路侧激光雷达、毫米波雷达、RGB相机及热成像相机。热成像相机可提升复杂光照条件下弱势道路使用者的检测能力，路侧传感器则能减少遮挡并扩展有效感知范围。此外，集成的传感器监测模块可持续评估传感器状态，在系统性能下降前检测异常。本系统已在专用实景测试场中部署验证，实验表明：相比纯车载传感方案，VALISENS将行人态势感知能力提升达18%，传感器监测模块准确率超过97%，证明了其有效性及对未来协同智能交通系统应用的支撑潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of single-vehicle perception systems, such as limited range, occlusions, and sensor failures in complex environments, this paper proposes VALISENS, a cooperative perception system that fuses data from multiple sensors across connected vehicles and intelligent infrastructure via V2X communication. The method integrates LiDAR, radar, RGB, and thermal cameras within a multi-agent framework, enhanced by a sensor monitoring module for anomaly detection. Experimental evaluation in a real-world testbed demonstrates an up to 18% improvement in pedestrian situational awareness over vehicle-only sensing, with the monitoring module achieving over 97% accuracy in sensor health assessment.</div>
<div class="mono" style="margin-top:8px">为解决单车感知系统在复杂环境中存在的感知范围有限、视线遮挡和传感器故障等问题，本文提出了VALISENS协同感知系统，该系统通过车路协同（V2X）通信，融合来自联网车辆和智能基础设施的多种传感器数据。该方法在一个多智能体框架内集成激光雷达、毫米波雷达、RGB摄像头和热成像摄像头，利用热成像检测弱势道路使用者，并通过路侧单元减少遮挡，同时包含一个持续监测传感器健康状态的模块。在真实世界测试平台上的实验结果表明，与仅使用车载传感器相比，该系统对行人的态势感知能力提升了18%，且传感器监测模块的准确率超过97%，验证了其对未来协同智能交通系统应用的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-Shot MARL Benchmark in the Cyber-Physical Mobility Lab</div>
<div class="meta-line">Authors: Julius Beerwerth, Jianye Xu, Simon Schäfer, Fynn Belderink, Bassam Alrifaee</div>
<div class="meta-line">First: 2026-01-23T09:26:36+00:00 · Latest: 2026-01-23T09:26:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16578v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a reproducible benchmark for evaluating sim-to-real transfer of Multi-Agent Reinforcement Learning (MARL) policies for Connected and Automated Vehicles (CAVs). The platform, based on the Cyber-Physical Mobility Lab (CPM Lab) [1], integrates simulation, a high-fidelity digital twin, and a physical testbed, enabling structured zero-shot evaluation of MARL motion-planning policies. We demonstrate its use by deploying a SigmaRL-trained policy [2] across all three domains, revealing two complementary sources of performance degradation: architectural differences between simulation and hardware control stacks, and the sim-to-real gap induced by increasing environmental realism. The open-source setup enables systematic analysis of sim-to-real challenges in MARL under realistic, reproducible conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>网络物理移动实验室中的零样本多智能体强化学习基准</div>
<div class="mono" style="margin-top:8px">我们提出了一个可复现的基准，用于评估联网自动驾驶车辆多智能体强化学习策略的仿真到现实迁移。该平台基于网络物理移动实验室[1]，整合了仿真、高保真数字孪生和物理测试台，支持对多智能体强化学习运动规划策略进行结构化零样本评估。我们通过在所有三个领域部署SigmaRL训练的策略[2]来展示其应用，揭示了性能下降的两个互补来源：仿真与硬件控制栈之间的架构差异，以及环境真实性提升导致的仿真到现实差距。该开源平台能够在现实可复现条件下，系统分析多智能体强化学习中的仿真到现实挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the sim-to-real transfer challenge for Multi-Agent Reinforcement Learning (MARL) policies in Connected and Automated Vehicles (CAVs) by establishing a reproducible benchmark. The method utilizes the Cyber-Physical Mobility Lab platform, which integrates simulation, a digital twin, and a physical testbed to enable structured zero-shot evaluation of MARL motion-planning policies. Key experimental findings from deploying a SigmaRL-trained policy reveal two primary sources of performance degradation: architectural mismatches between simulation and hardware control stacks, and the sim-to-real gap that widens with increasing environmental realism.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决网联自动驾驶车辆多智能体强化学习策略从仿真到现实迁移的可复现评估需求。方法基于网络物理移动实验室，提出了一个集成仿真、高保真数字孪生和物理测试平台的基准系统，用于对多智能体强化学习运动规划策略进行结构化的零样本测试。通过部署SigmaRL训练的策略进行实验，主要发现性能下降源于两个互补因素：仿真与硬件控制栈之间的架构差异，以及环境真实性增加导致的仿真与现实差距。</div>
</details>
</div>
<div class="card">
<div class="title">CLASH: Collaborative Large-Small Hierarchical Framework for Continuous Vision-and-Language Navigation</div>
<div class="meta-line">Authors: Liuyi Wang, Zongtao He, Jinlong Li, Ruihao Xia, Mengxian Hu, Chenpeng Yao, Chengju Liu, Yang Tang, Qijun Chen</div>
<div class="meta-line">First: 2025-12-11T07:20:06+00:00 · Latest: 2026-01-23T08:54:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10360v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10360v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-and-Language Navigation (VLN) requires robots to follow natural language instructions and navigate complex environments without prior maps. While recent vision-language large models demonstrate strong reasoning abilities, they often underperform task-specific panoramic small models in VLN tasks. To address this, we propose CLASH (Collaborative Large-Small Hierarchy), a VLN-CE framework that integrates a reactive small-model planner (RSMP) with a reflective large-model reasoner (RLMR). RSMP adopts a causal-learning-based dual-branch architecture to enhance generalization, while RLMR leverages panoramic visual prompting with chain-of-thought reasoning to support interpretable spatial understanding and navigation. We further introduce an uncertainty-aware collaboration mechanism (UCM) that adaptively fuses decisions from both models. For obstacle avoidance, in simulation, we replace the rule-based controller with a fully learnable point-goal policy, and in real-world deployment, we design a LiDAR-based clustering module for generating navigable waypoints and pair it with an online SLAM-based local controller. CLASH achieves state-of-the-art (SoTA) results (ranking 1-st) on the VLN-CE leaderboard, significantly improving SR and SPL on the test-unseen set over the previous SoTA methods. Real-world experiments demonstrate CLASH&#x27;s strong robustness, validating its effectiveness in both simulation and deployment scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLASH：用于连续视觉语言导航的协作式大小模型分层框架</div>
<div class="mono" style="margin-top:8px">视觉语言导航（VLN）要求机器人依据自然语言指令在无先验地图的复杂环境中导航。尽管近期视觉语言大模型展现出强大的推理能力，但在VLN任务中常逊于专用全景小模型。为此，我们提出CLASH（协作式大小模型分层框架），这是一个VLN-CE框架，将反应式小模型规划器（RSMP）与反思式大模型推理器（RLMR）相结合。RSMP采用基于因果学习的双分支架构以增强泛化能力，而RLMR利用全景视觉提示与思维链推理，支持可解释的空间理解与导航。我们进一步引入不确定性感知协作机制（UCM），自适应融合双模型决策。针对避障任务，在仿真中我们以完全可学习的点目标策略替代基于规则的控制器，在实际部署中则设计了基于激光雷达的聚类模块以生成可通行路径点，并与基于在线SLAM的局部控制器协同工作。CLASH在VLN-CE榜单上取得最先进成果（排名第一），在未见测试集上的SR与SPL指标较先前最优方法显著提升。真实环境实验验证了CLASH的强大鲁棒性，证明了其在仿真与部署场景中的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the performance gap in Vision-and-Language Navigation (VLN) where large vision-language models, despite strong reasoning, underperform specialized small models. The proposed CLASH framework integrates a reactive small-model planner for generalization and a reflective large-model reasoner for interpretable spatial reasoning, coupled with an uncertainty-aware mechanism to fuse their decisions. Experiments show CLASH achieves state-of-the-art results on the VLN-CE benchmark and demonstrates strong robustness in real-world deployment.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言导航在连续环境（VLN-CE）中，视觉语言大模型与任务专用小模型之间的性能差距问题，其中大模型虽推理能力强但在导航任务中表现不佳。提出的CLASH框架整合了用于泛化的反应式小模型规划器和用于可解释空间理解的反思式大模型推理器，通过不确定性感知机制融合两者决策，并采用可学习策略进行避障。实验表明，CLASH在VLN-CE基准测试中取得了最先进的成果，并在真实世界部署中展现出强大的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">FantasyVLN: Unified Multimodal Chain-of-Thought Reasoning for Vision-Language Navigation</div>
<div class="meta-line">Authors: Jing Zuo, Lingzhou Mu, Fan Jiang, Chengcheng Ma, Mu Xu, Yonggang Qi</div>
<div class="meta-line">First: 2026-01-20T13:54:10+00:00 · Latest: 2026-01-23T08:44:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13976v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13976v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving human-level performance in Vision-and-Language Navigation (VLN) requires an embodied agent to jointly understand multimodal instructions and visual-spatial context while reasoning over long action sequences. Recent works, such as NavCoT and NavGPT-2, demonstrate the potential of Chain-of-Thought (CoT) reasoning for improving interpretability and long-horizon planning. Moreover, multimodal extensions like OctoNav-R1 and CoT-VLA further validate CoT as a promising pathway toward human-like navigation reasoning. However, existing approaches face critical drawbacks: purely textual CoTs lack spatial grounding and easily overfit to sparse annotated reasoning steps, while multimodal CoTs incur severe token inflation by generating imagined visual observations, making real-time navigation impractical. In this work, we propose FantasyVLN, a unified implicit reasoning framework that preserves the benefits of CoT reasoning without explicit token overhead. Specifically, imagined visual tokens are encoded into a compact latent space using a pretrained Visual AutoRegressor (VAR) during CoT reasoning training, and the model jointly learns from textual, visual, and multimodal CoT modes under a unified multi-CoT strategy. At inference, our model performs direct instruction-to-action mapping while still enjoying reasoning-aware representations. Extensive experiments on LH-VLN show that our approach achieves reasoning-aware yet real-time navigation, improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FantasyVLN：面向视觉语言导航的统一多模态思维链推理框架</div>
<div class="mono" style="margin-top:8px">在视觉语言导航任务中实现人类水平性能，需要智能体同时理解多模态指令与视觉空间语境，并对长序列动作进行推理。NavCoT、NavGPT-2等近期研究展现了思维链推理在提升可解释性与长程规划能力方面的潜力，OctoNav-R1、CoT-VLA等多模态扩展进一步验证了该技术是实现类人导航推理的有效路径。然而现有方法存在明显缺陷：纯文本思维链缺乏空间基础且易受稀疏标注推理步骤过拟合，而多模态思维链因生成虚拟视觉观测导致标记激增，难以实现实时导航。本研究提出FantasyVLN——一种保留思维链推理优势且无需显式标记开销的统一隐式推理框架。该框架通过预训练视觉自回归器在思维链训练阶段将虚拟视觉标记编码至紧凑潜空间，并采用统一多思维链策略联合学习文本、视觉及多模态推理模式。在推理阶段，模型可直接进行指令到动作的映射，同时保持推理感知的表征能力。在LH-VLN数据集上的实验表明，本方法在实现推理感知的同时保持实时导航能力，较显式思维链方法在提升成功率与效率的同时，将推理延迟降低一个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to enhance Vision-and-Language Navigation (VLN) with Chain-of-Thought (CoT) reasoning while overcoming the limitations of existing methods, where textual CoTs lack spatial grounding and multimodal CoTs suffer from high computational overhead due to token inflation from imagined visual observations. The proposed method, FantasyVLN, introduces a unified implicit reasoning framework that encodes imagined visual tokens into a compact latent space using a pretrained Visual AutoRegressor (VAR) during training, enabling joint learning from textual, visual, and multimodal CoT modes without explicit token generation at inference. Experimental results on the LH-VLN dataset demonstrate that this approach achieves reasoning-aware navigation with real-time performance, significantly improving success rates and efficiency while reducing inference latency by an order of magnitude compared to explicit CoT methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现有思维链推理方法在视觉语言导航中的局限性，即纯文本思维链缺乏空间基础，而多模态思维链则因标记膨胀严重导致实时导航不切实际。所提出的FantasyVLN方法引入了一个统一的隐式推理框架，在训练过程中使用预训练的视觉自回归器将想象的视觉标记编码到紧凑的潜在空间中，并联合学习文本、视觉和多模态思维链模式。在LH-VLN基准上的实验结果表明，该方法实现了具有推理意识的导航，提高了成功率和效率，同时将推理延迟相比显式思维链方法降低了一个数量级。</div>
</details>
</div>
<div class="card">
<div class="title">Tunable Passivity Control for Centralized Multiport Networked Systems</div>
<div class="meta-line">Authors: Xingyuan Zhou, Peter Paik, S. Farokh Atashzar</div>
<div class="meta-line">First: 2025-11-07T06:57:24+00:00 · Latest: 2026-01-23T05:22:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05026v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05026v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Centralized Multiport Networked Dynamic (CMND) systems have emerged as a key architecture with applications in several complex network systems, such as multilateral telerobotics and multi-agent control. These systems consist of a hub node/subsystem connecting with multiple remote nodes/subsystems via a networked architecture. One challenge for this system is stability, which can be affected by non-ideal network artifacts. Conventional passivity-based approaches can stabilize the system under specialized applications like small-scale networked systems. However, those conventional passive stabilizers have several restrictions, such as distributing compensation across subsystems in a decentralized manner, limiting flexibility, and, at the same time, relying on the restrictive assumptions of node passivity. This paper synthesizes a centralized optimal passivity-based stabilization framework for CMND systems. It consists of a centralized passivity observer monitoring overall energy flow and an optimal passivity controller that distributes the just-needed dissipation among various nodes, guaranteeing strict passivity and, thus, L2 stability. The proposed data-driven model-free approach, i.e., Tunable Centralized Optimal Passivity Control (TCoPC), optimizes total performance based on the prescribed dissipation distribution strategy while ensuring stability. The controller can put high dissipation loads on some sub-networks while relaxing the dissipation on other nodes. Simulation results demonstrate the proposed frameworks performance in a complex task under different time-varying delay scenarios while relaxing the remote nodes minimum phase and passivity assumption, enhancing the scalability and generalizability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>集中式多端口网络系统的可调无源性控制</div>
<div class="mono" style="margin-top:8px">集中式多端口网络动态系统已成为关键架构，应用于多边遥操作和多智能体控制等复杂网络系统。该系统通过网络架构连接中心节点与多个远程节点，但稳定性受非理想网络因素影响。传统无源性方法虽能稳定小规模网络系统，但存在补偿分散、灵活性受限及依赖节点无源性假设等限制。本文提出一种集中式最优无源性稳定框架，包含监测整体能量流的集中无源性观测器和在各节点间分配所需耗散的最优无源性控制器，确保严格无源性与L2稳定性。所提出的数据驱动无模型方法——可调集中最优无源性控制，在保证稳定性的同时基于预设耗散分配策略优化整体性能，可对部分子网络施加高耗散负载，同时降低其他节点负担。仿真结果表明，该框架能在时变延迟场景下处理复杂任务，放宽远程节点最小相位和无源性假设，提升了可扩展性与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses stability challenges in Centralized Multiport Networked Dynamic (CMND) systems, such as those used in multilateral telerobotics, where non-ideal network artifacts can compromise stability, and conventional decentralized passivity-based stabilizers impose restrictive assumptions like node passivity. The proposed method, Tunable Centralized Optimal Passivity Control (TCoPC), employs a centralized passivity observer to monitor overall energy flow and an optimal passivity controller that distributes precisely needed dissipation among nodes in a model-free, data-driven manner, ensuring strict passivity and L2 stability while optimizing performance based on a tunable dissipation strategy. Experimental simulations demonstrate that TCoPC effectively maintains stability in complex tasks under varying time delays, relaxes the need for minimum phase and passivity assumptions at remote nodes, and enhances system scalability and generalizability.</div>
<div class="mono" style="margin-top:8px">本研究针对集中式多端口网络动态（CMND）系统（如多边遥操作系统）中的稳定性挑战，其中非理想的网络因素可能破坏稳定性，而传统的分散式无源稳定器存在假设限制且灵活性不足。所提出的方法——可调集中式最优无源控制（TCoPC）——引入了一个集中式、数据驱动且无需模型框架，包括一个监测整体能量流的无源观测器和一个在节点间策略性分配所需耗散的最优无源控制器，以强制执行严格无源性。在不同时变延迟场景下的复杂任务仿真实验表明，该框架在放宽传统节点无源性和最小相位要求的同时确保了L2稳定性，从而提升了系统的可扩展性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">FoldNet: Learning Generalizable Closed-Loop Policy for Garment Folding via Keypoint-Driven Asset and Demonstration Synthesis</div>
<div class="meta-line">Authors: Yuxing Chen, Bowen Xiao, He Wang</div>
<div class="meta-line">First: 2025-05-14T03:34:30+00:00 · Latest: 2026-01-23T04:37:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.09109v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.09109v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to the deformability of garments, generating a large amount of high-quality data for robotic garment manipulation tasks is highly challenging. In this paper, we present a synthetic garment dataset that can be used for robotic garment folding. We begin by constructing geometric garment templates based on keypoints and applying generative models to generate realistic texture patterns. Leveraging these keypoint annotations, we generate folding demonstrations in simulation and train folding policies via closed-loop imitation learning. To improve robustness, we propose KG-DAgger, which uses a keypoint-based strategy to generate demonstration data for recovering from failures. KG-DAgger significantly improves the model performance, boosting the real-world success rate by 25\%. After training with 15K trajectories (about 2M image-action pairs), the model achieves a 75\% success rate in the real world. Experiments in both simulation and real-world settings validate the effectiveness of our proposed framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FoldNet：通过关键点驱动的资产与演示合成学习可泛化的闭环衣物折叠策略</div>
<div class="mono" style="margin-top:8px">由于衣物的可变形特性，为机器人衣物操作任务生成大量高质量数据极具挑战性。本文提出一个可用于机器人衣物折叠的合成衣物数据集。我们首先基于关键点构建几何衣物模板，并应用生成模型生成逼真的纹理图案。利用这些关键点标注，我们在仿真中生成折叠演示，并通过闭环模仿学习训练折叠策略。为提高鲁棒性，我们提出KG-DAgger方法，采用基于关键点的策略生成用于从失败中恢复的演示数据。KG-DAgger显著提升了模型性能，将现实世界成功率提高了25%。经过1.5万条轨迹（约200万图像-动作对）的训练后，模型在现实世界中达到75%的成功率。仿真与真实环境实验均验证了所提框架的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating sufficient high-quality data for robotic garment manipulation due to fabric deformability. The method constructs geometric garment templates from keypoints, applies generative models for realistic textures, and synthesizes folding demonstrations in simulation to train closed-loop policies via imitation learning. A keypoint-guided data augmentation strategy, KG-DAgger, is introduced to enhance robustness by generating recovery demonstrations, which boosts the real-world success rate by 25%, achieving a final 75% success rate after training on 15K trajectories.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人衣物折叠任务中高质量数据生成困难的问题，提出了一种结合几何模板与生成纹理的合成数据集。方法利用关键点驱动在仿真中生成演示数据，并通过闭环模仿学习训练折叠策略，同时引入基于关键点的KG-DAgger增强策略以提升失败恢复能力。实验结果表明，KG-DAgger将现实世界成功率提升了25%，在训练1.5万条轨迹后达到75%的成功率，仿真和现实实验均验证了框架的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">RENEW: Risk- and Energy-Aware Navigation in Dynamic Waterways</div>
<div class="meta-line">Authors: Mingi Jeong, Alberto Quattrini Li</div>
<div class="meta-line">Venue: AAAI 2026 oral</div>
<div class="meta-line">First: 2026-01-23T03:33:52+00:00 · Latest: 2026-01-23T03:33:52+00:00</div>
<div class="meta-line">Comments: 9 pages, 10 figure, 4 tables, AAAI 2026 (main track; oral acceptance)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16424v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16424v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present RENEW, a global path planner for Autonomous Surface Vehicle (ASV) in dynamic environments with external disturbances (e.g., water currents). RENEW introduces a unified risk- and energy-aware strategy that ensures safety by dynamically identifying non-navigable regions and enforcing adaptive safety constraints. Inspired by maritime contingency planning, it employs a best-effort strategy to maintain control under adverse conditions. The hierarchical architecture combines high-level constrained triangulation for topological diversity with low-level trajectory optimization within safe corridors. Validated with real-world ocean data, RENEW is the first framework to jointly address adaptive non-navigability and topological path diversity for robust maritime navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RENEW：动态水道中的风险与能耗感知导航</div>
<div class="mono" style="margin-top:8px">本文提出RENEW，一种面向动态环境下受外部扰动（如水流）影响的自主水面艇（ASV）全局路径规划器。RENEW采用统一的风险与能耗感知策略，通过动态识别不可航行区域并实施自适应安全约束来保障航行安全。该方法受海事应急规划启发，采用尽力而为策略以在恶劣条件下维持控制。其分层架构将高层约束三角剖分（用于拓扑多样性）与低层安全走廊内的轨迹优化相结合。基于真实海洋数据验证，RENEW是首个同时解决自适应不可航行区域识别与拓扑路径多样性问题的鲁棒海事导航框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for safe and efficient path planning for Autonomous Surface Vehicles (ASV) operating in dynamic waterways with disturbances like currents. The proposed RENEW framework introduces a unified risk- and energy-aware strategy that dynamically identifies non-navigable regions and enforces adaptive safety constraints, inspired by maritime contingency planning. Its hierarchical method combines high-level constrained triangulation for topological path diversity with low-level trajectory optimization within safe corridors. Experimental validation using real-world ocean data demonstrates that RENEW is the first framework to jointly achieve adaptive non-navigability assessment and topological diversity for robust maritime navigation.</div>
<div class="mono" style="margin-top:8px">本研究针对自主水面艇在存在水流等扰动的动态水域中，对安全可靠且节能的路径规划的需求。提出的RENEW方法引入了一种统一的风险与能耗感知策略，通过动态识别不可航行区域并实施自适应安全约束来确保安全，采用了基于应急规划的最优努力策略。其分层架构将用于拓扑多样性的高层约束三角剖分与在安全走廊内的低层轨迹优化相结合。利用真实海洋数据的实验验证表明，RENEW是首个能同时实现自适应不可航行性评估和拓扑路径多样性，以完成鲁棒海事导航的框架。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning-Based Energy-Aware Coverage Path Planning for Precision Agriculture</div>
<div class="meta-line">Authors: Beining Wu, Zihao Ding, Leo Ostigaard, Jun Huang</div>
<div class="meta-line">Venue: Proceedings of the 2025 International Conference on Research in Adaptive and Convergent Systems.(2025)</div>
<div class="meta-line">First: 2026-01-23T02:33:14+00:00 · Latest: 2026-01-23T02:33:14+00:00</div>
<div class="meta-line">Comments: Accepted by RACS &#x27;25: International Conference on Research in Adaptive and Convergent Systems, November 16-19, 2025, Ho Chi Minh, Vietnam. 10 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16405v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16405v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coverage Path Planning (CPP) is a fundamental capability for agricultural robots; however, existing solutions often overlook energy constraints, resulting in incomplete operations in large-scale or resource-limited environments. This paper proposes an energy-aware CPP framework grounded in Soft Actor-Critic (SAC) reinforcement learning, designed for grid-based environments with obstacles and charging stations. To enable robust and adaptive decision-making under energy limitations, the framework integrates Convolutional Neural Networks (CNNs) for spatial feature extraction and Long Short-Term Memory (LSTM) networks for temporal dynamics. A dedicated reward function is designed to jointly optimize coverage efficiency, energy consumption, and return-to-base constraints. Experimental results demonstrate that the proposed approach consistently achieves over 90% coverage while ensuring energy safety, outperforming traditional heuristic algorithms such as Rapidly-exploring Random Tree (RRT), Particle Swarm Optimization (PSO), and Ant Colony Optimization (ACO) baselines by 13.4-19.5% in coverage and reducing constraint violations by 59.9-88.3%. These findings validate the proposed SAC-based framework as an effective and scalable solution for energy-constrained CPP in agricultural robotics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于强化学习的能量感知覆盖路径规划在精准农业中的应用</div>
<div class="mono" style="margin-top:8px">覆盖路径规划是农业机器人的基础能力，但现有方案常忽视能量约束，导致在大规模或资源受限环境中作业不完整。本文提出一种基于软演员-评论家强化学习的能量感知覆盖路径规划框架，适用于含障碍物与充电站的栅格环境。该框架集成卷积神经网络提取空间特征，并采用长短期记忆网络处理时序动态，以实现能量限制下的鲁棒自适应决策。通过设计专用奖励函数联合优化覆盖效率、能耗及返航约束。实验结果表明，所提方法在确保能量安全的前提下持续实现90%以上覆盖率，相比快速探索随机树、粒子群优化、蚁群优化等传统启发式算法，覆盖率提升13.4-19.5%，约束违反率降低59.9-88.3%。这些发现验证了所提基于软演员-评论家的框架是农业机器人能量受限覆盖路径规划的有效可扩展解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Coverage Path Planning (CPP) for agricultural robots often neglects energy constraints, leading to incomplete operations in large or resource-limited fields. To address this, the authors propose an energy-aware CPP framework using the Soft Actor-Critic (SAC) reinforcement learning algorithm, which integrates Convolutional Neural Networks (CNNs) for spatial feature extraction and Long Short-Term Memory (LSTM) networks for temporal dynamics to make adaptive decisions under energy limitations. Experimental results show the method consistently achieves over 90% coverage while ensuring energy safety, outperforming traditional heuristic algorithms like RRT, PSO, and ACO by 13.4-19.5% in coverage and reducing constraint violations by 59.9-88.3%.</div>
<div class="mono" style="margin-top:8px">农业机器人的覆盖路径规划（CPP）常忽视能量约束，导致在大规模或资源受限环境中作业不完整。为此，本研究提出一种基于软演员-评论家（SAC）强化学习的能量感知CPP框架，该框架结合卷积神经网络（CNN）提取空间特征，并利用长短期记忆（LSTM）网络处理时序动态，以在能量限制下实现自适应决策。实验结果表明，该方法在确保能量安全的前提下，持续实现超过90%的覆盖率，其覆盖性能较传统启发式算法（如RRT、PSO、ACO）提升13.4-19.5%，同时将约束违反率降低59.9-88.3%，验证了该框架在能量受限农业机器人CPP任务中的有效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">GNSS-based Lunar Orbit and Clock Estimation With Stochastic Cloning UD Filter</div>
<div class="meta-line">Authors: Keidai Iiyama, Grace Gao</div>
<div class="meta-line">First: 2026-01-23T01:54:35+00:00 · Latest: 2026-01-23T01:54:35+00:00</div>
<div class="meta-line">Comments: Submitted to the Journal of Guidance, Control, and Dynamics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16393v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16393v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a terrestrial GNSS-based orbit and clock estimation framework for lunar navigation satellites. To enable high-precision estimation under the low-observability conditions encountered at lunar distances, we develop a stochastic-cloning UD-factorized filter and delayed-state smoother that provide enhanced numerical stability when processing precise time-differenced carrier phase (TDCP) measurements. A comprehensive dynamics and measurement model is formulated, explicitly accounting for relativistic coupling between orbital and clock states, lunar time-scale transformations, and signal propagation delays including ionospheric, plasmaspheric, and Shapiro effects. The proposed approach is evaluated using high-fidelity Monte-Carlo simulations incorporating realistic multi-constellation GNSS geometry, broadcast ephemeris errors, lunar satellite dynamics, and ionospheric and plasmaspheric delay computed from empirical electron density models. Simulation results demonstrate that combining ionosphere-free pseudorange and TDCP measurements achieves meter-level orbit accuracy and sub-millimeter-per-second velocity accuracy, satisfying the stringent signal-in-space error requirements of future Lunar Augmented Navigation Services (LANS).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于GNSS的月球轨道与钟差估计：随机克隆UD滤波方法</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于地面全球导航卫星系统（GNSS）的月球导航卫星轨道与钟差估计框架。为应对月球距离下低可观测性条件的高精度估计需求，我们开发了随机克隆UD分解滤波器与延迟状态平滑器，在处理精密时间差分载波相位（TDCP）观测值时提供更强的数值稳定性。研究建立了完整的动力学与观测模型，明确考虑了轨道与钟差状态间的相对论耦合效应、月球时标转换、以及包含电离层、等离子体层与夏皮罗效应的信号传播延迟。通过高保真蒙特卡洛仿真对方法进行评估，仿真融合了真实多星座GNSS几何构型、广播星历误差、月球卫星动力学特性，以及基于经验电子密度模型计算的电离层与等离子体层延迟。结果表明，结合无电离层伪距与TDCP观测值可实现米级轨道精度与亚毫米/秒级速度精度，满足未来月球增强导航服务（LANS）对空间信号误差的严苛要求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of achieving high-precision orbit and clock estimation for lunar navigation satellites using terrestrial GNSS signals, which are characterized by low observability at lunar distances. The proposed method develops a stochastic-cloning UD-factorized filter combined with a delayed-state smoother to enhance numerical stability when processing precise time-differenced carrier phase (TDCP) measurements, within a comprehensive dynamics and measurement model that accounts for relativistic state coupling, lunar time-scale transformations, and various signal propagation delays. High-fidelity Monte-Carlo simulations incorporating realistic multi-constellation GNSS geometry and environmental models demonstrate that the approach, using ionosphere-free pseudorange and TDCP measurements, achieves meter-level orbit accuracy and sub-millimeter-per-second velocity accuracy, meeting the stringent requirements for future Lunar Augmented Navigation Services.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决利用地面GNSS信号为月球导航卫星实现高精度轨道与钟差估计的难题，该问题因月球距离下的低可观测性而颇具挑战。方法上，提出了一种随机克隆UD分解滤波器和延迟状态平滑器，以在处理精确时差载波相位（TDCP）观测值时增强数值稳定性，并建立了包含相对论状态耦合、月球时标转换及信号传播延迟的综合模型。高保真蒙特卡洛仿真结果表明，该方法结合无电离层伪距和TDCP观测值，可实现米级轨道精度和亚毫米每秒的速度精度，满足未来月球增强导航服务的严苛要求。</div>
</details>
</div>
<div class="card">
<div class="title">Digital twins for the design, interactive control, and deployment of modular, fiber-reinforced soft continuum arms</div>
<div class="meta-line">Authors: Seung Hyun Kim, Jiamiao Guo, Arman Tekinalp, Heng-Sheng Chang, Ugur Akcal, Tixian Wang, Darren Biskup, Benjamin Walt, Girish Chowdhary, Girish Krishnan, Prashant G. Mehta, Mattia Gazzola</div>
<div class="meta-line">First: 2025-07-14T10:05:07+00:00 · Latest: 2026-01-22T23:53:03+00:00</div>
<div class="meta-line">Comments: 8 pages, 4 figures This work has been submitted for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10121v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.10121v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Soft continuum arms (SCAs) promise versatile manipulation through mechanical compliance, for assistive devices, agriculture, search applications, or surgery. However, the strong nonlinear coupling between materials, morphology, and actuation renders design and control challenging, hindering real-world deployment. In this context, a modular fabrication strategy paired with reliable, interactive simulations would be highly beneficial, streamlining prototyping and control design. Here, we present a digital twin framework for modular SCAs realized using pneumatic Fiber-Reinforced Elastomeric Enclosures (FREEs). The approach models assemblies of FREE actuators through networks of Cosserat rods, favoring the accurate simulation of three-dimensional arm reconfigurations, while explicitly preserving internal modular architecture. This enables the quantitative analysis and scalable development of composite soft robot arms, overcoming limitations of current monolithic continuum models. To validate the framework, we introduce a three-dimensional reconstruction pipeline tailored to soft, slender, small-volume, and highly deformable structures, allowing reliable recovery of arm kinematics and strain distributions. Experimental results across multiple configurations and actuation regimes demonstrate close agreement with simulations. Finally, we embed the digital twins in a virtual environment to allow interactive control design and sim-to-real deployment, establishing a foundation for principled co-design and remote operation of modular soft continuum manipulators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向模块化纤维增强软体连续臂设计、交互控制与部署的数字孪生框架</div>
<div class="mono" style="margin-top:8px">软体连续臂（SCAs）凭借其机械顺应性，在辅助设备、农业、搜索应用或手术等领域展现出广阔的应用前景。然而，材料、形态与驱动之间的强非线性耦合使得设计与控制极具挑战，阻碍了实际部署。在此背景下，模块化制造策略结合可靠交互仿真的方法将大有裨益，可显著简化原型开发与控制设计。本文提出一种基于气动纤维增强弹性体封装（FREEs）模块化SCAs的数字孪生框架。该方法通过Cosserat杆网络对FREE执行器组件进行建模，实现了三维臂形重构的高精度仿真，并显式保留了内部模块化架构。这为复合软体机械臂的量化分析与可扩展开发提供了可能，克服了现有整体连续体模型的局限。为验证框架有效性，我们开发了专用于柔软、细长、小体积及高变形结构的二维重建流程，可可靠还原臂体运动学与应变分布。多构型多驱动模式下的实验结果均与仿真高度吻合。最后，我们将数字孪生嵌入虚拟环境，支持交互式控制设计与仿真到实物的部署，为模块化软体连续机械臂的系统化协同设计与远程操作奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Soft continuum arms offer compliance-driven manipulation but face design and control challenges due to nonlinear material-actuation coupling. This work introduces a digital twin framework using Cosserat rod networks to model modular pneumatic fiber-reinforced actuators, enabling accurate 3D simulation and preserving internal modularity. Experiments with a 3D reconstruction pipeline for deformable structures show close simulation agreement across configurations, and embedding twins in a virtual environment supports interactive control design and sim-to-real deployment.</div>
<div class="mono" style="margin-top:8px">软体连续臂凭借机械顺应性在多种应用中前景广阔，但其材料、形态与驱动间的强非线性耦合使设计与控制面临挑战。本研究提出一种数字孪生框架，采用Cosserat杆网络对气动纤维增强弹性体封装模块进行建模，实现了保留模块化架构的三维精确仿真。通过针对软细长小体积高变形结构的重建流程，实验结果表明仿真与多配置多驱动状态下的实际臂运动学及应变分布高度吻合，该数字孪生支持交互式控制设计及仿真到现实的部署。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
