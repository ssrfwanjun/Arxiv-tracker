<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 20:01</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_2001</div>
    <div class="row"><div class="card">
<div class="title">Iterative Refinement Improves Compositional Image Generation</div>
<div class="meta-line">Authors: Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak</div>
<div class="meta-line">First: 2026-01-21T18:59:40+00:00 · Latest: 2026-01-21T18:59:40+00:00</div>
<div class="meta-line">Comments: Project webpage: https://iterative-img-gen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15286v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iterative-img-gen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代优化提升组合式图像生成质量</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型已取得显著进展，但在处理需要同时满足多对象、关系和属性的复杂提示时仍面临挑战。现有的推理时策略（如带验证器的并行采样或单纯增加去噪步数）虽能提升提示对齐度，但在需要满足多重约束的复杂组合场景中仍显不足。受大语言模型中思维链推理成功的启发，我们提出一种迭代式测试时策略：T2I模型在视觉语言模型作为循环评判器的反馈指导下，通过多步骤渐进优化生成结果。该方法无需外部工具或先验知识，可灵活适配各类图像生成器和视觉语言模型。实验表明，该方法在多个基准测试中持续提升图像生成效果：在ConceptMix（k=7）的全正确率提升16.9%，在T2I-CompBench（3D空间类别）提升13.8%，在Visual Jenga场景解构任务中提升12.5%（均以计算量匹配的并行采样为基线）。除量化提升外，迭代优化通过将复杂提示分解为序列化修正，生成结果更具忠实度——人工评估者对本方法的偏好率达58.7%（基线为41.3%）。这些发现共同表明，迭代式自校正可作为组合式图像生成的普适性准则。完整结果与可视化内容详见项目网页。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image models often fail to accurately generate images from complex prompts involving multiple objects and relations. To address this, the authors propose an iterative refinement strategy where a vision-language model provides feedback to guide the image generator across multiple denoising steps, decomposing the prompt into sequential corrections. Experimental results show significant improvements, including a 16.9% increase in all-correct rate on the ConceptMix benchmark and human evaluators preferring the method 58.7% of the time over a parallel sampling baseline.</div>
<div class="mono" style="margin-top:8px">文本到图像模型在处理涉及多个物体、关系和属性的复杂提示时常常表现不佳。为解决此问题，作者提出了一种迭代优化策略，利用视觉语言模型提供反馈，在多步去噪过程中引导图像生成器，将复杂提示分解为顺序修正。实验结果表明该方法带来了显著提升：在ConceptMix基准上的全正确率提高了16.9%，在T2I-CompBench的3D空间类别上提升了13.8%，且人类评估者在58.7%的情况下更倾向于迭代方法，优于并行采样基线。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Generation Model for the Embodied World</div>
<div class="meta-line">Authors: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-21T18:59:18+00:00 · Latest: 2026-01-21T18:59:18+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15282v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/ReVidgen/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/ReVidgen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向具身世界的视频生成模型再思考</div>
<div class="mono" style="margin-top:8px">视频生成模型显著推动了具身智能的发展，为生成捕捉物理世界中感知、推理与行动的多样化机器人数据开辟了新可能。然而，合成能准确反映真实机器人交互的高质量视频仍具挑战，且缺乏标准化基准限制了公平比较与进展。为此，我们推出综合性机器人基准RBench，旨在通过五个任务领域和四种不同具身形态评估面向机器人的视频生成。该基准通过可复现的子指标（包括结构一致性、物理合理性与动作完整性）评估任务级正确性与视觉保真度。对25个代表性模型的评估揭示了其在生成物理真实机器人行为方面的显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达0.96，验证了其有效性。RBench虽为识别这些不足提供了必要视角，但实现物理真实性需超越评估层面，解决高质量训练数据严重短缺的问题。基于这些洞见，我们提出精炼的四阶段数据流程，由此构建了RoVid-X——目前最大的开源机器人视频生成数据集，包含400万个标注视频片段，覆盖数千项任务，并配有全面的物理属性标注。这一评估与数据协同的生态系统，为视频模型的严谨评估与可扩展训练奠定了坚实基础，加速具身AI向通用智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating high-quality videos that accurately reflect real-world robotic interactions, which is crucial for advancing embodied intelligence, and the lack of a standardized benchmark for fair comparison. The method involves introducing RBench, a comprehensive robotics benchmark for evaluating robot-oriented video generation across multiple task domains and embodiments, assessing task correctness and visual fidelity through sub-metrics like structural consistency and physical plausibility, and developing RoVid-X, a large open-source dataset with 4 million annotated video clips via a refined four-stage data pipeline. Key experimental results show that evaluation of 25 representative models reveals significant deficiencies in generating physically realistic robot behaviors, and RBench achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决生成准确反映真实世界机器人交互的高质量视频的挑战，这对推进具身智能至关重要，但缺乏标准化基准限制了进展。方法上引入了RBench，这是一个全面的机器人基准，在五个任务领域和四种具身形态上评估面向机器人的视频生成，通过结构一致性、物理合理性和动作完整性等指标评估任务正确性和视觉保真度。实验结果揭示了现有模型在生成物理真实的机器人行为方面存在显著缺陷，RBench与人类评估的斯皮尔曼相关系数达到0.96，同时研究进一步贡献了RoVid-X，一个包含400万个标注视频片段的大型数据集，以解决高质量训练数据短缺的问题。</div>
</details>
</div>
<div class="card">
<div class="title">MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs</div>
<div class="meta-line">Authors: Christoph Bartmann, Johannes Schimunek, Mykyta Ielanskyi, Philipp Seidl, Günter Klambauer, Sohvi Luukkonen</div>
<div class="meta-line">First: 2026-01-21T18:58:01+00:00 · Latest: 2026-01-21T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15279v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A molecule&#x27;s properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolecularIQ：通过分子图符号验证表征化学推理能力</div>
<div class="mono" style="margin-top:8px">分子的性质根本上由其分子图编码的组成与结构决定。因此，对分子性质进行推理需要解析和理解分子图的能力。大语言模型正日益应用于化学领域，处理分子名称转换、描述生成、文本引导生成以及性质或反应预测等任务。现有基准大多侧重通用化学知识，依赖可能产生泄漏或偏差的文献/代理标签，或将评估简化为选择题。我们提出MolecularIQ——一个专注于符号可验证任务的分子结构推理基准。该基准支持对分子图推理进行细粒度评估，并揭示将模型缺陷定位到特定任务和分子结构的能力模式。这为当前化学大语言模型的优势与局限提供了可操作的见解，并指导开发能够对分子结构进行可靠推理的模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate how well Large Language Models (LLMs) can reason about molecular properties, which are fundamentally determined by a molecule&#x27;s graph structure, as existing benchmarks often rely on general knowledge, potentially biased labels, or simplified multiple-choice formats. The method introduces MolecularIQ, a benchmark composed of symbolically verifiable tasks that require parsing and understanding molecular graphs, enabling a fine-grained analysis of reasoning capabilities. Key experimental findings reveal distinct capability patterns, localizing model failures to specific tasks and molecular structures, thereby providing actionable insights into the strengths and limitations of current chemistry LLMs and guiding the development of more faithful reasoning models.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要评估大语言模型（LLMs）对分子特性的推理能力，因为分子特性根本上由其图结构决定，而现有基准测试通常依赖于可能造成数据泄露且缺乏精确验证的通用知识或多选题形式。方法上，研究引入了MolecularIQ基准，该基准由一系列可符号验证的任务组成，要求模型直接对分子图进行解析和推理，从而实现对模型能力的细粒度分析。关键的实验结果表明，模型性能存在明显的模式差异，其失败可定位到特定的推理任务和分子子结构，这为当前化学大语言模型的局限性提供了可操作的见解，并指导开发更具忠实性的结构推理模型。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions</div>
<div class="meta-line">Authors: Yiran Hu, Huanghai Liu, Chong Wang, Kunran Li, Tien-Hsuan Wu, Haitao Li, Xinran Xu, Siqing Huo, Weihang Su, Ning Zheng, Siyuan Zheng, Qingyao Ai, Yun Liu, Renjun Bian, Yiqun Liu, Charles L. A. Clarke, Weixing Shen, Ben Kao</div>
<div class="meta-line">First: 2026-01-21T18:51:37+00:00 · Latest: 2026-01-21T18:51:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15267v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15267v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型在法律应用中的评估：挑战、方法与未来方向</div>
<div class="mono" style="margin-top:8px">大语言模型正日益融入法律应用领域，涵盖司法决策支持、法律实务辅助和面向公众的法律服务。尽管大语言模型在处理法律知识与任务方面展现出巨大潜力，但其在实际法律场景中的部署引发了超越表层准确性的关键问题，涉及法律推理过程的严谨性以及公平性、可靠性等可信度议题。因此，系统评估大语言模型在法律任务中的表现已成为其负责任应用的重要前提。本综述立足现实法律实践，梳理了评估法律任务中大语言模型面临的核心挑战，包括结果正确性、推理可靠性与可信度等关键维度的评估难点。基于这些挑战，我们依据任务设计、数据集和评估指标对现有评估方法与基准进行了系统梳理与分类。进一步探讨当前方法应对这些挑战的程度，指出其局限性，并展望未来研究方向，以构建更贴近现实、更可靠且更扎根法律实践的大语言模型评估框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing integration of large language models (LLMs) into judicial support, legal practice, and public services necessitates a systematic evaluation that moves beyond surface accuracy to assess the soundness of legal reasoning and trustworthiness factors like fairness and reliability. This survey analyzes the core challenges in evaluating LLMs for legal tasks—focusing on outcome correctness, reasoning reliability, and overall trustworthiness—and categorizes existing evaluation methods and benchmarks based on their task design, datasets, and metrics. The review finds that current approaches have significant limitations in addressing these real-world legal concerns and outlines future research directions for developing more realistic and legally grounded evaluation frameworks.</div>
<div class="mono" style="margin-top:8px">随着大语言模型越来越多地应用于司法决策支持、法律实务辅助等法律领域，其部署引发了超越表面准确性的关键关切，包括法律推理过程的严谨性以及公平性、可靠性等可信问题。本研究基于真实法律实践，系统梳理了评估大语言模型在法律任务中表现的主要挑战，分析了在结果正确性、推理可靠性和可信度评估方面的难点，并对现有评估方法和基准按照任务设计、数据集和评价指标进行了分类综述。研究进一步讨论了当前方法在应对这些挑战上的局限性，并展望了未来构建更贴近现实、更可靠且更扎根于法律实践的评价框架的研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Stewardship of an LLM-Assisted Clinical Benchmark with Physician Oversight</div>
<div class="meta-line">Authors: Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati</div>
<div class="meta-line">First: 2025-12-22T18:59:34+00:00 · Latest: 2026-01-21T18:48:54+00:00</div>
<div class="meta-line">Comments: Project codebase: https://github.com/junzeye/validate-medcalc-labels</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19691v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19691v2">PDF</a> · <a href="https://github.com/junzeye/validate-medcalc-labels">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We examine the reliability of a widely used clinical AI benchmark whose reference labels were partially generated by LLMs, and find that a substantial fraction are clinically misaligned. We introduce a phased stewardship procedure to amplify the positive impact of physician experts&#x27; feedback and then demonstrate, via a controlled RL experiment, how uncaught label bias can materially affect downstream LLM evaluation and alignment. Our results demonstrate that partially LLM-generated labels can embed systemic errors that distort not only evaluation but also downstream model alignment. By adopting a hybrid oversight system, we can prioritize scarce expert feedback to maintain benchmarks as living, clinically-grounded documents. Ensuring this alignment is a prerequisite for the safe deployment of LLMs in high-stakes medical decision support.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于医师监督的大语言模型辅助临床基准可扩展管理机制</div>
<div class="mono" style="margin-top:8px">本研究针对一项广泛使用的临床人工智能基准展开可靠性分析，发现其部分参考标签由大语言模型生成，且存在相当比例与临床实践不符的情况。我们提出分阶段管理流程，以增强医师专家反馈的积极影响，并通过受控强化学习实验证明未发现的标签偏差如何实质影响下游大语言模型评估与对齐。结果表明，部分由大语言模型生成的标签可能引入系统性误差，不仅扭曲评估结果，还会影响下游模型对齐。采用混合监督系统可优先配置稀缺的专家反馈资源，使基准保持为动态更新且扎根临床的文档体系。确保这种对齐是实现大语言模型在高风险医疗决策支持场景安全部署的前提条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reliability of a widely used clinical AI benchmark, revealing that a substantial fraction of its reference labels, which were partially generated by large language models (LLMs), are clinically misaligned. To address this, the authors introduce a phased stewardship procedure that amplifies the impact of physician expert feedback, and through a controlled reinforcement learning experiment, they demonstrate how uncaught label bias can materially affect downstream LLM evaluation and alignment. The key experimental findings show that partially LLM-generated labels can embed systemic errors that distort not only evaluation but also downstream model alignment, and that a hybrid oversight system can effectively prioritize scarce expert feedback to maintain benchmarks as clinically-grounded documents.</div>
<div class="mono" style="margin-top:8px">本研究调查了一个包含LLM生成参考标签的临床AI基准的可靠性，发现其中存在大量临床不匹配的条目。为此，作者提出了一种分阶段的管理程序，通过高效利用医生反馈来纠正错误，并通过一项受控强化学习实验展示了此类标签偏差如何扭曲模型评估和对齐。研究结果表明，部分LLM生成的标签会引入系统性错误，影响下游任务，但采用混合监督系统可以有效优先处理专家意见，从而维持临床基础的基准，这对于在高风险医疗环境中安全部署LLM至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI</div>
<div class="meta-line">Authors: Haocheng Lin</div>
<div class="meta-line">First: 2025-12-09T20:25:24+00:00 · Latest: 2026-01-21T18:42:26+00:00</div>
<div class="meta-line">Comments: Improved structure and clarity of the introduction and literature review; explicit articulation of the paper&#x27;s contributions; refined the integration of AI across labour, UBI, and governance</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11893v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11893v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid expansion of generative artificial intelligence (AI) is transforming work, creativity, and economic security in ways that extend beyond automation and productivity. This paper examines four interconnected dimensions of contemporary AI deployment: (1) transformations in employment and task composition (2) unequal diffusion of AI across sectors and socio-demographic groups (3) the role of universal basic income (UBI) as a stabilising response to AI-induced volatility (4) the effects of model alignment and content governance on human creativity, autonomy, and decision-making
  Using a hybrid approach that integrates labour market task exposure modelling, sectoral diffusion analysis, policy review, and qualitative discourse critique, the study develops an Inclusive AI Governance Framework. It introduces Level 1.5 autonomy as a human centred design principle that preserves evaluative authority while enabling partial automation, and highlights evidence of creative regression and emergent sycophancy in newer model generations. The paper argues that UBI should be embedded within a broader socio-technical governance ecosystem encompassing skills development, proportional regulation, and creativity preservation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越自动化：生成式人工智能时代对工作、创造力与治理的再思考</div>
<div class="mono" style="margin-top:8px">生成式人工智能的快速扩张正在以超越自动化和生产效率的方式，深刻改变着工作模式、创造力形态与经济安全。本文从四个相互关联的维度审视当代AI部署：(1)就业结构与任务构成的重塑 (2)AI技术在不同行业及社会群体间的不均衡扩散 (3)全民基本收入作为应对AI波动性稳定机制的作用 (4)模型对齐与内容治理对人类创造力、自主权及决策机制的影响。研究采用融合劳动力市场任务暴露模型、行业扩散分析、政策评估与质性话语批判的混合方法，构建了包容性AI治理框架。提出以&#x27;1.5级自主性&#x27;作为人本设计原则，在保留人类评估权威的同时实现局部自动化，并揭示了新一代模型中存在的创造力退化与谄媚倾向。论文主张将全民基本收入纳入涵盖技能发展、比例监管和创造力保护的社会技术治理生态系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how generative AI&#x27;s impact extends beyond simple automation, affecting employment structures, creative processes, and economic stability. The study employs a hybrid methodology combining labor market task exposure modeling, sectoral diffusion analysis, policy review, and qualitative discourse critique to develop an Inclusive AI Governance Framework. Key findings include evidence of creative regression and emergent sycophancy in newer AI models, leading to the proposal of Level 1.5 autonomy as a human-centered design principle and the argument that Universal Basic Income must be integrated within a broader governance ecosystem involving skills development and proportional regulation.</div>
<div class="mono" style="margin-top:8px">本研究动机源于生成式人工智能超越自动化的变革性影响，旨在探讨其对就业、不平等、政策应对和创造力的作用。方法上采用混合研究，整合了劳动力市场任务暴露建模、部门扩散分析、政策审查和定性话语批判，以构建一个包容性人工智能治理框架。主要实验发现包括提出了以人为中心的1.5级自主性设计原则，提供了新模型中出现创造性退化和涌现谄媚行为的证据，并论证了全民基本收入应嵌入更广泛的社会技术治理生态系统中。</div>
</details>
</div>
<div class="card">
<div class="title">Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?</div>
<div class="meta-line">Authors: Felix Schur, Niklas Pfister, Peng Ding, Sach Mukherjee, Jonas Peters</div>
<div class="meta-line">First: 2026-01-21T18:36:34+00:00 · Latest: 2026-01-21T18:36:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15254v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\ell_1$-regularized estimation and post-selection refitting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多实验、少重复、非配对数据与稀疏效应：因果推断是否可行？</div>
<div class="mono" style="margin-top:8px">我们研究在隐藏混杂因素下估计因果效应的问题，数据为非配对设置：在不同实验条件（环境）下观测到协变量$X$和结果$Y$，但未同时观测二者；仅观测到$X$或$Y$。在适当正则条件下，该问题可转化为工具变量（IV）回归，其中环境作为（可能高维的）工具变量。当环境数量多但每个环境观测极少时，标准双样本IV估计量无法保持一致性。我们提出一种基于工具变量-协变量样本交叉折叠分割的GMM型估计量，并证明其在环境数量增长而单环境样本量固定时具有一致性。进一步通过$\ell_1$正则化估计与后选择重拟合，将方法扩展至稀疏因果效应场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of estimating causal effects from unpaired observational data where covariates and outcomes are measured in separate environments, a scenario common in many experiments with few repetitions per condition. The authors frame the problem as an instrumental variable regression with environments as instruments and propose a GMM-type estimator that uses cross-fold sample splitting to handle many environments but sparse per-environment observations. Theoretical analysis confirms consistency as the number of environments increases, even with fixed sample sizes per environment, and the method is extended via ℓ₁-regularization and refitting to accommodate sparse causal effects.</div>
<div class="mono" style="margin-top:8px">本文研究了在未配对观测数据中估计因果效应的问题，其中协变量和结果在不同实验环境下分别测量，这在许多重复次数较少的实验中很常见。作者将该问题建模为以环境作为工具变量的工具变量回归，并提出了一种基于交叉折叠样本分割的GMM型估计器，以处理环境数量多但每个环境数据稀疏的情况。理论分析表明，随着环境数量增加，即使每个环境的样本量固定，估计器仍具有一致性；该方法还通过ℓ₁正则化估计和后选择重拟合扩展至稀疏因果效应场景。</div>
</details>
</div>
<div class="card">
<div class="title">Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism</div>
<div class="meta-line">Authors: Garrett G. Wen, Buxin Su, Natalie Collina, Zhun Deng, Weijie Su</div>
<div class="meta-line">First: 2026-01-21T18:30:42+00:00 · Latest: 2026-01-21T18:30:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15249v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors&#x27; assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions&#x27; ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于等渗机制的ML/AI会议最佳论文奖推荐方法</div>
<div class="mono" style="margin-top:8px">NeurIPS、ICML等机器学习与人工智能会议如今每年收到数万篇投稿，这对维持同行评审的质量与一致性构成重大挑战。最佳论文奖的评选尤为突出——它虽是评审流程的重要环节，近年却日益引发争议。本文提出一种作者协助机制以优化最佳论文评选。该方法采用等渗机制收集作者对自身投稿的排序式评估，并据此调整原始评审分数，从而最优估计论文的真实质量。我们证明：当作者的效用函数为调整后分数的凸可加函数时，作者有动机如实报告；利用ICLR（2019-2023）和NeurIPS（2021-2023）公开评审数据，我们验证了最佳论文奖场景下该凸性假设的合理性。关键的是，在作者仅有一个提名配额（即只能提名一篇论文）的特殊情形下，我们证明即使效用函数仅为非递减可加函数，真实性依然成立——这显著放宽了既有研究的前提假设。为实际应用，我们将机制扩展至处理常见的作者重叠场景。仿真结果表明，该机制能显著提升获奖论文质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The surge in submissions to major ML/AI conferences has strained the peer review process, especially for best paper awards, prompting the need for a more reliable selection method. This paper proposes an author-assisted mechanism that uses the Isotonic Mechanism to elicit authors&#x27; private rankings of their own submissions, which are then combined with raw review scores to better estimate true paper quality. Theoretical analysis shows authors are incentivized to report truthfully under convex additive utility, a condition validated on ICLR (2019-2023) and NeurIPS (2021-2023) data, with truthfulness guaranteed even for nondecreasing additive utility when authors have a single nomination. Simulations confirm the mechanism significantly enhances the quality of awarded papers.</div>
<div class="mono" style="margin-top:8px">随着NeurIPS、ICML等主要机器学习与人工智能会议投稿量激增，同行评审的质量与一致性面临挑战，最佳论文奖的评选尤其引发争议。为此，本研究提出一种作者辅助机制，采用等渗机制（Isotonic Mechanism）让作者对自己的投稿进行真实排名，并利用该排名调整原始评审分数以更准确地估计论文质量。理论分析表明，在凸可加效用函数下作者有动机诚实报告，这一凸性假设在ICLR（2019-2023）和NeurIPS（2021-2023）的公开评审数据中得到验证；关键突破在于，当作者仅有一个提名配额时，即使效用函数仅为非递减可加函数，诚实报告依然成立，这大幅放宽了先前工作的假设。该机制还扩展至处理作者重叠的常见场景，模拟实验证明其能显著提升获奖论文的质量。</div>
</details>
</div>
<div class="card">
<div class="title">On the Reliability and Stability of Selective Methods in Malware Classification Tasks</div>
<div class="meta-line">Authors: Alexander Herzog, Aliai Eusebi, Lorenzo Cavallaro</div>
<div class="meta-line">First: 2025-05-28T20:22:43+00:00 · Latest: 2026-01-21T18:26:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22843v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.22843v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance figures of modern drift-adaptive malware classifiers appear promising, but does this translate to genuine operational reliability? The standard evaluation paradigm primarily focuses on baseline performance metrics, neglecting confidence-error alignment and operational stability. While prior works established the importance of temporal evaluation and introduced selective classification in malware classification tasks, we take a complementary direction by investigating whether malware classifiers maintain reliable and stable confidence estimates under distribution shifts and exploring the tensions between scientific advancement and practical impacts when they do not. We propose Aurora, a framework to evaluate malware classifiers based on their confidence quality and operational resilience. Aurora subjects the confidence profile of a given model to verification to assess the reliability of its estimates. Unreliable confidence estimates erode operational trust, waste valuable annotation budgets on non-informative samples for active learning, and leave error-prone instances undetected in selective classification. Aurora is further complemented by a set of metrics designed to go beyond point-in-time performance, striving towards a more holistic assessment of operational stability throughout temporal evaluation periods. The fragility we observe in SOTA frameworks across datasets of varying drift severity suggests it may be time to revisit the underlying assumptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论恶意软件分类任务中选择性方法的可靠性与稳定性</div>
<div class="mono" style="margin-top:8px">现代漂移自适应恶意软件分类器的性能数据看似良好，但这能否转化为真正的操作可靠性？标准评估范式主要关注基线性能指标，忽视了置信度-误差对齐与操作稳定性。尽管先前研究已确立时序评估的重要性，并将选择性分类引入恶意软件分类任务，我们采取互补方向：探究恶意软件分类器在分布漂移下是否保持可靠稳定的置信度估计，并当置信度失准时，剖析科学进展与实际影响之间的张力。我们提出Aurora框架，基于置信度质量与操作韧性评估恶意软件分类器。该框架通过对给定模型的置信度分布进行验证，以评估其估计的可靠性。不可靠的置信度估计会削弱操作信任、在主动学习中浪费宝贵标注资源于非信息性样本，并导致选择性分类中易错实例未被察觉。Aurora进一步辅以一套超越单点性能的指标，致力于在时序评估周期内实现更全面的操作稳定性评估。我们在不同漂移严重程度数据集中观察到的SOTA框架脆弱性表明，或许应重新审视其底层假设。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study questions whether the promising performance of drift-adaptive malware classifiers translates to genuine operational reliability, noting that standard evaluations neglect confidence-error alignment and long-term stability. To address this, the authors propose Aurora, a framework that verifies a model&#x27;s confidence profile to assess the reliability of its estimates and operational resilience under distribution shifts, complemented by metrics for holistic temporal stability assessment. Experimental findings reveal significant fragility in state-of-the-art frameworks across datasets with varying drift severity, indicating unreliable confidence estimates that undermine operational trust, waste annotation budgets in active learning, and leave errors undetected in selective classification.</div>
<div class="mono" style="margin-top:8px">本研究质疑具有漂移适应能力的恶意软件分类器的优异性能是否确保真正的操作可靠性，指出标准评估忽略了置信度-误差对齐和长期稳定性。作者提出了Aurora框架，通过验证模型的置信度分布来评估其在分布偏移下估计的可靠性和操作韧性，并辅以旨在进行整体时间稳定性评估的指标。实验表明，在最先进的框架中，针对不同漂移严重程度的数据集均观察到显著的脆弱性，其不可靠的置信度估计会损害操作信任、在主动学习中浪费标注预算，并在选择性分类中遗留未被检测的错误。</div>
</details>
</div>
<div class="card">
<div class="title">Feasibility Preservation under Monotone Retrieval Truncation</div>
<div class="meta-line">Authors: Sean Plummer</div>
<div class="meta-line">First: 2026-01-21T18:25:16+00:00 · Latest: 2026-01-21T18:25:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15241v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation.
  We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.
  Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单调检索截断下的可行性保持</div>
<div class="mono" style="margin-top:8px">基于检索的系统通过仅暴露可用证据的截断子集来近似访问语料库。即使语料库中存在相关信息，截断也可能阻止兼容证据同时出现，导致基于相关性的评估无法捕捉的失败情况。本文从结构视角研究检索，将查询应答建模为截断条件下的可行性问题。
我们形式化检索为候选证据序列，并刻画极限可行性蕴含有限检索深度可行性的条件。证明单调截断足以保证单个查询的有限可见证性。对于查询类，我们确定见证证书的有限生成是获得一致检索边界所需的附加条件，并证明该条件是必要的。进一步通过尖锐反例展示非单调截断、非有限生成查询类及纯槽位覆盖下的失败情形。
这些结果共同将可行性保持确立为独立于相关性评分或优化的检索正确性准则，并阐明了基于截断的检索固有的结构局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates retrieval systems that truncate evidence sets, which can cause failures even when relevant information exists in the corpus, a problem not captured by standard relevance evaluation. The authors model query answering as a feasibility problem under truncation, formalizing retrieval as a sequence of candidate evidence sets and establishing conditions—specifically monotone truncation—under which feasibility in the limit implies feasibility at a finite retrieval depth. Key experimental findings show that monotone truncation guarantees finite witnessability for individual queries, while for query classes, finite generation of witness certificates is necessary and sufficient for a uniform retrieval bound, with sharp counterexamples demonstrating failures under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.</div>
<div class="mono" style="margin-top:8px">本文研究了基于截断的证据检索系统的结构局限性，即当相关证据片段被分隔时，即使信息存在也可能无法访问，这是标准相关性评估未捕捉的失效模式。作者将查询应答建模为一个可行性问题，分析了在给定完整语料库可回答的查询在有限检索深度下仍可回答的条件；他们证明单调截断足以保证单个查询的有限可见证性，并确定见证证书的有限生成是跨查询类获得统一边界的必要且充分条件。实验结果包括尖锐的反例，展示了在非单调截断、非有限生成查询类以及纯槽位覆盖下的失败情况，从而将可行性保持确立为独立于相关性评分的基本正确性准则。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion</div>
<div class="meta-line">Authors: Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang</div>
<div class="meta-line">First: 2026-01-20T05:00:26+00:00 · Latest: 2026-01-21T18:21:39+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13599v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13599v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a &#x27;draft-then-refine&#x27; framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model&#x27;s global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散中的扩散：在半自回归扩散模型中重获全局连贯性</div>
<div class="mono" style="margin-top:8px">全局离散扩散语言模型最引人注目的特征之一是其全局双向上下文能力。然而，现有的基于块的扩散研究倾向于引入自回归先验，这虽然带来益处，却可能导致模型在宏观层面丧失全局连贯性。为在保留半自回归范式优势的同时重获全局上下文理解能力，我们提出&#x27;扩散中的扩散&#x27;——一种&#x27;先草拟后精修&#x27;的框架，旨在克服块扩散模型固有的不可逆性与短视问题。该方法首先采用块扩散通过小尺寸块快速生成草稿，随后通过具有更大双向感受野的全局双向扩散对这些草稿进行精修。我们利用快照置信度重掩码技术识别最需要修改的关键词元，并应用混合尺度训练来扩展块扩散模型的全局能力。实验结果表明，我们的方法在OpenWebText数据集上为离散扩散模型设立了新基准。仅使用基线模型26%的微调预算，便将生成困惑度从25.7降至21.9，显著缩小了与自回归模型的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the loss of global coherence in block-based diffusion language models due to autoregressive priors, aiming to restore bidirectional contextual understanding while retaining semi-autoregressive efficiency. The proposed Diffusion in Diffusion framework employs a two-stage process: first generating draft text rapidly via block diffusion with small blocks, then refining it using global bidirectional diffusion with a larger receptive field, enhanced by snapshot confidence remasking to pinpoint tokens needing modification and mix-scale training to improve global capabilities. Experimental results on OpenWebText show the method sets a new benchmark for discrete diffusion models, reducing generative perplexity from 25.7 to 21.9 with only 26% of the fine-tuning budget of baselines, thereby narrowing the performance gap with autoregressive models.</div>
<div class="mono" style="margin-top:8px">本研究针对基于块扩散的语言模型因自回归先验而丧失全局连贯性的问题，旨在恢复双向上下文理解能力，同时保持半自回归范式的效率。提出的Diffusion in Diffusion框架采用两阶段流程：首先通过小块块扩散快速生成草稿文本，然后利用具有更大感受野的全局双向扩散进行细化，辅以快照置信度重掩码来识别需修改的关键词元，并通过混合尺度训练增强全局建模能力。在OpenWebText数据集上的实验结果表明，该方法为离散扩散模型设立了新基准，仅用基线模型26%的微调预算就将生成困惑度从25.7降至21.9，显著缩小了与自回归模型的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification</div>
<div class="meta-line">Authors: Fabi Nahian Madhurja, Rusab Sarmun, Muhammad E. H. Chowdhury, Adam Mushtak, Israa Al-Hashimi, Sohaib Bassam Zoghoul</div>
<div class="meta-line">First: 2026-01-21T18:15:47+00:00 · Latest: 2026-01-21T18:15:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15235v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model&#x27;s performance with expert radiologists, demonstrating competitive results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在二维笔画中追踪三维解剖结构：一种用于颈椎骨折识别的多阶段投影驱动方法</div>
<div class="mono" style="margin-top:8px">颈椎骨折是需要精确高效检测以进行有效临床管理的危重病症。本研究探讨了基于二维投影的椎骨分割在三维CT体积中进行椎骨级骨折检测的可行性，提出了一种用于颈椎（C1-C7）自动分析的端到端流程。通过优化的二维轴向、矢状和冠状投影近似三维体积，利用YOLOv8模型从所有视图中识别感兴趣区域并组合以近似三维颈椎区域，实现了94.45%的三维mIoU。与传统的三维分割方法相比，这种基于投影的定位策略在保持高性能的同时降低了计算复杂度。随后采用基于DenseNet121-Unet的多标签分割，利用基于方差和能量的投影，实现了87.86%的Dice分数。从这些二维分割掩模中战略性地近似三维椎骨掩模，能够提取单个椎骨体积。使用包含每个椎骨原始切片和投影的2.5D空间序列模型集成分析体积中的骨折，进行互补评估。该集成在椎骨级和患者级分别实现了68.15和82.26的F1分数，以及91.62和83.04的ROC-AUC分数。我们通过可解释性研究进一步验证了该方法，该研究提供了突出诊断相关解剖区域的显著性图可视化，并通过与专家放射科医生比较的观察者间变异性分析，展示了具有竞争力的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for precise and efficient detection of cervical spine fractures in 3D CT scans by developing a computationally efficient pipeline that leverages 2D projections. The method first localizes the spine region by combining YOLOv8 detections on optimized axial, sagittal, and coronal projections, achieving a 3D mIoU of 94.45%. It then segments individual vertebrae using a DenseNet121-Unet model on variance- and energy-based projections, attaining a Dice score of 87.86%, before analyzing the extracted 3D volumes for fractures with an ensemble of 2.5D Spatio-Sequential models. Key experimental results include vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively, with explainability studies and interobserver variability analysis showing competitive performance against expert radiologists.</div>
<div class="mono" style="margin-top:8px">本研究针对颈椎骨折在三维CT扫描中需要精确高效检测的需求，提出了一种通过二维投影降低计算复杂度的方法。该方法采用端到端流程：首先，使用YOLOv8在优化的轴向、矢状和冠状投影中识别感兴趣区域，以94.45%的mIoU定位三维脊柱区域；接着，基于DenseNet121-Unet的模型在方差和能量投影上进行多标签分割，获得87.86%的Dice分数；最后，集成2.5D时空序列模型分析提取的椎骨体积以检测骨折。关键实验结果包括椎骨级别和患者级别的F1分数分别为68.15和82.26，ROC-AUC分数分别为91.62和83.04，可解释性研究和观察者间分析表明其性能与专业放射科医生具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">From Construction to Injection: Edit-Based Fingerprints for Large Language Models</div>
<div class="meta-line">Authors: Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo, Linlin Wang</div>
<div class="meta-line">First: 2025-09-03T08:22:04+00:00 · Latest: 2026-01-21T17:56:42+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03122v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.03122v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Establishing reliable and verifiable fingerprinting mechanisms is fundamental to controlling the unauthorized redistribution of large language models (LLMs). However, existing approaches face two major challenges: (a) ensuring imperceptibility, including resistance to statistical identification and avoidance of accidental activation during fingerprint construction, and (b) preserving both model utility and fingerprint detectability under subsequent model modifications. To address these challenges, we propose an end-to-end fingerprinting framework with two components. First, we design a rule-based code-mixing fingerprint (CF) that maps natural-query-like prompts to multi-candidate targets, reducing accidental triggering via high-complexity code-mixing formulations. Second, we introduce Multi-Candidate Editing (MCEdit), which jointly optimizes multi-candidate targets and enforces margins between target and non-target outputs to improve post-modification detectability. Extensive experiments demonstrate that our framework provides a robust and practical solution for fingerprinting LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从构建到注入：基于编辑的大语言模型指纹方法</div>
<div class="mono" style="margin-top:8px">建立可靠且可验证的指纹机制对于控制大语言模型（LLM）的未经授权再分发至关重要。然而，现有方法面临两大挑战：（a）确保不可感知性，包括抵抗统计识别并避免在指纹构建过程中意外激活；（b）在后续模型修改下同时保持模型效用和指纹可检测性。为解决这些挑战，我们提出了一个包含两个组件的端到端指纹框架。首先，我们设计了一种基于规则的代码混合指纹（CF），将类似自然查询的提示映射到多候选目标，通过高复杂度的代码混合公式减少意外触发。其次，我们引入了多候选编辑（MCEdit），联合优化多候选目标，并在目标与非目标输出之间强制设置边界，以提高修改后的可检测性。大量实验表明，我们的框架为大语言模型指纹提供了一种鲁棒且实用的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for reliable and verifiable fingerprinting mechanisms to prevent unauthorized redistribution of large language models (LLMs), tackling challenges in ensuring imperceptibility and preserving utility and detectability after model modifications. The proposed method is an end-to-end framework featuring a rule-based code-mixing fingerprint that maps natural prompts to multi-candidate targets to reduce accidental activation, combined with Multi-Candidate Editing (MCEdit) to jointly optimize these targets and enforce output margins for improved post-modification robustness. Experimental results show that this framework offers a robust and practical solution for fingerprinting LLMs.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型未经授权分发的控制需求，旨在解决指纹机制在隐蔽性和抗修改鲁棒性方面的挑战。方法结合了基于规则的代码混合指纹——将自然提示映射到多个目标输出以减少意外触发，以及多候选编辑技术——联合优化这些目标并强制输出间距以保持修改后的可检测性。实验结果表明，该框架提供了鲁棒且实用的指纹解决方案，在保持模型效用的同时确保了指纹的可检测性。</div>
</details>
</div>
<div class="card">
<div class="title">Designing AI-Resilient Assessments Using Interconnected Problems: A Theoretically Grounded and Empirically Validated Framework</div>
<div class="meta-line">Authors: Kaihua Ding</div>
<div class="meta-line">First: 2025-12-11T15:53:19+00:00 · Latest: 2026-01-21T17:50:24+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures and 3 tables, under submission to IEEE FIE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10758v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.10758v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of generative AI tools has rendered traditional modular assessments in computing and data-centric education increasingly ineffective, creating a disconnect between academic evaluation and authentic skill measurement. This paper presents a theoretically grounded framework for designing AI-resilient assessments, supported by formal analysis and empirical validation.
  We make three primary contributions. First, we establish two formal propositions. (1) Assessments composed of interconnected problems, in which outputs serve as inputs to subsequent tasks, are inherently more AI-resilient than modular assessments due to their reliance on multi-step reasoning and sustained context. (2) Semi-structured problems with deterministic success criteria provide more reliable measures of student competency than fully open-ended projects, which allow AI systems to default to familiar solution templates. These results challenge widely cited recommendations in recent institutional and policy guidance that promote open-ended assessments as inherently more robust to AI assistance.
  Second, we validate these propositions through empirical analysis of three university data science courses (N = 117). We observe a substantial AI inflation effect: students achieve near-perfect scores on AI-assisted modular homework, while performance drops by approximately 30 percentage points on proctored exams (Cohen d = 1.51). In contrast, interconnected projects remain strongly aligned with modular assessments (r = 0.954, p &lt; 0.001) while maintaining AI resistance, whereas proctored exams show weaker alignment (r = 0.726, p &lt; 0.001).
  Third, we translate these findings into a practical assessment design procedure that enables educators to construct evaluations that promote deeper engagement, reflect industry practice, and resist trivial AI delegation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用互连问题设计抗人工智能评估：一个理论扎实且经验验证的框架</div>
<div class="mono" style="margin-top:8px">生成式人工智能工具的普及使得计算和数据中心教育中的传统模块化评估日益失效，导致学术评价与真实技能测量脱节。本文提出了一个理论扎实的抗人工智能评估设计框架，并辅以形式分析和实证验证。我们做出三项主要贡献：首先，确立两个形式化命题：（1）由互连问题构成的评估（即前序任务的输出作为后续任务的输入）因其依赖多步推理和持续语境，本质上比模块化评估更具抗人工智能性；（2）具有确定性成功标准的半结构化问题比完全开放式项目更能可靠地衡量学生能力，后者易使人工智能系统套用熟悉的解决方案模板。这些结论挑战了近期机构和政策指南中广泛提倡的‘开放式评估天生更能抵御人工智能辅助’的观点。其次，通过对三门大学数据科学课程（N=117）的实证分析验证了这些命题。我们观察到显著的人工智能膨胀效应：学生在人工智能辅助的模块化作业中取得接近满分的成绩，而在监考考试中表现下降约30个百分点（Cohen d=1.51）。相比之下，互连项目在保持抗人工智能性的同时，与模块化评估保持高度一致（r=0.954，p&lt;0.001），而监考考试的一致性较弱（r=0.726，p&lt;0.001）。第三，我们将这些发现转化为实用的评估设计流程，帮助教育工作者构建能促进深度参与、反映行业实践并抵御人工智能简单代劳的评估体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The proliferation of generative AI has undermined the effectiveness of traditional modular assessments in computing education, creating a gap between academic evaluation and true skill measurement. To address this, the paper proposes a framework for designing AI-resilient assessments based on interconnected problems, where outputs from one task serve as inputs for the next, and semi-structured problems with clear success criteria. Empirical validation across three university data science courses (N=117) revealed a substantial AI inflation effect, with near-perfect scores on AI-assisted modular homework but a 30-point drop on proctored exams. In contrast, interconnected projects maintained strong alignment with modular assessments while resisting AI assistance, outperforming proctored exams in measurement reliability.</div>
<div class="mono" style="margin-top:8px">生成式人工智能的普及削弱了计算教育中传统模块化评估的有效性，导致学术评价与真实技能测量脱节。为此，本文提出了一个设计抗人工智能评估的框架，其理论基础是：相互关联的问题（即前序任务的输出作为后续任务的输入）因依赖多步推理而更具抵抗力，且具有明确标准的半结构化问题比完全开放式项目更可靠。通过对三门大学数据科学课程（N=117）的实证验证，发现了显著的人工智能分数膨胀效应，即学生在人工智能辅助的模块化作业中得分接近满分，但在监考考试中表现下降约30个百分点；而相互关联的项目在保持与模块化评估高度一致的同时，展现出了抗人工智能特性。</div>
</details>
</div>
<div class="card">
<div class="title">SPECTRE: Conditional System Prompt Poisoning to Hijack LLMs</div>
<div class="meta-line">Authors: Viet Pham, Thai Le</div>
<div class="meta-line">First: 2025-05-22T16:47:15+00:00 · Latest: 2026-01-21T17:45:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16888v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.16888v3">PDF</a> · <a href="https://github.com/vietph34/CAIN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly deployed via third-party system prompts downloaded from public marketplaces. We identify a critical supply-chain vulnerability: conditional system prompt poisoning, where an adversary injects a ``sleeper agent&#x27;&#x27; into a benign-looking prompt. Unlike traditional jailbreaks that aim for broad refusal-breaking, our proposed framework, SPECTRE, optimizes system prompts to trigger LLMs to output targeted, compromised responses only for specific queries (e.g., ``Who should I vote for the US President?&#x27;&#x27;) while maintaining high utility on benign inputs. Operating in a strict black-box setting without model weight access, SPECTRE utilizes a two-stage optimization including a global semantic search followed by a greedy lexical refinement. Tested on open-source models and commercial APIs (GPT-4o-mini, GPT-3.5), SPECTRE achieves up to 70% F1 reduction on targeted queries with minimal degradation to general capabilities. We further demonstrate that these poisoned prompts evade standard defenses, including perplexity filters and typo-correction, by exploiting the natural noise found in real-world system prompts. Our code and data are available at https://github.com/vietph34/CAIN. WARNING: Our paper contains examples that might be sensitive to the readers!</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPECTRE：通过条件性系统提示词投毒劫持大语言模型</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）越来越多地通过从公共市场下载的第三方系统提示词进行部署。我们发现一个关键的供应链漏洞：条件性系统提示词投毒，攻击者可将‘潜伏代理’注入看似良性的提示词中。与传统旨在广泛突破拒绝机制的越狱攻击不同，我们提出的SPECTRE框架通过优化系统提示词，仅在特定查询（例如‘我该投票给哪位美国总统候选人？’）时触发LLMs输出定向的、受操控的响应，同时在良性输入上保持高可用性。在严格的黑盒设置下（无模型权重访问权限），SPECTRE采用两阶段优化方法，包括全局语义搜索和贪婪词汇精炼。在开源模型和商业API（GPT-4o-mini、GPT-3.5）上的测试表明，SPECTRE能在目标查询上实现高达70%的F1分数下降，而对通用能力影响极小。我们进一步证明，这些被投毒的提示词通过利用真实系统提示词中的自然噪声，可规避包括困惑度过滤器和拼写纠正在内的标准防御机制。代码与数据公开于：https://github.com/vietph34/CAIN。警告：本文包含可能引起读者敏感的内容示例！</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the security risks of deploying LLMs with third-party system prompts from public marketplaces, which are vulnerable to supply-chain attacks. The proposed method, SPECTRE, introduces a conditional system prompt poisoning framework that optimizes prompts to act as sleeper agents, triggering compromised responses only for specific targeted queries while preserving utility on benign inputs, using a two-stage black-box optimization involving global semantic search and greedy lexical refinement. Experimental results show SPECTRE achieves up to a 70% F1 reduction on targeted queries with minimal general capability degradation on models like GPT-4o-mini and GPT-3.5, and it evades standard defenses such as perplexity filters and typo-correction by exploiting natural noise in real-world prompts.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型部署中的供应链漏洞，即从公共市场下载的第三方系统提示可能被投毒的问题。提出的SPECTRE框架采用两阶段黑盒优化方法，结合全局语义搜索和贪婪词汇精炼，来制作仅在特定目标查询时触发妥协响应、同时保持对良性输入实用性的提示。在开源模型和商业API上的实验表明，SPECTRE能将目标查询的F1分数降低高达70%，且对通用性能影响最小，并能通过利用真实世界提示中的自然噪声来规避包括困惑度过滤器和拼写纠正在内的标准防御措施。</div>
</details>
</div>
<div class="card">
<div class="title">Deaf and Hard of Hearing Access to Intelligent Personal Assistants: Comparison of Voice-Based Options with an LLM-Powered Touch Interface</div>
<div class="meta-line">Authors: Paige S. DeVries, Michaela Okosi, Ming Li, Nora Dunphy. Gidey Gezae, Dante Conway, Abraham Glasser, Raja Kushalnagar, Christian Vogler</div>
<div class="meta-line">First: 2026-01-21T17:33:00+00:00 · Latest: 2026-01-21T17:33:00+00:00</div>
<div class="meta-line">Comments: Accepted for publication in ACM CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15209v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate intelligent personal assistants (IPAs) accessibility for deaf and hard of hearing (DHH) people who can use their voice in everyday communication. The inability of IPAs to understand diverse accents including deaf speech renders them largely inaccessible to non-signing and speaking DHH individuals. Using an Echo Show, we compare the usability of natural language input via spoken English; with Alexa&#x27;s automatic speech recognition and a Wizard-of-Oz setting with a trained facilitator re-speaking commands against that of a large language model (LLM)-assisted touch interface in a mixed-methods study. The touch method was navigated through an LLM-powered &quot;task prompter,&quot; which integrated the user&#x27;s history and smart environment to suggest contextually-appropriate commands. Quantitative results showed no significant differences across both spoken English conditions vs LLM-assisted touch. Qualitative results showed variability in opinions on the usability of each method. Ultimately, it will be necessary to have robust deaf-accented speech recognized natively by IPAs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>听障人士对智能个人助理的访问：语音交互与基于大语言模型的触控界面对比</div>
<div class="mono" style="margin-top:8px">本研究探讨了在日常交流中能够使用语音的聋哑及听障人士对智能个人助理的可访问性。由于智能个人助理无法理解包括聋人语音在内的多样化口音，导致其对于非手语且使用口语的听障群体基本不可用。通过一项混合方法研究，我们使用Echo Show设备，比较了以下方式的可用性：通过英语口语的自然语言输入（结合Alexa自动语音识别及由训练有素的协助者复述指令的“绿野仙踪”模拟设置），与基于大语言模型的触控界面。触控方法通过一个由大语言模型驱动的“任务提示器”进行导航，该提示器整合用户历史与智能环境以推荐情境适配的指令。定量结果显示，两种英语口语条件与基于大语言模型的触控方式之间无显著差异；定性结果则显示用户对各方法可用性的评价存在差异。最终，实现智能个人助理原生支持对聋人口音的鲁棒识别至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the accessibility gap in intelligent personal assistants (IPAs) for deaf and hard of hearing (DHH) individuals who use spoken language, as current IPAs often fail to understand diverse accents including deaf speech. The study compared the usability of three input methods on an Echo Show: standard voice input using Alexa&#x27;s automatic speech recognition, a Wizard-of-Oz condition where a facilitator re-spoke commands, and a novel touch interface powered by a large language model (LLM) that suggested contextually-appropriate commands based on user history and environment. Quantitative findings revealed no significant performance differences between the two spoken conditions and the LLM-assisted touch method, while qualitative feedback indicated varied user preferences regarding the usability of each approach. The work concludes that robust native recognition of deaf-accented speech by IPAs remains essential.</div>
<div class="mono" style="margin-top:8px">本研究针对使用口语的聋人及听力障碍人士，探讨智能个人助理的可访问性，因为标准助理通常无法理解包括聋人口音在内的多样口音。研究比较了基于语音的交互（使用标准自动语音识别和由人复述命令的 Wizard-of-Oz 设置）与基于大型语言模型的触摸界面（通过整合用户历史和环境信息的“任务提示器”提供情境化命令建议）的可用性。实验结果显示，语音和触摸方法在可用性上没有显著的定量差异，但定性反馈揭示了用户偏好的多样性，这强调了智能个人助理需要原生且稳健地识别聋人口音的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Targeting Alignment: Extracting Safety Classifiers of Aligned LLMs</div>
<div class="meta-line">Authors: Jean-Charles Noirot Ferrand, Yohan Beugin, Eric Pauley, Ryan Sheatsley, Patrick McDaniel</div>
<div class="meta-line">First: 2025-01-27T22:13:05+00:00 · Latest: 2026-01-21T17:29:42+00:00</div>
<div class="meta-line">Comments: Accepted to 2026 IEEE Secure and Trustworthy Machine Learning Conference (SaTML)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.16534v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.16534v3">PDF</a> · <a href="https://github.com/jcnf0/targeting-alignment">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment in large language models (LLMs) is used to enforce guidelines such as safety. Yet, alignment fails in the face of jailbreak attacks that modify inputs to induce unsafe outputs. In this paper, we introduce and evaluate a new technique for jailbreak attacks. We observe that alignment embeds a safety classifier in the LLM responsible for deciding between refusal and compliance, and seek to extract an approximation of this classifier: a surrogate classifier. To this end, we build candidate classifiers from subsets of the LLM. We first evaluate the degree to which candidate classifiers approximate the LLM&#x27;s safety classifier in benign and adversarial settings. Then, we attack the candidates and measure how well the resulting adversarial inputs transfer to the LLM. Our evaluation shows that the best candidates achieve accurate agreement (an F1 score above 80%) using as little as 20% of the model architecture. Further, we find that attacks mounted on the surrogate classifiers can be transferred to the LLM with high success. For example, a surrogate using only 50% of the Llama 2 model achieved an attack success rate (ASR) of 70% with half the memory footprint and runtime -- a substantial improvement over attacking the LLM directly, where we only observed a 22% ASR. These results show that extracting surrogate classifiers is an effective and efficient means for modeling (and therein addressing) the vulnerability of aligned models to jailbreaking attacks. The code is available at https://github.com/jcnf0/targeting-alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>目标对齐：提取对齐大语言模型的安全分类器</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的对齐旨在强化安全性等准则，但在通过修改输入诱导不安全输出的越狱攻击面前常会失效。本文提出并评估了一种新的越狱攻击技术。我们观察到对齐机制在LLM中嵌入了负责在拒绝与遵从间决策的安全分类器，并尝试提取该分类器的近似替代分类器。为此，我们从LLM的子集中构建候选分类器。首先评估候选分类器在正常与对抗场景中对LLM安全分类器的近似程度；随后攻击候选分类器，并测量所得对抗样本向原始LLM的迁移效果。实验表明，最优候选分类器仅需20%的模型架构即可实现精准匹配（F1分数超过80%）。进一步发现，针对替代分类器的攻击能以高成功率迁移至原始LLM：例如仅使用Llama 2模型50%参数的替代分类器，在内存占用和运行时间减半的情况下实现了70%的攻击成功率（ASR），而直接攻击原始LLM仅获得22%的ASR。这些结果表明，提取替代分类器是建模（进而应对）对齐模型越狱漏洞的高效方法。代码发布于https://github.com/jcnf0/targeting-alignment。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the vulnerability of aligned large language models (LLMs) to jailbreak attacks that bypass safety guidelines. The method involves extracting a surrogate safety classifier from a subset of the LLM&#x27;s architecture to approximate its internal refusal mechanism. Experimental results show that surrogate classifiers using only 20% of the model can achieve over 80% F1 score agreement with the original model&#x27;s safety decisions, and attacks generated on a 50% surrogate for Llama 2 achieve a 70% attack success rate, significantly higher than the 22% rate from direct attacks on the full model, demonstrating an efficient approach to model and exploit alignment vulnerabilities.</div>
<div class="mono" style="margin-top:8px">本研究针对对齐后大语言模型（LLM）易受越狱攻击绕过安全准则的漏洞，提出通过提取模型架构子集构建代理安全分类器，以近似其内部拒绝机制。实验结果表明，仅使用20%模型构建的分类器与原始模型具有高度一致性（F1 &gt;80%），且在此类代理分类器上优化的攻击能有效迁移至完整模型，在50%参数的Llama 2代理上实现了70%的攻击成功率，而直接攻击仅获得22%的成功率。</div>
</details>
</div>
<div class="card">
<div class="title">OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions</div>
<div class="meta-line">Authors: Maxim Popov, Regina Kurkova, Mikhail Iumanov, Jaafar Mahmoud, Sergey Kolyubin</div>
<div class="meta-line">Venue: IROS</div>
<div class="meta-line">First: 2025-03-13T13:07:51+00:00 · Latest: 2026-01-21T17:25:25+00:00</div>
<div class="meta-line">Comments: Project page: https://be2rlab.github.io/OSMa-Bench/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10331v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.10331v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://be2rlab.github.io/OSMa-Bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open Semantic Mapping (OSM) is a key technology in robotic perception, combining semantic segmentation and SLAM techniques. This paper introduces a dynamically configurable and highly automated LLM/LVLM-powered pipeline for evaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark). The study focuses on evaluating state-of-the-art semantic mapping algorithms under varying indoor lighting conditions, a critical challenge in indoor environments. We introduce a novel dataset with simulated RGB-D sequences and ground truth 3D reconstructions, facilitating the rigorous analysis of mapping performance across different lighting conditions. Through experiments on leading models such as ConceptGraphs, BBQ, and OpenScene, we evaluate the semantic fidelity of object recognition and segmentation. Additionally, we introduce a Scene Graph evaluation method to analyze the ability of models to interpret semantic structure. The results provide insights into the robustness of these models, forming future research directions for developing resilient and adaptable robotic systems. Project page is available at https://be2rlab.github.io/OSMa-Bench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSMa-Bench：评估不同光照条件下的开放语义建图</div>
<div class="mono" style="margin-top:8px">开放语义建图（OSM）是机器人感知领域的关键技术，融合了语义分割与SLAM方法。本文提出了一种动态可配置、高度自动化的LLM/LVLM驱动评估流程——OSMa-Bench（开放语义建图基准），用于评估OSM解决方案。研究聚焦于评估先进语义建图算法在室内多变光照条件下的表现，这是室内环境中的关键挑战。我们提出了包含模拟RGB-D序列与真实三维重建标注的全新数据集，支持对不同光照条件下建图性能的严格分析。通过对ConceptGraphs、BBQ、OpenScene等前沿模型的实验，评估了物体识别与分割的语义保真度。此外，我们引入了场景图评估方法，以分析模型解析语义结构的能力。实验结果揭示了这些模型的鲁棒性特征，为开发具有环境适应能力的机器人系统指明了未来研究方向。项目页面详见：https://be2rlab.github.io/OSMa-Bench/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to evaluate the robustness of Open Semantic Mapping (OSM) systems, which integrate semantic segmentation with SLAM, under challenging and variable indoor lighting conditions. The authors propose OSMa-Bench, a benchmark that employs a configurable, automated pipeline powered by large language and vision-language models to assess state-of-the-art algorithms. Using a novel dataset of simulated RGB-D sequences with ground truth 3D reconstructions, the study evaluates models like ConceptGraphs, BBQ, and OpenScene on semantic fidelity and introduces a Scene Graph method to analyze semantic structure interpretation. Experimental findings reveal the varying robustness of these models to lighting changes, offering insights for developing more resilient robotic perception systems.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决开放语义建图（OSM）系统在多变且具有挑战性的室内光照条件下的鲁棒性评估需求，该系统结合了语义分割与SLAM技术。作者提出了一个由大语言/视觉语言模型驱动的基准框架OSMa-Bench，并引入了一个包含模拟RGB-D序列和真实3D重建的新数据集，以系统测试不同光照场景下的建图性能。通过对ConceptGraphs、BBQ和OpenScene等模型进行实验，评估其语义保真度并采用新颖的场景图方法进行分析，结果揭示了这些模型的鲁棒性，为未来开发更具适应性的机器人感知系统指明了方向。</div>
</details>
</div>
<div class="card">
<div class="title">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-21T17:15:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayesianVLA：基于潜在动作查询的视觉语言动作模型贝叶斯分解方法</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型在机器人操作中展现出潜力，但常难以泛化至新指令或复杂多任务场景。本文发现当前训练范式存在关键缺陷：目标驱动的数据收集导致数据集偏差。此类数据集中，仅凭视觉观测即可高度预测语言指令，致使指令与动作间的条件互信息趋近于零，此现象称为“信息坍缩”。因此模型退化为仅依赖视觉的策略，忽略语言约束并在分布外场景中失效。为解决该问题，我们提出BayesianVLA框架，通过贝叶斯分解强制实现指令跟随。通过引入可学习的潜在动作查询，构建双分支架构以同时估计仅视觉先验$p(a \mid v)$和语言条件后验$π(a \mid v, \ell)$，进而优化策略以最大化动作与指令间的条件点互信息。该目标有效惩罚视觉捷径，奖励能显式解释语言指令的动作。无需新增数据，BayesianVLA显著提升泛化能力。在SimplerEnv和RoboCasa上的大量实验证明其显著优势，其中在挑战性分布外基准测试SimplerEnv上提升11.3%，验证了本方法在动作中稳健锚定语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions are predictable from visual context, causing an Information Collapse that reduces models to vision-only policies. To counteract this, BayesianVLA introduces a Bayesian decomposition framework using learnable Latent Action Queries to separately model a vision-only prior and a language-conditioned posterior, optimizing the policy to maximize the conditional pointwise mutual information between actions and instructions. Experiments on SimplerEnv and RoboCasa show the method substantially improves generalization without new data, achieving an 11.3% performance gain on an out-of-distribution benchmark.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差导致泛化能力不足，其中语言指令可从视觉观察中预测，引发信息坍缩，使模型退化为仅依赖视觉的策略。为解决该问题，BayesianVLA提出一种贝叶斯分解框架，通过可学习的潜在动作查询分别建模视觉先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息。在SimplerEnv和RoboCasa上的实验表明，该方法显著提升了泛化性能，如在分布外基准测试中取得了11.3%的改进，验证了其将语言可靠地融入动作的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Where Do AI Coding Agents Fail? An Empirical Study of Failed Agentic Pull Requests in GitHub</div>
<div class="meta-line">Authors: Ramtin Ehsani, Sakshi Pathak, Shriya Rawal, Abdullah Al Mujahid, Mia Mohammad Imran, Preetha Chatterjee</div>
<div class="meta-line">First: 2026-01-21T17:12:46+00:00 · Latest: 2026-01-21T17:12:46+00:00</div>
<div class="meta-line">Comments: Accepted at International Mining Software Repositories Conference (MSR 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15195v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI coding agents are now submitting pull requests (PRs) to software projects, acting not just as assistants but as autonomous contributors. As these agentic contributions are rapidly increasing across real repositories, little is known about how they behave in practice and why many of them fail to be merged. In this paper, we conduct a large-scale study of 33k agent-authored PRs made by five coding agents across GitHub. (RQ1) We first quantitatively characterize merged and not-merged PRs along four broad dimensions: 1) merge outcomes across task types, 2) code changes, 3) CI build results, and 4) review dynamics. We observe that tasks related to documentation, CI, and build update achieve the highest merge success, whereas performance and bug-fix tasks perform the worst. Not-merged PRs tend to involve larger code changes, touch more files, and often do not pass the project&#x27;s CI/CD pipeline validation. (RQ2) To further investigate why some agentic PRs are not merged, we qualitatively analyze 600 PRs to derive a hierarchical taxonomy of rejection patterns. This analysis complements the quantitative findings in RQ1 by uncovering rejection reasons not captured by quantitative metrics, including lack of meaningful reviewer engagement, duplicate PRs, unwanted feature implementations, and agent misalignment. Together, our findings highlight key socio-technical and human-AI collaboration factors that are critical to improving the success of future agentic workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI编码代理在何处失败？GitHub中失败代理式拉取请求的实证研究</div>
<div class="mono" style="margin-top:8px">AI编码代理现正向软件项目提交拉取请求（PR），不仅作为辅助工具，更成为自主贡献者。随着这类代理式贡献在实际代码库中快速增长，人们对其实际行为模式及大量PR未能合并的原因知之甚少。本文对GitHub上五种编码代理生成的3.3万条代理撰写PR展开大规模研究。（研究问题1）我们首先从四个维度定量分析已合并与未合并PR：1）跨任务类型的合并结果，2）代码变更，3）持续集成构建结果，4）评审动态。研究发现文档、CI和构建更新类任务合并成功率最高，而性能优化与缺陷修复任务表现最差。未合并PR往往涉及更大规模的代码变更、触及更多文件，且常未通过项目的CI/CD流水线验证。（研究问题2）为深入探究代理式PR未合并原因，我们对600条PR进行定性分析，构建了分层式的拒绝模式分类体系。该分析通过揭示定量指标未涵盖的拒绝原因（包括缺乏实质性评审互动、重复PR、非预期功能实现及代理行为偏差），补充了研究问题1的定量发现。综合研究结果凸显了影响未来代理式工作流成功的关键社会技术因素与人机协作要素。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates why many AI coding agents&#x27; pull requests (PRs) fail to be merged in real-world software projects, given their increasing autonomous contributions. The authors conducted a large-scale empirical analysis of 33,000 agent-authored PRs from five agents on GitHub, quantitatively characterizing merged and unmerged PRs across task types, code changes, CI build results, and review dynamics. They found that documentation and CI update tasks had the highest merge success, while performance and bug-fix tasks performed worst; unmerged PRs often involved larger changes, touched more files, and failed CI validation. A subsequent qualitative analysis of 600 PRs revealed a taxonomy of rejection reasons, including lack of reviewer engagement, duplicate PRs, unwanted features, and agent misalignment, highlighting critical socio-technical factors for improving agentic workflows.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究AI编码代理在真实软件项目中提交的拉取请求为何经常无法被合并，鉴于其在GitHub上的自主贡献日益增多。研究人员对来自五个代理的33,000个代理撰写的拉取请求进行了大规模实证分析，从任务类型、代码变更、CI构建结果和评审动态四个维度定量描述了已合并与未合并的PR。主要发现表明，文档和CI更新任务的合并成功率最高，而性能和错误修复任务表现最差；未合并的PR通常涉及更大、更复杂的变更且未能通过CI验证。对600个PR的补充定性分析揭示了一个拒绝原因的分类体系，包括缺乏有意义的评审参与、重复PR、不需要的功能实现以及代理错位，凸显了改进人机协作的关键社会技术因素。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Large Language Models for ABAP Code Generation: An Empirical Study on Iterative Improvement by Compiler Feedback</div>
<div class="meta-line">Authors: Stephan Wallraven, Tim Köhne, Hartmut Westenberger, Andreas Moser</div>
<div class="meta-line">First: 2026-01-21T17:06:41+00:00 · Latest: 2026-01-21T17:06:41+00:00</div>
<div class="meta-line">Comments: 20 pages, 10 figures, Author: Hartmut Westenberger (ORCID: 0009-0009-9063-8318)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15188v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work investigates the performance of Large Language Models (LLMs) in generating ABAP code. Despite successful applications of generative AI in many programming languages, there are hardly any systematic analyses of ABAP code generation to date. The aim of the study is to empirically analyze to what extent various LLMs can generate syntactically correct and functional ABAP code, how effectively they use compiler feedback for iterative improvement, and which task types pose special challenges. For this purpose, a benchmark with 180 tasks is conducted, consisting of adapted HumanEval tasks and practical SAP scenarios. The results show significant performance differences between the models: more powerful LLMs achieve success rates of around 75% after several iterations and benefit greatly from compiler feedback, while smaller models perform significantly weaker. Overall, the study highlights the high potential of powerful LLMs for ABAP development processes, especially in iterative error correction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型在ABAP代码生成中的基准测试：基于编译器反馈迭代改进的实证研究</div>
<div class="mono" style="margin-top:8px">本研究探讨大型语言模型生成ABAP代码的性能。尽管生成式AI已在多种编程语言中成功应用，但迄今鲜有对ABAP代码生成的系统分析。本研究旨在实证分析：各类LLM生成语法正确且功能完整的ABAP代码的能力、利用编译器反馈进行迭代改进的效果，以及哪些任务类型构成特殊挑战。为此构建了包含180项任务的基准测试集，涵盖改编的HumanEval任务与实际SAP场景。结果显示模型间性能差异显著：更强LLM经数次迭代后成功率约75%，且能充分利用编译器反馈；较小模型表现明显较弱。总体而言，本研究揭示了高性能LLM在ABAP开发流程中的巨大潜力，尤其在迭代纠错方面。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the lack of systematic analysis on ABAP code generation by Large Language Models (LLMs), aiming to empirically evaluate their ability to produce syntactically correct and functional code, utilize compiler feedback for iterative improvement, and identify challenging task types. The method involves a benchmark of 180 tasks, including adapted HumanEval problems and practical SAP scenarios, where various LLMs generate code and iteratively refine it based on compiler feedback. Key findings reveal significant performance disparities: more powerful LLMs achieve success rates around 75% after multiple iterations and benefit substantially from compiler feedback, whereas smaller models perform markedly worse, highlighting the potential of advanced LLMs in ABAP development, particularly for iterative error correction.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型（LLMs）在ABAP代码生成方面缺乏系统分析的问题展开，尽管LLMs在其他编程语言中已取得成功。它通过实证评估不同LLM生成语法正确且功能正常的ABAP代码的能力，特别考察了它们利用编译器反馈进行迭代改进的效果，并识别了具有特殊挑战的任务类型。基于包含180个任务的基准测试（改编自HumanEval和实际SAP场景），研究发现模型性能存在显著差异：更强大的LLM经过多次迭代后成功率可达约75%，且从编译器反馈中获益极大，而较小模型表现明显较差。结果凸显了先进LLM通过迭代纠错提升ABAP开发流程的巨大潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Management of a Deep Learning-Based Anomaly Detection System for 5G Networks</div>
<div class="meta-line">Authors: Lorenzo Fernández Maimó, Alberto Huertas Celdrán, Manuel Gil Pérez, Félix J. García Clemente, Gregorio Martínez Pérez</div>
<div class="meta-line">First: 2026-01-21T16:54:19+00:00 · Latest: 2026-01-21T16:54:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15177v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15177v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fog and mobile edge computing (MEC) will play a key role in the upcoming fifth generation (5G) mobile networks to support decentralized applications, data analytics and management into the network itself by using a highly distributed compute model. Furthermore, increasing attention is paid to providing user-centric cybersecurity solutions, which particularly require collecting, processing and analyzing significantly large amount of data traffic and huge number of network connections in 5G networks. In this regard, this paper proposes a MEC-oriented solution in 5G mobile networks to detect network anomalies in real-time and in autonomic way. Our proposal uses deep learning techniques to analyze network flows and to detect network anomalies. Moreover, it uses policies in order to provide an efficient and dynamic management system of the computing resources used in the anomaly detection process. The paper presents relevant aspects of the deployment of the proposal and experimental results to show its performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向5G网络的深度学习异常检测系统动态管理</div>
<div class="mono" style="margin-top:8px">雾计算与移动边缘计算（MEC）将在即将到来的第五代（5G）移动网络中发挥关键作用，通过高度分布式的计算模型，在网络内部支持去中心化应用、数据分析与管理。此外，提供以用户为中心的网络安全解决方案日益受到重视，这尤其需要在5G网络中收集、处理和分析海量数据流量及大量网络连接。为此，本文提出一种面向5G移动网络的MEC解决方案，以实时且自主的方式检测网络异常。该方案采用深度学习技术分析网络流量并检测异常，同时利用策略机制对异常检测过程中使用的计算资源进行高效动态管理。文中详细阐述了方案部署的相关要点，并通过实验结果展示了其性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for decentralized, real-time cybersecurity in 5G networks, which generate massive data traffic, this paper proposes a mobile edge computing (MEC)-oriented anomaly detection system. The method employs deep learning to analyze network flows for anomaly detection and utilizes policies for the dynamic management of the computing resources involved in this process. Experimental results demonstrate the system&#x27;s performance in real-time, autonomic anomaly detection.</div>
<div class="mono" style="margin-top:8px">为应对5G网络海量数据流量带来的去中心化、实时网络安全需求，本文提出了一种面向移动边缘计算（MEC）的异常检测系统。该方法利用深度学习分析网络流量以检测异常，并集成一个基于策略的框架，用于动态管理检测过程中使用的计算资源。实验结果展示了该系统在实现高效、自主的实时检测方面的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Finding Kissing Numbers with Game-theoretic Reinforcement Learning</div>
<div class="meta-line">Authors: Chengdong Ma, Théo Tao Zhaowei, Pengyu Li, Minghao Liu, Haojun Chen, Zihao Mao, Yuan Cheng, Yuan Qi, Yaodong Yang</div>
<div class="meta-line">First: 2025-11-17T14:02:00+00:00 · Latest: 2026-01-21T16:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13391v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13391v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Since Isaac Newton first studied the Kissing Number Problem in 1694, determining the maximal number of non-overlapping spheres around a central sphere has remained a fundamental challenge. This problem represents the local analogue of Hilbert&#x27;s 18th problem on sphere packing, bridging geometry, number theory, and information theory. Although significant progress has been made through lattices and codes, the irregularities of high-dimensional geometry and exponentially growing combinatorial complexity beyond 8 dimensions, which exceeds the complexity of Go game, limit the scalability of existing methods. Here we model this problem as a two-player matrix completion game that can be fully parallelized at large scale, and train the game-theoretic reinforcement learning system, PackingStar, to efficiently explore high-dimensional spaces. The matrix entries represent pairwise cosines of sphere center vectors; one player fills entries while another corrects suboptimal ones, jointly maximizing the matrix size, corresponding to the kissing number. This cooperative dynamics substantially improves sample quality, making the extremely large spaces tractable. PackingStar reproduces previous configurations and surpasses all human-known records from dimensions 25 to 31, with the configuration in 25 dimensions geometrically corresponding to the Leech lattice and suggesting possible optimality. It achieves the first breakthrough beyond rational structures from 1971 in 13 dimensions, discovers over 6000 new structures in 14 and other dimensions, and establishes new records for generalized kissing configurations under various angular constraints. These results demonstrate AI&#x27;s power to explore high-dimensional spaces beyond human intuition and open new pathways for the Kissing Number Problem and broader geometry problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于博弈论强化学习的接吻数求解</div>
<div class="mono" style="margin-top:8px">自艾萨克·牛顿于1694年首次研究接吻数问题以来，确定中心球周围非重叠球体的最大数量始终是基础性难题。该问题是希尔伯特第18个球堆积问题的局部对应，连接了几何、数论与信息论。尽管通过格与编码取得重要进展，但8维以上高维几何的不规则性与指数级增长的组合复杂度（超过围棋复杂度）限制了现有方法的可扩展性。本研究将该问题建模为可大规模并行化的双玩家矩阵补全博弈，并训练博弈论强化学习系统PackingStar以高效探索高维空间。矩阵元素表示球心向量间的成对余弦值；一方填充元素，另一方修正次优元素，共同最大化矩阵尺寸以对应接吻数。这种协作机制显著提升样本质量，使超大规模空间可处理。PackingStar复现了既有构型，并在25至31维超越所有人已知记录——其中25维构型几何对应Leech格并暗示潜在最优性。系统实现了自1971年有理结构以来在13维的首次突破，在14维及其他维度发现6000余个新结构，并在多种角度约束下建立了广义接吻构型的新记录。这些成果证明了人工智能探索超越人类直觉的高维空间的能力，为接吻数问题及更广泛的几何问题开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Kissing Number Problem, which seeks the maximum number of non-overlapping spheres that can touch a central sphere, is a long-standing geometric challenge with connections to sphere packing and high-dimensional complexity. To address the combinatorial explosion in dimensions beyond eight, this work models the problem as a two-player matrix completion game, where entries represent pairwise cosines of sphere center vectors; one player proposes entries while another corrects suboptimal ones, jointly maximizing the matrix size through a cooperative reinforcement learning system called PackingStar. This method efficiently explores high-dimensional spaces, reproducing known configurations and surpassing all previous human records from dimensions 25 to 31, achieving the first breakthrough beyond rational structures since 1971 in dimension 13, discovering over 6000 new structures in dimension 14 and others, and setting new records for generalized kissing configurations under various angular constraints.</div>
<div class="mono" style="margin-top:8px">接吻数问题旨在寻找一个中心球体周围可接触的最大非重叠球体数量，这是一个与球体堆积和信息论相关的长期几何难题。为解决8维以上维度中指数级增长的组合复杂性，研究者将该问题建模为一个双玩家矩阵补全博弈：一名玩家填充表示球心向量对间余弦的矩阵条目，另一名玩家修正次优条目，并利用强化学习训练了名为PackingStar的大规模并行系统。该方法重现了已知构型，在25至31维上超越了所有已知人类记录，其中25维的构型几何上对应于Leech格并暗示了最优性，在13维上取得了自1971年以来的首次突破，在14维等维度发现了超过6000个新结构，并为各种角度约束下的广义接吻构型建立了新记录。</div>
</details>
</div>
<div class="card">
<div class="title">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</div>
<div class="meta-line">Authors: Zanlin Ni, Shenzhi Wang, Yang Yue, Tianyu Yu, Weilin Zhao, Yeguo Hua, Tianyi Chen, Jun Song, Cheng Yu, Bo Zheng, Gao Huang</div>
<div class="meta-line">First: 2026-01-21T16:41:58+00:00 · Latest: 2026-01-21T16:41:58+00:00</div>
<div class="meta-line">Comments: Code and pre-trained models: https://github.com/LeapLabTHU/JustGRPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15165v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15165v1">PDF</a> · <a href="https://github.com/LeapLabTHU/JustGRPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://nzl-thu.github.io/the-flexibility-trap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: https://nzl-thu.github.io/the-flexibility-trap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵活性陷阱：为何任意顺序生成会限制扩散语言模型的推理潜力</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）突破了传统LLM严格的从左到右约束，实现了按任意顺序生成词元。直观上，这种灵活性意味着其解空间严格包含了固定自回归轨迹，理论上可为数学和编程等通用任务释放更优的推理潜力。因此，许多研究采用强化学习（RL）来激发dLLMs的推理能力。本文揭示了一个反直觉的现实：当前形式的任意顺序生成非但未扩展dLLMs的推理边界，反而使其收窄。我们发现dLLMs倾向于利用顺序灵活性规避对探索至关重要的高不确定性词元，导致解空间过早坍缩。这一发现挑战了现有dLLMs强化学习方法的前提——这些方法往往为保持顺序灵活性而投入大量复杂度（如处理组合轨迹和难解似然）。我们证明，通过主动放弃任意顺序生成并采用标准组相对策略优化（GRPO），能更有效地激发推理能力。所提方法JustGRPO极简却效果显著（如在GSM8K上达89.1%准确率），同时完整保留了dLLMs的并行解码能力。项目页面：https://nzl-thu.github.io/the-flexibility-trap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study challenges the assumption that arbitrary token generation order in diffusion language models inherently enhances reasoning capabilities, arguing that it instead leads to a premature collapse of the solution space as models exploit flexibility to avoid high-uncertainty tokens crucial for exploration. The authors propose JustGRPO, a method that intentionally forgoes arbitrary order generation and applies standard Group Relative Policy Optimization, demonstrating its effectiveness through experiments such as achieving 89.1% accuracy on GSM8K while retaining parallel decoding ability.</div>
<div class="mono" style="margin-top:8px">本文研究了扩散语言模型（dLLMs）的推理能力，这类模型允许以任意顺序生成标记，其灵活性理论上优于固定的自回归模型。作者发现，与直觉相反，这种任意顺序生成会导致模型绕过对探索至关重要的高不确定性标记，使解空间过早坍缩，从而阻碍推理。为解决此问题，他们提出了JustGRPO方法，该方法有意放弃任意顺序生成，转而应用标准的组相对策略优化。该方法虽极简却非常有效，在GSM8K上达到了89.1%的准确率，同时保留了并行解码能力。</div>
</details>
</div>
<div class="card">
<div class="title">V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks</div>
<div class="meta-line">Authors: Yaru Liu, Ao-bo Wang, Nanyang Ye</div>
<div class="meta-line">First: 2026-01-21T16:41:51+00:00 · Latest: 2026-01-21T16:41:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15164v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15164v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently &quot;succeed&quot; without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., &quot;get ready for work&quot;) into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out &quot;silent failures&quot; where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-CAGE：面向可扩展长程具身任务的情境感知生成与验证框架</div>
<div class="mono" style="margin-top:8px">从合成数据中学习长程具身行为仍面临挑战：生成场景常物理不可行，语言驱动程序常未满足任务语义即“成功”，高层指令需落地为可执行动作序列。为突破这些局限，我们提出V-CAGE——一个闭环框架，用于大规模生成鲁棒且语义对齐的操作数据集。首先，我们提出情境感知实例化机制，在场景合成中强制几何一致性。通过动态维护物体放置时的禁止空间区域映射，系统能防止穿模并确保杂乱环境中可达无冲突的配置。其次，为弥合抽象意图与底层控制间的鸿沟，我们采用分层指令分解模块，将高层目标（如“准备工作”）解构为组合动作基元，促进连贯的长程规划。关键的是，我们通过基于视觉语言模型的验证循环确保语义正确性：该模型作为视觉评判器，在每个子任务后执行严格拒绝采样，过滤代码执行但未达成视觉目标的“静默失败”。实验表明，相比未验证基线，V-CAGE生成的数据集具有更优的物理与语义保真度，显著提升下游策略的成功率与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating physically plausible scenes and semantically correct action sequences for long-horizon embodied tasks from synthetic data, where existing methods often produce implausible scenes or suffer from &#x27;silent failures&#x27; where code executes without achieving the intended goal. The proposed V-CAGE framework introduces a context-aware instantiation mechanism to ensure geometric consistency in scene synthesis by maintaining a map of prohibited areas to prevent object interpenetration, and employs a hierarchical instruction decomposition module to break high-level goals into executable action primitives. A key component is a VLM-based verification loop that performs rejection sampling after each subtask to filter out semantically incorrect outcomes. Experimental results show that V-CAGE generates datasets with higher physical and semantic fidelity, leading to significantly improved success rates and generalization for downstream policy learning compared to non-verified baselines.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决从合成数据中为长程具身任务生成物理合理场景和语义正确动作序列的挑战，现有方法常产生不合理场景或出现程序执行却未达成目标的‘静默失败’。提出的V-CAGE框架引入了一种上下文感知实例化机制，通过维护动态禁止区域地图来确保场景合成的几何一致性，并采用分层指令分解模块将高级目标分解为可执行基元。其关键创新是一个基于视觉语言模型（VLM）的验证循环，在每个子任务后执行拒绝采样以过滤语义不正确的结果。实验表明，V-CAGE生成的数据集具有更优的物理和语义保真度，相比未验证的基线方法，显著提高了下游策略的成功率和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Automated Rubrics for Reliable Evaluation of Medical Dialogue Systems</div>
<div class="meta-line">Authors: Yinzhu Chen, Abdine Maiga, Hossein A. Rahmani, Emine Yilmaz</div>
<div class="meta-line">First: 2026-01-21T16:40:41+00:00 · Latest: 2026-01-21T16:40:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15161v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15161v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) are increasingly used for clinical decision support, where hallucinations and unsafe suggestions may pose direct risks to patient safety. These risks are particularly challenging as they often manifest as subtle clinical errors that evade detection by generic metrics, while expert-authored fine-grained rubrics remain costly to construct and difficult to scale. In this paper, we propose a retrieval-augmented multi-agent framework designed to automate the generation of instance-specific evaluation rubrics. Our approach grounds evaluation in authoritative medical evidence by decomposing retrieved content into atomic facts and synthesizing them with user interaction constraints to form verifiable, fine-grained evaluation criteria. Evaluated on HealthBench, our framework achieves a Clinical Intent Alignment (CIA) score of 60.12%, a statistically significant improvement over the GPT-4o baseline (55.16%). In discriminative tests, our rubrics yield a mean score delta ($μ_Δ = 8.658$) and an AUROC of 0.977, nearly doubling the quality separation achieved by GPT-4o baseline (4.972). Beyond evaluation, our rubrics effectively guide response refinement, improving quality by 9.2% (from 59.0% to 68.2%). This provides a scalable and transparent foundation for both evaluating and improving medical LLMs. The code is available at https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>医疗对话系统可靠评估的自动化评价量表</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在临床决策支持中的应用日益广泛，但其产生的幻觉和不安全建议可能直接危及患者安全。这类风险尤其难以应对，因为它们常表现为细微的临床错误，难以被通用指标检测，而专家编写的细粒度评价量表构建成本高昂且难以规模化。本文提出一种检索增强的多智能体框架，旨在自动化生成针对具体实例的评价量表。该方法通过将检索内容分解为原子事实，并结合用户交互约束合成可验证的细粒度评价标准，使评估过程基于权威医学证据。在HealthBench上的评估显示，本框架的临床意图对齐（CIA）得分达60.12%，较GPT-4o基线（55.16%）实现统计显著提升。在判别性测试中，本量表产生的平均分差（μ_Δ=8.658）和AUROC值（0.977）接近GPT-4o基线（4.972）的两倍。除评估功能外，本量表能有效指导响应优化，将质量提升9.2%（从59.0%至68.2%）。这为评估和改进医疗LLMs提供了可扩展且透明的技术基础。代码发布于https://anonymous.4open.science/r/Automated-Rubric-Generation-AF3C/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to reliably evaluate medical dialogue systems using large language models (LLMs), as generic metrics often fail to detect subtle clinical errors and expert-authored rubrics are costly and non-scalable. The method introduces a retrieval-augmented multi-agent framework that automates the generation of instance-specific evaluation rubrics by grounding them in authoritative medical evidence, decomposing retrieved content into atomic facts, and synthesizing them with user constraints to form verifiable criteria. Key experimental results show the framework achieves a Clinical Intent Alignment score of 60.12%, significantly outperforming a GPT-4o baseline (55.16%), and in discriminative tests, it yields a mean score delta of 8.658 and an AUROC of 0.977, nearly doubling the quality separation of the baseline; additionally, the rubrics guide response refinement, improving quality by 9.2%.</div>
<div class="mono" style="margin-top:8px">本研究旨在可靠评估基于大语言模型的医疗对话系统，因为这些模型可能产生幻觉和不安全建议，对患者构成直接风险，这些风险通常表现为细微的临床错误，通用指标难以检测，而专家编写的细粒度评估准则成本高且难以扩展。方法上，提出了一个检索增强的多智能体框架，通过基于权威医学证据自动生成针对具体实例的评估准则；该框架将检索内容分解为原子事实，并结合用户约束进行合成，以创建可验证的细粒度标准。在HealthBench上的关键实验结果表明，该框架实现了60.12%的临床意图对齐分数，显著优于GPT-4o基线（55.16%）；在判别性测试中，其平均分数差为8.658，AUROC为0.977，几乎将基线的质量分离度提高了一倍；此外，该准则能有效指导响应优化，将质量从59.0%提升至68.2%，提高了9.2%。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Graphs are Implicit Reward Models: Path-Derived Signals Enable Compositional Reasoning</div>
<div class="meta-line">Authors: Yuval Kansal, Niraj K. Jha</div>
<div class="meta-line">First: 2026-01-21T16:38:59+00:00 · Latest: 2026-01-21T16:38:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have achieved near-expert performance in structured reasoning domains like mathematics and programming, yet their ability to perform compositional multi-hop reasoning in specialized scientific fields remains limited. We propose a bottom-up learning paradigm in which models are grounded in axiomatic domain facts and compose them to solve complex, unseen tasks. To this end, we present a post-training pipeline, based on a combination of supervised fine-tuning and reinforcement learning (RL), in which knowledge graphs act as implicit reward models. By deriving novel reward signals from knowledge graph paths, we provide verifiable, scalable, and grounded supervision that encourages models to compose intermediate axioms rather than optimize only final answers during RL. We validate this approach in the medical domain, training a 14B model on short-hop reasoning paths (1-3 hops) and evaluating its zero-shot generalization to complex multi-hop queries (4-5 hops). Our experiments show that path-derived rewards act as a &quot;compositional bridge&quot;, enabling our model to significantly outperform much larger models and frontier systems like GPT-5.2 and Gemini 3 Pro, on the most difficult reasoning tasks. Furthermore, we demonstrate the robustness of our approach to adversarial perturbations against option-shuffling stress tests. This work suggests that grounding the reasoning process in structured knowledge is a scalable and efficient path toward intelligent reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识图谱作为隐式奖励模型：路径衍生信号赋能组合推理</div>
<div class="mono" style="margin-top:8px">大型语言模型在数学与编程等结构化推理领域已接近专家水平，但其在专业科学领域进行组合式多跳推理的能力仍有限。我们提出一种自底向上的学习范式，使模型基于公理化领域事实进行组合以解决复杂未知任务。为此，我们提出一种结合监督微调与强化学习的后训练流程，其中知识图谱充当隐式奖励模型。通过从知识图谱路径中衍生新颖奖励信号，我们提供可验证、可扩展且具基础性的监督，促使模型在强化学习中组合中间公理而非仅优化最终答案。我们在医学领域验证该方法，使用短跳推理路径（1-3跳）训练140亿参数模型，并评估其对复杂多跳查询（4-5跳）的零样本泛化能力。实验表明，路径衍生奖励作为“组合桥梁”，使我们的模型在最困难推理任务上显著超越GPT-5.2和Gemini 3 Pro等更大规模模型及前沿系统。此外，我们通过选项重排压力测试验证了该方法对对抗性扰动的鲁棒性。本研究表明，将推理过程锚定于结构化知识是实现智能推理的可扩展高效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of large language models in performing compositional multi-hop reasoning within specialized scientific domains, despite their proficiency in structured reasoning tasks like mathematics. The proposed method introduces a bottom-up learning paradigm that grounds models in axiomatic domain facts, using a post-training pipeline combining supervised fine-tuning and reinforcement learning where knowledge graphs serve as implicit reward models. By deriving reward signals from knowledge graph paths, the approach provides verifiable and grounded supervision to encourage compositional reasoning over intermediate steps rather than focusing solely on final answers. Experimental validation in the medical domain with a 14B model trained on short-hop reasoning paths (1-3 hops) demonstrates zero-shot generalization to complex multi-hop queries (4-5 hops), significantly outperforming larger models and frontier systems like GPT-5.2 and Gemini 3 Pro on difficult reasoning tasks, while also showing robustness to adversarial perturbations in option-shuffling stress tests.</div>
<div class="mono" style="margin-top:8px">该研究针对大型语言模型在专业科学领域进行组合式多跳推理能力有限的问题，尽管它们在数学等结构化推理领域表现优异。方法提出了一种自底向上的学习范式，将模型基于公理化的领域事实，采用结合监督微调和强化学习的后训练流程，其中知识图谱作为隐式奖励模型；具体而言，从知识图谱路径派生的新颖奖励信号提供了可验证且基于事实的监督，以鼓励模型在强化学习中组合中间公理而非仅优化最终答案。在医学领域的实验验证中，使用14B模型在短跳推理路径上训练并测试其对复杂多跳查询的零样本泛化能力，结果表明路径派生的奖励使模型在困难任务上显著优于更大模型及前沿系统如GPT-5.2和Gemini 3 Pro，同时在选项重排压力测试中展现出对抗扰动的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Composite Flow Matching for Reinforcement Learning with Shifted-Dynamics Data</div>
<div class="meta-line">Authors: Lingkai Kong, Haichuan Wang, Tonghan Wang, Guojun Xiong, Milind Tambe</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-05-29T04:09:19+00:00 · Latest: 2026-01-21T16:37:21+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23062v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.23062v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Incorporating pre-collected offline data can substantially improve the sample efficiency of reinforcement learning (RL), but its benefits can break down when the transition dynamics in the offline dataset differ from those encountered online. Existing approaches typically mitigate this issue by penalizing or filtering offline transitions in regions with large dynamics gap. However, their dynamics-gap estimators often rely on KL divergence or mutual information, which can be ill-defined when offline and online dynamics have mismatched support. To address this challenge, we propose CompFlow, a principled framework built on the theoretical connection between flow matching and optimal transport. Specifically, we model the online dynamics as a conditional flow built upon the output distribution of a pretrained offline flow, rather than learning it directly from a Gaussian prior. This composite structure provides two advantages: (1) improved generalization when learning online dynamics under limited interaction data, and (2) a well-defined and stable estimate of the dynamics gap via the Wasserstein distance between offline and online transitions. Building on this dynamics-gap estimator, we further develop an optimistic active data collection strategy that prioritizes exploration in high-gap regions, and show theoretically that it reduces the performance gap to the optimal policy. Empirically, CompFlow consistently outperforms strong baselines across a range of RL benchmarks with shifted-dynamics data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向动态偏移数据强化学习的复合流匹配方法</div>
<div class="mono" style="margin-top:8px">利用预收集的离线数据可显著提升强化学习的样本效率，但当离线数据集中的状态转移动态与在线环境存在差异时，其优势可能失效。现有方法通常通过惩罚或过滤动态差异较大区域的离线转移来缓解此问题，但其动态差异估计器常依赖KL散度或互信息，在离线与在线动态支持集不匹配时可能失效。为此，我们提出CompFlow框架，基于流匹配与最优传输的理论关联构建原则性方法：将在线动态建模为基于预训练离线流输出分布的条件流，而非直接从高斯先验学习。该复合结构具有双重优势：(1)在有限交互数据下学习在线动态时提升泛化能力；(2)通过离线与在线转移的Wasserstein距离获得定义明确且稳定的动态差异估计。基于此估计器，我们进一步开发了乐观主动数据收集策略，优先探索高差异区域，并从理论上证明其能缩小与最优策略的性能差距。实验表明，在多种动态偏移的RL基准测试中，CompFlow均稳定优于现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of leveraging offline data in reinforcement learning when the transition dynamics differ from the online environment, as existing methods using KL divergence or mutual information can fail when dynamics have mismatched support. The proposed CompFlow framework models online dynamics as a conditional flow built upon a pretrained offline flow&#x27;s output distribution, utilizing the connection between flow matching and optimal transport. This approach improves generalization with limited online data and provides a stable dynamics-gap estimate via Wasserstein distance, enabling an optimistic active data collection strategy that prioritizes exploration in high-gap regions. Experimental results demonstrate that CompFlow consistently outperforms strong baselines across various RL benchmarks with shifted-dynamics data.</div>
<div class="mono" style="margin-top:8px">本研究针对强化学习中离线数据与在线环境动态不一致时利用离线数据的挑战，现有方法依赖KL散度或互信息，在动态支持不匹配时可能失效。提出的CompFlow方法基于流匹配与最优传输的理论联系，将在线动态建模为基于预训练离线流的条件流，从而在有限在线数据下改善泛化能力，并通过Wasserstein距离提供稳定的动态差距估计。在动态偏移的强化学习基准测试中，CompFlow一致优于现有基线方法，其乐观主动数据收集策略优先探索高差距区域，理论上被证明能缩小与最优策略的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Outcome-Based RL Provably Leads Transformers to Reason, but Only With the Right Data</div>
<div class="meta-line">Authors: Yuval Ran-Milo, Yotam Alexander, Shahar Mendel, Nadav Cohen</div>
<div class="meta-line">First: 2026-01-21T16:36:19+00:00 · Latest: 2026-01-21T16:36:19+00:00</div>
<div class="meta-line">Comments: 80 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformers trained via Reinforcement Learning (RL) with outcome-based supervision can spontaneously develop the ability to generate intermediate reasoning steps (Chain-of-Thought). Yet the mechanism by which sparse rewards drive gradient descent to discover such systematic reasoning remains poorly understood. We address this by analyzing the gradient flow dynamics of single-layer Transformers on a synthetic graph traversal task that cannot be solved without Chain-of-Thought (CoT) but admits a simple iterative solution. We prove that despite training solely on final-answer correctness, gradient flow drives the model to converge to a structured, interpretable algorithm that iteratively traverses the graph vertex-by-vertex. We characterize the distributional properties required for this emergence, identifying the critical role of &quot;simple examples&quot;: instances requiring fewer reasoning steps. When the training distribution places sufficient mass on these simpler instances, the model learns a generalizable traversal strategy that extrapolates to longer chains; when this mass vanishes, gradient-based learning becomes infeasible. We corroborate our theoretical results through experiments on synthetic data and with real-world language models on mathematical reasoning tasks, validating that our theoretical findings carry over to practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于结果的强化学习可证明引导Transformer进行推理，但仅适用于特定数据</div>
<div class="mono" style="margin-top:8px">通过基于结果的监督进行强化学习训练的Transformer能够自发产生中间推理步骤（思维链）。然而，稀疏奖励如何驱动梯度下降发现这种系统性推理的机制仍不明确。我们通过分析单层Transformer在合成图遍历任务上的梯度流动力学来解决这一问题——该任务无思维链无法解决，但存在简单迭代解。我们证明：尽管仅通过最终答案正确性进行训练，梯度流仍会驱动模型收敛至结构化、可解释的逐顶点迭代遍历算法。我们刻画了这种涌现所需的分布特性，指出“简单示例”（需要较少推理步骤的实例）的关键作用。当训练分布为这些简单实例分配足够概率质量时，模型将学习可泛化至更长链的遍历策略；若该概率质量消失，基于梯度的学习将不可行。我们通过合成数据实验及真实世界语言模型的数学推理任务验证了理论结果在实际场景中的适用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how Transformers trained with outcome-based Reinforcement Learning (RL) can spontaneously develop systematic reasoning, such as Chain-of-Thought (CoT), a mechanism not well understood. The authors analyze gradient flow dynamics in a single-layer Transformer on a synthetic graph traversal task that necessitates CoT, proving that training solely on final-answer correctness leads the model to converge to an interpretable, iterative traversal algorithm. Key experimental findings show that the emergence of this reasoning critically depends on the training distribution containing sufficient &quot;simple examples&quot; (instances requiring fewer steps), which enables generalization to longer chains; without such examples, learning fails, a result corroborated by experiments on synthetic data and real-world language models for mathematical reasoning.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究基于结果的强化学习如何使Transformer模型自发产生思维链推理，这一机制尚不明确。作者在必须通过中间推理步骤才能解决的合成图遍历任务上，分析了单层Transformer的梯度流动态。他们证明，仅基于最终答案正确性的训练会使模型收敛到一个可解释的、迭代的图遍历算法，并发现训练数据中‘简单示例’（所需推理步骤较少的实例）的存在对于这种能力的涌现以及向更长链的泛化至关重要；若缺乏此类示例，基于梯度的学习将无法进行。在合成数据以及真实世界语言模型数学推理任务上的实验验证了这些理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">How to Build AI Agents by Augmenting LLMs with Codified Human Expert Domain Knowledge? A Software Engineering Framework</div>
<div class="meta-line">Authors: Choro Ulan uulu, Mikhail Kulyabin, Iris Fuhrmann, Jan Joosten, Nuno Miguel Martins Pacheco, Filippos Petridis, Rebecca Johnson, Jan Bosch, Helena Holmström Olsson</div>
<div class="meta-line">First: 2026-01-21T16:23:22+00:00 · Latest: 2026-01-21T16:23:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15153v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Critical domain knowledge typically resides with few experts, creating organizational bottlenecks in scalability and decision-making. Non-experts struggle to create effective visualizations, leading to suboptimal insights and diverting expert time. This paper investigates how to capture and embed human domain knowledge into AI agent systems through an industrial case study. We propose a software engineering framework to capture human domain knowledge for engineering AI agents in simulation data visualization by augmenting a Large Language Model (LLM) with a request classifier, Retrieval-Augmented Generation (RAG) system for code generation, codified expert rules, and visualization design principles unified in an agent demonstrating autonomous, reactive, proactive, and social behavior. Evaluation across five scenarios spanning multiple engineering domains with 12 evaluators demonstrates 206% improvement in output quality, with our agent achieving expert-level ratings in all cases versus baseline&#x27;s poor performance, while maintaining superior code quality with lower variance. Our contributions are: an automated agent-based system for visualization generation and a validated framework for systematically capturing human domain knowledge and codifying tacit expert knowledge into AI agents, demonstrating that non-experts can achieve expert-level outcomes in specialized domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>如何通过编码化人类专家领域知识增强大语言模型构建AI智能体？一个软件工程框架</div>
<div class="mono" style="margin-top:8px">关键领域知识通常仅掌握在少数专家手中，导致组织在可扩展性和决策方面出现瓶颈。非专业人员难以创建有效的可视化方案，导致洞察力不足并挤占专家时间。本文通过工业案例研究，探讨如何将人类领域知识捕获并嵌入AI智能体系统。我们提出一个软件工程框架，通过为大型语言模型（LLM）集成请求分类器、用于代码生成的检索增强生成（RAG）系统、编码化的专家规则以及可视化设计原则，构建能展现自主、反应式、主动式及社交行为的智能体，从而捕获人类领域知识以开发仿真数据可视化AI智能体。在涵盖多个工程领域的五个场景中，由12名评估者参与的测试表明：输出质量提升206%，我们的智能体在所有案例中均获得专家级评分（基线表现较差），同时保持更优的代码质量与更低方差。我们的贡献包括：一个基于智能体的自动化可视化生成系统，以及一个经过验证的框架——用于系统化捕获人类领域知识、将隐性专家知识编码至AI智能体，证明非专业人员可在专业领域实现专家级成果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the bottleneck caused by limited access to human domain experts, which hinders scalability and decision-making, particularly in specialized tasks like simulation data visualization where non-experts produce suboptimal results. The authors propose a software engineering framework to codify expert knowledge into an AI agent by augmenting a Large Language Model with a request classifier, a Retrieval-Augmented Generation system for code, and embedded expert rules and design principles, enabling autonomous, reactive, proactive, and social agent behavior. In an evaluation across five engineering scenarios with 12 evaluators, the agent demonstrated a 206% improvement in output quality, achieving expert-level ratings in all cases compared to a poor-performing baseline, while also generating higher-quality code with lower variance.</div>
<div class="mono" style="margin-top:8px">本研究针对领域专家知识有限导致的组织可扩展性和决策瓶颈问题，特别是在仿真数据可视化等专业任务中，非专家常产生次优结果。作者提出了一个软件工程框架，通过增强大型语言模型（LLM），结合请求分类器、用于代码生成的检索增强生成（RAG）系统，以及编码化的专家规则和可视化设计原则，构建了一个具备自主、反应、主动和社交行为的AI智能体。在涵盖五个工程场景、由12名评估者参与的实验中，该智能体的输出质量提升了206%，在所有案例中均获得专家级评分，而基线表现较差，同时生成的代码质量更高、方差更低。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
