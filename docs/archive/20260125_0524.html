<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-25 05:24</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260125_0524</div>
    <div class="row"><div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：面向文档的具身视觉语言前端系统</div>
<div class="mono" style="margin-top:8px">GutenOCR是通过对Qwen2.5-VL-3B和Qwen2.5-VL-7B进行微调得到的一系列具身OCR前端模型。这些单检查点的视觉语言模型通过统一的提示驱动接口，实现了文本读取、检测与定位功能。模型基于商业文档、科学文献及合成定位数据训练，支持整页与局部读取，可提供行级与段落级边界框，并响应条件式“X在何处？”查询。我们提出了具身OCR评估框架，实验表明在1.05万份保留的商业与科学文档上，GutenOCR-7B的复合具身OCR分数较其骨干模型Qwen2.5-VL-7B提升超一倍（0.40至0.82）。在Fox与OmniDocBench v1.5基准测试中，该方法显著提升了区域/行级OCR性能及文本检测召回率，但在页面级线性化、色彩引导OCR及公式密集版式处理方面存在权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for unified document understanding models that combine reading, detection, and grounding capabilities. The authors introduce GutenOCR, a family of vision-language models created by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B on business documents, scientific articles, and synthetic grounding data, enabling prompt-based full-page and localized reading with bounding box outputs. Experimental results show that GutenOCR-7B more than doubles the composite grounded OCR score of its backbone model (from 0.40 to 0.82) on 10.5K held-out pages, substantially improves region- and line-level OCR and text-detection recall on Fox and OmniDocBench v1.5 benchmarks, while revealing trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发能够统一结合文本识别、检测与定位能力的文档理解模型。方法是通过在商业文档、科学文章和合成定位数据上微调Qwen2.5-VL视觉语言模型（30亿和70亿参数），构建出GutenOCR系列模型，其单一检查点可通过提示接口实现整页/局部阅读、边界框输出和条件查询。主要实验结果表明，GutenOCR-7B在10.5K份保留文档上的综合定位OCR分数相比其骨干模型提升了一倍以上（从0.40到0.82），在Fox和OmniDocBench基准测试中显著改善了区域/行级OCR和文本检测召回率，但也揭示了在页面级线性化、颜色引导OCR和公式密集布局方面的性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Counterfactual Training: Teaching Models Plausible and Actionable Explanations</div>
<div class="meta-line">Authors: Patrick Altmeyer, Aleksander Buszydlik, Arie van Deursen, Cynthia C. S. Liem</div>
<div class="meta-line">First: 2026-01-22T18:56:14+00:00 · Latest: 2026-01-22T18:56:14+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the 2026 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反事实训练：为模型提供合理且可操作的因果解释</div>
<div class="mono" style="margin-top:8px">我们提出了一种称为反事实训练的新型训练机制，利用反事实解释来增强模型的可解释性。反事实解释已成为对不透明机器学习模型进行事后解释的流行方法：它们揭示了事实输入需要如何改变，才能使模型产生期望的输出。为了在实际决策系统中发挥作用，反事实解释应基于底层数据具有合理性，并符合特征可变性的约束条件。因此，现有研究多集中于开发事后方法以生成满足这些要求的反事实解释。在本研究中，我们转而让模型直接对期望的最终目标负责：反事实训练在训练阶段使用反事实解释，以最小化学得表征与合理、可操作解释之间的差异。我们通过实证和理论证明，所提出的方法有助于训练出能提供内在理想反事实解释的模型，并同时提升对抗鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the explanatory capacity of machine learning models by ensuring that their counterfactual explanations are both plausible with respect to the data distribution and actionable under feature constraints, which is crucial for real-world decision-making. The proposed method, counterfactual training, integrates counterfactual explanations directly into the training phase to minimize the divergence between the model&#x27;s learned representations and these desirable explanations. Experimental results demonstrate that this approach not only yields models that inherently provide plausible and actionable counterfactuals but also improves their adversarial robustness.</div>
<div class="mono" style="margin-top:8px">该研究针对机器学习模型需提供合理且可操作的反事实解释的需求，这类解释对现实决策至关重要，但通常是在训练后生成。方法提出了反事实训练，这是一种新颖的训练机制，将反事实解释直接融入训练阶段，以使学习到的表征与这些理想的解释特性保持一致。实验结果表明，该方法不仅使模型能够内在地生成更好的反事实解释，还提高了其对抗鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing</div>
<div class="meta-line">Authors: Song Xia, Meiwen Ding, Chenqi Kong, Wenhan Yang, Xudong Jiang</div>
<div class="meta-line">First: 2026-01-22T18:52:21+00:00 · Latest: 2026-01-22T18:52:21+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16200v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16200v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\% to about 1\%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过特征空间平滑实现多模态大语言模型的可证明鲁棒性</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在多样化应用中展现出强大能力，但其特征表示易受对抗性扰动影响，导致预测错误。为应对此脆弱性，我们提出特征空间平滑（FS）方法，并从理论上证明FS能为MLLMs的特征表示提供可验证的鲁棒性保证。具体而言，FS将任意特征编码器转换为平滑变体，确保在$\ell_2$范数约束攻击下，干净特征与对抗特征间的余弦相似度存在可验证下界。进一步研究表明，通过提升原始编码器的高斯鲁棒性评分，可优化该特征余弦相似度下界（FCSB）的数值。基于此，我们提出即插即用模块——净化平滑映射器（PSM），该模块无需重新训练MLLMs即可提升模型的高斯鲁棒性评分，从而增强FS框架下的可验证鲁棒性。实验表明，结合PSM的FS不仅提供坚实的理论鲁棒性保证，其经验性能也优于对抗训练。跨多种MLLMs与下游任务的广泛实验验证了FS-PSM的有效性，能将各类白盒攻击的成功率从近90%降至约1%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) are powerful but susceptible to adversarial perturbations that corrupt their feature representations. To address this, the authors propose Feature-space Smoothing (FS), a method that transforms a feature encoder into a smoothed version with a provable lower bound on feature cosine similarity under ℓ₂-bounded attacks, thereby offering certified robustness. They further introduce a plug-and-play Purifier and Smoothness Mapper (PSM) module to improve the underlying Gaussian robustness score of the encoder, enhancing the certified bound without retraining the MLLM. Experimental results across various models and tasks show that FS with PSM significantly reduces the Attack Success Rate of white-box attacks from nearly 90% to about 1%, outperforming adversarial training.</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）能力强大，但其特征表示易受对抗性扰动影响而导致错误预测。为此，研究者提出了特征空间平滑（FS）方法，该方法将特征编码器转换为平滑变体，在ℓ₂有界攻击下为特征余弦相似度提供了可证明的下界，从而确保认证鲁棒性。此外，他们引入了一个即插即用的净化与平滑映射器（PSM）模块，用于提升MLLMs的高斯鲁棒性分数，从而在不重新训练模型的情况下增强认证鲁棒性。在多种MLLMs和下游任务上的广泛实验表明，结合PSM的FS方法能将各类白盒攻击的成功率从近90%大幅降低至约1%，其效果优于对抗训练。</div>
</details>
</div>
<div class="card">
<div class="title">Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling</div>
<div class="meta-line">Authors: Jack Cook, Junxian Guo, Guangxuan Xiao, Yujun Lin, Song Han</div>
<div class="meta-line">First: 2025-12-01T18:59:45+00:00 · Latest: 2026-01-22T18:49:14+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02010v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.02010v3">PDF</a> · <a href="http://github.com/mit-han-lab/fouroversix">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models have grown larger, interest has grown in low-precision numerical formats such as NVFP4 as a way to improve speed and reduce memory usage. However, quantizing models to NVFP4 remains difficult as the lack of precision generally degrades model performance. In this work, we address this issue with Four Over Six (4/6), a modification to the block-scaled NVFP4 quantization algorithm that yields reduced quantization error. Unlike integer formats, floating point formats have non-uniform step sizes which create larger quantization error on larger values. 4/6 takes advantage of this by adaptively scaling some blocks to smaller FP4 values, making the distribution of representable values more uniform and reducing quantization error for near-maximal values. We show that 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, resulting in performance gains during both pre-training and inference with minimal computational overhead. In pre-training experiments with the Nemotron 3 Nano 30B-A3B model architecture, we find that 4/6 brings training loss closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. Our code is available at http://github.com/mit-han-lab/fouroversix.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>四分之六：采用自适应块缩放的更精确NVFP4量化方法</div>
<div class="mono" style="margin-top:8px">随着大语言模型规模不断扩大，NVFP4等低精度数值格式因能提升速度并降低内存使用而备受关注。然而，将模型量化为NVFP4仍具挑战性，精度缺失通常会导致模型性能下降。本研究通过&#x27;四分之六&#x27;方法改进块缩放NVFP4量化算法，有效降低量化误差。与整数格式不同，浮点格式的非均匀步长会导致较大数值产生更大量化误差。4/6方法通过自适应地将部分块缩放至较小FP4值，使可表示数值分布更均匀，从而降低接近最大值区域的量化误差。我们证明4/6可在英伟达Blackwell GPU上高效实现，在预训练和推理阶段以最小计算开销获得性能提升。基于Nemotron 3 Nano 30B-A3B架构的预训练实验表明，相较于当前最先进的NVFP4训练方案，4/6能使训练损失更接近BF16基准。代码已开源：http://github.com/mit-han-lab/fouroversix。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the performance degradation caused by the limited precision of NVFP4 quantization in large language models, this work introduces Four Over Six (4/6), an adaptive block scaling method. The method exploits the non-uniform step sizes of floating-point formats by adaptively scaling some blocks to smaller FP4 values, thereby making the distribution of representable values more uniform and reducing quantization error, particularly for near-maximal values. Experimental results demonstrate that 4/6 can be efficiently implemented on NVIDIA Blackwell GPUs, and when applied to pre-training the Nemotron 3 Nano 30B-A3B model, it reduces training loss closer to BF16 baseline compared to existing NVFP4 training recipes, with minimal computational overhead.</div>
<div class="mono" style="margin-top:8px">为解决大语言模型低精度NVFP4量化导致的性能下降问题，本研究提出了自适应块缩放方法Four Over Six (4/6)。该方法通过动态将部分块缩放至更小的FP4值，使可表示值的分布更均匀，从而降低量化误差，特别针对浮点格式中较大值误差更大的问题。实验结果表明，4/6可在NVIDIA Blackwell GPU上高效实现，在Nemotron 3 Nano 30B-A3B模型预训练中，相比现有NVFP4训练方案，其训练损失更接近BF16基准。</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Geospatial Place Representation Learning from Large-Scale Point-of-Interest Graph Data</div>
<div class="meta-line">Authors: Mohammad Hashemi, Hossein Amiri, Andreas Zufle</div>
<div class="meta-line">First: 2025-06-25T15:10:31+00:00 · Latest: 2026-01-22T18:46:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02921v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.02921v3">PDF</a> · <a href="https://github.com/mohammadhashemii/PlaceRep">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning effective representations of urban environments requires capturing spatial structure beyond fixed administrative boundaries. Existing geospatial representation learning approaches typically aggregate Points of Interest(POI) into pre-defined administrative regions such as census units or ZIP code areas, assigning a single embedding to each region. However, POIs often form semantically meaningful groups that extend across, within, or beyond these boundaries, defining places that better reflect human activity and urban function. To address this limitation, we propose PlaceRep, a training-free geospatial representation learning method that constructs place-level representations by clustering spatially and semantically related POIs. PlaceRep summarizes large-scale POI graphs from U.S. Foursquare data to produce general-purpose urban region embeddings while automatically identifying places across multiple spatial scales. By eliminating model pre-training, PlaceRep provides a scalable and efficient solution for multi-granular geospatial analysis. Experiments using the tasks of population density estimation and housing price prediction as downstream tasks show that PlaceRep outperforms most state-of-the-art graph-based geospatial representation learning methods and achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation of PlaceRep is available at https://github.com/mohammadhashemii/PlaceRep.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大规模兴趣点图数据的免训练地理空间场所表征学习</div>
<div class="mono" style="margin-top:8px">学习有效的城市环境表征需捕捉超越固定行政边界的空间结构。现有地理空间表征学习方法通常将兴趣点聚合至人口普查单元或邮政编码区等预定义行政区域，并为每个区域分配单一嵌入向量。然而，兴趣点常形成跨越、穿透或超出这些边界的语义化群组，从而定义出更能反映人类活动与城市功能的场所。为突破此局限，我们提出PlaceRep——一种免训练的地理空间表征学习方法，通过聚类空间与语义相关的兴趣点来构建场所级表征。该方法基于美国Foursquare数据的大规模兴趣点图进行归纳，在自动识别多空间尺度场所的同时生成通用型城市区域嵌入向量。通过消除模型预训练环节，PlaceRep为多粒度地理空间分析提供了可扩展的高效解决方案。在人口密度估算和房价预测两项下游任务中的实验表明，PlaceRep优于多数基于图结构的先进地理空间表征学习方法，并在大规模兴趣点图上生成区域级表征时实现高达100倍的加速。项目代码已开源：https://github.com/mohammadhashemii/PlaceRep。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To overcome the limitation of existing geospatial representation methods that rely on fixed administrative boundaries and assign a single embedding to each pre-defined region, this study proposes PlaceRep, a training-free method for learning place-level representations. The method constructs these representations by clustering Points of Interest (POI) from large-scale graph data based on spatial and semantic relatedness, thereby automatically identifying semantically meaningful places across multiple scales without requiring model pre-training. Experimental evaluations on downstream tasks including population density estimation and housing price prediction demonstrate that PlaceRep outperforms most state-of-the-art graph-based geospatial methods and achieves up to a 100x speedup in generating region-level embeddings from large-scale POI graphs.</div>
<div class="mono" style="margin-top:8px">该研究针对现有地理空间表示方法将兴趣点（POI）聚合到固定行政边界的局限性，这些边界可能与反映人类活动和城市功能的语义上有意义的地点不匹配。提出的方法PlaceRep是一种免训练方法，通过对大规模图数据中空间和语义相关的POI进行聚类来构建地点级表示，无需模型预训练即可自动识别多空间尺度的地点。在人口密度估计和房价预测等下游任务上的实验结果表明，PlaceRep优于大多数最先进的基于图的地理空间方法，并在生成区域级嵌入时实现了高达100倍的加速。</div>
</details>
</div>
<div class="card">
<div class="title">A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows</div>
<div class="meta-line">Authors: El Mehdi Er Raqabi, Kevin Dalmeijer, Pascal Van Hentenryck</div>
<div class="meta-line">First: 2026-01-22T18:46:46+00:00 · Latest: 2026-01-22T18:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16194v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibility in the number of compartments, (ii) item-to-compartment compatibility, and (iii) item-to-item compatibility. The problem also accommodates practical operational requirements such as driver breaks. To solve the MCVRPMTW, we develop an exact branch-and-price (B&amp;P) algorithm in which the pricing problem is solved using a labeling algorithm. Several acceleration strategies are introduced to limit symmetry during label extensions, improve the stability of dual solutions in column generation, and enhance the branching process. To handle large-scale instances, we propose a rolling-space B&amp;P algorithm that integrates clustering techniques into the solution framework. Extensive computational experiments on instances inspired by a real-world industrial application demonstrate the effectiveness of the proposed approach and provide useful managerial insights for practical implementation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多时间窗多隔间车辆路径问题的滚动空间分支定价算法</div>
<div class="mono" style="margin-top:8px">本文研究了多时间窗多隔间车辆路径问题（MCVRPMTW），该问题在经典带时间窗车辆路径问题基础上扩展，考虑了配备多隔间的车辆及客户在多个配送时间窗内接受服务的需求。问题包含三个关键隔间相关特征：（i）隔间数量的灵活性，（ii）物品与隔间的兼容性，以及（iii）物品间的兼容性。同时整合了驾驶员休息等实际运营要求。针对MCVRPMTW，我们设计了一种精确的分支定价算法，其中定价子问题通过标号算法求解。为提升求解效率，引入了多种加速策略：限制标号扩展中的对称性、改进列生成中对偶解的稳定性、优化分支过程。针对大规模算例，提出一种融合聚类技术的滚动空间分支定价算法。基于实际工业应用场景的算例实验表明，所提方法具有显著有效性，并为实际应用提供了管理启示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), motivated by the need to model complex real-world logistics where vehicles have multiple compartments for different goods and customers have several possible delivery windows. The method develops an exact branch-and-price algorithm, using a labeling algorithm for the pricing subproblem and incorporating acceleration strategies to reduce symmetry, stabilize column generation, and improve branching; for large instances, a rolling-space variant integrates clustering techniques. Experimental results on instances from a real-world industrial application show the algorithm&#x27;s effectiveness and yield practical managerial insights.</div>
<div class="mono" style="margin-top:8px">本研究针对具有多时间窗的多隔间车辆路径问题（MCVRPMTW），其动机源于对复杂现实物流建模的需求，此类物流中车辆配备多个隔间以装载不同产品，且客户有多个可用的服务时间窗。方法上开发了一种精确的分支定价算法，使用标号算法求解定价子问题，并通过加速策略来减少对称性、稳定对偶解和改进分支过程。针对大规模算例，提出了一种融合聚类技术的滚动空间分支定价算法。在源自工业应用的算例上的大量计算实验证明了该算法的有效性，并为实际应用提供了有用的管理启示。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-01-22T18:24:00+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v1">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">如何利用人工智能为科学问题探索新的最优解？先前关于测试时扩展的研究（如AlphaEvolve）通过提示冻结的大型语言模型进行搜索。我们在测试时实施强化学习，使大型语言模型能够持续训练，并针对具体测试问题积累经验。这种持续学习形式具有特殊性：其目标是产生单一卓越解决方案而非平均意义上的多个良好方案，且专注于解决当前问题而非泛化至其他问题。因此，我们的学习目标和搜索子程序均优先考虑最具潜力的解决方案。我们将此方法称为“测试时训练发现法”。延续先前研究，我们聚焦于具有连续奖励的问题，并在数学、GPU内核工程、算法设计和生物学领域报告所有尝试问题的结果。该方法在几乎所有领域均创下新纪录：（1）埃尔德什最小重叠问题与自相关不等式；（2）GPUMode内核竞赛（较现有技术提速达2倍）；（3）历史AtCoder算法竞赛；（4）单细胞分析去噪问题。所有解决方案均经专家或主办方评审。与先前依赖封闭前沿模型的最佳成果不同，本研究全部结果均基于开源模型OpenAI gpt-oss-120b实现，并可通过公开代码复现。测试时训练通过Thinking Machines的Tinker API执行，每个问题成本仅数百美元。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to advance AI&#x27;s capability to discover novel state-of-the-art solutions for scientific problems, moving beyond prior test-time scaling methods that rely on prompting frozen LLMs. The proposed method, Test-Time Training to Discover (TTT-Discover), employs reinforcement learning at test time, allowing the LLM to continuously train with experience specific to the test problem, focusing on generating a single optimal solution rather than average performance across tasks. Key experimental results demonstrate that TTT-Discover achieves new state-of-the-art performance across diverse domains, including mathematics (Erdős&#x27; minimum overlap problem and an autocorrelation inequality), GPU kernel engineering (up to 2x speed improvement), algorithm design (past AtCoder competitions), and biology (single-cell analysis denoising), using an open model and reproducible code.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升人工智能在科学问题中发现新颖最优解的能力，超越先前依赖提示冻结大语言模型的测试时扩展方法。所提出的方法“测试时训练发现”（TTT-Discover）在测试时采用强化学习，使大语言模型能够针对特定测试问题持续训练，专注于生成单一最优解而非任务间的平均性能。关键实验结果表明，TTT-Discover在多个领域实现了新的最优性能，包括数学（埃尔德什最小重叠问题和自相关不等式）、GPU内核工程（内核比先前技术快达2倍）、算法设计（过往AtCoder竞赛）和生物学（单细胞分析中的去噪问题），所有解决方案均经专家验证，并使用开源模型和公开代码实现。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints</div>
<div class="meta-line">Authors: Yiyao Yang</div>
<div class="meta-line">First: 2026-01-22T18:19:52+00:00 · Latest: 2026-01-22T18:19:52+00:00</div>
<div class="meta-line">Comments: 22 pages, 5 figures, 5 propositions</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16174v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越预测不确定性：基于结构约束的可靠表征学习</div>
<div class="mono" style="margin-top:8px">机器学习中的不确定性估计传统上聚焦于预测阶段，旨在量化模型输出的置信度，同时默认将学习到的表征视为确定且可靠的。本研究挑战了这一隐含假设，主张可靠性应被视为学习表征本身的一阶属性。我们提出一个原则性的可靠表征学习框架，该框架显式建模表征层面的不确定性，并利用结构约束作为归纳偏置来正则化可行表征空间。我们的方法直接在表征空间中引入不确定性感知正则化，鼓励表征不仅具有预测性，同时具备稳定性、良好校准性以及对噪声和结构扰动的鲁棒性。通过融入稀疏性、关系结构或特征组依赖等结构约束，可在不假设结构完全正确或无噪声的前提下，定义有意义的几何结构并减少学习表征中的虚假变异性。重要的是，所提框架独立于特定模型架构，可与广泛的表征学习方法集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work challenges the conventional focus on predictive uncertainty by arguing that learned representations themselves should be reliable. It proposes a framework that explicitly models representation-level uncertainty and uses structural constraints, like sparsity or relational dependencies, as inductive biases to regularize the representation space. The method encourages stable, well-calibrated, and robust representations, and experiments show it can be integrated with various architectures to reduce spurious variability without assuming perfect structural knowledge.</div>
<div class="mono" style="margin-top:8px">本研究挑战了传统上仅关注预测不确定性的做法，主张学习到的表征本身应具备可靠性，而不仅仅是最终模型输出。所提出的框架对表征层面的不确定性进行建模，并利用稀疏性或关系依赖等结构性约束作为归纳偏置来正则化表征空间，旨在产生稳定、校准良好且鲁棒的表征。该方法与具体架构无关，实验结果表明，在噪声和结构扰动下，该方法能有效减少虚假变异性并提升表征的可靠性，相关细节在22页内容中通过5张图表和命题进行了阐述。</div>
</details>
</div>
<div class="card">
<div class="title">Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</div>
<div class="meta-line">Authors: Steven Kolawole, Lucio Dery, Jean-François Kagy, Virginia Smith, Graham Neubig, Ameet Talwalkar</div>
<div class="meta-line">First: 2024-02-08T04:48:26+00:00 · Latest: 2026-01-22T18:13:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 fiigures, 16 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.05406v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.05406v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Structured pruning is a promising approach to create smaller, faster large language models. However, existing methods typically rely on computing the gradient via backward passes, which can inflate memory requirements and compute costs. In this work we introduce Bonsai, a gradient-free structured pruning method that eliminates the need for backpropagation, significantly reducing memory requirements and compute costs while achieving state-of-the-art pruning performance. Bonsai uses forward-pass-only perturbative pruning to enable efficient compression of large models on a broader range of hardware configurations. Unlike existing structured pruning approaches, Bonsai not only achieves better compression with fewer resources but also produces models that are twice as fast as those generated by semi-structured pruning. As a concrete demonstration, we use Bonsai to prune 7B and 8B models to 50% sparsity on a single A6000 GPU -- a task challenging for backprop-based methods in memory-constrained settings, as they require 2-3x the memory. Our results show that removing backprop as a requirement not only enables pruning larger models on constrained hardware but can also lead to state-of-the-art efficiency and performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>即刻剪枝：仅需前向传播的大语言模型结构化剪枝</div>
<div class="mono" style="margin-top:8px">结构化剪枝是创建更小、更快大语言模型的有效途径，但现有方法通常依赖反向传播计算梯度，这会增加内存与计算成本。本研究提出无需梯度的结构化剪枝方法Bonsai，通过仅需前向传播的扰动剪枝技术，在显著降低内存与计算需求的同时实现最优剪枝性能。该方法不仅以更少资源获得更好压缩效果，所生成模型的推理速度更是半结构化剪枝的两倍。实验证明，在单张A6000 GPU上可将70亿和80亿参数模型剪枝至50%稀疏度——这对依赖反向传播的方法极具挑战（需2-3倍内存）。研究表明，摆脱反向传播约束不仅能在受限硬件上剪枝更大模型，还能实现最优效率与性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Structured pruning aims to create smaller and faster large language models, but existing methods often rely on gradient-based backpropagation, which increases memory and computational costs. To address this, the authors introduce Bonsai, a gradient-free structured pruning method that uses only forward passes through perturbative pruning, significantly reducing memory requirements and enabling compression on hardware with limited resources. Experimental results demonstrate that Bonsai achieves state-of-the-art pruning performance, compressing 7B and 8B models to 50% sparsity on a single A6000 GPU, where backprop-based methods struggle due to higher memory demands, and produces models that are twice as fast as those from semi-structured pruning while maintaining better compression efficiency.</div>
<div class="mono" style="margin-top:8px">结构化剪枝旨在创建更小更快的大语言模型，但现有的基于梯度的方法会增加内存和计算成本。本研究提出了Bonsai，一种无需梯度的结构化剪枝方法，仅通过前向传播的扰动剪枝实现，显著降低了资源需求，同时达到了先进的剪枝性能。实验结果表明，Bonsai能在单块A6000 GPU上将70亿和80亿参数模型剪枝至50%稀疏度，产生的模型速度比半结构化剪枝快一倍，且所需内存比基于反向传播的方法少2-3倍。</div>
</details>
</div>
<div class="card">
<div class="title">BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Digital Behavioural Change</div>
<div class="meta-line">Authors: Manuela González-González, Soufiane Belharbi, Muhammad Osama Zeeshan, Masoumeh Sharafi, Muhammad Haseeb Aslam, Marco Pedersoli, Alessandro Lameiras Koerich, Simon L Bacon, Eric Granger</div>
<div class="meta-line">First: 2025-05-25T21:29:00+00:00 · Latest: 2026-01-22T18:06:39+00:00</div>
<div class="meta-line">Comments: 45 pages, 21 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19328v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.19328v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ambivalence and hesitancy (A/H), a closely related construct, is the primary reasons why individuals delay, avoid, or abandon health behaviour changes. It is a subtle and conflicting emotion that sets a person in a state between positive and negative orientations, or between acceptance and refusal to do something. It manifests by a discord in affect between multiple modalities or within a modality, such as facial and vocal expressions, and body language. Although experts can be trained to recognize A/H as done for in-person interactions, integrating them into digital health interventions is costly and less effective. Automatic A/H recognition is therefore critical for the personalization and cost-effectiveness of digital behaviour change interventions. However, no datasets currently exists for the design of machine learning models to recognize A/H. This paper introduces the Behavioural Ambivalence/Hesitancy (BAH) dataset collected for multimodal recognition of A/H in videos. It contains 1,427 videos with a total duration of 10.60 hours captured from 300 participants across Canada answering predefined questions to elicit A/H. It is intended to mirror real-world online personalized behaviour change interventions. BAH is annotated by three experts to provide timestamps that indicate where A/H occurs, and frame- and video-level annotations with A/H cues. Video transcripts, cropped and aligned faces, and participants&#x27; meta-data are also provided. Since A and H manifest similarly in practice, we provide a binary annotation indicating the presence or absence of A/H. Additionally, this paper includes benchmarking results using baseline models on BAH for frame- and video-level recognition, zero-shot prediction, and personalization using source-free domain adaptation. The data, code, and pretrained weights are available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于视频中矛盾/犹豫识别的BAH数据集——面向数字化行为改变</div>
<div class="mono" style="margin-top:8px">矛盾与犹豫（A/H）是导致个体延迟、回避或放弃健康行为改变的主要原因，表现为一种介于积极与消极取向、接受与拒绝之间的微妙冲突情绪。其通过面部表情、语音表达和肢体语言等多模态或单模态内部的情感不一致性显现。虽然专家可通过培训识别面对面互动中的A/H，但将其整合到数字健康干预中成本高昂且效率有限。因此，自动识别A/H对数字化行为改变干预的个性化与成本效益至关重要。目前尚缺乏用于训练A/H识别机器学习模型的数据集。本文介绍了为视频多模态A/H识别而收集的行为矛盾/犹豫（BAH）数据集，包含来自加拿大300名参与者回答预设问题所录制的1,427条视频（总时长10.60小时），旨在模拟真实在线个性化行为干预场景。数据集由三位专家标注，提供A/H发生的时间戳、帧级与视频级A/H线索标注，同时包含视频转录文本、裁剪对齐的面部图像及参与者元数据。鉴于实践中A与H表现相似，我们采用二元标注标识A/H存在与否。此外，本文还提供了基于BAH的基准模型实验结果，涵盖帧级/视频级识别、零样本预测及无源域自适应个性化任务。数据、代码与预训练权重均已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Ambivalence and hesitancy (A/H) are key barriers to health behavior change, but their subtle, multimodal nature makes automatic recognition for digital interventions challenging due to a lack of suitable datasets. To address this, the authors introduce the Behavioural Ambivalence/Hesitancy (BAH) dataset, comprising 1,427 videos from 300 participants, annotated by experts for A/H presence with frame- and video-level labels, and provide baseline models for recognition tasks. Experimental benchmarking demonstrates the dataset&#x27;s utility for frame- and video-level A/H classification, zero-shot prediction, and personalization via source-free domain adaptation.</div>
<div class="mono" style="margin-top:8px">矛盾与犹豫是阻碍健康行为改变的主要因素，但其微妙且多模态的特性使得在数字干预中缺乏合适数据集进行自动识别。为此，研究者提出了行为矛盾/犹豫（BAH）数据集，包含来自300名参与者的1,427个视频，由专家标注了帧级和视频级的矛盾/犹豫线索，并提供了基于基线模型的识别与个性化任务基准测试。实验结果表明，该数据集能有效支持帧级和视频级的矛盾/犹豫识别、零样本预测以及无源域自适应，为开发经济高效的个性化数字健康工具奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems</div>
<div class="meta-line">Authors: Prakash Dhungana, Sayed Ahmad Salehi</div>
<div class="meta-line">First: 2026-01-22T17:59:31+00:00 · Latest: 2026-01-22T17:59:31+00:00</div>
<div class="meta-line">Comments: 12 pages, 8 figures, and 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework&#x27;s effectiveness, achieving 99.63\% accuracy on clean data and maintaining robust performance (exceeding 94\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向资源受限系统的鲁棒高效关键词检测：领域增量持续学习框架</div>
<div class="mono" style="margin-top:8px">部署在边缘设备上的轻量级关键词检测系统因噪声和录音条件变化导致的领域偏移，面临准确性与鲁棒性挑战。为此，我们提出一种兼顾计算效率的持续学习综合框架，使模型能适应新领域。该框架集成双输入卷积神经网络，同时利用梅尔频率倒谱系数和梅尔频谱特征，并配备多级去噪流程（含离散小波变换与谱减法）、模型更新模块和原型更新模块。与以往仅更新特定层的方法不同，本方法凭借紧凑的模型架构实现了完整量化模型的更新。运行时通过类别原型和置信度驱动筛选部分输入样本，经伪标注后与回放缓冲区数据结合进行增量重训练。在含噪声测试集上的实验表明：该框架在纯净数据上达到99.63%准确率，且在多样噪声环境（信噪比低至-10 dB）中仍保持超过94%的鲁棒性能。研究证实，将高效去噪与基于原型的持续学习相结合，可使关键词检测模型在资源受限的动态环境中自主稳定运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Keyword spotting systems on edge devices suffer from accuracy degradation due to domain shifts from noise and varying recording conditions. To address this, the authors propose a continual learning framework that integrates a dual-input CNN using MFCC and Mel-spectrogram features, a multi-stage denoising pipeline with wavelet transform and spectral subtraction, and a prototype-based update mechanism that retrains the full quantized model using pseudo-labeled samples and a rehearsal buffer. Experiments on noisy datasets show the framework achieves 99.63% accuracy on clean data and maintains over 94% accuracy across diverse noisy environments, even at -10 dB SNR, demonstrating robust adaptation in resource-constrained settings.</div>
<div class="mono" style="margin-top:8px">边缘设备上的关键词检测系统因噪声变化导致的域偏移而面临准确性下降的问题。为此，研究者提出了一种持续学习框架，该框架集成了使用MFCC和梅尔谱图特征的双输入卷积神经网络、包含小波变换和谱减法的多级去噪流程，以及基于原型的更新机制，该机制利用伪标记样本和回放缓冲区对完整量化模型进行再训练。在噪声数据集上的实验表明，该框架在干净数据上达到99.63%的准确率，即使在-10 dB信噪比下仍保持超过94%的准确率，证明了其在资源受限环境中具有强大的自适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">MCGrad: Multicalibration at Web Scale</div>
<div class="meta-line">Authors: Niek Tax, Lorenzo Perini, Fridolin Linder, Daniel Haimovich, Dima Karamshuk, Nastaran Okati, Milan Vojnovic, Pavlos Athanasios Apostolopoulos</div>
<div class="meta-line">Venue: KDD 2026</div>
<div class="meta-line">First: 2025-09-24T08:34:38+00:00 · Latest: 2026-01-22T17:41:06+00:00</div>
<div class="meta-line">Comments: Accepted at KDD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19884v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.19884v3">PDF</a> · <a href="https://github.com/facebookincubator/MCGrad">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose MCGrad, a novel and scalable multicalibration algorithm. Multicalibration - calibration in subgroups of the data - is an important property for the performance of machine learning-based systems. Existing multicalibration methods have thus far received limited traction in industry. We argue that this is because existing methods (1) require such subgroups to be manually specified, which ML practitioners often struggle with, (2) are not scalable, or (3) may harm other notions of model performance such as log loss and Area Under the Precision-Recall Curve (PRAUC). MCGrad does not require explicit specification of protected groups, is scalable, and often improves other ML evaluation metrics instead of harming them. MCGrad has been in production at Meta, and is now part of hundreds of production models. We present results from these deployments as well as results on public datasets. We provide an open source implementation of MCGrad at https://github.com/facebookincubator/MCGrad.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MCGrad：网络规模的多重校准技术</div>
<div class="mono" style="margin-top:8px">我们提出MCGrad，一种新颖且可扩展的多重校准算法。多重校准——在数据子组中进行校准——是基于机器学习的系统性能的重要属性。现有的多重校准方法迄今在工业界应用有限。我们认为这是因为现有方法（1）需要手动指定此类子组，而机器学习从业者往往难以做到；（2）不可扩展；或（3）可能损害模型性能的其他指标，如对数损失和精确率-召回率曲线下面积（PRAUC）。MCGrad无需显式指定受保护组，具有可扩展性，并且通常能改进其他机器学习评估指标而非损害它们。MCGrad已在Meta投入生产，现已成为数百个生产模型的一部分。我们展示了这些部署结果以及公开数据集上的结果。我们在https://github.com/facebookincubator/MCGrad提供了MCGrad的开源实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limited industry adoption of multicalibration methods, which ensure calibration across data subgroups, due to their reliance on manually specified subgroups, lack of scalability, and potential negative impact on other performance metrics like log loss and PRAUC. The proposed MCGrad method is a scalable multicalibration algorithm that automatically identifies subgroups without requiring explicit specification, operates efficiently at web scale, and is designed to maintain or improve other key evaluation metrics. Experimental results from deployments at Meta across hundreds of production models and on public datasets demonstrate that MCGrad effectively achieves multicalibration while often enhancing metrics such as log loss and PRAUC, confirming its practical utility in large-scale industrial settings.</div>
<div class="mono" style="margin-top:8px">本研究针对多重校准方法在工业界应用有限的问题，这些方法虽能确保数据子组的校准性能，但通常需要手动指定子组、缺乏可扩展性，并可能损害如对数损失和精确率-召回率曲线下面积等其他性能指标。提出的MCGrad方法是一种可扩展的多重校准算法，能自动识别子组而无需显式指定，从而克服了这些实际障碍。在Meta数百个生产模型中的部署结果以及公开数据集上的实验表明，MCGrad有效实现了多重校准，同时往往能改善而非损害其他关键评估指标，如对数损失和精确率-召回率曲线下面积。</div>
</details>
</div>
<div class="card">
<div class="title">Beat-ssl: Capturing Local ECG Morphology through Heartbeat-level Contrastive Learning with Soft Targets</div>
<div class="meta-line">Authors: Muhammad Ilham Rizqyawan, Peter Macfarlane, Stathis Hadjidemetriou, Fani Deligianni</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-22T17:40:23+00:00 · Latest: 2026-01-22T17:40:23+00:00</div>
<div class="meta-line">Comments: Accepted at ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16147v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16147v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Obtaining labelled ECG data for developing supervised models is challenging. Contrastive learning (CL) has emerged as a promising pretraining approach that enables effective transfer learning with limited labelled data. However, existing CL frameworks either focus solely on global context or fail to exploit ECG-specific characteristics. Furthermore, these methods rely on hard contrastive targets, which may not adequately capture the continuous nature of feature similarity in ECG signals. In this paper, we propose Beat-SSL, a contrastive learning framework that performs dual-context learning through both rhythm-level and heartbeat-level contrasting with soft targets. We evaluated our pretrained model on two downstream tasks: 1) multilabel classification for global rhythm assessment, and 2) ECG segmentation to assess its capacity to learn representations across both contexts. We conducted an ablation study and compared the best configuration with three other methods, including one ECG foundation model. Despite the foundation model&#x27;s broader pretraining, Beat-SSL reached 93% of its performance in multilabel classification task and surpassed all other methods in the segmentation task by 4%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Beat-SSL：通过基于软目标的心跳级对比学习捕捉局部心电图形态</div>
<div class="mono" style="margin-top:8px">获取标注心电图数据以开发监督模型具有挑战性。对比学习作为一种有前景的预训练方法，能够在有限标注数据下实现有效的迁移学习。然而，现有对比学习框架要么仅关注全局上下文，要么未能充分利用心电图特有的特征。此外，这些方法依赖硬对比目标，可能无法充分捕捉心电图信号中特征相似性的连续性。本文提出Beat-SSL对比学习框架，通过节律级和心跳级的软目标双重上下文对比进行学习。我们在两个下游任务评估预训练模型：1）全局节律评估的多标签分类，2）心电图分割以评估其跨上下文学习表征的能力。通过消融研究，将最优配置与包括一个心电图基础模型在内的三种方法比较。尽管基础模型预训练范围更广，Beat-SSL在多标签分类任务中达到其93%的性能，并在分割任务中以4%优势超越所有其他方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The difficulty of obtaining labeled ECG data motivates the development of self-supervised learning methods. This paper introduces Beat-SSL, a contrastive learning framework that captures both global rhythm and local heartbeat contexts using soft targets to better model the continuous similarity of ECG features. Experimental evaluation on multilabel classification and segmentation tasks shows that the pretrained model achieves 93% of the performance of a larger ECG foundation model in classification and surpasses other compared methods by 4% in segmentation.</div>
<div class="mono" style="margin-top:8px">由于获取带标签的心电图数据存在困难，本文提出了一种自监督学习方法。该方法名为Beat-SSL，它通过使用软目标的双上下文对比学习框架，同时捕捉全局节律和局部心跳形态，这比硬目标更能反映心电图特征的连续相似性。在多标签分类和心电图分割任务上的实验评估表明，预训练模型在分类任务上达到了一个更大的心电图基础模型性能的93%，并在分割任务上以4%的优势超越了其他方法。</div>
</details>
</div>
<div class="card">
<div class="title">Computing Fixpoints of Learned Functions: Chaotic Iteration and Simple Stochastic Games</div>
<div class="meta-line">Authors: Paolo Baldan, Sebastian Gurke, Barbara König, Florian Wittbold</div>
<div class="meta-line">First: 2026-01-22T17:36:19+00:00 · Latest: 2026-01-22T17:36:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16142v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The problem of determining the (least) fixpoint of (higher-dimensional) functions over the non-negative reals frequently occurs when dealing with systems endowed with a quantitative semantics. We focus on the situation in which the functions of interest are not known precisely but can only be approximated. As a first contribution we generalize an iteration scheme called dampened Mann iteration, recently introduced in the literature. The improved scheme relaxes previous constraints on parameter sequences, allowing learning rates to converge to zero or not converge at all. While seemingly minor, this flexibility is essential to enable the implementation of chaotic iterations, where only a subset of components is updated in each step, allowing to tackle higher-dimensional problems. Additionally, by allowing learning rates to converge to zero, we can relax conditions on the convergence speed of function approximations, making the method more adaptable to various scenarios. We also show that dampened Mann iteration applies immediately to compute the expected payoff in various probabilistic models, including simple stochastic games, not covered by previous work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习函数的不动点计算：混沌迭代与简单随机博弈</div>
<div class="mono" style="margin-top:8px">在处理具有定量语义的系统时，确定非负实数上（高维）函数的（最小）不动点问题频繁出现。本文聚焦于目标函数无法精确获知、仅能近似逼近的情形。作为第一项贡献，我们推广了近期文献提出的阻尼曼恩迭代方案，通过放宽对参数序列的限制，允许学习率收敛至零或完全不收敛。这一看似微小的灵活性对实现混沌迭代至关重要——混沌迭代每步仅更新部分分量，从而能处理更高维问题。同时，学习率收敛至零的特性可放宽函数逼近收敛速度的条件，增强方法对不同场景的适应性。我们还证明阻尼曼恩迭代可直接用于计算各类概率模型（包括先前工作未涵盖的简单随机博弈）中的期望收益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of computing fixpoints of quantitative functions when only approximations are available, a common scenario in systems with quantitative semantics. The method introduces a generalized dampened Mann iteration scheme that relaxes constraints on parameter sequences, permitting learning rates to converge to zero or not converge at all. This flexibility enables chaotic iterations, where only a subset of components is updated per step, making higher-dimensional problems tractable, and relaxes conditions on the convergence speed of function approximations. Key experimental findings demonstrate that the improved scheme can compute expected payoffs in probabilistic models, including simple stochastic games, which were not covered by prior work.</div>
<div class="mono" style="margin-top:8px">本研究针对在函数未知且仅能近似的情况下，计算具有定量语义的函数不动点的问题。方法上，提出了一种广义的阻尼曼恩迭代方案，放宽了对参数序列的限制，允许学习率收敛至零或不收敛，从而支持混沌迭代，即每一步仅更新部分组件，以更有效地处理高维问题。关键实验结果表明，该方法通过放宽函数近似的收敛速度条件，扩展了适用性，并成功计算了概率模型（包括简单随机博弈）中的期望收益，填补了先前工作的空白。</div>
</details>
</div>
<div class="card">
<div class="title">On the Intrinsic Dimensions of Data in Kernel Learning</div>
<div class="meta-line">Authors: Rustem Takhanov</div>
<div class="meta-line">First: 2026-01-22T17:32:24+00:00 · Latest: 2026-01-22T17:32:24+00:00</div>
<div class="meta-line">Comments: Accepted to The 29th International Conference on Artificial Intelligence and Statistics (AISTATS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16139v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16139v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The manifold hypothesis suggests that the generalization performance of machine learning methods improves significantly when the intrinsic dimension of the input distribution&#x27;s support is low. In the context of KRR, we investigate two alternative notions of intrinsic dimension. The first, denoted $d_ρ$, is the upper Minkowski dimension defined with respect to the canonical metric induced by a kernel function $K$ on a domain $Ω$. The second, denoted $d_K$, is the effective dimension, derived from the decay rate of Kolmogorov $n$-widths associated with $K$ on $Ω$. Given a probability measure $μ$ on $Ω$, we analyze the relationship between these $n$-widths and eigenvalues of the integral operator $φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$. We show that, for a fixed domain $Ω$, the Kolmogorov $n$-widths characterize the worst-case eigenvalue decay across all probability measures $μ$ supported on $Ω$. These eigenvalues are central to understanding the generalization behavior of constrained KRR, enabling us to derive an excess error bound of order $O(n^{-\frac{2+d_K}{2+2d_K} + ε})$ for any $ε&gt; 0$, when the training set size $n$ is large. We also propose an algorithm that estimates upper bounds on the $n$-widths using only a finite sample from $μ$. For distributions close to uniform, we prove that $ε$-accurate upper bounds on all $n$-widths can be computed with high probability using at most $O\left(ε^{-d_ρ}\log\frac{1}ε\right)$ samples, with fewer required for small $n$. Finally, we compute the effective dimension $d_K$ for various fractal sets and present additional numerical experiments. Our results show that, for kernels such as the Laplace kernel, the effective dimension $d_K$ can be significantly smaller than the Minkowski dimension $d_ρ$, even though $d_K = d_ρ$ provably holds on regular domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>核学习中数据本征维度的研究</div>
<div class="mono" style="margin-top:8px">流形假说认为，当输入分布支撑集的本征维度较低时，机器学习方法的泛化性能会显著提升。在核岭回归（KRR）框架下，我们研究了两种本征维度的定义。第一种记为$d_ρ$，是基于核函数$K$在定义域$Ω$上诱导的典型度量的上闵可夫斯基维度。第二种记为$d_K$，是有效维度，源于$K$在$Ω$上关联的柯尔莫哥洛夫$n$宽度的衰减率。给定$Ω$上的概率测度$μ$，我们分析了这些$n$宽度与积分算子$φ\to \int_ΩK(\cdot,x)φ(x)dμ(x)$特征值之间的关系。我们证明，对于固定定义域$Ω$，柯尔莫哥洛夫$n$宽度刻画了所有支撑在$Ω$上的概率测度$μ$对应的最坏情况特征值衰减。这些特征值是理解约束KRR泛化行为的关键，使我们能够推导出当训练集规模$n$较大时，超额误差界为$O(n^{-\frac{2+d_K}{2+2d_K} + ε})$（对任意$ε&gt;0$）。我们还提出一种算法，仅利用$μ$的有限样本估计$n$宽度的上界。对于接近均匀的分布，我们证明以高概率计算所有$n$宽度的$ε$精度上界最多需要$O\left(ε^{-d_ρ}\log\frac{1}ε\right)$个样本，且小$n$时所需样本更少。最后，我们计算了多种分形集的有效维度$d_K$并展示了数值实验。结果表明，对于拉普拉斯核等核函数，有效维度$d_K$可能显著小于闵可夫斯基维度$d_ρ$，尽管在规则定义域上可严格证明$d_K = d_ρ$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the manifold hypothesis that low intrinsic dimensionality improves generalization, this paper investigates two notions of intrinsic dimension for kernel ridge regression (KRR): the Minkowski dimension d_ρ and the effective dimension d_K derived from Kolmogorov n-widths. The method analyzes the relationship between these n-widths and eigenvalues of the kernel integral operator, showing that the n-widths characterize worst-case eigenvalue decay across probability measures on a domain. Key experimental findings include an excess error bound of O(n^{-(2+d_K)/(2+2d_K) + ε}) for constrained KRR, an algorithm to estimate n-widths from finite samples requiring O(ε^{-d_ρ} log(1/ε)) samples for near-uniform distributions, and numerical demonstrations that d_K can be much smaller than d_ρ for kernels like the Laplace kernel on fractal sets, though they coincide on regular domains.</div>
<div class="mono" style="margin-top:8px">受流形假设启发，即低内在维度能提升泛化性能，本文研究了核岭回归中两种内在维度概念：闵可夫斯基维度 d_ρ 和基于柯尔莫哥洛夫 n-宽度导出的有效维度 d_K。方法分析了这些 n-宽度与核积分算子特征值的关系，证明 n-宽度刻画了域上所有概率测度的最坏情况特征值衰减。关键实验结果包括：对大样本规模 n 得到了 O(n^{-(2+d_K)/(2+2d_K) + ε}) 的过剩误差界；提出了一种从有限样本估计 n-宽度的算法，对接近均匀的分布仅需 O(ε^{-d_ρ} log(1/ε)) 样本；数值实验表明，对于拉普拉斯核等核函数，在分形集上 d_K 可显著小于 d_ρ，尽管在规则域上两者理论上相等。</div>
</details>
</div>
<div class="card">
<div class="title">Automatic Classification of Arabic Literature into Historical Eras</div>
<div class="meta-line">Authors: Zainab Alhathloul, Irfan Ahmad</div>
<div class="meta-line">First: 2026-01-22T17:32:19+00:00 · Latest: 2026-01-22T17:32:19+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16138v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16138v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Arabic language has undergone notable transformations over time, including the emergence of new vocabulary, the obsolescence of others, and shifts in word usage. This evolution is evident in the distinction between the classical and modern Arabic eras. Although historians and linguists have partitioned Arabic literature into multiple eras, relatively little research has explored the automatic classification of Arabic texts by time period, particularly beyond the domain of poetry. This paper addresses this gap by employing neural networks and deep learning techniques to automatically classify Arabic texts into distinct eras and periods. The proposed models are evaluated using two datasets derived from two publicly available corpora, covering texts from the pre-Islamic to the modern era. The study examines class setups ranging from binary to 15-class classification and considers both predefined historical eras and custom periodizations. Results range from F1-scores of 0.83 and 0.79 on the binary-era classification task using the OpenITI and APCD datasets, respectively, to 0.20 on the 15-era classification task using OpenITI and 0.18 on the 12-era classification task using APCD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阿拉伯文学历史时期的自动分类研究</div>
<div class="mono" style="margin-top:8px">阿拉伯语历经显著演变，包括新词汇涌现、旧词淘汰及用词习惯变迁，古典与现代阿拉伯语时期的差异尤为明显。尽管历史学家和语言学家已将阿拉伯文学划分为多个时期，但针对阿拉伯文本（尤其是诗歌以外领域）按时代自动分类的研究相对匮乏。本文通过神经网络与深度学习技术，填补了这一空白，实现了阿拉伯文本按不同时代与时期的自动分类。研究采用两个公开语料库构建的数据集进行评估，涵盖前伊斯兰时期至现代文本，探讨了从二分类到15分类的多种设置，并比较了预设历史分期与自定义分期方案。结果显示：在二分类任务中，OpenITI和APCD数据集的F1分数分别为0.83和0.79；而在OpenITI的15分类任务中降至0.20，APCD的12分类任务中为0.18。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the linguistic evolution of Arabic and the lack of research on automatic temporal classification beyond poetry, this study employs neural networks and deep learning to classify Arabic texts into historical eras. The method involves evaluating models on two public corpora, OpenITI and APCD, with tasks ranging from binary to 15-class classification based on predefined or custom periodizations. Key experimental results show F1-scores of 0.83 and 0.79 for binary-era classification on the respective datasets, but performance declines to 0.20 and 0.18 for more granular 15-era and 12-era tasks.</div>
<div class="mono" style="margin-top:8px">本研究基于阿拉伯语的语言演变以及除诗歌外自动时代分类研究的缺乏，采用神经网络和深度学习技术对阿拉伯语文本进行历史时期分类。方法涉及在两个公开可用的语料库（OpenITI和APCD）上评估模型，这些语料库涵盖从伊斯兰前时期到现代时期的文本，分类任务从二元到15类设置，并使用预定义和自定义的时期划分。关键实验结果显示，在二元时代分类任务上，两个数据集的F1分数分别为0.83和0.79，但在更细粒度的15时代和12时代分类任务中，性能下降至0.20和0.18。</div>
</details>
</div>
<div class="card">
<div class="title">ViSymRe: Vision-guided Multimodal Symbolic Regression</div>
<div class="meta-line">Authors: Da Li, Junping Yin, Jin Xu, Xinxin Li, Juan Zhang</div>
<div class="meta-line">First: 2024-12-15T10:05:31+00:00 · Latest: 2026-01-22T17:29:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11139v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.11139v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extracting simple mathematical expression from an observational dataset to describe complex natural phenomena is one of the core objectives of artificial intelligence (AI). This field is known as symbolic regression (SR). Traditional SR models are based on genetic programming (GP) or reinforcement learning (RL), facing well-known challenges, such as low efficiency and overfitting. Recent studies have integrated SR with large language models (LLMs), enabling fast zero-shot inference by learning mappings from millions of dataset-expression pairs. However, since the input and output are inherently different modalities, such models often struggle to converge effectively. In this paper, we introduce ViSymRe, a vision-guided multimodal SR model that incorporates the third resource, expression graph, to bridge the modality gap. Different from traditional multimodal models, ViSymRe is trained to extract vision, termed virtual vision, from datasets, without relying on the global availability of expression graphs, which addresses the essential challenge of visual SR, i.e., expression graphs are not available during inference. Evaluation results on multiple mainstream benchmarks show that ViSymRe achieves more competitive performance than the state-of-the-art dataset-only baselines. The expressions predicted by ViSymRe not only fit the dataset well but are also simple and structurally accurate, goals that SR models strive to achieve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSymRe：视觉引导的多模态符号回归</div>
<div class="mono" style="margin-top:8px">从观测数据集中提取简洁数学表达式以描述复杂自然现象，是人工智能（AI）的核心目标之一，该领域称为符号回归（SR）。传统SR模型基于遗传编程（GP）或强化学习（RL），面临效率低下和过拟合等常见挑战。近期研究将SR与大型语言模型（LLM）结合，通过从数百万数据集-表达式对中学习映射，实现快速零样本推理。然而，由于输入与输出本质属于不同模态，此类模型常难以有效收敛。本文提出ViSymRe，一种视觉引导的多模态SR模型，通过引入第三种资源——表达式图——来弥合模态差异。与传统多模态模型不同，ViSymRe训练从数据集中提取视觉信息（称为虚拟视觉），无需依赖表达式图的全局可用性，从而解决了视觉SR的关键挑战：推理阶段表达式图不可得。在多个主流基准测试上的评估结果表明，ViSymRe的性能优于当前仅依赖数据集的先进基线模型。ViSymRe预测的表达式不仅与数据高度吻合，且兼具简洁性与结构准确性，这正是SR模型追求的目标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve symbolic regression (SR) for extracting mathematical expressions from data, addressing inefficiencies in genetic programming and reinforcement learning methods and the modality gap in large language model (LLM)-based approaches. The proposed ViSymRe model introduces a vision-guided multimodal method that incorporates expression graphs as an intermediate resource during training to bridge the modality gap; it learns to extract &#x27;virtual vision&#x27; from datasets without requiring expression graphs at inference time. Experimental evaluations on multiple benchmarks demonstrate that ViSymRe outperforms state-of-the-art dataset-only baselines, producing expressions that are accurate, simple, and structurally sound.</div>
<div class="mono" style="margin-top:8px">该研究旨在通过解决观测数据集与数学表达式之间的模态差异来改进符号回归（SR），这种差异阻碍了基于大语言模型（LLM）方法的有效收敛。所提出的方法ViSymRe引入了一种视觉引导的多模态模型，在训练中利用表达式图作为中间表示，从而能够从数据集中提取“虚拟视觉”，而无需在推理时使用表达式图。在多个主流基准测试上的评估结果表明，ViSymRe的性能优于当前最先进的仅使用数据集的基线方法，其预测的表达式不仅对数据拟合良好，而且结构简单准确。</div>
</details>
</div>
<div class="card">
<div class="title">Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add</div>
<div class="meta-line">Authors: Zhengchi Ma, Anru R. Zhang</div>
<div class="meta-line">First: 2026-01-22T17:15:26+00:00 · Latest: 2026-01-22T17:15:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16120v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16120v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?
  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry&quot; regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry&quot; regime), the optimal synthetic size depends on generator accuracy and on whether the generator&#x27;s residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不平衡学习中的合成增强：何时有效、何时有害及添加量的控制</div>
<div class="mono" style="margin-top:8px">不平衡分类问题中，少数类样本的稀缺常导致标准训练过程偏向多数类，在重要但罕见的案例上表现不佳。合成增强是经典且广泛应用的解决方案，但两个基本问题尚未明确：合成增强何时真正有效？应生成多少合成样本？
我们建立了不平衡学习中合成增强的统一统计框架，研究在添加合成少数类样本的不平衡数据上训练的模型，并以平衡总体风险进行评估。理论表明合成数据并非总是有益的。在“局部对称”机制下，不平衡并非平衡最优解附近误差的主要来源，因此添加合成样本无法提升学习速率，甚至可能因放大生成器失配而降低性能。当增强可能有效时（“局部不对称”机制），最优合成量取决于生成器精度，以及生成器的残余失配是否与固有的多数类-少数类偏移方向一致。这种结构可能导致最佳合成量偏离朴素完全平衡策略，有时仅需微调，而在生成器存在系统性偏差时则需大幅调整。实践中，我们推荐“验证调优合成量”方法：通过在完全平衡基线附近区间最小化平衡验证损失来选择合成量，同时允许数据驱动的合理偏离。仿真实验和真实脓毒症预测研究验证了该理论，并阐明了合成增强的有效场景、无效场景及其数量的优化方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Imbalanced classification often leads to poor performance on minority classes, and while synthetic augmentation is a common remedy, its effectiveness and optimal scale remain unclear. The authors develop a statistical framework to analyze models trained on imbalanced data augmented with synthetic minority samples, evaluated under balanced risk. Their theory identifies two regimes: in &#x27;local symmetry,&#x27; imbalance is not the main error source, so augmentation may not help and can even degrade performance due to generator mismatch; in &#x27;local asymmetry,&#x27; augmentation can help, with the optimal synthetic size depending on generator accuracy and alignment with the intrinsic class shift, sometimes deviating significantly from naive full balancing. Experimental results from simulations and a sepsis prediction study support the framework and demonstrate the practical utility of a validation-tuned approach for selecting synthetic size.</div>
<div class="mono" style="margin-top:8px">不平衡分类常导致少数类性能不佳，合成增强是常用补救措施，但其有效性和最佳规模尚不明确。作者建立了一个统计框架，分析在添加合成少数类样本的不平衡数据上训练的模型，并以平衡风险进行评估。理论揭示了两种机制：在“局部对称”情况下，不平衡并非主要误差来源，因此增强可能无益，甚至因生成器不匹配而损害性能；在“局部不对称”情况下，增强可能有效，最佳合成规模取决于生成器准确度及其与内在类别偏移的对齐程度，有时会显著偏离简单的完全平衡。实验（包括脓毒症预测研究）支持该理论，并展示了通过最小化平衡验证损失来选择合成数量的实用建议——验证调整合成规模（VTSS）。</div>
</details>
</div>
<div class="card">
<div class="title">Enhanced Climbing Image Nudged Elastic Band method with Hessian Eigenmode Alignment</div>
<div class="meta-line">Authors: Rohit Goswami, Miha Gunde, Hannes Jónsson</div>
<div class="meta-line">First: 2026-01-19T00:21:52+00:00 · Latest: 2026-01-22T17:11:23+00:00</div>
<div class="meta-line">Comments: 25 pages. 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12630v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12630v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate determination of transition states is central to an understanding of reaction kinetics. Double-endpoint methods where both initial and final states are specified, such as the climbing image nudged elastic band (CI-NEB), identify the minimum energy path between the two and thereby the saddle point on the energy surface that is relevant for the given transition, thus providing an estimate of the transition state within the harmonic approximation of transition state theory. Such calculations can, however, incur high computational costs and may suffer stagnation on exceptionally flat or rough energy surfaces. Conversely, methods that only require specification of an initial set of atomic coordinates, such as the minimum mode following (MMF) method, offer efficiency but can converge on saddle points that are not relevant for transition of interest. Here, we present an adaptive hybrid algorithm that integrates the CI-NEB with the MMF method so as to get faster convergence to the relevant saddle point. The method is benchmarked for the Baker-Chan (BC) saddle point test set using the PET-MAD machine-learned potential as well as 59 transitions of a heptamer island on Pt(111) from the OptBench benchmark set. A Bayesian analysis of the performance shows a median reduction in energy and force calculations of 46% [95% CrI: -55%, -37%] relative to CI-NEB for the BC set, while a 28% reduction is found for the transitions of the heptamer island. These results establish this hybrid method as a highly effective tool for high-throughput automated chemical discovery of atomic rearrangements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Hessian本征模对齐的增强型爬坡图像微动弹性带方法</div>
<div class="mono" style="margin-top:8px">精确确定过渡态是理解反应动力学的关键。双端点方法（如爬坡图像微动弹性带法）通过指定初态和终态，可识别两者间的最小能量路径及对应能垒面上的鞍点，从而在过渡态理论的谐波近似下提供过渡态估计。然而，此类计算常伴随高计算成本，且在异常平坦或粗糙的能垒面上易陷入停滞。相反，仅需初始原子坐标的方法（如最小模跟踪法）虽效率较高，但可能收敛至与目标反应无关的鞍点。本文提出一种自适应混合算法，将爬坡图像微动弹性带法与最小模跟踪法结合，以加速收敛至目标鞍点。该方法通过PET-MAD机器学习势能对Baker-Chan鞍点测试集，以及OptBench基准集中Pt(111)表面七聚体岛59个过渡路径进行验证。贝叶斯分析表明：对于Baker-Chan测试集，混合算法将能量与力计算量中位数降低46%[95%置信区间：-55%, -37%]；对七聚体岛过渡路径则降低28%。这些结果证明该混合算法是原子重排高通量自动化化学发现的高效工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate transition state identification is crucial for reaction kinetics, but double-endpoint methods like climbing image nudged elastic band (CI-NEB) can be computationally expensive and may stagnate on flat or rough energy surfaces, while single-endpoint methods like minimum mode following (MMF) are efficient but may converge to irrelevant saddle points. To address this, the authors develop an adaptive hybrid algorithm that integrates CI-NEB with MMF, using Hessian eigenmode alignment to guide convergence toward the relevant saddle point more efficiently. Benchmarking on the Baker-Chan test set with a machine-learned potential and on 59 heptamer island transitions on Pt(111) shows median reductions in energy and force calculations of 46% and 28%, respectively, compared to CI-NEB, demonstrating its effectiveness for high-throughput automated discovery of atomic rearrangements.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决爬升图像微动弹性带（CI-NEB）方法在定位相关过渡态时计算效率低、可能停滞的问题，同时克服最小模跟踪（MMF）方法可能收敛到无关鞍点的局限。作者开发了一种自适应混合算法，将CI-NEB与MMF相结合，利用Hessian本征模对齐来引导搜索朝向正确的鞍点。在Baker-Chan测试集和Pt(111)上七聚体岛的59个跃迁上的基准测试表明，该方法显著提升了计算效率，与标准CI-NEB相比，前者在能量和力计算上中位数减少了46%，后者减少了28%。</div>
</details>
</div>
<div class="card">
<div class="title">GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Shutong Ding, Ke Hu, Shan Zhong, Haoyang Luo, Weinan Zhang, Jingya Wang, Jun Wang, Ye Shi</div>
<div class="meta-line">First: 2025-05-24T15:57:07+00:00 · Latest: 2026-01-22T17:10:05+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.18763v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.18763v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO&#x27;s superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenPO：生成扩散模型与在线策略强化学习的融合</div>
<div class="mono" style="margin-top:8px">强化学习（RL）的最新进展展示了基于生成扩散的策略在探索能力和多模态方面的强大潜力。尽管离线RL和离线策略RL已取得显著进步，但将扩散策略整合到PPO等在线策略框架中仍待深入探索。鉴于大规模并行GPU加速模拟器（如IsaacLab）的广泛应用——这类模拟器专为在线策略RL算法优化，能快速训练复杂机器人任务——这一空白尤为关键。核心挑战在于计算扩散策略下的状态-动作对数似然：高斯策略可直接计算，而基于流的模型因不可逆的前向-反向过程及离散化误差（如欧拉-丸山近似）导致计算困难。为填补此空白，我们提出GenPO——一种利用精确扩散反演构建可逆动作映射的生成式策略优化框架。GenPO引入新颖的双重虚拟动作机制，通过交替更新实现可逆性，解决了对数似然计算障碍。此外，我们还将动作对数似然用于无偏熵和KL散度估计，实现在线策略更新中的KL自适应学习率与熵正则化。在八项IsaacLab基准测试（包括足式运动、灵巧操作、空中控制和机械臂任务）上的大量实验表明，GenPO优于现有RL基线方法。值得注意的是，GenPO是首个成功将扩散策略整合到在线策略RL中的方法，释放了其在大规模并行训练和现实机器人部署中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the underexplored integration of generative diffusion models into on-policy reinforcement learning frameworks like PPO, which is crucial for leveraging large-scale parallel simulators optimized for on-policy training. The proposed GenPO method overcomes the intractability of computing state-action log-likelihoods under diffusion policies by introducing exact diffusion inversion to construct invertible action mappings and a doubled dummy action mechanism for invertibility via alternating updates. Experiments across eight IsaacLab benchmarks, including locomotion, manipulation, and control tasks, demonstrate GenPO&#x27;s superiority over existing RL baselines, marking the first successful integration of diffusion policies into on-policy RL for scalable robotic training.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于将生成扩散策略整合到基于策略的强化学习框架中，因为现有方法主要关注离线或离策略设置，而像IsaacLab这样的大规模并行模拟器广泛使用基于策略的算法。所提出的方法GenPO引入了一个生成策略优化框架，利用精确的扩散反演构建可逆的动作映射，通过一种新颖的双重虚拟动作机制解决了计算状态-动作对数似然的难题。在八个IsaacLab基准测试（包括腿部运动和灵巧操作任务）上的实验结果表明，GenPO优于现有的强化学习基线，并首次成功实现了扩散策略在基于策略的强化学习中的可扩展机器人训练。</div>
</details>
</div>
<div class="card">
<div class="title">Variable Splitting Binary Tree Models Based on Bayesian Context Tree Models for Time Series Segmentation</div>
<div class="meta-line">Authors: Yuta Nakahara, Shota Saito, Kohei Horinouchi, Koshi Shimada, Naoki Ichijo, Manabu Kobayashi, Toshiyasu Matsushima</div>
<div class="meta-line">First: 2026-01-22T16:58:34+00:00 · Latest: 2026-01-22T16:58:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16112v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16112v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a variable splitting binary tree (VSBT) model based on Bayesian context tree (BCT) models for time series segmentation. Unlike previous applications of BCT models, the tree structure in our model represents interval partitioning on the time domain. Moreover, interval partitioning is represented by recursive logistic regression models. By adjusting logistic regression coefficients, our model can represent split positions at arbitrary locations within each interval. This enables more compact tree representations. For simultaneous estimation of both split positions and tree depth, we develop an effective inference algorithm that combines local variational approximation for logistic regression with the context tree weighting (CTW) algorithm. We present numerical examples on synthetic data demonstrating the effectiveness of our model and algorithm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于贝叶斯上下文树模型的时间序列分割可变分裂二叉树模型</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于贝叶斯上下文树（BCT）模型的可变分裂二叉树（VSBT）模型，用于时间序列分割。与先前BCT模型的应用不同，本模型中的树结构表示时间域上的区间划分。此外，区间划分通过递归逻辑回归模型表示。通过调整逻辑回归系数，我们的模型能够在每个区间内任意位置表示分裂点，从而实现更紧凑的树表示。为同时估计分裂位置和树深度，我们开发了一种有效的推断算法，将逻辑回归的局部变分近似与上下文树加权（CTW）算法相结合。通过合成数据的数值示例，我们展示了模型与算法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve time series segmentation by enabling more flexible and compact representations of interval partitions. The method introduces a variable splitting binary tree (VSBT) model that builds upon Bayesian context tree (BCT) models, where the tree structure encodes interval partitioning on the time domain using recursive logistic regression to allow splits at arbitrary positions. For inference, the approach combines local variational approximation for logistic regression with the context tree weighting algorithm to jointly estimate split positions and tree depth. Experimental results on synthetic data demonstrate the model&#x27;s effectiveness in achieving accurate segmentation.</div>
<div class="mono" style="margin-top:8px">本研究旨在改进时间序列分割，通过开发一种能够紧凑表示在任意位置分割的模型，克服了先前贝叶斯上下文树模型未针对时间域区间划分设计的限制。该方法引入了可变分割二叉树模型，其中树结构通过递归逻辑回归编码区间划分，允许通过回归系数调整分割位置，并采用一种结合局部变分近似和上下文树加权算法的推理算法来联合估计分割位置和树深度。在合成数据上的数值实验结果表明，所提出的模型和算法在实现准确分割方面具有有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Deep Learning Models for Raman Spectroscopy Across Open-Source Datasets</div>
<div class="meta-line">Authors: Adithya Sineesh, Akshita Kamsali</div>
<div class="meta-line">First: 2026-01-22T16:54:53+00:00 · Latest: 2026-01-22T16:54:53+00:00</div>
<div class="meta-line">Comments: 17 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning classifiers for Raman spectroscopy are increasingly reported to outperform classical chemometric approaches. However their evaluations are often conducted in isolation or compared against traditional machine learning methods or trivially adapted vision-based architectures that were not originally proposed for Raman spectroscopy. As a result, direct comparisons between existing deep learning models developed specifically for Raman spectral analysis on shared open-source datasets remain scarce. To the best of our knowledge, this study presents one of the first systematic benchmarks comparing three or more published Raman-specific deep learning classifiers across multiple open-source Raman datasets. We evaluate five representative deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source Raman datasets selected to support standard evaluation, fine-tuning, and explicit distribution-shift testing. We report classification accuracies and macro-averaged F1 scores to provide a fair and reproducible comparison of deep learning models for Raman spectra based classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于开源数据集的拉曼光谱深度学习模型基准测试</div>
<div class="mono" style="margin-top:8px">拉曼光谱的深度学习分类器被越来越多地报道优于经典化学计量学方法。然而，其评估往往孤立进行，或仅与传统机器学习方法或未经专门适配的视觉架构比较，这些架构最初并非为拉曼光谱设计。因此，在共享开源数据集上，专门为拉曼光谱分析开发的现有深度学习模型之间的直接比较仍显不足。据我们所知，本研究首次系统性地对三个及以上已发表的拉曼专用深度学习分类器在多个开源拉曼数据集上进行了基准测试。我们在三个开源拉曼数据集上，采用统一的训练和超参数调优协议，评估了五种代表性深度学习架构，这些数据集支持标准评估、微调和显式分布偏移测试。我们报告了分类准确率和宏平均F1分数，为基于拉曼光谱分类的深度学习模型提供了公平且可复现的比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the lack of systematic comparisons among deep learning models specifically designed for Raman spectroscopy, as existing evaluations often involve isolated assessments or comparisons with non-specialized methods. To fill this gap, the authors conduct a benchmark by evaluating five representative Raman-specific deep learning architectures under a unified training and hyperparameter tuning protocol across three open-source datasets, enabling standard evaluation, fine-tuning, and distribution-shift testing. The experimental results, reported in terms of classification accuracies and macro-averaged F1 scores, provide a fair and reproducible comparison, highlighting the performance of these models in Raman spectral classification tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对拉曼光谱领域缺乏专门深度学习模型系统比较的问题，因为现有评估常采用孤立测试或与非专用方法对比。为填补这一空白，作者通过统一训练和超参数调优协议，在三个开源数据集上评估了五种代表性拉曼专用深度学习架构，支持标准评估、微调和分布偏移测试。关键实验结果包括分类准确率和宏观平均F1分数，为这些模型在拉曼光谱分类中的性能提供了公平且可复现的评估。</div>
</details>
</div>
<div class="card">
<div class="title">Clustering-Guided Spatial-Spectral Mamba for Hyperspectral Image Classification</div>
<div class="meta-line">Authors: Zack Dewis, Yimin Zhu, Zhengsen Xu, Mabel Heffring, Saeid Taleghanidoozdoozan, Quinn Ledingham, Lincoln Linlin Xu</div>
<div class="meta-line">First: 2026-01-22T16:47:07+00:00 · Latest: 2026-01-22T16:47:07+00:00</div>
<div class="meta-line">Comments: 5 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16098v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although Mamba models greatly improve Hyperspectral Image (HSI) classification, they have critical challenges in terms defining efficient and adaptive token sequences for improve performance. This paper therefore presents CSSMamba (Clustering-guided Spatial-Spectral Mamba) framework to better address the challenges, with the following contributions. First, to achieve efficient and adaptive token sequences for improved Mamba performance, we integrate the clustering mechanism into a spatial Mamba architecture, leading to a cluster-guided spatial Mamba module (CSpaMamba) that reduces the Mamba sequence length and improves Mamba feature learning capability. Second, to improve the learning of both spatial and spectral information, we integrate the CSpaMamba module with a spectral mamba module (SpeMamba), leading to a complete clustering-guided spatial-spectral Mamba framework. Third, to further improve feature learning capability, we introduce an Attention-Driven Token Selection mechanism to optimize Mamba token sequencing. Last, to seamlessly integrate clustering into the Mamba model in a coherent manner, we design a Learnable Clustering Module that learns the cluster memberships in an adaptive manner. Experiments on the Pavia University, Indian Pines, and Liao-Ning 01 datasets demonstrate that CSSMamba achieves higher accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>聚类引导的空谱Mamba高光谱图像分类方法</div>
<div class="mono" style="margin-top:8px">尽管Mamba模型显著提升了高光谱图像（HSI）分类性能，但在构建高效且自适应的token序列以进一步提升性能方面仍面临关键挑战。为此，本文提出CSSMamba（聚类引导的空谱Mamba）框架以更好地应对这些挑战，主要贡献包括：首先，为构建高效自适应token序列以提升Mamba性能，将聚类机制集成到空间Mamba架构中，形成聚类引导的空间Mamba模块（CSpaMamba），该模块可缩短Mamba序列长度并增强特征学习能力；其次，为同时优化空间与光谱信息学习，将CSpaMamba模块与光谱Mamba模块（SpeMamba）结合，构建完整的聚类引导空谱Mamba框架；第三，引入注意力驱动的token选择机制以优化Mamba序列构建；最后，设计可学习聚类模块，以自适应方式学习聚类归属关系，实现聚类与Mamba模型的有机融合。在帕维亚大学、印第安松树及辽宁01数据集上的实验表明，相较于当前最先进的CNN、Transformer及Mamba方法，CSSMamba在分类精度和边界保持方面均表现更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of defining efficient and adaptive token sequences for Mamba models in hyperspectral image classification, this paper proposes CSSMamba, a clustering-guided spatial-spectral Mamba framework. The method integrates a learnable clustering module to reduce sequence length and improve feature learning in a spatial Mamba module, combines it with a spectral Mamba module, and employs an attention-driven token selection mechanism. Experimental results on three datasets show that CSSMamba achieves higher classification accuracy and better boundary preservation compared to state-of-the-art CNN, Transformer, and Mamba-based methods.</div>
<div class="mono" style="margin-top:8px">针对Mamba模型在高光谱图像分类中定义高效自适应标记序列的挑战，本文提出了CSSMamba框架。该方法将聚类机制集成到空间Mamba模块（CSpaMamba）中以减少序列长度并增强特征学习，将其与光谱Mamba模块（SpeMamba）结合，并采用注意力驱动的标记选择机制以及可学习的聚类模块进行自适应优化。在帕维亚大学、印第安松树和辽宁01数据集上的实验结果表明，与最先进的CNN、Transformer和基于Mamba的方法相比，CSSMamba实现了更高的分类精度和更好的边界保持能力。</div>
</details>
</div>
<div class="card">
<div class="title">Likelihood Matching for Diffusion Models</div>
<div class="meta-line">Authors: Lei Qian, Wu Su, Yanqi Huang, Song Xi Chen</div>
<div class="meta-line">First: 2025-08-05T16:51:29+00:00 · Latest: 2026-01-22T16:44:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.03636v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.03636v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transitional moments between every two time points. A stochastic sampler is introduced to facilitate computation that leverages both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to the score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的似然匹配方法</div>
<div class="mono" style="margin-top:8px">本文提出一种用于训练扩散模型的似然匹配方法：首先建立目标数据分布似然与反向扩散采样路径似然之间的等价关系。为高效计算反向采样似然，采用拟似然法，通过匹配条件均值与协方差的高斯分布近似每个反向转移密度。通过最大化拟似然估计扩散生成的得分函数与海森矩阵函数，确保任意两个时间点间前两阶转移矩的一致匹配。引入随机采样器以利用估计的得分与海森信息进行计算。我们证明了拟极大似然估计的一致性，并为所提采样器提供非渐近收敛保证，量化了由得分/海森估计、维度及扩散步数引起的近似误差率。实证与仿真评估验证了所提似然匹配方法的有效性及其理论结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work addresses the challenge of training diffusion models by proposing a Likelihood Matching approach that establishes an equivalence between the target data distribution&#x27;s likelihood and the likelihood along the reverse diffusion sample path. The method efficiently computes a reverse sample likelihood by approximating each reverse transition density with a Gaussian distribution whose conditional mean and covariance are matched, leading to the estimation of the score and Hessian functions via quasi-maximum likelihood estimation to ensure consistent matching of the first two transitional moments. Theoretical analysis confirms the consistency of the estimation and provides non-asymptotic convergence guarantees for a proposed stochastic sampler that leverages both score and Hessian information, quantifying errors from estimation, dimensionality, and diffusion steps. Empirical evaluations demonstrate the method&#x27;s effectiveness and validate the theoretical findings.</div>
<div class="mono" style="margin-top:8px">本研究针对扩散模型训练中的挑战，提出了一种似然匹配方法，建立了目标数据分布的似然与反向扩散采样路径上的似然之间的等价关系。该方法通过用匹配条件均值和协方差的高斯拟似然来近似每个反向转移密度，从而高效计算反向样本似然，并通过最大化该拟似然来估计得分函数和海森矩阵函数，以确保每两个时间点之间前两个转移矩的一致匹配。研究引入了一个利用估计的得分和海森信息的随机采样器。理论分析证实了拟最大似然估计的一致性，并为该采样器提供了非渐近收敛保证，量化了由得分/海森估计、维度和扩散步数引起的近似误差。实证评估证明了该方法的有效性并验证了理论结果。</div>
</details>
</div>
<div class="card">
<div class="title">Delayed Assignments in Online Non-Centroid Clustering with Stochastic Arrivals</div>
<div class="meta-line">Authors: Saar Cohen</div>
<div class="meta-line">First: 2026-01-22T16:42:05+00:00 · Latest: 2026-01-22T16:42:05+00:00</div>
<div class="meta-line">Comments: To Appear in the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16091v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16091v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clustering is a fundamental problem, aiming to partition a set of elements, like agents or data points, into clusters such that elements in the same cluster are closer to each other than to those in other clusters. In this paper, we present a new framework for studying online non-centroid clustering with delays, where elements, that arrive one at a time as points in a finite metric space, should be assigned to clusters, but assignments need not be immediate. Specifically, upon arrival, each point&#x27;s location is revealed, and an online algorithm has to irrevocably assign it to an existing cluster or create a new one containing, at this moment, only this point. However, we allow decisions to be postponed at a delay cost, instead of following the more common assumption of immediate decisions upon arrival. This poses a critical challenge: the goal is to minimize both the total distance costs between points in each cluster and the overall delay costs incurred by postponing assignments. In the classic worst-case arrival model, where points arrive in an arbitrary order, no algorithm has a competitive ratio better than sublogarithmic in the number of points. To overcome this strong impossibility, we focus on a stochastic arrival model, where points&#x27; locations are drawn independently across time from an unknown and fixed probability distribution over the finite metric space. We offer hope for beyond worst-case adversaries: we devise an algorithm that is constant competitive in the sense that, as the number of points grows, the ratio between the expected overall costs of the output clustering and an optimal offline clustering is bounded by a constant.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机到达在线非质心聚类中的延迟分配</div>
<div class="mono" style="margin-top:8px">聚类是一个基础性问题，旨在将一组元素（如智能体或数据点）划分为若干簇，使得同一簇内的元素彼此间距离小于与其他簇元素间的距离。本文提出一种研究带延迟的在线非质心聚类的新框架：元素作为有限度量空间中的点逐个到达，需被分配至簇中，但分配不必立即执行。具体而言，每个点到达时其位置被揭示，在线算法必须不可撤销地将其分配至现有簇，或创建仅包含该点的新簇。但我们允许以延迟成本为代价推迟决策，而非遵循常见的到达即决策假设。这带来关键挑战：目标是最小化各簇内点间总距离成本与延迟分配产生的总延迟成本。在经典最坏情况到达模型（点以任意顺序到达）中，算法的竞争比无法优于点数的亚对数级别。为突破这一强不可能性，我们聚焦于随机到达模型——点的位置随时间推移从有限度量空间的未知固定概率分布中独立抽取。我们为超越最坏情况对抗性假设提供了希望：设计出一种常数竞争算法，即随着点数增长，输出聚类与最优离线聚类的期望总成本之比受常数约束。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of online non-centroid clustering where immediate assignment of arriving points is not required, allowing for delayed decisions at a cost, which aims to balance clustering quality with delay penalties. The method introduces a stochastic arrival model where points are drawn independently from an unknown distribution over a finite metric space, and it devises an algorithm that makes irrevocable assignment decisions, permitting postponements. Key experimental findings show that, in contrast to the poor competitive ratios under worst-case arrivals, the algorithm achieves a constant competitive ratio, meaning the expected total cost of its clustering is bounded by a constant factor of the optimal offline cost as the number of points increases.</div>
<div class="mono" style="margin-top:8px">本研究针对在线非质心聚类中无需立即分配到达点、允许以成本为代价延迟决策的挑战，旨在平衡聚类质量与延迟惩罚。方法引入了随机到达模型，其中点从有限度量空间上的未知分布中独立抽取，并设计了一种算法，在可能延迟后做出不可撤销的分配决策，以最小化簇内距离成本和延迟成本的总和。关键实验结果表明，与最坏情况到达下的较差竞争比相比，该算法实现了常数竞争比，即随着点数增加，其聚类期望成本被限制在最优离线成本的常数倍内。</div>
</details>
</div>
<div class="card">
<div class="title">FedIA: Towards Domain-Robust Aggregation in Federated Graph Learning</div>
<div class="meta-line">Authors: Zhanting Zhou, KaHou Tam, Yiding Feng, Ziqiang Zheng, Zeyu Ma, Yang Yang</div>
<div class="meta-line">First: 2025-09-17T13:04:11+00:00 · Latest: 2026-01-22T16:28:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18171v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.18171v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Graph Learning (FGL) enables a central server to coordinate model training across distributed clients without local graph data being shared. However, FGL significantly suffers from cross-silo domain shifts, where each &quot;silo&quot; (domain) contains a limited number of clients with distinct graph topologies. These heterogeneities induce divergent optimization trajectories, ultimately leading to global model divergence. In this work, we reveal a severe architectural pathology termed Structural Orthogonality: the topology-dependent message passing mechanism forces gradients from different domains to target disjoint coordinates in the parameter space. Through a controlled comparison between backbones, we statistically prove that GNN updates are near-perpendicular across domains (with projection ratios $\to$ 0). Consequently, naive averaging leads to Consensus Collapse, a phenomenon where sparse, informative structural signals from individual domains are diluted by the near-zero updates of others. This forces the global model into a &quot;sub-optimal&quot; state that fails to represent domain-specific structural patterns, resulting in poor generalization. To address this, we propose FedIA, a lightweight server-side framework designed to reconcile update conflicts without auxiliary communication. FedIA operates in two stages: (1) Global Importance Masking (GIM) identifies a shared parameter subspace to filter out domain-specific structural noise and prevent signal dilution; (2) Confidence-Aware Momentum Weighting (CAM) dynamically re-weights client contributions based on gradient reliability to amplify stable optimization signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FedIA：迈向联邦图学习中的领域鲁棒性聚合</div>
<div class="mono" style="margin-top:8px">联邦图学习（FGL）使中央服务器能够协调分布式客户端间的模型训练，而无需共享本地图数据。然而，FGL严重受跨孤岛领域偏移的影响，每个“孤岛”（领域）包含数量有限且图拓扑结构各异的客户端。这种异质性导致优化轨迹发散，最终引发全局模型分化。本文揭示了一种严重的架构病理现象——结构正交性：依赖拓扑的消息传递机制迫使来自不同领域的梯度在参数空间中指向互不相交的坐标。通过对骨干网络的受控比较，我们统计证明图神经网络更新在跨领域间近乎正交（投影比趋于0）。因此，简单平均会导致共识崩溃，即来自单个领域的稀疏但信息丰富的结构信号被其他领域的近零更新稀释，迫使全局模型陷入无法表征领域特定结构模式的“次优”状态，导致泛化能力下降。为解决此问题，我们提出FedIA——一种轻量级服务器端框架，旨在无需额外通信的情况下协调更新冲突。FedIA分两阶段运行：（1）全局重要性掩码识别共享参数子空间，以过滤领域特定结构噪声并防止信号稀释；（2）置信感知动量加权基于梯度可靠性动态调整客户端贡献权重，以增强稳定的优化信号。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Federated Graph Learning (FGL) suffers from performance degradation due to cross-silo domain shifts, where distinct local graph topologies cause divergent client optimization trajectories and lead to global model divergence. The work diagnoses a core issue termed Structural Orthogonality, where gradients from different domains target nearly perpendicular directions in parameter space, causing naive aggregation to dilute informative signals in a Consensus Collapse. To reconcile these update conflicts without extra communication, the proposed FedIA framework employs a two-stage server-side method: Global Importance Masking (GIM) filters domain-specific structural noise by identifying a shared parameter subspace, and Confidence-Aware Momentum Weighting (CAM) dynamically re-weights client updates based on gradient reliability to amplify stable signals.</div>
<div class="mono" style="margin-top:8px">本研究针对联邦图学习中的跨领域偏移问题，即不同客户端间图拓扑结构的差异导致优化轨迹发散并损害全局模型性能。作者首先揭示了一种称为结构正交性的关键病理现象，通过统计证明不同领域的图神经网络梯度更新近乎垂直，导致朴素平均聚合时产生共识崩溃。为解决此问题，他们提出了FedIA框架，该框架在服务器端采用全局重要性掩码来过滤领域特定的结构噪声，并利用置信感知动量加权根据梯度可靠性动态调整客户端更新的权重。实验结果表明，FedIA能有效调和更新冲突，在不增加通信开销的情况下保留领域特定的结构模式，从而提升了模型的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Probably Approximately Correct Maximum A Posteriori Inference</div>
<div class="meta-line">Authors: Matthew Shorvon, Frederik Mallmann-Trenn, David S. Watson</div>
<div class="meta-line">First: 2026-01-22T16:28:01+00:00 · Latest: 2026-01-22T16:28:01+00:00</div>
<div class="meta-line">Comments: 7 pages main text, 16 total, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computing the conditional mode of a distribution, better known as the $\mathit{maximum\ a\ posteriori}$ (MAP) assignment, is a fundamental task in probabilistic inference. However, MAP estimation is generally intractable, and remains hard even under many common structural constraints and approximation schemes. We introduce $\mathit{probably\ approximately\ correct}$ (PAC) algorithms for MAP inference that provide provably optimal solutions under variable and fixed computational budgets. We characterize tractability conditions for PAC-MAP using information theoretic measures that can be estimated from finite samples. Our PAC-MAP solvers are efficiently implemented using probabilistic circuits with appropriate architectures. The randomization strategies we develop can be used either as standalone MAP inference techniques or to improve on popular heuristics, fortifying their solutions with rigorous guarantees. Experiments confirm the benefits of our method in a range of benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>概率近似正确最大后验推断</div>
<div class="mono" style="margin-top:8px">计算分布的条件众数，即通常所说的$\mathit{最大后验}$（MAP）赋值，是概率推断中的基础任务。然而，MAP估计通常难以精确求解，即使在许多常见的结构约束和近似方案下仍然困难。我们提出了用于MAP推断的$\mathit{概率近似正确}$（PAC）算法，这些算法在可变和固定的计算预算下能提供可证明的最优解。我们利用可从有限样本中估计的信息论度量，刻画了PAC-MAP的可处理性条件。我们的PAC-MAP求解器通过具有适当架构的概率电路高效实现。所开发的随机化策略既可作为独立的MAP推断技术使用，也可用于改进常用启发式方法，为其解提供严格的理论保证。实验在一系列基准测试中验证了本方法的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the intractability of maximum a posteriori (MAP) inference, a core problem in probabilistic reasoning that remains difficult even with common approximations. The method introduces probably approximately correct (PAC) algorithms for MAP inference, which guarantee optimal solutions within variable or fixed computational budgets, leveraging tractability conditions characterized by information-theoretic measures estimated from samples. These PAC-MAP solvers are efficiently implemented using probabilistic circuits with suitable architectures, and their randomization strategies can enhance existing heuristics with rigorous guarantees. Experimental results across various benchmarks demonstrate the effectiveness of the approach.</div>
<div class="mono" style="margin-top:8px">该研究针对最大后验概率推断这一概率推理核心问题的难解性展开，即使在常见近似方案下该问题依然困难。方法引入了近似正确概率的最大后验概率推断算法，在定义的计算预算内保证最优解，利用从数据估计的信息论度量来刻画可处理性条件，并通过概率电路高效实现。在多个基准测试中的实验结果表明，所提出的近似正确概率最大后验概率求解器能有效提供严格保证，并可改进现有启发式方法。</div>
</details>
</div>
<div class="card">
<div class="title">Explainable AI to Improve Machine Learning Reliability for Industrial Cyber-Physical Systems</div>
<div class="meta-line">Authors: Annemarie Jutte, Uraz Odyurt</div>
<div class="meta-line">First: 2026-01-22T16:18:22+00:00 · Latest: 2026-01-22T16:18:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16074v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16074v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial Cyber-Physical Systems (CPS) are sensitive infrastructure from both safety and economics perspectives, making their reliability critically important. Machine Learning (ML), specifically deep learning, is increasingly integrated in industrial CPS, but the inherent complexity of ML models results in non-transparent operation. Rigorous evaluation is needed to prevent models from exhibiting unexpected behaviour on future, unseen data. Explainable AI (XAI) can be used to uncover model reasoning, allowing a more extensive analysis of behaviour. We apply XAI to to improve predictive performance of ML models intended for industrial CPS. We analyse the effects of components from time-series data decomposition on model predictions using SHAP values. Through this method, we observe evidence on the lack of sufficient contextual information during model training. By increasing the window size of data instances, informed by the XAI findings, we are able to improve model performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可解释人工智能提升工业信息物理系统机器学习可靠性</div>
<div class="mono" style="margin-top:8px">工业信息物理系统（CPS）在安全与经济层面均属敏感基础设施，其可靠性至关重要。机器学习（尤其是深度学习）正日益融入工业CPS，但模型固有的复杂性导致其运行不透明。需通过严格评估防止模型在未来未知数据上出现异常行为。可解释人工智能（XAI）能揭示模型推理逻辑，支持更全面的行为分析。本研究应用XAI提升面向工业CPS的机器学习模型预测性能，利用SHAP值分析时序数据分解组件对模型预测的影响。该方法揭示了模型训练中上下文信息不足的问题，通过依据XAI发现扩大数据实例的窗口尺寸，最终提升了模型性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the critical need for reliability in Industrial Cyber-Physical Systems (CPS), where the integration of complex, non-transparent machine learning models poses a risk of unexpected behavior. The method employs Explainable AI (XAI), specifically SHAP value analysis, to dissect how components from time-series data decomposition influence model predictions, thereby revealing the model&#x27;s reasoning. Key experimental findings show that this XAI analysis uncovered insufficient contextual information during training; subsequently, increasing the data window size based on these insights led to measurable improvements in model performance.</div>
<div class="mono" style="margin-top:8px">工业信息物理系统（CPS）对可靠性要求极高，但集成的复杂、不透明的机器学习模型存在行为不可预测的风险。为此，本研究应用可解释人工智能（XAI），特别是利用SHAP值，分析了时间序列数据分解的组成部分如何影响模型预测。实验结果表明，模型在训练期间缺乏足够的上下文信息；因此，基于XAI的发现增加数据窗口大小，最终提升了模型的预测性能。</div>
</details>
</div>
<div class="card">
<div class="title">CLASP: An online learning algorithm for Convex Losses And Squared Penalties</div>
<div class="meta-line">Authors: Ricardo N. Ferreira, Cláudia Soares, João Xavier</div>
<div class="meta-line">First: 2026-01-22T16:13:52+00:00 · Latest: 2026-01-22T16:13:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16072v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16072v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study Constrained Online Convex Optimization (COCO), where a learner chooses actions iteratively, observes both unanticipated convex loss and convex constraint, and accumulates loss while incurring penalties for constraint violations. We introduce CLASP (Convex Losses And Squared Penalties), an algorithm that minimizes cumulative loss together with squared constraint violations. Our analysis departs from prior work by fully leveraging the firm non-expansiveness of convex projectors, a proof strategy not previously applied in this setting. For convex losses, CLASP achieves regret $O\left(T^{\max\{β,1-β\}}\right)$ and cumulative squared penalty $O\left(T^{1-β}\right)$ for any $β\in (0,1)$. Most importantly, for strongly convex problems, CLASP provides the first logarithmic guarantees on both regret and cumulative squared penalty. In the strongly convex case, the regret is upper bounded by $O( \log T )$ and the cumulative squared penalty is also upper bounded by $O( \log T )$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLASP：面向凸损失与平方惩罚的在线学习算法</div>
<div class="mono" style="margin-top:8px">本研究探讨约束在线凸优化问题，其中学习者迭代选择行动，同时观测未预期的凸损失与凸约束，在累积损失的同时为违反约束承担惩罚。我们提出CLASP算法，该算法同步最小化累积损失与约束违反的平方惩罚。本分析区别于先前研究，其创新在于充分利用凸投影算子的严格非扩张性——这一证明策略此前未应用于该场景。对于凸损失问题，CLASP对任意β∈(0,1)可实现O(T^{max{β,1-β}})的遗憾界与O(T^{1-β})的累积平方惩罚界。最重要的是，对于强凸问题，CLASP首次在遗憾界与累积平方惩罚界上同时实现对数级保证：强凸情形下遗憾上界为O(log T)，累积平方惩罚上界亦为O(log T)。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses Constrained Online Convex Optimization (COCO), where a learner must minimize cumulative loss while managing penalties for constraint violations in an iterative, unanticipated environment. The authors propose the CLASP algorithm, which minimizes cumulative loss alongside squared constraint violations by leveraging a novel analysis based on the firm non-expansiveness property of convex projectors, a proof technique not previously applied in this context. Experimental analysis shows that for convex losses, CLASP achieves regret scaling as O(T^{max{β,1-β}}) and cumulative squared penalty as O(T^{1-β}) for any β in (0,1); crucially, for strongly convex problems, it provides the first logarithmic guarantees, with both regret and cumulative squared penalty bounded by O(log T).</div>
<div class="mono" style="margin-top:8px">本研究针对约束在线凸优化问题，学习者在迭代过程中需最小化累积损失，同时处理约束违反带来的惩罚。方法提出了CLASP算法，通过利用凸投影算子的强非扩张性这一新证明策略，最小化累积损失和平方约束违反。实验结果表明，对于凸损失，CLASP实现了O(T^{max{β,1-β}})的遗憾和O(T^{1-β})的累积平方惩罚，其中β∈(0,1)；对于强凸问题，它首次提供了对数级保证，遗憾和累积平方惩罚均以O(log T)为上界。</div>
</details>
</div>
<div class="card">
<div class="title">On damage of interpolation to adversarial robustness in regression</div>
<div class="meta-line">Authors: Jingfu Peng, Yuhong Yang</div>
<div class="meta-line">First: 2026-01-22T16:09:00+00:00 · Latest: 2026-01-22T16:09:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16070v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16070v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) typically involve a large number of parameters and are trained to achieve zero or near-zero training error. Despite such interpolation, they often exhibit strong generalization performance on unseen data, a phenomenon that has motivated extensive theoretical investigations. Comforting results show that interpolation indeed may not affect the minimax rate of convergence under the squared error loss. In the mean time, DNNs are well known to be highly vulnerable to adversarial perturbations in future inputs. A natural question then arises: Can interpolation also escape from suboptimal performance under a future $X$-attack? In this paper, we investigate the adversarial robustness of interpolating estimators in a framework of nonparametric regression. A finding is that interpolating estimators must be suboptimal even under a subtle future $X$-attack, and achieving perfect fitting can substantially damage their robustness. An interesting phenomenon in the high interpolation regime, which we term the curse of simple size, is also revealed and discussed. Numerical experiments support our theoretical findings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论插值对回归中对抗鲁棒性的损害</div>
<div class="mono" style="margin-top:8px">深度神经网络通常包含大量参数，并被训练以实现零或接近零的训练误差。尽管存在这种插值现象，它们往往在未见数据上表现出强大的泛化性能，这一现象已引发广泛的理论研究。令人欣慰的结果表明，在平方误差损失下，插值确实可能不影响收敛的极小极大速率。同时，深度神经网络众所周知对未来输入中的对抗扰动高度脆弱。一个自然的问题随之产生：插值是否也能在未来$X$攻击下避免次优性能？本文在非参数回归框架下研究插值估计器的对抗鲁棒性。研究发现，即使在微妙的未来$X$攻击下，插值估计器也必然是次优的，实现完美拟合会显著损害其鲁棒性。我们还揭示并讨论了一个高插值区域的有趣现象，称之为“简单尺寸诅咒”。数值实验支持了我们的理论发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether interpolating estimators, which achieve zero or near-zero training error, can maintain optimal adversarial robustness in nonparametric regression under future X-attacks. The authors analyze the adversarial robustness of such estimators theoretically and reveal that interpolation inevitably leads to suboptimal performance even under subtle attacks, with perfect fitting substantially damaging robustness; they also identify a &#x27;curse of simple size&#x27; phenomenon in high interpolation regimes. Numerical experiments confirm these theoretical findings, demonstrating that interpolation compromises adversarial robustness.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在非参数回归中达到近乎零训练误差的插值估计器是否能在对抗性攻击下保持最优鲁棒性。研究通过理论分析插值估计器在细微未来X攻击框架下的性能，发现完美拟合本质上会损害鲁棒性并导致次优表现。关键结果表明，插值迫使估计器即使对微小对抗扰动也表现出脆弱性，同时研究揭示了高插值区域存在的“简单尺寸诅咒”现象，数值实验支持了这些理论发现。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
