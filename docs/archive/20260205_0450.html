<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-05 04:50</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260205_0450</div>
    <div class="row"><div class="card">
<div class="title">Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives</div>
<div class="meta-line">Authors: Owen Dong, Lily Gao, Manish Kota, Bennett A. Landmana, Jelena Bekvalac, Gaynor Western, Katherine D. Van Schaik</div>
<div class="meta-line">First: 2026-02-03T17:14:23+00:00 · Latest: 2026-02-03T17:14:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03750v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03750v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>零样本大型视觉语言模型提示在古放射学X射线档案中实现自动骨骼识别</div>
<div class="mono" style="margin-top:8px">古放射学利用现代成像技术研究考古与人类遗骸，为千年尺度的人类健康模式提供了新视角。然而，实地考察中收集的X光片存在异质性：骨骼常脱离关节、摆放位置随意、左右标记常缺失，且死亡年龄、骨骼年代、性别及成像设备等因素带来高度变异性，使得内容导航（如识别特定投影视角的图像子集）耗时费力，成为专家分析流程的瓶颈。本文提出一种零样本提示策略，利用先进的大型视觉语言模型（LVLM）自动识别图像中的主要骨骼、投影视角和左右侧。该流程将原始DICOM文件转换为骨骼窗PNG图像，通过精心设计的提示提交至LVLM，接收结构化JSON输出并提取整理至电子表格以待验证。经专家委员会认证的古放射学家对100张随机样本的评审显示，系统在主要骨骼识别准确率达92%、投影视角80%、左右侧100%，并对模糊案例标注低或中置信度标志。结果表明，LVLM能显著加速大型古放射学数据集的编码词开发，为未来人类学研究流程实现高效内容导航。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Paleoradiology X-ray archives are difficult to navigate due to heterogeneous images with disarticulated bones, ad hoc positioning, and high variability from biological and imaging factors, creating a bottleneck for expert analysis. To address this, the authors propose a zero-shot prompting strategy using a state-of-the-art Large Vision Language Model (LVLM); their pipeline converts DICOM files to bone-windowed PNGs, processes them with an engineered prompt, and extracts structured JSON outputs for validation. On a sample of 100 expert-reviewed images, the system achieved 92% accuracy for main bone identification, 80% for projection view, and 100% for laterality, with confidence flags for ambiguous cases, demonstrating its potential to accelerate dataset codeword development and streamline anthropology workflows.</div>
<div class="mono" style="margin-top:8px">古放射学X射线档案因骨骼散乱、摆放随意和缺乏侧位标记而高度异质，导致人工内容导航和分拣效率低下。为此，研究者提出一种零样本提示策略，利用先进的大型视觉语言模型（LVLM），将原始DICOM文件转换为骨窗PNG图像，并通过精心设计的提示获取结构化JSON输出，以自动识别主要骨骼、投照视角和侧位。在专家对100张随机图像的验证中，系统在主骨识别上达到92%的准确率，投照视角为80%，侧位为100%，并对模糊病例设置了低或中置信度标志，这表明LVLM能显著加速大型古放射学数据集的编码词开发，优化未来人类学研究的工作流程。</div>
</details>
</div>
<div class="card">
<div class="title">Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment</div>
<div class="meta-line">Authors: Johny J. Lopez, Md Meftahul Ferdaus, Mahdi Abdelguerfi</div>
<div class="meta-line">First: 2026-02-03T17:03:46+00:00 · Latest: 2026-02-03T17:03:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03742v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03742v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向地下基础设施评估的边缘优化视觉语言模型</div>
<div class="mono" style="margin-top:8px">地下基础设施（如污水管道和涵洞系统）的自主检测对公共安全和城市可持续性至关重要。尽管配备视觉传感器的机器人平台能高效检测结构缺陷，但从检测结果自动生成人类可读的摘要仍是一大挑战，尤其在资源受限的边缘设备上。本文提出一种新颖的两阶段端到端地下缺陷摘要生成流程，将轻量级RAPID-SCAN分割模型与部署在边缘计算平台的微调视觉语言模型（VLM）相结合。第一阶段采用RAPID-SCAN（基于紧凑自适应网络的资源感知管道检测与缺陷分割模型），仅用0.64M参数即实现0.834的F1分数，高效完成缺陷分割。第二阶段使用微调的Phi-3.5 VLM，从分割输出中生成简洁的领域特定自然语言摘要。我们构建了包含人工验证描述的检测图像数据集用于VLM微调与评估。为实现实时性能，采用硬件特定优化的训练后量化技术，在保持摘要质量的同时显著降低模型规模与推理延迟。我们在移动机器人平台上部署并评估完整流程，验证了其在真实检测场景中的有效性。结果表明，边缘可部署的集成AI系统能够弥合自动缺陷检测与基础设施维护可行洞察之间的鸿沟，为更具可扩展性和自主性的检测解决方案铺平道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating human-readable summaries from visual defect detections in underground infrastructure inspection, which is crucial for public safety but difficult to perform on resource-constrained edge devices. The method introduces a two-stage pipeline: first, a lightweight segmentation model called RAPID-SCAN performs efficient defect segmentation, achieving a 0.834 F1-score with only 0.64M parameters; second, a fine-tuned Phi-3.5 Vision-Language Model generates concise, domain-specific natural language summaries from the segmentation outputs, supported by a curated dataset for training and evaluation. Experimental results demonstrate that through post-training quantization and hardware-specific optimization, the system achieves significant reductions in model size and inference latency without compromising summarization quality, and its deployment on a mobile robotic platform validates effectiveness in real-world inspection scenarios, bridging automated detection with actionable maintenance insights.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决地下基础设施检测中，从视觉缺陷检测结果生成人类可读摘要的挑战，特别是在资源受限的边缘设备上。方法提出了一个两阶段流程：首先，使用轻量级分割模型（RAPID-SCAN，参数量0.64M）进行缺陷分割，取得了0.834的F1分数；其次，利用基于定制数据集微调的Phi-3.5视觉语言模型，从分割输出生成自然语言摘要。在移动机器人平台上的实验部署表明，训练后量化技术实现了实时性能，显著降低了模型大小和推理延迟，同时保持了摘要质量，证明了该系统为维护提供可操作见解的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">RegionReasoner: Region-Grounded Multi-Round Visual Reasoning</div>
<div class="meta-line">Authors: Wenfang Sun, Hao Chen, Yingjun Du, Yefeng Zheng, Cees G. M. Snoek</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T16:52:16+00:00 · Latest: 2026-02-03T16:52:16+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03733v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RegionReasoner：基于区域的多轮视觉推理框架</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型在视觉推理领域取得了显著进展，但现有系统多依赖单步或纯文本推理，限制了其在多轮视觉语境中迭代优化理解的能力。为突破此局限，我们提出了一个涵盖检测与分割任务训练集与测试集的多轮视觉推理基准，支持迭代推理场景下的系统评估。我们进一步提出RegionReasoner——一种强化学习框架，通过要求每个推理轨迹显式引用对应的参考边界框来实现具身推理，同时通过全局-局部一致性奖励保持语义连贯性。该奖励机制从全局场景描述和区域级描述中提取关键物体与名词，使其与推理轨迹对齐，确保跨推理步骤的一致性。RegionReasoner通过结合定位保真度与全局-局部语义对齐的结构化奖励进行优化。在检测与分割任务上的实验表明，RegionReasoner-7B模型与我们新提出的基准RegionDial-Bench共同显著提升了多轮推理准确率、空间定位精度和全局-局部一致性，为该新兴研究方向建立了坚实基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of existing large vision-language models that rely on single-step or text-only reasoning, which hampers iterative refinement across multiple visual contexts. To overcome this, the authors introduce a new multi-round visual reasoning benchmark and propose RegionReasoner, a reinforcement learning framework that grounds each reasoning step by explicitly citing reference bounding boxes and enforces semantic coherence through a global-local consistency reward aligning objects and nouns from scene and region captions with the reasoning trace. Experimental results on detection and segmentation tasks demonstrate that RegionReasoner-7B, evaluated with the new RegionDial-Bench, significantly improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency.</div>
<div class="mono" style="margin-top:8px">针对现有视觉语言模型依赖单步或纯文本推理、缺乏跨视觉上下文迭代精炼的局限，本文引入了一个新的多轮视觉推理基准，并提出了RegionReasoner强化学习框架。该方法通过要求每个推理轨迹显式引用参考边界框来强制实现接地推理，并通过全局-局部一致性奖励来保持语义连贯性，该奖励从场景和区域描述中提取关键对象和名词，并与推理轨迹对齐。在检测和分割任务上的实验表明，RegionReasoner-7B结合新引入的RegionDial-Bench基准，在多轮推理准确性、空间接地精度和全局-局部一致性方面均有显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Bottlenecks in Safety Fine-Tuning of Vision Language Models</div>
<div class="meta-line">Authors: Yi Ding, Lijun Li, Bing Cao, Jing Shao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-30T17:59:45+00:00 · Latest: 2026-02-03T16:20:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.18533v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.18533v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) have achieved remarkable performance across a wide range of tasks. However, their deployment in safety-critical domains poses significant challenges. Existing safety fine-tuning methods, which focus on textual or multimodal content, fall short in addressing challenging cases or disrupt the balance between helpfulness and harmlessness. Our evaluation highlights a safety reasoning gap: these methods lack safety visual reasoning ability, leading to such bottlenecks. To address this limitation and enhance both visual perception and reasoning in safety-critical contexts, we propose a novel dataset that integrates multi-image inputs with safety Chain-of-Thought (CoT) labels as fine-grained reasoning logic to improve model performance. Specifically, we introduce the Multi-Image Safety (MIS) dataset, an instruction-following dataset tailored for multi-image safety scenarios, consisting of training and test splits. Our experiments demonstrate that fine-tuning InternVL2.5-8B with MIS significantly outperforms both powerful open-source models and API-based models in challenging multi-image tasks requiring safety-related visual reasoning. This approach not only delivers exceptional safety performance but also preserves general capabilities without any trade-offs. Specifically, fine-tuning with MIS increases average accuracy by 0.83% across five general benchmarks and reduces the Attack Success Rate (ASR) on multiple safety benchmarks by a large margin.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反思视觉语言模型安全微调中的瓶颈问题</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）已在广泛任务中取得显著性能，但其在安全关键领域的部署仍面临重大挑战。现有专注于文本或多模态内容的安全微调方法难以应对复杂案例，或破坏了有用性与无害性之间的平衡。我们的评估揭示了一个安全推理缺口：这些方法缺乏安全视觉推理能力，从而形成瓶颈。为弥补这一局限并增强安全关键场景下的视觉感知与推理能力，我们提出一种新颖数据集，通过整合多图像输入与安全思维链（CoT）标签作为细粒度推理逻辑来提升模型性能。具体而言，我们构建了多图像安全（MIS）数据集——一个专为多图像安全场景设计的指令遵循数据集，包含训练集和测试集。实验表明，使用MIS对InternVL2.5-8B进行微调后，在需要安全相关视觉推理的复杂多图像任务中，其性能显著优于主流开源模型及基于API的模型。该方法不仅实现了卓越的安全性能，同时完全保持了模型的通用能力。具体而言，MIS微调使模型在五项通用基准上的平均准确率提升0.83%，并在多个安全基准上大幅降低了攻击成功率（ASR）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the safety reasoning gap in Vision-Language Models (VLMs), where existing safety fine-tuning methods fail on challenging cases and disrupt the helpfulness-harmlessness balance due to a lack of safety visual reasoning ability. To enhance visual perception and reasoning in safety-critical contexts, the authors propose the Multi-Image Safety (MIS) dataset, which integrates multi-image inputs with safety Chain-of-Thought labels to provide fine-grained reasoning logic for model fine-tuning. Experimental results show that fine-tuning InternVL2.5-8B with MIS significantly outperforms other models in multi-image safety tasks, reduces the Attack Success Rate on safety benchmarks, and improves average accuracy on general benchmarks by 0.83% without trade-offs.</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型在安全关键领域的部署受到现有安全微调方法的限制，这些方法难以处理复杂案例并破坏了有用性与无害性之间的平衡，主要源于视觉感知中的安全推理差距。为解决这一瓶颈，作者提出了多图像安全数据集，该数据集整合了多图像输入和安全思维链标签，以提供细粒度推理逻辑来增强视觉感知和推理能力。实验结果表明，使用该数据集对InternVL2.5-8B进行微调后，在多图像安全任务中显著优于其他模型，降低了安全基准上的攻击成功率，并在不牺牲性能的情况下将通用基准的平均准确率提高了0.83%。</div>
</details>
</div>
<div class="card">
<div class="title">Ground-R1: Incentivizing Grounded Visual Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Meng Cao, Haoze Zhao, Can Zhang, Xiaojun Chang, Ian Reid, Xiaodan Liang</div>
<div class="meta-line">First: 2025-05-26T17:51:47+00:00 · Latest: 2026-02-03T16:15:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20272v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.20272v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have become powerful general-purpose assistants, yet their predictions often lack reliability and interpretability due to insufficient grounding in visual evidence. The emerging thinking-with-images paradigm seeks to address this issue by explicitly anchoring reasoning to image regions. However, we empirically find that most existing methods suffer from a systematic scale-driven bias in optimization, where training rewards are dominated by large visual regions, suppressing learning from small but semantically critical evidence and leading to spurious grounding at inference time. To address this limitation, we propose Ground-R1, a de-biased thinking-with-images framework trained via a novel Scale Relative Policy Optimization (SRPO) objective that replaces standard GRPO. Specifically, our SRPO recalibrates reward learning across evidence regions of different sizes through scale-aware binning and intra-/inter-bin comparisons, enabling balanced credit assignment during training. Experimental results on general LVLM, high-resolution, and visual grounding benchmarks validate the effectiveness of Ground-R1 and show that SRPO yields consistent gains over standard GRPO in both response accuracy and evidence grounding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Ground-R1：通过强化学习激励基于视觉证据的推理</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型已成为强大的通用助手，但其预测常因缺乏对视觉证据的充分锚定而可靠性不足、可解释性差。新兴的“以图思考”范式试图通过将推理显式关联至图像区域来解决此问题。然而，我们实证发现现有方法普遍存在系统性尺度驱动优化偏差：训练奖励被大尺寸视觉区域主导，抑制了从小型但语义关键证据中学习的能力，导致推理时产生虚假锚定。为突破此局限，我们提出Ground-R1——一种基于新型尺度相对策略优化目标的去偏差“以图思考”框架，该目标取代了标准GRPO。具体而言，SRPO通过尺度感知分箱及箱内/箱间比较，重新校准跨不同尺寸证据区域的奖励学习，实现训练期间均衡的贡献分配。在通用LVLM、高分辨率及视觉定位基准上的实验结果验证了Ground-R1的有效性，并表明SRPO在回答准确性与证据锚定方面均较标准GRPO获得持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the issue of insufficient visual grounding in Large Vision-Language Models (LVLMs), which leads to unreliable and uninterpretable predictions. To counteract a systematic scale-driven bias in existing methods that favors large image regions over small but critical evidence, the authors propose Ground-R1, a framework trained with a novel Scale Relative Policy Optimization (SRPO) objective. Experiments on general LVLM, high-resolution, and visual grounding benchmarks demonstrate that SRPO consistently outperforms standard GRPO, improving both response accuracy and the quality of evidence grounding.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型因视觉证据基础不足而导致预测不可靠和难以解释的问题，尽管“以图思考”范式旨在将推理锚定到图像区域。所提出的方法Ground-R1引入了一个去偏见的框架，通过新颖的尺度相对策略优化目标进行训练，该目标通过尺度感知的分箱和箱内/箱间比较，重新校准了不同大小证据区域的奖励学习，以缓解抑制从小型关键证据中学习的系统性尺度驱动偏差。在通用LVLM、高分辨率和视觉基础基准上的实验结果表明，SRPO在响应准确性和证据基础质量上均持续优于标准GRPO。</div>
</details>
</div>
<div class="card">
<div class="title">MVP-LAM: Learning Action-Centric Latent Action via Cross-Viewpoint Reconstruction</div>
<div class="meta-line">Authors: Jung Min Lee, Dohyeok Lee, Seokhun Ju, Taehyun Cho, Jin Woo Koo, Li Zhao, Sangwoo Hong, Jungwoo Lee</div>
<div class="meta-line">First: 2026-02-03T15:51:25+00:00 · Latest: 2026-02-03T15:51:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03668v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning \emph{latent actions} from diverse human videos enables scaling robot learning beyond embodiment-specific robot datasets, and these latent actions have recently been used as pseudo-action labels for vision-language-action (VLA) model pretraining. To make VLA pretraining effective, latent actions should contain information about the underlying agent&#x27;s actions despite the absence of ground-truth labels. We propose \textbf{M}ulti-\textbf{V}iew\textbf{P}oint \textbf{L}atent \textbf{A}ction \textbf{M}odel (\textbf{MVP-LAM}), which learns discrete latent actions that are highly informative about ground-truth actions from time-synchronized multi-view videos. MVP-LAM trains latent actions with a \emph{cross-viewpoint reconstruction} objective, so that a latent action inferred from one view must explain the future in another view, reducing reliance on viewpoint-specific cues. On Bridge V2, MVP-LAM produces more action-centric latent actions, achieving higher mutual information with ground-truth actions and improved action prediction, including under out-of-distribution evaluation. Finally, pretraining VLAs with MVP-LAM latent actions improves downstream manipulation performance on the SIMPLER and LIBERO-Long benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MVP-LAM：通过跨视角重构学习以动作为核心的潜在动作</div>
<div class="mono" style="margin-top:8px">从多样化的人类视频中学习潜在动作，能够使机器人学习超越特定具身数据集，这些潜在动作近期已被用作视觉-语言-动作模型预训练的伪动作标签。为使VLA预训练有效，潜在动作应包含底层智能体动作的信息，尽管缺乏真实标签。我们提出多视角潜在动作模型，该模型从时间同步的多视角视频中学习与真实动作高度相关的离散潜在动作。MVP-LAM通过跨视角重构目标训练潜在动作，使得从一个视角推断的潜在动作必须能解释另一视角的未来状态，从而减少对视角特定线索的依赖。在Bridge V2数据集上，MVP-LAM生成更以动作为核心的潜在动作，实现了与真实动作更高的互信息及改进的动作预测性能，包括在分布外评估中。最后，使用MVP-LAM潜在动作预训练VLA模型，提升了在SIMPLER和LIBERO-Long基准测试中的下游操作性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to learn latent actions from diverse human videos to scale robot learning beyond embodiment-specific datasets, addressing the need for action-centric pseudo-labels in vision-language-action (VLA) model pretraining. The method introduces MVP-LAM, which learns discrete latent actions from time-synchronized multi-view videos via a cross-viewpoint reconstruction objective, ensuring that latent actions inferred from one view can predict the future in another view to reduce viewpoint-specific biases. Experimental results on Bridge V2 show that MVP-LAM produces latent actions with higher mutual information with ground-truth actions and better action prediction, even under out-of-distribution conditions, and pretraining VLAs with these latent actions improves downstream manipulation performance on SIMPLER and LIBERO-Long benchmarks.</div>
<div class="mono" style="margin-top:8px">该研究旨在从多样化的人类视频中学习潜在动作，以扩展机器人学习至特定具身数据集之外，解决视觉-语言-动作（VLA）模型预训练中对动作中心伪标签的需求。方法MVP-LAM通过跨视角重建目标，从时间同步的多视角视频中学习离散潜在动作，即从一个视角推断的潜在动作必须能预测另一视角的未来状态，以减少对视角特定线索的依赖。在Bridge V2上的实验表明，MVP-LAM产生的潜在动作与真实动作具有更高的互信息，动作预测性能更优，包括在分布外评估中，且使用这些潜在动作预训练VLA模型提升了在SIMPLER和LIBERO-Long基准上的下游操作性能。</div>
</details>
</div>
<div class="card">
<div class="title">MM-SCALE: Grounded Multimodal Moral Reasoning via Scalar Judgment and Listwise Alignment</div>
<div class="meta-line">Authors: Eunkyu Park, Wesley Hanwen Deng, Cheyon Jin, Matheus Kunzler Maldaner, Jordan Wheeler, Jason I. Hong, Hong Shen, Adam Perer, Ken Holstein, Motahhare Eslami, Gunhee Kim</div>
<div class="meta-line">First: 2026-02-03T15:48:00+00:00 · Latest: 2026-02-03T15:48:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03665v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) continue to struggle to make morally salient judgments in multimodal and socially ambiguous contexts. Prior works typically rely on binary or pairwise supervision, which often fail to capture the continuous and pluralistic nature of human moral reasoning. We present MM-SCALE (Multimodal Moral Scale), a large-scale dataset for aligning VLMs with human moral preferences through 5-point scalar ratings and explicit modality grounding. Each image-scenario pair is annotated with moral acceptability scores and grounded reasoning labels by humans using an interface we tailored for data collection, enabling listwise preference optimization over ranked scenario sets. By moving from discrete to scalar supervision, our framework provides richer alignment signals and finer calibration of multimodal moral reasoning. Experiments show that VLMs fine-tuned on MM-SCALE achieve higher ranking fidelity and more stable safety calibration than those trained with binary signals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MM-SCALE：基于标量判断与列表对齐的多模态道德推理框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型在多模态与社会模糊情境下的道德判断仍面临挑战。现有研究多依赖二元或成对监督，难以捕捉人类道德推理的连续性与多元性。我们提出MM-SCALE（多模态道德标度）数据集，通过五点标量评分与显式模态锚定实现视觉语言模型与人类道德偏好的对齐。每个图像-场景对均通过定制化标注界面获取人类标注的道德可接受度分数与锚定推理标签，支持对排序场景集的列表偏好优化。该框架通过标量监督替代离散监督，提供更丰富的对齐信号与更精细的多模态道德推理校准。实验表明，基于MM-SCALE微调的视觉语言模型相比二元信号训练模型，在排序保真度与安全校准稳定性方面均有显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models often fail to make nuanced moral judgments in ambiguous multimodal contexts, as existing methods typically use binary or pairwise supervision that cannot capture the continuous and pluralistic nature of human moral reasoning. To address this, the authors introduce MM-SCALE, a dataset and framework that uses 5-point scalar ratings and explicit modality grounding for each image-scenario pair, enabling listwise preference optimization over ranked sets. Experimental results demonstrate that models fine-tuned with this scalar supervision achieve higher ranking fidelity and more stable safety calibration compared to those trained with binary signals.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在多模态和社交模糊情境中常难以做出细致的道德判断，因为先前方法依赖二元或成对监督，无法捕捉人类道德推理的连续性和多元性。为此，研究者提出了MM-SCALE，通过人工标注的5点标量评分和显式的模态基础构建数据集与框架，提供更丰富的对齐信号，支持对排序场景集的列表式偏好优化。实验表明，使用MM-SCALE微调的模型相比基于二元信号训练的模型，在排序保真度和安全校准稳定性方面表现更优。</div>
</details>
</div>
<div class="card">
<div class="title">KTV: Keyframes and Key Tokens Selection for Efficient Training-Free Video LLMs</div>
<div class="meta-line">Authors: Baiyang Song, Jun Peng, Yuxin Zhang, Guangyao Chen, Feidiao Yang, Jianyuan Guo</div>
<div class="meta-line">First: 2026-02-03T15:08:30+00:00 · Latest: 2026-02-03T15:08:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03615v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03615v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-free video understanding leverages the strong image comprehension capabilities of pre-trained vision language models (VLMs) by treating a video as a sequence of static frames, thus obviating the need for costly video-specific training. However, this paradigm often suffers from severe visual redundancy and high computational overhead, especially when processing long videos. Crucially, existing keyframe selection strategies, especially those based on CLIP similarity, are prone to biases and may inadvertently overlook critical frames, resulting in suboptimal video comprehension. To address these significant challenges, we propose \textbf{KTV}, a novel two-stage framework for efficient and effective training-free video understanding. In the first stage, KTV performs question-agnostic keyframe selection by clustering frame-level visual features, yielding a compact, diverse, and representative subset of frames that mitigates temporal redundancy. In the second stage, KTV applies key visual token selection, pruning redundant or less informative tokens from each selected keyframe based on token importance and redundancy, which significantly reduces the number of tokens fed into the LLM. Extensive experiments on the Multiple-Choice VideoQA task demonstrate that KTV outperforms state-of-the-art training-free baselines while using significantly fewer visual tokens, \emph{e.g.}, only 504 visual tokens for a 60-min video with 10800 frames, achieving $44.8\%$ accuracy on the MLVU-Test benchmark. In particular, KTV also exceeds several training-based approaches on certain benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KTV：面向高效免训练视频大语言模型的关键帧与关键令牌选择</div>
<div class="mono" style="margin-top:8px">免训练视频理解利用预训练视觉语言模型强大的图像理解能力，将视频视为静态帧序列，从而避免昂贵的视频专用训练。然而，该范式常受严重视觉冗余和高计算开销困扰，尤其在处理长视频时。关键的是，现有基于CLIP相似性等关键帧选择策略易产生偏差，可能无意忽略关键帧，导致视频理解效果欠佳。为应对这些挑战，本文提出KTV——一种高效免训练视频理解的两阶段框架：第一阶段通过聚类帧级视觉特征进行问题无关的关键帧选择，生成紧凑、多样且具代表性的帧子集以缓解时序冗余；第二阶段基于令牌重要性与冗余性，从各关键帧中剪枝冗余或低信息量视觉令牌，显著减少输入大语言模型的令牌数量。在多项选择视频问答任务上的实验表明，KTV在使用更少视觉令牌（例如对10800帧的60分钟视频仅需504个视觉令牌）的情况下，优于当前最优免训练基线方法，在MLVU-Test基准上达到44.8%准确率，且在某些基准上甚至超越部分需训练的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the inefficiency and potential bias of existing training-free video understanding methods that process all frames, this study introduces KTV, a two-stage framework that first selects keyframes via clustering of visual features to reduce redundancy and then prunes less informative visual tokens from each keyframe based on importance. Experimental results on Multiple-Choice VideoQA show that KTV outperforms state-of-the-art training-free baselines with far fewer tokens, achieving 44.8% accuracy on MLVU-Test using only 504 tokens for a 60-minute video, and even surpasses some training-based methods on certain benchmarks.</div>
<div class="mono" style="margin-top:8px">针对现有免训练视频理解方法存在的视觉冗余和关键帧选择偏差问题，本文提出了KTV框架，该框架采用两阶段策略：首先通过聚类帧级特征进行与问题无关的关键帧选择，然后基于重要性剪枝冗余视觉令牌。在多项选择视频问答任务上的实验表明，KTV在显著减少令牌使用量的情况下优于当前最先进的免训练基线，例如在60分钟视频中仅使用504个视觉令牌即在MLVU-Test基准上达到44.8%的准确率，甚至在某些基准上超越了部分基于训练的方法。</div>
</details>
</div>
<div class="card">
<div class="title">TIPS Over Tricks: Simple Prompts for Effective Zero-shot Anomaly Detection</div>
<div class="meta-line">Authors: Alireza Salehi, Ehsan Karami, Sepehr Noey, Sahand Noey, Makoto Yamada, Reshad Hosseini, Mohammad Sabokrou</div>
<div class="meta-line">Venue: ICASSP</div>
<div class="meta-line">First: 2026-02-03T14:48:11+00:00 · Latest: 2026-02-03T14:48:11+00:00</div>
<div class="meta-line">Comments: This is the extended version of the paper accepted in ICASSP&#x27;26, which will be publicly available in May. Authors&#x27; contributions may vary among the versions</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03594v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03594v1">PDF</a> · <a href="http://github.com/AlirezaSalehy/Tipsomaly">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Anomaly detection identifies departures from expected behavior in safety-critical settings. When target-domain normal data are unavailable, zero-shot anomaly detection (ZSAD) leverages vision-language models (VLMs). However, CLIP&#x27;s coarse image-text alignment limits both localization and detection due to (i) spatial misalignment and (ii) weak sensitivity to fine-grained anomalies; prior work compensates with complex auxiliary modules yet largely overlooks the choice of backbone. We revisit the backbone and use TIPS-a VLM trained with spatially aware objectives. While TIPS alleviates CLIP&#x27;s issues, it exposes a distributional gap between global and local features. We address this with decoupled prompts-fixed for image-level detection and learnable for pixel-level localization-and by injecting local evidence into the global score. Without CLIP-specific tricks, our TIPS-based pipeline improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, delivering strong generalization with a lean architecture. Code is available at github.com/AlirezaSalehy/Tipsomaly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>技巧之上：用于高效零样本异常检测的简洁提示</div>
<div class="mono" style="margin-top:8px">异常检测旨在识别安全关键场景中偏离预期行为的情况。当目标域正常数据不可用时，零样本异常检测（ZSAD）利用视觉语言模型（VLM）。然而，CLIP的粗粒度图文对齐因（i）空间错位和（ii）对细粒度异常敏感性不足，限制了定位与检测性能；先前研究通过复杂辅助模块补偿，却大多忽视了主干网络的选择。我们重新审视主干网络，采用具备空间感知训练目标的TIPS-VLM。虽然TIPS缓解了CLIP的问题，但揭示了全局与局部特征间的分布差异。我们通过解耦提示（固定用于图像级检测，可学习用于像素级定位）及向全局评分注入局部证据来解决此问题。无需CLIP专用技巧，基于TIPS的流程在七个工业数据集上提升图像级性能1.1-3.9%、像素级性能1.5-6.9%，以轻量架构实现强泛化能力。代码发布于github.com/AlirezaSalehy/Tipsomaly。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in zero-shot anomaly detection (ZSAD) using vision-language models (VLMs), where CLIP&#x27;s coarse image-text alignment leads to spatial misalignment and weak sensitivity to fine-grained anomalies. The method employs TIPS, a VLM trained with spatially aware objectives, and introduces decoupled prompts—fixed for image-level detection and learnable for pixel-level localization—along with injecting local evidence into the global score to bridge a distributional gap between global and local features. Experimental results show that this pipeline, without CLIP-specific tricks, improves image-level performance by 1.1-3.9% and pixel-level by 1.5-6.9% across seven industrial datasets, demonstrating strong generalization with a lean architecture.</div>
<div class="mono" style="margin-top:8px">本研究针对CLIP在零样本异常检测中的局限性，即空间错位和对细粒度异常敏感性不足的问题。方法采用具有空间感知训练能力的视觉语言模型TIPS，并引入解耦提示策略——固定提示用于图像级检测，可学习提示用于像素级定位——同时将局部证据注入全局评分。在七个工业数据集上的实验表明，图像级性能提升1.1-3.9%，像素级性能提升1.5-6.9%，以简洁架构实现了强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Generating a Paracosm for Training-Free Zero-Shot Composed Image Retrieval</div>
<div class="meta-line">Authors: Tong Wang, Yunhan Zhao, Shu Kong</div>
<div class="meta-line">First: 2026-01-31T16:42:55+00:00 · Latest: 2026-02-03T14:05:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00813v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.00813v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Composed Image Retrieval (CIR) is the task of retrieving a target image from a database using a multimodal query, which consists of a reference image and a modification text. The text specifies how to alter the reference image to form a ``mental image&#x27;&#x27;, based on which CIR should find the target image in the database. The fundamental challenge of CIR is that this ``mental image&#x27;&#x27; is not physically available and is only implicitly defined by the query. The contemporary literature pursues zero-shot methods and uses a Large Multimodal Model (LMM) to generate a textual description for a given multimodal query, and then employs a Vision-Language Model (VLM) for textual-visual matching to search the target image. In contrast, we address CIR from first principles by directly generating the ``mental image&#x27;&#x27; for more accurate matching. Particularly, we prompt an LMM to generate a ``mental image&#x27;&#x27; for a given multimodal query and propose to use this ``mental image&#x27;&#x27; to search for the target image. As the ``mental image&#x27;&#x27; has a synthetic-to-real domain gap with real images, we also generate a synthetic counterpart for each real image in the database to facilitate matching. In this sense, our method uses LMM to construct a ``paracosm&#x27;&#x27;, where it matches the multimodal query and database images. Hence, we call this method Paracosm. Notably, Paracosm is a training-free zero-shot CIR method. It significantly outperforms existing zero-shot methods on four challenging benchmarks, achieving state-of-the-art performance for zero-shot CIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成用于免训练零样本组合图像检索的拟真世界</div>
<div class="mono" style="margin-top:8px">组合图像检索（CIR）是通过多模态查询从数据库中检索目标图像的任务，该查询由参考图像和修改文本组成。文本指定如何修改参考图像以形成“心理图像”，CIR应基于此在数据库中查找目标图像。CIR的根本挑战在于该“心理图像”并非物理存在，仅由查询隐式定义。现有研究采用零样本方法，使用大型多模态模型（LMM）为给定多模态查询生成文本描述，再通过视觉语言模型（VLM）进行文本-视觉匹配以搜索目标图像。相比之下，我们从基本原理出发，直接生成“心理图像”以实现更精准匹配。具体而言，我们提示LMM为给定多模态查询生成“心理图像”，并提议使用该图像搜索目标图像。由于“心理图像”与真实图像存在合成到真实的领域差距，我们还为数据库中每张真实图像生成合成对应物以促进匹配。在此意义上，我们的方法利用LMM构建了一个“拟真世界”，在其中匹配多模态查询与数据库图像，故称该方法为Paracosm。值得注意的是，Paracosm是一种免训练的零样本CIR方法。在四个具有挑战性的基准测试中，它显著优于现有零样本方法，实现了零样本CIR的最先进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge in Composed Image Retrieval (CIR) where the target &#x27;mental image&#x27; is only implicitly defined by a reference image and modification text, making direct retrieval difficult. The proposed method, Paracosm, generates this mental image directly using a Large Multimodal Model (LMM) for query matching and creates synthetic counterparts for real database images to bridge the domain gap, all without any training. Experiments on four benchmarks show that this training-free, zero-shot approach significantly outperforms existing methods, achieving state-of-the-art performance.</div>
<div class="mono" style="margin-top:8px">本研究针对组合图像检索任务中目标“心理图像”仅由参考图像和修改文本隐式定义、难以直接检索的挑战。所提出的Paracosm方法使用大型多模态模型直接生成查询的心理图像，并为真实数据库图像创建合成对应物以弥合域间差距，从而实现无需训练、基于合成图像匹配的零样本检索。实验结果表明，该方法在四个基准测试上显著优于现有零样本方法，达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Logical Anomaly Classification via Constraint Decomposition and Instruction Fine-Tuning</div>
<div class="meta-line">Authors: Xufei Zhang, Xinjiao Zhou, Ziling Deng, Dongdong Geng, Jianxiong Wang</div>
<div class="meta-line">First: 2026-02-03T13:48:09+00:00 · Latest: 2026-02-03T13:48:09+00:00</div>
<div class="meta-line">Comments: 6 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03530v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03530v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Logical anomalies are violations of predefined constraints on object quantity, spatial layout, and compositional relationships in industrial images. While prior work largely treats anomaly detection as a binary decision, such formulations cannot indicate which logical rule is broken and therefore offer limited value for quality assurance. We introduce Logical Anomaly Classification (LAC), a task that unifies anomaly detection and fine-grained violation classification in a single inference step. To tackle LAC, we propose LogiCls, a vision-language framework that decomposes complex logical constraints into a sequence of verifiable subqueries. We further present a data-centric instruction synthesis pipeline that generates chain-of-thought (CoT) supervision for these subqueries, coupling precise grounding annotations with diverse image-text augmentations to adapt vision language models (VLMs) to logic-sensitive reasoning. Training is stabilized by a difficulty-aware resampling strategy that emphasizes challenging subqueries and long tail constraint types. Extensive experiments demonstrate that LogiCls delivers robust, interpretable, and accurate industrial logical anomaly classification, providing both the predicted violation categories and their evidence trails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于约束分解与指令微调的可解释逻辑异常分类</div>
<div class="mono" style="margin-top:8px">逻辑异常指工业图像中物体数量、空间布局与组合关系违反预设约束的现象。现有研究多将异常检测视为二元决策，此类方法无法指明具体违反的逻辑规则，对质量保障的价值有限。本文提出逻辑异常分类任务，在单次推理中统一实现异常检测与细粒度违规分类。为此，我们设计LogiCls视觉语言框架，将复杂逻辑约束分解为可验证的子查询序列，并构建以数据为中心的指令合成流程，为子查询生成思维链监督信号——通过结合精准定位标注与多样化图文增强技术，使视觉语言模型适应逻辑敏感推理。采用难度感知重采样策略强化训练，重点关注困难子查询与长尾约束类型。大量实验表明，LogiCls能提供兼具鲁棒性、可解释性与精确性的工业逻辑异常分类，同时输出违规类别及其证据链。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of binary anomaly detection in industrial images, which fails to specify which logical constraints are violated, thus hindering quality assurance. The proposed method, LogiCls, decomposes complex logical constraints into verifiable subqueries and employs a data-centric instruction synthesis pipeline to generate chain-of-thought supervision, enhanced by difficulty-aware resampling for training stability. Experiments show that LogiCls achieves robust and interpretable classification, accurately identifying violation categories with evidence trails.</div>
<div class="mono" style="margin-top:8px">该研究针对工业场景中二元异常检测的局限性，即无法指明具体违反的逻辑约束，从而影响质量保证。所提出的LogiCls方法将复杂逻辑约束分解为可验证的子查询，并采用以数据为中心的指令合成流程生成思维链监督，辅以难度感知的重采样策略以稳定训练。实验结果表明，LogiCls实现了鲁棒、可解释且准确的逻辑异常分类，能同时提供违规类别及其证据链。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Skeleton and Flesh: Efficient Multimodal Table Reasoning with Disentangled Alignment and Structure-aware Guidance</div>
<div class="meta-line">Authors: Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Youcheng Pan, Xiaoqiang Zhou, Min Zhang</div>
<div class="meta-line">First: 2026-02-03T13:08:31+00:00 · Latest: 2026-02-03T13:08:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03491v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03491v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning over table images remains challenging for Large Vision-Language Models (LVLMs) due to complex layouts and tightly coupled structure-content information. Existing solutions often depend on expensive supervised training, reinforcement learning, or external tools, limiting efficiency and scalability. This work addresses a key question: how to adapt LVLMs to table reasoning with minimal annotation and no external tools? Specifically, we first introduce DiSCo, a Disentangled Structure-Content alignment framework that explicitly separates structural abstraction from semantic grounding during multimodal alignment, efficiently adapting LVLMs to tables structures. Building on DiSCo, we further present Table-GLS, a Global-to-Local Structure-guided reasoning framework that performs table reasoning via structured exploration and evidence-grounded inference. Extensive experiments across diverse benchmarks demonstrate that our framework efficiently enhances LVLM&#x27;s table understanding and reasoning capabilities, particularly generalizing to unseen table structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦骨架与血肉：基于分离对齐与结构感知引导的高效多模态表格推理</div>
<div class="mono" style="margin-top:8px">由于复杂的布局和紧密耦合的结构-内容信息，大型视觉语言模型在表格图像推理方面仍面临挑战。现有方案常依赖昂贵的监督训练、强化学习或外部工具，限制了效率与可扩展性。本研究旨在解决一个关键问题：如何以最少标注且无需外部工具的方式使大型视觉语言模型适应表格推理？具体而言，我们首先提出DiSCo——一种分离式结构-内容对齐框架，在多模态对齐过程中显式分离结构抽象与语义基础，高效适配表格结构。基于DiSCo，我们进一步提出Table-GLS——一种全局到局部的结构引导推理框架，通过结构化探索和证据支撑的推理进行表格分析。跨多个基准的广泛实验表明，该框架能有效提升大型视觉语言模型的表格理解与推理能力，尤其对未见过的表格结构展现出良好的泛化性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling Large Vision-Language Models (LVLMs) to reason over table images, which is difficult due to complex layouts and intertwined structure-content information, without relying on expensive supervised training or external tools. The method introduces DiSCo, a Disentangled Structure-Content alignment framework that separates structural abstraction from semantic grounding during multimodal alignment, and builds upon it with Table-GLS, a Global-to-Local Structure-guided reasoning framework that performs reasoning through structured exploration and evidence-grounded inference. Experimental results across diverse benchmarks show that the framework efficiently enhances LVLMs&#x27; table understanding and reasoning, with strong generalization to unseen table structures.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型视觉语言模型在表格图像上进行推理的挑战，该挑战源于复杂的布局以及结构与内容的紧密耦合。所提出的方法引入了DiSCo框架，在多模态对齐过程中将结构抽象与语义基础解耦，并在此基础上构建了Table-GLS推理框架，通过结构化探索和基于证据的推理来指导模型。在多个基准测试上的广泛实验表明，该方法能有效提升模型对表格的理解和推理能力，特别是在未见过的表格结构上表现出良好的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Contextualized Visual Personalization in Vision-Language Models</div>
<div class="meta-line">Authors: Yeongtak Oh, Sangwon Yu, Junsung Park, Han Cheol Moon, Jisoo Mok, Sungroh Yoon</div>
<div class="meta-line">First: 2026-02-03T12:21:26+00:00 · Latest: 2026-02-03T12:21:26+00:00</div>
<div class="meta-line">Comments: Project Page: https://github.com/oyt9306/CoViP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03454v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03454v1">PDF</a> · <a href="https://github.com/oyt9306/CoViP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite recent progress in vision-language models (VLMs), existing approaches often fail to generate personalized responses based on the user&#x27;s specific experiences, as they lack the ability to associate visual inputs with a user&#x27;s accumulated visual-textual context. We newly formalize this challenge as contextualized visual personalization, which requires the visual recognition and textual retrieval of personalized visual experiences by VLMs when interpreting new images. To address this issue, we propose CoViP, a unified framework that treats personalized image captioning as a core task for contextualized visual personalization and improves this capability through reinforcement-learning-based post-training and caption-augmented generation. We further introduce diagnostic evaluations that explicitly rule out textual shortcut solutions and verify whether VLMs truly leverage visual context. Extensive experiments demonstrate that existing open-source and proprietary VLMs exhibit substantial limitations, while CoViP not only improves personalized image captioning but also yields holistic gains across downstream personalization tasks. These results highlight CoViP as a crucial stage for enabling robust and generalizable contextualized visual personalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中的情境化视觉个性化</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）近期取得进展，现有方法常因缺乏将视觉输入与用户累积的视觉-文本情境相关联的能力，难以基于用户具体经验生成个性化响应。我们首次将此挑战形式化为情境化视觉个性化，要求VLMs在解读新图像时，能对个性化视觉经验进行视觉识别与文本检索。为此提出CoViP统一框架，将个性化图像描述作为情境化视觉个性化的核心任务，并通过基于强化学习的后训练与描述增强生成提升该能力。我们进一步引入诊断性评估，明确排除文本捷径方案，验证VLMs是否真正利用视觉情境。大量实验表明，现有开源与专有VLMs存在显著局限，而CoViP不仅改进个性化图像描述，还在下游个性化任务中实现整体性能提升。这些成果标志着CoViP成为实现稳健、可泛化情境化视觉个性化的关键阶段。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inability of current vision-language models to generate personalized responses based on users&#x27; accumulated visual-textual experiences, a challenge formalized as contextualized visual personalization. The proposed method, CoViP, treats personalized image captioning as a core task and enhances it through reinforcement-learning-based post-training and caption-augmented generation, with diagnostic evaluations designed to prevent textual shortcuts and ensure genuine visual context use. Experimental results show that existing VLMs have significant limitations, while CoViP improves personalized captioning and delivers holistic gains across downstream personalization tasks, establishing it as a key advancement for robust and generalizable contextualized visual personalization.</div>
<div class="mono" style="margin-top:8px">该研究针对当前视觉语言模型无法基于用户累积的视觉-文本上下文生成个性化响应的问题，将其形式化为情境化视觉个性化的挑战。提出的CoViP框架将个性化图像描述作为核心任务，通过基于强化学习的后训练和描述增强生成来提升该能力，并设计了诊断性评估以防止文本捷径并确保模型真正利用视觉上下文。大量实验表明，现有开源和专有视觉语言模型存在显著局限，而CoViP不仅改善了个性化图像描述，还在下游个性化任务中带来了整体性能提升，这标志着CoViP是实现鲁棒且可泛化的情境化视觉个性化的关键进展。</div>
</details>
</div>
<div class="card">
<div class="title">V2P-Bench: Evaluating Video-Language Understanding with Visual Prompts for Better Human-Model Interaction</div>
<div class="meta-line">Authors: Yiming Zhao, Yu Zeng, Yukun Qi, YaoYang Liu, Xikun Bao, Lin Chen, Zehui Chen, Qing Miao, Chenxi Liu, Jie Zhao, Feng Zhao</div>
<div class="meta-line">First: 2025-03-22T11:30:46+00:00 · Latest: 2026-02-03T10:18:20+00:00</div>
<div class="meta-line">Comments: Project Page: https://vlm-reasoning.github.io/V2P-Bench/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.17736v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.17736v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vlm-reasoning.github.io/V2P-Bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have made significant strides in the field of video understanding in recent times. Nevertheless, existing video benchmarks predominantly rely on text prompts for evaluation, which often require complex referential language and diminish both the accuracy and efficiency of human model interaction in turn. To address this limitation, we propose V2P-Bench, a robust and comprehensive benchmark for evaluating the ability of LVLMs to understand Video Visual Prompts in human model interaction scenarios. V2P-Bench consists of 980 videos and 1172 well-structured high-quality QA pairs, each paired with manually annotated visual prompt frames. The benchmark spans three main tasks and twelve categories, thereby enabling fine-grained, instance-level evaluation. Through an in-depth analysis of current LVLMs, we identify several key findings: 1) Visual prompts are both more model-friendly and user-friendly in interactive scenarios than text prompts, leading to significantly improved model performance and enhanced user experience. 2) Models are reasonably capable of zero-shot understanding of visual prompts, but struggle with spatiotemporal understanding. Even o1 achieves only 71.8%, far below the human expert score of 88.3%, while most open-source models perform below 60%. 3) LVLMs exhibit pervasive Hack Phenomena in video question answering tasks, which become more pronounced as video length increases and frame sampling density decreases, thereby inflating performance scores artificially. We anticipate that V2P-Bench will not only shed light on these challenges but also serve as a foundational tool for advancing human model interaction and improving the evaluation of video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V2P-Bench：通过视觉提示评估视频-语言理解能力以优化人机交互</div>
<div class="mono" style="margin-top:8px">近年来，大规模视觉语言模型在视频理解领域取得显著进展。然而，现有视频基准主要依赖文本提示进行评估，这通常需要复杂的指代语言，进而降低了人机交互的准确性和效率。为突破此局限，我们提出V2P-Bench——一个用于评估LVLMs在人机交互场景中理解视频视觉提示能力的鲁棒性综合基准。该基准包含980个视频与1172组结构清晰的高质量问答对，每组均配有手动标注的视觉提示帧，涵盖三大任务与十二个类别，支持细粒度实例级评估。通过对当前LVLMs的深入分析，我们得出关键发现：1）在交互场景中，视觉提示比文本提示更适配模型需求与用户体验，能显著提升模型性能；2）模型具备零样本理解视觉提示的合理能力，但在时空理解方面存在困难。即使o1模型仅达71.8%，远低于人类专家88.3%的水平，多数开源模型表现低于60%；3）LVLMs在视频问答任务中普遍存在“伪解现象”，该现象随视频时长增加与帧采样密度降低而加剧，导致性能评分虚高。我们期待V2P-Bench不仅能揭示这些挑战，更能成为推进人机交互与优化视频理解评估的基础工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of text-only prompts in video-language benchmarks, which often involve complex referential language and hinder accurate and efficient human-model interaction, this work introduces V2P-Bench, a comprehensive benchmark for evaluating Large Vision-Language Models (LVLMs) using visual prompts. The method involves constructing a dataset of 980 videos with 1,172 high-quality QA pairs, each annotated with visual prompt frames, spanning three tasks and twelve categories for fine-grained evaluation. Key experimental findings reveal that visual prompts are more user- and model-friendly than text prompts, significantly boosting performance; models show reasonable zero-shot understanding of visual prompts but struggle with spatiotemporal reasoning, with top models like o1 scoring 71.8% versus a human expert score of 88.3%; and LVLMs exhibit pervasive &#x27;Hack Phenomena&#x27; in video QA, where performance is artificially inflated with longer videos and sparser frame sampling.</div>
<div class="mono" style="margin-top:8px">现有的大型视觉语言模型（LVLM）视频基准主要依赖文本提示，这通常需要复杂的指代语言，降低了人机交互的准确性和效率。为此，研究者提出了V2P-Bench，这是一个包含980个视频和1172个带视觉提示帧标注的高质量问答对的基准，涵盖三个主要任务和十二个类别，支持细粒度的实例级评估。实验分析发现：视觉提示比文本提示更友好，能显著提升模型性能和用户体验；模型对视觉提示具备一定的零样本理解能力，但在时空理解上存在困难，即使最佳模型o1也仅达到71.8%，远低于人类专家的88.3%；此外，LVLM在视频问答中普遍存在“Hack现象”，即随着视频时长增加和帧采样密度降低，性能分数被人为夸大。</div>
</details>
</div>
<div class="card">
<div class="title">POP: Prefill-Only Pruning for Efficient Large Model Inference</div>
<div class="meta-line">Authors: Junhui He, Zhihui Fu, Jun Wang, Qingan Li</div>
<div class="meta-line">First: 2026-02-03T09:22:26+00:00 · Latest: 2026-02-03T09:22:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03295v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03295v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) and Vision-Language Models (VLMs) have demonstrated remarkable capabilities. However, their deployment is hindered by significant computational costs. Existing structured pruning methods, while hardware-efficient, often suffer from significant accuracy degradation. In this paper, we argue that this failure stems from a stage-agnostic pruning approach that overlooks the asymmetric roles between the prefill and decode stages. By introducing a virtual gate mechanism, our importance analysis reveals that deep layers are critical for next-token prediction (decode) but largely redundant for context encoding (prefill). Leveraging this insight, we propose Prefill-Only Pruning (POP), a stage-aware inference strategy that safely omits deep layers during the computationally intensive prefill stage while retaining the full model for the sensitive decode stage. To enable the transition between stages, we introduce independent Key-Value (KV) projections to maintain cache integrity, and a boundary handling strategy to ensure the accuracy of the first generated token. Extensive experiments on Llama-3.1, Qwen3-VL, and Gemma-3 across diverse modalities demonstrate that POP achieves up to 1.37$\times$ speedup in prefill latency with minimal performance loss, effectively overcoming the accuracy-efficiency trade-off limitations of existing structured pruning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POP：仅预填充剪枝实现高效大模型推理</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）与视觉语言模型（VLMs）已展现出卓越能力，但其部署受限于高昂计算成本。现有结构化剪枝方法虽硬件高效，却常导致显著精度下降。本文指出，这一缺陷源于未区分预填充与解码阶段不对称作用的阶段无关剪枝策略。通过引入虚拟门机制，重要性分析表明深层对下一词元预测（解码）至关重要，但对上下文编码（预填充）高度冗余。基于此，我们提出仅预填充剪枝（POP），这是一种阶段感知推理策略：在计算密集的预填充阶段安全跳过深层，同时在敏感的解码阶段保留完整模型。为实现阶段切换，我们引入独立键值投影以保持缓存完整性，并设计边界处理策略确保首生成词元精度。在Llama-3.1、Qwen3-VL和Gemma-3等多模态模型上的实验表明，POP在预填充延迟上最高实现1.37倍加速且性能损失极小，有效突破了现有结构化剪枝方法在精度与效率间的权衡局限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency of large models while overcoming the accuracy degradation of existing structured pruning methods, this study identifies the asymmetric roles of model layers in prefill and decode stages. The proposed Prefill-Only Pruning (POP) method strategically omits deep layers during the prefill stage but retains the full model for decoding, employing independent KV projections and a boundary handling strategy to maintain cache integrity and first-token accuracy. Experiments on models like Llama-3.1 and Qwen3-VL show that POP achieves up to 1.37× prefill speedup with minimal performance loss, effectively mitigating the accuracy-efficiency trade-off.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大语言模型和视觉语言模型部署时计算成本高昂的问题，以及现有结构化剪枝方法常导致精度显著下降的局限。所提出的方法名为仅预填充剪枝（POP），这是一种阶段感知的推理策略，通过虚拟门机制分析层重要性，发现深层对上下文编码（预填充阶段）冗余但对下一令牌预测（解码阶段）关键。因此，该方法仅在计算密集的预填充阶段剪枝深层，并采用独立的键值投影和边界处理策略来保持缓存完整性和首令牌精度。在Llama-3.1和Qwen3-VL等模型上的实验表明，POP能实现高达1.37倍的预填充加速，且性能损失极小，有效克服了现有结构化剪枝方法的精度-效率权衡限制。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond the Vision Encoder: Identifying and Mitigating Spatial Bias in Large Vision-Language Models</div>
<div class="meta-line">Authors: Yingjie Zhu, Xuefeng Bai, Kehai Chen, Yang Xiang, Youcheng Pan, Yongshuai Hou, Weili Guan, Jun Yu, Min Zhang</div>
<div class="meta-line">First: 2025-09-26T07:07:03+00:00 · Latest: 2026-02-03T08:40:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21984v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21984v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have achieved remarkable success across a wide range of multimodal tasks, yet their robustness to spatial variations remains insufficiently understood. In this work, we conduct a systematic study of the spatial bias of LVLMs, examining how models respond when identical key visual information is placed at different locations within an image. Through controlled probing experiments, we observe that current LVLMs often produce inconsistent outputs under such spatial shifts, revealing a clear spatial bias in their semantic understanding. Further analysis indicates that this bias does not stem from the vision encoder, but rather from a mismatch in attention mechanisms between the vision encoder and the large language model, which disrupts the global information flow. Motivated by this insight, we propose Adaptive Global Context Injection (AGCI), a lightweight mechanism that dynamically injects shared global visual context into each image token. AGCI works without architectural modifications, mitigating spatial bias by enhancing the semantic accessibility of image tokens while preserving the model&#x27;s intrinsic capabilities. Extensive experiments demonstrate that AGCI not only enhances the spatial robustness of LVLMs, but also achieves strong performance on various downstream tasks and hallucination benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越视觉编码器：识别并缓解大型视觉语言模型中的空间偏差</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）在多模态任务中取得了显著成功，但其对空间变化的鲁棒性仍未得到充分理解。本研究系统性地探讨了LVLMs的空间偏差，通过控制性探测实验发现，当相同关键视觉信息置于图像不同位置时，现有模型常产生不一致的输出，揭示了其语义理解中存在明显的空间偏差。进一步分析表明，该偏差并非源于视觉编码器，而是由视觉编码器与大语言模型间的注意力机制不匹配导致，这破坏了全局信息流。基于此，我们提出自适应全局上下文注入（AGCI），一种轻量级机制，可动态地将共享全局视觉上下文注入每个图像标记中。AGCI无需修改模型架构，通过增强图像标记的语义可访问性来缓解空间偏差，同时保留模型固有能力。大量实验证明，AGCI不仅提升了LVLMs的空间鲁棒性，还在多种下游任务和幻觉基准测试中表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the spatial bias in Large Vision-Language Models (LVLMs), where models produce inconsistent outputs when identical visual information is placed at different image locations, revealing a robustness gap. The authors identify that this bias arises from an attention mismatch between the vision encoder and language model, not the encoder itself, and propose Adaptive Global Context Injection (AGCI), a lightweight method that dynamically injects global visual context into image tokens to restore information flow without architectural changes. Experiments show AGCI effectively mitigates spatial bias, improves spatial robustness, and maintains strong performance on downstream tasks and hallucination benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型视觉语言模型（LVLMs）中的空间偏见问题，即当相同的视觉信息被置于图像不同位置时，模型会产生不一致的输出，揭示了其鲁棒性不足。作者发现这种偏见并非源于视觉编码器，而是来自视觉与语言组件之间的注意力机制不匹配，从而破坏了全局信息流。为解决此问题，他们提出了自适应全局上下文注入（AGCI），这是一种轻量级方法，无需修改模型架构即可动态地将共享的全局视觉上下文注入图像令牌中。实验表明，AGCI有效缓解了空间偏见，增强了空间鲁棒性，并在多种下游任务和幻觉基准测试中保持了强劲性能。</div>
</details>
</div>
<div class="card">
<div class="title">LaVPR: Benchmarking Language and Vision for Place Recognition</div>
<div class="meta-line">Authors: Ofer Idan, Dan Badur, Yosi Keller, Yoli Shavit</div>
<div class="meta-line">First: 2026-02-03T08:38:38+00:00 · Latest: 2026-02-03T08:38:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03253v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03253v1">PDF</a> · <a href="https://github.com/oferidan1/LaVPR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Place Recognition (VPR) often fails under extreme environmental changes and perceptual aliasing. Furthermore, standard systems cannot perform &quot;blind&quot; localization from verbal descriptions alone, a capability needed for applications such as emergency response. To address these challenges, we introduce LaVPR, a large-scale benchmark that extends existing VPR datasets with over 650,000 rich natural-language descriptions. Using LaVPR, we investigate two paradigms: Multi-Modal Fusion for enhanced robustness and Cross-Modal Retrieval for language-based localization. Our results show that language descriptions yield consistent gains in visually degraded conditions, with the most significant impact on smaller backbones. Notably, adding language allows compact models to rival the performance of much larger vision-only architectures. For cross-modal retrieval, we establish a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss, which substantially outperforms standard contrastive methods across vision-language models. Ultimately, LaVPR enables a new class of localization systems that are both resilient to real-world stochasticity and practical for resource-constrained deployment. Our dataset and code are available at https://github.com/oferidan1/LaVPR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LaVPR：面向地点识别的语言与视觉基准测试</div>
<div class="mono" style="margin-top:8px">视觉地点识别（VPR）在极端环境变化和感知混淆场景下常出现失效，且标准系统无法仅通过语言描述实现“盲”定位——这种能力在应急响应等应用中至关重要。为应对这些挑战，我们提出了LaVPR，这是一个通过超过65万条丰富自然语言描述扩展现有VPR数据集的大规模基准。基于LaVPR，我们探索了两种范式：用于增强鲁棒性的多模态融合，以及基于语言定位的跨模态检索。实验表明，语言描述在视觉退化条件下能带来稳定性能提升，对轻量化骨干网络的影响尤为显著。值得注意的是，引入语言模态可使紧凑模型达到与更大规模纯视觉架构相当的性能。在跨模态检索方面，我们采用低秩自适应（LoRA）与多重相似度损失构建的基线方法，在各类视觉-语言模型中均显著优于标准对比方法。LaVPR最终催生了一类新型定位系统，既能适应现实世界的随机性，又适合资源受限场景的部署。数据集与代码已开源：https://github.com/oferidan1/LaVPR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual Place Recognition (VPR) struggles with severe environmental changes and perceptual aliasing, and cannot localize from verbal descriptions alone, a limitation for applications like emergency response. To address this, the authors introduce LaVPR, a large-scale benchmark extending existing VPR datasets with over 650,000 natural-language descriptions, and investigate two paradigms: Multi-Modal Fusion for robustness and Cross-Modal Retrieval for language-based localization. Experimental results show that incorporating language descriptions consistently improves performance in visually degraded conditions, with the most significant gains for smaller model backbones, enabling compact models to match larger vision-only architectures. For cross-modal retrieval, a baseline using Low-Rank Adaptation (LoRA) and Multi-Similarity loss substantially outperforms standard contrastive methods across various vision-language models.</div>
<div class="mono" style="margin-top:8px">视觉地点识别（VPR）在极端环境变化和感知混淆下常常失效，且无法仅从语言描述进行“盲”定位，这限制了其在应急响应等场景的应用。为解决这些问题，本研究提出了LaVPR，这是一个大规模基准数据集，通过为现有VPR数据集添加超过65万条丰富的自然语言描述进行扩展，并研究了两种范式：用于增强鲁棒性的多模态融合和用于基于语言的定位的跨模态检索。实验结果表明，在视觉退化条件下，加入语言描述能持续提升性能，且对较小骨干网络的提升最为显著，使紧凑模型能够媲美大得多的纯视觉架构的性能。在跨模态检索方面，使用低秩自适应（LoRA）和多相似性损失的基线方法显著优于跨各种视觉-语言模型的标准对比方法。</div>
</details>
</div>
<div class="card">
<div class="title">SwiftVLM: Efficient Vision-Language Model Inference via Cross-Layer Token Bypass</div>
<div class="meta-line">Authors: Chen Qian, Xinran Yu, Danyang Li, Guoxuan Chi, Zheng Yang, Qiang Ma, Xin Miao</div>
<div class="meta-line">First: 2026-02-03T05:42:51+00:00 · Latest: 2026-02-03T05:42:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03134v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03134v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual token pruning is a promising approach for reducing the computational cost of vision-language models (VLMs), and existing methods often rely on early pruning decisions to improve efficiency. While effective on coarse-grained reasoning tasks, they suffer from significant performance degradation on tasks requiring fine-grained visual details. Through layer-wise analysis, we reveal substantial discrepancies in visual token importance across layers, showing that tokens deemed unimportant at shallow layers can later become highly relevant for text-conditioned reasoning. To avoid irreversible critical information loss caused by premature pruning, we introduce a new pruning paradigm, termed bypass, which preserves unselected visual tokens and forwards them to subsequent pruning stages for re-evaluation. Building on this paradigm, we propose SwiftVLM, a simple and training-free method that performs pruning at model-specific layers with strong visual token selection capability, while enabling independent pruning decisions across layers. Experiments across multiple VLMs and benchmarks demonstrate that SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SwiftVLM：通过跨层令牌旁路实现高效视觉语言模型推理</div>
<div class="mono" style="margin-top:8px">视觉令牌剪枝是降低视觉语言模型计算成本的有效方法，现有方法常依赖早期剪枝决策以提升效率。尽管在粗粒度推理任务上表现良好，但在需要细粒度视觉细节的任务中，这些方法往往导致性能显著下降。通过逐层分析，我们发现视觉令牌的重要性在不同层间存在显著差异：浅层中被视为不重要的令牌可能在后续文本条件推理中变得高度相关。为避免因过早剪枝造成不可逆的关键信息损失，我们提出一种名为“旁路”的新剪枝范式，该范式保留未选中的视觉令牌并将其传递至后续剪枝阶段进行重新评估。基于此范式，我们提出了SwiftVLM——一种简单且无需训练的方法，可在具有强视觉令牌选择能力的模型特定层执行剪枝，同时支持跨层的独立剪枝决策。在多个视觉语言模型和基准测试上的实验表明，SwiftVLM始终优于现有剪枝策略，实现了更优的精度-效率权衡及更可靠的视觉令牌选择行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the performance degradation of existing visual token pruning methods on fine-grained vision-language tasks due to premature pruning decisions, this study introduces a new pruning paradigm called bypass that preserves unselected tokens for later re-evaluation. The proposed SwiftVLM method implements this paradigm by performing training-free, layer-specific pruning with independent decisions across layers, leveraging strong token selection capabilities. Experimental results across multiple VLMs and benchmarks show that SwiftVLM consistently outperforms existing pruning strategies, achieving superior accuracy-efficiency trade-offs and more faithful visual token selection behavior.</div>
<div class="mono" style="margin-top:8px">该研究针对现有视觉语言模型中的视觉令牌剪枝方法在处理需要细粒度视觉细节的任务时，因早期剪枝决策导致不可逆信息丢失而性能下降的问题。提出的SwiftVLM引入了一种旁路范式，保留未选中的令牌以供后续层重新评估，实现了无需训练、特定于模型层的剪枝，且各层剪枝决策独立。在多个视觉语言模型和基准测试上的实验表明，SwiftVLM优于现有剪枝策略，实现了更优的精度-效率权衡和更可靠的令牌选择行为。</div>
</details>
</div>
<div class="card">
<div class="title">FinMTM: A Multi-Turn Multimodal Benchmark for Financial Reasoning and Agent Evaluation</div>
<div class="meta-line">Authors: Chenxi Zhang, Ziliang Gan, Liyun Zhu, Youwei Pang, Qing Zhang, Rongjunchen Zhang</div>
<div class="meta-line">First: 2026-02-03T05:38:24+00:00 · Latest: 2026-02-03T05:38:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03130v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The financial domain poses substantial challenges for vision-language models (VLMs) due to specialized chart formats and knowledge-intensive reasoning requirements. However, existing financial benchmarks are largely single-turn and rely on a narrow set of question formats, limiting comprehensive evaluation in realistic application scenarios. To address this gap, we propose FinMTM, a multi-turn multimodal benchmark that expands diversity along both data and task dimensions. On the data side, we curate and annotate 11{,}133 bilingual (Chinese and English) financial QA pairs grounded in financial visuals, including candlestick charts, statistical plots, and report figures. On the task side, FinMTM covers single- and multiple-choice questions, multi-turn open-ended dialogues, and agent-based tasks. We further design task-specific evaluation protocols, including a set-overlap scoring rule for multiple-choice questions, a weighted combination of turn-level and session-level scores for multi-turn dialogues, and a composite metric that integrates planning quality with final outcomes for agent tasks. Extensive experimental evaluation of 22 VLMs reveal their limitations in fine-grained visual perception, long-context reasoning, and complex agent workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FinMTM：面向金融推理与智能体评估的多轮多模态基准</div>
<div class="mono" style="margin-top:8px">金融领域因专业图表格式与知识密集型推理需求，对视觉语言模型构成显著挑战。然而现有金融基准多为单轮任务且依赖有限题型，难以支撑实际场景的全面评估。为此，我们提出FinMTM——一个在数据与任务维度同步拓展的多轮多模态基准。数据层面，我们构建并标注了11,133组基于金融可视化图像（含K线图、统计图及报表图示）的中英双语问答对。任务层面，FinMTM涵盖单选/多选题、多轮开放式对话及智能体任务。我们进一步设计了针对性评估方案：为多选题设计集合重叠评分规则，为多轮对话构建轮次与会话级加权评分体系，为智能体任务整合规划质量与最终结果的复合指标。对22个视觉语言模型的实验表明，其在细粒度视觉感知、长上下文推理及复杂智能体工作流方面仍存在局限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of existing financial benchmarks, which are predominantly single-turn and lack diverse question formats, thereby failing to comprehensively evaluate vision-language models (VLMs) in realistic financial scenarios. To address this, the authors introduce FinMTM, a multi-turn multimodal benchmark that enhances diversity in both data and tasks by curating 11,133 bilingual QA pairs based on financial visuals like candlestick charts and report figures, and by covering single/multiple-choice questions, multi-turn dialogues, and agent-based tasks with tailored evaluation protocols. Experimental results from evaluating 22 VLMs reveal significant shortcomings in fine-grained visual perception, long-context reasoning, and handling complex agent workflows.</div>
<div class="mono" style="margin-top:8px">本研究针对现有金融基准测试主要局限于单轮问答且问题格式单一的问题，这限制了对视觉语言模型在真实金融场景中的全面评估。方法上提出了FinMTM，一个多轮多模态基准测试，通过在数据维度上构建了基于K线图、统计图等金融视觉内容的11,133个双语问答对，在任务维度上涵盖了单选/多选、多轮开放对话和基于智能体的任务，并设计了针对性的评估协议。对22个视觉语言模型的广泛实验评估揭示了它们在细粒度视觉感知、长上下文推理和处理复杂智能体工作流方面存在显著不足。</div>
</details>
</div>
<div class="card">
<div class="title">ReasonEdit: Editing Vision-Language Models using Human Reasoning</div>
<div class="meta-line">Authors: Jiaxing Qiu, Kaihua Hou, Roxana Daneshjou, Ahmed Alaa, Thomas Hartvigsen</div>
<div class="meta-line">First: 2026-02-02T18:06:14+00:00 · Latest: 2026-02-03T04:18:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02408v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02408v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model editing aims to correct errors in large, pretrained models without altering unrelated behaviors. While some recent works have edited vision-language models (VLMs), no existing editors tackle reasoning-heavy tasks, which typically require humans and models to reason about images. We therefore propose ReasonEdit, the first VLM editor to let users explain their reasoning during editing, introducing a new, practical model editing setup. ReasonEdit continuously stores human reasoning in a codebook, and retrieves only relevant facts during inference using a novel topology-balanced multimodal embedding method inspired by network science. Across four VLMs on multiple rationale-based visual question answering datasets, ReasonEdit achieves state-of-the-art editing performance, ultimately showing that using human reasoning during editing greatly improves edit generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReasonEdit：基于人类推理的视觉语言模型编辑方法</div>
<div class="mono" style="margin-top:8px">模型编辑旨在修正大型预训练模型的错误而不影响无关行为。虽然近期已有针对视觉语言模型（VLM）的编辑方法，但尚未有编辑器能处理需要人类与模型共同进行图像推理的复杂任务。为此，我们提出首个支持用户在编辑过程中解释推理逻辑的VLM编辑器ReasonEdit，开创了全新的实用模型编辑范式。该方法通过受网络科学启发的拓扑平衡多模态嵌入技术，持续将人类推理存储于编码本，并在推理阶段仅检索相关事实。在四个VLM模型及多个基于推理的视觉问答数据集上的实验表明，ReasonEdit实现了最先进的编辑性能，最终证明编辑过程中引入人类推理能显著提升编辑泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the lack of model editing methods for vision-language models (VLMs) on reasoning-heavy tasks, where errors require correction without affecting unrelated model behaviors. The proposed method, ReasonEdit, introduces a novel editing setup that incorporates human reasoning explanations, continuously storing them in a codebook and retrieving relevant facts during inference via a topology-balanced multimodal embedding technique inspired by network science. Experimental results on multiple rationale-based visual question answering datasets across four VLMs demonstrate state-of-the-art editing performance, showing that integrating human reasoning significantly improves edit generalization.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要纠正大型视觉语言模型（VLM）中的错误而不影响无关能力，特别是在现有编辑器难以处理的需要大量推理的任务上。方法提出了ReasonEdit，这是一种新颖的编辑框架，允许用户在编辑过程中融入人类推理解释，将这些推理事实存储在码本中，并在推理时使用一种受网络科学启发的拓扑平衡多模态嵌入技术来检索相关事实。在四个VLM上对多个基于推理的视觉问答数据集的实验结果表明，ReasonEdit实现了最先进的编辑性能，证明整合人类推理能显著提升编辑的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">IVC-Prune: Revealing the Implicit Visual Coordinates in LVLMs for Vision Token Pruning</div>
<div class="meta-line">Authors: Zhichao Sun, Yidong Ma, Gang Liu, Yibo Chen, Xu Tang, Yao Hu, Yongchao Xu</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T03:39:31+00:00 · Latest: 2026-02-03T03:39:31+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03060v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03060v1">PDF</a> · <a href="https://github.com/FireRedTeam/IVC-Prune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) achieve impressive performance across multiple tasks. A significant challenge, however, is their prohibitive inference cost when processing high-resolution visual inputs. While visual token pruning has emerged as a promising solution, existing methods that primarily focus on semantic relevance often discard tokens that are crucial for spatial reasoning. We address this gap through a novel insight into \emph{how LVLMs process spatial reasoning}. Specifically, we reveal that LVLMs implicitly establish visual coordinate systems through Rotary Position Embeddings (RoPE), where specific token positions serve as \textbf{implicit visual coordinates} (IVC tokens) that are essential for spatial reasoning. Based on this insight, we propose \textbf{IVC-Prune}, a training-free, prompt-aware pruning strategy that retains both IVC tokens and semantically relevant foreground tokens. IVC tokens are identified by theoretically analyzing the mathematical properties of RoPE, targeting positions at which its rotation matrices approximate identity matrix or the $90^\circ$ rotation matrix. Foreground tokens are identified through a robust two-stage process: semantic seed discovery followed by contextual refinement via value-vector similarity. Extensive evaluations across four representative LVLMs and twenty diverse benchmarks show that IVC-Prune reduces visual tokens by approximately 50\% while maintaining $\geq$ 99\% of the original performance and even achieving improvements on several benchmarks. Source codes are available at https://github.com/FireRedTeam/IVC-Prune.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IVC-Prune：揭示LVLM中的隐式视觉坐标以实现视觉令牌剪枝</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLM）在多项任务中展现出卓越性能，但其处理高分辨率视觉输入时的高昂推理成本构成显著挑战。视觉令牌剪枝作为一种有前景的解决方案，现有方法主要关注语义相关性，常会丢弃对空间推理至关重要的令牌。本研究通过深入探究LVLM处理空间推理的机制填补了这一空白。具体而言，我们发现LVLM通过旋转位置编码（RoPE）隐式构建了视觉坐标系，其中特定令牌位置作为对空间推理至关重要的隐式视觉坐标（IVC令牌）。基于此发现，我们提出IVC-Prune——一种无需训练、提示感知的剪枝策略，同时保留IVC令牌和语义相关的前景令牌。IVC令牌通过理论分析RoPE的数学特性进行识别，定位其旋转矩阵近似单位矩阵或90°旋转矩阵的位置。前景令牌则通过鲁棒的两阶段流程识别：先进行语义种子发现，再通过值向量相似性进行上下文精炼。在四种代表性LVLM和二十个多样化基准测试上的广泛评估表明，IVC-Prune能减少约50%的视觉令牌，同时保持≥99%的原始性能，并在多个基准上实现性能提升。源代码发布于https://github.com/FireRedTeam/IVC-Prune。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high inference cost of Large Vision-Language Models (LVLMs) on high-resolution images, this research investigates how LVLMs perform spatial reasoning to improve visual token pruning. The method, IVC-Prune, is a training-free, prompt-aware strategy based on the insight that LVLMs implicitly establish visual coordinate systems via Rotary Position Embeddings (RoPE), where certain token positions act as crucial implicit visual coordinates (IVC tokens). It retains these IVC tokens, identified by analyzing RoPE&#x27;s mathematical properties for specific rotation matrices, alongside semantically relevant foreground tokens found through a two-stage process of seed discovery and contextual refinement. Experiments on four LVLMs across twenty benchmarks show that IVC-Prune reduces visual tokens by about 50% while preserving at least 99% of the original model performance, with gains on some tasks.</div>
<div class="mono" style="margin-top:8px">为降低大视觉语言模型处理高分辨率图像时的高昂推理成本，本研究探究了模型处理空间信息的方式以改进视觉令牌剪枝。提出的IVC-Prune是一种无需训练的策略，它识别并保留两类关键令牌：一是隐式视觉坐标令牌，即旋转位置嵌入近似单位矩阵或90度旋转矩阵的位置，对空间推理至关重要；二是语义相关的前景令牌，通过种子发现和上下文精炼的两阶段过程选取。在四个大视觉语言模型和二十个基准测试上的实验表明，IVC-Prune能减少约50%的视觉令牌，同时保持至少99%的原始模型性能，并在部分任务上实现性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Bongards at the Boundary of Perception and Reasoning: Programs or Language?</div>
<div class="meta-line">Authors: Cassidy Langenfeld, Claas Beger, Gloria Geng, Wasu Top Piriyakulkij, Keya Hu, Yewen Pu, Kevin Ellis</div>
<div class="meta-line">First: 2026-02-03T03:04:27+00:00 · Latest: 2026-02-03T03:04:27+00:00</div>
<div class="meta-line">Comments: 6 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have made great strides in everyday visual tasks, such as captioning a natural image, or answering commonsense questions about such images. But humans possess the puzzling ability to deploy their visual reasoning abilities in radically new situations, a skill rigorously tested by the classic set of visual reasoning challenges known as the Bongard problems. We present a neurosymbolic approach to solving these problems: given a hypothesized solution rule for a Bongard problem, we leverage LLMs to generate parameterized programmatic representations for the rule and perform parameter fitting using Bayesian optimization. We evaluate our method on classifying Bongard problem images given the ground truth rule, as well as on solving the problems from scratch.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>邦加德问题：感知与推理边界的程序化还是语言化探索？</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在日常视觉任务（如自然图像描述或常识问答）中取得显著进展。但人类具备在全新情境中运用视觉推理的非凡能力，这一能力可通过经典的邦加德视觉推理问题集进行严格测试。本文提出一种神经符号方法解决此类问题：针对假设的邦加德问题规则，利用大语言模型生成参数化程序表征，并通过贝叶斯优化进行参数拟合。该方法分别在已知真实规则下的图像分类任务和从零开始解题任务中进行评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand whether Vision-Language Models (VLMs) can handle abstract visual reasoning in novel scenarios, as tested by the classic Bongard problems, which challenge the boundary between perception and reasoning. The method employs a neurosymbolic approach: it uses Large Language Models (LLMs) to generate parameterized programmatic representations for hypothesized solution rules and then performs parameter fitting via Bayesian optimization. Key experimental results show the method&#x27;s effectiveness in classifying Bongard problem images when provided with the ground truth rule and also in solving the problems from scratch, demonstrating a step toward more robust visual reasoning.</div>
<div class="mono" style="margin-top:8px">这项研究的动机是探究视觉语言模型（VLMs）能否在如经典邦加德问题所测试的新颖场景中匹配人类的视觉推理能力。方法采用了一种神经符号方法：对于一个假设的解决方案规则，它利用大型语言模型（LLMs）生成参数化的程序化表示，然后使用贝叶斯优化进行参数拟合。关键的实验结果表明，该方法在给定真实规则时对邦加德问题图像进行分类以及从头开始解决问题方面是有效的，这在一系列问题中得到了评估。</div>
</details>
</div>
<div class="card">
<div class="title">VOILA: Value-of-Information Guided Fidelity Selection for Cost-Aware Multimodal Question Answering</div>
<div class="meta-line">Authors: Rahul Atul Bhope, K. R. Jayaram, Vinod Muthusamy, Ritesh Kumar, Vatche Isahagian, Nalini Venkatasubramanian</div>
<div class="meta-line">First: 2026-02-03T02:19:47+00:00 · Latest: 2026-02-03T02:19:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03007v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03007v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite significant costs from retrieving and processing high-fidelity visual inputs, most multimodal vision-language systems operate at fixed fidelity levels. We introduce VOILA, a framework for Value-Of-Information-driven adaptive fidelity selection in Visual Question Answering (VQA) that optimizes what information to retrieve before model execution. Given a query, VOILA uses a two-stage pipeline: a gradient-boosted regressor estimates correctness likelihood at each fidelity from question features alone, then an isotonic calibrator refines these probabilities for reliable decision-making. The system selects the minimum-cost fidelity maximizing expected utility given predicted accuracy and retrieval costs. We evaluate VOILA across three deployment scenarios using five datasets (VQA-v2, GQA, TextVQA, LoCoMo, FloodNet) and six Vision-Language Models (VLMs) with 7B-235B parameters. VOILA consistently achieves 50-60% cost reductions while retaining 90-95% of full-resolution accuracy across diverse query types and model architectures, demonstrating that pre-retrieval fidelity selection is vital to optimize multimodal inference under resource constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VOILA：面向成本感知多模态问答的信息价值引导保真度选择框架</div>
<div class="mono" style="margin-top:8px">尽管检索和处理高保真度视觉输入会产生显著成本，现有多模态视觉语言系统大多采用固定保真度运行。本文提出VOILA框架，通过信息价值驱动的自适应保真度选择机制，在视觉问答任务中实现模型执行前的信息检索优化。该框架采用两阶段流程：首先基于梯度提升回归器仅从问题特征预估各保真度下的正确率概率，再通过保序校准器精化概率以支持可靠决策。系统根据预测准确率与检索成本，选择能最大化期望效用的最小成本保真度。我们在三种部署场景下，使用五个数据集（VQA-v2、GQA、TextVQA、LoCoMo、FloodNet）和六个参数量为7B-235B的视觉语言模型进行评估。VOILA在不同查询类型和模型架构中持续实现50-60%的成本削减，同时保持90-95%的全分辨率准确率，证明检索前保真度选择对资源受限环境下优化多模态推理至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high costs of retrieving and processing high-fidelity visual inputs in multimodal systems, which typically operate at fixed fidelity, this paper introduces VOILA, a framework for Value-Of-Information guided adaptive fidelity selection in Visual Question Answering. The method employs a two-stage pipeline: a gradient-boosted regressor first estimates the correctness likelihood for each fidelity level using only question features, followed by an isotonic calibrator to refine these probabilities for reliable decision-making, thereby selecting the minimum-cost fidelity that maximizes expected utility. Experimental evaluation across three deployment scenarios using five datasets and six Vision-Language Models demonstrates that VOILA consistently achieves 50-60% cost reductions while retaining 90-95% of full-resolution accuracy, highlighting the importance of pre-retrieval fidelity selection for optimizing multimodal inference under resource constraints.</div>
<div class="mono" style="margin-top:8px">针对多模态系统中检索和处理高保真视觉输入成本高昂且通常采用固定保真度的问题，本研究提出了VOILA框架，用于在视觉问答（VQA）中实现基于信息价值的自适应保真度选择。该方法采用两阶段流程：首先，梯度提升回归器仅利用问题特征估计每个保真度级别的正确性似然；随后，等渗校准器细化这些概率，以支持可靠、成本感知的决策，选择能最大化期望效用的最小成本保真度。在三种部署场景下，使用五个数据集（VQA-v2、GQA、TextVQA、LoCoMo、FloodNet）和六个参数量从7B到235B的视觉语言模型进行实验评估，结果表明，VOILA在不同查询类型和模型架构上持续实现50-60%的成本降低，同时保持90-95%的全分辨率准确率，证明了在资源约束下，检索前的保真度选择对于优化多模态推理至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Forest and Trees in Images and Long Captions for Visually Grounded Understanding</div>
<div class="meta-line">Authors: Byeongju Woo, Zilin Wang, Byeonghyun Pak, Sangwoo Mo, Stella X. Yu</div>
<div class="meta-line">First: 2026-02-03T01:31:55+00:00 · Latest: 2026-02-03T01:31:55+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02977v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02977v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models such as CLIP struggle with long captions because they align images and texts as undifferentiated wholes. Fine-grained vision-language understanding requires hierarchical semantics capturing both global context and localized details across visual and textual domains. Yet linguistic hierarchies from syntax or semantics rarely match visual organization, and purely visual hierarchies tend to fragment scenes into appearance-driven parts without semantic focus. We propose CAFT (Cross-domain Alignment of Forests and Trees), a hierarchical image-text representation learning framework that aligns global and local semantics across images and long captions without pixel-level supervision. Coupling a fine-to-coarse visual encoder with a hierarchical text transformer, it uses a hierarchical alignment loss that matches whole images with whole captions while biasing region-sentence correspondences, so that coarse semantics are built from fine-grained evidence rather than from aggregation untethered to part-level grounding. Trained on 30M image-text pairs, CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and exhibits strong scaling behavior. Experiments show that hierarchical cross-domain alignment enables fine-grained, visually grounded image-text representations to emerge without explicit region-level supervision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像与长描述中整体与局部的对齐：面向视觉基础理解的研究</div>
<div class="mono" style="margin-top:8px">CLIP等大型视觉语言模型在处理长描述时面临挑战，因其将图像与文本作为无差别的整体进行对齐。细粒度视觉语言理解需要能同时捕捉视觉与文本领域全局语境与局部细节的层次化语义。然而，基于句法或语义的语言层次结构很少与视觉组织结构匹配，而纯视觉层次结构倾向于将场景分割为外观驱动的局部，缺乏语义焦点。我们提出CAFT（跨域森林与树木对齐框架），这是一种无需像素级监督的层次化图文表示学习框架，可在图像与长描述间对齐全局与局部语义。该框架通过细粒度到粗粒度的视觉编码器与层次化文本Transformer耦合，采用层次化对齐损失函数：在匹配完整图像与完整描述的同时，偏置区域-句子的对应关系，使得粗粒度语义基于细粒度证据构建，而非脱离局部基础的特征聚合。在3000万图文对上训练的CAFT，在六个长文本检索基准测试中达到最先进性能，并展现出强大的扩展能力。实验表明，层次化跨域对齐能在无显式区域级监督的情况下，催生具有细粒度视觉基础的图文表示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of large vision-language models like CLIP in handling long captions, as they treat images and texts as undifferentiated wholes, lacking fine-grained hierarchical semantics that align global context with localized details. The proposed method, CAFT (Cross-domain Alignment of Forests and Trees), introduces a hierarchical image-text representation learning framework that couples a fine-to-coarse visual encoder with a hierarchical text transformer, using a hierarchical alignment loss to match whole images with whole captions while biasing region-sentence correspondences, thereby building coarse semantics from fine-grained evidence without pixel-level supervision. Experimental results on 30M image-text pairs show that CAFT achieves state-of-the-art performance on six long-text retrieval benchmarks and demonstrates strong scaling behavior, enabling fine-grained, visually grounded representations to emerge without explicit region-level supervision.</div>
<div class="mono" style="margin-top:8px">本研究针对CLIP等大型视觉语言模型在处理长文本描述时的局限性，这些模型将图像和文本视为无差别的整体，缺乏连接全局上下文与局部细节的细粒度层次语义。提出的CAFT方法引入了一个层次化图像-文本表示学习框架，将细到粗的视觉编码器与层次化文本Transformer耦合，使用层次化对齐损失来匹配完整图像与完整描述，同时偏置区域-句子的对应关系。在3000万图像-文本对上的实验表明，该方法在六个长文本检索基准上取得了最先进的性能，结果显示层次化跨域对齐能够在没有显式区域级监督的情况下，产生细粒度的、视觉接地的表示。</div>
</details>
</div>
<div class="card">
<div class="title">TRACE: Temporal Radiology with Anatomical Change Explanation for Grounded X-ray Report Generation</div>
<div class="meta-line">Authors: OFM Riaz Rahman Aranya, Kevin Desai</div>
<div class="meta-line">First: 2026-02-03T01:03:41+00:00 · Latest: 2026-02-03T01:03:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02963v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02963v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporal comparison of chest X-rays is fundamental to clinical radiology, enabling detection of disease progression, treatment response, and new findings. While vision-language models have advanced single-image report generation and visual grounding, no existing method combines these capabilities for temporal change detection. We introduce Temporal Radiology with Anatomical Change Explanation (TRACE), the first model that jointly performs temporal comparison, change classification, and spatial localization. Given a prior and current chest X-ray, TRACE generates natural language descriptions of interval changes (worsened, improved, stable) while grounding each finding with bounding box coordinates. TRACE demonstrates effective spatial localization with over 90% grounding accuracy, establishing a foundation for this challenging new task. Our ablation study uncovers an emergent capability: change detection arises only when temporal comparison and spatial grounding are jointly learned, as neither alone enables meaningful change detection. This finding suggests that grounding provides a spatial attention mechanism essential for temporal reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TRACE：基于解剖结构变化解释的时序放射学胸部X光报告生成</div>
<div class="mono" style="margin-top:8px">胸部X光片的时序对比是临床放射学的基础，有助于检测疾病进展、治疗反应及新发现。尽管视觉语言模型在单图像报告生成和视觉定位方面取得进展，现有方法尚未将这些能力结合用于时序变化检测。我们提出基于解剖结构变化解释的时序放射学模型（TRACE），这是首个能同时执行时序对比、变化分类和空间定位的模型。给定先期与当前胸部X光片，TRACE生成描述间隔变化（恶化、改善、稳定）的自然语言报告，并通过边界框坐标对每个发现进行空间定位。TRACE展现出超过90%定位准确率的有效空间定位能力，为这一挑战性新任务奠定基础。消融实验揭示了一个涌现能力：仅当时序对比与空间定位被联合学习时，变化检测才会出现，单独任一任务均无法实现有意义的变化检测。这一发现表明，空间定位提供了时序推理所必需的空间注意力机制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the critical clinical need for automated temporal comparison of chest X-rays to track disease progression, treatment response, and new findings, a task not yet combined with spatial localization in existing vision-language models. The authors introduce TRACE, a model that jointly learns temporal comparison, change classification (worsened/improved/stable), and spatial grounding for interval changes between prior and current X-rays, generating natural language reports with bounding box coordinates. Experimental results show TRACE achieves over 90% grounding accuracy, and an ablation study reveals an emergent capability: meaningful change detection only arises when temporal comparison and spatial grounding are learned jointly, suggesting grounding provides an essential spatial attention mechanism for temporal reasoning.</div>
<div class="mono" style="margin-top:8px">本研究针对胸部X光分析中缺乏结合时序比较和视觉定位方法的问题，这是追踪疾病进展的关键任务。提出的TRACE模型联合学习比较先期与当前X光片、分类变化，并生成带有空间定位边界框的自然语言报告。实验表明该模型实现了超过90%的定位准确率，消融研究揭示有效的变化检测仅在时序比较和空间定位联合训练时才会出现，这表明定位作为一种空间注意力机制对时序推理至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Nüwa: Mending the Spatial Integrity Torn by VLM Token Pruning</div>
<div class="meta-line">Authors: Yihong Huang, Fei Ma, Yihua Shao, Jingcai Guo, Zitong Yu, Laizhong Cui, Qi Tian</div>
<div class="meta-line">First: 2026-02-03T00:51:03+00:00 · Latest: 2026-02-03T00:51:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02951v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02951v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision token pruning has proven to be an effective acceleration technique for the efficient Vision Language Model (VLM). However, existing pruning methods demonstrate excellent performance preservation in visual question answering (VQA) and suffer substantial degradation on visual grounding (VG) tasks. Our analysis of the VLM&#x27;s processing pipeline reveals that strategies utilizing global semantic similarity and attention scores lose the global spatial reference frame, which is derived from the interactions of tokens&#x27; positional information. Motivated by these findings, we propose $\text{Nüwa}$, a two-stage token pruning framework that enables efficient feature aggregation while maintaining spatial integrity. In the first stage, after the vision encoder, we apply three operations, namely separation, alignment, and aggregation, which are inspired by swarm intelligence algorithms to retain information-rich global spatial anchors. In the second stage, within the LLM, we perform text-guided pruning to retain task-relevant visual tokens. Extensive experiments demonstrate that $\text{Nüwa}$ achieves SOTA performance on multiple VQA benchmarks (from 94% to 95%) and yields substantial improvements on visual grounding tasks (from 7% to 47%).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>女娲：修复视觉语言模型令牌剪枝导致的空间完整性撕裂</div>
<div class="mono" style="margin-top:8px">视觉令牌剪枝已被证明是提升视觉语言模型效率的有效加速技术。然而，现有剪枝方法在视觉问答任务中表现优异，但在视觉定位任务上却出现显著性能下降。通过对VLM处理流程的分析，我们发现依赖全局语义相似性和注意力分数的策略会丢失由令牌位置信息交互产生的全局空间参考框架。基于此，我们提出$\text{Nüwa}$——一个两阶段令牌剪枝框架，能在保持空间完整性的同时实现高效特征聚合。第一阶段在视觉编码器后，采用受群体智能算法启发的分离、对齐和聚合操作，以保留信息丰富的全局空间锚点。第二阶段在大型语言模型内部，执行文本引导的剪枝以保留任务相关的视觉令牌。大量实验表明，$\text{Nüwa}$在多个VQA基准测试中达到SOTA性能（从94%提升至95%），并在视觉定位任务上取得显著改进（从7%提升至47%）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision token pruning accelerates Vision Language Models (VLMs) but often degrades performance on visual grounding tasks due to loss of spatial reference frames. To address this, the authors propose Nüwa, a two-stage pruning framework: first, after the vision encoder, it uses separation, alignment, and aggregation operations inspired by swarm intelligence to preserve global spatial anchors; second, within the LLM, it performs text-guided pruning to keep task-relevant visual tokens. Experiments show Nüwa achieves state-of-the-art performance on VQA benchmarks (improving from 94% to 95%) and substantial gains on visual grounding tasks (from 7% to 47%).</div>
<div class="mono" style="margin-top:8px">视觉令牌剪枝可加速视觉语言模型，但现有方法在视觉问答任务中表现良好，却在视觉定位任务上因空间参考框架丢失而性能大幅下降。为此，研究者提出Nüwa，一个两阶段剪枝框架：第一阶段在视觉编码器后，采用受群体智能启发的分离、对齐和聚合操作以保留信息丰富的全局空间锚点；第二阶段在大语言模型内进行文本引导的剪枝以保留任务相关视觉令牌。实验表明，Nüwa在多个视觉问答基准上达到最优性能（从94%提升至95%），并在视觉定位任务上取得显著改进（从7%提升至47%）。</div>
</details>
</div>
<div class="card">
<div class="title">ViThinker: Active Vision-Language Reasoning via Dynamic Perceptual Querying</div>
<div class="meta-line">Authors: Weihang You, Qingchan Zhu, David Liu, Yi Pan, Geng Yuan, Hanqi Jiang</div>
<div class="meta-line">First: 2026-02-02T22:29:57+00:00 · Latest: 2026-02-02T22:29:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02873v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02873v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought (CoT) reasoning excels in language models but struggles in vision-language models due to premature visual-to-text conversion that discards continuous information such as geometry and spatial layout. While recent methods enhance CoT through static enumeration or attention-based selection, they remain passive, i.e., processing pre-computed inputs rather than actively seeking task-relevant details. Inspired by human active perception, we introduce ViThinker, a framework that enables vision-language models to autonomously generate decision (query) tokens triggering the synthesis of expert-aligned visual features on demand. ViThinker internalizes vision-expert capabilities during training, performing generative mental simulation during inference without external tool calls. Through a two-stage curriculum: first distilling frozen experts into model parameters, then learning task-driven querying via sparsity penalties, i.e., ViThinker discovers minimal sufficient perception for each reasoning step. Evaluations across vision-centric benchmarks demonstrate consistent improvements, validating that active query generation outperforms passive approaches in both perceptual grounding and reasoning accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViThinker：基于动态感知查询的主动视觉语言推理</div>
<div class="mono" style="margin-top:8px">思维链推理在语言模型中表现优异，但在视觉语言模型中因过早将视觉信息转换为文本而丢失几何结构与空间布局等连续信息，导致效果受限。现有方法通过静态枚举或注意力选择增强思维链，但仍属被动处理预计算输入，未能主动获取任务相关细节。受人类主动感知机制启发，我们提出ViThinker框架，使视觉语言模型能自主生成决策（查询）标记，按需触发专家对齐的视觉特征合成。ViThinker在训练阶段内化视觉专家能力，在推理时无需调用外部工具即可执行生成式心理模拟。通过两阶段课程学习：先蒸馏冻结专家至模型参数，再通过稀疏性惩罚学习任务驱动的查询机制，ViThinker能为每个推理步骤发现最小充分感知。在视觉中心基准测试中的评估显示其性能持续提升，验证了主动查询生成在感知基础与推理准确性上均优于被动方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of Chain-of-Thought reasoning in vision-language models, where converting visual inputs to text prematurely discards continuous spatial and geometric information, and passive methods fail to actively seek task-relevant details. The proposed ViThinker framework enables active reasoning by having the model autonomously generate decision tokens that trigger the on-demand synthesis of expert-aligned visual features, internalizing expert capabilities through a two-stage training curriculum involving distillation and sparsity-constrained learning of task-driven queries. Experimental results on vision-centric benchmarks show that this active query generation approach consistently improves both perceptual grounding and reasoning accuracy over passive methods.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型中思维链推理的局限性，即视觉输入过早转换为文本会丢失关键的连续空间信息，且现有增强方法仍是被动的。提出的ViThinker框架通过让模型自主生成决策令牌来触发按需合成的专家对齐视觉特征，从而实现主动推理，该框架通过一个两阶段的训练课程（包括蒸馏和利用稀疏性惩罚学习任务驱动的查询）将专家能力内化。在多个以视觉为中心的基准测试上的实验结果表明，该方法取得了持续的性能提升，验证了这种主动查询生成方法在感知基础和推理准确性上均优于被动方法。</div>
</details>
</div>
<div class="card">
<div class="title">AdaptMMBench: Benchmarking Adaptive Multimodal Reasoning for Mode Selection and Reasoning Process</div>
<div class="meta-line">Authors: Xintong Zhang, Xiaowen Zhang, Jongrong Wu, Zhi Gao, Shilin Yan, Zhenxin Diao, Kunpeng Gao, Xuanyan Chen, Yuwei Wu, Yunde Jia, Qing Li</div>
<div class="meta-line">First: 2026-02-02T19:00:27+00:00 · Latest: 2026-02-02T19:00:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02676v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02676v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adaptive multimodal reasoning has emerged as a promising frontier in Vision-Language Models (VLMs), aiming to dynamically modulate between tool-augmented visual reasoning and text reasoning to enhance both effectiveness and efficiency. However, existing evaluations rely on static difficulty labels and simplistic metrics, which fail to capture the dynamic nature of difficulty relative to varying model capacities. Consequently, they obscure the distinction between adaptive mode selection and general performance while neglecting fine-grained process analyses. In this paper, we propose AdaptMMBench, a comprehensive benchmark for adaptive multimodal reasoning across five domains: real-world, OCR, GUI, knowledge, and math, encompassing both direct perception and complex reasoning tasks. AdaptMMBench utilizes a Matthews Correlation Coefficient (MCC) metric to evaluate the selection rationality of different reasoning modes, isolating this meta-cognition ability by dynamically identifying task difficulties based on models&#x27; capability boundaries. Moreover, AdaptMMBench facilitates multi-dimensional process evaluation across key step coverage, tool effectiveness, and computational efficiency. Our evaluation reveals that while adaptive mode selection scales with model capacity, it notably decouples from final accuracy. Conversely, key step coverage aligns with performance, though tool effectiveness remains highly inconsistent across model architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaptMMBench：面向模态选择与推理过程的自适应多模态推理基准评测</div>
<div class="mono" style="margin-top:8px">自适应多模态推理已成为视觉语言模型领域的前沿方向，其核心在于动态协调工具增强的视觉推理与文本推理，以提升效能与效率。然而现有评估方法依赖静态难度标签与简化指标，无法捕捉难度随模型能力变化的动态特性，既模糊了自适应模态选择与通用性能的界限，也缺乏细粒度过程分析。本文提出AdaptMMBench——一个涵盖现实场景、OCR、图形界面、知识与数学五大领域的自适应多模态推理综合基准，包含直接感知与复杂推理任务。该基准采用马修斯相关系数量化评估不同推理模态的选择合理性，通过动态识别模型能力边界对应的任务难度，实现对元认知能力的独立评测。此外，AdaptMMBench支持关键步骤覆盖度、工具效用与计算效率的多维过程评估。实验表明：自适应模态选择能力虽随模型规模提升，却与最终准确率显著解耦；而关键步骤覆盖度与性能正相关，但工具效用在各模型架构间仍存在高度不一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of dynamic evaluation for adaptive multimodal reasoning in Vision-Language Models (VLMs), which aims to switch between tool-augmented visual and text reasoning. The authors introduce AdaptMMBench, a benchmark spanning five domains, which uses a Matthews Correlation Coefficient metric to assess the rationality of mode selection by dynamically determining task difficulty relative to model capabilities, and it also enables multi-dimensional process evaluation. Key findings show that adaptive selection ability improves with model size but is decoupled from final accuracy, while coverage of key reasoning steps correlates with performance, though tool effectiveness varies significantly across architectures.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型中自适应多模态推理缺乏动态评估的问题，该能力旨在模型根据任务动态切换工具增强的视觉推理与文本推理。作者提出了AdaptMMBench，一个涵盖五个领域的综合基准，它使用马修斯相关系数指标，通过根据模型能力边界动态确定任务难度来评估模式选择的合理性，并支持对关键步骤覆盖、工具有效性和计算效率的多维过程评估。实验结果表明，自适应选择能力随模型容量提升，但与最终准确率脱钩；关键步骤覆盖与性能相关，而工具有效性在不同模型架构间存在高度不一致。</div>
</details>
</div>
<div class="card">
<div class="title">LongVPO: From Anchored Cues to Self-Reasoning for Long-Form Video Preference Optimization</div>
<div class="meta-line">Authors: Zhenpeng Huang, Jiaqi Li, Zihan Jia, Xinhao Li, Desen Meng, Lingxue Song, Xi Chen, Liang Li, Limin Wang</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-02-02T17:03:37+00:00 · Latest: 2026-02-02T17:03:37+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02341v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02341v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present LongVPO, a novel two-stage Direct Preference Optimization framework that enables short-context vision-language models to robustly understand ultra-long videos without any long-video annotations. In Stage 1, we synthesize preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying visual-similarity and question-specificity filtering to mitigate positional bias and ensure unambiguous supervision. We also approximate the reference model&#x27;s scoring over long contexts by evaluating only the anchor clip, reducing computational overhead. In Stage 2, we employ a recursive captioning pipeline on long videos to generate scene-level metadata, then use a large language model to craft multi-segment reasoning queries and dispreferred responses, aligning the model&#x27;s preferences through multi-segment reasoning tasks. With only 16K synthetic examples and no costly human labels, LongVPO outperforms the state-of-the-art open-source models on multiple long-video benchmarks, while maintaining strong short-video performance (e.g., on MVBench), offering a scalable paradigm for efficient long-form video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LongVPO：从锚定线索到自推理的长视频偏好优化</div>
<div class="mono" style="margin-top:8px">我们提出LongVPO，一种新颖的两阶段直接偏好优化框架，使短上下文视觉语言模型能够稳健理解超长视频，无需任何长视频标注。第一阶段，通过将问题锚定至独立短视频片段、穿插干扰项，并应用视觉相似性与问题特异性过滤来合成偏好三元组，以缓解位置偏差并确保明确的监督。我们仅通过评估锚定片段来近似参考模型在长上下文中的评分，从而降低计算开销。第二阶段，对长视频采用递归描述流程生成场景级元数据，随后利用大语言模型构建多片段推理查询与负向响应，通过多片段推理任务对齐模型偏好。仅使用16K合成样本且无需昂贵人工标注，LongVPO在多个长视频基准测试中超越最先进开源模型，同时保持强大的短视频性能（如在MVBench上），为高效长视频理解提供了可扩展范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling short-context vision-language models to understand ultra-long videos without requiring expensive long-video annotations. The proposed LongVPO method is a two-stage Direct Preference Optimization framework: Stage 1 synthesizes preference triples by anchoring questions to individual short clips, interleaving them with distractors, and applying filtering to mitigate bias, while approximating reference model scoring on only the anchor clip to reduce computation; Stage 2 employs a recursive captioning pipeline on long videos to generate scene-level metadata, then uses a large language model to craft multi-segment reasoning queries and dispreferred responses for alignment. Experimental results show that with only 16K synthetic examples and no human labels, LongVPO outperforms state-of-the-art open-source models on multiple long-video benchmarks while maintaining strong short-video performance on MVBench, offering a scalable paradigm for efficient long-form video understanding.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决如何让短上下文视觉语言模型理解超长视频，同时避免使用昂贵的长视频标注数据。提出的LongVPO方法是一个两阶段的直接偏好优化框架：第一阶段通过将问题锚定到单个短视频片段、插入干扰项并进行过滤来合成偏好三元组，以减轻位置偏差，并通过近似参考模型评分来降低计算成本；第二阶段采用递归描述管道和大语言模型生成多片段推理查询，以实现偏好对齐。实验结果表明，仅使用16K个合成示例且无需人工标注，该模型在多个长视频基准测试中超越了最先进的开源模型，同时在MVBench等短视频任务上保持了强劲性能。</div>
</details>
</div>
<div class="card">
<div class="title">U2-BENCH: Benchmarking Large Vision-Language Models on Ultrasound Understanding</div>
<div class="meta-line">Authors: Anjie Le, Henan Liu, Yue Wang, Zhenyu Liu, Rongkun Zhu, Taohan Weng, Jinze Yu, Boyang Wang, Yalun Wu, Kaiwen Yan, Quanlin Sun, Meirui Jiang, Jialun Pei, Siya Liu, Haoyun Zheng, Zhoujun Li, Alison Noble, Jacques Souquet, Xiaoqing Guo, Manxi Lin, Hongcheng Guo</div>
<div class="meta-line">First: 2025-05-23T11:48:48+00:00 · Latest: 2026-02-02T13:10:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17779v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.17779v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ultrasound is a widely-used imaging modality critical to global healthcare, yet its interpretation remains challenging due to its varying image quality on operators, noises, and anatomical structures. Although large vision-language models (LVLMs) have demonstrated impressive multimodal capabilities across natural and medical domains, their performance on ultrasound remains largely unexplored. We introduce U2-BENCH, the first comprehensive benchmark to evaluate LVLMs on ultrasound understanding across classification, detection, regression, and text generation tasks. U2-BENCH aggregates 7,241 cases spanning 15 anatomical regions and defines 8 clinically inspired tasks, such as diagnosis, view recognition, lesion localization, clinical value estimation, and report generation, across 50 ultrasound application scenarios. We evaluate 23 state-of-the-art LVLMs, both open- and closed-source, general-purpose and medical-specific. Our results reveal strong performance on image-level classification, but persistent challenges in spatial reasoning and clinical language generation. U2-BENCH establishes a rigorous and unified testbed to assess and accelerate LVLM research in the uniquely multimodal domain of medical ultrasound imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U2-BENCH：大型视觉语言模型在超声理解领域的基准测试</div>
<div class="mono" style="margin-top:8px">超声是一种广泛使用的成像方式，对全球医疗保健至关重要，但由于其图像质量受操作者、噪声和解剖结构影响而多变，解读仍具挑战性。尽管大型视觉语言模型（LVLMs）在自然和医学领域展现出卓越的多模态能力，但其在超声领域的性能尚未得到充分探索。我们推出U2-BENCH，这是首个全面评估LVLMs在超声理解任务（涵盖分类、检测、回归和文本生成）的基准。U2-BENCH汇集了7,241个病例，覆盖15个解剖区域，并定义了8项临床启发任务（如诊断、视图识别、病灶定位、临床价值估计和报告生成），涉及50个超声应用场景。我们评估了23个最先进的LVLMs，包括开源与闭源、通用与医学专用模型。结果显示，模型在图像级分类任务中表现强劲，但在空间推理和临床语言生成方面仍面临持续挑战。U2-BENCH为医学超声成像这一独特多模态领域建立了一个严谨统一的测试平台，以评估并加速LVLM研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the widespread clinical use of ultrasound and its inherent interpretation challenges, coupled with the lack of evaluation for large vision-language models (LVLMs) in this domain. The method involves constructing U2-BENCH, a comprehensive benchmark comprising 7,241 ultrasound cases across 15 anatomical regions and 50 application scenarios, designed to evaluate LVLMs on eight clinically inspired tasks spanning classification, detection, regression, and text generation. Key experimental findings from evaluating 23 state-of-the-art LVLMs reveal that while these models perform strongly on image-level classification tasks, they exhibit persistent difficulties in spatial reasoning and generating clinically accurate language.</div>
<div class="mono" style="margin-top:8px">该研究针对超声图像解读的挑战——超声作为关键医疗成像方式，其图像质量易受操作者、噪声和解剖结构影响而难以分析，旨在探索大型视觉语言模型在此领域的应用能力。研究提出了U2-BENCH，一个包含7,241个病例、覆盖15个解剖区域和8项临床任务（如诊断和报告生成）的综合基准，用于评估23个先进的大型视觉语言模型。实验结果表明，这些模型在图像级分类任务上表现良好，但在空间推理和生成临床准确语言方面仍存在困难，强调了在超声特定多模态理解方面需进一步研发。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
