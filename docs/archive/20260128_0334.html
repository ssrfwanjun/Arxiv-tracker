<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 03:34</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0334</div>
    <div class="row"><div class="card">
<div class="title">ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models</div>
<div class="meta-line">Authors: Brian Ondov, Chia-Hsuan Chang, Yujia Zhou, Mauro Giuffrè, Hua Xu</div>
<div class="meta-line">First: 2026-01-26T18:58:46+00:00 · Latest: 2026-01-26T18:58:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18796v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18796v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ctELM：基于嵌入语言模型解码与操控临床试验嵌入</div>
<div class="mono" style="margin-top:8px">文本嵌入已成为各类语言应用的核心组件，但现有方法在解释、探索与逆向重构嵌入空间方面存在局限，这降低了透明度并限制了潜在的生成式应用场景。本研究采用近期提出的嵌入语言模型方法，将大语言模型与临床试验嵌入对齐。我们开发了开源、领域无关的ELM架构与训练框架，设计了针对临床试验的训练任务，并引入经专家验证的合成数据集。通过系统训练一系列ELM模型，我们探究了不同任务与训练策略的影响。最终模型ctELM能够仅凭嵌入准确描述和比较未见过的临床试验，并能从新向量生成合理的临床试验方案。进一步实验表明，生成的试验摘要能响应沿年龄、性别等概念向量移动嵌入的操作。我们公开的ELM实现与实验结果将助力生物医学及其他领域的大语言模型与嵌入空间对齐研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance the interpretability and generative utility of text embeddings in specialized domains, this work adapts the Embedding Language Model (ELM) method to align large language models with embeddings of clinical trial texts. The authors develop an open-source, domain-agnostic ELM framework, design specific training tasks for clinical trials, and create an expert-validated synthetic dataset to train a series of models. Experimental results show that the final model, ctELM, can accurately describe and compare unseen clinical trials from their embeddings, generate plausible trial abstracts, and produce text that responsively changes when embeddings are manipulated along concept vectors like subject age and sex.</div>
<div class="mono" style="margin-top:8px">为提高文本嵌入在专业领域的可解释性和生成能力，本研究采用嵌入语言模型（ELM）方法，将大语言模型与临床试验文本的嵌入表示对齐。作者开发了一个开源、领域无关的ELM架构与训练框架，设计了针对临床试验的训练任务，并引入了经专家验证的合成数据集。实验结果表明，最终模型ctELM能够仅从嵌入向量准确描述和比较未见过的临床试验，生成合理的试验摘要，并且当嵌入沿受试者年龄、性别等概念向量移动时，生成的文本会做出相应的可预测变化。</div>
</details>
</div>
<div class="card">
<div class="title">Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</div>
<div class="meta-line">Authors: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</div>
<div class="meta-line">First: 2026-01-26T18:57:00+00:00 · Latest: 2026-01-26T18:57:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18795v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18795v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复用计算量：通过条件化极离策略前缀扩展强化学习在难题上的应用</div>
<div class="mono" style="margin-top:8px">传统用于大语言模型推理的强化学习方法在难题上浪费算力，因为正确的同策略轨迹稀少、策略梯度消失且学习停滞。为引导更高效的强化学习，我们考虑以离策略轨迹形式复用旧采样计算量（来自先前的推理或强化学习训练）。标准离策略方法通过离策略数据进行监督，导致强化学习优化过程不稳定。我们提出PrefixRL方法：以成功离策略轨迹的前缀为条件，运行同策略强化学习来完成后续轨迹，从而规避离策略不稳定性。该方法通过调节离策略前缀长度来控制问题难度，从而增强难题上的学习信号。我们证明PrefixRL目标不仅与标准强化学习目标一致，且具有更高的样本效率。实验中发现反向泛化现象：仅在前缀化问题上训练可泛化至分布外无前缀任务，且学习策略常与前缀策略不同。实验中通过基础模型的拒绝采样获取离策略轨迹，形成自我改进循环。在复杂推理问题上，即使计入初始拒绝采样的计算成本，PrefixRL达到相同训练奖励的速度仍比最强基线（离策略数据监督微调后强化学习）快2倍，最终奖励提升3倍。该优势可迁移至保留基准测试，且当离策略轨迹源自不同模型家族时仍保持有效，验证了其在实际场景中的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of standard reinforcement learning (RL) for large language models on hard reasoning problems, where on-policy traces are scarce and policy gradients vanish, leading to stalled learning. The proposed method, PrefixRL, reuses off-policy traces from prior inference or training by conditioning on their prefixes and performing on-policy RL to complete them, thereby avoiding the instabilities of direct off-policy supervision and modulating problem difficulty via prefix length. Experimental results show that PrefixRL achieves the same training reward twice as fast as strong baselines, even after accounting for initial rejection sampling compute, and increases final reward by threefold, with gains transferring to held-out benchmarks and remaining effective when off-policy traces come from different model families.</div>
<div class="mono" style="margin-top:8px">该研究针对大型语言模型在困难推理问题上标准强化学习方法效率低下的问题，其中在线策略学习信号稀疏且梯度消失。为解决此问题，作者提出了PrefixRL方法，该方法以通过拒绝采样获取的成功离线策略轨迹的前缀为条件，然后执行在线策略强化学习来完成轨迹，从而避免了直接离线策略监督的不稳定性，同时调节问题难度。实验结果表明，PrefixRL在困难问题上达到相同训练奖励的速度比强基线快两倍，并将最终奖励提升三倍，其增益可迁移到保留基准测试中，且即使离线策略数据来自不同模型家族也依然有效。</div>
</details>
</div>
<div class="card">
<div class="title">Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets</div>
<div class="meta-line">Authors: Iaroslav Chelombitko, Mika Hämäläinen, Aleksey Komissarov</div>
<div class="meta-line">First: 2026-01-26T18:55:28+00:00 · Latest: 2026-01-26T18:55:28+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figues, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18791v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing &#x27;glottosets&#x27; from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p &lt; 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于子词的跨242种语言比较语言学：利用维基百科语料集</div>
<div class="mono" style="margin-top:8px">本研究采用子词方法对242种拉丁与西里尔文字语言进行大规模比较分析。通过维基百科词典构建&#x27;语料集&#x27;，提出基于字节对编码（BPE）的跨语言同步比较框架。利用基于排序的子词向量，系统分析词汇重叠度、词汇分化度及语言相似性。评估显示：在15种语言中，BPE切分与语素边界的对齐度较随机基线提升95%（F1值0.34对0.15）。BPE词汇相似度与语言谱系关联性显著相关（Mantel r=0.329, p&lt;0.001），罗曼语族形成最紧密聚类（平均距离0.51），跨语系语言对呈现明显分离（0.82）。对26,939个跨语言同形词分析发现，48.7%在相关语言中获得不同切分，其变异程度与谱系距离相关。本研究在统一分析框架下，为类型学多样语言的词汇模式提供了量化宏观语言学见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to conduct a large-scale, quantitative comparison of lexical patterns across diverse languages, addressing the need for unified frameworks in macro-linguistics. The method constructs &#x27;glottosets&#x27; from Wikipedia lexicons for 242 Latin and Cyrillic-script languages and employs Byte-Pair Encoding (BPE) to generate rank-based subword vectors for analyzing vocabulary overlap, lexical divergence, and language similarity. Key findings show that BPE segmentation aligns with morpheme boundaries significantly better than a random baseline (F1=0.34 vs. 0.15), BPE vocabulary similarity correlates with genetic language relatedness (Mantel r=0.329, p&lt;0.001) with Romance languages forming the tightest cluster, and analysis of cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation tied to phylogenetic distance.</div>
<div class="mono" style="margin-top:8px">本研究旨在对242种不同的语言进行大规模的比较语言学分析。其方法是从维基百科词典中构建&#x27;glottosets&#x27;，并采用字节对编码（BPE）生成基于排名的子词向量，用于分析词汇重叠、词汇差异和语言相似性。关键的实验结果表明，BPE分割与语素边界的对齐显著优于随机基线（F1=0.34对比0.15）；BPE词汇相似性与语言遗传亲缘关系相关（Mantel r=0.329），其中罗曼语族语言形成最紧密的聚类；对跨语言同形异义词的分析显示，48.7%的此类词在相关语言中获得了不同的分割，这种差异与谱系距离相关。</div>
</details>
</div>
<div class="card">
<div class="title">Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System</div>
<div class="meta-line">Authors: Tiffany Wang, Yuqian Sun, Yi Wang, Melissa Roemmele, John Joon Young Chung, Max Kreminski</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2026-01-26T18:51:20+00:00 · Latest: 2026-01-26T18:51:20+00:00</div>
<div class="meta-line">Comments: Extended abstract presented at the 2025 Wordplay Workshop at EMNLP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的交互式叙事设计技术：以Dramamancer系统为例</div>
<div class="mono" style="margin-top:8px">大语言模型的兴起为连接作者意图与玩家能动性的交互式叙事提供了新范式。本文以Dramamancer系统为例探讨该范式，该系统利用大语言模型将作者创作的故事框架转化为玩家驱动的叙事流程。本扩展摘要概述了与该系统相关的设计技术和评估考量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to leverage Large Language Models (LLMs) to reconcile authorial intent with player agency in interactive narratives. The method involves the Dramamancer system, which employs an LLM to dynamically generate player-driven playthroughs from author-created story schemas. The work outlines specific design techniques and considerations for evaluating such systems, as presented in an extended abstract at EMNLP.</div>
<div class="mono" style="margin-top:8px">本研究旨在利用大型语言模型（LLMs）来协调互动叙事中的作者意图与玩家能动性，以解决结构化叙事与用户自由度之间的平衡难题。其方法涉及Dramamancer系统，该系统利用LLM动态解释并改编作者定义的故事图式，以生成响应式的、由玩家驱动的叙事流程。该扩展摘要介绍了相关的设计技术，并概述了对此类系统进行评估的考量因素，但所提供的文本未详述具体的实验结果。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic</div>
<div class="meta-line">Authors: Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani</div>
<div class="meta-line">First: 2026-01-26T18:50:21+00:00 · Latest: 2026-01-26T18:50:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18783v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高速公路卡车战术决策的多目标强化学习效率优化研究</div>
<div class="mono" style="margin-top:8px">在高速公路驾驶中平衡安全性、效率与运营成本对重型车辆构成复杂的决策难题。传统标量奖励函数通过聚合竞争性目标往往模糊其权衡结构。本研究提出基于近端策略优化的多目标强化学习框架，通过可扩展仿真平台学习能显式表征目标权衡的连续策略集。该方法生成的三目标帕累托最优策略集涵盖：以碰撞率与任务完成度衡量的安全性、能耗成本量化的能源效率、驾驶员成本量化的时间效率。所得平滑可解释的帕累托前沿支持沿不同竞争目标灵活选择驾驶行为，无需重新训练即可实现策略间无缝切换，为自动驾驶卡车提供鲁棒自适应决策方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of balancing safety, efficiency, and cost in highway driving for heavy-duty trucks, where conventional scalar reward functions obscure the inherent trade-offs between these competing objectives. The method employs a Proximal Policy Optimization-based multi-objective reinforcement learning framework to learn a continuous set of Pareto-optimal policies, explicitly capturing trade-offs among safety (collisions and completion), energy efficiency (energy cost), and time efficiency (driver cost). Experimental evaluation on a scalable truck simulation platform demonstrates that the approach yields a smooth and interpretable Pareto frontier, enabling flexible selection of driving behaviors and seamless transitions between policies without retraining, thus providing a robust and adaptive decision-making strategy for autonomous trucking.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决重型车辆在高速公路驾驶中平衡安全性、效率和运营成本的挑战，传统的标量奖励函数往往会掩盖这些竞争目标之间的内在权衡。该方法采用基于近端策略优化的多目标强化学习框架，学习一组连续的帕累托最优策略，明确捕捉了安全性（以碰撞和任务完成度衡量）、能源效率和时间效率之间的权衡。在可扩展的卡车模拟平台上的实验评估表明，该方法能产生平滑且可解释的帕累托前沿，允许灵活选择驾驶行为，并能在无需重新训练的情况下在不同策略间无缝切换，从而为自动驾驶卡车应用提供了一种鲁棒的自适应决策策略。</div>
</details>
</div>
<div class="card">
<div class="title">POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</div>
<div class="meta-line">Authors: Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-26T18:47:21+00:00 · Latest: 2026-01-26T18:47:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POPE：通过特权在线策略探索学习解决难题的推理能力</div>
<div class="mono" style="margin-top:8px">强化学习（RL）提升了大型语言模型（LLM）的推理能力，但现有先进方法仍难以从许多训练问题中学习。在难题上，在线策略RL很少能探索出正确的轨迹，导致零奖励且缺乏驱动改进的学习信号。我们发现，经典RL中解决探索问题的自然方法（如熵奖励、放宽重要性比率裁剪或直接优化pass@k目标）均无法解决此问题，且常破坏优化稳定性而不提升可解性。一种自然替代方案是利用简单问题的迁移学习，但我们证明在RL训练中混合简单与难题会因优化射线干扰而适得其反——优化会聚焦于已可解问题，反而阻碍对难题的进展。为此，我们提出特权在线策略探索（POPE），该方法利用人类或其他先知解决方案作为特权信息引导难题探索，而非将其作为训练目标（如离线策略RL或从监督微调预热）。POPE通过为难题添加先知解的前缀，使RL在引导轨迹中获得非零奖励。关键在于，通过指令遵循与推理的协同作用，习得的行为能迁移回原始无引导问题。实验表明，POPE显著扩展了可解问题集，并在挑战性推理基准上大幅提升性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the exploration challenge in reinforcement learning for large language models on hard reasoning problems, where on-policy methods often fail to encounter any correct rollouts, resulting in no learning signal. The proposed method, Privileged On-Policy Exploration (POPE), uses oracle solutions as privileged information to guide exploration by augmenting hard problems with prefixes of these solutions, enabling non-zero rewards during guided rollouts while ensuring the learned behavior transfers back to the original problems. Experiments show that POPE significantly expands the set of solvable problems and improves performance on challenging reasoning benchmarks compared to alternatives like entropy bonuses or mixing easy and hard problems, which were found to be ineffective or counterproductive.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型在困难推理问题上强化学习探索的挑战，其中在线策略方法常因无法生成正确轨迹而缺乏学习信号。提出的方法——特权在线策略探索（POPE），利用人类或其他预言机解决方案作为特权信息，通过为难题添加预言机解的前缀来引导探索，从而在引导过程中获得非零奖励，并确保学习到的行为能迁移回原始问题。实验结果表明，POPE显著扩大了可解决问题的范围，并在具有挑战性的推理基准上大幅提升了性能，优于熵奖励或混合难易问题等替代方法。</div>
</details>
</div>
<div class="card">
<div class="title">PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation</div>
<div class="meta-line">Authors: Abhishek Divekar, Anirban Majumder</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-26T18:46:49+00:00 · Latest: 2026-01-26T18:46:49+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18777v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18777v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRECISE：基于预测驱动排序估计降低大语言模型评估偏差</div>
<div class="mono" style="margin-top:8px">传统评估搜索、排序与RAG系统质量需大量人工相关性标注。近期部分部署系统尝试将大语言模型（LLM）作为自动评估工具，但其固有偏差阻碍了直接用于指标估计。本文提出扩展预测驱动推断（PPI）的统计框架，通过结合少量人工标注与LLM判断，为需要子实例标注的指标生成可靠估计。该方法仅需100条人工标注查询与10,000个未标注样本，较传统方法显著降低标注需求。我们构建的PRECISE框架针对基于LLM的查询重构应用进行相关性提升推断，将PPI扩展至查询-文档层级的子实例标注。通过重构指标集成空间，计算复杂度从O(2^|C|)降至O(2^K)，其中|C|代表语料库规模（百万量级）。在主流检索数据集上的实验表明，该方法能降低关键业务指标Precision@K的估计方差，并在低资源场景中有效校正LLM偏差。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the bias and high annotation costs in using Large Language Models (LLMs) as automated judges for evaluating search, ranking, and RAG systems. The method introduces PRECISE, a statistical framework that extends Prediction-Powered Inference (PPI) to combine a small set of human annotations (e.g., 100 queries) with a large volume of LLM judgments on unlabeled data (e.g., 10,000 examples) for reliable metric estimation, specifically reformulating the problem to handle sub-instance annotations at the query-document level and reducing computational complexity from exponential to manageable levels. Experimental results on retrieval datasets show that PRECISE effectively corrects for LLM bias, reduces the variance of estimates for metrics like Precision@K, and operates reliably in low-resource settings.</div>
<div class="mono" style="margin-top:8px">为应对搜索和排序系统评估中人工标注成本高昂以及使用大语言模型作为自动评估器存在固有偏差的问题，本研究提出了PRECISE框架，该框架扩展了预测驱动推断方法。该方法将少量人工标注查询与大量未标注数据上的大语言模型判断相结合，以可靠地估计子实例级别的指标，并通过重构指标积分空间将计算复杂度从指数级降低。在多个检索数据集上的实验表明，PRECISE能有效减少关键指标估计的方差，校正大语言模型的偏差，相比传统方法显著降低了标注需求。</div>
</details>
</div>
<div class="card">
<div class="title">Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory</div>
<div class="meta-line">Authors: Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang</div>
<div class="meta-line">First: 2026-01-26T18:42:33+00:00 · Latest: 2026-01-26T18:42:33+00:00</div>
<div class="meta-line">Comments: Dep-Search 1st version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18771v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#x27; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dep-Search：基于持久化记忆的依赖感知推理轨迹学习框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在复杂推理任务中展现出卓越能力，尤其是在结合搜索机制系统探索外部知识库时。该领域已从传统检索增强生成（RAG）框架演进至更复杂的搜索框架，通过显式搜索策略协调多步推理。然而，现有搜索框架仍严重依赖隐式自然语言推理来确定搜索策略及跨推理步骤的信息利用方式。这种隐式推理依赖在管理子问题间依赖关系、高效复用已检索知识、通过强化学习优化搜索策略等方面存在根本性挑战。为突破这些局限，我们提出Dep-Search——一种依赖感知搜索框架，通过GRPO整合结构化推理、检索与持久化记忆，实现了对现有搜索框架的超越。Dep-Search引入显式控制机制，使模型能够：分解具有依赖关系的问题、按需检索信息、从记忆访问历史知识、将长推理上下文总结为可复用的记忆单元。在七个多样化问答数据集上的实验表明，Dep-Search显著提升LLMs处理复杂多跳推理任务的能力，在不同规模模型上均取得对基准模型的实质性改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in existing search-augmented LLMs, which rely on implicit natural language reasoning for search strategies, leading to challenges in managing dependencies between sub-questions and reusing retrieved knowledge. The proposed method, Dep-Search, introduces a dependency-aware framework that integrates structured reasoning, retrieval, and persistent memory using GRPO, enabling explicit control for question decomposition with dependencies, on-demand retrieval, memory access, and summarization of reasoning contexts into reusable entries. Experimental results on seven diverse QA datasets show that Dep-Search significantly improves LLMs&#x27; performance on complex multi-hop reasoning tasks, achieving substantial gains over strong baselines across various model scales.</div>
<div class="mono" style="margin-top:8px">本研究针对现有基于搜索增强的大语言模型依赖隐式自然语言推理来制定搜索策略的局限性，这导致在管理子问题间依赖关系和复用检索知识方面存在挑战。所提出的方法Dep-Search引入了一个依赖感知的框架，通过GRPO整合结构化推理、检索和持久性记忆，实现了对具有依赖关系的问题分解、按需检索、记忆访问以及将长推理上下文总结为可复用条目的显式控制。在七个多样化问答数据集上的实验结果表明，Dep-Search显著提升了大语言模型处理复杂多跳推理任务的能力，在不同模型规模上均大幅超越了强基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation</div>
<div class="meta-line">Authors: Andrei Kozyrev, Nikita Khramov, Gleb Solovev, Anton Podkopaev</div>
<div class="meta-line">First: 2025-05-28T20:26:11+00:00 · Latest: 2026-01-26T18:27:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22846v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.22846v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive Theorem Proving was repeatedly shown to be fruitful when combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We identify retrieval-based premise selection as a central component of effective Rocq proof generation and propose a novel approach based on a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator&#x27;s performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and demonstrate that incorporating multi-agent debate during the planning stage increases the proof success rate by 20% overall and nearly doubles it for complex theorems, while the reflection mechanism further enhances stability and consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RocqStar：利用相似性驱动的检索与智能体系统进行Rocq生成</div>
<div class="mono" style="margin-top:8px">交互式定理证明与生成式人工智能的结合已被反复证明富有成效。本文评估了多种Rocq生成方法，并阐明了潜在的改进方向。我们指出基于检索的前提选择是有效生成Rocq证明的核心环节，并提出了一种基于自注意力嵌入模型的新方法。对所设计方法的评估显示，生成器性能相对提升最高达28%。我们采用专为形式化验证设计的智能体多阶段系统来解决Rocq证明书写问题，并验证了其高效性。通过消融实验证明：在规划阶段引入多智能体辩论机制可使整体证明成功率提升20%，对复杂定理的证明成功率提升近一倍，而反思机制则进一步增强了系统的稳定性与一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to improve the automation of interactive theorem proving in Rocq by enhancing proof generation. The method introduces a similarity-driven retrieval system using a self-attentive embedder model for premise selection and employs a multi-stage agentic system with planning-stage multi-agent debate and a reflection mechanism. Key experimental results show a 28% relative performance increase from the retrieval model, while the agentic system boosts the overall proof success rate by 20%, nearly doubling it for complex theorems, with reflection further improving stability.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过改进当前生成式人工智能方法，提升交互式定理证明中Rocq证明的生成能力。所提出的方法整合了一个新颖的基于相似性的检索系统（使用自注意力嵌入模型进行前提选择）和一个为形式化验证定制的多阶段智能体系统，该系统包括采用多智能体辩论的规划阶段和反思机制。实验结果表明，检索组件使生成器性能相对提升高达28%，而规划阶段的多智能体辩论将整体证明成功率提高了20%，对于复杂定理的证明成功率更是提升了近一倍，反思机制则进一步增强了系统的稳定性和一致性。</div>
</details>
</div>
<div class="card">
<div class="title">$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks</div>
<div class="meta-line">Authors: Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah</div>
<div class="meta-line">First: 2026-01-26T18:25:07+00:00 · Latest: 2026-01-26T18:25:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18754v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18754v1">PDF</a> · <a href="https://github.com/maferrag/AlphaSecBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings.
  We introduce $α^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $α^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $α^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage).
  We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $α^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>$α^3$-SecBench：面向6G网络中基于大语言模型的无人机智能体的大规模安全、韧性与可信评估套件</div>
<div class="mono" style="margin-top:8px">自主无人机系统正日益部署于安全关键的网络化环境中，需在恶意攻击者存在时保持可靠运行。现有基准测试主要评估基于大语言模型的无人机智能体在推理、导航和效率方面的表现，而对对抗条件下安全性、韧性和可信度的系统性评估仍属空白，尤其在新兴6G环境中。
我们提出$α^3$-SecBench，首个面向基于大语言模型的无人机智能体的大规模评估套件，用于测试其在真实对抗干扰下的安全感知自主能力。该框架基于$α^3$-Bench的多轮对话式无人机任务，在良性任务场景中叠加了20,000个经过验证的安全覆盖攻击场景，覆盖感知、认知、规划、控制、通信、边缘/云基础设施及大语言模型推理等七个自主层级。$α^3$-SecBench从三个正交维度评估智能体：安全性（攻击检测与漏洞归因）、韧性（安全降级行为）和可信度（策略合规的工具使用）。
我们使用从113,475个任务（涵盖175种威胁类型）中采样的数千个对抗增强无人机任务场景，评估了来自主要工业供应商和顶尖AI实验室的23个前沿大语言模型。结果显示，虽然多数模型能可靠检测异常行为，但在有效缓解、漏洞归因和可信控制行动方面表现参差不齐。标准化综合得分介于12.9%至57.1%之间，凸显了异常检测与安全感知自主决策能力间的显著差距。本套件已在GitHub开源：https://github.com/maferrag/AlphaSecBench</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of systematic evaluation of security, resilience, and trust for LLM-based UAV agents operating in adversarial environments, especially within emerging 6G networks. The method introduces α³-SecBench, a large-scale evaluation suite that augments benign multi-turn UAV missions with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, and assesses agents across security, resilience, and trust dimensions. Key experimental findings from evaluating 23 state-of-the-art LLMs on thousands of adversarially augmented episodes reveal that while many models can detect anomalies, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent, with overall performance scores ranging only from 12.9% to 57.1%, indicating a significant gap between detection and security-aware autonomous decision-making.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在对抗性6G网络环境中，对基于大语言模型的无人机代理进行系统性安全、韧性和信任评估的缺失。方法上，研究团队提出了$α^{3}$-SecBench评估套件，该套件在良性的多轮对话无人机任务基础上，叠加了20,000个针对七个自主层级的已验证攻击场景，并从安全、韧性和信任三个维度对智能体进行评估。通过对23个先进大语言模型在数千个对抗性任务上的测试，主要实验结果表明，尽管许多模型能够可靠地检测异常行为，但在有效缓解攻击、归因漏洞和执行可信控制动作方面表现不一，整体标准化得分在12.9%至57.1%之间，凸显了异常检测与具备安全意识的自主决策能力之间存在显著差距。</div>
</details>
</div>
<div class="card">
<div class="title">HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs</div>
<div class="meta-line">Authors: Xinyue Zeng, Junhong Lin, Yujun Yan, Feng Guo, Liang Shi, Jun Wu, Dawei Zhou</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-01-26T18:23:09+00:00 · Latest: 2026-01-26T18:23:09+00:00</div>
<div class="meta-line">Comments: Have been accepted by ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18753v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18753v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HalluGuard：揭秘大语言模型中数据驱动与推理驱动的幻觉现象</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在医疗、法律和科学发现等高风险领域的可靠性常因幻觉问题而受损。这些错误通常源于两类原因：数据驱动的幻觉和推理驱动的幻觉。然而，现有检测方法通常仅针对单一来源，且依赖任务特定的启发式规则，限制了其在复杂场景中的泛化能力。为克服这些局限，我们提出了幻觉风险边界，这是一个统一的理论框架，将幻觉风险形式化分解为数据驱动与推理驱动两部分，分别对应训练时的数据失配和推理时的不稳定性，为分析幻觉的产生与演变提供了理论依据。基于此框架，我们开发了HalluGuard——一种基于神经正切核（NTK）的评分方法，利用NTK诱导的几何结构和捕获的表征来联合识别数据驱动与推理驱动的幻觉。我们在10个多样化基准测试、11个竞争性基线模型和9种主流LLM骨干网络上评估HalluGuard，其在检测各类LLM幻觉方面均取得了最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of hallucinations in Large Language Models (LLMs), which undermine reliability in critical domains, by distinguishing between data-driven and reasoning-driven sources. It introduces a unified theoretical framework, the Hallucination Risk Bound, to decompose hallucination risk into these components, and proposes HalluGuard, an NTK-based score that leverages neural tangent kernel geometry and representations to jointly detect both types. Experimental evaluation across 10 benchmarks, 11 baselines, and 9 LLM backbones demonstrates that HalluGuard achieves state-of-the-art performance in detecting diverse hallucinations.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型在医疗、法律等高风险领域中的幻觉问题，区分了数据驱动和推理驱动两种来源。通过提出幻觉风险边界理论框架，将幻觉风险分解为这两个组成部分，并基于此开发了HalluGuard，一种利用神经正切核几何和表示的NTK评分方法，以联合检测两类幻觉。在10个基准测试、11个基线方法和9个主流大语言模型上的实验评估表明，该方法在检测多种幻觉形式中达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Trust, Don&#x27;t Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback</div>
<div class="meta-line">Authors: Seyed Amir Hosseini, Maryam Abdolali, Amirhosein Tavakkoli, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi</div>
<div class="meta-line">First: 2026-01-26T18:21:48+00:00 · Latest: 2026-01-26T18:21:48+00:00</div>
<div class="meta-line">Comments: Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18751v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18751v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信任、不信任或翻转：基于多专家反馈的鲁棒偏好强化学习</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PBRL）通过从成对轨迹比较中学习，为显式奖励工程提供了有前景的替代方案。然而，现实世界的偏好数据常来自具有不同可靠性的异构标注者：部分准确、部分含噪声、部分系统对抗。现有PBRL方法或平等对待所有反馈，或尝试过滤不可靠来源，但面对系统提供错误偏好的对抗性标注者时均告失效。本文提出TriTrust-PBRL（TTP）统一框架，可从多专家偏好反馈中联合学习共享奖励模型和专家特定信任参数。核心洞见在于：信任参数在基于梯度的优化过程中会自然演化为正值（信任）、接近零（忽略）或负值（翻转），使模型能自动反转对抗性偏好并恢复有效信号，而非简单丢弃污染反馈。我们通过理论分析建立可识别性保证，并通过梯度分析阐明专家分离如何在无显式监督的训练过程中自然涌现。在涵盖操作任务（MetaWorld）和运动控制（DM Control）的四个领域中进行多场景腐蚀实验，TTP在对抗性腐蚀下保持接近理论最优性能，而标准PBRL方法则完全失效。值得注意的是，TTP在仅使用标识索引、无需额外专家特征的情况下，成功从包含可靠与对抗标注者的混合专家池中学习，其性能超越现有基线方法，并能无缝集成至现有PBRL流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Preference-based reinforcement learning faces challenges when dealing with heterogeneous annotators, including adversarial ones who systematically provide incorrect preferences, as existing methods either treat all feedback equally or discard unreliable sources, both failing under adversarial conditions. This work introduces TriTrust-PBRL (TTP), a framework that jointly learns a shared reward model and expert-specific trust parameters through gradient-based optimization, where these parameters naturally evolve to trust reliable experts, ignore noisy ones, or invert adversarial preferences. Experimental evaluation across manipulation and locomotion domains demonstrates that TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard methods fail catastrophically, and successfully learns from mixed expert pools containing both reliable and adversarial annotators.</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习在从异质人类标注者学习时面临鲁棒性挑战，特别是面对系统性提供错误偏好的对抗性标注者。提出的TriTrust-PBRL（TTP）框架通过基于梯度的优化联合学习共享奖励模型和专家特定的信任参数，这些参数自然演化为信任可靠专家、忽略噪声专家或反转对抗性偏好。在操作和运动领域的实验评估表明，TTP在对抗性污染下保持接近最优的性能，而标准PBRL方法完全失败，能够成功从包含可靠和对抗性专家的混合池中学习，且不需要额外的专家特征。</div>
</details>
</div>
<div class="card">
<div class="title">Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour</div>
<div class="meta-line">Authors: Max Norris, Kobi Gal, Sahan Bulathwela</div>
<div class="meta-line">First: 2025-11-04T14:20:56+00:00 · Latest: 2026-01-26T18:20:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.02599v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.02599v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modelling student knowledge is a key challenge when leveraging AI in education, with major implications for personalised learning. The Knowledge Tracing (KT) task aims to predict how students will respond to educational questions in learning environments, based on their prior interactions. Existing KT models typically use response correctness along with metadata like skill tags and timestamps, often overlooking the question text, which is an important source of pedagogical insight. This omission poses a lost opportunity while limiting predictive performance. We propose Next Token Knowledge Tracing (NTKT), a novel approach that reframes KT as a next-token prediction task using pretrained Large Language Models (LLMs). NTKT represents both student histories and question content as sequences of text, allowing LLMs to learn patterns in both behaviour and language. Our series of experiments significantly improves performance over state-of-the-art neural KT models and generalises much better to cold-start questions and users. These findings highlight the importance of question content in KT and demonstrate the benefits of leveraging pretrained representations of LLMs to model student learning more effectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>下一标记知识追踪：利用预训练大语言模型表征解码学生行为</div>
<div class="mono" style="margin-top:8px">学生知识建模是教育领域应用人工智能的关键挑战，对个性化学习具有重要影响。知识追踪任务旨在根据学生先前的交互记录，预测其在学习环境中对教育问题的作答表现。现有知识追踪模型通常仅使用作答正确率及技能标签、时间戳等元数据，往往忽略题目文本这一重要的教学洞察来源。这种缺失不仅错失分析机会，也限制了预测性能。我们提出下一标记知识追踪这一创新方法，将知识追踪重构为基于预训练大语言模型的下一标记预测任务。该方法将学生历史记录与题目内容均表示为文本序列，使大语言模型能够同时学习行为模式与语言规律。系列实验表明，该方法显著超越了当前最先进的神经知识追踪模型性能，并在冷启动题目和用户场景中展现出更优异的泛化能力。这些发现凸显了题目内容在知识追踪中的重要性，并证明了利用预训练大语言模型表征能更有效地建模学生学习过程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation in Knowledge Tracing (KT) models, which typically ignore the pedagogical content of question text, focusing instead on correctness and metadata. The proposed Next Token Knowledge Tracing (NTKT) method reformulates KT as a next-token prediction task using pretrained Large Language Models (LLMs), encoding both student interaction histories and question content as text sequences to capture behavioral and linguistic patterns. Experiments show that NTKT outperforms state-of-the-art neural KT models and demonstrates superior generalization to cold-start questions and users, underscoring the value of incorporating question text and leveraging LLM representations for student modeling.</div>
<div class="mono" style="margin-top:8px">本研究针对现有知识追踪模型通常忽略问题文本中的教学内容，从而限制了预测性能的问题。提出的方法——下一标记知识追踪，通过将学生历史记录和问题内容表示为文本序列，将知识追踪重构为下一标记预测任务，使预训练大语言模型能够从行为模式和语言模式中学习。实验结果表明，该方法显著优于最先进的神经知识追踪模型，并在冷启动问题和用户上表现出更好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval</div>
<div class="meta-line">Authors: Amir Aavani</div>
<div class="meta-line">First: 2026-01-26T18:07:40+00:00 · Latest: 2026-01-26T18:07:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18747v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18747v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.
  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\mathbf{P}$&#x27;&#x27; -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\mathbf{P}$. We introduce \texttt{ComputePN}, a novel evaluation algorithm that makes $\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative&#x27;&#x27; response mechanism, \texttt{ComputePN} ensures the efficient evaluation of any query in $\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>捕获P类：论布尔检索的表达能力与高效评估</div>
<div class="mono" style="margin-top:8px">现代信息检索正从简单的文档过滤转向复杂的神经符号推理工作流。然而，当前检索架构在处理这一新范式所需的严格逻辑与算术约束时，面临根本性的效率困境。标准的基于迭代器的引擎（逐文档处理）本身不支持复杂的嵌套逻辑图；强制其执行此类查询通常会导致不可行的运行时性能。相反，朴素的递归方法（逐词项处理）虽能支持这些结构，但在执行广泛逻辑排除时会产生极高的内存消耗。本文提出，检索引擎必须具备“捕获$\mathbf{P}$类”的能力——即能以计算高效的方式直接在索引上评估任何多项式时间属性。我们基于有向无环图定义了一种形式化检索语言$\mathcal{L}_R$，并证明其精确捕获了复杂度类$\mathbf{P}$。我们引入了新型评估算法\texttt{ComputePN}，使$\mathcal{L}_R$可高效处理。该算法通过结合原生DAG遍历与内存高效的“正负响应”机制，确保对$\mathcal{L}_R$中任意查询的高效评估。此项工作为将搜索索引转化为通用计算引擎奠定了理论基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the transition of information retrieval towards complex neuro-symbolic workflows, which exposes an efficiency dilemma: existing iterator-based engines struggle with the runtime performance of nested logical graphs, while naive recursive methods incur prohibitive memory costs for logical exclusions. To address this, the authors propose that a retrieval engine must efficiently evaluate any polynomial-time property, defining a formal Retrieval Language (ℒ_R) based on Directed Acyclic Graphs (DAGs) and proving it captures the complexity class P. They introduce the ComputePN algorithm, which combines native DAG traversal with a memory-efficient Positive-Negative mechanism to enable tractable evaluation of any query in ℒ_R. Experimental findings demonstrate that this approach provides a theoretical foundation for transforming a search index into a general-purpose computational engine capable of handling rigorous logical and arithmetic constraints efficiently.</div>
<div class="mono" style="margin-top:8px">本研究针对现代信息检索系统在处理复杂神经符号推理工作流时面临的效率困境，这些系统难以满足严格的逻辑约束要求。作者提出检索引擎必须能够直接在索引上评估任何多项式时间属性，并通过基于有向无环图的形式化检索语言来精确捕捉复杂度类P。他们引入了ComputePN算法，该算法结合了原生DAG遍历和内存高效的“正负”响应机制。实验结果表明，该方法能够高效评估形式化语言中的任何查询，为将搜索索引转变为通用计算引擎奠定了理论基础。</div>
</details>
</div>
<div class="card">
<div class="title">TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models</div>
<div class="meta-line">Authors: Fangxu Yu, Xingang Guo, Lingzhi Yuan, Haoqiang Kang, Hongyu Zhao, Lianhui Qin, Furong Huang, Bin Hu, Tianyi Zhou</div>
<div class="meta-line">First: 2026-01-26T18:04:54+00:00 · Latest: 2026-01-26T18:04:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18744v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18744v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tsrbench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series data is ubiquitous in real-world scenarios and crucial for critical applications ranging from energy management to traffic control. Consequently, the ability to reason over time series is a fundamental skill for generalist models to solve practical problems. However, this dimension is notably absent from existing benchmarks of generalist models. To bridge this gap, we introduce TSRBench, a comprehensive multi-modal benchmark designed to stress-test the full spectrum of time series reasoning capabilities. TSRBench features: i) a diverse set of 4125 problems from 14 domains, and is categorized into 4 major dimensions: Perception, Reasoning, Prediction, and Decision-Making. ii) 15 tasks from the 4 dimensions evaluating essential reasoning capabilities (e.g., numerical reasoning). Through extensive experiments, we evaluated over 30 leading proprietary and open-source LLMs, VLMs, and TSLLMs within TSRBench. Our findings reveal that: i) scaling laws hold for perception and reasoning but break down for prediction; ii) strong reasoning does not guarantee accurate context-aware forecasting, indicating a decoupling between semantic understanding and numerical prediction; and iii) despite the complementary nature of textual and visual represenations of time series as inputs, current multimodal models fail to effectively fuse them for reciprocal performance gains. TSRBench provides a standardized evaluation platform that not only highlights existing challenges but also offers valuable insights to advance generalist models. Our code and dataset are available at https://tsrbench.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TSRBench：面向通用模型的多任务多模态时间序列推理综合基准</div>
<div class="mono" style="margin-top:8px">时间序列数据在现实场景中无处不在，对从能源管理到交通控制等关键应用至关重要。因此，时间序列推理能力是通用模型解决实际问题的基本技能。然而，现有通用模型基准明显缺乏这一维度。为填补这一空白，我们提出了TSRBench——一个全面的多模态基准，旨在全面测试时间序列推理能力。TSRBench具备以下特点：i) 涵盖14个领域的4125个多样化问题，并划分为感知、推理、预测和决策四大维度；ii) 包含四大维度下的15项任务，用于评估数值推理等核心推理能力。通过大量实验，我们在TSRBench中评估了超过30个领先的专有及开源大语言模型、视觉语言模型和时间序列大模型。研究发现：i) 缩放定律适用于感知与推理任务，但在预测任务中失效；ii) 强推理能力不能保证准确的上下文感知预测，表明语义理解与数值预测之间存在解耦；iii) 尽管文本与视觉形式的时间序列输入具有互补性，当前多模态模型仍无法有效融合两者以实现协同性能提升。TSRBench提供了一个标准化评估平台，不仅揭示了现有挑战，更为推进通用模型发展提供了宝贵见解。代码与数据集已发布于 https://tsrbench.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Time series reasoning is essential for generalist models in practical applications, yet existing benchmarks lack this dimension. To address this, the authors introduce TSRBench, a multi-modal benchmark comprising 4125 problems across 14 domains, categorized into Perception, Reasoning, Prediction, and Decision-Making dimensions with 15 tasks. Evaluating over 30 leading models reveals that scaling laws apply to perception and reasoning but not prediction, strong reasoning does not ensure accurate forecasting, and current multimodal models fail to effectively fuse textual and visual time series representations for mutual benefit.</div>
<div class="mono" style="margin-top:8px">时间序列推理对于通用模型解决现实应用至关重要，但现有基准测试缺乏这一维度。为此，作者提出了TSRBench，这是一个多模态基准测试，包含来自14个领域的4125个问题，分为感知、推理、预测和决策四个主要维度，涵盖15项任务。通过对30多个领先模型的评估，主要发现包括：缩放定律适用于感知和推理但不适用于预测，强大的推理能力不能保证准确的上下文感知预测，以及当前的多模态模型未能有效融合时间序列的文本和视觉表示以实现性能的相互增益。</div>
</details>
</div>
<div class="card">
<div class="title">SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification</div>
<div class="meta-line">Authors: Ignacio Antequera-Sánchez, Juan Luis Suárez-Díaz, Rosana Montes, Francisco Herrera</div>
<div class="meta-line">First: 2026-01-26T18:01:46+00:00 · Latest: 2026-01-26T18:01:46+00:00</div>
<div class="meta-line">Comments: 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18739v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18739v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) detection is a fundamental requirement for the reliable deployment of artificial intelligence applications in open-world environments. However, addressing the heterogeneous nature of OOD data, ranging from low-level corruption to semantic shifts, remains a complex challenge that single-stage detectors often fail to resolve. To address this issue, we propose SeNeDiF-OOD, a novel methodology based on Semantic Nested Dichotomy Fusion. This framework decomposes the detection task into a hierarchical structure of binary fusion nodes, where each layer is designed to integrate decision boundaries aligned with specific levels of semantic abstraction. To validate the proposed framework, we present a comprehensive case study using MonuMAI, a real-world architectural style recognition system exposed to an open environment. This application faces a diverse range of inputs, including non-monument images, unknown architectural styles, and adversarial attacks, making it an ideal testbed for our proposal. Through extensive experimental evaluation in this domain, results demonstrate that our hierarchical fusion methodology significantly outperforms traditional baselines, effectively filtering these diverse OOD categories while preserving in-distribution performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SeNeDiF-OOD：面向开放世界分类的分布外检测方法——基于语义嵌套二分融合的案例研究：以纪念碑风格分类为例</div>
<div class="mono" style="margin-top:8px">分布外检测是人工智能应用在开放世界环境中可靠部署的基本要求。然而，面对从低级损坏到语义偏移等异构性质的分布外数据，单阶段检测器往往难以应对这一复杂挑战。为此，我们提出SeNeDiF-OOD——一种基于语义嵌套二分融合的新方法。该框架将检测任务分解为二元融合节点的层次结构，每层设计用于整合与特定语义抽象层级对齐的决策边界。为验证该框架，我们通过MonuMAI（一个暴露于开放环境的真实世界建筑风格识别系统）开展全面案例研究。该应用面临多样化输入，包括非纪念碑图像、未知建筑风格及对抗攻击，是验证本方法的理想测试平台。在该领域的广泛实验评估表明，我们的层次融合方法显著优于传统基线，能有效过滤各类分布外样本，同时保持分布内性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for reliable out-of-distribution (OOD) detection in open-world AI applications, where single-stage detectors struggle with the heterogeneous nature of OOD data, from low-level corruptions to semantic shifts. The method introduces SeNeDiF-OOD, a Semantic Nested Dichotomy Fusion framework that decomposes detection into a hierarchical structure of binary fusion nodes, each integrating decision boundaries aligned with specific levels of semantic abstraction. In a case study on monument style classification using the MonuMAI system, experimental results show that this hierarchical fusion significantly outperforms traditional baselines in filtering diverse OOD categories like non-monument images, unknown styles, and adversarial attacks, while maintaining in-distribution performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决开放世界人工智能应用中可靠检测异构分布外数据的挑战，这些数据包括低级损坏和语义偏移，而单阶段检测器往往难以应对。所提出的SeNeDiF-OOD方法引入了一个分层框架，将OOD检测任务分解为二元融合节点的层次结构，其中每一层都整合了与特定语义抽象级别对齐的决策边界。在基于MonuMAI建筑风格识别系统的案例研究中，该系统面临非古迹图像和未知风格等多种OOD输入，实验结果表明该分层融合方法在有效过滤这些异构OOD类别的同时保持了分布内性能，显著优于传统基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems</div>
<div class="meta-line">Authors: Jusheng Zhang, Yijia Fan, Kaitong Cai, Jing Yang, Jiawei Yao, Jian Wang, Guanlong Qu, Ziliang Chen, Keze Wang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-26T17:58:53+00:00 · Latest: 2026-01-26T17:58:53+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18735v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18735v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We introduce Agora, a framework that reframes coordination as a decentralized market for uncertainty. Agora formalizes epistemic uncertainty into a structured, tradable asset (perceptual, semantic, inferential), and enforces profitability-driven trading among agents based on rational economic rules. A market-aware broker, extending Thompson Sampling, initiates collaboration and guides the system toward cost-efficient equilibria. Experiments on five multimodal benchmarks (MMMU, MMBench, MathVision, InfoVQA, CC-OCR) show that Agora outperforms strong VLMs and heuristic multi-agent strategies, e.g., achieving +8.5% accuracy over the best baseline on MMMU while reducing cost by over 3x. These results establish market-based coordination as a principled and scalable paradigm for building economically viable multi-agent visual intelligence systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为何独自保留疑虑？多智能体赌博机系统中的视觉不确定性交易</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）赋能强大的多智能体系统，但其规模化在经济上不可持续：信息不对称下协调异构智能体常导致成本激增。现有范式（如智能体混合与基于知识的路由器）依赖忽略成本的启发式代理，破坏了不确定性结构，导致可证明的次优协调。我们提出Agora框架，将协调重构为不确定性的去中心化市场。Agora将认知不确定性形式化为结构化可交易资产（感知、语义、推断），并基于理性经济规则强制智能体间进行盈利驱动的交易。扩展汤普森采样的市场感知代理启动协作，引导系统实现成本效益均衡。在五个多模态基准（MMMU、MMBench、MathVision、InfoVQA、CC-OCR）上的实验表明，Agora优于强VLM与启发式多智能体策略，如在MMMU上以超过基线3倍的成本降低实现+8.5%准确率提升。这些成果确立了基于市场的协调作为构建经济可行多智能体视觉智能系统的原则性可扩展范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the economic unsustainability of scaling Vision-Language Model (VLM) multi-agent systems, where coordinating heterogeneous agents under information asymmetry leads to spiraling costs. The proposed Agora framework reframes coordination as a decentralized market for uncertainty, formalizing epistemic uncertainty into structured, tradable assets and enforcing profitability-driven trading among agents based on rational economic rules, guided by a market-aware broker extending Thompson Sampling. Experiments on five multimodal benchmarks demonstrate that Agora outperforms strong VLMs and heuristic multi-agent strategies, achieving, for example, an 8.5% accuracy improvement over the best baseline on MMMU while reducing costs by over threefold.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉-语言模型多智能体系统扩展的经济不可持续性，现有协调方法使用忽略成本和破坏不确定性结构的启发式代理，导致性能次优。提出的Agora框架将协调重构为一个去中心化市场，将认知不确定性形式化为结构化、可交易的资产（感知、语义、推理），并基于理性经济规则强制智能体间进行利润驱动的交易，由一个扩展汤普森采样的市场感知代理引导。在五个多模态基准（MMMU、MMBench、MathVision、InfoVQA、CC-OCR）上的实验表明，Agora优于强大的视觉-语言模型和启发式多智能体策略，例如在MMMU上比最佳基线准确率提升8.5%，同时成本降低超过三倍。</div>
</details>
</div>
<div class="card">
<div class="title">Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge</div>
<div class="meta-line">Authors: Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen, Ziye Wang, Ximeng Meng, Stone Tao, Yiran Qin, Xiaohong Liu, Ruimao Zhang, Lei Bai, Yilun Du, Hao Su, Philip Torr, Zhenfei Yin, Ruihao Gong, Yejun Zeng, Fengjun Zhong, Shenghao Jin, Jinyang Guo, Xianglong Liu, Xiaojun Jia, Tianqi Shan, Wenqi Ren, Simeng Qin, Jialing Yang, Xiaoyu Ma, Tianxing Chen, Zixuan Li, Zijian Cai, Yan Qin, Yusen Qin, Qiangyu Chen, Kaixuan Wang, Zhaoming Han, Yao Mu, Ping Luo, Yuanqi Yao, Haoming Song, Jan-Nico Zaech, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-26T17:56:19+00:00 · Latest: 2026-01-26T17:56:19+00:00</div>
<div class="meta-line">Comments: MARS Challenge @ NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI. Challenge page: https://mars-eai.github.io/MARS-Challenge-Webpage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18733v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mars-eai.github.io/MARS-Challenge-Webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体机器人系统（MARS）挑战赛的进展与创新</div>
<div class="mono" style="margin-top:8px">多模态大语言模型与视觉-语言-行动模型的最新进展显著推动了具身人工智能领域的发展。随着该领域向更复杂的任务场景过渡，多智能体系统框架正成为实现可扩展、高效协作解决方案的关键。这一转变主要受三大因素驱动：智能体能力提升、通过任务委派增强系统效率，以及实现高级人机交互。为应对多智能体协作带来的挑战，我们在NeurIPS 2025 SpaVLE研讨会上提出举办多智能体机器人系统（MARS）挑战赛。竞赛聚焦两大关键领域：规划与控制——参赛者将探索利用视觉语言模型进行多智能体具身规划以协调任务，并通过策略执行在动态环境中完成机器人操控。通过对参赛方案的评估，本挑战赛为具身多智能体系统的设计与协调提供宝贵见解，助力未来高级协作式人工智能系统的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to address complex task scenarios in Embodied AI, where multi-agent systems are essential for scalable and collaborative solutions. The method involves organizing the Multi-Agent Robotic System (MARS) Challenge, which focuses on multi-agent embodied planning using vision-language models and policy execution for robotic manipulation in dynamic environments. Key experimental findings from evaluating participant submissions provide insights into the design and coordination of embodied multi-agent systems, advancing the development of collaborative AI.</div>
<div class="mono" style="margin-top:8px">该研究的动机源于应对具身人工智能中复杂任务场景的需求，其中多智能体系统对于实现可扩展和协作的解决方案至关重要。方法包括组织多智能体机器人系统（MARS）挑战赛，重点关注使用视觉语言模型进行动态环境中的任务协调和机器人操作的规划与控制。通过评估参赛者提交的方案，关键实验结果为具身多智能体系统的设计和协调提供了宝贵见解，推动了协作人工智能的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Use of Preferences in Artificial Intelligence Algorithms</div>
<div class="meta-line">Authors: Joshua S. Gans</div>
<div class="meta-line">First: 2026-01-26T17:55:56+00:00 · Latest: 2026-01-26T17:55:56+00:00</div>
<div class="meta-line">Comments: 54 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18732v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18732v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning systems embed preferences either in training losses or through post-processing of calibrated predictions. Applying information design methods from Strack and Yang (2024), this paper provides decision problem agnostic conditions under which separation training preference free and applying preferences ex post is optimal. Unlike prior work that requires specifying downstream objectives, the welfare results here apply uniformly across decision problems. The key primitive is a diminishing-value-of-information condition: relative to a fixed (normalised) preference-free loss, preference embedding makes informativeness less valuable at the margin, inducing a mean-preserving contraction of learned posteriors. Because the value of information is convex in beliefs, preference-free training weakly dominates for any expected utility decision problem. This provides theoretical foundations for modular AI pipelines that learn calibrated probabilities and implement asymmetric costs through downstream decision rules. However, separation requires users to implement optimal decision rules. When cognitive constraints bind, as documented in human AI decision-making, preference embedding can dominate by automating threshold computation. These results provide design guidance: preserve optionality through post-processing when objectives may shift; embed preferences when decision-stage frictions dominate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能算法中偏好的最优运用</div>
<div class="mono" style="margin-top:8px">机器学习系统通过训练损失函数或校准预测的后处理来嵌入偏好。本文运用Strack与Yang（2024）的信息设计方法，提出了与决策问题无关的条件，证明分离无偏好训练与事后偏好应用是最优策略。相较于需要预设下游目标的先前研究，本文的福利结果可普适应用于各类决策问题。核心基础是信息边际价值递减条件：相对于固定的（归一化）无偏好损失，偏好嵌入会降低信息在边际上的价值，从而引发学习后验分布的均值保持收缩。由于信息价值在信念中呈凸性，无偏好训练对任何期望效用决策问题都具有弱优势。这为模块化AI流程提供了理论基础：先学习校准概率，再通过下游决策规则实施非对称成本。但分离策略要求用户执行最优决策规则。当存在人类-AI决策中常见的认知约束时，偏好嵌入可通过自动化阈值计算占据优势。研究结果为系统设计提供指引：目标可能变动时应通过后处理保持灵活性；当决策阶段摩擦占主导时则应嵌入偏好。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether preferences in AI systems should be embedded during training or applied after calibration, motivated by the need for decision-agnostic optimality. The method applies information design principles, establishing that separation—training without preferences and applying them ex post—is optimal under a diminishing marginal value of information condition, which induces a mean-preserving contraction of posteriors. Key experimental findings show that preference-free training weakly dominates for any expected utility decision problem, providing a foundation for modular AI pipelines, but preference embedding can dominate when users face cognitive constraints in implementing optimal decision rules, offering design guidance for balancing optionality and automation.</div>
<div class="mono" style="margin-top:8px">本文研究人工智能系统中的偏好应嵌入训练过程还是应用于校准后的预测，其动机在于寻求与决策问题无关的最优条件。该方法应用信息设计原理，确立了一个信息边际价值递减条件——即偏好嵌入会降低信息的有用性，从而诱导学习后验的均值保持收缩。关键实验结果表明，对于任何期望效用决策问题，无偏好训练都弱占优，这为模块化AI流程提供了理论基础；然而，当用户在实施最优决策规则时面临认知约束，偏好嵌入可能占优，从而为平衡可选性与自动化提供了设计指导。</div>
</details>
</div>
<div class="card">
<div class="title">One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment</div>
<div class="meta-line">Authors: Hongru Cai, Yongqi Li, Tiezheng Yu, Fengbin Zhu, Wenjie Wang, Fuli Feng, Wenjie Li</div>
<div class="meta-line">First: 2026-01-26T17:55:52+00:00 · Latest: 2026-01-26T17:55:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18731v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18731v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user&#x27;s reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以一适万：基于元学习的个性化大语言模型对齐奖励建模</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的对齐旨在使模型输出符合人类偏好，而个性化对齐则进一步使模型适应个体用户。这依赖于能够捕捉用户特定偏好并自动提供个性化反馈的奖励模型。然而，开发此类模型面临两大挑战：个体用户反馈稀缺，以及需高效适应未见用户。我们认为，解决这些约束需将范式从拟合数据学习用户偏好，转向学习偏好适应的过程。为此，我们提出元奖励建模（MRM），将个性化奖励建模重构为元学习问题。具体而言，我们将每个用户的奖励模型表示为基奖励函数的加权组合，并采用模型无关元学习（MAML）框架优化权重初始化，以支持有限反馈下的快速适应。为确保鲁棒性，我们引入鲁棒个性化目标（RPO），在元优化中更关注难以学习的用户。在个性化偏好数据集上的大量实验表明，MRM能增强少样本个性化能力，提升用户鲁棒性，且持续优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenges of personalized alignment for large language models, specifically the scarcity of individual user feedback and the need for efficient adaptation to new users. The proposed method, Meta Reward Modeling (MRM), frames personalized reward modeling as a meta-learning problem, representing a user&#x27;s reward model as a weighted combination of base functions and optimizing their initialization using a MAML-style framework for fast adaptation; it further introduces a Robust Personalization Objective to emphasize hard-to-learn users during meta optimization. Experimental results on personalized preference datasets demonstrate that MRM improves few-shot personalization, enhances robustness across users, and consistently outperforms baseline methods.</div>
<div class="mono" style="margin-top:8px">该研究针对个性化大语言模型对齐中的挑战，即个体用户反馈稀缺和需要高效适应新用户，通过从学习静态用户偏好转向学习偏好适应过程来解决问题。所提出的方法——元奖励建模（MRM）——将个性化奖励建模构建为一个元学习问题，将每个用户的奖励模型表示为基函数的加权组合，并采用MAML风格的框架优化其初始化，同时引入鲁棒个性化目标（RPO）以在元优化中优先处理难以学习的用户。在个性化偏好数据集上的广泛实验验证了MRM能有效提升少样本个性化性能，增强用户鲁棒性，并持续优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Physiology-Informed Generative Multi-Task Network for Contrast-Free CT Perfusion</div>
<div class="meta-line">Authors: Wasif Khan, John Rees, Kyle B. See, Simon Kato, Ziqian Huang, Amy Lazarte, Kyle Douglas, Xiangyang Lou, Teng J. Peng, Dhanashree Rajderkar, Pina Sanelli, Amita Singh, Ibrahim Tuna, Christina A. Wilson, Ruogu Fang</div>
<div class="meta-line">First: 2025-05-12T22:58:55+00:00 · Latest: 2026-01-26T17:55:25+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22673v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.22673v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Perfusion imaging is extensively utilized to assess hemodynamic status and tissue perfusion in various organs. Computed tomography perfusion (CTP) imaging plays a key role in the early assessment and planning of stroke treatment. While CTP provides essential perfusion parameters to identify abnormal blood flow in the brain, the use of contrast agents in CTP can lead to allergic reactions and adverse side effects, along with costing USD 4.9 billion worldwide in 2022. To address these challenges, we propose a novel deep learning framework called Multitask Automated Generation of Intermodal CT perfusion maps (MAGIC). This framework combines generative artificial intelligence and physiological information to map non-contrast computed tomography (CT) imaging to multiple contrast-free CTP imaging maps. We demonstrate enhanced image fidelity by incorporating physiological characteristics into the loss terms. Our network was trained and validated using CT image data from patients referred for stroke at UF Health and demonstrated robustness to abnormalities in brain perfusion activity. A double-blinded study was conducted involving seven experienced neuroradiologists and vascular neurologists. This study validated MAGIC&#x27;s visual quality and diagnostic accuracy showing favorable performance compared to clinical perfusion imaging with intravenous contrast injection. Overall, MAGIC holds great promise in revolutionizing healthcare by offering contrast-free, cost-effective, and rapid perfusion imaging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于生理学信息的生成式多任务网络用于无对比剂CT灌注成像</div>
<div class="mono" style="margin-top:8px">灌注成像广泛应用于评估各器官的血流动力学状态和组织灌注。计算机断层扫描灌注（CTP）成像在卒中治疗的早期评估与规划中具有关键作用。虽然CTP能提供识别脑部异常血流的关键灌注参数，但CTP中对比剂的使用可能导致过敏反应和不良反应，且2022年全球相关费用达49亿美元。为应对这些挑战，我们提出了一种名为&#x27;多任务跨模态CT灌注图自动生成（MAGIC）&#x27;的新型深度学习框架。该框架结合生成式人工智能与生理学信息，将非对比剂CT成像映射至多幅无对比剂CTP成像图。我们通过将生理特征融入损失函数项，证明了图像保真度的提升。该网络使用佛罗里达大学健康中心卒中患者的CT影像数据进行训练验证，并展现出对脑灌注活动异常的鲁棒性。一项涉及七位经验丰富的神经放射科医师和血管神经科医师的双盲研究证实，MAGIC在视觉质量与诊断准确性方面，相较于静脉注射对比剂的临床灌注成像具有更优表现。总体而言，MAGIC通过提供无对比剂、经济高效且快速的灌注成像，在医疗健康领域具有革命性潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the clinical and economic challenges of contrast-enhanced computed tomography perfusion (CTP), such as allergic risks and high costs, by developing a contrast-free alternative. The proposed Multitask Automated Generation of Intermodal CT perfusion maps (MAGIC) framework employs a deep learning approach that integrates generative AI with physiological information to synthesize multiple perfusion maps directly from non-contrast CT images, enhancing fidelity through physiology-informed loss terms. Experimental validation on stroke patient data demonstrated robustness to perfusion abnormalities, and a double-blinded study with seven specialists confirmed that MAGIC achieves visual quality and diagnostic accuracy comparable to standard contrast-based CTP.</div>
<div class="mono" style="margin-top:8px">本研究针对对比增强计算机断层扫描灌注成像的临床与经济挑战，如过敏风险和高成本，开发了一种无需对比剂的替代方案。所提出的多任务自动化跨模态CT灌注图生成框架采用深度学习，结合生成式人工智能与生理学信息，直接从非对比CT图像合成多种灌注图，并通过生理学启发的损失项提升保真度。在卒中患者数据上的实验验证表明该模型对灌注异常具有鲁棒性，一项由七位专家参与的双盲研究证实，该框架在视觉质量和诊断准确性上可与标准对比剂灌注成像相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">ARTI-6: Towards Six-dimensional Articulatory Speech Encoding</div>
<div class="meta-line">Authors: Jihwan Lee, Sean Foley, Thanathai Lertpetchpun, Kevin Huang, Yoonjeong Lee, Tiantian Feng, Louis Goldstein, Dani Byrd, Shrikanth Narayanan</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-25T19:18:35+00:00 · Latest: 2026-01-26T17:51:41+00:00</div>
<div class="meta-line">Comments: Accepted for ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21447v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21447v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose ARTI-6, a compact six-dimensional articulatory speech encoding framework derived from real-time MRI data that captures crucial vocal tract regions including the velum, tongue root, and larynx. ARTI-6 consists of three components: (1) a six-dimensional articulatory feature set representing key regions of the vocal tract; (2) an articulatory inversion model, which predicts articulatory features from speech acoustics leveraging speech foundation models, achieving a prediction correlation of 0.87; and (3) an articulatory synthesis model, which reconstructs intelligible speech directly from articulatory features, showing that even a low-dimensional representation can generate natural-sounding speech. Together, ARTI-6 provides an interpretable, computationally efficient, and physiologically grounded framework for advancing articulatory inversion, synthesis, and broader speech technology applications. The source code and speech samples are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ARTI-6：迈向六维发音语音编码</div>
<div class="mono" style="margin-top:8px">我们提出ARTI-6，一种基于实时MRI数据构建的紧凑六维发音语音编码框架，可捕捉包括软腭、舌根和喉部在内的关键声道区域。ARTI-6包含三个组件：(1) 表征声道关键区域的六维发音特征集；(2) 发音反演模型，利用语音基础模型从声学信号预测发音特征，预测相关性达0.87；(3) 发音合成模型，可直接从发音特征重建清晰可懂的语音，证明即使低维表征也能生成自然语音。ARTI-6共同构成了一个可解释、计算高效且具生理学基础的框架，可推动发音反演、合成及更广泛的语音技术应用。源代码与语音样本已公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to develop a compact and interpretable articulatory speech representation to advance speech technology. The method, ARTI-6, constructs a six-dimensional feature set from real-time MRI data to represent key vocal tract regions, and builds two models: an inversion model that predicts these features from speech acoustics using foundation models, and a synthesis model that reconstructs speech from the features. Key experimental results show the inversion model achieves a high prediction correlation of 0.87, and the synthesis model demonstrates that this low-dimensional representation can generate intelligible and natural-sounding speech.</div>
<div class="mono" style="margin-top:8px">为了利用紧凑且可解释的表征来推进发音语音处理，本研究提出了ARTI-6，这是一个从实时MRI数据推导出的六维编码框架，用于捕捉软腭、舌根等关键声道区域。该方法包含一个发音特征集、一个利用语音基础模型从声学信号预测这些特征的倒置模型，以及一个从特征重建语音的合成模型。实验结果表明，倒置模型实现了0.87的高预测相关性，合成模型则证明这种低维表征能够生成清晰且自然的语音，为语音技术应用提供了一个高效的框架。</div>
</details>
</div>
<div class="card">
<div class="title">TensLoRA: Tensor Alternatives for Low-Rank Adaptation</div>
<div class="meta-line">Authors: Axel Marmoret, Reda Bensaid, Jonathan Lys, Vincent Gripon, François Leduc-Primeau</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-22T17:15:23+00:00 · Latest: 2026-01-26T17:51:38+00:00</div>
<div class="meta-line">Comments: Published at ICASSP 2026. 5 pages, 1 figure, 2 tables. Code can be found at https://github.com/ax-le/TensLoRA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.19391v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.19391v2">PDF</a> · <a href="https://github.com/ax-le/TensLoRA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Rank Adaptation (LoRA) is widely used to efficiently adapt Transformers by adding trainable low-rank matrices to attention projections. While effective, these matrices are considered independent for each attention projection (Query, Key, and Value) and each layer. Recent extensions have considered joint, tensor-based adaptations, but only in limited forms and without a systematic framework. We introduce TensLoRA, a unified framework that aggregates LoRA updates into higher-order tensors and models a broad family of tensor-based low-rank adaptations. Our formulation generalizes existing tensor-based methods and enables mode-specific compression rates, allowing parameter budgets to be tailored according to the modality and task. Experiments on vision and language benchmarks reveal that the tensor construction directly impacts performance, sometimes better than standard LoRA under similar parameter counts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TensLoRA：低秩适配的张量替代方案</div>
<div class="mono" style="margin-top:8px">低秩适配（LoRA）通过向注意力投影添加可训练的低秩矩阵，被广泛用于高效适配Transformer模型。尽管有效，这些矩阵通常被视为每个注意力投影（查询、键、值）和每个层独立的。近期扩展研究考虑了联合的、基于张量的适配方法，但仅限于有限形式且缺乏系统化框架。我们提出TensLoRA，一个统一框架，将LoRA更新聚合为高阶张量，并建模广泛的基于张量的低秩适配方法族。我们的公式推广了现有基于张量的方法，支持模态特定的压缩率，允许根据模态和任务定制参数预算。在视觉和语言基准测试上的实验表明，张量构建方式直接影响性能，有时在相似参数规模下优于标准LoRA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of standard Low-Rank Adaptation (LoRA), which treats low-rank update matrices for each attention projection and layer as independent, lacking a systematic framework for joint, tensor-based adaptations. The proposed TensLoRA method introduces a unified framework that aggregates LoRA updates into higher-order tensors, modeling a broad family of tensor-based low-rank adaptations that generalize existing methods and enable mode-specific compression rates for tailored parameter allocation. Experimental results on vision and language benchmarks show that the specific tensor construction significantly impacts performance, with some configurations outperforming standard LoRA under comparable parameter budgets.</div>
<div class="mono" style="margin-top:8px">本研究针对标准低秩适应（LoRA）方法的局限性，即每个注意力投影和层的低秩矩阵被独立处理，缺乏系统化的张量联合适应框架。提出的TensLoRA方法将LoRA更新聚合为高阶张量，创建了一个统一的框架，能够建模广泛的张量式低秩适应方法，同时推广现有方法并支持模态特定的压缩率，从而实现参数预算的定制化分配。在视觉和语言基准测试中的实验结果表明，张量构建方式直接影响性能，某些配置在相似参数规模下优于标准LoRA方法。</div>
</details>
</div>
<div class="card">
<div class="title">HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences</div>
<div class="meta-line">Authors: Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe</div>
<div class="meta-line">First: 2026-01-26T17:48:23+00:00 · Latest: 2026-01-26T17:48:23+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18724v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18724v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as &quot;HalluCitation&quot; and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HalluCitation问题：基于ACL会议300篇幻觉论文揭示虚假引用的影响</div>
<div class="mono" style="margin-top:8px">近期，我们在审稿论文、预印本或已发表论文中频繁观察到与现有研究不符的幻觉引用或参考文献。此类虚假引用对科学可靠性构成严重威胁，若出现在录用论文中，还可能损害会议的公信力。本研究将此类引用定义为“HalluCitation”，并系统调查其普遍性与影响。我们分析了2024-2025年ACL、NAACL和EMNLP发表的所有论文（含主会、Findings及研讨会论文），发现近300篇论文至少含有一处HalluCitation，其中多数发表于2025年。值得注意的是，半数问题论文出现在最新的EMNLP 2025会议中，表明该问题正快速蔓延。此外，EMNLP 2025主会和Findings论文中超过100篇存在此类问题，已对学术信誉造成实际影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the growing concern over hallucinated citations, termed HalluCitation, which refer to non-existent references that undermine scientific reliability and conference credibility. The method involves a systematic analysis of all papers from ACL, NAACL, and EMNLP in 2024 and 2025, covering main conferences, Findings, and workshops. Key findings reveal nearly 300 papers contain at least one HalluCitation, with half identified at EMNLP 2025, indicating a rapid increase, and over 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, highlighting significant credibility impacts.</div>
<div class="mono" style="margin-top:8px">本研究针对日益严重的幻觉引用问题展开调查，这种引用指向不存在的文献，损害科学可靠性和会议信誉。方法上，系统分析了2024年和2025年ACL、NAACL和EMNLP的所有论文，包括主会、Findings和研讨会论文。主要实验结果表明，近300篇论文至少包含一处幻觉引用，其中一半出现在最新的EMNLP 2025会议上，显示问题迅速加剧；超过100篇此类论文被EMNLP 2025主会和Findings接收，对信誉造成了显著影响。</div>
</details>
</div>
<div class="card">
<div class="title">Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules</div>
<div class="meta-line">Authors: Naeyma N. Islam, Thomas R. Caulfield</div>
<div class="meta-line">Venue: Biomolecules 2025, 15, 849</div>
<div class="meta-line">First: 2026-01-26T17:39:59+00:00 · Latest: 2026-01-26T17:39:59+00:00</div>
<div class="meta-line">Comments: 30 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18716v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18716v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alzheimer&#x27;s disease (AD) is marked by the pathological accumulation of amyloid beta-42 (Abeta-42), contributing to synaptic dysfunction and neurodegeneration. While extracellular amyloid plaques are well-studied, increasing evidence highlights intracellular Abeta-42 as an early and toxic driver of disease progression. In this study, we present a novel, AI-assisted drug design approach to promote targeted degradation of Abeta-42 via the ubiquitin-proteasome system (UPS), using E3 ligase-directed molecular glues. We systematically evaluated the ternary complex formation potential of Abeta-42 with three E3 ligases: CRBN, VHL, and MDM2, through structure-based modeling, ADMET screening, and docking. We then developed a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) to generate ligase-specific small molecules, incorporating protein sequence embeddings and torsional angle-aware molecular graphs. Our results demonstrate that this generative model can produce chemically valid, novel, and target-specific molecular glues capable of facilitating Abeta-42 degradation. This integrated approach offers a promising framework for designing UPS-targeted therapies for neurodegenerative diseases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分子胶的条件生成建模：一种面向可合成类药分子的现实人工智能方法</div>
<div class="mono" style="margin-top:8px">阿尔茨海默病（AD）以淀粉样蛋白β-42（Abeta-42）的病理性积聚为特征，导致突触功能障碍和神经退行性变。尽管细胞外淀粉样斑块已得到充分研究，但越来越多的证据表明细胞内Abeta-42是疾病进展的早期毒性驱动因素。本研究提出了一种新颖的人工智能辅助药物设计方法，通过E3连接酶导向的分子胶，利用泛素-蛋白酶体系统（UPS）促进Abeta-42的靶向降解。我们通过基于结构的建模、ADMET筛选和分子对接，系统评估了Abeta-42与三种E3连接酶（CRBN、VHL和MDM2）形成三元复合物的潜力。随后，我们开发了一种连接酶条件化连接树变分自编码器（LC-JT-VAE），结合蛋白质序列嵌入和扭转角感知分子图，生成连接酶特异性小分子。结果表明，该生成模型能够产生化学有效、新颖且靶向特异性的分子胶，可促进Abeta-42降解。这一综合方法为设计针对神经退行性疾病的UPS靶向疗法提供了有前景的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses Alzheimer&#x27;s disease by targeting intracellular amyloid beta-42 (Abeta-42) for degradation via the ubiquitin-proteasome system, using E3 ligase-directed molecular glues. The method involved evaluating Abeta-42&#x27;s interaction with CRBN, VHL, and MDM2 ligases through structure-based modeling and docking, followed by developing a Ligase-Conditioned Junction Tree Variational Autoencoder (LC-JT-VAE) that integrates protein sequence embeddings and torsional angle-aware graphs to generate ligase-specific molecules. Experimental results showed the model successfully produced novel, chemically valid, and target-specific molecular glues capable of facilitating Abeta-42 degradation, offering a promising framework for neurodegenerative disease therapies.</div>
<div class="mono" style="margin-top:8px">本研究针对阿尔茨海默病中细胞内毒性淀粉样蛋白-β42（Abeta-42）的靶向降解需求，旨在设计招募E3连接酶的分子胶。方法包括通过基于结构的建模和ADMET筛选评估Abeta-42与CRBN、VHL和MDM2连接酶的相互作用，并开发了一种连接酶条件化的连接树变分自编码器（LC-JT-VAE），该模型整合了蛋白质序列嵌入和扭转角感知分子图以生成连接酶特异性分子。实验结果表明，该模型能成功产生新颖、可合成且靶点特异的小分子，可通过泛素-蛋白酶体系统促进Abeta-42降解，为神经退行性疾病治疗提供了框架。</div>
</details>
</div>
<div class="card">
<div class="title">Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning</div>
<div class="meta-line">Authors: Judith Vilella-Cantos, Mauro Martini, Marcello Chiaberge, Mónica Ballesta, David Valiente</div>
<div class="meta-line">First: 2026-01-26T17:38:56+00:00 · Latest: 2026-01-26T17:38:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低成本高效益：基于套娃表征学习的葡萄园激光雷达地点识别</div>
<div class="mono" style="margin-top:8px">农业环境因其非结构化特性及缺乏显著地标，使得定位任务极具挑战。尽管农业场景在物体分类与分割领域已有研究，但移动机器人的地点识别任务在当前技术体系中仍非易事。本研究提出MinkUNeXt-VINE——一种基于轻量级深度学习的方法，通过其预处理流程与套娃表征学习的多损失函数策略，在葡萄园环境中超越了现有最优方法。该方法优先采用低成本稀疏激光雷达输入与低维输出，确保实时场景下的高效性能。此外，我们通过多种评估案例及两个采用不同激光雷达传感器的长期葡萄园数据集，对结果进行了全面消融研究。实验结果表明，该方法在权衡输出效率方面表现优异，并在低成本、低分辨率输入数据上展现出鲁棒性能。相关代码已公开以供复现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of place recognition for mobile robots in unstructured agricultural environments like vineyards, which lack distinctive landmarks. The authors propose MinkUNeXt-VINE, a lightweight deep learning method that uses a specific pre-processing technique and a Matryoshka Representation Learning multi-loss approach to create efficient, low-dimensional representations from sparse, low-cost LiDAR data. Experimental results on two long-term vineyard datasets with different LiDAR sensors show that the method surpasses state-of-the-art performance, demonstrating robust and efficient place recognition even with low-resolution input.</div>
<div class="mono" style="margin-top:8px">本研究针对缺乏显著地标、非结构化的农业环境（如葡萄园）中移动机器人的激光雷达地点识别难题。作者提出了MinkUNeXt-VINE，一种轻量级深度学习方法，采用专门的预处理和套娃表征学习多损失函数策略以实现高效率。在两个使用不同激光雷达传感器的长期葡萄园数据集上的实验结果表明，该方法超越了现有最优性能，特别是在使用低成本、稀疏的激光雷达输入和适用于实时操作的低维输出时，提供了鲁棒且高效的性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Point transformer for protein structural heterogeneity analysis using CryoEM</div>
<div class="meta-line">Authors: Muyuan Chen, Muchen Li, Renjie Liao</div>
<div class="meta-line">First: 2026-01-26T17:38:52+00:00 · Latest: 2026-01-26T17:38:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18713v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18713v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Structural dynamics of macromolecules is critical to their structural-function relationship. Cryogenic electron microscopy (CryoEM) provides snapshots of vitrified protein at different compositional and conformational states, and the structural heterogeneity of proteins can be characterized through computational analysis of the images. For protein systems with multiple degrees of freedom, it is still challenging to disentangle and interpret the different modes of dynamics. Here, by implementing Point Transformer, a self-attention network designed for point cloud analysis, we are able to improve the performance of heterogeneity analysis on CryoEM data, and characterize the dynamics of highly complex protein systems in a more human-interpretable way.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于点变换器的冷冻电镜蛋白质结构异质性分析</div>
<div class="mono" style="margin-top:8px">大分子的结构动力学对其结构-功能关系至关重要。冷冻电子显微镜（CryoEM）可捕捉玻璃化蛋白质在不同组成与构象状态下的瞬时图像，通过计算分析可表征蛋白质的结构异质性。对于具有多自由度的蛋白质体系，解析和阐释不同动态模式仍具挑战。本研究通过采用专为点云分析设计的自注意力网络——点变换器，提升了冷冻电镜数据的异质性分析性能，并以更易于人类理解的方式表征了高度复杂蛋白质体系的动态行为。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the challenge of interpreting complex structural dynamics in proteins from cryo-electron microscopy (CryoEM) data, where multiple conformational states coexist. The method implements Point Transformer, a self-attention neural network designed for point cloud analysis, to enhance the computational disentanglement of heterogeneous protein conformations. Experimental results demonstrate that this approach improves the performance of heterogeneity analysis and enables more interpretable characterization of dynamics in highly complex protein systems.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从冷冻电镜数据中解析和解释蛋白质结构异质性多种动态模式的挑战，这些数据捕获了蛋白质在不同状态下的快照。方法采用Point Transformer这一针对点云分析的自注意力网络，以增强对冷冻电镜图像的计算分析。实验结果表明，该方法提高了异质性分析的性能，使得对高度复杂蛋白质系统动态的表征更加易于人类理解。</div>
</details>
</div>
<div class="card">
<div class="title">Ordering-based Causal Discovery via Generalized Score Matching</div>
<div class="meta-line">Authors: Vy Vo, He Zhao, Trung Le, Edwin V. Bonilla, Dinh Phung</div>
<div class="meta-line">First: 2026-01-22T18:08:31+00:00 · Latest: 2026-01-26T17:35:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16249v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16249v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning DAG structures from purely observational data remains a long-standing challenge across scientific domains. An emerging line of research leverages the score of the data distribution to initially identify a topological order of the underlying DAG via leaf node detection and subsequently performs edge pruning for graph recovery. This paper extends the score matching framework for causal discovery, which is originally designated for continuous data, and introduces a novel leaf discriminant criterion based on the discrete score function. Through simulated and real-world experiments, we demonstrate that our theory enables accurate inference of true causal orders from observed discrete data and the identified ordering can significantly boost the accuracy of existing causal discovery baselines on nearly all of the settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于排序的因果发现：广义得分匹配方法</div>
<div class="mono" style="margin-top:8px">从纯观测数据中学习有向无环图结构是跨科学领域长期存在的挑战。新兴研究利用数据分布的得分函数，通过叶节点检测初步识别底层有向无环图的拓扑排序，随后进行边剪枝以恢复图结构。本文扩展了原本针对连续数据的因果发现得分匹配框架，提出了一种基于离散得分函数的新型叶节点判别准则。通过仿真与真实数据实验，我们证明该理论能够从观测离散数据中准确推断真实因果顺序，且所识别的排序能在几乎所有设定下显著提升现有因果发现基线的准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Learning directed acyclic graph (DAG) structures from observational data is a fundamental challenge in causal discovery. This paper extends the score matching framework, originally for continuous data, to discrete data by introducing a novel leaf discriminant criterion based on the discrete score function, enabling the identification of a topological order through leaf node detection followed by edge pruning. Experimental results on simulated and real-world data show that the method accurately infers true causal orders and that the identified ordering significantly improves the accuracy of existing causal discovery baselines in nearly all tested settings.</div>
<div class="mono" style="margin-top:8px">从纯观测数据中学习有向无环图（DAG）结构是一个长期存在的挑战。本文扩展了原本为连续数据设计的得分匹配框架，通过引入基于离散得分函数的新叶节点判别准则来识别拓扑序。在模拟和真实世界数据上的实验表明，该方法能够准确推断真实因果序，并且利用该排序在几乎所有测试设置中显著提升了现有因果发现基线的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model</div>
<div class="meta-line">Authors: Jan Hagnberger, Mathias Niepert</div>
<div class="meta-line">First: 2026-01-26T17:34:16+00:00 · Latest: 2026-01-26T17:34:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning-based surrogate models have emerged as more efficient alternatives to numerical solvers for physical simulations over complex geometries, such as car bodies. Many existing models incorporate the simulation mesh as an additional input, thereby reducing prediction errors. However, generating a simulation mesh for new geometries is computationally costly. In contrast, mesh-free methods, which do not rely on the simulation mesh, typically incur higher errors. Motivated by these considerations, we introduce SMART, a neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry, without requiring access to the simulation mesh. The geometry and simulation parameters are encoded into a shared latent space that captures both structural and parametric characteristics of the physical field. A physics decoder then attends to the encoder&#x27;s intermediate latent representations to map spatial queries to physical quantities. Through this cross-layer interaction, the model jointly updates latent geometric features and the evolving physical field. Extensive experiments show that SMART is competitive with and often outperforms existing methods that rely on the simulation mesh as input, demonstrating its capabilities for industry-level simulations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMART：基于Transformer代理模型从原始几何实现可扩展的无网格空气动力学仿真</div>
<div class="mono" style="margin-top:8px">基于机器学习的代理模型已成为复杂几何体（如汽车车身）物理仿真中数值求解器的高效替代方案。现有模型常将仿真网格作为额外输入以降低预测误差，但为新几何生成仿真网格的计算成本高昂。相比之下，不依赖仿真网格的无网格方法通常误差较大。为此，我们提出SMART神经代理模型，仅通过几何点云表示即可预测任意查询位置的物理量，无需仿真网格。该模型将几何与仿真参数编码至共享潜在空间，以捕捉物理场的结构与参数特征；物理解码器通过跨层交互机制关注编码器的中间潜在表示，将空间查询映射为物理量，实现潜在几何特征与演化物理场的联合更新。大量实验表明，SMART在性能上可与依赖仿真网格输入的现有方法竞争甚至超越，展现了工业级仿真的应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational cost of generating simulation meshes for new geometries while maintaining accuracy, this paper introduces SMART, a mesh-free neural surrogate model that predicts physical quantities at arbitrary query locations using only a point-cloud representation of the geometry. The method encodes geometry and simulation parameters into a shared latent space and employs a physics decoder with cross-layer attention to jointly update latent geometric features and the physical field. Experimental results demonstrate that SMART is competitive with and often outperforms existing mesh-dependent surrogate models, showcasing its potential for scalable, industry-level aerodynamic simulations.</div>
<div class="mono" style="margin-top:8px">本研究针对空气动力学仿真中生成仿真网格计算成本高的问题，开发了一种基于原始点云几何的无网格代理模型。提出的SMART模型采用基于Transformer的架构，将几何和仿真参数编码到共享的隐空间中，以捕捉结构性和参数化特征，并通过具有跨层注意力机制的物理解码器，在无需网格输入的情况下将空间查询映射到物理量。实验结果表明，SMART在精度上可与现有依赖网格的方法竞争甚至更优，验证了其在可扩展工业级仿真中的应用潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs</div>
<div class="meta-line">Authors: Zhichao Yang, Sepehr Janghorbani, Dongxu Zhang, Jun Han, Qian Qian, Andrew Ressler, Gregory D. Lyng, Sanjit Singh Batra, Robert E. Tillman</div>
<div class="meta-line">First: 2026-01-26T17:34:10+00:00 · Latest: 2026-01-26T17:34:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18706v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18706v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rubrics are essential for evaluating open-ended LLM responses, especially in safety-critical domains such as healthcare. However, creating high-quality and domain-specific rubrics typically requires significant human expertise time and development cost, making rubric-based evaluation and training difficult to scale. In this work, we introduce Health-SCORE, a generalizable and scalable rubric-based training and evaluation framework that substantially reduces rubric development costs without sacrificing performance. We show that Health-SCORE provides two practical benefits beyond standalone evaluation: it can be used as a structured reward signal to guide reinforcement learning with safety-aware supervision, and it can be incorporated directly into prompts to improve response quality through in-context learning. Across open-ended healthcare tasks, Health-SCORE achieves evaluation quality comparable to human-created rubrics while significantly lowering development effort, making rubric-based evaluation and training more scalable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Health-SCORE：面向可扩展的评估准则以改进健康领域大语言模型</div>
<div class="mono" style="margin-top:8px">评估准则对于评估开放式大语言模型响应至关重要，尤其在医疗等安全关键领域。然而，创建高质量且领域特定的评估准则通常需要大量专家人力与开发成本，使得基于准则的评估与训练难以规模化。本研究提出Health-SCORE——一个可泛化、可扩展的基于准则的训练与评估框架，能在保持性能的同时大幅降低准则开发成本。我们证明Health-SCORE除独立评估外还具有两项实际优势：可作为结构化奖励信号引导具备安全监督的强化学习，并能直接嵌入提示词通过上下文学习提升响应质量。在开放式医疗任务中，Health-SCORE实现了与人工创建准则相当的评估质量，同时显著降低开发成本，使基于准则的评估与训练更具可扩展性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Rubrics are crucial for evaluating open-ended LLM responses in safety-critical domains like healthcare, but creating high-quality, domain-specific rubrics is resource-intensive, hindering scalability. To address this, the authors introduce Health-SCORE, a scalable framework that reduces rubric development costs while maintaining performance; it generates structured reward signals for reinforcement learning with safety-aware supervision and can be integrated into prompts for in-context learning to improve responses. Experiments on open-ended healthcare tasks show that Health-SCORE achieves evaluation quality comparable to human-created rubrics with significantly less development effort, thereby enhancing the scalability of rubric-based evaluation and training.</div>
<div class="mono" style="margin-top:8px">在医疗等安全关键领域，基于量规的评估对于开放式大语言模型回答至关重要，但人工制定高质量量规成本高昂。为此，研究者提出了Health-SCORE框架，旨在通过自动化生成量规来降低开发成本，同时保持性能。实验结果表明，该框架在开放式医疗任务上的评估质量与人工量规相当，并能作为结构化奖励信号指导强化学习，以及通过上下文学习直接提升回答质量，从而提高了量规评估与训练的可扩展性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
