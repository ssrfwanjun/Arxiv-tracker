<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-19 04:40</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260119_0440</div>
    <div class="row"><div class="card">
<div class="title">Alterbute: Editing Intrinsic Attributes of Objects in Images</div>
<div class="meta-line">Authors: Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-15T18:59:53+00:00 · Latest: 2026-01-15T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page is available at https://talreiss.github.io/alterbute/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://talreiss.github.io/alterbute/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Alterbute, a diffusion-based method for editing an object&#x27;s intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., &#x27;&#x27;Porsche 911 Carrera&#x27;&#x27;) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alterbute：编辑图像中物体的内在属性</div>
<div class="mono" style="margin-top:8px">我们提出Alterbute，一种基于扩散模型的图像物体内在属性编辑方法。该方法支持改变物体的颜色、纹理、材质甚至形状，同时保持其感知身份与场景上下文。现有方法要么依赖无法保持身份的无监督先验，要么采用过度限制的监督方式阻碍有意义的属性变化。我们的方法基于：（1）松弛训练目标，允许模型根据身份参考图像、描述目标内在属性的文本提示、以及定义外在背景的背景图像与物体掩码，同时改变内在与外在属性。在推理阶段，通过复用原始背景与物体掩码限制外在变化，确保仅修改目标内在属性；（2）视觉命名实体——细粒度视觉身份类别（如“保时捷911卡雷拉”），将具有身份定义特征但允许内在属性变化的物体归组。我们利用视觉语言模型从大型公共图像数据集中自动提取VNE标签与内在属性描述，实现可扩展的身份保持监督。Alterbute在身份保持的物体内在属性编辑任务上优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of editing intrinsic attributes like color, texture, material, and shape of objects in images while preserving the object&#x27;s identity and scene context, as existing methods often fail to maintain identity or restrict meaningful variation. The proposed method, Alterbute, employs a diffusion-based approach with a relaxed training objective that conditions changes on an identity reference image, a textual prompt for target attributes, and a background with an object mask; at inference, it reuses the original background and mask to restrict extrinsic changes. It also introduces Visual Named Entities (VNEs), fine-grained identity categories automatically labeled via a vision-language model from a large dataset, to provide scalable, identity-preserving supervision. Experimentally, Alterbute outperforms existing methods in identity-preserving intrinsic attribute editing.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决图像中物体内在属性（如颜色、纹理、材质和形状）编辑的挑战，同时保持物体的身份和场景上下文，因为现有方法往往无法保持身份或限制了有意义的变异。所提出的方法Alterbute采用基于扩散的模型，通过宽松的训练目标，将变化条件设定在身份参考图像、描述目标属性的文本提示以及背景图像和物体掩码上；在推理时，通过重用原始背景和掩码来限制外在变化，从而仅改变内在属性。该方法还引入了视觉命名实体（VNEs），即通过视觉语言模型从大型数据集中自动标注的细粒度身份类别，以提供可扩展的、保持身份的监督。实验结果表明，Alterbute在保持身份的内在属性编辑方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion</div>
<div class="meta-line">Authors: Cheng Chen, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Peng Di, Hang Yu, Lianli Gao</div>
<div class="meta-line">First: 2026-01-15T18:59:10+00:00 · Latest: 2026-01-15T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10710v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从单对单到多对多：面向深度视觉-语言融合的动态跨层注入</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通过仅将视觉编码器输出与大型语言模型（LLM）输入相连的粗糙非对称连接，造成了严重的视觉特征瓶颈。这种静态架构从根本上限制了LLM与层次化视觉知识实现全面对齐的能力，削弱了其将局部细节与全局语义准确整合为连贯推理的效能。为此，我们提出跨层注入（CLI）——一种新颖轻量的框架，可在两种模态间构建动态的多对多桥梁。CLI包含两个高效协同的参数化组件：自适应多投影（AMP）模块（用于协调来自不同视觉层的特征）和自适应门控融合（AGF）机制（使LLM能依据实时解码上下文选择性注入最相关的视觉信息）。通过将CLI集成至LLaVA-OneVision和LLaVA-1.5，我们在18个多样化基准测试中验证了其有效性与通用性，实验表明性能显著提升。CLI作为一种可扩展范式，通过赋予LLM按需访问完整视觉层次的能力，开启了更深层次的多模态理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the visual feature bottleneck in Vision-Language Models (VLMs), where a static, one-to-one connection between the vision encoder and the large language model (LLM) limits comprehensive alignment with hierarchical visual knowledge. To overcome this, the authors propose Cross-Layer Injection (CLI), a lightweight framework that establishes a dynamic many-to-many bridge via an Adaptive Multi-Projection module to harmonize features from diverse vision layers and an Adaptive Gating Fusion mechanism for context-aware injection into the LLM. Experimental integration into LLaVA-OneVision and LLaVA-1.5 across 18 benchmarks demonstrates significant performance improvements, validating CLI as a scalable paradigm for deeper multimodal understanding.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（VLM）中存在的视觉特征瓶颈问题，即视觉编码器与大语言模型（LLM）之间静态的一对一连接限制了模型与层次化视觉知识的全面对齐。为此，作者提出了跨层注入（CLI）这一轻量级框架，它通过自适应多投影模块协调来自不同视觉层的特征，并利用自适应门控融合机制，根据LLM的实时解码上下文有选择地注入最相关的视觉信息，从而建立动态的多对多桥梁。将CLI集成到LLaVA-OneVision和LLaVA-1.5中，在18个多样化基准测试上的广泛实验表明，其性能显著提升，证明了CLI通过按需访问完整视觉层次来实现更深层次多模态理解的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</div>
<div class="meta-line">Authors: Jorge Ortiz</div>
<div class="meta-line">First: 2025-12-31T23:27:32+00:00 · Latest: 2026-01-15T17:31:17+00:00</div>
<div class="meta-line">Comments: Preprint. Diagnostic study of confidence-based abstention under evidence truncation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00138v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00138v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频问答中用于可预测可靠性的显式弃权调控机制</div>
<div class="mono" style="margin-top:8px">视觉语言模型在高风险场景部署需采用选择性预测，使系统在不确定时主动弃权以避免代价高昂的错误。本研究探讨了基于置信度的弃权机制能否在视频问答中提供对错误率的可靠控制，以及该控制在分布偏移下是否保持稳健。基于NExT-QA数据集和Gemini 2.0 Flash模型，我们得出两项结论：首先，置信度阈值能在分布内提供机制性控制，通过调节阈值ε可生成平滑的风险-覆盖权衡曲线，有效降低错误率</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study is motivated by the need for reliable selective prediction in high-stakes vision-language model deployments, where systems must abstain when uncertain to avoid costly errors. The method investigates whether confidence-based abstention provides predictable control over error rates in video question answering, specifically using NExT-QA and Gemini 2.0 Flash models. The key experimental findings show that confidence thresholding offers mechanistic in-distribution control, as sweeping the threshold produces smooth risk-coverage tradeoffs and reduces error rates, though its robustness under distribution shift is also a central concern.</div>
<div class="mono" style="margin-top:8px">为使视觉语言模型能在高风险场景中安全部署，本研究旨在探究基于置信度的弃权机制能否为视频问答任务提供可靠且可预测的错误率控制，尤其是在分布偏移的情况下。研究方法是对NExT-QA数据集和Gemini 2.0 Flash模型进行诊断性分析，通过调节置信度阈值来考察风险与覆盖率之间的权衡关系。主要实验结果表明，置信度阈值法在分布内能提供机制性的错误率控制，产生平滑的风险-覆盖率曲线，但当可用证据被截断时，其可靠性受到挑战，这揭示了该方法在分布偏移下的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</div>
<div class="meta-line">Authors: Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna</div>
<div class="meta-line">First: 2026-01-15T17:27:44+00:00 · Latest: 2026-01-15T17:27:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10611v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10611v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Today&#x27;s strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&amp;F on video tracking).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Molmo2：具备视频理解与定位能力的视觉语言模型开源权重与数据集</div>
<div class="mono" style="margin-top:8px">当前最先进的视频语言模型（VLMs）仍属闭源系统。最强的开源权重模型要么依赖闭源VLMs生成的合成数据进行知识蒸馏，要么未公开其训练数据与方案，导致开源社区缺乏改进前沿视频（及图像）语言模型的基础。关键的是，许多下游应用不仅需要高层视频理解，更需像素级指向或跟踪的定位能力——即便闭源模型也欠缺此功能。我们推出Molmo2系列VLMs，在开源模型中达到前沿水平，并在单图、多图及视频任务的指向驱动定位中展现出卓越新能力。核心贡献包括7个新视频数据集与2个多图数据集：涵盖预训练用高细节视频描述数据集、微调用自由形式视频问答数据集、复杂查询的新物体跟踪数据集及创新的视频指向数据集——均未使用闭源VLMs。我们同时提出采用高效打包与消息树编码的训练方案，证明视觉令牌的双向注意力机制与新颖令牌权重策略可提升性能。顶尖的80亿参数模型在短视频、计数与描述任务上超越同类开源权重与数据模型，长视频任务表现亦具竞争力。在视频定位方面，Molmo2显著优于Qwen3-VL等开源模型（视频计数准确率35.5对29.6），部分任务甚至超越Gemini 3 Pro等闭源模型（视频指向F1值38.4对20.0，视频跟踪J&amp;F值56.2对41.1）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of open-source, state-of-the-art video-language models (VLMs) with grounding capabilities, as current leading models are either proprietary or rely on distilled data from them. The method introduces Molmo2, a family of VLMs trained on a newly collected suite of seven video and two multi-image datasets without using closed VLMs, employing an efficient packing and message-tree encoding scheme, bi-directional attention on vision tokens, and a novel token-weight strategy. Key experimental results show that the 8B model outperforms other open-weight models on tasks like short video understanding, counting, and captioning, and significantly exceeds models like Qwen3-VL and even proprietary ones like Gemini 3 Pro in video grounding tasks such as pointing and tracking.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于当前缺乏开源且具备定位能力的高性能视频-语言模型，因为最先进的模型要么是专有的，要么依赖于闭源数据。方法上提出了Molmo2模型系列，基于七个新视频数据集和两个多图像数据集进行训练，这些数据均未使用闭源模型收集，并采用了高效的打包和消息树编码方案、视觉令牌的双向注意力机制以及新颖的令牌权重策略。主要实验结果表明，其8B模型在短视频、计数和字幕生成上优于其他开源模型，在视频定位任务上超越了Gemini 3 Pro等专有模型（F1分数38.4对20.0，J&amp;F分数56.2对41.1），同时在长视频任务上表现具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Misalignment in Vision-Language Models under Perceptual Degradation</div>
<div class="meta-line">Authors: Guo Cheng</div>
<div class="meta-line">First: 2026-01-13T09:13:05+00:00 · Latest: 2026-01-15T17:10:05+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08355v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08355v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知退化下视觉语言模型的语义失配研究</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）正日益应用于自动驾驶和具身人工智能系统，其可靠的感知能力对安全的语义推理与决策至关重要。尽管当前VLMs在多模态基准测试中表现优异，但其对现实感知退化的鲁棒性仍鲜为人知。本研究通过以Cityscapes数据集的语义分割作为代表性感知模块，系统探究了上游视觉感知受控退化时VLMs的语义失配现象。我们引入了仅导致传统分割指标适度下降的感知现实型干扰，却观察到下游VLM行为出现严重故障，包括幻觉物体提及、安全关键实体遗漏以及不一致的安全判断。为量化这些影响，我们提出了一套语言层面的失配度量指标，用于捕捉幻觉、关键遗漏和安全误判，并分析了多款对比式与生成式VLM中这些指标与分割质量的关系。研究结果揭示了像素级鲁棒性与多模态语义可靠性间的明显脱节，凸显了当前基于VLM系统的关键局限，并表明亟需在安全关键应用中建立能显式考量感知不确定性的评估框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) are crucial for safety-critical applications like autonomous driving, but their robustness to degraded visual inputs is not well understood. This work systematically evaluates VLM semantic misalignment by applying controlled, perception-realistic corruptions to the visual inputs of a semantic segmentation module on the Cityscapes dataset. The experiments reveal that even moderate drops in segmentation metrics cause severe downstream VLM failures, including object hallucinations, omissions of critical entities, and inconsistent safety judgments, demonstrating a significant disconnect between pixel-level and semantic-level robustness.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在自动驾驶等安全关键应用中至关重要，但其对视觉感知退化的鲁棒性尚不明确。本研究通过在城市景观数据集的语义分割输出上施加受控的、感知真实的退化，系统性地研究了视觉语言模型中的语义错位问题。实验结果表明，即使传统分割指标仅出现适度下降，也会导致严重的下游模型故障，包括物体幻觉、安全关键实体的遗漏以及不一致的安全判断，这些影响通过新提出的语言级错位指标得以量化。这些发现揭示了像素级鲁棒性与多模态语义可靠性之间的脱节，凸显了当前视觉语言模型系统的关键局限，以及需要开发能明确考虑感知不确定性的评估框架。</div>
</details>
</div>
<div class="card">
<div class="title">Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Mikel Williams-Lekuona, Georgina Cosma</div>
<div class="meta-line">First: 2025-12-17T12:19:54+00:00 · Latest: 2026-01-15T16:58:39+00:00</div>
<div class="meta-line">Comments: Camera-ready version for ECIR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15372v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15372v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision transformers in vision-language models typically use the same amount of compute for every image, regardless of whether it is simple or complex. We propose ICAR (Image Complexity-Aware Retrieval), an adaptive computation approach that enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both the early-exit and full-depth paths. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance, attaining a Pearson correlation coefficient of 0.959 with human labelling whilst delivering 4.4x faster complexity prediction. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% faster image encoding while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效视觉语言模型的图像复杂度感知自适应检索</div>
<div class="mono" style="margin-top:8px">视觉语言模型中的视觉Transformer通常对每张图像使用相同的计算量，无论其简单或复杂。我们提出ICAR（图像复杂度感知检索），这是一种自适应计算方法，使视觉Transformer能够对简单图像使用更少计算，同时对复杂图像执行完整的网络深度处理。关键挑战在于保持跨模态对齐：来自不同处理深度的嵌入必须保持文本匹配的兼容性。ICAR通过双路径训练解决该问题，该训练从早期退出路径和完整深度路径生成兼容的嵌入。这确保了无论图像是早期退出还是完整处理，其表征与文本嵌入都能在同一语义空间中保持兼容。与现有需要昂贵重排序的两阶段方法不同，ICAR无需额外开销即可实现直接图文匹配。为确定计算量需求，我们开发了ConvNeXt-IC，将图像复杂度评估视为分类任务。通过采用现代分类器主干而非专用架构，ConvNeXt-IC实现了最先进的性能，其复杂度预测速度提升4.4倍，与人工标注的皮尔逊相关系数达0.959。在融合真实网络数据的标准基准测试中，ICAR在保持类别级性能及95%实例级性能的同时，实现了20%的图像编码加速，为视觉语言系统的可持续扩展提供了可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of vision transformers in vision-language models applying uniform computational cost to all images, regardless of complexity. The proposed method, ICAR, introduces an adaptive computation framework where a vision transformer uses less compute for simple images via early exits and full depth for complex ones, maintaining cross-modal alignment through a dual-path training strategy that ensures compatible embeddings from both paths. Key experimental results show that ICAR achieves 20% faster image encoding while maintaining category-level performance and 95% of instance-level retrieval accuracy on benchmarks augmented with web data, and its complexity predictor, ConvNeXt-IC, attains a 0.959 Pearson correlation with human labels while being 4.4x faster.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉-语言模型中视觉Transformer对所有图像均采用统一计算量而导致的效率低下问题。提出的方法ICAR是一种自适应计算框架，其中基于ConvNeXt的分类器ConvNeXt-IC首先评估图像复杂度，以将简单图像路由至早期退出路径，复杂图像则进行全深度处理；通过双路径训练确保来自两条路径的图像嵌入具有兼容性，从而维持跨模态对齐以实现直接文本匹配。实验结果表明，ConvNeXt-IC与人工复杂度标注的皮尔逊相关系数达到0.959，且预测速度比先前方法快4.4倍；ICAR在包含网络数据的基准测试中实现了20%的图像编码加速，同时保持了类别级别的性能以及95%的实例级别检索准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure</div>
<div class="meta-line">Authors: Luxuan Fu, Chong Liu, Bisheng Yang, Zhen Dong</div>
<div class="meta-line">First: 2026-01-15T16:16:34+00:00 · Latest: 2026-01-15T16:16:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10551v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10551v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>释放大型视觉语言模型在路侧基础设施智能感知中的潜力</div>
<div class="mono" style="margin-top:8px">城市路侧基础设施的自动化感知对智慧城市管理至关重要，但通用模型常难以捕捉必要的细粒度属性与领域规则。尽管大型视觉语言模型在开放世界识别方面表现优异，却常无法依据工程标准准确解析复杂的设施状态，导致实际应用可靠性不足。为此，我们提出一种领域自适应框架，将视觉语言模型转化为专业的基础设施智能分析智能体。该方法融合了数据高效微调策略与知识驱动的推理机制：首先基于Grounding DINO进行开放词汇微调，以最少监督实现多样化资产的鲁棒定位；随后通过Qwen-VL的LoRA适配进行深度语义属性推理。为减少幻觉并确保专业合规性，我们引入了双模态检索增强生成模块，在推理过程中动态检索权威行业标准与视觉范例。基于新构建的城市路侧场景数据集评估，本框架实现了58.9%的检测平均精度与95.5%的属性识别准确率，为基础设施智能监测提供了稳健解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of general-purpose vision-language models in accurately perceiving and interpreting fine-grained attributes of urban roadside infrastructure according to engineering standards, which is crucial for reliable smart city management. The proposed method transforms large vision-language models into specialized agents through a domain-adapted framework, combining open-vocabulary fine-tuning on Grounding DINO for robust asset localization with LoRA-based adaptation on Qwen-VL for semantic reasoning, and introduces a dual-modality retrieval-augmented generation module to mitigate hallucinations by dynamically retrieving authoritative standards and visual exemplars. Experimental results on a comprehensive urban roadside dataset show the framework achieves 58.9 mAP for detection and 95.5% accuracy for attribute recognition, demonstrating robust performance for intelligent infrastructure monitoring.</div>
<div class="mono" style="margin-top:8px">本研究针对通用视觉语言模型在准确感知和解释城市路边基础设施细粒度属性及领域规则方面的不足，这对于智慧城市管理至关重要。所提出的方法通过一个领域自适应框架，将大型视觉语言模型转化为专用智能体，该框架结合了基于Grounding DINO的开放词汇微调以实现最小监督下的鲁棒资产定位，以及基于Qwen-VL的LoRA适配以进行深度语义推理，并通过一个双模态检索增强生成模块来减少幻觉并确保符合权威行业标准。在一个综合的城市路边场景数据集上的实验评估表明，该框架实现了58.9%的mAP检测性能和95.5%的属性识别准确率，证明了其在智能基础设施监控方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery</div>
<div class="meta-line">Authors: Chong Liu, Luxuan Fu, Yang Jia, Zhen Dong, Bisheng Yang</div>
<div class="meta-line">First: 2026-01-15T15:57:18+00:00 · Latest: 2026-01-15T15:57:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVII-3D：基于稀疏街景图像实现分米级三维定位与理解的道路基础设施盘点技术进展</div>
<div class="mono" style="margin-top:8px">数字孪生与精确资产盘点的自动化创建是智慧城市建设和设施全生命周期管理的关键任务。然而，受限于鲁棒性不足、定位不精确及细粒度状态理解缺失，利用经济高效的稀疏图像仍具挑战。为此，本文提出SVII-3D——一个面向资产整体数字化的统一框架：首先融合LoRA微调的开集检测与空间注意力匹配网络，实现稀疏视角观测的鲁棒关联；其次引入几何引导优化机制以修正结构误差，达成分米级精度的三维定位；最后突破静态几何映射局限，集成基于多模态提示的视觉-语言模型智能体，自动诊断细粒度运行状态。实验表明SVII-3D显著提升识别精度并最小化定位误差，为高保真基础设施数字化提供了可扩展的经济解决方案，有效弥合稀疏感知与自动化智能维护间的鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for automated, cost-effective digital twins and asset inventories in smart cities, which is hindered by the limitations of using sparse street imagery, including poor robustness, inaccurate localization, and lack of fine-grained state understanding. To address this, the proposed SVII-3D framework integrates a LoRA fine-tuned open-set detector with a spatial-attention matching network for robust cross-view association, employs a geometry-guided refinement mechanism for precise decimeter-level 3D localization, and incorporates a Vision-Language Model agent with multi-modal prompting to diagnose fine-grained operational states. Experimental results show that the method significantly improves identification accuracy and minimizes localization errors, offering a scalable solution for high-fidelity infrastructure digitization.</div>
<div class="mono" style="margin-top:8px">本研究旨在为智慧城市自动化创建数字孪生和精确资产清单，以解决使用经济高效的稀疏街景图像时面临的鲁棒性不足、定位不准确和缺乏细粒度状态理解等挑战。提出的SVII-3D框架整合了三个关键方法：融合LoRA微调的开集检测与空间注意力匹配网络以实现鲁棒的跨视图关联，采用几何引导的细化机制实现分米级精度的3D定位，并集成一个利用多模态提示的视觉-语言模型代理来诊断细粒度的运行状态。实验结果表明，SVII-3D显著提高了识别精度并最小化了定位误差，为高保真基础设施数字化提供了一个可扩展且经济高效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">mergetune: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-15T15:15:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v1">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, \emph{continued fine-tuning (CFT)}, which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6\% on base-novel generalisation without adding parameters. % We show \emph{the first time} superior performance than CLIP on both DTD and EuroSAT, on cross-dataset transfer. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at \href{https://github.com/Surrey-UP-Lab/MERGETUNE}{https://github.com/Surrey-UP-Lab/MERGETUNE}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mergetune：视觉语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">微调视觉语言模型（如CLIP）常导致预训练知识的灾难性遗忘。先前研究主要旨在缓解适应过程中的遗忘，但遗忘在此过程中往往不可避免。我们提出一种新范式——持续微调（CFT），旨在零样本模型适应后恢复预训练知识。我们提出一种基于线性模式连通性（LMC）的简单、模型无关的CFT策略（命名为MERGETUNE），可事后应用于现有微调模型而无需架构修改。给定微调模型，我们持续微调其可训练参数（如软提示或线性头），以寻找一个具有两条低损失路径的持续模型：分别通向零样本（如CLIP）和微调（如CoOp）解。通过利用损失景观的几何特性，持续模型隐式合并两个解，恢复微调对应模型中丢失的预训练知识。挑战在于原始LMC约束需要预训练任务的数据回放。我们通过二阶代理近似零样本模型的约束，无需大规模数据回放。实验表明，MERGETUNE在不增加参数的情况下，将CoOp的基类-新类泛化调和平均值提升+5.6%。在跨数据集迁移中，我们首次在DTD和EuroSAT上实现优于CLIP的性能。在鲁棒微调评估中，MERGETUNE的LMC合并模型以更低推理成本超越集成基线，与零样本模型集成时获得进一步增益和最优结果。代码发布于https://github.com/Surrey-UP-Lab/MERGETUNE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the persistent issue of catastrophic forgetting in fine-tuned vision-language models, this research introduces a novel continued fine-tuning paradigm aimed at recovering lost pretrained knowledge post-adaptation. The proposed method, MERGETUNE, is a model-agnostic strategy that leverages linear mode connectivity to guide the continued fine-tuning of a model&#x27;s trainable parameters, seeking a solution with low-loss paths to both the original zero-shot and the fine-tuned models, thereby merging their capabilities without architectural changes; it approximates the connectivity constraint with a second-order surrogate to avoid large-scale data replay. Experimental results demonstrate that MERGETUNE improves the harmonic mean of CoOp by 5.6% on base-novel generalization, achieves superior performance to CLIP on datasets like DTD and EuroSAT in cross-dataset transfer, and, in robust fine-tuning evaluations, surpasses ensemble baselines with lower inference cost, reaching state-of-the-art results when ensembled with the zero-shot model.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型（VLM）微调中持续存在的灾难性遗忘问题，本研究提出了一种新颖的持续微调（CFT）范式，旨在模型适配后恢复丢失的预训练知识。所提出的方法MERGETUNE是一种模型无关的策略，它利用线性模式连通性（LMC）来指导模型可训练参数的持续微调，寻找一个同时与原始零样本模型和微调模型具有低损失路径的解，从而在不改变架构的情况下隐式融合两者的知识；该方法通过二阶代理近似LMC约束，避免了大规模数据回放。实验结果表明，MERGETUNE在基础-新类泛化上将CoOp的调和平均提高了5.6%，在跨数据集迁移任务中于DTD和EuroSAT等数据集上性能优于CLIP，并且在鲁棒微调评估中，以更低的推理成本超越了集成基线，与零样本模型集成时进一步取得了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">Urban Socio-Semantic Segmentation with Vision-Language Reasoning</div>
<div class="meta-line">Authors: Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li</div>
<div class="meta-line">First: 2026-01-15T15:00:36+00:00 · Latest: 2026-01-15T15:00:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10477v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10477v1">PDF</a> · <a href="https://github.com/AMAP-ML/SocioReasoner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&#x27;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言推理的城市社会语义分割</div>
<div class="mono" style="margin-top:8px">作为人类活动的枢纽，城市地表包含丰富的语义实体。从卫星图像中分割这些多样实体对一系列下游应用至关重要。当前先进的分割模型能可靠分割由物理属性定义的实体（如建筑、水体），但在处理社会定义类别（如学校、公园）时仍存在困难。本研究通过视觉语言模型推理实现社会语义分割。为此，我们推出了名为SocioSeg的城市社会语义分割数据集，该新资源包含卫星影像、数字地图以及按层级结构组织的社会语义实体像素级标签。此外，我们提出名为SocioReasoner的新型视觉语言推理框架，通过跨模态识别与多阶段推理模拟人类识别标注社会语义实体的过程。我们采用强化学习优化这一不可微分流程，激发视觉语言模型的推理能力。实验证明该方法优于当前最先进模型，并展现出强大的零样本泛化性能。数据集与代码公开于https://github.com/AMAP-ML/SocioReasoner。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of segmenting socially defined urban entities, such as schools and parks, from satellite imagery, which existing segmentation models struggle with despite their proficiency in identifying physically defined categories. The authors introduce the SocioSeg dataset, featuring satellite images, digital maps, and hierarchical pixel-level labels for social semantics, and propose the SocioReasoner framework that uses vision-language reasoning to mimic human annotation through cross-modal recognition and multi-stage reasoning, optimized via reinforcement learning. Experimental results show that this approach outperforms state-of-the-art models and exhibits strong zero-shot generalization capabilities.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从卫星图像中分割社会定义的城市场景实体（如学校、公园）的难题，现有分割模型虽能可靠处理物理定义的类别，但仍在此类任务上存在困难。作者引入了包含分层像素级社会语义标签的SocioSeg数据集，并提出了SocioReasoner框架，该框架通过跨模态识别和多阶段推理模拟人类标注过程，并利用强化学习进行优化。实验结果表明，该方法优于现有先进模型，并展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning</div>
<div class="meta-line">Authors: Guoqiang Liang, Jianyi Wang, Zhonghua Wu, Shangchen Zhou</div>
<div class="meta-line">First: 2026-01-06T11:00:17+00:00 · Latest: 2026-01-15T14:19:47+00:00</div>
<div class="meta-line">Comments: Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02918v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02918v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ethanliang99.github.io/ZOOMIQA-Projectpage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or providing low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA by jointly generating quality descriptions and scores. However, existing VLM-based IQA methods often suffer from unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions, and 2) reinforcement learning (RL) for dynamic policy exploration, stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, with a Progressive Re-sampling Strategy for mitigating annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Zoom-IQA：基于可靠区域感知推理的图像质量评估</div>
<div class="mono" style="margin-top:8px">图像质量评估是计算机视觉领域的长期难题。传统方法通常仅预测数值分数而缺乏解释，或提供缺乏精确分数的低层次描述。近期基于推理的视觉语言模型通过联合生成质量描述与分数，展现出解决IQA问题的强大潜力。然而，现有基于VLM的IQA方法因视觉与文本线索融合能力有限，常存在推理不可靠的问题。本研究提出Zoom-IQA模型，通过显式模拟关键认知行为——不确定性感知、区域推理与迭代优化——来应对这一挑战。具体采用两阶段训练框架：1）在自建的Grounded-Rationale-IQA数据集上进行监督微调，使模型学会将评估依据锚定于关键区域；2）通过强化学习进行动态策略探索，辅以KL-Coverage正则化器稳定训练过程以防止推理与评分多样性坍缩，并采用渐进重采样策略缓解标注偏差。大量实验表明，Zoom-IQA在鲁棒性、可解释性与泛化能力方面均有提升。在图像修复等下游任务中的应用进一步验证了该方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing vision-language models (VLMs) for image quality assessment (IQA), which often produce unreliable reasoning due to insufficient integration of visual and textual cues. The proposed Zoom-IQA model explicitly emulates key cognitive behaviors—uncertainty awareness, region reasoning, and iterative refinement—through a two-stage training pipeline: first, supervised fine-tuning on a Grounded-Rationale-IQA dataset to teach region-grounded assessment, followed by reinforcement learning stabilized by a KL-Coverage regularizer and a Progressive Re-sampling Strategy to prevent diversity collapse and mitigate annotation bias. Experimental results demonstrate that Zoom-IQA achieves improved robustness, explainability, and generalization, with effectiveness further validated in downstream tasks like image restoration.</div>
<div class="mono" style="margin-top:8px">本研究针对现有基于视觉语言模型（VLM）的图像质量评估（IQA）方法因视觉与文本线索整合不足而导致推理不可靠的问题，提出Zoom-IQA模型。该方法通过模拟不确定性感知、区域推理和迭代优化等关键认知行为，采用两阶段训练流程：首先在自建的Grounded-Rationale-IQA数据集上进行监督微调，使模型能够将评估依据锚定在关键区域；随后通过强化学习进行动态策略探索，并引入KL-Coverage正则化器防止推理与评分多样性坍缩，结合渐进重采样策略缓解标注偏差。实验表明，Zoom-IQA在鲁棒性、可解释性和泛化能力方面均有提升，在图像修复等下游任务中进一步验证了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Global Context Compression with Interleaved Vision-Text Transformation</div>
<div class="meta-line">Authors: Dian Jiao, Jiaxin Duan, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang</div>
<div class="meta-line">First: 2026-01-15T13:29:16+00:00 · Latest: 2026-01-15T13:29:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10378v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer&#x27;s input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全局上下文压缩与交错视觉-文本转换</div>
<div class="mono" style="margin-top:8px">视觉语言模型在端到端OCR中的近期成就，为文本信息的低损耗压缩开辟了新途径。这启发了早期研究将Transformer输入渲染为图像进行预填充，通过视觉编码有效减少令牌数量，从而缓解注意力计算的二次增长。然而，这种局部压缩在逐令牌推理阶段无法节省计算或内存成本。本文研究全局上下文压缩，在预填充和推理阶段均节省令牌。为此，我们提出VIST2——一种新型Transformer，它将输入文本块与其视觉编码交错排列，并仅依赖前文中的视觉令牌来预测下一个文本令牌分布。基于此思路，我们将文本块渲染为草图图像，并分多阶段训练VIST2：从课程调度的光学语言建模预训练开始，再到模态交错指令微调。通过规模从0.6B到8B的VIST2系列模型进行大量实验，探索训练方案和超参数。在4倍压缩比下，所得模型在长文本生成任务上显著优于基线，平均实现首令牌生成速度提升3倍、内存使用降低77%、浮点运算量减少74%。代码与数据集将公开以支持后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency of partial compression methods that only reduce tokens during prefilling but not during token-by-token inference, this research introduces VIST2, a Transformer model designed for global context compression. The method interleaves text chunks with their visual encodings, using exclusively visual tokens from the pre-context to predict subsequent text token distributions, and involves rendering text into sketch images followed by multi-stage training including curriculum-scheduled pretraining and modal-interleaved instruction tuning. Experimental results with models scaled from 0.6B to 8B parameters show that at a 4× compression ratio, VIST2 significantly outperforms baselines on long writing tasks, delivering on average a 3× speedup in first-token generation, a 77% reduction in memory usage, and a 74% reduction in FLOPS.</div>
<div class="mono" style="margin-top:8px">为解决现有部分压缩方法仅在预填充阶段减少令牌、而在逐令牌推理阶段无法节省计算或内存成本的问题，本研究提出了VIST2，一种用于全局上下文压缩的Transformer模型。该方法将输入文本块与其视觉编码交错排列，并仅依赖前文中的视觉令牌来预测后续文本令牌的分布；其技术流程包括将文本渲染为草图图像，并采用包含课程式调度的光学语言建模预训练和模态交错指令微调的多阶段训练方案。在参数规模从0.6B到8B的模型上进行广泛实验的结果表明，在4倍压缩比下，VIST2在长文本生成任务上显著优于基线模型，平均实现了首令牌生成速度提升3倍、内存使用减少77%以及浮点运算量减少74%的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models</div>
<div class="meta-line">Authors: Peng-Fei Zhang, Zi Huang</div>
<div class="meta-line">First: 2026-01-15T11:45:56+00:00 · Latest: 2026-01-15T11:45:56+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10313v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10313v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型通用多模态攻击的层次化精炼</div>
<div class="mono" style="margin-top:8px">现有视觉语言预训练模型的对抗攻击多为样本特异性方法，在大规模数据集或新场景中扩展时会产生巨大计算开销。为克服此局限，本文提出层次化精炼攻击——一种面向VLP模型的通用多模态攻击框架。该框架在样本层面和优化层面同步精炼通用对抗扰动：在图像模态中，通过解耦对抗样本为干净图像与扰动分量，实现独立处理以更有效破坏跨模态对齐；引入ScMix增强策略以丰富视觉语境，强化通用扰动的全局与局部效用，降低对伪特征的依赖；在优化层面，利用历史梯度与预估未来梯度构建时序层次结构，避免局部最优并稳定通用扰动学习。在文本模态中，通过融合句内与句间重要性度量定位全局关键词，并将其作为通用文本扰动。跨下游任务、VLP模型及数据集的广泛实验验证了所提通用多模态攻击的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency of sample-specific adversarial attacks on vision-language pretraining (VLP) models, this work proposes the Hierarchical Refinement Attack (HRA), a universal multimodal attack framework. The method refines universal adversarial perturbations (UAPs) hierarchically: for images, it disentangles clean images from perturbations and employs a ScMix augmentation strategy to diversify visual contexts and reduce reliance on spurious features, while also using a temporal hierarchy of gradients to stabilize optimization; for text, it identifies globally influential words as universal perturbations. Experimental results across various VLP models, datasets, and downstream tasks demonstrate the framework&#x27;s superior attack performance.</div>
<div class="mono" style="margin-top:8px">针对视觉语言预训练（VLP）模型中样本特定对抗攻击计算开销大的问题，本研究提出了分层精炼攻击（HRA），一种通用的多模态攻击框架。该方法分层优化通用对抗扰动（UAP）：对于图像，它将干净图像与扰动解耦，并采用ScMix增强策略以多样化视觉上下文并减少对虚假特征的依赖，同时利用历史与未来预估梯度的时序层次来稳定优化；对于文本，它通过结合句内和句间重要性度量来识别具有全局影响力的词作为通用文本扰动。在多种VLP模型、数据集和下游任务上的广泛实验证明了该攻击方法的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">A Study of Commonsense Reasoning over Visual Object Properties</div>
<div class="meta-line">Authors: Abhishek Kolari, Mohammadhossein Khojasteh, Yifan Jiang, Floris den Hengst, Filip Ilievski</div>
<div class="meta-line">First: 2025-08-14T11:28:40+00:00 · Latest: 2026-01-15T11:10:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10956v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10956v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by human categorization, object property reasoning involves identifying and recognizing low-level details and higher-level abstractions. While current visual question answering (VQA) studies consider multiple object properties, such as size, they typically blend perception and reasoning and lack representativeness in terms of reasoning and image categories, making it unclear whether and how vision-language models (VLMs) abstract and reason over depicted objects. To this end, we introduce a systematic evaluation framework comprising images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions, informed by prior work on common sense. We develop a procedure to instantiate this framework in two VQA object reasoning benchmarks: OPTICS-CNT, comprising 360 images paired with 1,080 multi-level, count-based questions, and OPTICS-CMP, with 2.1k comparison questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations relative to humans, with the best-performing model achieving below 40% counting and 70% comparison accuracy. VLMs struggle particularly with photographic images, counterfactual reasoning, physical and functional properties, and higher counts. We make the OPTICS benchmark data and code available to support future work on scalable benchmarking methods, generalized annotation guidelines, and advanced reasoning VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉对象属性常识推理研究</div>
<div class="mono" style="margin-top:8px">受人类分类能力启发，对象属性推理涉及对低层次细节与高层次抽象特征的识别与认知。当前视觉问答研究虽涉及尺寸等多重对象属性，但常混淆感知与推理过程，且在推理逻辑与图像类别代表性上存在不足，导致视觉语言模型对描绘对象的抽象与推理机制尚不明确。为此，我们基于常识研究基础，构建了包含三类代表性图像、三级递进复杂度推理层次及四维对象属性的系统评估框架。通过在两大量化视觉问答基准（OPTICS-CNT含360张图像及1080道多层级计数问题，OPTICS-CMP含2100道对比问题）中实例化该框架，对12个前沿视觉语言模型进行零样本实验。结果显示模型存在显著局限：最优模型计数准确率不足40%，对比准确率低于70%，尤其在真实照片、反事实推理、物理功能属性及高数量场景中表现欠佳。我们公开OPTICS基准数据与代码，以支持可扩展评测方法、通用标注准则及高阶推理视觉语言模型的后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the unclear capability of vision-language models (VLMs) in abstracting and reasoning over object properties, as existing visual question answering (VQA) benchmarks often blend perception with reasoning and lack representativeness. To systematically evaluate VLMs, the authors introduce a framework based on three image types, three reasoning levels, and four property dimensions, instantiated in two benchmarks: OPTICS-CNT with 1,080 count-based questions and OPTICS-CMP with 2.1k comparison questions. In zero-shot experiments with 12 state-of-the-art VLMs, results show significant limitations compared to humans, with the best model achieving below 40% accuracy on counting and 70% on comparison, particularly struggling with photographic images, counterfactual reasoning, physical/functional properties, and higher counts.</div>
<div class="mono" style="margin-top:8px">本研究旨在系统评估视觉语言模型（VLMs）在物体属性常识推理上的能力，因为现有的视觉问答（VQA）基准通常将感知与推理混为一谈且缺乏代表性。作者基于三种图像类型、三个推理复杂度级别和四个属性维度，构建了一个结构化评估框架，并将其具体化为两个新基准：用于计数问题的OPTICS-CNT和用于比较问题的OPTICS-CMP。在对12个先进VLM进行的零样本实验中，结果显示其性能与人类存在显著差距，最佳模型在计数任务上准确率低于40%，在比较任务上低于70%，尤其在真实照片图像、反事实推理以及更高层次的物理和功能属性方面表现出明显困难。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Adversarial Gumbel MCTS for Abstract Visual Composition Generation</div>
<div class="meta-line">Authors: Zirui Zhao, Boye Niu, David Hsu, Wee Sun Lee</div>
<div class="meta-line">First: 2025-12-01T03:38:44+00:00 · Latest: 2026-01-15T07:18:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01242v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.01242v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study abstract visual composition, in which identity is primarily determined by the spatial configuration and relations among a small set of geometric primitives (e.g., parts, symmetry, topology). They are invariant primarily to texture and photorealistic detail. Composing such structures from fixed components under geometric constraints and vague goal specification (such as text) is non-trivial due to combinatorial placement choices, limited data, and discrete feasibility (overlap-free, allowable orientations), which create a sparse solution manifold ill-suited to purely statistical pixel-space generators. We propose a constraint-guided framework that combines explicit geometric reasoning with neural semantics. An AlphaGo-style search enforces feasibility, while a fine-tuned vision-language model scores semantic alignment as reward signals. Our algorithm uses a policy network as a heuristic in Monte-Carlo Tree Search and fine-tunes the network via search-generated plans. Inspired by the Generative Adversarial Network, we use the generated instances for adversarial reward refinement. Over time, the generation should approach the actual data more closely when the reward model cannot distinguish between generated instances and ground-truth. In the Tangram Assembly task, our approach yields higher validity and semantic fidelity than diffusion and auto-regressive baselines, especially as constraints tighten.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成对抗式Gumbel蒙特卡洛树搜索用于抽象视觉构图生成</div>
<div class="mono" style="margin-top:8px">本研究聚焦抽象视觉构图，其本质特征由少量几何基元（如部件、对称性、拓扑结构）的空间配置与关系决定，对纹理和写实细节具有不变性。在几何约束与模糊目标（如文本描述）下，使用固定组件构建此类结构面临组合排列选择、数据稀缺及离散可行性（无重叠、允许朝向）等挑战，形成稀疏解空间，不适用于纯统计的像素空间生成器。我们提出一种约束引导框架，融合显式几何推理与神经语义理解：通过AlphaGo式搜索确保可行性，利用微调的视觉语言模型对语义对齐度进行奖励评分。算法采用策略网络作为蒙特卡洛树搜索的启发函数，并通过搜索生成的规划方案微调网络。受生成对抗网络启发，利用生成实例进行对抗性奖励优化——当奖励模型无法区分生成实例与真实数据时，生成结果将逐步逼近实际数据分布。在七巧板拼图任务中，本方法在有效性与语义保真度上均优于扩散模型和自回归基线，尤其在约束收紧时优势显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating abstract visual compositions, where the identity is defined by spatial arrangements of geometric primitives under constraints like overlap avoidance and allowable orientations, which creates a sparse solution space difficult for standard pixel-based generative models. The proposed method integrates explicit geometric reasoning with neural semantics by employing a Monte-Carlo Tree Search guided by a policy network to enforce feasibility, while a fine-tuned vision-language model provides semantic alignment scores as rewards; the system adversarially refines these rewards using generated instances to better approximate real data. Experimental results on the Tangram Assembly task demonstrate that this approach achieves higher validity and semantic fidelity compared to diffusion and auto-regressive baselines, particularly under tighter constraints.</div>
<div class="mono" style="margin-top:8px">本研究针对抽象视觉组合生成问题，其身份由几何基元在约束下的空间排列定义，由于组合选择多样和可行解稀疏，该任务具有挑战性。所提方法结合几何推理与神经语义，通过采用AlphaGo风格的蒙特卡洛树搜索，由策略网络引导以确保可行性，同时使用微调的视觉语言模型提供语义对齐奖励，并利用生成实例进行对抗性奖励优化以提升判别能力。在七巧板组装任务上的实验结果表明，该方法相比扩散和自回归基线，在更严格的约束下实现了更高的有效性和语义保真度。</div>
</details>
</div>
<div class="card">
<div class="title">Fusionista2.0: Efficiency Retrieval System for Large-Scale Datasets</div>
<div class="meta-line">Authors: Huy M. Le, Dat Tien Nguyen, Phuc Binh Nguyen, Gia Bao Le Tran, Phu Truong Thien, Cuong Dinh, Minh Nguyen, Nga Nguyen, Thuy T. N. Nguyen, Tan Nhat Nguyen, Binh T. Nguyen</div>
<div class="meta-line">First: 2025-11-15T15:23:44+00:00 · Latest: 2026-01-15T06:23:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12255v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12255v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Video Browser Showdown (VBS) challenges systems to deliver accurate results under strict time constraints. To meet this demand, we present Fusionista2.0, a streamlined video retrieval system optimized for speed and usability. All core modules were re-engineered for efficiency: preprocessing now relies on ffmpeg for fast keyframe extraction, optical character recognition uses Vintern-1B-v3.5 for robust multilingual text recognition, and automatic speech recognition employs faster-whisper for real-time transcription. For question answering, lightweight vision-language models provide quick responses without the heavy cost of large models. Beyond these technical upgrades, Fusionista2.0 introduces a redesigned user interface with improved responsiveness, accessibility, and workflow efficiency, enabling even non-expert users to retrieve relevant content rapidly. Evaluations demonstrate that retrieval time was reduced by up to 75% while accuracy and user satisfaction both increased, confirming Fusionista2.0 as a competitive and user-friendly system for large-scale video search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fusionista2.0：面向大规模数据集的效率检索系统</div>
<div class="mono" style="margin-top:8px">视频浏览器大赛（VBS）要求系统在严格时间限制下提供精准结果。为满足这一需求，我们推出Fusionista2.0——一个为速度和可用性优化的精简视频检索系统。所有核心模块均经过效率重构：预处理环节采用ffmpeg实现快速关键帧提取，光学字符识别使用Vintern-1B-v3.5进行鲁棒的多语言文本识别，自动语音识别则通过faster-whisper实现实时转录。在问答模块中，轻量级视觉语言模型以较低成本提供快速响应。除技术升级外，Fusionista2.0还重新设计了用户界面，显著提升了响应速度、可访问性与工作流效率，使非专业用户也能快速检索相关内容。评估数据显示，系统检索时间最高减少75%，同时准确率与用户满意度均获提升，证实Fusionista2.0是具备竞争力且用户友好的大规模视频检索系统。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Video Browser Showdown (VBS) requires systems to perform accurate video retrieval under strict time limits, motivating the development of Fusionista2.0, a system optimized for speed and usability. The method involves re-engineering core modules for efficiency: using ffmpeg for fast keyframe extraction, Vintern-1B-v3.5 for robust multilingual optical character recognition, faster-whisper for real-time automatic speech recognition, and lightweight vision-language models for quick question answering, all integrated into a redesigned, responsive user interface. Experimental evaluations show that retrieval time was reduced by up to 75%, while both accuracy and user satisfaction increased, confirming the system&#x27;s competitiveness for large-scale video search.</div>
<div class="mono" style="margin-top:8px">为满足视频浏览器竞赛在严格时间限制下进行准确视频检索的需求，本研究提出了经过效率优化的Fusionista2.0系统。该方法对核心模块进行了重构：使用ffmpeg进行快速关键帧提取，采用Vintern-1B-v3.5进行多语言OCR识别，利用faster-whisper实现实时语音识别，并部署轻量级视觉语言模型进行问答，同时重新设计了用户界面。实验评估表明，该系统将检索时间减少了高达75%，同时提高了准确性和用户满意度，证实了其在大规模视频搜索中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">V-Zero: Self-Improving Multimodal Reasoning with Zero Annotation</div>
<div class="meta-line">Authors: Han Wang, Yi Yang, Jingyuan Hu, Minfeng Zhu, Wei Chen</div>
<div class="meta-line">First: 2026-01-15T05:47:43+00:00 · Latest: 2026-01-15T05:47:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10094v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10094v1">PDF</a> · <a href="https://github.com/SatonoDia/V-Zero">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal learning have significantly enhanced the reasoning capabilities of vision-language models (VLMs). However, state-of-the-art approaches rely heavily on large-scale human-annotated datasets, which are costly and time-consuming to acquire. To overcome this limitation, we introduce V-Zero, a general post-training framework that facilitates self-improvement using exclusively unlabeled images. V-Zero establishes a co-evolutionary loop by instantiating two distinct roles: a Questioner and a Solver. The Questioner learns to synthesize high-quality, challenging questions by leveraging a dual-track reasoning reward that contrasts intuitive guesses with reasoned results. The Solver is optimized using pseudo-labels derived from majority voting over its own sampled responses. Both roles are trained iteratively via Group Relative Policy Optimization (GRPO), driving a cycle of mutual enhancement. Remarkably, without a single human annotation, V-Zero achieves consistent performance gains on Qwen2.5-VL-7B-Instruct, improving visual mathematical reasoning by +1.7 and general vision-centric by +2.6, demonstrating the potential of self-improvement in multimodal systems. Code is available at https://github.com/SatonoDia/V-Zero</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-Zero：零标注下的自增强多模态推理</div>
<div class="mono" style="margin-top:8px">多模态学习的最新进展显著提升了视觉语言模型的推理能力。然而，当前最先进的方法严重依赖大规模人工标注数据集，其获取成本高昂且耗时。为克服这一局限，我们提出了V-Zero——一个通用的后训练框架，仅利用未标注图像实现自我增强。该框架通过实例化两个独立角色（提问者与解答者）建立协同进化循环：提问者通过对比直觉猜测与推理结果的双轨推理奖励机制，学习合成高质量挑战性问题；解答者则基于自身采样响应的多数投票生成伪标签进行优化。两者通过组相对策略优化进行迭代训练，形成相互增强的循环。值得注意的是，在完全无需人工标注的情况下，V-Zero在Qwen2.5-VL-7B-Instruct模型上实现了持续性能提升，视觉数学推理能力提升+1.7，通用视觉中心任务提升+2.6，彰显了多模态系统自我增强的潜力。代码已开源：https://github.com/SatonoDia/V-Zero</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high cost and time required for human-annotated datasets in multimodal learning, this paper introduces V-Zero, a self-improving post-training framework that uses only unlabeled images. The method establishes a co-evolutionary loop with a Questioner that synthesizes challenging questions using a dual-track reasoning reward and a Solver optimized via pseudo-labels from majority voting of its own responses, both trained iteratively with Group Relative Policy Optimization. Experimental results show that without human annotation, V-Zero improves the Qwen2.5-VL-7B-Instruct model, achieving gains of +1.7 on visual mathematical reasoning and +2.6 on general vision-centric tasks.</div>
<div class="mono" style="margin-top:8px">为解决多模态学习中人工标注数据集成本高、耗时长的问题，本文提出了V-Zero，一种仅使用未标注图像的自改进后训练框架。该方法建立了一个协同进化循环，包含一个通过双轨推理奖励合成挑战性问题的提问者，以及一个通过自身响应多数投票生成的伪标签进行优化的求解器，两者均通过组相对策略优化进行迭代训练。实验结果表明，在无人标注的情况下，V-Zero在Qwen2.5-VL-7B-Instruct模型上实现了性能提升，视觉数学推理能力提高了+1.7，通用视觉中心推理能力提高了+2.6。</div>
</details>
</div>
<div class="card">
<div class="title">Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model</div>
<div class="meta-line">Authors: Siwen Jiao, Tianxiong Lv, Kangan Qian, Chenxu Zhao, Xiuyuan Zhu, Tianlun Li, Xiaolong Cheng, Jinyu Li, Zhihao Liao, Yang Cai</div>
<div class="meta-line">First: 2026-01-12T16:26:42+00:00 · Latest: 2026-01-15T03:58:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07695v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07695v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) face a critical bottleneck in achieving precise numerical prediction for 3D scene understanding. Traditional reinforcement learning (RL) approaches, primarily based on relative ranking, often suffer from severe reward sparsity and gradient instability, failing to effectively exploit the verifiable signals provided by 3D physical constraints. Notably, in standard GRPO frameworks, relative normalization causes &quot;near-miss&quot; samples (characterized by small but non-zero errors) to suffer from advantage collapse. This leads to a severe data utilization bottleneck where valuable boundary samples are discarded during optimization. To address this, we introduce the Smooth Numerical Reward Activation (SNRA) operator and the Absolute-Preserving GRPO (AP-GRPO) framework. SNRA employs a dynamically parameterized Sigmoid function to transform raw feedback into a dense, continuous reward continuum. Concurrently, AP-GRPO integrates absolute scalar gradients to mitigate the numerical information loss inherent in conventional relative-ranking mechanisms. By leveraging this approach, we constructed Numerical3D-50k, a dataset comprising 50,000 verifiable 3D subtasks. Empirical results indicate that AP-GRPO achieves performance parity with large-scale supervised methods while maintaining higher data efficiency, effectively activating latent 3D reasoning in VLMs without requiring architectural modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑算子：平滑可验证奖励激活视觉语言模型的空间推理能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型在实现三维场景理解的精确数值预测方面面临关键瓶颈。传统基于相对排序的强化学习方法常受奖励稀疏性和梯度不稳定性困扰，难以有效利用三维物理约束提供的可验证信号。尤其在标准GRPO框架中，相对归一化会导致“近失”样本（即误差微小但非零）遭遇优势崩溃，造成宝贵边界样本在优化中被丢弃的严重数据利用瓶颈。为此，我们提出平滑数值奖励激活算子与绝对保持GRPO框架。SNRA采用动态参数化Sigmoid函数将原始反馈转化为稠密连续的奖励连续体；AP-GRPO则通过整合绝对标量梯度来缓解传统相对排序机制固有的数值信息损失。基于此构建的Numerical3D-50k数据集包含5万个可验证三维子任务。实验表明，AP-GRPO在保持更高数据效率的同时达到与大规模监督方法相当的性能，无需修改模型架构即可有效激活视觉语言模型的潜在三维推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the bottleneck in Vision-Language Models (VLMs) for precise numerical prediction in 3D scene understanding, where traditional reinforcement learning methods suffer from reward sparsity and gradient instability, particularly causing &#x27;near-miss&#x27; samples to be discarded. The proposed method introduces the Smooth Numerical Reward Activation (SNRA) operator, which uses a dynamic Sigmoid function to create a dense reward signal, and the Absolute-Preserving GRPO (AP-GRPO) framework to preserve absolute gradient information, mitigating information loss from relative ranking. Key experimental findings, based on a new 50,000-task dataset (Numerical3D-50k), show that AP-GRPO matches the performance of large-scale supervised methods with higher data efficiency, effectively activating latent 3D reasoning in VLMs without architectural changes.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在三维场景理解中进行精确数值预测的瓶颈问题，传统强化学习方法存在奖励稀疏和梯度不稳定，导致“接近正确”的样本被丢弃。提出的解决方案是平滑数值奖励激活算子和绝对保留GRPO框架，它使用参数化Sigmoid函数生成密集连续的奖励，并整合绝对标量梯度以保留数值信息。在构建的Numerical3D-50k数据集上的实验表明，该框架在不修改模型架构的情况下，以更高的数据效率达到了大规模监督方法的性能水平，有效激活了视觉语言模型的潜在三维推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">The Spatial Blindspot of Vision-Language Models</div>
<div class="meta-line">Authors: Nahid Alam, Leema Krishna Murali, Siddhant Bharadwaj, Patrick Liu, Timothy Chung, Drishti Sharma, Akshata A, Kranthi Kiran, Wesley Tam, Bala Krishna S Vegesna</div>
<div class="meta-line">First: 2026-01-15T00:30:34+00:00 · Latest: 2026-01-15T00:30:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have advanced rapidly, but their ability to capture spatial relationships remains a blindspot. Current VLMs are typically built with contrastive language-image pretraining (CLIP) style image encoders. The training recipe often flattens images into 1D patch sequences, discarding the 2D structure necessary for spatial reasoning. We argue that this lack of spatial awareness is a missing dimension in VLM design and a bottleneck for applications requiring spatial grounding, such as robotics and embodied AI. To address this, we investigate (i) image encoders trained with alternative objectives and (ii) 2D positional encodings. Our experiments show that these architectural choices can lead to improved spatial reasoning on several benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型的空间盲点</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）发展迅速，但其捕捉空间关系的能力仍存在盲点。当前VLM通常基于对比语言-图像预训练（CLIP）风格的图像编码器构建，其训练方法常将图像展平为一维补丁序列，丢弃了空间推理所需的二维结构。我们认为，这种空间感知能力的缺失是VLM设计中的一个空白维度，也是机器人学与具身AI等需要空间基础的应用瓶颈。为此，我们研究了（i）采用替代目标训练的图像编码器与（ii）二维位置编码。实验表明，这些架构选择可在多个基准测试中提升空间推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models (VLMs) often fail to capture spatial relationships, which is a critical limitation for applications like robotics and embodied AI. This spatial blindspot is attributed to the prevalent use of CLIP-style image encoders that flatten images into 1D patch sequences, discarding essential 2D structure. To address this, the study investigates alternative image encoder training objectives and the incorporation of 2D positional encodings. Experimental results demonstrate that these architectural modifications lead to measurable improvements in spatial reasoning performance across multiple benchmarks.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在捕捉空间关系方面存在明显缺陷，这成为机器人学和具身智能等应用的关键瓶颈。该问题源于当前主流的基于CLIP风格的图像编码器通常将图像展平为一维序列，从而丢失了必要的二维结构信息。为此，本研究探索了替代的图像编码器训练目标以及二维位置编码的引入。实验结果表明，这些架构上的改进在多个基准测试上有效提升了模型的空间推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">MedVL-SAM2: A unified 3D medical vision-language model for multimodal reasoning and prompt-driven segmentation</div>
<div class="meta-line">Authors: Yang Xing, Jiong Wu, Savas Ozdemir, Ying Zhang, Yang Yang, Wei Shao, Kuang Gong</div>
<div class="meta-line">First: 2026-01-14T21:21:00+00:00 · Latest: 2026-01-14T21:21:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09879v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in medical vision-language models (VLMs) has achieved strong performance on image-level text-centric tasks such as report generation and visual question answering (VQA). However, achieving fine-grained visual grounding and volumetric spatial reasoning in 3D medical VLMs remains challenging, particularly when aiming to unify these capabilities within a single, generalizable framework. To address this challenge, we proposed MedVL-SAM2, a unified 3D medical multimodal model that concurrently supports report generation, VQA, and multi-paradigm segmentation, including semantic, referring, and interactive segmentation. MedVL-SAM2 integrates image-level reasoning and pixel-level perception through a cohesive architecture tailored for 3D medical imaging, and incorporates a SAM2-based volumetric segmentation module to enable precise multi-granular spatial reasoning. The model is trained in a multi-stage pipeline: it is first pre-trained on a large-scale corpus of 3D CT image-text pairs to align volumetric visual features with radiology-language embeddings. It is then jointly optimized with both language-understanding and segmentation objectives using a comprehensive 3D CT segmentation dataset. This joint training enables flexible interaction via language, point, or box prompts, thereby unifying high-level visual reasoning with spatially precise localization. Our unified architecture delivers state-of-the-art performance across report generation, VQA, and multiple 3D segmentation tasks. Extensive analyses further show that the model provides reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning, demonstrating that high-level semantic reasoning and precise 3D localization can be jointly achieved within a unified 3D medical VLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedVL-SAM2：用于多模态推理与提示驱动分割的统一三维医学视觉语言模型</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型（VLMs）近期在图像级文本中心任务（如报告生成和视觉问答）上取得了显著进展。然而，在三维医学VLMs中实现细粒度视觉定位和体素空间推理仍具挑战性，尤其是在单一通用框架内统一这些能力。为此，我们提出MedVL-SAM2——一个统一的三维医学多模态模型，同步支持报告生成、视觉问答及多范式分割（包括语义分割、指代分割和交互式分割）。该模型通过专为三维医学影像设计的融合架构，整合图像级推理与像素级感知，并引入基于SAM2的体素分割模块以实现精准多粒度空间推理。训练采用多阶段流程：首先在大规模三维CT图文对语料库上进行预训练，以对齐体素视觉特征与放射学语言嵌入；随后使用综合三维CT分割数据集，联合优化语言理解与分割目标。这种联合训练支持通过语言、点或框提示进行灵活交互，从而统一高层视觉推理与空间精确定位。我们的统一架构在报告生成、视觉问答及多项三维分割任务中均达到最先进性能。深入分析进一步表明，该模型能提供可靠的三维视觉定位、可控交互式分割及鲁棒跨模态推理，证明高层语义推理与精准三维定位可在统一的三维医学VLM中协同实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the challenge of achieving fine-grained visual grounding and volumetric spatial reasoning within a unified 3D medical vision-language model, this work introduces MedVL-SAM2. The method integrates image-level reasoning and pixel-level perception through a cohesive architecture, employing a multi-stage training pipeline that first pre-trains on 3D CT image-text pairs and then jointly optimizes for language understanding and segmentation using a comprehensive 3D CT segmentation dataset. Key experimental results demonstrate state-of-the-art performance in report generation, VQA, and multiple 3D segmentation tasks, with the model providing reliable 3D visual grounding, controllable interactive segmentation, and robust cross-modal reasoning.</div>
<div class="mono" style="margin-top:8px">现有的医学视觉语言模型在报告生成和视觉问答等图像级任务上表现出色，但在细粒度的三维视觉定位和体空间推理方面仍面临挑战。为此，研究者提出了MedVL-SAM2，这是一个统一的三维医学多模态模型，通过整合基于SAM2的体分割模块，能够同时支持报告生成、视觉问答以及语义、参考和交互式分割。该模型采用多阶段训练流程，包括在大规模三维CT图像-文本对上进行预训练，并结合语言理解和分割目标进行联合优化。实验结果表明，该模型在报告生成、视觉问答和多种三维分割任务上达到了最先进的性能，提供了可靠的三维视觉定位、可控的交互式分割和稳健的跨模态推理能力。</div>
</details>
</div>
<div class="card">
<div class="title">ViSIL: Unified Evaluation of Information Loss in Multimodal Video Captioning</div>
<div class="meta-line">Authors: Po-han Li, Shenghui Chen, Ufuk Topcu, Sandeep Chinchali</div>
<div class="meta-line">First: 2026-01-14T20:14:47+00:00 · Latest: 2026-01-14T20:14:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09851v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal video captioning condenses dense footage into a structured format of keyframes and natural language. By creating a cohesive multimodal summary, this approach anchors generative AI in rich semantic evidence and serves as a lightweight proxy for high-efficiency retrieval. However, traditional metrics like BLEU or ROUGE fail to quantify information coverage across disparate modalities, such as comparing a paragraph of text to a sequence of keyframes. To address this, we propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that quantifies the video information not captured by a summary via vision-language model (VLM) inference. By measuring the information loss, ViSIL is a unified metric that enables direct comparison across multimodal summary formats despite their structural discrepancies. Our results demonstrate that ViSIL scores show a statistically significant correlation with both human and VLM performance on Video Question Answering (VQA) tasks. ViSIL also enables summary selection to optimize the trade-off between information loss and processing speed, establishing a Pareto-optimal frontier that outperforms text summaries by $7\%$ in VQA accuracy without increasing processing load.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViSIL：多模态视频描述中信息损失的统一评估</div>
<div class="mono" style="margin-top:8px">多模态视频描述将密集视频内容压缩为关键帧与自然语言的结构化格式。通过创建统一的多模态摘要，该方法将生成式AI锚定于丰富的语义证据，并作为高效检索的轻量级代理。然而，传统指标（如BLEU或ROUGE）无法量化跨异质模态的信息覆盖度，例如比较文本段落与关键帧序列。为此，我们提出视频摘要信息损失（ViSIL）评分——一种基于信息论的框架，通过视觉语言模型（VLM）推理量化摘要未捕获的视频信息。通过测量信息损失，ViSIL作为统一指标，可在结构差异显著的多模态摘要格式间直接比较。实验表明，ViSIL评分与人类及VLM在视频问答（VQA）任务上的表现均呈现统计显著相关性。ViSIL还能通过摘要选择优化信息损失与处理速度的权衡，构建的帕累托最优前沿在VQA准确率上超越纯文本摘要7%，且未增加处理负载。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inability of traditional metrics like BLEU or ROUGE to quantify information coverage across different modalities in multimodal video captioning, which combines keyframes and text. To solve this, the authors propose the Video Summary Information Loss (ViSIL) score, an information-theoretic framework that uses vision-language model inference to measure the video information not captured by a summary. Experimental results show ViSIL scores correlate significantly with human and VLM performance on Video Question Answering tasks, and using ViSIL for summary selection establishes a Pareto-optimal frontier that improves VQA accuracy by 7% over text summaries without increasing processing load.</div>
<div class="mono" style="margin-top:8px">该研究针对多模态视频描述中传统指标（如BLEU或ROUGE）无法量化跨模态（如关键帧与文本）信息覆盖的问题。为此，作者提出了视频摘要信息损失（ViSIL）分数，这是一个基于信息论的框架，利用视觉语言模型推理来测量摘要未捕获的视频信息。实验结果表明，ViSIL分数与人类和VLM在视频问答任务上的表现存在显著相关性，并且使用ViSIL进行摘要选择能够建立一个帕累托最优前沿，在视频问答准确率上比纯文本摘要提高7%，同时不增加处理负担。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Animal Identification for Long Videos</div>
<div class="meta-line">Authors: Xuyang Fang, Sion Hannuna, Edwin Simpson, Neill Campbell</div>
<div class="meta-line">First: 2026-01-14T17:53:59+00:00 · Latest: 2026-01-14T17:53:59+00:00</div>
<div class="meta-line">Comments: 11 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09663v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09663v1">PDF</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves}{here">Code1</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($&gt;$97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长视频的自监督动物个体识别方法</div>
<div class="mono" style="margin-top:8px">长时视频中的动物个体识别对行为生态学、野生动物监测和畜牧管理至关重要。传统方法依赖大量人工标注，现有自监督方法则因内存限制和时间误差传播问题，计算成本高昂且不适用于长序列。本文提出一种高效的自监督方法，将动物识别重构为全局聚类任务而非序列跟踪问题。该方法基于单视频中个体数量已知且固定的常见场景，仅需边界框检测和总数信息。通过采样帧对、使用冻结预训练主干网络，并采用匈牙利算法进行批内伪标签分配的自举机制，可在无需身份标签的情况下学习判别性特征。我们借鉴视觉语言模型的二元交叉熵损失函数，在每批次GPU内存消耗低于1GB（比标准对比方法低一个数量级）的情况下实现最先进准确率（&gt;97%）。在具有挑战性的真实数据集（3D-POP鸽群视频和8头犊牛进食视频）上的评估表明，本框架达到甚至超越了基于1000多帧标注数据的监督基线，有效消除了人工标注瓶颈。这项研究使得在消费级硬件上实现高精度动物个体识别成为可能，在资源受限的研究场景中具有广泛适用性。本文所有代码已发布于\href{https://huggingface.co/datasets/tonyFang04/8-calves}{此处}。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for efficient individual animal identification in long videos, crucial for behavioral ecology and wildlife monitoring, by overcoming the limitations of manual annotation and computationally intensive self-supervised methods. The method reframes identification as a global clustering task, assuming a fixed number of individuals, and uses only bounding boxes and the total count; it employs frame pair sampling, a frozen pre-trained backbone, and a self-bootstrapping mechanism with the Hungarian algorithm for pseudo-label assignment to learn features without identity labels. Key experimental results show state-of-the-art accuracy exceeding 97% with less than 1 GB of GPU memory per batch, outperforming standard contrastive methods and matching supervised baselines on datasets like 3D-POP pigeons and 8-calves feeding videos, effectively eliminating the manual annotation bottleneck.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决长视频中个体动物识别效率低下的问题，这对于行为生态学和野生动物监测至关重要，通过克服手动标注和计算密集型自监督方法的局限性。该方法将识别重新定义为全局聚类任务，假设个体数量固定，仅使用边界框和总数；它采用帧对采样、冻结的预训练主干网络，以及结合匈牙利算法的自引导机制进行伪标签分配，从而无需身份标签即可学习特征。关键实验结果表明，在3D-POP鸽子和8头小牛进食视频等数据集上，准确率超过97%，GPU内存消耗低于1 GB，优于标准对比方法，并与监督基线相当，有效消除了手动标注的瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">LiteEmbed: Adapting CLIP to Rare Classes</div>
<div class="meta-line">Authors: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</div>
<div class="meta-line">First: 2026-01-14T17:53:11+00:00 · Latest: 2026-01-14T17:53:11+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiteEmbed：将CLIP适配至稀有类别</div>
<div class="mono" style="margin-top:8px">CLIP等大规模视觉语言模型虽在零样本识别上表现优异，却难以处理预训练中罕见类别（包括新兴实体与文化特定类别）。本文提出LiteEmbed——一种轻量级CLIP少样本个性化框架，无需重训练编码器即可扩展新类别。该框架通过基于PCA的解耦方法分离粗粒度语义方向与细粒度差异，在CLIP词嵌入空间内执行子空间引导的文本嵌入优化。通过粗粒度对齐与细粒度分离两个互补目标，在保持全局语义一致性的同时增强视觉相似类别间的区分度。优化后的嵌入即插即用，可在分类、检索、分割与检测任务中无缝替代CLIP原始文本特征。大量实验表明，该方法显著超越现有技术，为CLIP适配低代表性、稀有及未见类别提供了有效方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large vision-language models like CLIP perform poorly on rare or emerging classes not well-represented in their pretraining data. To address this, LiteEmbed introduces a lightweight few-shot personalization method that optimizes new text embeddings within CLIP&#x27;s vocabulary without retraining the model&#x27;s encoders. The method uses a PCA-based decomposition to guide optimization with two objectives: coarse alignment for semantic consistency and fine separation for discriminability among similar classes. Experiments show LiteEmbed achieves significant performance improvements over prior methods on adapting CLIP to rare classes across various vision tasks.</div>
<div class="mono" style="margin-top:8px">像CLIP这样的大规模视觉语言模型在预训练数据中较少出现的稀有或新兴类别上表现不佳。为此，LiteEmbed提出了一种轻量级的少样本个性化方法，在不重新训练模型编码器的情况下，优化CLIP词汇表中的新文本嵌入。该方法基于PCA分解将粗粒度语义方向与细粒度变化分离，通过粗粒度对齐和细粒度分离两个目标，在保持全局语义一致性的同时增强相似类别间的区分度。实验表明，LiteEmbed在多种视觉任务上，对于将CLIP适配到稀有、代表性不足或未见类别方面，相比先前方法取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Image2Garment: Simulation-ready Garment Generation from a Single Image</div>
<div class="meta-line">Authors: Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu, Yang Zheng, Hugo Bertiche, Menglei Chai, Thabo Beeler, Gordon Wetzstein</div>
<div class="meta-line">First: 2026-01-14T17:47:33+00:00 · Latest: 2026-01-14T17:47:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09658v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09658v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Image2Garment：从单张图像生成仿真就绪的服装</div>
<div class="mono" style="margin-top:8px">从单张图像估计物理精确、仿真就绪的服装具有挑战性，原因在于缺乏图像到物理的数据集以及该问题本身的不适定性。现有方法要么需要多视角捕捉和昂贵的可微分仿真，要么仅预测服装几何而缺乏真实仿真所需的材质属性。我们提出一种前馈框架，通过先微调视觉语言模型从真实图像推断材质成分与织物属性，再利用小型材料物理测量数据集训练轻量级预测器，将这些属性映射至对应的物理织物参数，从而规避上述限制。该方法引入了两个新数据集（FTAG与T2P），无需迭代优化即可从单张图像生成仿真就绪的服装。实验表明，我们的估计器在材质成分估计与织物属性预测上具有更优精度，且通过物理参数估计器处理后，相比现有图像到服装方法能实现更高保真度的仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating physically accurate, simulation-ready garments from a single image, a problem made difficult by the lack of image-to-physics datasets and its ill-posed nature. The method involves a feed-forward framework that first fine-tunes a vision-language model to infer material composition and fabric attributes from images, and then trains a lightweight predictor to map these attributes to physical fabric parameters using a small material-physics dataset. Experimental results demonstrate that the approach achieves superior accuracy in material composition and fabric attribute estimation, and by using these predictions, it enables higher-fidelity garment simulations compared to existing state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决从单张图像生成物理精确、可直接用于仿真的服装的挑战，这一难题因缺乏图像到物理的数据集及其不适定性而变得困难。所提出的方法采用前馈框架，首先微调一个视觉语言模型从图像中推断材料成分和织物属性，然后训练一个轻量级预测器，利用一个小型材料物理测量数据集将这些属性映射到物理织物参数。实验结果表明，该方法在材料成分和织物属性估计上实现了更高的准确性，并且通过这些预测，与现有最先进的图像到服装方法相比，能够实现更高保真度的仿真效果。</div>
</details>
</div>
<div class="card">
<div class="title">Head Pursuit: Probing Attention Specialization in Multimodal Transformers</div>
<div class="meta-line">Authors: Lorenzo Basile, Valentino Maiorca, Diego Doimo, Francesco Locatello, Alberto Cazzaniga</div>
<div class="meta-line">Venue: NeurIPS 2025 spotlight</div>
<div class="meta-line">First: 2025-10-24T14:41:47+00:00 · Latest: 2026-01-14T15:47:59+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025 (spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21518v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21518v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language and vision-language models have shown impressive performance across a wide range of tasks, but their internal mechanisms remain only partly understood. In this work, we study how individual attention heads in text-generative models specialize in specific semantic or visual attributes. Building on an established interpretability method, we reinterpret the practice of probing intermediate activations with the final decoding layer through the lens of signal processing. This lets us analyze multiple samples in a principled way and rank attention heads based on their relevance to target concepts. Our results show consistent patterns of specialization at the head level across both unimodal and multimodal transformers. Remarkably, we find that editing as few as 1% of the heads, selected using our method, can reliably suppress or enhance targeted concepts in the model output. We validate our approach on language tasks such as question answering and toxicity mitigation, as well as vision-language tasks including image classification and captioning. Our findings highlight an interpretable and controllable structure within attention layers, offering simple tools for understanding and editing large-scale generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意力头追踪：探究多模态Transformer中的注意力专业化机制</div>
<div class="mono" style="margin-top:8px">语言模型与视觉语言模型在广泛任务中展现出卓越性能，但其内部机制尚未完全明晰。本研究聚焦于文本生成模型中单个注意力头对特定语义或视觉属性的专业化机制。基于既有可解释性方法，我们通过信号处理视角重新阐释了利用最终解码层探测中间激活的实践，从而建立多样本分析框架，并依据注意力头与目标概念的相关性进行排序。研究结果表明，单模态与多模态Transformer在注意力头层面均存在一致的专业化模式。值得注意的是，采用本方法筛选仅1%的注意力头进行编辑，即可可靠地抑制或增强模型输出中的目标概念。我们在问答、毒性缓解等语言任务，以及图像分类、描述生成等视觉语言任务上验证了该方法的有效性。本研究揭示了注意力层内可解释且可控的结构，为理解与编辑大规模生成模型提供了简洁工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how individual attention heads in text-generative and multimodal transformers specialize in processing specific semantic or visual concepts, aiming to better understand their internal mechanisms. The method reinterprets the probing of intermediate activations through a signal processing lens, enabling principled analysis across multiple samples to rank heads by their relevance to target attributes. Experimental results reveal consistent patterns of head-level specialization and demonstrate that editing just 1% of heads, selected via this method, can reliably suppress or enhance targeted concepts in outputs, validated across language tasks like question answering and toxicity mitigation, and vision-language tasks including image classification and captioning.</div>
<div class="mono" style="margin-top:8px">为了更好地理解文本生成和视觉语言模型的内部机制，本研究探究了单个注意力头如何专门处理特定语义或视觉概念。该方法基于现有可解释性技术，通过信号处理视角重新解释对中间激活的探测，从而支持多样本的原则性分析，并根据注意力头与目标属性的相关性进行排序。实验揭示了在单模态和多模态Transformer中注意力头层面存在一致的专门化模式，并证明通过该方法选择并编辑少至1%的注意力头，即可可靠地抑制或增强输出中的目标概念，该发现在问答、毒性缓解、图像分类和字幕生成等任务上得到了验证。</div>
</details>
</div>
<div class="card">
<div class="title">OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding</div>
<div class="meta-line">Authors: Sheng-Yu Huang, Jaesung Choe, Yu-Chiang Frank Wang, Cheng Sun</div>
<div class="meta-line">First: 2026-01-14T15:45:57+00:00 · Latest: 2026-01-14T15:45:57+00:00</div>
<div class="meta-line">Comments: project page: https://peterjohnsonhuang.github.io/openvoxel-pages/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09575v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09575v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://peterjohnsonhuang.github.io/openvoxel-pages/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenVoxel：面向开放词汇3D场景理解的免训练体素分组与描述方法</div>
<div class="mono" style="margin-top:8px">本文提出OpenVoxel——一种免训练算法，用于为开放词汇3D场景理解任务实现稀疏体素的分组与描述。基于从多视角图像获取的稀疏体素栅格化模型，OpenVoxel能够生成描述场景中不同物体的语义化分组。通过结合视觉语言模型与多模态大语言模型，该方法通过对各组进行文字描述构建信息丰富的场景地图，进而支持开放词汇分割、指代表达分割等进阶3D场景理解任务。与现有方法不同，本方法无需训练且不依赖CLIP/BERT文本编码器的嵌入向量，而是直接采用多模态大语言模型进行文本到文本检索。大量实验表明，该方法在复杂指代表达分割任务中性能显著优于近期研究。代码将开源发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for open-vocabulary 3D scene understanding without requiring task-specific training. The proposed OpenVoxel method is a training-free algorithm that first groups sparse voxels from a 3D scene rasterization into meaningful object segments, then directly employs Vision Language Models and Multi-modal Large Language Models to generate descriptive captions for each group via text-to-text search, avoiding reliance on CLIP/BERT text embeddings. Experimental results show that this approach achieves superior performance compared to recent methods, particularly excelling in complex referring expression segmentation tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在无需特定任务训练即可实现开放词汇的3D场景理解。提出的OpenVoxel方法是一种免训练算法，首先将3D场景栅格化得到的稀疏体素分组为有意义的物体片段，然后直接利用视觉语言模型和多模态大语言模型，通过文本到文本搜索为每个组生成描述性标题，避免了使用CLIP/BERT文本嵌入。实验结果表明，该方法相比近期研究取得了更优的性能，尤其在复杂的指代表达分割任务中表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">PrivLEX: Detecting legal concepts in images through Vision-Language Models</div>
<div class="meta-line">Authors: Darya Baranouskaya, Andrea Cavallaro</div>
<div class="meta-line">First: 2026-01-14T12:51:48+00:00 · Latest: 2026-01-14T12:51:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09449v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX&#x27;s ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PrivLEX：通过视觉语言模型检测图像中的法律概念</div>
<div class="mono" style="margin-top:8px">本文提出PrivLEX——一种基于法律定义的个人数据概念进行决策的新型图像隐私分类器。作为首个与法律概念对齐的可解释隐私分类器，PrivLEX利用视觉语言模型的识别能力，通过零样本VLM概念检测实现无需训练阶段显式概念标注的、基于无标签概念瓶颈模型的可解释分类。我们验证了PrivLEX识别图像中个人数据概念的能力，并进一步分析了图像隐私数据集人工标注者对这些概念的敏感度认知。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to develop an interpretable image privacy classifier grounded in legally defined personal data concepts, addressing the need for transparency in automated privacy decisions. The method, named PrivLEX, employs a zero-shot Vision-Language Model for concept detection and integrates this into a label-free Concept Bottleneck Model, eliminating the requirement for explicit concept labels during training. Experimental results demonstrate that PrivLEX can effectively identify personal data concepts within images and provides an analysis of how sensitive these concepts are perceived by human annotators in existing privacy datasets.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发一种基于法律定义的个人数据概念的可解释图像隐私分类器，以解决自动化隐私决策中透明度的需求。该方法名为PrivLEX，利用零样本视觉语言模型进行概念检测，并将其集成到一个无需标签的概念瓶颈模型中，从而在训练过程中无需显式的概念标注。实验结果表明，PrivLEX能够有效识别图像中的个人数据概念，并分析了现有隐私数据集中人类标注者对这些概念敏感度的感知。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models</div>
<div class="meta-line">Authors: Junjie Li, Ziao Wang, Jianghong Ma, Xiaofeng Zhang</div>
<div class="meta-line">First: 2025-09-27T02:57:37+00:00 · Latest: 2026-01-14T12:33:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.00040v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.00040v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) achieve strong benchmark performance, but controlling their behavior through instruction tuning remains difficult. Reducing the budget of instruction tuning dataset often causes regressions, as heuristic strategies treat models as black boxes and overlook the latent capabilities that govern learning. We introduce Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. CADC discovers intrinsic capabilities in an unsupervised manner from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. This transforms black-box instruction tuning into a controllable, capability-driven process. With as little as 5% of the original data, CADC surpasses full-data training on multimodal benchmarks. These results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a principle paradigm for instruction data curation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭示内在能力：视觉语言模型数据策展的新范式</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）在基准测试中表现优异，但通过指令微调控制其行为仍具挑战。减少指令微调数据集的预算常导致性能衰退，因为启发式策略将模型视为黑箱，忽视了主导学习的潜在能力。我们提出能力归因数据策展（CADC）框架，将策展从任务特定启发式转向内在能力分析。CADC通过基于梯度的学习轨迹以无监督方式发现内在能力，借助影响估计将训练数据归因于这些能力，并通过平衡选择与分阶段排序构建能力感知的课程。这使黑箱指令微调转变为可控的、能力驱动的过程。仅使用原始数据5%的情况下，CADC在多模态基准测试中超越了全数据训练。这些结果验证了内在能力作为模型学习的基本构建模块，并确立了CADC作为指令数据策展的原则性范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the difficulty of controlling vision-language models through instruction tuning and the performance regressions caused by reduced data budgets, this paper introduces Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. The method discovers intrinsic capabilities unsupervised from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. Experimental results show that with only 5% of the original data, CADC surpasses full-data training on multimodal benchmarks, validating intrinsic capabilities as fundamental learning building blocks and establishing CADC as a principle paradigm for instruction data curation.</div>
<div class="mono" style="margin-top:8px">针对指令微调难以控制视觉语言模型行为以及减少训练数据量常导致性能退化的问题，本文提出了能力归因数据筛选（CADC）框架，将数据筛选从任务特定启发式方法转向内在能力分析。该方法通过基于梯度的学习轨迹无监督地发现模型内在能力，利用影响估计将训练数据归因于这些能力，并通过平衡选择和分阶段排序来构建能力感知的课程。实验结果表明，仅使用原始训练数据的5%，CADC在多模态基准测试上的表现就超越了全数据训练，从而验证了内在能力是模型学习的基本构建模块，并确立了CADC作为指令数据筛选的原则性范式。</div>
</details>
</div>
<div class="card">
<div class="title">ProfVLM: A Lightweight Video-Language Model for Multi-View Proficiency Estimation</div>
<div class="meta-line">Authors: Edoardo Bianchi, Jacopo Staiano, Antonio Liotta</div>
<div class="meta-line">First: 2025-09-30T14:00:41+00:00 · Latest: 2026-01-14T12:30:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26278v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26278v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing approaches treat action quality assessment and skill proficiency estimation as classification problems, outputting discrete labels without interpretable reasoning. We reformulate this task as generative vision language modeling, introducing ProfVLM, a compact model that jointly predicts proficiency levels and generates expert-like natural language feedback from multi-view videos. ProfVLM leverages conditional language generation to provide actionable insights along with quantitative evaluation scores. Central to our method is an AttentiveGatedProjector that dynamically fuses and projects multi-view egocentric and exocentric features from a frozen TimeSformer backbone into a language model fine-tuned for feedback generation. Trained on EgoExo4D with expert commentaries, ProfVLM surpasses state-of-the-art methods while using up to 20x fewer parameters and reducing training time by up to 60% compared to existing classification-based methods. By providing natural language critiques aligned with performance levels, this work shows that generative vision-language modeling offers a powerful and efficient paradigm shift for interpretable action quality assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProfVLM：用于多视角技能熟练度评估的轻量级视频语言模型</div>
<div class="mono" style="margin-top:8px">现有方法将动作质量评估与技能熟练度估计视为分类问题，仅输出离散标签而缺乏可解释的推理过程。本研究将该任务重构为生成式视觉语言建模，提出ProfVLM——一个能够从多视角视频中联合预测熟练度等级并生成类专家自然语言反馈的紧凑模型。ProfVLM利用条件语言生成技术，在提供量化评估分数的同时输出可操作的改进建议。方法的核心是注意力门控投影器，该模块动态融合并投影来自冻结TimeSformer骨干网络的多视角第一人称与第三人称特征，将其输入专为反馈生成微调的语言模型。基于EgoExo4D数据集及专家注释进行训练后，ProfVLM在性能上超越现有最优方法，且参数量减少达20倍，训练时间较现有分类方法降低60%。通过生成与表现水平对齐的自然语言评析，本研究表明生成式视觉语言建模为可解释的动作质量评估提供了强大高效的范式转变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To move beyond discrete classification labels and enable interpretable reasoning for action quality assessment and skill proficiency estimation, this work introduces ProfVLM, a lightweight generative vision-language model. The method reformulates the task by using an AttentiveGatedProjector to dynamically fuse multi-view egocentric and exocentric video features from a frozen TimeSformer backbone and project them into a language model fine-tuned for conditional text generation, which jointly predicts proficiency levels and produces expert-like natural language feedback. Experiments on the EgoExo4D dataset show that ProfVLM outperforms state-of-the-art classification methods while using up to 20 times fewer parameters and reducing training time by up to 60%, demonstrating that generative modeling provides an efficient and interpretable paradigm for the task.</div>
<div class="mono" style="margin-top:8px">为了超越离散的分类标签，实现动作质量评估与技能熟练度估计的可解释推理，本研究将该任务重新定义为生成式视觉-语言建模，提出了ProfVLM模型。该方法采用一个注意力门控投影器，动态融合来自冻结TimeSformer骨干网络的多视角第一人称和第三人称特征，并将其投影到经过微调、用于生成熟练度分数和专家式反馈的语言模型中。在EgoExo4D数据集上的实验表明，ProfVLM在性能上超越了现有最佳方法，同时参数量减少了多达20倍，训练时间缩短了高达60%，证明了生成式建模为该任务提供了一种高效且可解释的新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Frequency Is What You Need: Considering Word Frequency When Text Masking Benefits Vision-Language Model Pre-training</div>
<div class="meta-line">Authors: Mingliang Liang, Martha Larson</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2024-12-20T18:51:41+00:00 · Latest: 2026-01-14T11:07:46+00:00</div>
<div class="meta-line">Comments: Accepted by WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.16148v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.16148v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) can be trained more efficiently if training sets can be reduced in size. Recent work has shown the benefits of masking text during VLM training using a variety of strategies (truncation, random masking, block masking and syntax masking) and has reported syntax masking as the top performer. In this paper, we analyze the impact of different text masking strategies on the word frequency in the training data, and show that this impact is connected to model success. This finding motivates Contrastive Language-Image Pre-training with Word Frequency Masking (CLIPF), our proposed masking approach, which directly leverages word frequency. Extensive experiments demonstrate the advantages of CLIPF over syntax masking and other existing approaches, particularly when the number of input tokens decreases. We show that not only CLIPF, but also other existing masking strategies, outperform syntax masking when enough epochs are used during training, a finding of practical importance for selecting a text masking method for VLM training. Our code is available online.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>频率即所需：文本掩码时考虑词频对视觉语言模型预训练的增益</div>
<div class="mono" style="margin-top:8px">若训练集规模可缩减，视觉语言模型（VLM）的训练效率将得以提升。近期研究表明，在VLM训练中采用多种文本掩码策略（截断、随机掩码、块掩码及句法掩码）具有显著效益，其中句法掩码表现最佳。本文通过分析不同文本掩码策略对训练数据词频分布的影响，揭示该影响与模型性能存在关联。基于此发现，我们提出基于词频掩码的对比语言-图像预训练方法（CLIPF），该方法直接利用词频信息进行掩码。大量实验证明，CLIPF在输入标记数减少时，性能优于句法掩码及其他现有方法。研究还发现，当训练周期足够时，不仅CLIPF，其他现有掩码策略也能超越句法掩码，这一结论对VLM训练中文本掩码方法的选择具有重要实践意义。代码已开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how text masking strategies affect word frequency distributions during vision-language model pre-training, finding that this impact correlates with model performance. The authors propose Contrastive Language-Image Pre-training with Word Frequency Masking (CLIPF), a method that directly utilizes word frequency information for masking. Experiments show CLIPF outperforms existing strategies like syntax masking, especially with fewer input tokens, and reveal that with sufficient training epochs, other existing masking methods can also surpass syntax masking.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在视觉语言模型预训练中，文本掩码策略如何影响词频分布，并发现这种影响与模型性能相关。作者提出了基于词频掩码的对比语言-图像预训练方法（CLIPF），该方法直接利用词频信息进行掩码。实验结果表明，CLIPF在输入令牌较少时优于语法掩码及其他现有方法，并且当训练周期足够时，多种掩码策略的表现都能超过语法掩码。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
