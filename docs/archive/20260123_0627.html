<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 06:27</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0627</div>
    <div class="row"><div class="card">
<div class="title">Iterative Refinement Improves Compositional Image Generation</div>
<div class="meta-line">Authors: Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak</div>
<div class="meta-line">First: 2026-01-21T18:59:40+00:00 · Latest: 2026-01-21T18:59:40+00:00</div>
<div class="meta-line">Comments: Project webpage: https://iterative-img-gen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15286v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iterative-img-gen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代优化提升组合式图像生成质量</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型已取得显著进展，但在处理需要同时满足多对象、关系和属性的复杂提示时仍面临挑战。现有的推理时策略（如带验证器的并行采样或单纯增加去噪步数）虽能提升提示对齐度，但在需要满足多重约束的复杂组合场景中仍显不足。受大语言模型中思维链推理成功的启发，我们提出一种迭代式测试时策略：T2I模型在视觉语言模型作为循环评判者的反馈指导下，通过多步骤渐进优化生成结果。该方法无需外部工具或先验知识，可灵活适配各类图像生成器与视觉语言模型。实验表明，该方法在多个基准测试中持续提升图像生成性能：在ConceptMix（k=7）的全正确率提升16.9%，在T2I-CompBench（3D空间类别）提升13.8%，在Visual Jenga场景解构任务中提升12.5%（均以计算量匹配的并行采样为基线）。除量化提升外，迭代优化通过将复杂提示分解为序列修正，生成结果更具忠实度——人工评估者对本方法的偏好率达58.7%（基线为41.3%）。这些发现共同表明，迭代自校正可作为组合式图像生成的普适性优化原则。完整结果与可视化案例详见项目网页。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image models often fail to accurately generate images from complex prompts involving multiple objects and relations. To address this, the authors propose an iterative refinement strategy where a vision-language model provides feedback to guide the image generator across multiple denoising steps, decomposing the complex prompt into sequential corrections. Experimental results show significant improvements, including a 16.9% increase in all-correct rate on the ConceptMix benchmark and a 58.7% preference rate from human evaluators compared to a parallel sampling baseline.</div>
<div class="mono" style="margin-top:8px">文本到图像模型在处理涉及多个对象和关系的复杂提示时常常表现不佳。为解决此问题，研究者提出了一种迭代优化策略，利用视觉语言模型提供反馈，在多步去噪过程中引导图像生成器，将复杂提示分解为顺序修正。实验结果表明该方法带来了显著提升，如在ConceptMix基准测试上的全正确率提高了16.9%，并在人类评估中以58.7%的偏好率优于并行采样基线，证明了迭代自校正对于组合式图像生成的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Generation Model for the Embodied World</div>
<div class="meta-line">Authors: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-21T18:59:18+00:00 · Latest: 2026-01-21T18:59:18+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15282v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/ReVidgen/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/ReVidgen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向具身世界的视频生成模型再思考</div>
<div class="mono" style="margin-top:8px">视频生成模型显著推动了具身智能的发展，为生成捕捉物理世界中感知、推理与行动的多样化机器人数据开辟了新可能。然而，合成能准确反映真实机器人交互的高质量视频仍具挑战，且缺乏标准化基准限制了公平比较与进展。为填补这一空白，我们提出了综合性机器人基准RBench，旨在通过五个任务领域和四种不同具身形态评估面向机器人的视频生成。该基准通过可复现的子指标（包括结构一致性、物理合理性与动作完整性）评估任务级正确性与视觉保真度。对25个代表性模型的评估揭示了它们在生成物理真实机器人行为方面的显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达0.96，验证了其有效性。尽管RBench为识别这些缺陷提供了必要视角，但实现物理真实性需超越评估层面，解决高质量训练数据严重短缺的问题。基于这些洞察，我们提出了精炼的四阶段数据流程，由此构建了RoVid-X——目前最大的开源机器人视频生成数据集，包含400万个标注视频片段，覆盖数千项任务，并辅以全面的物理属性标注。整体上，这一评估与数据协同的生态系统为视频模型的严谨评估与可扩展训练奠定了坚实基础，加速了具身AI向通用智能的演进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating high-quality videos that accurately reflect real-world robotic interactions, which is crucial for advancing embodied intelligence but hindered by the lack of a standardized benchmark. The method introduces RBench, a comprehensive robotics benchmark evaluating robot-oriented video generation across five task domains and four embodiments, using metrics for structural consistency, physical plausibility, and action completeness, alongside RoVid-X, a refined data pipeline producing a large annotated dataset. Experimental results reveal significant deficiencies in existing models for generating physically realistic robot behaviors, with RBench achieving a 0.96 Spearman correlation with human evaluations, validating its effectiveness, and RoVid-X provides 4 million annotated clips to support scalable training.</div>
<div class="mono" style="margin-top:8px">该研究针对合成准确反映真实世界机器人交互的高质量视频的挑战，这对推进具身智能至关重要，同时缺乏标准化基准进行公平比较。方法上，引入了一个全面的机器人基准RBench，通过结构一致性、物理合理性和动作完整性等指标，评估跨多个任务领域和具身的机器人导向视频生成，并开发了一个精炼的数据管道来创建大型标注数据集RoVid-X。实验结果表明，对25个模型的评估揭示了它们在生成物理真实机器人行为方面的显著缺陷，而RBench与人类评估的斯皮尔曼相关系数达到0.96，同时发布的RoVid-X提供了400万个标注视频片段以支持可扩展的训练。</div>
</details>
</div>
<div class="card">
<div class="title">FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion</div>
<div class="meta-line">Authors: Zichen Xi, Hao-Xiang Chen, Nan Xue, Hongyu Yan, Qi-Yuan Feng, Levent Burak Kara, Joaquim Jorge, Qun-Ce Xu</div>
<div class="meta-line">First: 2026-01-21T18:32:27+00:00 · Latest: 2026-01-21T18:32:27+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSSC：基于一步潜在扩散的通用生成式单目语义场景补全</div>
<div class="mono" style="margin-top:8px">从单目RGB图像进行语义场景补全（SSC）是一项基础而具有挑战性的任务，其难点在于从单一视角推断被遮挡三维几何结构存在固有模糊性。虽然前馈方法已取得进展，但往往难以生成被遮挡区域的合理细节并保持物体的基本空间关系。这种针对整个三维空间的精确生成推理能力在实际应用中至关重要。本文提出FlowSSC——首个直接应用于单目语义场景补全的生成式框架。FlowSSC将SSC任务视为条件生成问题，可与现有前馈SSC方法无缝集成以显著提升其性能。为实现实时推理且不损失质量，我们引入在紧凑三平面潜在空间中运行的捷径流匹配技术。与需要数百步迭代的标准扩散模型不同，本方法通过捷径机制实现单步高保真生成，使其能在自主系统中实际部署。在SemanticKITTI数据集上的大量实验表明，FlowSSC取得了最先进的性能，显著优于现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of monocular semantic scene completion, where inferring occluded 3D geometry from a single RGB image is inherently ambiguous, leading existing feed-forward methods to often produce implausible details and fail to preserve spatial relationships. To overcome this, the authors propose FlowSSC, a generative framework that treats SSC as a conditional generation problem and integrates with existing feed-forward methods; it employs a novel Shortcut Flow-matching technique operating in a compact triplane latent space to enable high-fidelity, one-step generation for real-time inference. Experimental results on SemanticKITTI show that FlowSSC achieves state-of-the-art performance, significantly boosting the capabilities of baseline methods.</div>
<div class="mono" style="margin-top:8px">该研究针对单目语义场景补全的挑战，即从单张RGB图像推断被遮挡的3D几何结构存在固有模糊性，导致难以生成合理的细节并保持空间关系。所提出的方法FlowSSC是一个生成式框架，将SSC视为条件生成问题，可与现有前馈方法集成；它引入了在三角平面潜在空间中操作的捷径流匹配，以实现高保真度的单步生成，从而支持实时推理。在SemanticKITTI上的实验结果表明，FlowSSC实现了最先进的性能，显著超越了现有基线。</div>
</details>
</div>
<div class="card">
<div class="title">MonoRace: Winning Champion-Level Drone Racing with Robust Monocular AI</div>
<div class="meta-line">Authors: Stavrow A. Bahnam, Robin Ferede, Till M. Blaha, Anton E. Lang, Erin Lucassen, Quentin Missinne, Aderik E. C. Verraest, Christophe De Wagter, Guido C. H. E. de Croon</div>
<div class="meta-line">First: 2026-01-21T17:53:29+00:00 · Latest: 2026-01-21T17:53:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15222v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15222v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous drone racing represents a major frontier in robotics research. It requires an Artificial Intelligence (AI) that can run on board light-weight flying robots under tight resource and time constraints, while pushing the physical system to its limits. The state of the art in this area consists of a system with a stereo camera and an inertial measurement unit (IMU) that beat human drone racing champions in a controlled indoor environment. Here, we present MonoRace: an onboard drone racing approach that uses a monocular, rolling-shutter camera and IMU that generalizes to a competition environment without any external motion tracking system. The approach features robust state estimation that combines neural-network-based gate segmentation with a drone model. Moreover, it includes an offline optimization procedure that leverages the known geometry of gates to refine any state estimation parameter. This offline optimization is based purely on onboard flight data and is important for fine-tuning the vital external camera calibration parameters. Furthermore, the guidance and control are performed by a neural network that foregoes inner loop controllers by directly sending motor commands. This small network runs on the flight controller at 500Hz. The proposed approach won the 2025 Abu Dhabi Autonomous Drone Racing Competition (A2RL), outperforming all competing AI teams and three human world champion pilots in a direct knockout tournament. It set a new milestone in autonomous drone racing research, reaching speeds up to 100 km/h on the competition track and successfully coping with problems such as camera interference and IMU saturation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MonoRace：基于鲁棒单目AI的冠军级无人机竞速系统</div>
<div class="mono" style="margin-top:8px">自主无人机竞速是机器人研究的重要前沿领域，其要求人工智能系统能在资源与时间严格受限的轻量化飞行机器人上实时运行，并将物理系统推向极限。当前该领域最先进系统采用立体相机与惯性测量单元组合，已在受控室内环境中击败人类冠军飞手。本文提出MonoRace：一种仅使用单目卷帘快门相机与惯性测量单元的机载竞速方案，可在无外部运动追踪系统的竞赛环境中实现泛化。该方案具备鲁棒状态估计能力，将基于神经网络的障碍门分割与无人机动力学模型相结合，并包含利用已知障碍门几何结构优化状态估计参数的离线调优流程——该流程完全基于机载飞行数据，对精细校准关键的外部相机参数至关重要。其导控系统采用直接输出电机指令的轻量神经网络，绕过了传统内环控制器，以500Hz频率在飞控芯片上实时运行。本方案在2025年阿布扎比自主无人机竞速锦标赛中，于淘汰赛中战胜所有AI参赛队及三位人类世界冠军飞手，创下赛道时速100公里的新纪录，成功应对相机干扰与惯性测量单元饱和等挑战，标志着自主无人机竞速研究的新里程碑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to advance autonomous drone racing by developing an AI system that operates under strict onboard computational constraints without relying on external tracking. The proposed MonoRace system uses only a monocular rolling-shutter camera and IMU, combining neural-network-based gate segmentation with a drone model for robust state estimation and employing an offline optimization procedure to refine parameters like camera calibration using flight data. A key innovation is a small neural network that runs at 500Hz on the flight controller to directly output motor commands, bypassing traditional inner-loop controllers. In the 2025 Abu Dhabi Autonomous Drone Racing Competition, this system outperformed all other AI teams and three human world champion pilots, achieving speeds up to 100 km/h and demonstrating robustness against challenges like camera interference and IMU saturation.</div>
<div class="mono" style="margin-top:8px">自主无人机竞速需要能在严格资源限制下实现高速飞行的机载高效人工智能。本研究提出了MonoRace系统，它仅使用单目卷帘快门相机和惯性测量单元进行状态估计，将基于神经网络的门框分割与无人机模型相结合，并利用机载飞行数据和已知门框几何进行离线优化以细化校准参数。控制部分由一个紧凑的神经网络以500Hz频率直接输出电机指令。该系统在2025年阿布扎比自主无人机竞速赛中获胜，超越了所有参赛AI团队和人类世界冠军飞行员，最高速度达100公里/小时，并能稳健应对相机干扰和惯性测量单元饱和等问题。</div>
</details>
</div>
<div class="card">
<div class="title">OSMa-Bench: Evaluating Open Semantic Mapping Under Varying Lighting Conditions</div>
<div class="meta-line">Authors: Maxim Popov, Regina Kurkova, Mikhail Iumanov, Jaafar Mahmoud, Sergey Kolyubin</div>
<div class="meta-line">Venue: IROS</div>
<div class="meta-line">First: 2025-03-13T13:07:51+00:00 · Latest: 2026-01-21T17:25:25+00:00</div>
<div class="meta-line">Comments: Project page: https://be2rlab.github.io/OSMa-Bench/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10331v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.10331v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://be2rlab.github.io/OSMa-Bench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open Semantic Mapping (OSM) is a key technology in robotic perception, combining semantic segmentation and SLAM techniques. This paper introduces a dynamically configurable and highly automated LLM/LVLM-powered pipeline for evaluating OSM solutions called OSMa-Bench (Open Semantic Mapping Benchmark). The study focuses on evaluating state-of-the-art semantic mapping algorithms under varying indoor lighting conditions, a critical challenge in indoor environments. We introduce a novel dataset with simulated RGB-D sequences and ground truth 3D reconstructions, facilitating the rigorous analysis of mapping performance across different lighting conditions. Through experiments on leading models such as ConceptGraphs, BBQ, and OpenScene, we evaluate the semantic fidelity of object recognition and segmentation. Additionally, we introduce a Scene Graph evaluation method to analyze the ability of models to interpret semantic structure. The results provide insights into the robustness of these models, forming future research directions for developing resilient and adaptable robotic systems. Project page is available at https://be2rlab.github.io/OSMa-Bench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OSMa-Bench：评估不同光照条件下的开放语义建图</div>
<div class="mono" style="margin-top:8px">开放语义建图（OSM）是机器人感知中的关键技术，融合了语义分割与SLAM技术。本文提出了一种动态可配置、高度自动化的LLM/LVLM驱动评估流程——OSMa-Bench（开放语义建图基准），用于评估OSM解决方案。研究聚焦于评估室内多变光照条件下（室内环境的关键挑战）的先进语义建图算法。我们提出了包含模拟RGB-D序列与真实三维重建标注的全新数据集，支持对不同光照条件下建图性能的严谨分析。通过对ConceptGraphs、BBQ、OpenScene等主流模型的实验，评估了物体识别与分割的语义保真度。此外，我们引入了场景图评估方法，以分析模型解析语义结构的能力。实验结果揭示了这些模型的鲁棒性特征，为开发具有适应性与韧性的机器人系统指明了未来研究方向。项目页面详见：https://be2rlab.github.io/OSMa-Bench/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need to evaluate Open Semantic Mapping (OSM) systems under challenging indoor lighting variations, a critical factor for robust robotic perception. The authors introduce OSMa-Bench, a benchmark featuring a novel, dynamically configurable evaluation pipeline powered by LLMs/LVLMs and a new dataset with simulated RGB-D sequences and ground-truth 3D reconstructions to systematically test lighting conditions. Experiments on models like ConceptGraphs, BBQ, and OpenScene, assessed via semantic fidelity and a novel Scene Graph method, reveal insights into their robustness and structural interpretation capabilities, guiding future development of more resilient robotic systems.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估融合语义分割与SLAM的开放语义映射（OSM）系统在多变且具有挑战性的室内光照条件下的鲁棒性。作者提出了一个由大语言/视觉语言模型驱动的基准框架OSMa-Bench，并引入了一个包含模拟RGB-D序列和真实3D重建的新数据集，以系统测试不同光照场景下的建图性能。通过对ConceptGraphs、BBQ和OpenScene等模型的实验，评估了其在物体识别与分割上的语义保真度，同时采用一种新颖的场景图评估方法来分析语义结构理解能力，揭示了模型的鲁棒性差异，为开发更具适应性的机器人感知系统指明了未来研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-21T17:15:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayesianVLA：基于潜在动作查询的视觉语言动作模型贝叶斯分解</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但常难以泛化至新指令或复杂多任务场景。我们指出当前训练范式的关键缺陷：目标驱动的数据收集导致数据集偏差。此类数据集中，仅凭视觉观测即可高度预测语言指令，致使指令与动作间的条件互信息趋近于零，此现象称为“信息坍缩”。因此，模型退化为仅依赖视觉的策略，忽略语言约束并在分布外（OOD）场景中失效。为解决该问题，我们提出BayesianVLA——一种通过贝叶斯分解强制遵循指令的新框架。通过引入可学习的潜在动作查询，构建双分支架构以同时估计仅视觉先验$p(a \mid v)$和语言条件后验$π(a \mid v, \ell)$，进而优化策略以最大化动作与指令间的条件点互信息（PMI）。该目标有效惩罚视觉捷径，并奖励能显式解释语言指令的动作。无需新增数据，BayesianVLA显著提升泛化能力。在SimplerEnv和RoboCasa上的大量实验验证了显著性能提升，其中在挑战性OOD基准SimplerEnv上实现11.3%的性能改进，证明了本方法在动作中稳健关联语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions are predictable from visual context, leading to an Information Collapse that degrades models into vision-only policies. To counteract this, the proposed BayesianVLA framework introduces learnable Latent Action Queries within a dual-branch architecture to separately estimate a vision-only action prior and a language-conditioned posterior, then optimizes the policy to maximize the conditional pointwise mutual information between actions and instructions. Experiments on SimplerEnv and RoboCasa benchmarks show the method substantially improves out-of-distribution generalization without new data, achieving an 11.3% performance gain on a challenging OOD benchmark.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差导致泛化能力不足，其中语言指令可从视觉观察中预测，引发信息坍缩，使模型退化为仅依赖视觉的策略。为解决该问题，本文提出的BayesianVLA框架通过引入可学习的潜在动作查询，构建双分支架构以分别估计仅视觉先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息，从而惩罚视觉捷径。在SimplerEnv和RoboCasa基准上的大量实验表明，该方法显著提升了泛化性能，如在具有挑战性的分布外任务上获得了11.3%的改进，验证了其能有效将语言约束落实到动作中。</div>
</details>
</div>
<div class="card">
<div class="title">V-CAGE: Context-Aware Generation and Verification for Scalable Long-Horizon Embodied Tasks</div>
<div class="meta-line">Authors: Yaru Liu, Ao-bo Wang, Nanyang Ye</div>
<div class="meta-line">First: 2026-01-21T16:41:51+00:00 · Latest: 2026-01-21T16:41:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15164v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15164v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning long-horizon embodied behaviors from synthetic data remains challenging because generated scenes are often physically implausible, language-driven programs frequently &quot;succeed&quot; without satisfying task semantics, and high-level instructions require grounding into executable action sequences. To address these limitations, we introduce V-CAGE, a closed-loop framework for generating robust, semantically aligned manipulation datasets at scale. First, we propose a context-aware instantiation mechanism that enforces geometric consistency during scene synthesis. By dynamically maintaining a map of prohibited spatial areas as objects are placed, our system prevents interpenetration and ensures reachable, conflict-free configurations in cluttered environments. Second, to bridge the gap between abstract intent and low-level control, we employ a hierarchical instruction decomposition module. This decomposes high-level goals (e.g., &quot;get ready for work&quot;) into compositional action primitives, facilitating coherent long-horizon planning. Crucially, we enforce semantic correctness through a VLM-based verification loop. Acting as a visual critic, the VLM performs rigorous rejection sampling after each subtask, filtering out &quot;silent failures&quot; where code executes but fails to achieve the visual goal. Experiments demonstrate that V-CAGE yields datasets with superior physical and semantic fidelity, significantly boosting the success rate and generalization of downstream policies compared to non-verified baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-CAGE：面向可扩展长程具身任务的情境感知生成与验证框架</div>
<div class="mono" style="margin-top:8px">从合成数据中学习长程具身行为仍面临挑战：生成场景常违反物理规律，语言驱动程序常在不满足任务语义时即显示“成功”，高层指令需具体化为可执行动作序列。为应对这些局限，我们提出V-CAGE——一个闭环框架，用于大规模生成鲁棒且语义对齐的操作数据集。首先，我们提出情境感知实例化机制，在场景合成时强制保持几何一致性。通过动态维护物体放置时的空间禁置区域地图，系统能防止物体穿模，确保杂乱环境中可达且无冲突的配置。其次，为弥合抽象意图与底层控制间的鸿沟，我们采用分层指令分解模块，将高层目标（如“准备工作”）分解为组合式动作基元，促进连贯的长程规划。关键的是，我们通过基于视觉语言模型的验证循环强制执行语义正确性：该模型作为视觉评判器，在每个子任务后执行严格拒绝采样，过滤代码可执行但未达成视觉目标的“静默失败”。实验表明，相较于未验证基线，V-CAGE生成的数据集具有更优的物理与语义保真度，显著提升下游策略的成功率与泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses challenges in learning long-horizon embodied behaviors from synthetic data, where generated scenes are often physically implausible and language programs may succeed without correct semantics. The proposed V-CAGE framework introduces a context-aware instantiation mechanism to ensure geometric consistency during scene synthesis by maintaining a map of prohibited areas, and a hierarchical instruction decomposition module to translate high-level goals into action primitives. A key innovation is a VLM-based verification loop that performs rejection sampling after each subtask to filter out semantically incorrect outcomes. Experiments show that V-CAGE produces datasets with superior physical and semantic fidelity, significantly improving the success rate and generalization of downstream policies compared to non-verified baselines.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决从合成数据中为长时程具身任务生成物理合理场景和语义正确动作序列的挑战，现有方法常产生不合理场景或出现“静默失败”，即程序执行但未达成目标。提出的V-CAGE框架引入了上下文感知实例化机制，通过维护禁止区域地图来防止物体穿透，确保场景合成的几何一致性，并采用分层指令分解模块将高级目标分解为可执行的动作基元；此外，它利用基于VLM的验证循环在每个子任务后进行拒绝采样，过滤掉语义错误的结果。实验表明，V-CAGE生成的数据集具有更优的物理和语义保真度，与未验证的基线相比，显著提高了下游策略的成功率和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Locomotion Dynamics of an Underactuated Three-Link Robotic Vehicle</div>
<div class="meta-line">Authors: Leonid Raz, Yizhar Or</div>
<div class="meta-line">First: 2024-07-31T11:58:18+00:00 · Latest: 2026-01-21T16:01:57+00:00</div>
<div class="meta-line">Comments: Accepted to IEEE Transactions on Robotics, January 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.21540v2">Abs</a> · <a href="https://arxiv.org/pdf/2407.21540v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The wheeled three-link snake robot is a well-known example of an underactuated system modelled using nonholonomic constraints, preventing lateral slippage (skid) of the wheels. A kinematically controlled configuration assumes that both joint angles are directly prescribed as phase-shifted periodic input. In another configuration of the robot, only one joint is periodically actuated while the second joint is passively governed by a visco-elastic torsion spring. In our work, we constructed the two configurations of the wheeled robot and conducted motion experiments under different actuation inputs. Analysis of the motion tracking measurements reveals a significant amount of wheels&#x27; skid, in contrast to the assumptions used in standard nonholonomic models. Therefore, we propose modified dynamic models which include wheels&#x27; skid and viscous friction forces, as well as rolling resistance. After parameter fitting, these dynamic models reach good agreement with the motion measurements, including effects of input&#x27;s frequency on the mean speed and net displacement per period. This illustrates the importance of incorporating wheels&#x27; skid and friction into the system&#x27;s model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>欠驱动三连杆机器人车辆的运动动力学</div>
<div class="mono" style="margin-top:8px">轮式三连杆蛇形机器人是采用非完整约束建模的欠驱动系统典型实例，其约束条件禁止车轮侧向滑移（打滑）。在运动学受控构型中，假设两个关节角均直接由相移周期输入给定。在另一种机器人构型中，仅一个关节受周期性驱动，而第二个关节由粘弹性扭转弹簧被动控制。本研究构建了轮式机器人的两种构型，并在不同驱动输入下进行了运动实验。运动跟踪测量分析表明，与标准非完整模型假设相反，车轮存在显著打滑现象。因此，我们提出了改进的动力学模型，该模型纳入车轮打滑、粘性摩擦力及滚动阻力。经参数拟合后，这些动力学模型与运动测量数据高度吻合，包括输入频率对平均速度及周期净位移的影响。这证明了在系统模型中纳入车轮打滑与摩擦力的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the locomotion dynamics of an underactuated three-link wheeled snake robot, motivated by the discrepancy between standard nonholonomic models that assume no wheel skid and the skidding observed in real-world robotic motion. The method involves constructing two robot configurations—one with two actively controlled joints and another with one active joint and one passive visco-elastic joint—and conducting motion experiments under various actuation inputs. Key experimental findings reveal significant wheel skid, leading to the proposal of modified dynamic models that incorporate skid, viscous friction, and rolling resistance; after parameter fitting, these models show good agreement with measurements, accurately capturing the effects of input frequency on mean speed and net displacement.</div>
<div class="mono" style="margin-top:8px">本研究探讨欠驱动三连杆轮式机器人的运动动力学，其动机在于标准非完整模型假设无车轮打滑，而实际机器人车辆中观察到打滑现象，两者存在差异。研究方法包括构建两种机器人配置——一种具有两个主动控制关节，另一种具有一个主动关节和一个被动粘弹性关节——并在不同驱动输入下进行运动实验。关键实验结果表明存在显著的车轮打滑，与模型假设相反，从而提出了修正的动态模型，该模型纳入了打滑、粘性摩擦和滚动阻力；经过参数拟合后，这些模型与测量结果吻合良好，捕捉了输入频率对平均速度和位移等影响。</div>
</details>
</div>
<div class="card">
<div class="title">Semilinear single-track vehicle models with distributed tyre friction dynamics</div>
<div class="meta-line">Authors: Luigi Romano, Ole Morten Aamo, Jan Åslund, Erik Frisk</div>
<div class="meta-line">Venue: Nonlinear Dyn 114, 138 (2026)</div>
<div class="meta-line">First: 2026-01-11T10:55:51+00:00 · Latest: 2026-01-21T15:43:17+00:00</div>
<div class="meta-line">Comments: 37 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06854v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06854v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a novel family of single-track vehicle models that incorporate a distributed representation of transient tyre dynamics, whilst simultaneously accounting for nonlinear effects induced by friction. The core of the proposed framework is represented by the distributed Friction with Bristle Dynamics (FrBD) model, which unifies and extends classical formulations such as Dahl and LuGre by describing the rolling contact process as a spatially distributed system governed by semilinear partial differential equations (PDEs). This model is systematically integrated into a single-track vehicle framework, where the resulting semilinear ODE-PDE interconnection captures the interaction between lateral vehicle motion and tyre deformation. Two main variants are considered: one with rigid tyre carcass and another with flexible carcass, each admitting a compact state-space representation. Local and global well-posedness properties for the coupled system are established rigorously, highlighting the dissipative and physically consistent properties of the distributed FrBD model. A linearisation procedure is also presented, enabling spectral analysis and transfer function derivation, and potentially facilitating the synthesis of controllers and observers. Numerical simulations demonstrate the model&#x27;s capability to capture micro-shimmy oscillations and transient lateral responses to advanced steering manoeuvres. The proposed formulation advances the state-of-the-art in vehicle dynamics modelling by providing a physically grounded, mathematically rigorous, and computationally tractable approach to incorporating transient tyre behaviour in lateral vehicle dynamics, when accounting for the effect of limited friction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有分布式轮胎摩擦动力学的半线性单轨车辆模型</div>
<div class="mono" style="margin-top:8px">本文提出了一类新颖的单轨车辆模型，该模型采用分布式表示法描述瞬态轮胎动力学，同时考虑摩擦引起的非线性效应。该框架的核心是分布式摩擦鬃毛动力学模型，通过将滚动接触过程描述为由半线性偏微分方程控制的空间分布式系统，统一并扩展了Dahl和LuGre等经典模型。该模型被系统地集成到单轨车辆框架中，由此产生的半线性ODE-PDE互联结构捕捉了车辆横向运动与轮胎变形之间的相互作用。研究考虑了两种主要变体：一种采用刚性轮胎胎体，另一种采用柔性胎体，每种变体均具有紧凑的状态空间表示。严格建立了耦合系统的局部与全局适定性，突显了分布式FrBD模型的耗散性与物理一致性。同时提出了线性化方法，支持谱分析和传递函数推导，并为控制器与观测器的设计提供可能。数值仿真表明，该模型能够准确捕捉微幅摆振现象及对高级转向操作的瞬态横向响应。该建模方法通过提供物理基础扎实、数学严谨且计算可行的途径，在考虑有限摩擦效应的前提下将瞬态轮胎行为纳入车辆横向动力学，推动了车辆动力学建模领域的前沿发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance the modeling of lateral vehicle dynamics by incorporating a physically grounded representation of transient tyre behavior under limited friction conditions. The method introduces a distributed Friction with Bristle Dynamics (FrBD) model, formulated as semilinear partial differential equations (PDEs), which unifies classical friction models and is integrated into a single-track vehicle framework, resulting in a coupled ODE-PDE system for both rigid and flexible tyre carcass variants. Key findings include rigorous proofs of the system&#x27;s well-posedness and dissipative properties, a linearisation procedure enabling spectral analysis, and numerical simulations demonstrating the model&#x27;s ability to capture micro-shimmy oscillations and transient lateral responses to steering inputs.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过引入一种分布式的、瞬态的轮胎摩擦表征来提升车辆动力学模型的物理保真度，这对于在有限摩擦条件下精确捕捉微幅摆振和瞬态横向响应等现象至关重要。该方法提出了一个分布式鬃毛摩擦动力学模型，以半线性偏微分方程形式描述，并将其集成到单轨车辆框架中，形成了一个耦合的常微分-偏微分方程系统；研究开发了刚性和柔性轮胎胎体两种变体，并严格分析了其适定性。数值模拟的关键实验结果表明，该模型能够复现微幅摆振和高级转向操作中的瞬态横向响应，从而确立了一种物理基础扎实、数学严谨的轮胎动力学建模方法。</div>
</details>
</div>
<div class="card">
<div class="title">Influence of Operator Expertise on Robot Supervision and Intervention</div>
<div class="meta-line">Authors: Yanran Jiang, Pavan Sikka, Leimin Tian, Dana Kuliic, Cecile Paris</div>
<div class="meta-line">First: 2026-01-21T15:14:23+00:00 · Latest: 2026-01-21T15:14:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15069v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15069v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With increasing levels of robot autonomy, robots are increasingly being supervised by users with varying levels of robotics expertise. As the diversity of the user population increases, it is important to understand how users with different expertise levels approach the supervision task and how this impacts performance of the human-robot team. This exploratory study investigates how operators with varying expertise levels perceive information and make intervention decisions when supervising a remote robot. We conducted a user study (N=27) where participants supervised a robot autonomously exploring four unknown tunnel environments in a simulator, and provided waypoints to intervene when they believed the robot had encountered difficulties. By analyzing the interaction data and questionnaire responses, we identify differing patterns in intervention timing and decision-making strategies across novice, intermediate, and expert users.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>操作者专业水平对机器人监督与干预的影响</div>
<div class="mono" style="margin-top:8px">随着机器人自主性水平的提升，机器人正越来越多地由具备不同机器人技术专业背景的用户进行监督。随着用户群体的多样化，理解不同专业水平的用户如何执行监督任务及其对人机团队绩效的影响变得至关重要。本探索性研究调查了不同专业水平的操作者在监督远程机器人时如何感知信息并做出干预决策。我们开展了一项用户研究（N=27），参与者在模拟器中监督一个自主探索四个未知隧道环境的机器人，并在认为机器人遇到困难时通过设置路径点进行干预。通过分析交互数据与问卷反馈，我们识别了新手、中级和专家用户在干预时机与决策策略上的差异模式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the growing diversity of robot supervisors with varying expertise, this study investigates how operator expertise influences supervision and intervention decisions for a remote robot. The method involved a user study (N=27) where participants supervised a robot autonomously exploring simulated tunnel environments and intervened by providing waypoints when they perceived difficulties. Key experimental findings revealed distinct patterns in intervention timing and decision-making strategies among novice, intermediate, and expert users.</div>
<div class="mono" style="margin-top:8px">随着机器人监督者专业知识背景日益多样，本研究旨在探索操作员专业水平如何影响其对远程机器人的监督和干预决策。研究方法为一项用户研究（N=27），参与者在模拟器中监督一个自主探索未知隧道环境的机器人，并在认为机器人遇到困难时通过提供路径点进行干预。通过对交互数据和问卷响应的分析，主要实验结果表明，新手、中级和专家用户在干预时机和决策策略上存在明显不同的模式。</div>
</details>
</div>
<div class="card">
<div class="title">Systematic Evaluation of Hip Exoskeleton Assistance Parameters for Enhancing Gait Stability During Ground Slip Perturbations</div>
<div class="meta-line">Authors: Maria T. Tagliaferri, Inseung Kang</div>
<div class="meta-line">First: 2026-01-21T14:58:37+00:00 · Latest: 2026-01-21T14:58:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15056v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15056v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Falls are the leading cause of injury related hospitalization and mortality among older adults. Consequently, mitigating age-related declines in gait stability and reducing fall risk during walking is a critical goal for assistive devices. Lower-limb exoskeletons have the potential to support users in maintaining stability during walking. However, most exoskeleton controllers are optimized to reduce the energetic cost of walking rather than to improve stability. While some studies report stability benefits with assistance, the effects of specific parameters, such as assistance magnitude and duration, remain unexplored. To address this gap, we systematically modulated the magnitude and duration of torque provided by a bilateral hip exoskeleton during slip perturbations in eight healthy adults, quantifying stability using whole-body angular momentum (WBAM). WBAM responses were governed by a significant interaction between assistance magnitude and duration, with duration determining whether exoskeleton assistance was stabilizing or destabilizing relative to not wearing the exoskeleton device. Compared to an existing energy-optimized controller, experimentally identified stability-optimal parameters reduced WBAM range by 25.7% on average. Notably, substantial inter-subject variability was observed in the parameter combinations that minimized WBAM during perturbations. We found that optimizing exoskeleton assistance for energetic outcomes alone is insufficient for improving reactive stability during gait perturbations. Stability-focused exoskeleton control should prioritize temporal assistance parameters and include user-specific personalization. This study represents an important step toward personalized, stability-focused exoskeleton control, with direct implications for improving stability and reducing fall risk in older adults.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>髋关节外骨骼辅助参数对地面滑移扰动中步态稳定性增强的系统性评估</div>
<div class="mono" style="margin-top:8px">跌倒是老年人因伤住院和死亡的主要原因。因此，缓解与年龄相关的步态稳定性下降并降低行走中的跌倒风险，是辅助设备的关键目标。下肢外骨骼有潜力帮助用户在行走中保持稳定。然而，大多数外骨骼控制器旨在降低行走能耗而非提升稳定性。尽管部分研究报道了辅助带来的稳定性益处，但具体参数（如辅助幅度和持续时间）的影响尚未明确。为填补这一空白，我们在八名健康成年人中系统调节了双侧髋关节外骨骼在滑移扰动中提供的扭矩幅度和持续时间，并使用全身角动量量化稳定性。WBAM响应受辅助幅度与持续时间的显著交互作用影响，其中持续时间决定了外骨骼辅助相对于不穿戴设备是稳定还是失稳。与现有能耗优化控制器相比，实验确定的稳定性最优参数平均将WBAM范围降低了25.7%。值得注意的是，在扰动期间最小化WBAM的参数组合存在显著的个体间差异。研究发现，仅针对能耗优化外骨骼辅助不足以改善步态扰动中的反应稳定性。以稳定性为核心的外骨骼控制应优先考虑时间辅助参数，并纳入用户特异性个性化设置。本研究为实现个性化、以稳定性为导向的外骨骼控制迈出了重要一步，对提升老年人稳定性和降低跌倒风险具有直接意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the critical need for reducing fall risks in older adults, this study systematically investigates how hip exoskeleton assistance parameters affect gait stability during slip perturbations, moving beyond the typical focus on energy optimization. The method involved modulating the magnitude and duration of torque from a bilateral hip exoskeleton during induced slips in eight healthy adults, with stability quantified via whole-body angular momentum (WBAM). Key findings revealed a significant interaction between assistance magnitude and duration, where duration dictated whether assistance was stabilizing or destabilizing; stability-optimal parameters reduced WBAM range by 25.7% compared to an energy-optimized controller, but substantial inter-subject variability highlighted the necessity for personalized, stability-focused control strategies.</div>
<div class="mono" style="margin-top:8px">为应对降低老年人跌倒风险的迫切需求，本研究系统探讨了髋关节外骨骼辅助参数在滑倒扰动中对步态稳定性的影响。研究者在八名健康成年人诱发滑倒过程中，通过双侧髋关节外骨骼调节扭矩大小和持续时间，并采用全身角动量量化稳定性。实验结果表明，辅助大小与持续时间存在显著交互作用，其中持续时间决定辅助是稳定还是失稳；与能量优化控制器相比，稳定性最优参数使全身角动量范围平均降低25.7%，且受试者间存在显著差异，这凸显了个性化、以稳定性为核心的控制策略的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">CADGrasp: Learning Contact and Collision Aware General Dexterous Grasping in Cluttered Scenes</div>
<div class="meta-line">Authors: Jiyao Zhang, Zhiyuan Ma, Tianhao Wu, Zeyuan Chen, Hao Dong</div>
<div class="meta-line">First: 2026-01-21T14:43:29+00:00 · Latest: 2026-01-21T14:43:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15039v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15039v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous grasping in cluttered environments presents substantial challenges due to the high degrees of freedom of dexterous hands, occlusion, and potential collisions arising from diverse object geometries and complex layouts. To address these challenges, we propose CADGrasp, a two-stage algorithm for general dexterous grasping using single-view point cloud inputs. In the first stage, we predict sparse IBS, a scene-decoupled, contact- and collision-aware representation, as the optimization target. Sparse IBS compactly encodes the geometric and contact relationships between the dexterous hand and the scene, enabling stable and collision-free dexterous grasp pose optimization. To enhance the prediction of this high-dimensional representation, we introduce an occupancy-diffusion model with voxel-level conditional guidance and force closure score filtering. In the second stage, we develop several energy functions and ranking strategies for optimization based on sparse IBS to generate high-quality dexterous grasp poses. Extensive experiments in both simulated and real-world settings validate the effectiveness of our approach, demonstrating its capability to mitigate collisions while maintaining a high grasp success rate across diverse objects and complex scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CADGrasp：学习杂乱场景中接触与碰撞感知的通用灵巧抓取</div>
<div class="mono" style="margin-top:8px">在杂乱环境中进行灵巧抓取面临巨大挑战，原因包括灵巧手的高自由度、遮挡以及多样物体几何与复杂布局引发的潜在碰撞。为解决这些挑战，我们提出CADGrasp——一种基于单视角点云输入的两阶段通用灵巧抓取算法。第一阶段，我们预测稀疏IBS（一种场景解耦、接触与碰撞感知的表征）作为优化目标。稀疏IBS紧凑编码了灵巧手与场景间的几何及接触关系，支持稳定且无碰撞的灵巧抓取姿态优化。为提升这一高维表征的预测效果，我们引入了具有体素级条件引导和力闭合分数筛选的占用扩散模型。第二阶段，我们基于稀疏IBS开发了多种能量函数和排序策略进行优化，以生成高质量的灵巧抓取姿态。在仿真和真实场景中的大量实验验证了本方法的有效性，证明其能在多种物体和复杂场景中保持高抓取成功率的同时有效减少碰撞。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of dexterous grasping in cluttered scenes, where high hand degrees of freedom, occlusion, and complex object layouts lead to collision risks. The proposed CADGrasp method uses a two-stage approach: first, it predicts a sparse IBS representation from a single-view point cloud to encode contact and collision relationships, employing an occupancy-diffusion model with voxel-level guidance and filtering for robustness; second, it optimizes grasp poses using energy functions and ranking based on this representation. Experimental results in simulation and real-world settings show the method effectively reduces collisions while achieving high grasp success rates across diverse objects and scenes.</div>
<div class="mono" style="margin-top:8px">本研究针对灵巧抓取在杂乱场景中的挑战，即手的高自由度、遮挡和复杂物体布局导致的碰撞风险和抓取困难。提出的CADGrasp方法是一个两阶段算法：首先，它从单视角点云预测稀疏IBS表示，以编码接触和碰撞关系，使用具有体素级引导和过滤的占用扩散模型来增强预测；其次，基于该表示通过能量函数和排序策略优化抓取姿态。在仿真和真实环境中的大量实验结果表明，该方法能有效减少碰撞，同时在多样物体和复杂场景中保持高抓取成功率。</div>
</details>
</div>
<div class="card">
<div class="title">ExPrIS: Knowledge-Level Expectations as Priors for Object Interpretation from Sensor Data</div>
<div class="meta-line">Authors: Marian Renz, Martin Günther, Felix Igelbrink, Oscar Lima, Martin Atzmueller</div>
<div class="meta-line">Venue: Künstl Intell (2026)</div>
<div class="meta-line">First: 2026-01-21T14:27:38+00:00 · Latest: 2026-01-21T14:27:38+00:00</div>
<div class="meta-line">Comments: This preprint has not undergone peer review or any post-submission improvements or corrections. The Version of Record of this article is published in KI - Künstliche Intelligenz, and is available online at https://doi.org/10.1007/s13218-026-00901-7</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15025v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15025v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While deep learning has significantly advanced robotic object recognition, purely data-driven approaches often lack semantic consistency and fail to leverage valuable, pre-existing knowledge about the environment. This report presents the ExPrIS project, which addresses this challenge by investigating how knowledge-level expectations can serve as to improve object interpretation from sensor data. Our approach is based on the incremental construction of a 3D Semantic Scene Graph (3DSSG). We integrate expectations from two sources: contextual priors from past observations and semantic knowledge from external graphs like ConceptNet. These are embedded into a heterogeneous Graph Neural Network (GNN) to create an expectation-biased inference process. This method moves beyond static, frame-by-frame analysis to enhance the robustness and consistency of scene understanding over time. The report details this architecture, its evaluation, and outlines its planned integration on a mobile robotic platform.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExPrIS：将知识层期望作为传感器数据对象解释的先验</div>
<div class="mono" style="margin-top:8px">尽管深度学习显著推动了机器人目标识别的发展，但纯数据驱动方法常缺乏语义一致性，且未能有效利用环境中已有的宝贵先验知识。本报告介绍了ExPrIS项目，该项目通过研究如何将知识层期望作为先验来改进传感器数据的对象解释，以应对这一挑战。我们的方法基于增量式构建的三维语义场景图（3DSSG），整合了两种来源的期望：来自历史观测的上下文先验，以及来自外部知识图谱（如ConceptNet）的语义知识。这些期望被嵌入到异构图神经网络（GNN）中，形成一种期望偏置的推理过程。该方法突破了静态逐帧分析的局限，提升了场景理解在时序上的鲁棒性与一致性。报告详细阐述了该架构的设计、评估结果，并规划了在移动机器人平台上的集成方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of purely data-driven robotic object recognition, which often lacks semantic consistency and fails to utilize existing environmental knowledge, this research proposes the ExPrIS project. The method incrementally constructs a 3D Semantic Scene Graph (3DSSG) and integrates expectations from contextual priors and external semantic graphs like ConceptNet, embedding them into a heterogeneous Graph Neural Network for expectation-biased inference. Experimental evaluation demonstrates that this approach enhances the robustness and temporal consistency of scene understanding compared to static, frame-by-frame analysis.</div>
<div class="mono" style="margin-top:8px">本研究针对纯数据驱动的深度学习在机器人物体识别中缺乏语义一致性和未能利用现有环境知识的局限性。方法基于增量构建三维语义场景图（3DSSG），并整合来自历史观察的上下文先验和外部语义图（如ConceptNet）的期望，将其嵌入异构图神经网络（GNN）中以创建期望偏置的推理过程。实验评估表明，与静态的逐帧分析相比，该方法提高了场景理解的鲁棒性和时间一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Risk Estimation for Automated Driving</div>
<div class="meta-line">Authors: Leon Tolksdorf, Arturo Tejada, Jonas Bauernfeind, Christian Birkner, Nathan van de Wouw</div>
<div class="meta-line">First: 2026-01-21T14:14:50+00:00 · Latest: 2026-01-21T14:14:50+00:00</div>
<div class="meta-line">Comments: 10 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15018v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15018v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety is a central requirement for automated vehicles. As such, the assessment of risk in automated driving is key in supporting both motion planning technologies and safety evaluation. In automated driving, risk is characterized by two aspects. The first aspect is the uncertainty on the state estimates of other road participants by an automated vehicle. The second aspect is the severity of a collision event with said traffic participants. Here, the uncertainty aspect typically causes the risk to be non-zero for near-collision events. This makes risk particularly useful for automated vehicle motion planning. Namely, constraining or minimizing risk naturally navigates the automated vehicle around traffic participants while keeping a safety distance based on the level of uncertainty and the potential severity of the impending collision. Existing approaches to calculate the risk either resort to empirical modeling or severe approximations, and, hence, lack generalizability and accuracy. In this paper, we combine recent advances in collision probability estimation with the concept of collision severity to develop a general method for accurate risk estimation. The proposed method allows us to assign individual severity functions for different collision constellations, such as, e.g., frontal or side collisions. Furthermore, we show that the proposed approach is computationally efficient, which is beneficial, e.g., in real-time motion planning applications. The programming code for an exemplary implementation of Gaussian uncertainties is also provided.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶风险估计</div>
<div class="mono" style="margin-top:8px">安全性是自动驾驶车辆的核心要求。因此，自动驾驶中的风险评估对支持运动规划技术和安全评估至关重要。自动驾驶风险具有两个特征维度：一是自动驾驶车辆对其他道路参与者状态估计的不确定性；二是与这些交通参与者发生碰撞事件的严重程度。不确定性特征通常导致近碰撞事件的风险值非零，这使得风险指标在自动驾驶运动规划中尤为实用——通过约束或最小化风险，可基于不确定程度和潜在碰撞严重性，使自动驾驶车辆在保持安全距离的前提下绕行其他交通参与者。现有风险计算方法多依赖经验模型或粗略近似，缺乏普适性与精确性。本文结合碰撞概率估计的最新进展与碰撞严重性概念，提出一种精确风险估计的通用方法。该方法可为不同碰撞形态（如正面或侧面碰撞）分配独立的严重性函数，且计算高效，适用于实时运动规划等场景。文中还提供了高斯不确定性示例实现的编程代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for accurate and generalizable risk estimation in automated driving, which is crucial for motion planning and safety evaluation, as existing methods often rely on empirical models or approximations that compromise accuracy and generalizability. The proposed method integrates recent advances in collision probability estimation with collision severity modeling, allowing for customizable severity functions for different collision types, such as frontal or side impacts. Experimental results demonstrate that the approach is computationally efficient, making it suitable for real-time motion planning applications, and the authors provide programming code for an exemplary implementation using Gaussian uncertainties.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自动驾驶中风险估计的准确性和普适性问题，这对于运动规划和安全评估至关重要。该方法结合了碰撞概率估计的最新进展与碰撞严重性建模，允许为不同碰撞类型（如正面或侧面碰撞）定制个体严重性函数。实验结果表明，所提出的方法计算效率高，适用于实时应用，并且相比现有的经验或近似方法，能提供更准确的风险评估。</div>
</details>
</div>
<div class="card">
<div class="title">Context-aware Learned Mesh-based Simulation via Trajectory-Level Meta-Learning</div>
<div class="meta-line">Authors: Philipp Dahlinger, Niklas Freymuth, Tai Hoang, Tobias Würth, Michael Volpp, Luise Kärger, Gerhard Neumann</div>
<div class="meta-line">First: 2025-11-07T13:34:02+00:00 · Latest: 2026-01-21T14:05:44+00:00</div>
<div class="meta-line">Comments: 35 pages. Submitted to Transactions on Machine Learning Research (TMLR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05234v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05234v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulating object deformations is a critical challenge across many scientific domains, including robotics, manufacturing, and structural mechanics. Learned Graph Network Simulators (GNSs) offer a promising alternative to traditional mesh-based physics simulators. Their speed and inherent differentiability make them particularly well suited for applications that require fast and accurate simulations, such as robotic manipulation or manufacturing optimization. However, existing learned simulators typically rely on single-step observations, which limits their ability to exploit temporal context. Without this information, these models fail to infer, e.g., material properties. Further, they rely on auto-regressive rollouts, which quickly accumulate error for long trajectories. We instead frame mesh-based simulation as a trajectory-level meta-learning problem. Using Conditional Neural Processes, our method enables rapid adaptation to new simulation scenarios from limited initial data while capturing their latent simulation properties. We utilize movement primitives to directly predict fast, stable and accurate simulations from a single model call. The resulting approach, Movement-primitive Meta-MeshGraphNet (M3GN), provides higher simulation accuracy at a fraction of the runtime cost compared to state-of-the-art GNSs across several tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于轨迹级元学习的上下文感知网格模拟方法</div>
<div class="mono" style="margin-top:8px">物体形变模拟是机器人学、制造与结构力学等多个科学领域的关键挑战。学习型图网络模拟器（GNSs）为传统基于网格的物理模拟器提供了有前景的替代方案，其速度优势和内在可微性使其特别适用于需要快速精准模拟的应用场景，如机器人操控或制造优化。然而，现有学习型模拟器通常依赖单步观测，限制了其利用时序上下文信息的能力，导致无法推断材料属性等参数，且依赖自回归推演会因误差累积而影响长轨迹精度。本研究将网格模拟重构为轨迹级元学习问题，通过条件神经过程实现从有限初始数据快速适应新模拟场景，同时捕捉其潜在模拟特性。利用运动基元直接通过单次模型调用预测快速、稳定且精确的模拟结果。所提出的运动基元元网格图网络（M3GN）在多项任务中，以远低于先进GNSs的运行成本实现了更高的模拟精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses limitations in learned Graph Network Simulators (GNSs) for object deformation, which rely on single-step observations and auto-regressive rollouts that accumulate error, hindering inference of material properties and long-term accuracy. The method frames mesh-based simulation as a trajectory-level meta-learning problem using Conditional Neural Processes, enabling rapid adaptation to new scenarios from limited data and capturing latent simulation properties, and employs movement primitives for direct, stable trajectory prediction in a single model call. Experimental results show that the proposed Movement-primitive Meta-MeshGraphNet (M3GN) achieves higher simulation accuracy with significantly reduced runtime compared to state-of-the-art GNSs across multiple tasks.</div>
<div class="mono" style="margin-top:8px">针对现有基于学习的图网络模拟器依赖单步观测且在自回归推演中误差累积的问题，本研究将基于网格的模拟构建为一个轨迹层面的元学习问题。所提出的方法M3GN（运动基元元网格图网络）采用条件神经过程实现从有限初始数据对新场景的快速适应，并利用运动基元通过单次模型调用直接预测完整、稳定且准确的模拟轨迹。实验结果表明，在多个任务上，M3GN相比最先进的图网络模拟器，以显著降低的运行时间成本获得了更高的模拟精度。</div>
</details>
</div>
<div class="card">
<div class="title">DWPP: Dynamic Window Pure Pursuit Considering Velocity and Acceleration Constraints</div>
<div class="meta-line">Authors: Fumiya Ohnishi, Masaki Takahashi</div>
<div class="meta-line">First: 2026-01-21T14:05:42+00:00 · Latest: 2026-01-21T14:05:42+00:00</div>
<div class="meta-line">Comments: 28 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15006v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15006v1">PDF</a> · <a href="https://github.com/ros-navigation/navigation2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pure pursuit and its variants are widely used for mobile robot path tracking owing to their simplicity and computational efficiency. However, many conventional approaches do not explicitly account for velocity and acceleration constraints, resulting in discrepancies between commanded and actual velocities that result in overshoot and degraded tracking performance. To address this problem, this paper proposes dynamic window pure pursuit (DWPP), which fundamentally reformulates the command velocity computation process to explicitly incorporate velocity and acceleration constraints. Specifically, DWPP formulates command velocity computation in the velocity space (the $v$-$ω$ plane) and selects the command velocity as the point within the dynamic window that is closest to the line $ω= κv$. Experimental results demonstrate that DWPP avoids constraint-violating commands and achieves superior path-tracking accuracy compared with conventional pure pursuit methods. The proposed method has been integrated into the official Nav2 repository and is publicly available (https://github.com/ros-navigation/navigation2).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DWPP：考虑速度与加速度约束的动态窗口纯追踪算法</div>
<div class="mono" style="margin-top:8px">纯追踪及其变体算法因结构简单、计算高效而被广泛应用于移动机器人路径跟踪。然而，传统方法大多未显式考虑速度与加速度约束，导致指令速度与实际速度存在偏差，引发超调及跟踪性能下降。针对此问题，本文提出动态窗口纯追踪算法，通过重构指令速度计算流程，将速度与加速度约束显式纳入计算体系。具体而言，DWPP在速度空间（$v$-$ω$平面）构建指令速度计算模型，并在动态窗口内选取最接近直线$ω= κv$的点作为指令速度。实验结果表明，相较于传统纯追踪方法，DWPP能有效规避约束违规指令，并实现更优的路径跟踪精度。本方法已集成至官方Nav2代码库并开源发布（https://github.com/ros-navigation/navigation2）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of conventional pure pursuit path-tracking methods for mobile robots, which often ignore velocity and acceleration constraints, leading to command-actuation discrepancies, overshoot, and poor tracking. The proposed Dynamic Window Pure Pursuit (DWPP) method reformulates the command velocity computation by explicitly incorporating these constraints; it operates in the velocity space (v-ω plane) and selects the command velocity as the point within a dynamic window that is closest to the ideal kinematic line ω = κv. Experimental results show that DWPP successfully avoids constraint-violating commands and achieves superior path-tracking accuracy compared to traditional pure pursuit methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决移动机器人路径跟踪中传统纯追踪方法因未明确考虑速度与加速度约束，导致指令与实际速度存在偏差、产生超调及跟踪性能下降的问题。为此，本文提出动态窗口纯追踪（DWPP）方法，该方法从根本上重构了指令速度的计算过程，显式地纳入速度与加速度约束；具体而言，DWPP在速度空间（v-ω平面）中构建动态窗口，并选择窗口内最接近直线ω = κv的点作为指令速度。实验结果表明，与传统纯追踪方法相比，DWPP能有效避免违反约束的指令，并实现了更优的路径跟踪精度。</div>
</details>
</div>
<div class="card">
<div class="title">Graph-Based Adaptive Planning for Coordinated Dual-Arm Robotic Disassembly of Electronic Devices (eGRAP)</div>
<div class="meta-line">Authors: Adip Ranjan Das, Maria Koskinopoulou</div>
<div class="meta-line">First: 2026-01-21T13:57:03+00:00 · Latest: 2026-01-21T13:57:03+00:00</div>
<div class="meta-line">Comments: 7 Pages, 8 Figures, 5 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14998v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14998v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">E-waste is growing rapidly while recycling rates remain low. We propose an electronic-device Graph-based Adaptive Planning (eGRAP) that integrates vision, dynamic planning, and dual-arm execution for autonomous disassembly. A camera-equipped arm identifies parts and estimates their poses, and a directed graph encodes which parts must be removed first. A scheduler uses topological ordering of this graph to select valid next steps and assign them to two robot arms, allowing independent tasks to run in parallel. One arm carries a screwdriver (with an eye-in-hand depth camera) and the other holds or handles components. We demonstrate eGRAP on 3.5in hard drives: as parts are unscrewed and removed, the system updates its graph and plan online. Experiments show consistent full disassembly of each HDD, with high success rates and efficient cycle times, illustrating the method&#x27;s ability to adaptively coordinate dual-arm tasks in real time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图的自适应规划用于电子设备协调双臂机器人拆解（eGRAP）</div>
<div class="mono" style="margin-top:8px">电子废弃物快速增长，而回收率仍然较低。我们提出了一种基于图的电子设备自适应规划方法（eGRAP），该方法集成了视觉、动态规划和双臂执行，以实现自主拆解。配备摄像头的机械臂识别部件并估计其姿态，有向图编码了哪些部件必须优先移除。调度器利用该图的拓扑排序选择有效的后续步骤，并将其分配给两个机器人手臂，使独立任务能够并行执行。一只手臂携带螺丝刀（配有手眼深度相机），另一只手臂则抓持或处理组件。我们在3.5英寸硬盘驱动器上演示了eGRAP：随着部件被拧下和移除，系统在线更新其图和计划。实验表明，每个硬盘驱动器均能实现一致性的完整拆解，具有高成功率和高效循环时间，展示了该方法实时自适应协调双臂任务的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the rapid growth of e-waste and low recycling rates, this research introduces eGRAP, a system for autonomous robotic disassembly. The method integrates vision for part identification and pose estimation, encodes disassembly precedence constraints in a directed graph, and uses a scheduler based on topological ordering to dynamically assign parallel tasks to two robot arms—one for unscrewing and one for handling components. Experimental results on 3.5-inch hard drives demonstrate consistent full disassembly with high success rates and efficient cycle times, showcasing real-time adaptive coordination.</div>
<div class="mono" style="margin-top:8px">本研究针对电子废弃物快速增长而回收率低的现状，提出了eGRAP系统，一种基于图的电子设备自适应规划方法，用于自主拆卸。该方法整合了视觉识别部件与位姿估计，将有向图编码拆卸顺序，并利用基于拓扑排序的调度器动态分配有效任务给两个协调的机械臂——一个持螺丝刀，另一个负责部件抓取——从而实现并行操作。在3.5英寸硬盘上的实验结果表明，系统能持续完成完整拆卸，具有高成功率和高效的循环时间，展现了其实时自适应协调双机械臂任务的能力。</div>
</details>
</div>
<div class="card">
<div class="title">HumanDiffusion: A Vision-Based Diffusion Trajectory Planner with Human-Conditioned Goals for Search and Rescue UAV</div>
<div class="meta-line">Authors: Faryal Batool, Iana Zhura, Valerii Serpiva, Roohan Ahmed Khan, Ivan Valuev, Issatay Tokmurziyev, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-21T13:22:22+00:00 · Latest: 2026-01-21T13:22:22+00:00</div>
<div class="meta-line">Comments: This paper has been accepted at HRI, Late Breaking Report, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14973v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable human--robot collaboration in emergency scenarios requires autonomous systems that can detect humans, infer navigation goals, and operate safely in dynamic environments. This paper presents HumanDiffusion, a lightweight image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB imagery. The system combines YOLO-11--based human detection with diffusion-driven trajectory generation, enabling a quadrotor to approach a target person and deliver medical assistance without relying on prior maps or computationally intensive planning pipelines. Trajectories are predicted in pixel space, ensuring smooth motion and a consistent safety margin around humans. We evaluate HumanDiffusion in simulation and real-world indoor mock-disaster scenarios. On a 300-sample test set, the model achieves a mean squared error of 0.02 in pixel-space trajectory reconstruction. Real-world experiments demonstrate an overall mission success rate of 80% across accident-response and search-and-locate tasks with partial occlusions. These results indicate that human-conditioned diffusion planning offers a practical and robust solution for human-aware UAV navigation in time-critical assistance settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanDiffusion：面向搜救无人机的基于视觉与人类条件化目标的扩散轨迹规划器</div>
<div class="mono" style="margin-top:8px">紧急场景下可靠的人机协作需要自主系统能够检测人类、推断导航目标并在动态环境中安全运行。本文提出HumanDiffusion，一种轻量级的图像条件化扩散规划器，可直接从RGB图像生成具备人类感知的导航轨迹。该系统结合基于YOLO-11的人类检测与扩散驱动的轨迹生成技术，使四旋翼无人机无需依赖先验地图或计算密集型规划流程，即可接近目标人员并提供医疗援助。轨迹在像素空间中进行预测，确保运动平滑并保持与人类持续的安全距离。我们在仿真和真实室内模拟灾难场景中对HumanDiffusion进行评估。在300个样本的测试集上，该模型在像素空间轨迹重建中达到0.02的均方误差。真实环境实验显示，在存在部分遮挡的事故响应与搜索定位任务中，整体任务成功率达80%。结果表明，人类条件化扩散规划为时效性强的辅助场景中的人类感知无人机导航提供了实用且鲁棒的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for reliable human-robot collaboration in emergencies by developing a vision-based autonomous system for search and rescue UAVs. The proposed HumanDiffusion method is a lightweight, image-conditioned diffusion planner that generates human-aware navigation trajectories directly from RGB images, combining YOLO-11 for human detection with diffusion-driven trajectory prediction in pixel space to ensure smooth motion and safety margins without prior maps. Experimental evaluation in simulation and real-world indoor mock-disaster scenarios showed a mean squared error of 0.02 in pixel-space trajectory reconstruction on a 300-sample test set and an 80% overall mission success rate in tasks involving accident response and search with occlusions, demonstrating the method&#x27;s practicality for time-critical assistance.</div>
<div class="mono" style="margin-top:8px">为实现紧急情况下可靠的人机协作，本研究为搜救无人机开发了一种基于视觉的轨迹规划器，其需检测人类、推断目标并安全导航。所提出的HumanDiffusion方法是一个轻量级的、以图像为条件的扩散模型，它通过结合YOLO-11进行人体检测，并在像素空间中利用扩散驱动进行轨迹预测，直接从RGB图像生成考虑人类安全的导航轨迹，无需先验地图。在模拟和真实室内模拟灾难场景中的实验评估表明，在包含300个样本的测试集上，轨迹重建的平均平方误差为0.02，并且在涉及部分遮挡的任务中实现了80%的整体任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">TIDAL: Temporally Interleaved Diffusion and Action Loop for High-Frequency VLA Control</div>
<div class="meta-line">Authors: Yuteng Sun, Haoran Wang, Ruofei Bai, Zhengguo Li, Jun Li, Meng Yee, Chuah, Wei Yun Yau</div>
<div class="meta-line">First: 2026-01-21T12:43:11+00:00 · Latest: 2026-01-21T12:43:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14945v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14945v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale Vision-Language-Action (VLA) models offer semantic generalization but suffer from high inference latency, limiting them to low-frequency batch-and-execute paradigm. This frequency mismatch creates an execution blind spot, causing failures in dynamic environments where targets move during the open-loop execution window. We propose TIDAL (Temporally Interleaved Diffusion and Action Loop), a hierarchical framework that decouples semantic reasoning from high-frequency actuation. TIDAL operates as a backbone-agnostic module for diffusion-based VLAs, using a dual-frequency architecture to redistribute the computational budget. Specifically, a low-frequency macro-intent loop caches semantic embeddings, while a high-frequency micro-control loop interleaves single-step flow integration with execution. This design enables approximately 9 Hz control updates on edge hardware (vs. approximately 2.4 Hz baselines) without increasing marginal overhead. To handle the resulting latency shift, we introduce a temporally misaligned training strategy where the policy learns predictive compensation using stale semantic intent alongside real-time proprioception. Additionally, we address the insensitivity of static vision encoders to velocity by incorporating a differential motion predictor. TIDAL is architectural, making it orthogonal to system-level optimizations. Experiments show a 2x performance gain over open-loop baselines in dynamic interception tasks. Despite a marginal regression in static success rates, our approach yields a 4x increase in feedback frequency and extends the effective horizon of semantic embeddings beyond the native action chunk size. Under non-paused inference protocols, TIDAL remains robust where standard baselines fail due to latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TIDAL：面向高频视觉-语言-动作控制的时序交错扩散与动作循环框架</div>
<div class="mono" style="margin-top:8px">大规模视觉-语言-动作模型虽具备语义泛化能力，但高推理延迟使其受限于低频批处理执行范式。这种频率失配会形成执行盲区，导致动态环境中目标在开环执行窗口内移动时任务失败。本文提出TIDAL（时序交错扩散与动作循环）分层框架，将语义推理与高频执行解耦。TIDAL作为扩散式VLA的架构无关模块，采用双频架构重新分配计算资源：低频宏观意图循环缓存语义嵌入，高频微观控制循环将单步流集成与执行交错进行。该设计在边缘硬件上实现约9Hz控制更新（基线约2.4Hz）且不增加边际开销。为应对延迟偏移，我们提出时序错位训练策略，使策略能基于滞后的语义意图与实时本体感知学习预测补偿。此外，通过引入差分运动预测器解决静态视觉编码器对速度不敏感的问题。TIDAL属于架构级改进，与系统级优化正交。实验表明，在动态拦截任务中性能较开环基线提升2倍。尽管静态成功率略有下降，但反馈频率提升4倍，并将语义嵌入的有效作用范围扩展至原生动作块尺寸之外。在非暂停推理协议下，TIDAL在标准基线因延迟失效时仍保持鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high inference latency of large-scale Vision-Language-Action (VLA) models, which limits them to low-frequency control and causes failures in dynamic environments, this work introduces TIDAL, a hierarchical framework that decouples semantic reasoning from high-frequency actuation. The method employs a backbone-agnostic, dual-frequency architecture with a low-frequency loop for caching semantic embeddings and a high-frequency loop that interleaves single-step flow integration with execution, alongside temporally misaligned training and a differential motion predictor to handle latency and velocity. Experiments demonstrate that TIDAL enables approximately 9 Hz control updates on edge hardware, a 4x increase in feedback frequency, a 2x performance gain in dynamic interception tasks over open-loop baselines, and robustness under non-paused inference, despite a marginal regression in static success rates.</div>
<div class="mono" style="margin-top:8px">为解决大规模视觉-语言-动作（VLA）模型推理延迟高、局限于低频控制，从而导致动态环境中任务失败的问题，本研究提出了TIDAL，一种将语义推理与高频执行解耦的分层框架。该方法采用与主干网络无关的双频架构，包含一个用于缓存语义嵌入的低频循环和一个将单步流集成与执行交错进行的高频循环，并结合了时间错位训练策略和差分运动预测器来处理延迟和速度不敏感问题。实验结果表明，TIDAL在边缘硬件上实现了约9 Hz的控制更新，将反馈频率提升至基线方法的4倍，在动态拦截任务中性能提升2倍，并在非暂停推理协议下保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Collision Probability Estimation for Optimization-based Vehicular Motion Planning</div>
<div class="meta-line">Authors: Leon Tolksdorf, Arturo Tejada, Christian Birkner, Nathan van de Wouw</div>
<div class="meta-line">First: 2025-05-27T13:16:03+00:00 · Latest: 2026-01-21T12:21:52+00:00</div>
<div class="meta-line">Comments: 14 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.21161v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.21161v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many motion planning algorithms for automated driving require estimating the probability of collision (POC) to account for uncertainties in the measurement and estimation of the motion of road users. Common POC estimation techniques often utilize sampling-based methods that suffer from computational inefficiency and a non-deterministic estimation, i.e., each estimation result for the same inputs is slightly different. In contrast, optimization-based motion planning algorithms require computationally efficient POC estimation, ideally using deterministic estimation, such that typical optimization algorithms for motion planning retain feasibility. Estimating the POC analytically, however, is challenging because it depends on understanding the collision conditions (e.g., vehicle&#x27;s shape) and characterizing the uncertainty in motion prediction. In this paper, we propose an approach in which we estimate the POC between two vehicles by over-approximating their shapes by a multi-circular shape approximation. The position and heading of the predicted vehicle are modelled as random variables, contrasting with the literature, where the heading angle is often neglected. We guarantee that the provided POC is an over-approximation, which is essential in providing safety guarantees. For the particular case of Gaussian uncertainty in the position and heading, we present a computationally efficient algorithm for computing the POC estimate. This algorithm is then used in a path-following stochastic model predictive controller (SMPC) for motion planning. With the proposed algorithm, the SMPC generates reproducible trajectories while the controller retains its feasibility in the presented test cases and demonstrates the ability to handle varying levels of uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于优化的车辆运动规划碰撞概率估计</div>
<div class="mono" style="margin-top:8px">自动驾驶的许多运动规划算法需要估计碰撞概率（POC），以考虑道路使用者运动测量和估计中的不确定性。常见的POC估计技术通常采用基于采样的方法，这些方法存在计算效率低和估计非确定性的问题，即相同输入的每次估计结果略有不同。相比之下，基于优化的运动规划算法需要计算高效的POC估计，理想情况下使用确定性估计，以便典型的运动规划优化算法保持可行性。然而，解析估计POC具有挑战性，因为它依赖于理解碰撞条件（例如车辆形状）并表征运动预测中的不确定性。本文提出一种方法，通过多圆形形状近似来过度逼近两辆车的形状，从而估计它们之间的POC。预测车辆的位置和航向被建模为随机变量，这与文献中常忽略航向角的情况形成对比。我们保证提供的POC是过度逼近，这对于提供安全保证至关重要。针对位置和航向存在高斯不确定性的特定情况，我们提出一种计算高效的POC估计算法。该算法随后用于运动规划中的路径跟踪随机模型预测控制器（SMPC）。通过所提算法，SMPC生成可复现的轨迹，同时控制器在测试案例中保持可行性，并展示了处理不同不确定性水平的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for computationally efficient and deterministic collision probability estimation in optimization-based motion planning for automated driving, overcoming the limitations of sampling-based methods that are slow and non-deterministic. The proposed method over-approximates vehicle shapes using multi-circular approximations and models the position and heading of predicted vehicles as random variables, explicitly including often-neglected heading uncertainty, to provide a guaranteed over-approximation of collision probability. For Gaussian uncertainty, a computationally efficient algorithm is developed and integrated into a stochastic model predictive controller, which in experimental test cases generated reproducible trajectories, maintained controller feasibility, and successfully handled varying uncertainty levels.</div>
<div class="mono" style="margin-top:8px">为解决基于采样的碰撞概率估计方法在运动规划中计算效率低且结果非确定性的问题，本文提出一种分析方法，该方法使用多圆形近似来过度逼近车辆形状，并将位置和航向角均建模为随机变量，特别针对高斯不确定性。核心方法是一种计算高效的算法，用于计算有保证的碰撞概率过度逼近值，并将其集成到随机模型预测控制器中以生成轨迹。实验结果表明，该控制器能生成可复现的轨迹，在测试案例中保持可行性，并能有效处理不同程度的不确定性。</div>
</details>
</div>
<div class="card">
<div class="title">Allocation for Omnidirectional Aerial Robots: Incorporating Power Dynamics</div>
<div class="meta-line">Authors: Eugenio Cuniato, Mike Allenspach, Thomas Stastny, Helen Oleynikova, Roland Siegwart, Michael Pantic</div>
<div class="meta-line">First: 2024-12-20T17:48:56+00:00 · Latest: 2026-01-21T12:12:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.16107v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.16107v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tilt-rotor aerial robots are more dynamic and versatile than fixed-rotor platforms, since the thrust vector and body orientation are decoupled. However, the coordination of servos and propellers (the allocation problem) is not trivial, especially accounting for overactuation and actuator dynamics. We incrementally build and present three novel allocation methods for tilt-rotor aerial robots, comparing them to state-of-the-art methods on a real system performing dynamic maneuvers. We extend the state-of-the-art geometric allocation into a differential allocation, which uses the platform&#x27;s redundancy and does not suffer from singularities. We expand it by incorporating actuator dynamics and propeller power dynamics. These allow us to model dynamic propeller acceleration limits, bringing two main advantages: balancing propeller speed without the need for nullspace goals and allowing the platform to selectively turn off propellers during flight, opening the door to new manipulation possibilities. We also use actuator dynamics and limits to normalize the allocation problem, making it easier to tune and allowing it to track 70% faster trajectories than a geometric allocation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全向空中机器人的分配策略：融合动力动力学</div>
<div class="mono" style="margin-top:8px">倾转旋翼空中机器人因推力矢量与机体姿态解耦，比固定旋翼平台更具动态性与多功能性。然而，舵机与螺旋桨的协调（即分配问题）尤为复杂，需兼顾过驱动与执行器动力学。本文逐步构建并提出了三种适用于倾转旋翼空中机器人的新型分配方法，通过在真实系统上执行动态机动与前沿方法进行对比。我们将前沿的几何分配扩展为微分分配，利用平台冗余性且规避奇异性。进一步融合执行器动力学与螺旋桨功率动力学，实现了动态螺旋桨加速度限制建模，带来两大优势：无需零空间目标即可平衡螺旋桨转速，并允许平台在飞行中选择性停转螺旋桨，为新型操控可能开辟路径。同时，利用执行器动力学与限值对分配问题进行归一化处理，使其更易调参，并能追踪比几何分配快70%的轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the allocation problem for tilt-rotor aerial robots, motivated by the need to effectively coordinate their decoupled thrust and orientation control while accounting for overactuation and actuator dynamics. The method incrementally develops three novel allocators: a differential extension of geometric allocation to avoid singularities, an incorporation of actuator dynamics, and a further integration of propeller power dynamics to model acceleration limits. Key experimental results on a real system show that these methods enable dynamic propeller speed balancing, selective propeller deactivation, and the ability to track trajectories 70% faster than a baseline geometric allocator during dynamic maneuvers.</div>
<div class="mono" style="margin-top:8px">本研究针对倾转旋翼空中机器人的分配问题，其动机源于需要有效协调其解耦的推力与姿态控制，并考虑过驱动和执行器动力学。方法上逐步提出了三种新颖的分配策略：对几何分配进行微分扩展以避免奇异性，纳入执行器动力学，并进一步整合螺旋桨功率动力学以建模加速度限制。在真实系统上的关键实验结果表明，这些方法能够实现动态的螺旋桨转速平衡、选择性停转螺旋桨以用于操控，并且使轨迹跟踪速度比标准几何分配器快70%。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language Models on the Edge for Real-Time Robotic Perception</div>
<div class="meta-line">Authors: Sarat Ahmad, Maryam Hafeez, Syed Ali Raza Zaidi</div>
<div class="meta-line">First: 2026-01-21T12:09:48+00:00 · Latest: 2026-01-21T12:09:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14921v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14921v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) enable multimodal reasoning for robotic perception and interaction, but their deployment in real-world systems remains constrained by latency, limited onboard resources, and privacy risks of cloud offloading. Edge intelligence within 6G, particularly Open RAN and Multi-access Edge Computing (MEC), offers a pathway to address these challenges by bringing computation closer to the data source. This work investigates the deployment of VLMs on ORAN/MEC infrastructure using the Unitree G1 humanoid robot as an embodied testbed. We design a WebRTC-based pipeline that streams multimodal data to an edge node and evaluate LLaMA-3.2-11B-Vision-Instruct deployed at the edge versus in the cloud under real-time conditions. Our results show that edge deployment preserves near-cloud accuracy while reducing end-to-end latency by 5\%. We further evaluate Qwen2-VL-2B-Instruct, a compact model optimized for resource-constrained environments, which achieves sub-second responsiveness, cutting latency by more than half but at the cost of accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向实时机器人感知的边缘视觉语言模型部署</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）为机器人感知与交互提供了多模态推理能力，但其在实际系统中的部署仍受限于延迟、机载资源有限以及云端卸载的隐私风险。6G网络中的边缘智能（特别是开放式无线接入网与多接入边缘计算）通过将计算任务靠近数据源，为解决这些挑战提供了可行路径。本研究以宇树G1人形机器人为实体测试平台，探索了在ORAN/MEC基础设施上部署VLMs的方案。我们设计了一套基于WebRTC的多模态数据流传输管道至边缘节点，并在实时条件下对比评估了边缘部署与云端部署的LLaMA-3.2-11B-Vision-Instruct模型性能。实验结果表明：边缘部署在保持接近云端精度的同时，将端到端延迟降低了5%。进一步评估了专为资源受限环境优化的紧凑模型Qwen2-VL-2B-Instruct，该模型实现了亚秒级响应，延迟降低超50%，但需以精度下降为代价。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenges of deploying Vision-Language Models (VLMs) for real-time robotic perception, where cloud-based solutions suffer from latency, resource constraints, and privacy concerns. The method leverages 6G edge intelligence, specifically Open RAN and Multi-access Edge Computing (MEC), by designing a WebRTC-based pipeline to stream multimodal data from a Unitree G1 humanoid robot to an edge node, comparing the performance of the LLaMA-3.2-11B-Vision-Instruct model at the edge versus in the cloud. Experimental results demonstrate that edge deployment maintains near-cloud accuracy while reducing end-to-end latency by 5%, and a more compact model, Qwen2-VL-2B-Instruct, achieves sub-second responsiveness with over 50% latency reduction, albeit with a trade-off in accuracy.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型在实时机器人感知应用中面临的延迟、资源受限和云端卸载隐私风险等问题，本研究探索了基于6G基础设施（特别是开放无线接入网和多接入边缘计算）的边缘智能部署方案。该方法以宇树G1人形机器人为实体测试平台，设计了一个基于WebRTC的流水线，将多模态数据流式传输至边缘节点，并对比了LLaMA-3.2-11B-Vision-Instruct模型在边缘与云端部署的性能。实验结果表明，边缘部署在保持接近云端精度的同时，将端到端延迟降低了5%；而针对资源受限环境优化的紧凑模型Qwen2-VL-2B-Instruct实现了亚秒级响应，延迟减少超50%，但以牺牲部分精度为代价。</div>
</details>
</div>
<div class="card">
<div class="title">HumanoidVLM: Vision-Language-Guided Impedance Control for Contact-Rich Humanoid Manipulation</div>
<div class="meta-line">Authors: Yara Mahmoud, Yasheerah Yaqoot, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-21T11:04:19+00:00 · Latest: 2026-01-21T11:04:19+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14874v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14874v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots must adapt their contact behavior to diverse objects and tasks, yet most controllers rely on fixed, hand-tuned impedance gains and gripper settings. This paper introduces HumanoidVLM, a vision-language driven retrieval framework that enables the Unitree G1 humanoid to select task-appropriate Cartesian impedance parameters and gripper configurations directly from an egocentric RGB image. The system couples a vision-language model for semantic task inference with a FAISS-based Retrieval-Augmented Generation (RAG) module that retrieves experimentally validated stiffness-damping pairs and object-specific grasp angles from two custom databases, and executes them through a task-space impedance controller for compliant manipulation. We evaluate HumanoidVLM on 14 visual scenarios and achieve a retrieval accuracy of 93%. Real-world experiments show stable interaction dynamics, with z-axis tracking errors typically within 1-3.5 cm and virtual forces consistent with task-dependent impedance settings. These results demonstrate the feasibility of linking semantic perception with retrieval-based control as an interpretable path toward adaptive humanoid manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanoidVLM：面向接触丰富人形机器人操作的视觉-语言引导阻抗控制</div>
<div class="mono" style="margin-top:8px">人形机器人需针对不同物体与任务调整接触行为，但多数控制器依赖固定、手动调谐的阻抗增益与夹持器设置。本文提出HumanoidVLM——一种视觉-语言驱动的检索框架，使Unitree G1人形机器人能直接从第一视角RGB图像中选取适合任务的笛卡尔阻抗参数与夹持器配置。该系统将用于语义任务推断的视觉-语言模型与基于FAISS的检索增强生成模块相结合，从两个定制数据库中检索经实验验证的刚度-阻尼参数对及物体专用抓取角度，并通过任务空间阻抗控制器执行以实现顺应性操作。我们在14个视觉场景中评估HumanoidVLM，检索准确率达93%。真实世界实验显示稳定的交互动力学特性：Z轴跟踪误差通常保持在1-3.5厘米内，虚拟力与任务相关的阻抗设置一致。这些结果证明了将语义感知与基于检索的控制相联结的可行性，为自适应人形机器人操作提供了可解释的实现路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Humanoid robots require adaptable contact behavior for diverse manipulation tasks, but existing controllers often use fixed impedance parameters. This paper presents HumanoidVLM, a framework that uses an egocentric RGB image and a vision-language model to infer the task semantics, then retrieves appropriate Cartesian impedance parameters and gripper configurations from custom databases via a FAISS-based Retrieval-Augmented Generation module. The retrieved settings are executed by a task-space impedance controller. Experimental evaluation on 14 visual scenarios achieved 93% retrieval accuracy, and real-world tests demonstrated stable interaction with z-axis tracking errors within 1-3.5 cm and virtual forces consistent with the task-dependent impedance settings, showing the feasibility of linking semantic perception with retrieval-based control for adaptive manipulation.</div>
<div class="mono" style="margin-top:8px">为解决人形机器人操作中阻抗参数固定的局限性，本文提出HumanoidVLM，一种视觉语言框架，能够从第一人称RGB图像中选择适合任务的笛卡尔阻抗参数和夹爪配置。该方法将用于语义任务推理的视觉语言模型与基于FAISS的检索增强生成模块相结合，从定制数据库中检索经过实验验证的刚度-阻尼对和抓取角度，并通过任务空间阻抗控制器执行。在14个视觉场景中的实验评估实现了93%的检索准确率，真实世界测试显示出稳定的交互动态，包括z轴跟踪误差在1-3.5厘米内，虚拟力与任务相关的阻抗设置一致，证明了将语义感知与基于检索的控制相结合以实现自适应操作的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">On-the-fly hand-eye calibration for the da Vinci surgical robot</div>
<div class="meta-line">Authors: Zejian Cui, Ferdinando Rodriguez y Baena</div>
<div class="meta-line">First: 2026-01-21T10:58:28+00:00 · Latest: 2026-01-21T10:58:28+00:00</div>
<div class="meta-line">Comments: 16 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14871v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14871v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Robot-Assisted Minimally Invasive Surgery (RMIS), accurate tool localization is crucial to ensure patient safety and successful task execution. However, this remains challenging for cable-driven robots, such as the da Vinci robot, because erroneous encoder readings lead to pose estimation errors. In this study, we propose a calibration framework to produce accurate tool localization results through computing the hand-eye transformation matrix on-the-fly. The framework consists of two interrelated algorithms: the feature association block and the hand-eye calibration block, which provide robust correspondences for key points detected on monocular images without pre-training, and offer the versatility to accommodate various surgical scenarios by adopting an array of filter approaches, respectively. To validate its efficacy, we test the framework extensively on publicly available video datasets that feature multiple surgical instruments conducting tasks in both in vitro and ex vivo scenarios, under varying illumination conditions and with different levels of key point measurement accuracy. The results show a significant reduction in tool localization errors under the proposed calibration framework, with accuracies comparable to other state-of-the-art methods while being more time-efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>达芬奇手术机器人实时手眼标定方法</div>
<div class="mono" style="margin-top:8px">在机器人辅助微创手术中，精确的工具定位对确保患者安全和手术成功至关重要。然而，对于达芬奇机器人这类线缆驱动系统，编码器读数误差会导致位姿估计偏差，这仍是当前的技术难点。本研究提出一种实时计算手眼变换矩阵的标定框架，以实现精准工具定位。该框架包含两个互相关联的算法模块：特征关联模块通过无预训练的单目图像关键点检测建立鲁棒对应关系；手眼标定模块采用多重滤波策略以适应多样化手术场景。通过在公开视频数据集上的系统性验证——涵盖体外与离体场景下多种手术器械操作、不同光照条件及关键点检测精度——结果表明：本标定框架能显著降低工具定位误差，在保持与前沿方法相当精度的同时具备更优的时间效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate tool localization is essential for patient safety in Robot-Assisted Minimally Invasive Surgery (RMIS), but remains challenging for cable-driven robots like the da Vinci due to encoder errors. To address this, the study proposes an on-the-fly calibration framework that computes the hand-eye transformation matrix using two interrelated algorithms: a feature association block for robust, pre-training-free key point correspondences from monocular images, and a hand-eye calibration block employing filter approaches for versatility across surgical scenarios. Extensive testing on public video datasets featuring multiple instruments in in vitro and ex vivo conditions under varying illumination and key point accuracy levels demonstrates that the framework significantly reduces tool localization errors, achieving accuracy comparable to state-of-the-art methods with greater time efficiency.</div>
<div class="mono" style="margin-top:8px">在机器人辅助微创手术中，精确的工具定位对患者安全至关重要，但达芬奇等线驱机器人因编码器误差而难以实现。为此，本研究提出了一种实时手眼标定框架，包含两个算法：特征关联模块无需预训练即可从单目图像中鲁棒匹配关键点，手眼标定模块则采用多种滤波器以适应不同手术场景。在公开视频数据集上的广泛测试表明，该框架在不同光照和测量精度条件下显著降低了工具定位误差，其精度与先进方法相当，且时间效率更高。</div>
</details>
</div>
<div class="card">
<div class="title">GuideTouch: An Obstacle Avoidance Device with Tactile Feedback for Visually Impaired</div>
<div class="meta-line">Authors: Timofei Kozlov, Artem Trandofilov, Georgii Gazaryan, Issatay Tokmurziyev, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-20T10:12:05+00:00 · Latest: 2026-01-21T10:36:57+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13813v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13813v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safe navigation for the visually impaired individuals remains a critical challenge, especially concerning head-level obstacles, which traditional mobility aids often fail to detect. We introduce GuideTouch, a compact, affordable, standalone wearable device designed for autonomous obstacle avoidance. The system integrates two vertically aligned Time-of-Flight (ToF) sensors, enabling three-dimensional environmental perception, and four vibrotactile actuators that provide directional haptic feedback. Proximity and direction information is communicated via an intuitive 4-point vibrotactile feedback system located across the user&#x27;s shoulders and upper chest. For real-world robustness, the device includes a unique centrifugal self-cleaning optical cover mechanism and a sound alarm system for location if the device is dropped. We evaluated the haptic perception accuracy across 22 participants (17 male and 5 female, aged 21-48, mean 25.7, sd 6.1). Statistical analysis confirmed a significant difference between the perception accuracy of different patterns. The system demonstrated high recognition accuracy, achieving an average of 92.9% for single and double motor (primary directional) patterns. Furthermore, preliminary experiments with 14 visually impaired users validated this interface, showing a recognition accuracy of 93.75% for primary directional cues. The results demonstrate that GuideTouch enables intuitive spatial perception and could significantly improve the safety, confidence, and autonomy of users with visual impairments during independent navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GuideTouch：面向视障人士的触觉反馈避障设备</div>
<div class="mono" style="margin-top:8px">视障人士的安全导航仍是严峻挑战，传统助行工具常无法探测头部高度的障碍物。本文推出GuideTouch——一款紧凑、经济、独立的可穿戴自主避障设备。该系统集成两个垂直排列的飞行时间传感器实现三维环境感知，以及四个振动触觉执行器提供定向触觉反馈。通过分布于用户肩部与上胸的直观四点振动触觉反馈系统传递距离与方向信息。为提升实际环境适应性，设备配备独特的离心自清洁光学盖板机制，以及遗失报警定位系统。我们在22名参与者中评估触觉感知准确率，统计分析证实不同振动模式的感知准确率存在显著差异。系统展现出高识别准确率：单/双电机（主方向）模式平均达92.9%。此外，14名视障用户的初步实验验证了该界面的有效性，对主方向线索的识别准确率达93.75%。结果表明GuideTouch能实现直观空间感知，可显著提升视障用户独立导航时的安全性、信心与自主性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the critical safety challenge of head-level obstacle detection for visually impaired individuals, which traditional aids often miss, this research introduces GuideTouch, a compact standalone wearable device. The method integrates two vertically aligned Time-of-Flight sensors for 3D environmental perception and four vibrotactile actuators on the user&#x27;s shoulders and chest to provide intuitive directional haptic feedback; it also includes a self-cleaning optical cover and a sound alarm for robustness. Experimental results with 22 participants showed high recognition accuracy for haptic patterns (average 92.9% for primary directional cues), and preliminary tests with 14 visually impaired users confirmed a 93.75% accuracy for these directional cues, demonstrating the system&#x27;s potential to enhance safety and autonomy during navigation.</div>
<div class="mono" style="margin-top:8px">为解决视障人士安全导航的关键挑战，特别是传统辅助工具常无法检测到的头部高度障碍物，本研究介绍了GuideTouch，一款紧凑、经济、独立的可穿戴设备。该方法集成了两个垂直排列的飞行时间传感器以实现三维环境感知，以及四个振动触觉执行器，在用户肩部和上胸部提供定向触觉反馈，并辅以自清洁光学盖和声音警报系统以增强鲁棒性。对22名参与者的实验结果显示，触觉模式识别准确率高，主要定向线索的平均准确率达92.9%；初步测试中14名视障用户对这些定向线索的识别准确率为93.75%，表明该系统有望提升导航过程中的安全性和自主性。</div>
</details>
</div>
<div class="card">
<div class="title">DroneVLA: VLA based Aerial Manipulation</div>
<div class="meta-line">Authors: Fawad Mehboob, Monijesu James, Amir Habel, Jeffrin Sam, Miguel Altamirano Cabrera, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-20T10:08:00+00:00 · Latest: 2026-01-21T10:32:20+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13809v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As aerial platforms evolve from passive observers to active manipulators, the challenge shifts toward designing intuitive interfaces that allow non-expert users to command these systems naturally. This work introduces a novel concept of autonomous aerial manipulation system capable of interpreting high-level natural language commands to retrieve objects and deliver them to a human user. The system is intended to integrate a MediaPipe based on Grounding DINO and a Vision-Language-Action (VLA) model with a custom-built drone equipped with a 1-DOF gripper and an Intel RealSense RGB-D camera. VLA performs semantic reasoning to interpret the intent of a user prompt and generates a prioritized task queue for grasping of relevant objects in the scene. Grounding DINO and dynamic A* planning algorithm are used to navigate and safely relocate the object. To ensure safe and natural interaction during the handover phase, the system employs a human-centric controller driven by MediaPipe. This module provides real-time human pose estimation, allowing the drone to employ visual servoing to maintain a stable, distinct position directly in front of the user, facilitating a comfortable handover. We demonstrate the system&#x27;s efficacy through real-world experiments for localization and navigation, which resulted in a 0.164m, 0.070m, and 0.084m of max, mean euclidean, and root-mean squared errors, respectively, highlighting the feasibility of VLA for aerial manipulation operations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DroneVLA：基于视觉语言动作模型的空中操控系统</div>
<div class="mono" style="margin-top:8px">随着空中平台从被动观测者演变为主动操控者，设计直观界面让非专业用户自然操控系统成为关键挑战。本研究提出一种新型自主空中操控系统，能够解析高级自然语言指令以抓取物体并递交给人类用户。该系统整合了基于Grounding DINO的MediaPipe模块、视觉-语言-动作模型，以及搭载单自由度夹爪与英特尔实感RGB-D相机的定制无人机。VLA模型通过语义推理解析用户指令意图，生成场景相关物体的优先级抓取任务队列；Grounding DINO与动态A*规划算法协同实现导航与安全物体转移。为确保交接阶段安全自然的交互，系统采用MediaPipe驱动的人体中心控制器，通过实时人体姿态估计使无人机利用视觉伺服保持稳定悬停于用户正前方，实现舒适物体交接。通过实际定位导航实验验证系统效能，最大误差、平均欧氏误差与均方根误差分别为0.164米、0.070米与0.084米，证明了VLA模型在空中操控任务中的可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for intuitive interfaces that enable non-expert users to command aerial manipulation systems using natural language. The proposed system, DroneVLA, integrates a Vision-Language-Action (VLA) model for semantic reasoning and task planning, Grounding DINO and a dynamic A* algorithm for object navigation, and a MediaPipe-based human-centric controller for visual servoing during handover, all deployed on a custom drone with a gripper and RGB-D camera. Experimental results from real-world tests demonstrate the system&#x27;s localization and navigation accuracy, with maximum, mean Euclidean, and root-mean squared errors of 0.164m, 0.070m, and 0.084m respectively, validating the feasibility of using VLA models for autonomous aerial manipulation tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对空中平台从被动观测转向主动操控时对直观控制界面的需求，旨在让非专业用户能够使用自然语言指令无人机。方法整合了视觉-语言-行动模型进行语义推理和任务规划、Grounding DINO进行目标检测、动态A*算法进行导航，以及基于MediaPipe的以人为中心的控制器在交接阶段进行视觉伺服，所有系统部署于配备夹爪和RGB-D相机的定制无人机上。真实环境实验结果表明，定位与导航效果良好，最大误差、平均欧氏误差和均方根误差分别为0.164米、0.070米和0.084米，验证了使用视觉-语言-行动模型实现自主空中操控任务的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">From Observation to Prediction: LSTM for Vehicle Lane Change Forecasting on Highway On/Off-Ramps</div>
<div class="meta-line">Authors: Mohamed Abouras, Catherine M. Elias</div>
<div class="meta-line">First: 2026-01-21T10:31:03+00:00 · Latest: 2026-01-21T10:31:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14848v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14848v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On and off-ramps are understudied road sections even though they introduce a higher level of variation in highway interactions. Predicting vehicles&#x27; behavior in these areas can decrease the impact of uncertainty and increase road safety. In this paper, the difference between this Area of Interest (AoI) and a straight highway section is studied. Multi-layered LSTM architecture to train the AoI model with ExiD drone dataset is utilized. In the process, different prediction horizons and different models&#x27; workflow are tested. The results show great promise on horizons up to 4 seconds with prediction accuracy starting from about 76% for the AoI and 94% for the general highway scenarios on the maximum horizon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从观察到预测：基于LSTM的高速公路匝道车辆换道行为预测</div>
<div class="mono" style="margin-top:8px">高速公路匝道作为交通行为变异度较高的路段尚未得到充分研究。预测车辆在这些区域的行为可降低不确定性影响并提升道路安全。本文研究了该兴趣区域与直线高速路段的差异，采用多层LSTM架构结合ExiD无人机数据集进行建模，测试了不同预测时间跨度和模型工作流程。结果表明：在最长4秒的预测跨度中，兴趣区域预测准确率约76%，常规高速场景达94%，展现出良好预测潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to improve road safety on highway on/off-ramps, which are understudied areas that introduce high behavioral uncertainty. The method employs a multi-layered LSTM architecture trained on the ExiD drone dataset to forecast vehicle lane changes, testing different prediction horizons and model workflows. Key experimental findings indicate the model shows great promise, achieving up to about 76% prediction accuracy for the ramp areas and 94% for general highway sections on a maximum horizon of 4 seconds.</div>
<div class="mono" style="margin-top:8px">本研究旨在提高高速公路匝道区域的行驶安全，这些区域研究不足且行为不确定性较高。方法采用多层LSTM架构，利用ExiD无人机数据集进行训练，以预测车辆换道行为，并测试了不同的预测时间范围和模型工作流程。主要实验结果表明，该模型在长达4秒的预测范围内表现出良好前景，在目标区域的预测准确率最高可达约76%，在一般高速公路场景下则达到94%。</div>
</details>
</div>
<div class="card">
<div class="title">Implementing Knowledge Representation and Reasoning with Object Oriented Design</div>
<div class="meta-line">Authors: Abdelrhman Bassiouny, Tom Schierenbeck, Sorin Arion, Benjamin Alt, Naren Vasantakumaar, Giang Nguyen, Michael Beetz</div>
<div class="meta-line">Venue: IJCAI</div>
<div class="meta-line">First: 2026-01-21T10:14:29+00:00 · Latest: 2026-01-21T10:14:29+00:00</div>
<div class="meta-line">Comments: 9 pages, 2 figures, submitted to the 2026 International Joint Conference on Artificial Intelligence (IJCAI)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14840v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14840v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces KRROOD, a framework designed to bridge the integration gap between modern software engineering and Knowledge Representation &amp; Reasoning (KR&amp;R) systems. While Object-Oriented Programming (OOP) is the standard for developing complex applications, existing KR&amp;R frameworks often rely on external ontologies and specialized languages that are difficult to integrate with imperative code. KRROOD addresses this by treating knowledge as a first-class programming abstraction using native class structures, bridging the gap between the logic programming and OOP paradigms. We evaluate the system on the OWL2Bench benchmark and a human-robot task learning scenario. Experimental results show that KRROOD achieves strong performance while supporting the expressive reasoning required for real-world autonomous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于面向对象设计实现知识表示与推理</div>
<div class="mono" style="margin-top:8px">本文介绍了KRROOD框架，旨在弥合现代软件工程与知识表示与推理系统之间的集成鸿沟。虽然面向对象编程已成为开发复杂应用的标准范式，但现有KR&amp;R框架常依赖外部本体和专用语言，难以与命令式代码集成。KRROOD通过将知识作为原生类结构的一等编程抽象，有效衔接了逻辑编程与面向对象编程范式。我们在OWL2Bench基准测试和人机任务学习场景中评估了该系统。实验结果表明，KRROOD在保持卓越性能的同时，能够满足现实自主系统所需的表达性推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the integration gap between modern software engineering practices, particularly Object-Oriented Programming (OOP), and specialized Knowledge Representation and Reasoning (KR&amp;R) systems, which often rely on external ontologies and languages. To address this, the authors propose KRROOD, a framework that treats knowledge as a first-class programming abstraction by using native class structures, thereby bridging logic programming and OOP paradigms. Experimental evaluation on the OWL2Bench benchmark and a human-robot task learning scenario demonstrates that KRROOD achieves strong performance while supporting the expressive reasoning necessary for real-world autonomous systems.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决现代软件工程（严重依赖面向对象编程）与知识表示与推理系统（通常依赖外部本体和专用语言）之间的集成鸿沟。为此，作者提出了KRROOD框架，该框架将知识视为使用原生类结构的一等编程抽象，从而弥合了逻辑编程与面向对象编程范式之间的差距。在OWL2Bench基准测试和人机任务学习场景中的实验评估表明，KRROOD在支持现实世界自主系统所需的表现性推理的同时，实现了强大的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Moving Beyond Compliance in Soft-Robotic Catheters Through Modularity for Precision Therapies</div>
<div class="meta-line">Authors: B. Calmé, N. J. Greenidge, A. Metcalf, A. Bacchetti, G. Loza, D. Kpeglo, P. Lloyd, V. Pensabene, J. H. Chandler, P. Valdastri</div>
<div class="meta-line">First: 2026-01-21T10:11:59+00:00 · Latest: 2026-01-21T10:11:59+00:00</div>
<div class="meta-line">Comments: 31 pages, 6 figures, 7 supplementary figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14837v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14837v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Soft robotic instruments could navigate delicate, tortuous anatomy more safely than rigid tools, but clinical adoption is limited by insufficient tip functionalization and real-time feedback at the tissue interface. Few sensing and therapeutic modules are compact, robust, and adaptable enough to measure, and respond to, subtle physiological cues during intraluminal procedures. We present a 1.47 mm diameter modular soft robotic catheter that integrates sensing, actuation, and therapy while retaining the compliance needed for safe endoluminal navigation. Validated across multiple in vivo settings, we emphasize its utility in endoscopic retrograde cholangiopancreatography (ERCP), a highly technical procedure and a key access route to the pancreas, an organ that is fragile, difficult to instrument, and central to diseases such as pancreatic cancer. Our architecture supports up to four independently controlled functional units, allowing customizable combinations of anchoring, manipulation, sensing, and targeted drug delivery. In a live porcine model, we demonstrate semi-autonomous deployment into the pancreatic duct and 7.5 cm of endoscopic navigation within it, a region currently inaccessible with standard catheters. A closed-loop autonomous/shared-control system that combines a learned model, magnetic actuation, onboard shape sensing, and visual marker tracking further improves cannulation accuracy. Together, these results establish a scalable platform for multifunctional soft robotic catheters and a new paradigm for complex endoluminal interventions, with potential to reduce radiation exposure, shorten training, and accelerate clinical translation of soft robotic technologies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越顺应性：模块化软体机器人导管实现精准治疗</div>
<div class="mono" style="margin-top:8px">软体机器人器械比刚性工具能更安全地在脆弱曲折的解剖结构中导航，但临床应用受限于尖端功能化不足和组织界面的实时反馈缺失。现有传感与治疗模块鲜有兼具紧凑性、鲁棒性和适应性，难以在腔内手术中测量并响应细微生理信号。我们提出一种直径1.47毫米的模块化软体机器人导管，集成传感、驱动与治疗功能，同时保持安全腔内导航所需的顺应性。通过多场景体内验证，重点展示其在经内镜逆行性胰胆管造影术（ERCP）中的应用价值——该技术是胰腺（一个脆弱难操作且与胰腺癌等疾病密切相关的器官）的关键介入路径。该架构支持最多四个独立控制的功能单元，可实现锚定、操控、传感与靶向给药的自定义组合。在活体猪模型中，我们演示了半自主部署至胰管并在管内进行7.5厘米的内镜导航（该区域目前标准导管无法到达）。结合学习模型、磁力驱动、内置形状传感与视觉标记跟踪的闭环自主/共享控制系统进一步提升了插管精度。这些成果共同构建了多功能软体机器人导管的可扩展平台，为复杂腔内介入创立了新范式，有望减少辐射暴露、缩短培训周期并加速软体机器人技术的临床转化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for safer navigation and enhanced functionality in delicate anatomical procedures, this research introduces a modular soft robotic catheter with a 1.47 mm diameter that integrates sensing, actuation, and therapy. The method employs an architecture supporting up to four independently controlled units for anchoring, manipulation, sensing, and targeted drug delivery, validated through in vivo experiments including endoscopic retrograde cholangiopancreatography (ERCP). Key experimental results demonstrate semi-autonomous deployment and 7.5 cm of navigation within the pancreatic duct of a live porcine model, a region inaccessible to standard catheters, with a closed-loop control system combining learned models, magnetic actuation, and onboard sensing improving cannulation accuracy.</div>
<div class="mono" style="margin-top:8px">软体机器人导管可在脆弱解剖结构中更安全地导航，但其临床应用因末端功能化和实时反馈不足而受限。为此，研究团队开发了一种直径1.47毫米的模块化软体机器人导管，集成了传感、驱动和治疗功能，同时保持了腔内导航所需的柔顺性。在活体猪模型中，该系统实现了在胰管内的半自主部署和7.5厘米的导航（该区域标准导管无法进入），且结合学习模型、磁驱动和内置形状传感的闭环控制系统提高了插管准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Warm-Starting Collision-Free Model Predictive Control With Object-Centric Diffusion</div>
<div class="meta-line">Authors: Arthur Haffemayer, Alexandre Chapin, Armand Jordana, Krzysztof Wojciechowski, Florent Lamiraux, Nicolas Mansard, Vladimir Petrik</div>
<div class="meta-line">First: 2026-01-06T10:02:54+00:00 · Latest: 2026-01-21T09:53:52+00:00</div>
<div class="meta-line">Comments: An open-source implementation is provided https://ahaffemayer.github.io/diffusion_warmstart_slot/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02873v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02873v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ahaffemayer.github.io/diffusion_warmstart_slot/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Acting in cluttered environments requires predicting and avoiding collisions while still achieving precise control. Conventional optimization-based controllers can enforce physical constraints, but they struggle to produce feasible solutions quickly when many obstacles are present. Diffusion models can generate diverse trajectories around obstacles, yet prior approaches lacked a general and efficient way to condition them on scene structure. In this paper, we show that combining diffusion-based warm-starting conditioned with a latent object-centric representation of the scene and with a collision-aware model predictive controller (MPC) yields reliable and efficient motion generation under strict time limits. Our approach conditions a diffusion transformer on the system state, task, and surroundings, using an object-centric slot attention mechanism to provide a compact obstacle representation suitable for control. The sampled trajectories are refined by an optimal control problem that enforces rigid-body dynamics and signed-distance collision constraints, producing feasible motions in real time. On benchmark tasks, this hybrid method achieved markedly higher success rates and lower latency than sampling-based planners or either component alone. Real-robot experiments with a torque-controlled Panda confirm reliable and safe execution with MPC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物体中心扩散的碰撞规避模型预测控制热启动方法</div>
<div class="mono" style="margin-top:8px">在复杂环境中执行任务需要预测并规避碰撞，同时保持精确控制。传统基于优化的控制器虽能强制执行物理约束，但在多障碍物场景中难以快速生成可行解。扩散模型可生成绕障的多样化轨迹，但现有方法缺乏基于场景结构的通用高效条件化机制。本文提出：结合基于扩散的热启动（通过场景的潜在物体中心表示进行条件化）与碰撞感知模型预测控制器（MPC），可在严格时限内实现可靠高效的运动生成。该方法通过物体中心槽注意力机制提供适用于控制的紧凑障碍物表示，将系统状态、任务及环境信息作为扩散变换器的条件输入。采样轨迹通过最优控制问题进行精细化处理，强制执行刚体动力学与符号距离碰撞约束，实时生成可行运动。在基准任务中，该混合方法相比基于采样的规划器或单一组件，显著提高了成功率并降低了延迟。基于力矩控制熊猫机器人的真实实验验证了MPC框架下可靠安全的执行性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating feasible collision-free motions in cluttered environments under strict time constraints, where conventional optimization-based controllers are slow and diffusion models lack efficient scene conditioning. The proposed method combines a diffusion transformer, conditioned on system state, task, and an object-centric scene representation from a slot attention mechanism, with a collision-aware model predictive controller (MPC). The diffusion model provides diverse warm-start trajectories, which the MPC then refines by enforcing rigid-body dynamics and signed-distance collision constraints. Experimental results on benchmark tasks show this hybrid approach achieves significantly higher success rates and lower latency compared to sampling-based planners or using either component alone, with real-robot experiments on a torque-controlled Panda confirming reliable and safe execution.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决在严格时间限制下，于杂乱环境中生成可行无碰撞运动的挑战，传统基于优化的控制器在此类场景中往往难以快速找到解。所提出的方法结合了一个扩散变换器（通过槽注意力机制提供的以物体为中心的紧凑场景表示进行条件化，以生成多样化的热启动轨迹）和一个强制执行刚体动力学与符号距离碰撞约束的碰撞感知模型预测控制器来优化轨迹。在基准任务上的实验结果表明，这种混合方法相比基于采样的规划器或单独使用任一组件，实现了显著更高的成功率和更低的延迟，在扭矩控制的Panda机械臂上的真实机器人实验也证实了其可靠且安全的实时执行能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
