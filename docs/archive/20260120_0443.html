<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-20 04:43</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260120_0443</div>
    <div class="row"><div class="card">
<div class="title">ShapeR: Robust Conditional 3D Shape Generation from Casual Captures</div>
<div class="meta-line">Authors: Yawar Siddiqui, Duncan Frost, Samir Aroudj, Armen Avetisyan, Henry Howard-Jenkins, Daniel DeTone, Pierre Moulon, Qirui Wu, Zhengqin Li, Julian Straub, Richard Newcombe, Jakob Engel</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-16T18:51:24+00:00 · Latest: 2026-01-16T18:51:24+00:00</div>
<div class="meta-line">Comments: Project Page: http://facebookresearch.github.io/ShapeR Video: https://www.youtube.com/watch?v=EbY30KAA55I</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11514v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11514v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="http://facebookresearch.github.io/ShapeR">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShapeR：基于随意拍摄视频的鲁棒条件式三维形状生成</div>
<div class="mono" style="margin-top:8px">三维形状生成领域近期虽取得显著进展，但现有方法大多依赖干净、无遮挡且分割良好的输入数据，这些条件在实际场景中难以满足。本文提出ShapeR——一种从随意拍摄序列生成条件式三维物体形状的新方法。给定图像序列，我们利用现成的视觉-惯性SLAM、三维检测算法和视觉-语言模型，为每个物体提取稀疏SLAM点集、多视角位姿图像及机器生成描述。通过训练整流流变换器有效融合这些模态信息，最终生成高保真度度量三维形状。为应对随意拍摄数据的挑战，我们采用包括实时组合增强、跨物体与场景级数据集的课程训练方案、背景干扰处理策略在内的多项鲁棒性技术。此外，我们构建了包含7个真实场景中178个带几何标注野外物体的新评估基准。实验表明，ShapeR在此挑战性设定下显著优于现有方法，其倒角距离指标较当前最优技术提升2.7倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for 3D shape generation from real-world, casually captured data which is often occluded and poorly segmented, this paper introduces ShapeR. The method first extracts sparse SLAM points, posed multi-view images, and machine-generated captions for each object using off-the-shelf algorithms, then employs a rectified flow transformer conditioned on these modalities to generate metric 3D shapes. Key experimental results, evaluated on a new benchmark of 178 in-the-wild objects, show that ShapeR outperforms existing approaches, achieving a 2.7x improvement in Chamfer distance over the state of the art.</div>
<div class="mono" style="margin-top:8px">现有三维形状生成方法通常依赖干净、无遮挡的输入，难以处理现实世界中随意捕获的数据。为此，本文提出了ShapeR方法，它从随意捕获的图像序列中，利用现成的视觉-惯性SLAM、三维检测算法和视觉-语言模型，为每个物体提取稀疏SLAM点、多视角图像和机器生成的描述，并训练一个基于整流流的变换器来融合这些模态进行生成，同时采用组合增强和课程训练等策略提升鲁棒性。在一个包含7个真实场景、178个物体的新评估基准上的实验表明，ShapeR显著优于现有方法，在倒角距离指标上比当前最优方法提升了2.7倍。</div>
</details>
</div>
<div class="card">
<div class="title">MHA2MLA-VLM: Enabling DeepSeek&#x27;s Economical Multi-Head Latent Attention across Vision-Language Models</div>
<div class="meta-line">Authors: Xiaoran Fan, Zhichao Sun, Tao Ji, Lixing Shen, Tao Gui</div>
<div class="meta-line">First: 2026-01-16T17:45:34+00:00 · Latest: 2026-01-16T17:45:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11464v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11464v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As vision-language models (VLMs) tackle increasingly complex and multimodal tasks, the rapid growth of Key-Value (KV) cache imposes significant memory and computational bottlenecks during inference. While Multi-Head Latent Attention (MLA) offers an effective means to compress the KV cache and accelerate inference, adapting existing VLMs to the MLA architecture without costly pretraining remains largely unexplored. In this work, we present MHA2MLA-VLM, a parameter-efficient and multimodal-aware framework for converting off-the-shelf VLMs to MLA. Our approach features two core techniques: (1) a modality-adaptive partial-RoPE strategy that supports both traditional and multimodal settings by selectively masking nonessential dimensions, and (2) a modality-decoupled low-rank approximation method that independently compresses the visual and textual KV spaces. Furthermore, we introduce parameter-efficient fine-tuning to minimize adaptation cost and demonstrate that minimizing output activation error, rather than parameter distance, substantially reduces performance loss. Extensive experiments on three representative VLMs show that MHA2MLA-VLM restores original model performance with minimal supervised data, significantly reduces KV cache footprint, and integrates seamlessly with KV quantization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MHA2MLA-VLM：实现DeepSeek经济型多头部潜在注意力在视觉语言模型中的跨模态应用</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型处理日益复杂的多模态任务，推理过程中键值缓存的高速增长导致显著的内存与计算瓶颈。虽然多头部潜在注意力为压缩键值缓存和加速推理提供了有效方案，但如何在不进行昂贵预训练的前提下将现有视觉语言模型适配至该架构仍属未充分探索的领域。本研究提出MHA2MLA-VLM——一种参数高效且具备多模态感知能力的框架，可将现成视觉语言模型转换为多头部潜在注意力架构。该框架包含两项核心技术：(1) 模态自适应部分旋转位置编码策略，通过选择性屏蔽非必要维度同时支持传统与多模态场景；(2) 模态解耦的低秩近似方法，可独立压缩视觉与文本键值空间。此外，我们引入参数高效微调以最小化适配成本，并证明最小化输出激活误差（而非参数距离）能显著降低性能损失。在三个代表性视觉语言模型上的大量实验表明，MHA2MLA-VLM能以极少量监督数据恢复原始模型性能，显著降低键值缓存占用，并与键值量化技术实现无缝集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the memory and computational bottlenecks caused by the growing Key-Value (KV) cache in vision-language models (VLMs) during inference. To enable efficient adaptation of existing VLMs to a Multi-Head Latent Attention (MLA) architecture without full pretraining, the method introduces MHA2MLA-VLM, a framework featuring a modality-adaptive partial-RoPE strategy for flexible attention and a modality-decoupled low-rank approximation to compress visual and textual KV spaces independently, combined with parameter-efficient fine-tuning focused on minimizing output activation error. Experimental results on three VLMs demonstrate that the approach effectively restores original model performance with limited data, substantially reduces KV cache size, and works compatibly with KV quantization techniques.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型推理过程中，因键值缓存增长导致的内存与计算瓶颈。作者提出了MHA2MLA-VLM，一个参数高效的框架，可将现有视觉语言模型转换为多头潜在注意力架构而无需完全预训练；其核心是模态自适应的部分旋转位置编码策略，以及模态解耦的低秩近似方法，分别压缩视觉和文本的键值空间。在三个代表性模型上的实验表明，该方法仅需少量监督数据即可恢复原模型性能，显著减少键值缓存占用，并能与键值量化无缝集成。</div>
</details>
</div>
<div class="card">
<div class="title">Causal-SAM-LLM: Large Language Models as Causal Reasoners for Robust Medical Segmentation</div>
<div class="meta-line">Authors: Tao Tang, Shijie Xu, Jionglong Su, Zhixiang Lu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-07-04T13:52:16+00:00 · Latest: 2026-01-16T16:16:45+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.03585v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.03585v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The clinical utility of deep learning models for medical image segmentation is severely constrained by their inability to generalize to unseen domains. This failure is often rooted in the models learning spurious correlations between anatomical content and domain-specific imaging styles. To overcome this fundamental challenge, we introduce Causal-SAM-LLM, a novel framework that elevates Large Language Models (LLMs) to the role of causal reasoners. Our framework, built upon a frozen Segment Anything Model (SAM) encoder, incorporates two synergistic innovations. First, Linguistic Adversarial Disentanglement (LAD) employs a Vision-Language Model to generate rich, textual descriptions of confounding image styles. By training the segmentation model&#x27;s features to be contrastively dissimilar to these style descriptions, it learns a representation robustly purged of non-causal information. Second, Test-Time Causal Intervention (TCI) provides an interactive mechanism where an LLM interprets a clinician&#x27;s natural language command to modulate the segmentation decoder&#x27;s features in real-time, enabling targeted error correction. We conduct an extensive empirical evaluation on a composite benchmark from four public datasets (BTCV, CHAOS, AMOS, BraTS), assessing generalization under cross-scanner, cross-modality, and cross-anatomy settings. Causal-SAM-LLM establishes a new state of the art in out-of-distribution (OOD) robustness, improving the average Dice score by up to 6.2 points and reducing the Hausdorff Distance by 15.8 mm over the strongest baseline, all while using less than 9% of the full model&#x27;s trainable parameters. Our work charts a new course for building robust, efficient, and interactively controllable medical AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Causal-SAM-LLM：将大语言模型作为因果推理器以实现鲁棒的医学图像分割</div>
<div class="mono" style="margin-top:8px">深度学习模型在医学图像分割中的临床应用，因其难以泛化至未见过的领域而受到严重制约。这一失败通常源于模型学习了解剖内容与领域特异性成像风格之间的虚假相关性。为克服这一根本性挑战，我们提出了Causal-SAM-LLM，这是一个将大语言模型提升为因果推理器角色的创新框架。该框架基于冻结的Segment Anything Model编码器构建，融合了两项协同创新技术：首先，语言对抗解耦利用视觉-语言模型生成丰富的、描述混淆图像风格的文本信息，通过训练分割模型的特征使其与这些风格描述形成对比性差异，从而学习到一种能稳健剔除非因果信息的表征。其次，测试时因果干预提供了一种交互机制，即大语言模型解析临床医生的自然语言指令，实时调整分割解码器的特征，实现针对性的错误纠正。我们在一个由四个公开数据集（BTCV、CHAOS、AMOS、BraTS）组成的复合基准上进行了广泛的实证评估，测试了跨扫描仪、跨模态和跨解剖结构的泛化能力。Causal-SAM-LLM在分布外鲁棒性方面确立了新的技术标杆，将平均Dice分数最高提升了6.2个百分点，并将豪斯多夫距离降低了15.8毫米，同时仅使用了完整模型可训练参数的不到9%。我们的工作为构建鲁棒、高效且可交互控制的医学人工智能系统开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The clinical application of deep learning for medical image segmentation is limited by poor generalization to unseen domains, often due to models learning spurious correlations between anatomy and imaging style. To address this, the authors propose Causal-SAM-LLM, a framework that uses a frozen Segment Anything Model encoder and positions Large Language Models as causal reasoners. The method introduces Linguistic Adversarial Disentanglement to purge style information via text descriptions and Test-Time Causal Intervention, where an LLM interprets clinician commands to correct errors in real-time. Evaluated on a composite benchmark from four public datasets under cross-scanner, modality, and anatomy settings, the framework sets a new state of the art in out-of-distribution robustness, improving the average Dice score by up to 6.2 points and reducing the Hausdorff Distance by 15.8 mm over the strongest baseline while using less than 9% of trainable parameters.</div>
<div class="mono" style="margin-top:8px">深度学习在医学图像分割中的临床应用因模型对未见域的泛化能力差而受限，这通常源于模型学习了解剖内容与域特异性成像风格之间的虚假关联。为解决这一根本挑战，研究者提出了Causal-SAM-LLM，该框架将大语言模型提升为因果推理者，并基于冻结的Segment Anything Model编码器构建。该方法包含两个协同创新：一是语言对抗解缠，利用视觉语言模型生成丰富的文本式风格描述，并通过训练分割特征与这些描述对比相异，从而学习到稳健去除非因果信息的表示；二是测试时因果干预，提供一种交互机制，让大语言模型解读临床医生的自然语言指令，实时调制分割解码器的特征以实现针对性错误纠正。在由四个公共数据集组成的复合基准上进行广泛评估，涵盖跨扫描仪、跨模态和跨解剖设置，该框架在分布外鲁棒性上达到了新的最先进水平，相比最强基线，平均Dice分数提升高达6.2分，Hausdorff距离减少15.8毫米，同时仅使用不到9%的可训练参数。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Vision Language Models with Logic Reasoning for Situational Awareness</div>
<div class="meta-line">Authors: Pavana Pradeep, Krishna Kant, Suya Yu</div>
<div class="meta-line">First: 2026-01-16T14:16:38+00:00 · Latest: 2026-01-16T14:16:38+00:00</div>
<div class="meta-line">Comments: Accepted for publication in IEEE Transactions on AI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11322v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11322v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) offer the ability to generate high-level, interpretable descriptions of complex activities from images and videos, making them valuable for situational awareness (SA) applications. In such settings, the focus is on identifying infrequent but significant events with high reliability and accuracy, while also extracting fine-grained details and assessing recognition quality. In this paper, we propose an approach that integrates VLMs with traditional computer vision methods through explicit logic reasoning to enhance SA in three key ways: (a) extracting fine-grained event details, (b) employing an intelligent fine-tuning (FT) strategy that achieves substantially higher accuracy than uninformed selection, and (c) generating justifications for VLM outputs during inference. We demonstrate that our intelligent FT mechanism improves the accuracy and provides a valuable means, during inferencing, to either confirm the validity of the VLM output or indicate why it may be questionable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过逻辑推理增强视觉语言模型的情境感知能力</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）能够从图像和视频中生成复杂活动的高层次、可解释描述，使其在情境感知（SA）应用中具有重要价值。此类应用需以高可靠性和准确性识别偶发但关键的事件，同时提取细粒度细节并评估识别质量。本文提出一种通过显式逻辑推理将VLMs与传统计算机视觉方法相结合的技术路径，从三个维度提升SA能力：（a）提取细粒度事件细节；（b）采用智能微调策略，其准确率显著高于无指导选择；（c）在推理过程中为VLM输出生成解释依据。实验表明，智能微调机制不仅提升了准确率，还能在推理时有效验证VLM输出的可靠性或提示其潜在问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) are promising for situational awareness but require enhanced reliability and detail for identifying rare, significant events. The proposed method integrates VLMs with traditional computer vision via explicit logic reasoning to extract fine-grained details, employs an intelligent fine-tuning strategy for higher accuracy, and generates justifications for model outputs. Experiments show the intelligent fine-tuning mechanism substantially improves accuracy and provides a means to validate or question the VLM&#x27;s inferences during operation.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在态势感知应用中前景广阔，但需提高对偶发关键事件检测的可靠性和细粒度细节提取能力。本研究提出一种通过显式逻辑推理将视觉语言模型与传统计算机视觉方法相结合的方法，旨在从三方面进行增强：提取细粒度事件细节、采用比随机选择准确率显著更高的智能微调策略，以及在推理过程中为模型输出生成解释。实验结果表明，该智能微调机制有效提升了模型准确率，并为验证或质疑视觉语言模型的预测提供了依据。</div>
</details>
</div>
<div class="card">
<div class="title">X-Distill: Cross-Architecture Vision Distillation for Visuomotor Learning</div>
<div class="meta-line">Authors: Maanping Shao, Feihong Zhang, Gu Zhang, Baiye Cheng, Zhengrong Xue, Huazhe Xu</div>
<div class="meta-line">First: 2026-01-16T13:15:55+00:00 · Latest: 2026-01-16T13:15:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11269v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11269v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visuomotor policies often leverage large pre-trained Vision Transformers (ViTs) for their powerful generalization capabilities. However, their significant data requirements present a major challenge in the data-scarce context of most robotic learning settings, where compact CNNs with strong inductive biases can be more easily optimized. To address this trade-off, we introduce X-Distill, a simple yet highly effective method that synergizes the strengths of both architectures. Our approach involves an offline, cross-architecture knowledge distillation, transferring the rich visual representations of a large, frozen DINOv2 teacher to a compact ResNet-18 student on the general-purpose ImageNet dataset. This distilled encoder, now endowed with powerful visual priors, is then jointly fine-tuned with a diffusion policy head on the target manipulation tasks. Extensive experiments on $34$ simulated benchmarks and $5$ challenging real-world tasks demonstrate that our method consistently outperforms policies equipped with from-scratch ResNet or fine-tuned DINOv2 encoders. Notably, X-Distill also surpasses 3D encoders that utilize privileged point cloud observations or much larger Vision-Language Models. Our work highlights the efficacy of a simple, well-founded distillation strategy for achieving state-of-the-art performance in data-efficient robotic manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>X-Distill：面向视觉运动学习的跨架构视觉蒸馏</div>
<div class="mono" style="margin-top:8px">视觉运动策略常利用预训练的大型视觉变换器（ViTs）以获取强大的泛化能力。然而，在大多数数据稀缺的机器人学习场景中，其庞大的数据需求构成主要挑战，而具有强归纳偏置的紧凑卷积神经网络（CNNs）则更易优化。为平衡此矛盾，我们提出X-Distill——一种简洁高效的方法，能协同融合两种架构的优势。该方法通过离线跨架构知识蒸馏，在通用ImageNet数据集上将大型冻结DINOv2教师模型的丰富视觉表征迁移至紧凑的ResNet-18学生模型。经蒸馏的编码器获得强大视觉先验后，与扩散策略头在目标操作任务上联合微调。在34个模拟基准和5项复杂现实任务上的大量实验表明，本方法持续优于采用从头训练ResNet或微调DINOv2编码器的策略。值得注意的是，X-Distill亦超越依赖特权点云观测或更庞大视觉语言模型的3D编码器。本研究凸显了基于严谨蒸馏的简洁策略在数据高效机器人操作中实现顶尖性能的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of deploying large Vision Transformers (ViTs) for visuomotor policies in data-scarce robotic settings, where compact CNNs are easier to optimize but lack generalization. The proposed X-Distill method performs offline cross-architecture knowledge distillation, transferring visual representations from a frozen DINOv2 ViT teacher to a compact ResNet-18 student on ImageNet, then fine-tuning the distilled encoder with a diffusion policy head on target tasks. Experiments across 34 simulated and 5 real-world manipulation tasks show that X-Distill consistently outperforms policies using from-scratch ResNet or fine-tuned DINOv2 encoders, and even surpasses 3D encoders with privileged point cloud data or larger vision-language models, demonstrating state-of-the-art data-efficient performance.</div>
<div class="mono" style="margin-top:8px">本研究针对数据稀缺的机器人场景中部署大型视觉Transformer（ViT）用于视觉运动策略的挑战，其中紧凑的CNN更易优化但泛化能力不足。方法X-Distill采用离线跨架构知识蒸馏，在ImageNet上将冻结的DINOv2 ViT教师的视觉表征迁移到ResNet-18学生模型，随后与扩散策略头在目标任务上联合微调。在34个模拟和5个真实世界操作任务上的实验表明，X-Distill consistently outperforms policies using from-scratch ResNet or fine-tuned DINOv2 encoders, and even surpasses 3D encoders with privileged point cloud data or larger vision-language models, demonstrating state-of-the-art data-efficient performance。</div>
</details>
</div>
<div class="card">
<div class="title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</div>
<div class="meta-line">Authors: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</div>
<div class="meta-line">First: 2026-01-05T18:56:34+00:00 · Latest: 2026-01-16T13:04:59+00:00</div>
<div class="meta-line">Comments: Project page: https://sotamak1r.github.io/VINO-web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02358v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02358v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sotamak1r.github.io/VINO-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VINO：基于交错全模态上下文的统一视觉生成器</div>
<div class="mono" style="margin-top:8px">本文提出VINO——一个在单一框架内实现图像与视频生成及编辑的统一视觉生成器。区别于依赖任务专用模型或独立模态模块的传统方案，VINO采用共享的扩散主干网络，可同时接受文本、图像和视频作为条件输入，从而在单一模型中实现广泛的视觉创作与编辑任务。具体而言，VINO将视觉语言模型与多模态扩散Transformer耦合，将多模态输入编码为交错的条件标记，进而引导扩散过程。该设计支持多参考基准定位、长指令序列跟随、静态与动态内容的连贯身份保持，同时避免了模态专用的架构组件。为训练此统一系统，我们提出了多阶段训练流程，逐步将基础视频生成模型扩展为支持图像/视频输入输出的统一多任务生成器。在多样化生成与编辑基准测试中，VINO展现出卓越的视觉质量、精准的指令跟随能力、增强的参考与属性保持特性，以及更可控的多身份编辑效果。本研究为可扩展的统一视觉生成提供了可行路径，并揭示了交错上下文计算作为通用视觉创作基础技术的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to unify diverse visual generation and editing tasks, which are typically handled by separate, task-specific models, into a single, efficient framework. The proposed method, VINO, integrates a vision-language model with a Multimodal Diffusion Transformer (MMDiT), encoding text, image, and video inputs as interleaved tokens to condition a shared diffusion backbone for generating both images and videos. Experimental results show that VINO achieves strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits across various benchmarks, demonstrating the effectiveness of its unified, interleaved conditioning approach.</div>
<div class="mono" style="margin-top:8px">本研究旨在将通常由独立任务特定模型处理的多样化视觉生成与编辑任务，统一到一个高效的单框架中。所提出的VINO方法将视觉语言模型与多模态扩散变换器（MMDiT）相结合，将文本、图像和视频输入编码为交错的条件令牌，用以引导一个共享的扩散主干网络，实现图像和视频的生成与编辑。实验结果表明，VINO在各种基准测试中展现出强大的视觉质量、对指令的忠实遵循、改进的参考属性与身份保持能力，以及更可控的多身份编辑效果。</div>
</details>
</div>
<div class="card">
<div class="title">Language-Agnostic Visual Embeddings for Cross-Script Handwriting Retrieval</div>
<div class="meta-line">Authors: Fangke Chen, Tianhao Dong, Sirry Chen, Guobin Zhang, Yishu Zhang, Yining Chen</div>
<div class="meta-line">First: 2026-01-16T12:55:41+00:00 · Latest: 2026-01-16T12:55:41+00:00</div>
<div class="meta-line">Comments: 9 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11248v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11248v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Handwritten word retrieval is vital for digital archives but remains challenging due to large handwriting variability and cross-lingual semantic gaps. While large vision-language models offer potential solutions, their prohibitive computational costs hinder practical edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings. By jointly optimizing instance-level alignment and class-level semantic consistency, our approach anchors visual embeddings to language-agnostic semantic prototypes, enforcing invariance across scripts and writing styles. Experiments show that our method outperforms 28 baselines and achieves state-of-the-art accuracy on within-language retrieval benchmarks. We further conduct explicit cross-lingual retrieval, where the query language differs from the target language, to validate the effectiveness of the learned cross-lingual representations. Achieving strong performance with only a fraction of the parameters required by existing models, our framework enables accurate and resource-efficient cross-script handwriting retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言无关的视觉嵌入用于跨文字手写检索</div>
<div class="mono" style="margin-top:8px">手写词检索对数字档案至关重要，但由于手写体差异大和跨语言语义鸿沟，仍具挑战性。尽管大型视觉语言模型提供了潜在解决方案，但其高昂计算成本阻碍了实际边缘部署。为此，我们提出一种轻量级非对称双编码器框架，学习统一、风格不变的视觉嵌入。通过联合优化实例级对齐和类级语义一致性，我们的方法将视觉嵌入锚定到语言无关的语义原型，强制实现跨文字和书写风格的不变性。实验表明，该方法在28个基线模型中表现最优，并在同语言检索基准上达到最先进准确率。我们进一步进行显式跨语言检索（查询语言与目标语言不同），验证所学跨语言表征的有效性。仅需现有模型参数量的极小部分即可实现强劲性能，本框架实现了精准且资源高效的跨文字手写检索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Handwritten word retrieval faces challenges from handwriting variability and cross-lingual semantic gaps, while existing vision-language models are computationally expensive for edge deployment. To address this, we propose a lightweight asymmetric dual-encoder framework that learns unified, style-invariant visual embeddings by jointly optimizing instance-level alignment and class-level semantic consistency, anchoring embeddings to language-agnostic semantic prototypes. Experiments demonstrate that our method outperforms 28 baselines, achieves state-of-the-art accuracy on within-language retrieval benchmarks, and shows strong performance in explicit cross-lingual retrieval where query and target languages differ, all while using only a fraction of the parameters of existing models.</div>
<div class="mono" style="margin-top:8px">本研究针对数字档案中手写词检索的挑战，其中手写体的巨大变异性和跨语言语义鸿沟使得准确搜索变得困难。为实现边缘设备的实际部署，作者提出了一种轻量级非对称双编码器框架，通过联合优化实例级对齐和类级语义一致性，学习统一且风格不变的视觉嵌入，从而将嵌入锚定到语言无关的语义原型。实验结果表明，该方法在28个基线模型上表现优异，在语言内检索基准上达到了最先进的准确率，并在显式的跨语言检索场景中验证了其有效性，同时仅需现有模型的一小部分参数量。</div>
</details>
</div>
<div class="card">
<div class="title">Image-Text Knowledge Modeling for Unsupervised Multi-Scenario Person Re-Identification</div>
<div class="meta-line">Authors: Zhiqi Pang, Lingling Zhao, Yang Liu, Chunyu Wang, Gaurav Sharma</div>
<div class="meta-line">First: 2026-01-16T12:45:01+00:00 · Latest: 2026-01-16T12:45:01+00:00</div>
<div class="meta-line">Comments: 12 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.11243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose unsupervised multi-scenario (UMS) person re-identification (ReID) as a new task that expands ReID across diverse scenarios (cross-resolution, clothing change, etc.) within a single coherent framework. To tackle UMS-ReID, we introduce image-text knowledge modeling (ITKM) -- a three-stage framework that effectively exploits the representational power of vision-language models. We start with a pre-trained CLIP model with an image encoder and a text encoder. In Stage I, we introduce a scenario embedding in the image encoder and fine-tune the encoder to adaptively leverage knowledge from multiple scenarios. In Stage II, we optimize a set of learned text embeddings to associate with pseudo-labels from Stage I and introduce a multi-scenario separation loss to increase the divergence between inter-scenario text representations. In Stage III, we first introduce cluster-level and instance-level heterogeneous matching modules to obtain reliable heterogeneous positive pairs (e.g., a visible image and an infrared image of the same person) within each scenario. Next, we propose a dynamic text representation update strategy to maintain consistency between text and image supervision signals. Experimental results across multiple scenarios demonstrate the superiority and generalizability of ITKM; it not only outperforms existing scenario-specific methods but also enhances overall performance by integrating knowledge from multiple scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向无监督多场景行人重识别的图文知识建模</div>
<div class="mono" style="margin-top:8px">本文提出无监督多场景行人重识别这一新任务，旨在单一框架内扩展跨分辨率、换装等多种场景的ReID能力。为此，我们提出图文知识建模——一种三阶段框架，有效利用视觉语言模型的表征能力。该框架以预训练的CLIP模型为基础，包含图像编码器和文本编码器。第一阶段：在图像编码器中引入场景嵌入，通过微调使编码器自适应融合多场景知识。第二阶段：优化一组可学习文本嵌入以关联第一阶段生成的伪标签，并引入多场景分离损失以增强场景间文本表征的差异性。第三阶段：首先通过簇级和实例级异构匹配模块获取各场景内可靠的异构正样本对（如同行人的可见光与红外图像）；随后提出动态文本表征更新策略，保持文本与图像监督信号的一致性。多场景实验结果表明，ITKM不仅优于现有场景专用方法，还能通过整合多场景知识提升整体性能，具有优越的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of unsupervised multi-scenario person re-identification (UMS-ReID), which aims to recognize individuals across diverse conditions like varying resolutions and clothing changes without labeled data. The method, Image-Text Knowledge Modeling (ITKM), is a three-stage framework built upon a pre-trained CLIP model: it first fine-tunes the image encoder with scenario embeddings, then optimizes text embeddings with a separation loss to distinguish scenarios, and finally employs heterogeneous matching and dynamic text updates to align image and text representations. Experiments show that ITKM surpasses existing scenario-specific techniques and improves overall performance by effectively integrating knowledge from multiple scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对无监督多场景行人重识别（UMS-ReID）的挑战，旨在无需标注数据的情况下，跨分辨率变化、衣物更换等多种条件识别同一行人。所提出的方法——图像-文本知识建模（ITKM）——是一个基于预训练CLIP模型的三阶段框架：首先通过场景嵌入微调图像编码器，然后利用分离损失优化文本嵌入以区分不同场景，最后采用异构匹配和动态文本更新策略来对齐图像与文本表示。实验结果表明，ITKM不仅优于现有的特定场景方法，还能通过有效整合多场景知识提升整体性能。</div>
</details>
</div>
<div class="card">
<div class="title">SceneFoundry: Generating Interactive Infinite 3D Worlds</div>
<div class="meta-line">Authors: ChunTeng Chen, YiChen Hsu, YiWen Liu, WeiFang Sun, TsaiChing Ni, ChunYi Lee, Min Sun, YuanFu Yang</div>
<div class="meta-line">First: 2026-01-09T14:33:10+00:00 · Latest: 2026-01-16T11:20:40+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05810v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05810v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://anc891203.github.io/SceneFoundry-Demo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to automatically generate large-scale, interactive, and physically realistic 3D environments is crucial for advancing robotic learning and embodied intelligence. However, existing generative approaches often fail to capture the functional complexity of real-world interiors, particularly those containing articulated objects with movable parts essential for manipulation and navigation. This paper presents SceneFoundry, a language-guided diffusion framework that generates apartment-scale 3D worlds with functionally articulated furniture and semantically diverse layouts for robotic training. From natural language prompts, an LLM module controls floor layout generation, while diffusion-based posterior sampling efficiently populates the scene with articulated assets from large-scale 3D repositories. To ensure physical usability, SceneFoundry employs differentiable guidance functions to regulate object quantity, prevent articulation collisions, and maintain sufficient walkable space for robotic navigation. Extensive experiments demonstrate that our framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, enabling scalable embodied AI research. project page: https://anc891203.github.io/SceneFoundry-Demo/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SceneFoundry：生成交互式无限3D世界</div>
<div class="mono" style="margin-top:8px">自动生成大规模、交互式且物理真实的3D环境的能力，对于推动机器人学习和具身智能至关重要。然而，现有生成方法往往难以捕捉真实室内环境的功能复杂性，尤其是那些包含对操作和导航至关重要的可动部件的铰接物体。本文提出SceneFoundry，一个语言引导的扩散框架，可为机器人训练生成公寓规模的3D世界，其中包含功能铰接的家具和语义多样的布局。基于自然语言提示，一个LLM模块控制楼层布局生成，而基于扩散的后验采样则高效地从大规模3D资源库中选取铰接资产填充场景。为确保物理可用性，SceneFoundry采用可微引导函数来调控物体数量、防止铰接碰撞，并为机器人导航维持足够的可通行空间。大量实验表明，我们的框架能在多样场景类型和条件下生成结构有效、语义连贯且功能交互的环境，从而支持可扩展的具身AI研究。项目页面：https://anc891203.github.io/SceneFoundry-Demo/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for automatically generating large-scale, interactive, and physically realistic 3D environments to advance robotic learning and embodied intelligence, addressing the limitations of existing methods in capturing functional complexity, especially with articulated objects. The method, SceneFoundry, is a language-guided diffusion framework that uses an LLM module to control floor layout generation from natural language prompts and employs diffusion-based posterior sampling to populate scenes with articulated assets from 3D repositories, enhanced by differentiable guidance functions to ensure physical usability by regulating object quantity, preventing articulation collisions, and maintaining walkable space. Experimental results demonstrate that the framework generates structurally valid, semantically coherent, and functionally interactive environments across diverse scene types and conditions, facilitating scalable embodied AI research.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决机器人学习中对自动生成大规模交互式3D环境的需求，因为现有方法通常无法捕捉具有可动部件功能结构的真实室内场景复杂性。提出的SceneFoundry框架采用语言引导的扩散方法：大型语言模型根据自然语言提示控制平面布局生成，而基于扩散的后验采样从大规模3D资源库中填充场景可动资产，并通过可微引导函数确保物理可用性，包括调控物体数量、防止关节碰撞以及保持机器人导航所需的可行走空间。实验结果表明，该框架能够生成结构有效、语义连贯且功能交互的环境，适用于多种场景类型和条件，为具身智能研究提供了可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Attention Debiasing for Token Pruning in Vision Language Models</div>
<div class="meta-line">Authors: Kai Zhao, Wubang Yuan, Yuchen Lin, Liting Ruan, Xiaofeng Lu, Deng-Ping Fan, Ming-Ming Cheng, Dan Zeng</div>
<div class="meta-line">First: 2025-08-25T08:56:32+00:00 · Latest: 2026-01-16T09:19:57+00:00</div>
<div class="meta-line">Comments: https://github.com/intcomp/attention-bias</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.17807v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.17807v2">PDF</a> · <a href="https://github.com/intcomp/attention-bias">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) typically encode substantially more visual tokens than text tokens, resulting in significant token redundancy. Pruning uninformative visual tokens is therefore crucial for improving computational efficiency, and language-to-vision attention has become a widely used importance criterion for this purpose. However, we find that attention in VLMs is systematically biased. It disproportionately favors tokens appearing later in the sequence, manifesting as over-attention to lower image regions, and assigns inflated scores to semantically empty padding tokens. These behaviors stem from intrinsic recency bias and attention sink effects inherited from large language models (LLMs), and they distort attention-based pruning by preserving irrelevant visual content. To derive a pruning criterion better aligned with semantic relevance, we introduce two lightweight yet effective debiasing techniques that restore the reliability of attention. The first compensates for positional distortions by removing recency-induced attention trends, producing a content-aware and position-agnostic importance measure. The second suppresses attention sink effects by eliminating spurious attention on padding tokens. Our method is model-agnostic, pruning-method-agnostic, and task-agnostic, enabling plug-and-play integration with existing VLM pruning models. Despite its simplicity, our approach consistently delivers strong performance gains. We evaluate our method on ten vision-language benchmarks spanning both image-based and video-based tasks, in comparison with seven state-of-the-art visual token pruning methods and across two representative VLM architectures. Our method achieves substantial performance gains, demonstrating strong effectiveness and generalizability. Our code is available at https://github.com/intcomp/attention-bias.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型中注意力去偏的令牌剪枝方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型通常编码的视觉令牌远多于文本令牌，导致显著的令牌冗余。因此，剪枝无信息的视觉令牌对提升计算效率至关重要，而语言到视觉的注意力机制已成为广泛采用的重要性评估标准。然而，我们发现视觉语言模型中的注意力存在系统性偏差：它不成比例地偏向序列中靠后的令牌（表现为对图像下部区域的过度关注），并为语义空白的填充令牌分配虚高分数。这些现象源于大语言模型固有的近因偏差和注意力汇聚效应，会扭曲基于注意力的剪枝过程，保留无关视觉内容。为建立更符合语义相关性的剪枝标准，我们提出两种轻量高效的去偏技术以恢复注意力可靠性：第一项技术通过消除近因诱导的注意力趋势来补偿位置扭曲，生成内容感知且位置无关的重要性度量；第二项技术通过抑制填充令牌的伪注意力来削弱注意力汇聚效应。本方法具有模型无关性、剪枝方法无关性和任务无关性，可与现有视觉语言模型剪枝方案即插即用。尽管设计简洁，该方法持续带来显著性能提升。我们在涵盖图像与视频任务的十项视觉语言基准测试中，对比七种前沿视觉令牌剪枝方法及两种代表性视觉语言模型架构进行评估。实验表明本方法取得实质性性能增益，展现出强有效性和泛化能力。代码已开源：https://github.com/intcomp/attention-bias。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models encode many redundant visual tokens, and attention-based pruning is commonly used to improve efficiency, but attention scores are systematically biased by recency effects and attention sinks, favoring later tokens and padding. To correct this, the authors propose two lightweight debiasing techniques: one removes positional trends to create a content-aware importance measure, and another suppresses attention to padding tokens. Evaluated on ten benchmarks against seven pruning methods and two VLM architectures, the approach consistently achieves substantial performance gains, demonstrating strong effectiveness and generalizability.</div>
<div class="mono" style="margin-top:8px">视觉语言模型编码了大量冗余的视觉标记，通常使用语言到视觉的注意力进行剪枝，但注意力存在系统性偏差，由于大语言模型中的近因偏差和注意力汇聚效应，会过度关注序列后部的标记和填充标记。为此，研究者提出了两种轻量级去偏技术：一是消除近因引起的趋势以生成位置无关的重要性度量，二是抑制对填充标记的虚假注意力，从而获得更符合语义相关性的剪枝准则。该方法在十个视觉语言基准测试中，与七种先进剪枝方法及两种代表性VLM架构进行比较，均实现了显著的性能提升，展现了强大的有效性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Cascading multi-agent anomaly detection in surveillance systems via vision-language models and embedding-based classification</div>
<div class="meta-line">Authors: Tayyab Rehman, Giovanni De Gasperis, Aly Shmahell</div>
<div class="meta-line">First: 2026-01-08T11:31:47+00:00 · Latest: 2026-01-16T08:21:29+00:00</div>
<div class="meta-line">Comments: Author email changed, Acknowlegement changes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06204v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.06204v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intelligent anomaly detection in dynamic visual environments requires reconciling real-time performance with semantic interpretability. Conventional approaches address only fragments of this challenge. Reconstruction-based models capture low-level deviations without contextual reasoning, object detectors provide speed but limited semantics, and large vision-language systems deliver interpretability at prohibitive computational cost. This work introduces a cascading multi-agent framework that unifies these complementary paradigms into a coherent and interpretable architecture. Early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked to interpret semantically ambiguous events. The system employs adaptive escalation thresholds and a publish-subscribe communication backbone, enabling asynchronous coordination and scalable deployment across heterogeneous hardware. Extensive evaluation on large-scale monitoring data demonstrates that the proposed cascade achieves a threefold reduction in latency compared to direct vision-language inference, while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling. The framework advances beyond conventional detection pipelines by combining early-exit efficiency, adaptive multi-agent reasoning, and explainable anomaly attribution, establishing a reproducible and energy-efficient foundation for scalable intelligent visual monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型与嵌入分类的级联多智能体监控系统异常检测</div>
<div class="mono" style="margin-top:8px">动态视觉环境中的智能异常检测需兼顾实时性能与语义可解释性。传统方法仅能应对局部挑战：基于重建的模型捕获低级偏差但缺乏上下文推理，目标检测器速度快但语义有限，大型视觉语言系统虽具可解释性但计算成本过高。本研究提出一种级联多智能体框架，将这些互补范式统一为连贯且可解释的架构。早期模块执行重建门控过滤与目标级评估，高层推理智能体则被选择性调用以解释语义模糊事件。系统采用自适应升级阈值与发布-订阅通信机制，支持异构硬件间的异步协调与可扩展部署。大规模监控数据评估表明，该级联方案相比直接视觉语言推理延迟降低三倍，同时保持高感知保真度（PSNR = 38.3 dB, SSIM = 0.965）与一致的语义标注。该框架融合早期退出效率、自适应多智能体推理与可解释异常归因，突破了传统检测流程，为可扩展智能视觉监控奠定了可复现且高能效的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to address the challenge of achieving both real-time performance and semantic interpretability in intelligent anomaly detection for dynamic visual environments, where conventional methods like reconstruction models, object detectors, or large vision-language models offer only partial solutions. The proposed method is a cascading multi-agent framework that integrates these complementary paradigms: early modules perform reconstruction-gated filtering and object-level assessment, while higher-level reasoning agents are selectively invoked for ambiguous events, using adaptive escalation thresholds and a publish-subscribe communication backbone for asynchronous coordination. Experimental results on large-scale monitoring data show the cascade achieves a threefold latency reduction compared to direct vision-language inference while maintaining high perceptual fidelity (PSNR = 38.3 dB, SSIM = 0.965) and consistent semantic labeling, advancing detection pipelines through early-exit efficiency, adaptive reasoning, and explainable anomaly attribution.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决动态视觉环境中智能异常检测同时实现实时性能与语义可解释性的挑战，传统方法仅提供部分解决方案。所提出的方法是一个级联多智能体框架，它统一了互补范式：早期模块执行基于重建的门控过滤和对象级评估，而高级推理智能体被选择性调用以处理语义模糊事件，所有模块通过自适应升级阈值和发布-订阅通信主干进行协调。在大规模监控数据上的实验评估表明，该框架相比直接的视觉-语言推理实现了三倍的延迟降低，同时保持了高感知保真度（PSNR = 38.3 dB, SSIM = 0.965）和一致的语义标注。</div>
</details>
</div>
<div class="card">
<div class="title">MERGETUNE: Continued fine-tuning of vision-language models</div>
<div class="meta-line">Authors: Wenqing Wang, Da Li, Xiatian Zhu, Josef Kittler</div>
<div class="meta-line">First: 2026-01-15T15:15:53+00:00 · Latest: 2026-01-16T04:31:59+00:00</div>
<div class="meta-line">Comments: 20 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10497v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.10497v2">PDF</a> · <a href="https://github.com/Surrey-UP-Lab/MERGETUNE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning vision-language models (VLMs) such as CLIP often leads to catastrophic forgetting of pretrained knowledge. Prior work primarily aims to mitigate forgetting during adaptation; however, forgetting often remains inevitable during this process. We introduce a novel paradigm, continued fine-tuning (CFT), which seeks to recover pretrained knowledge after a zero-shot model has already been adapted. We propose a simple, model-agnostic CFT strategy (named MERGETUNE) guided by linear mode connectivity (LMC), which can be applied post hoc to existing fine-tuned models without requiring architectural changes. Given a fine-tuned model, we continue fine-tuning its trainable parameters (e.g., soft prompts or linear heads) to search for a continued model which has two low-loss paths to the zero-shot (e.g., CLIP) and the fine-tuned (e.g., CoOp) solutions. By exploiting the geometry of the loss landscape, the continued model implicitly merges the two solutions, restoring pretrained knowledge lost in the fine-tuned counterpart. A challenge is that the vanilla LMC constraint requires data replay from the pretraining task. We approximate this constraint for the zero-shot model via a second-order surrogate, eliminating the need for large-scale data replay. Experiments show that MERGETUNE improves the harmonic mean of CoOp by +5.6% on base-novel generalisation without adding parameters. On robust fine-tuning evaluations, the LMC-merged model from MERGETUNE surpasses ensemble baselines with lower inference cost, achieving further gains and state-of-the-art results when ensembled with the zero-shot model. Our code is available at https://github.com/Surrey-UP-Lab/MERGETUNE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MERGETUNE：视觉语言模型的持续微调</div>
<div class="mono" style="margin-top:8px">对CLIP等视觉语言模型进行微调常导致预训练知识的灾难性遗忘。现有研究主要致力于缓解适应过程中的遗忘，但该过程往往难以完全避免。本文提出一种新范式——持续微调，旨在零样本模型完成适应后恢复预训练知识。我们提出一种基于线性模式连接指导的简易模型无关策略（命名为MERGETUNE），该策略可事后应用于现有微调模型而无需改动架构。给定微调模型，我们持续优化其可训练参数以寻找能通过两条低损耗路径分别连接零样本模型与微调模型的持续模型。通过利用损失曲面的几何特性，该模型能隐式融合两种解决方案，恢复微调模型中丢失的预训练知识。传统线性模式连接约束需预训练任务数据回放，我们通过二阶替代方法逼近零样本模型的约束，从而避免大规模数据回放。实验表明：MERGETUNE在基础-新类别泛化任务中将CoOp的调和平均值提升5.6%且未增加参数；在鲁棒微调评估中，基于该策略的线性模式连接融合模型以更低推理成本超越集成基线，与零样本模型集成后获得更优性能并达到最先进水平。代码已开源：https://github.com/Surrey-UP-Lab/MERGETUNE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the persistent issue of catastrophic forgetting in fine-tuned vision-language models (VLMs), this research introduces a novel continued fine-tuning (CFT) paradigm aimed at recovering lost pretrained knowledge post-adaptation. The proposed method, MERGETUNE, is a model-agnostic strategy that leverages linear mode connectivity (LMC) to guide the continued fine-tuning of a model&#x27;s trainable parameters, seeking a solution with low-loss paths to both the original zero-shot model and the fine-tuned model, thereby implicitly merging their knowledge without architectural changes; a key innovation is approximating the LMC constraint with a second-order surrogate to avoid large-scale data replay. Experimental results demonstrate that MERGETUNE improves the harmonic mean of CoOp by +5.6% on base-novel generalization without adding parameters, and in robust fine-tuning evaluations, the LMC-merged model surpasses ensemble baselines with lower inference cost, achieving state-of-the-art results when further ensembled with the zero-shot model.</div>
<div class="mono" style="margin-top:8px">为解决视觉语言模型（如CLIP）微调后出现的灾难性遗忘问题，本研究提出了一种持续微调（CFT）范式，旨在模型适配后恢复丢失的预训练知识。所提出的MERGETUNE方法是一种模型无关的策略，利用线性模式连通性（LMC）来指导模型可训练参数的持续微调，寻找一个同时与原始零样本模型和微调模型具有低损失路径的解，从而在不改变架构的情况下合并两者的能力；该方法通过二阶近似来模拟LMC约束，避免了大规模数据回放。实验结果表明，MERGETUNE在不增加参数的情况下，将CoOp在基础-新类别泛化上的调和平均提高了5.6%，在鲁棒微调评估中，LMC合并模型以更低的推理成本超越了集成基线，并与零样本模型进一步集成时取得了最先进的结果。</div>
</details>
</div>
<div class="card">
<div class="title">MMedExpert-R1: Strengthening Multimodal Medical Reasoning via Domain-Specific Adaptation and Clinical Guideline Reinforcement</div>
<div class="meta-line">Authors: Meidan Ding, Jipeng Zhang, Wenxuan Wang, Haiqin Zhong, Xiaoling Luo, Wenting Chen, Linlin Shen</div>
<div class="meta-line">First: 2026-01-16T02:32:07+00:00 · Latest: 2026-01-16T02:32:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10949v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10949v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical Vision-Language Models (MedVLMs) excel at perception tasks but struggle with complex clinical reasoning required in real-world scenarios. While reinforcement learning (RL) has been explored to enhance reasoning capabilities, existing approaches face critical mismatches: the scarcity of deep reasoning data, cold-start limits multi-specialty alignment, and standard RL algorithms fail to model clinical reasoning diversity. We propose MMedExpert-R1, a novel reasoning MedVLM that addresses these challenges through domain-specific adaptation and clinical guideline reinforcement. We construct MMedExpert, a high-quality dataset of 10K samples across four specialties with step-by-step reasoning traces. Our Domain-Specific Adaptation (DSA) creates specialty-specific LoRA modules to provide diverse initialization, while Guideline-Based Advantages (GBA) explicitly models different clinical reasoning perspectives to align with real-world diagnostic strategies. Conflict-Aware Capability Integration then merges these specialized experts into a unified agent, ensuring robust multi-specialty alignment. Comprehensive experiments demonstrate state-of-the-art performance, with our 7B model achieving 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, establishing a robust foundation for reliable multimodal medical reasoning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMedExpert-R1：通过领域特定适应与临床指南强化增强多模态医学推理</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型（MedVLMs）在感知任务上表现出色，但在实际场景所需的复杂临床推理方面存在不足。尽管已有研究探索利用强化学习（RL）提升推理能力，但现有方法面临关键不匹配问题：深度推理数据稀缺、冷启动限制多专科对齐，且标准RL算法难以建模临床推理的多样性。我们提出MMedExpert-R1，一种新型推理型MedVLM，通过领域特定适应和临床指南强化应对这些挑战。我们构建了MMedExpert数据集，包含跨四个专科的1万条样本及逐步推理轨迹。领域特定适应（DSA）创建专科特定的LoRA模块以提供多样化初始化，而基于指南的优势（GBA）显式建模不同临床推理视角以符合真实诊断策略。冲突感知能力整合随后将这些专科专家模型融合为统一智能体，确保稳健的多专科对齐。综合实验表明其达到最先进性能：7B参数模型在MedXpert-MM上取得27.50分，在OmniMedVQA上获得83.03分，为可靠的多模态医学推理系统奠定了坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of Medical Vision-Language Models (MedVLMs) in complex clinical reasoning, which include a scarcity of deep reasoning data, cold-start issues in multi-specialty alignment, and the failure of standard reinforcement learning (RL) to model clinical reasoning diversity, this work introduces MMedExpert-R1. The method constructs a high-quality dataset (MMedExpert) with step-by-step reasoning traces and employs Domain-Specific Adaptation (DSA) to create specialty-specific LoRA modules for diverse initialization, alongside Guideline-Based Advantages (GBA) to explicitly model different clinical reasoning perspectives. Comprehensive experiments show that the 7B model achieves state-of-the-art performance, scoring 27.50 on MedXpert-MM and 83.03 on OmniMedVQA, demonstrating robust multi-specialty alignment and establishing a foundation for reliable multimodal medical reasoning.</div>
<div class="mono" style="margin-top:8px">针对医学视觉语言模型在复杂临床推理中存在的深度推理数据稀缺、多专科对齐冷启动以及标准强化学习算法无法建模临床推理多样性等局限，本研究提出了MMedExpert-R1。该方法构建了包含逐步推理轨迹的高质量数据集MMedExpert，并采用领域特定适配（DSA）创建专科特定的LoRA模块以实现多样化初始化，同时利用基于指南的优势（GBA）显式建模不同的临床推理视角以对齐真实诊断策略。综合实验表明，其7B模型取得了最先进的性能，在MedXpert-MM上达到27.50分，在OmniMedVQA上达到83.03分，为可靠的多模态医学推理系统奠定了坚实基础。</div>
</details>
</div>
<div class="card">
<div class="title">PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis</div>
<div class="meta-line">Authors: K Lokesh, Abhirama Subramanyam Penamakuri, Uday Agarwal, Apoorva Challa, Shreya K Gowda, Somesh Gupta, Anand Mishra</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-16T02:18:29+00:00 · Latest: 2026-01-16T02:18:29+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2026 Main Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10945v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10945v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditionally, AI research in medical diagnosis has largely centered on image analysis. While this has led to notable advancements, the absence of patient-reported symptoms continues to hinder diagnostic accuracy. To address this, we propose a Pre-Consultation Dialogue Framework (PCDF) that mimics real-world diagnostic procedures, where doctors iteratively query patients before reaching a conclusion. Specifically, we simulate diagnostic dialogues between two vision-language models (VLMs): a DocVLM, which generates follow-up questions based on the image and dialogue history, and a PatientVLM, which responds using a symptom profile derived from the ground-truth diagnosis. We additionally conducted a small-scale clinical validation of the synthetic symptoms generated by our framework, with licensed clinicians confirming their clinical relevance, symptom coverage, and overall realism. These findings indicate that the resulting DocVLM-PatientVLM interactions form coherent, multi-turn consultations paired with images and diagnoses, which we then use to fine-tune the DocVLM. This dialogue-based supervision leads to substantial gains over image-only training, highlighting the value of realistic symptom elicitation for diagnosis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PatientVLM与DocVLM的相遇：基于视觉语言模型的预咨询对话实现高效诊断</div>
<div class="mono" style="margin-top:8px">传统医学诊断AI研究主要集中于图像分析，虽取得显著进展，但缺乏患者主诉症状仍制约诊断准确性。为此，我们提出预咨询对话框架（PCDF），模拟医生在得出结论前迭代询问患者的真实诊断流程。具体而言，我们构建两个视觉语言模型（VLM）间的诊断对话：DocVLM基于图像和对话历史生成追问，PatientVLM则根据真实诊断衍生的症状档案进行应答。我们进一步对框架生成的合成症状开展小规模临床验证，持证临床医师确认其临床相关性、症状覆盖度和整体真实性。研究表明，DocVLM与PatientVLM的交互可形成连贯的多轮次咨询（含图像与诊断），并用于微调DocVLM。这种基于对话的监督机制相较纯图像训练取得显著提升，凸显了真实症状采集对诊断的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the limitation of image-only AI medical diagnosis, which lacks patient-reported symptoms, this work introduces a Pre-Consultation Dialogue Framework (PCDF) to simulate real-world diagnostic interactions. The method employs two vision-language models: a DocVLM that asks follow-up questions based on medical images and dialogue history, and a PatientVLM that responds using symptom profiles from ground-truth diagnoses. Key experimental results include a small-scale clinical validation where licensed clinicians confirmed the clinical relevance and realism of the synthetic symptoms, and the use of these generated dialogues to fine-tune the DocVLM, yielding substantial performance improvements over image-only training.</div>
<div class="mono" style="margin-top:8px">针对仅依赖图像分析而缺乏患者自述症状的AI诊断局限性，本研究提出了一个模拟真实世界诊断流程的预咨询对话框架（PCDF）。该方法使用两个视觉语言模型：一个基于医学图像和对话历史生成后续问题的DocVLM，以及一个利用真实诊断得出的症状档案进行回答的PatientVLM，从而生成合成的诊断对话。关键的实验结果包括一项小规模临床验证，其中执业临床医生确认了生成症状的临床相关性、覆盖范围和整体真实性；利用这些对话对DocVLM进行微调后，其性能相比仅使用图像训练有显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Image2Garment: Simulation-ready Garment Generation from a Single Image</div>
<div class="meta-line">Authors: Selim Emir Can, Jan Ackermann, Kiyohiro Nakayama, Ruofan Liu, Tong Wu, Yang Zheng, Hugo Bertiche, Menglei Chai, Thabo Beeler, Gordon Wetzstein</div>
<div class="meta-line">First: 2026-01-14T17:47:33+00:00 · Latest: 2026-01-15T21:21:50+00:00</div>
<div class="meta-line">Comments: Project Page: https://image2garment.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09658v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09658v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://image2garment.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Image2Garment：从单张图像生成仿真就绪的服装</div>
<div class="mono" style="margin-top:8px">从单张图像估计物理精确、仿真就绪的服装具有挑战性，原因在于缺乏图像到物理的数据集以及该问题本身的不适定性。现有方法要么需要多视角捕捉和昂贵的可微分仿真，要么仅预测服装几何而缺少真实仿真所需的材料属性。我们提出一种前馈框架，通过先微调视觉语言模型从真实图像推断材料成分与织物属性，再利用小型材料物理测量数据集训练轻量级预测器，将这些属性映射至对应的物理织物参数，从而规避上述限制。该方法引入了两个新数据集（FTAG与T2P），无需迭代优化即可从单张图像生成仿真就绪的服装。实验表明，我们的估计器在材料成分估计与织物属性预测上具有更优精度，且通过物理参数估计器处理后，相比现有图像到服装方法能实现更高保真度的仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating physically accurate, simulation-ready 3D garments from a single image, a problem made difficult by the lack of image-to-physics datasets and its ill-posed nature. The proposed method introduces a feed-forward framework that first fine-tunes a vision-language model to infer material composition and fabric attributes from an input image, and then uses a lightweight predictor trained on a new material-physics dataset to map these attributes to physical simulation parameters. Experimental results demonstrate that the approach achieves superior accuracy in material composition estimation and fabric attribute prediction, and subsequently enables higher-fidelity garment simulations compared to prior state-of-the-art methods, all without requiring iterative optimization or multi-view capture.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从单张图像生成物理精确、可直接用于仿真的服装这一挑战，该问题因缺乏图像到物理的数据集及其不适定性而变得困难。所提出的方法采用前馈框架，首先微调一个视觉语言模型从图像中推断材料成分和织物属性，然后使用一个在小型材料物理数据集上训练的轻量级预测器将这些属性映射到物理仿真参数。实验结果表明，该方法在材料和属性估计上实现了更高的准确性，并且与先进的图像到服装方法相比，能生成保真度更高的服装仿真结果。</div>
</details>
</div>
<div class="card">
<div class="title">Can Vision-Language Models Understand Construction Workers? An Exploratory Study</div>
<div class="meta-line">Authors: Hieu Bui, Nathaniel E. Chodosh, Arash Tavakoli</div>
<div class="meta-line">First: 2026-01-15T20:10:03+00:00 · Latest: 2026-01-15T20:10:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10835v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As robotics become increasingly integrated into construction workflows, their ability to interpret and respond to human behavior will be essential for enabling safe and effective collaboration. Vision-Language Models (VLMs) have emerged as a promising tool for visual understanding tasks and offer the potential to recognize human behaviors without extensive domain-specific training. This capability makes them particularly appealing in the construction domain, where labeled data is scarce and monitoring worker actions and emotional states is critical for safety and productivity. In this study, we evaluate the performance of three leading VLMs, GPT-4o, Florence 2, and LLaVa-1.5, in detecting construction worker actions and emotions from static site images. Using a curated dataset of 1,000 images annotated across ten action and ten emotion categories, we assess each model&#x27;s outputs through standardized inference pipelines and multiple evaluation metrics. GPT-4o consistently achieved the highest scores across both tasks, with an average F1-score of 0.756 and accuracy of 0.799 in action recognition, and an F1-score of 0.712 and accuracy of 0.773 in emotion recognition. Florence 2 performed moderately, with F1-scores of 0.497 for action and 0.414 for emotion, while LLaVa-1.5 showed the lowest overall performance, with F1-scores of 0.466 for action and 0.461 for emotion. Confusion matrix analyses revealed that all models struggled to distinguish semantically close categories, such as collaborating in teams versus communicating with supervisors. While the results indicate that general-purpose VLMs can offer a baseline capability for human behavior recognition in construction environments, further improvements, such as domain adaptation, temporal modeling, or multimodal sensing, may be needed for real-world reliability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型能否理解建筑工人？一项探索性研究</div>
<div class="mono" style="margin-top:8px">随着机器人技术日益融入建筑工作流程，其解读和响应人类行为的能力对于实现安全高效协作至关重要。视觉语言模型已成为视觉理解任务的有力工具，有望无需大量领域特定训练即可识别人类行为。这一特性使其在建筑领域尤其具有吸引力——该领域标注数据稀缺，而监测工人行为与情绪状态对安全和生产力至关重要。本研究评估了GPT-4o、Florence 2和LLaVa-1.5三种主流视觉语言模型在静态工地图像中检测建筑工人行为与情绪的表现。通过使用包含1000张图像、标注十类行为与十类情绪的数据集，我们采用标准化推理流程和多项评估指标对各模型输出进行分析。GPT-4o在两项任务中均表现最佳：行为识别的平均F1分数达0.756、准确率0.799；情绪识别的F1分数为0.712、准确率0.773。Florence 2表现中等（行为F1分数0.497，情绪0.414），LLaVa-1.5整体表现最低（行为F1分数0.466，情绪0.461）。混淆矩阵分析显示所有模型均难以区分语义相近的类别（如团队协作与上级沟通）。结果表明通用视觉语言模型可为建筑环境的人类行为识别提供基础能力，但实际应用仍需通过领域适应、时序建模或多模态感知等技术提升可靠性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable safe and effective human-robot collaboration in construction, where labeled data is scarce, this study explores whether general-purpose Vision-Language Models (VLMs) can recognize worker actions and emotions from static images. The method involved evaluating three VLMs—GPT-4o, Florence 2, and LLaVa-1.5—using a curated dataset of 1,000 images annotated with ten action and ten emotion categories, assessed via standardized inference pipelines and multiple metrics. Key findings show GPT-4o achieved the highest performance (e.g., action F1-score 0.756, emotion F1-score 0.712), while Florence 2 and LLaVa-1.5 performed moderately to poorly; all models struggled with semantically similar categories, indicating a need for domain adaptation or multimodal enhancements for reliable real-world use.</div>
<div class="mono" style="margin-top:8px">为实现建筑工地人机协作的安全高效，本研究针对该领域标注数据稀缺的问题，探索通用视觉语言模型能否从静态图像中识别工人的行为和情绪。研究方法是对GPT-4o、Florence 2和LLaVa-1.5这三个模型进行评估，使用一个包含1000张图像、标注有十类行为和十类情绪的数据集，并通过标准化推理流程和多种指标进行分析。主要实验结果表明，GPT-4o性能最佳（如行为识别的F1分数为0.756），但所有模型在语义相近的类别（如团队协作与上级沟通）上都存在混淆，这说明视觉语言模型虽能提供基础识别能力，但要实现可靠的现实应用可能仍需领域适应或多模态感知等改进。</div>
</details>
</div>
<div class="card">
<div class="title">Alterbute: Editing Intrinsic Attributes of Objects in Images</div>
<div class="meta-line">Authors: Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-15T18:59:53+00:00 · Latest: 2026-01-15T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page is available at https://talreiss.github.io/alterbute/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://talreiss.github.io/alterbute/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Alterbute, a diffusion-based method for editing an object&#x27;s intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., &#x27;&#x27;Porsche 911 Carrera&#x27;&#x27;) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alterbute：编辑图像中物体的固有属性</div>
<div class="mono" style="margin-top:8px">我们提出Alterbute，一种基于扩散模型的图像物体固有属性编辑方法。该方法支持改变物体的颜色、纹理、材质甚至形状，同时保持其感知身份与场景上下文。现有方法要么依赖无监督先验（常难以保持身份），要么采用过度受限的监督（阻碍有意义的固有属性变化）。我们的方法基于：（1）宽松的训练目标，允许模型根据身份参考图像、描述目标固有属性的文本提示、以及定义外部背景的背景图像与物体掩码，同时改变固有与外部属性。在推理阶段，通过复用原始背景与物体掩码限制外部变化，从而确保仅修改目标固有属性；（2）视觉命名实体——细粒度视觉身份类别（如“保时捷911卡雷拉”），将具有身份定义特征但允许固有属性变化的物体归组。我们利用视觉语言模型从大型公共图像数据集中自动提取VNE标签与固有属性描述，实现可扩展的身份保持监督。Alterbute在身份保持的物体固有属性编辑任务上优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enable precise editing of an object&#x27;s intrinsic attributes like color, texture, material, and shape in an image while preserving its identity and scene context, addressing limitations of prior methods that either fail to maintain identity or restrict intrinsic variations. The method, Alterbute, employs a diffusion-based approach with a relaxed training objective conditioned on an identity reference image, a textual prompt for target attributes, and a background with an object mask; at inference, it reuses the original background and mask to restrict extrinsic changes. It also introduces Visual Named Entities (VNEs), fine-grained identity categories automatically labeled using a vision-language model from a large dataset to provide scalable, identity-preserving supervision. Experimentally, Alterbute outperforms existing methods in identity-preserving intrinsic attribute editing.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决图像中物体内在属性（如颜色、纹理、材质和形状）编辑的挑战，同时保持物体的身份和场景上下文，因为现有方法往往难以在身份保持与有意义的属性变化之间取得平衡。所提出的方法Alterbute采用基于扩散的框架，其宽松的训练目标以身份参考图像、描述目标属性的文本提示以及背景与物体掩码为条件；在推理时，通过重用原始背景和掩码来限制外在变化，从而仅改变内在属性。该方法还引入了视觉命名实体（VNEs），即通过视觉语言模型从大型数据集中自动标注的细粒度身份类别，以提供可扩展的、保持身份的监督。实验结果表明，Alterbute在保持身份的内在属性编辑任务上优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion</div>
<div class="meta-line">Authors: Cheng Chen, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Peng Di, Hang Yu, Lianli Gao</div>
<div class="meta-line">First: 2026-01-15T18:59:10+00:00 · Latest: 2026-01-15T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10710v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从单对单到多对多：面向深度视觉-语言融合的动态跨层注入</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通过仅将视觉编码器输出单向连接至大语言模型（LLM）输入的粗粒度非对称架构，形成了严重的视觉特征瓶颈。这种静态结构从根本上限制了LLM与层次化视觉知识实现全面对齐的能力，削弱了其将局部细节与全局语义准确整合为连贯推理的效能。为此，我们提出跨层注入（CLI）框架——一种轻量级创新方案，可在双模态间构建动态多对多桥梁。CLI包含两个高效协同的参数优化组件：自适应多投影（AMP）模块（协调多层级视觉特征）与自适应门控融合（AGF）机制（使LLM能依据实时解码上下文选择性注入最相关的视觉信息）。通过将CLI集成至LLaVA-OneVision与LLaVA-1.5模型，我们在18个多样化基准测试中验证了其有效性与泛化能力。实验结果表明CLI带来显著性能提升，确立了其作为可扩展范式的价值——通过赋予LLM按需访问完整视觉层级的能力，开启了更深层次的多模态理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the visual feature bottleneck in Vision-Language Models (VLMs), where a static, one-to-one connection between the vision encoder and the large language model (LLM) limits comprehensive alignment with hierarchical visual knowledge. To overcome this, the authors propose Cross-Layer Injection (CLI), a lightweight framework featuring an Adaptive Multi-Projection module to harmonize features from multiple vision layers and an Adaptive Gating Fusion mechanism that allows the LLM to selectively inject relevant visual information based on its decoding context. Experimental integration into LLaVA-OneVision and LLaVA-1.5 across 18 benchmarks demonstrates significant performance improvements, establishing CLI as an effective method for enhancing multimodal understanding through dynamic, many-to-many cross-modal connections.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型（VLMs）中存在的视觉特征瓶颈问题，即视觉编码器与大语言模型（LLM）之间静态的一对一连接限制了与层次化视觉知识的全面对齐。为解决此问题，作者提出了跨层注入（CLI）这一轻量级框架，其包含自适应多投影模块以协调来自多个视觉层的特征，以及自适应门控融合机制，使LLM能够根据其解码上下文选择性注入相关视觉信息。通过在LLaVA-OneVision和LLaVA-1.5上进行实验，并在18个多样化基准测试中验证，结果表明该方法显著提升了性能，确立了CLI作为一种通过动态多对多融合来增强多模态理解的有效范式。</div>
</details>
</div>
<div class="card">
<div class="title">Future Optical Flow Prediction Improves Robot Control &amp; Video Generation</div>
<div class="meta-line">Authors: Kanchana Ranasinghe, Honglu Zhou, Yu Fang, Luyu Yang, Le Xue, Ran Xu, Caiming Xiong, Silvio Savarese, Michael S Ryoo, Juan Carlos Niebles</div>
<div class="meta-line">First: 2026-01-15T18:49:48+00:00 · Latest: 2026-01-15T18:49:48+00:00</div>
<div class="meta-line">Comments: Project Site (Code, Models, Demo): https://fofpred.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10781v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://fofpred.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>未来光流预测提升机器人控制与视频生成</div>
<div class="mono" style="margin-top:8px">未来运动表征（如光流）对控制和生成任务具有重要价值。然而，预测可泛化的空间密集运动表征仍是关键挑战，且从噪声现实数据中学习此类预测的研究相对不足。我们提出FOFPred——一种新型语言条件光流预测模型，采用统一的视觉语言模型与扩散架构。该独特组合通过像素级生成保真度实现强大的多模态推理能力，用于未来运动预测。模型基于网络规模的人类活动数据（高度可扩展但非结构化）进行训练。为从含噪声的视频-字幕数据中提取有效信号，我们采用关键的数据预处理技术，并结合强图像预训练的统⼀架构。训练完成的模型可扩展应用于控制与生成两大下游任务。在语言驱动场景下的机器人操作与视频生成评估表明，FOFPred具备跨领域通用性，验证了统一VLM-扩散架构的价值，以及从多样化网络数据中进行可扩展学习对未来光流预测的意义。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of forecasting generalizable dense motion representations like optical flow from noisy real-world data, which is valuable for control and generative tasks but remains underexplored. The authors propose FOFPred, a model that uniquely combines a Vision-Language Model (VLM) with a Diffusion architecture to enable multimodal reasoning and high-fidelity pixel-level prediction of future optical flow. The model is trained on scalable but unstructured web-scale human activity data using specific preprocessing techniques and strong image pretraining. Experimental results demonstrate its cross-domain versatility, showing improved performance in both robotic manipulation and language-conditioned video generation tasks.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，光流等未来运动表征对于控制和生成任务具有重要价值，但从嘈杂的真实世界数据中预测通用且空间密集的运动表征仍是一个关键挑战。方法上，研究提出了FOFPred，一种语言条件化的光流预测模型，它结合了视觉语言模型（VLM）和扩散架构，以实现多模态推理和高保真的像素级预测。主要实验结果表明，FOFPred在经过专门预处理的网络规模人类活动数据上训练后，能有效处理下游机器人操控和语言驱动视频生成任务，证实了其跨领域通用性，以及统一架构和可扩展学习方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</div>
<div class="meta-line">Authors: Jorge Ortiz</div>
<div class="meta-line">First: 2025-12-31T23:27:32+00:00 · Latest: 2026-01-15T17:31:17+00:00</div>
<div class="meta-line">Comments: Preprint. Diagnostic study of confidence-based abstention under evidence truncation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00138v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00138v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频问答中可预测可靠性的显式弃权调控机制</div>
<div class="mono" style="margin-top:8px">视觉语言模型在高风险场景部署时需采用选择性预测机制，使系统在不确定时主动弃权以避免代价高昂的错误。本研究探究基于置信度的弃权机制能否在视频问答任务中实现对错误率的可靠控制，以及该控制在分布偏移下是否保持稳健。通过NExT-QA数据集与Gemini 2.0 Flash模型实验获得两项发现：首先，置信度阈值法可在分布内实现机制性控制，调节阈值ε可产生平滑的风险-覆盖权衡曲线，有效降低错误率</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether confidence-based abstention can provide reliable and predictable control over error rates for vision-language models in high-stakes video question answering, particularly under distribution shift. The method involves a diagnostic analysis using the NExT-QA dataset and the Gemini 2.0 Flash model, sweeping a confidence threshold to examine the risk-coverage trade-off. The key experimental findings are that confidence thresholding offers mechanistic control over error rates in-distribution, producing smooth trade-off curves, but this control degrades under distribution shift, highlighting a reliability gap.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究基于置信度的弃权机制能否为视频问答系统提供可靠且可预测的错误率控制，特别是在分布变化下，以支持需要避免代价高昂错误的高风险部署。该方法使用NExT-QA数据集和Gemini 2.0 Flash模型进行诊断分析，将置信度阈值作为显式的弃权机制。关键实验结果表明，在分布内调整置信度阈值能产生平滑的风险-覆盖权衡，有效降低错误率，但根据摘要的截断内容，这种控制在分布变化下的可靠性仍有待进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</div>
<div class="meta-line">Authors: Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna</div>
<div class="meta-line">First: 2026-01-15T17:27:44+00:00 · Latest: 2026-01-15T17:27:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10611v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10611v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Today&#x27;s strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&amp;F on video tracking).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Molmo2：具备视频理解与定位能力的开源视觉语言模型权重与数据集</div>
<div class="mono" style="margin-top:8px">当前最先进的视频语言模型（VLM）仍为闭源。最强的开源权重模型要么依赖闭源VLM生成的合成数据进行蒸馏训练，要么未公开其训练数据或方法，导致开源社区缺乏改进前沿视频（及图像）语言模型的基础。关键的是，许多下游应用不仅需要高层次视频理解，还需像素级的指向或跟踪定位能力——即使闭源模型也尚未完全具备。我们推出Molmo2系列VLM，它在开源模型中达到领先水平，并在单图、多图及视频任务中展现出卓越的指向定位能力。核心贡献包括7个新视频数据集和2个多图数据集：涵盖用于预训练的高细节视频描述数据集、用于微调的自由形式视频问答数据集、含复杂查询的新物体跟踪数据集，以及创新的视频指向数据集——所有数据均未使用闭源VLM。我们还提出了基于高效打包与消息树编码的训练方案，证明视觉令牌的双向注意力机制与新颖的令牌权重策略能提升性能。我们的8B顶级模型在短视频、计数和描述任务上超越同类开源权重与数据模型，在长视频任务中表现相当；在视频定位任务中，Molmo2显著优于Qwen3-VL等开源模型（视频计数准确率35.5对29.6），并在部分任务上超越Gemini 3 Pro等闭源模型（视频指向F1值38.4对20.0，视频跟踪J&amp;F值56.2对41.1）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of open-source video-language models (VLMs) that are both state-of-the-art and capable of grounding (e.g., pointing or tracking), as current strong models are either proprietary or rely on synthetic data from them. The method involves introducing Molmo2, a family of VLMs trained on a newly collected suite of seven video and two multi-image datasets without using closed VLMs, and employing a training recipe with efficient packing, message-tree encoding, bi-directional attention on vision tokens, and a novel token-weight strategy. Key experimental results show that the 8B model outperforms other open-weight and data models in tasks like short video understanding, counting, and captioning, and significantly exceeds models like Qwen3-VL and even proprietary ones like Gemini 3 Pro in video grounding metrics such as counting accuracy, pointing F1, and tracking J&amp;F scores.</div>
<div class="mono" style="margin-top:8px">针对当前高性能视频-语言模型多为闭源，且开源模型依赖私有模型数据或训练方案不透明，缺乏像素级定位能力的问题，本研究提出了开放权重的Molmo2模型系列。其核心方法在于，在不使用闭源模型的前提下，构建了七个新的视频数据集和两个多图像数据集，并采用了一种包含高效数据打包、消息树编码、视觉令牌双向注意力及新颖令牌权重策略的训练方案。主要实验结果表明，其8B模型在开源模型中，于短视频、计数和描述任务上达到最优性能，在长视频任务上具有竞争力，并在视频定位任务（如指向和跟踪）上显著超越了Qwen3-VL等开源模型和Gemini 3 Pro等闭源模型。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Misalignment in Vision-Language Models under Perceptual Degradation</div>
<div class="meta-line">Authors: Guo Cheng</div>
<div class="meta-line">First: 2026-01-13T09:13:05+00:00 · Latest: 2026-01-15T17:10:05+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08355v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08355v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知退化下视觉语言模型的语义失配研究</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）正日益应用于自动驾驶和具身人工智能系统，其中可靠的感知对于安全的语义推理和决策至关重要。尽管当前VLMs在多模态基准测试中表现出色，但其对实际感知退化的鲁棒性仍鲜为人知。本研究通过以Cityscapes数据集的语义分割作为代表性感知模块，系统探究了上游视觉感知受控退化下VLMs的语义失配现象。我们引入了仅导致传统分割指标适度下降但会引发下游VLM行为严重故障的感知现实性退化，包括幻觉对象提及、安全关键实体遗漏以及不一致的安全判断。为量化这些影响，我们提出了一套语言层面的失配度量指标，用于捕捉幻觉、关键遗漏和安全误判，并分析了其与多种对比式和生成式VLM分割质量的关系。研究结果揭示了像素级鲁棒性与多模态语义可靠性之间的明显脱节，凸显了当前基于VLM系统的关键局限，并表明在安全关键应用中亟需建立能显式考量感知不确定性的评估框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Models (VLMs) are critical for safety-sensitive applications like autonomous driving, but their robustness to degraded visual inputs is not well understood. This work systematically evaluates VLM semantic misalignment by applying controlled, perception-realistic corruptions to the visual inputs of a semantic segmentation module on the Cityscapes dataset. The experiments reveal that even moderate drops in segmentation metrics cause severe downstream failures in multiple VLMs, including object hallucination, omission of critical entities, and inconsistent safety judgments, demonstrating a significant disconnect between pixel-level robustness and semantic reliability.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在自动驾驶等安全关键系统中至关重要，但其对视觉感知退化的鲁棒性尚不明确。本研究通过向Cityscapes数据集的语义分割输出施加受控的、符合感知现实的退化，系统性地评估了视觉语言模型的语义错位问题。实验结果表明，即使标准分割指标仅适度下降，也会导致多种视觉语言模型出现严重的下游故障，包括物体幻觉、安全关键实体的遗漏以及不一致的安全判断，这揭示了像素级鲁棒性与多模态语义可靠性之间存在显著脱节。</div>
</details>
</div>
<div class="card">
<div class="title">Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Mikel Williams-Lekuona, Georgina Cosma</div>
<div class="meta-line">First: 2025-12-17T12:19:54+00:00 · Latest: 2026-01-15T16:58:39+00:00</div>
<div class="meta-line">Comments: Camera-ready version for ECIR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15372v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15372v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision transformers in vision-language models typically use the same amount of compute for every image, regardless of whether it is simple or complex. We propose ICAR (Image Complexity-Aware Retrieval), an adaptive computation approach that enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both the early-exit and full-depth paths. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance, attaining a Pearson correlation coefficient of 0.959 with human labelling whilst delivering 4.4x faster complexity prediction. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% faster image encoding while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效视觉语言模型的图像复杂度感知自适应检索</div>
<div class="mono" style="margin-top:8px">视觉语言模型中的视觉Transformer通常对每张图像使用相同的计算量，无论其简单或复杂。我们提出ICAR（图像复杂度感知检索），这是一种自适应计算方法，使视觉Transformer能够对简单图像使用较少计算，同时对复杂图像执行完整的网络深度处理。关键挑战在于保持跨模态对齐：来自不同处理深度的嵌入必须保持文本匹配的兼容性。ICAR通过双路径训练解决此问题，该训练从早期退出路径和完整深度路径生成兼容嵌入。这确保了无论图像是早期退出还是完整处理，其表征与文本嵌入都能在同一语义空间中保持兼容。与现有需要昂贵重排序的两阶段方法不同，ICAR无需额外开销即可实现直接图文匹配。为确定计算量，我们开发了ConvNeXt-IC，将图像复杂度评估视为分类任务。通过采用现代分类器主干而非专用架构，ConvNeXt-IC实现了最先进性能，与人工标注的皮尔逊相关系数达0.959，同时复杂度预测速度提升4.4倍。在结合真实网络数据的标准基准测试中，ICAR实现了20%的图像编码加速，在保持类别级性能的同时达到实例级性能的95%，为视觉语言系统的可持续扩展提供了可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of vision transformers in vision-language models, which uniformly apply the same computational resources to all images regardless of their complexity. The proposed method, ICAR (Image Complexity-Aware Retrieval), introduces an adaptive computation approach where simple images are processed with less compute via early exits, while complex images use the full network depth; to maintain cross-modal alignment for text matching, ICAR employs a dual-path training strategy that ensures compatible embeddings from both early-exit and full-depth paths. Experimental results show that ICAR achieves 20% faster image encoding while preserving category-level performance and 95% of instance-level retrieval accuracy on benchmarks augmented with web data, and its complexity predictor, ConvNeXt-IC, attains a Pearson correlation of 0.959 with human labels and is 4.4x faster than previous methods.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉语言模型中视觉变换器对所有图像统一计算导致的效率低下问题，提出了一种自适应计算方法ICAR，通过早期退出减少简单图像的计算量，同时完整处理复杂图像，并采用双路径训练保持跨模态对齐以确保不同深度的嵌入兼容性。关键实验结果表明，在增强真实网络数据的标准基准测试中，ICAR实现了20%的图像编码加速，同时保持了类别级性能及95%的实例级准确度，其复杂度预测器ConvNeXt-IC达到了与人工标注0.959的皮尔逊相关系数，且预测速度提升4.4倍。</div>
</details>
</div>
<div class="card">
<div class="title">Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure</div>
<div class="meta-line">Authors: Luxuan Fu, Chong Liu, Bisheng Yang, Zhen Dong</div>
<div class="meta-line">First: 2026-01-15T16:16:34+00:00 · Latest: 2026-01-15T16:16:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10551v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10551v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated perception of urban roadside infrastructure is crucial for smart city management, yet general-purpose models often struggle to capture the necessary fine-grained attributes and domain rules. While Large Vision Language Models (VLMs) excel at open-world recognition, they often struggle to accurately interpret complex facility states in compliance with engineering standards, leading to unreliable performance in real-world applications. To address this, we propose a domain-adapted framework that transforms VLMs into specialized agents for intelligent infrastructure analysis. Our approach integrates a data-efficient fine-tuning strategy with a knowledge-grounded reasoning mechanism. Specifically, we leverage open-vocabulary fine-tuning on Grounding DINO to robustly localize diverse assets with minimal supervision, followed by LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning. To mitigate hallucinations and enforce professional compliance, we introduce a dual-modality Retrieval-Augmented Generation (RAG) module that dynamically retrieves authoritative industry standards and visual exemplars during inference. Evaluated on a comprehensive new dataset of urban roadside scenes, our framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, demonstrating a robust solution for intelligent infrastructure monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>释放大型视觉语言模型在路侧基础设施智能感知中的潜力</div>
<div class="mono" style="margin-top:8px">城市路侧基础设施的自动化感知对智慧城市管理至关重要，但通用模型往往难以捕捉必要的细粒度属性与领域规则。尽管大型视觉语言模型在开放世界识别方面表现优异，却常无法依据工程标准准确解析复杂的设施状态，导致实际应用可靠性不足。为此，我们提出一种领域自适应框架，将视觉语言模型转化为专业化的基础设施智能分析智能体。该方法融合了数据高效微调策略与知识驱动的推理机制：首先基于Grounding DINO进行开放词汇微调，以最少监督实现多样化资产的鲁棒定位；随后通过Qwen-VL的LoRA适配实现深层语义属性推理。为减少幻觉并确保专业合规性，我们引入了双模态检索增强生成模块，在推理过程中动态检索权威行业标准与视觉范例。基于新构建的城市路侧场景数据集评估，本框架实现了58.9%的检测平均精度与95.5%的属性识别准确率，为基础设施智能监测提供了稳健解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of general-purpose vision-language models in accurately perceiving and interpreting fine-grained attributes and domain-specific rules of urban roadside infrastructure, which is essential for smart city management. The proposed method transforms large vision-language models into specialized agents through a domain-adapted framework that combines open-vocabulary fine-tuning on Grounding DINO for robust asset localization with minimal supervision and LoRA-based adaptation on Qwen-VL for deep semantic attribute reasoning, enhanced by a dual-modality retrieval-augmented generation module to mitigate hallucinations and enforce compliance with authoritative industry standards. Experimental evaluation on a comprehensive dataset of urban roadside scenes demonstrates that the framework achieves a detection performance of 58.9 mAP and an attribute recognition accuracy of 95.5%, indicating its effectiveness for intelligent infrastructure monitoring.</div>
<div class="mono" style="margin-top:8px">本研究针对通用视觉语言模型在准确感知城市路边基础设施方面的局限性，该任务需要细粒度属性识别并遵循工程标准。所提出的方法结合了Grounding DINO的数据高效微调以实现开放词汇目标定位，以及Qwen-VL的LoRA适配以进行语义推理，并通过一个双模态检索增强生成模块来整合权威标准和视觉范例以减少幻觉。在新型城市路边场景数据集上的评估结果显示，该框架实现了58.9%的mAP检测性能和95.5%的属性识别准确率，证明了其在基础设施智能监测中的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">SVII-3D: Advancing Roadside Infrastructure Inventory with Decimeter-level 3D Localization and Comprehension from Sparse Street Imagery</div>
<div class="meta-line">Authors: Chong Liu, Luxuan Fu, Yang Jia, Zhen Dong, Bisheng Yang</div>
<div class="meta-line">First: 2026-01-15T15:57:18+00:00 · Latest: 2026-01-15T15:57:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10535v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10535v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automated creation of digital twins and precise asset inventories is a critical task in smart city construction and facility lifecycle management. However, utilizing cost-effective sparse imagery remains challenging due to limited robustness, inaccurate localization, and a lack of fine-grained state understanding. To address these limitations, SVII-3D, a unified framework for holistic asset digitization, is proposed. First, LoRA fine-tuned open-set detection is fused with a spatial-attention matching network to robustly associate observations across sparse views. Second, a geometry-guided refinement mechanism is introduced to resolve structural errors, achieving precise decimeter-level 3D localization. Third, transcending static geometric mapping, a Vision-Language Model agent leveraging multi-modal prompting is incorporated to automatically diagnose fine-grained operational states. Experiments demonstrate that SVII-3D significantly improves identification accuracy and minimizes localization errors. Consequently, this framework offers a scalable, cost-effective solution for high-fidelity infrastructure digitization, effectively bridging the gap between sparse perception and automated intelligent maintenance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVII-3D：基于稀疏街景图像实现分米级三维定位与理解的道路基础设施盘点技术进展</div>
<div class="mono" style="margin-top:8px">数字孪生自动构建与精准资产盘点是智慧城市建设和设施全生命周期管理的关键任务。然而，受限于鲁棒性不足、定位不精确及细粒度状态理解缺失，利用经济高效的稀疏图像仍具挑战。为此，本文提出SVII-3D——面向资产全要素数字化的统一框架：首先融合LoRA微调的开集检测与空间注意力匹配网络，实现稀疏视角观测的鲁棒关联；其次引入几何引导优化机制修正结构误差，达成分米级精度的三维定位；最后突破静态几何映射局限，集成基于多模态提示的视觉-语言模型智能体，自动诊断细粒度运行状态。实验表明SVII-3D显著提升识别精度并最小化定位误差，为高保真基础设施数字化提供了可扩展、经济高效的解决方案，有效弥合稀疏感知与自动化智能维护间的鸿沟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for automated, cost-effective creation of digital twins and precise asset inventories for smart cities, which is challenging with sparse street imagery due to robustness, localization accuracy, and fine-grained state comprehension issues. The method, SVII-3D, is a unified framework that first fuses LoRA fine-tuned open-set detection with a spatial-attention matching network for robust cross-view association, then employs a geometry-guided refinement mechanism for precise decimeter-level 3D localization, and finally integrates a Vision-Language Model agent with multi-modal prompting to diagnose fine-grained operational states. Experimental results show that the framework significantly improves identification accuracy and minimizes localization errors, offering a scalable solution for high-fidelity infrastructure digitization that bridges sparse perception and automated maintenance.</div>
<div class="mono" style="margin-top:8px">该研究旨在为智慧城市自动化创建数字孪生和精确资产清单，以解决使用经济高效的稀疏街景图像时面临的挑战，如鲁棒性有限、定位不准确和缺乏细粒度状态理解。提出的SVII-3D框架集成了LoRA微调的开集检测器与空间注意力匹配网络以实现鲁棒的跨视图关联，采用几何引导的细化机制实现分米级精度的3D定位，并引入基于多模态提示的视觉语言模型代理来诊断细粒度运行状态。实验结果表明，SVII-3D显著提高了识别精度并最小化了定位误差，为高保真基础设施数字化提供了一个可扩展且经济高效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Urban Socio-Semantic Segmentation with Vision-Language Reasoning</div>
<div class="meta-line">Authors: Yu Wang, Yi Wang, Rui Dai, Yujie Wang, Kaikui Liu, Xiangxiang Chu, Yansheng Li</div>
<div class="meta-line">First: 2026-01-15T15:00:36+00:00 · Latest: 2026-01-15T15:00:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10477v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10477v1">PDF</a> · <a href="https://github.com/AMAP-ML/SocioReasoner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As hubs of human activity, urban surfaces consist of a wealth of semantic entities. Segmenting these various entities from satellite imagery is crucial for a range of downstream applications. Current advanced segmentation models can reliably segment entities defined by physical attributes (e.g., buildings, water bodies) but still struggle with socially defined categories (e.g., schools, parks). In this work, we achieve socio-semantic segmentation by vision-language model reasoning. To facilitate this, we introduce the Urban Socio-Semantic Segmentation dataset named SocioSeg, a new resource comprising satellite imagery, digital maps, and pixel-level labels of social semantic entities organized in a hierarchical structure. Additionally, we propose a novel vision-language reasoning framework called SocioReasoner that simulates the human process of identifying and annotating social semantic entities via cross-modal recognition and multi-stage reasoning. We employ reinforcement learning to optimize this non-differentiable process and elicit the reasoning capabilities of the vision-language model. Experiments demonstrate our approach&#x27;s gains over state-of-the-art models and strong zero-shot generalization. Our dataset and code are available in https://github.com/AMAP-ML/SocioReasoner.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言推理的城市社会语义分割</div>
<div class="mono" style="margin-top:8px">作为人类活动的枢纽，城市地表包含丰富的语义实体。从卫星图像中分割这些多样实体对一系列下游应用至关重要。当前先进的语义分割模型能可靠分割由物理属性定义的实体（如建筑、水体），但在处理社会定义类别（如学校、公园）时仍面临挑战。本研究通过视觉语言模型推理实现了社会语义分割。为此，我们构建了名为SocioSeg的城市社会语义分割数据集，该资源包含卫星影像、数字地图及按层级结构组织的社会语义实体像素级标注。同时，我们提出名为SocioReasoner的新型视觉语言推理框架，通过跨模态识别与多阶段推理模拟人类识别标注社会语义实体的认知过程。采用强化学习优化这一不可微分流程，以激发视觉语言模型的推理潜能。实验表明，该方法优于当前最优模型并具备强大的零样本泛化能力。数据集与代码已开源：https://github.com/AMAP-ML/SocioReasoner。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of segmenting socially defined urban entities, such as schools and parks, from satellite imagery, which existing segmentation models struggle with despite their proficiency in identifying physically defined categories. The authors introduce a new dataset, SocioSeg, containing satellite images, digital maps, and hierarchical pixel-level labels for social semantics, and propose a vision-language reasoning framework, SocioReasoner, that mimics human annotation through cross-modal recognition and multi-stage reasoning, optimized via reinforcement learning. Experimental results show that this approach outperforms state-of-the-art models and exhibits strong zero-shot generalization capabilities.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决从卫星图像中分割社会定义类别（如学校、公园）的挑战，现有模型对此类实体分割效果不佳，尽管它们在物理定义实体上表现良好。作者引入了具有分层像素级标签的SocioSeg数据集，并提出了SocioReasoner框架，该框架通过跨模态识别和多阶段推理，结合强化学习优化，模拟人类标注过程。实验表明，该方法优于现有最先进模型，并展现出强大的零样本泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Zoom-IQA: Image Quality Assessment with Reliable Region-Aware Reasoning</div>
<div class="meta-line">Authors: Guoqiang Liang, Jianyi Wang, Zhonghua Wu, Shangchen Zhou</div>
<div class="meta-line">First: 2026-01-06T11:00:17+00:00 · Latest: 2026-01-15T14:19:47+00:00</div>
<div class="meta-line">Comments: Project Page: https://ethanliang99.github.io/ZOOMIQA-Projectpage</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02918v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02918v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ethanliang99.github.io/ZOOMIQA-Projectpage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image Quality Assessment (IQA) is a long-standing problem in computer vision. Previous methods typically focus on predicting numerical scores without explanation or providing low-level descriptions lacking precise scores. Recent reasoning-based vision language models (VLMs) have shown strong potential for IQA by jointly generating quality descriptions and scores. However, existing VLM-based IQA methods often suffer from unreliable reasoning due to their limited capability of integrating visual and textual cues. In this work, we introduce Zoom-IQA, a VLM-based IQA model to explicitly emulate key cognitive behaviors: uncertainty awareness, region reasoning, and iterative refinement. Specifically, we present a two-stage training pipeline: 1) supervised fine-tuning (SFT) on our Grounded-Rationale-IQA (GR-IQA) dataset to teach the model to ground its assessments in key regions, and 2) reinforcement learning (RL) for dynamic policy exploration, stabilized by our KL-Coverage regularizer to prevent reasoning and scoring diversity collapse, with a Progressive Re-sampling Strategy for mitigating annotation bias. Extensive experiments show that Zoom-IQA achieves improved robustness, explainability, and generalization. The application to downstream tasks, such as image restoration, further demonstrates the effectiveness of Zoom-IQA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Zoom-IQA：基于可靠区域感知推理的图像质量评估</div>
<div class="mono" style="margin-top:8px">图像质量评估（IQA）是计算机视觉领域的长期难题。现有方法通常仅预测数值分数而缺乏解释，或提供缺乏精确分数的低层次描述。近期基于推理的视觉语言模型（VLMs）通过联合生成质量描述与分数，展现出解决IQA问题的强大潜力。然而，现有基于VLM的IQA方法因整合视觉与文本线索的能力有限，常存在推理不可靠的问题。本研究提出Zoom-IQA——一种基于VLM的IQA模型，其显式模拟了关键认知行为：不确定性感知、区域推理与迭代优化。具体而言，我们设计了两阶段训练流程：1）在自建的Grounded-Rationale-IQA（GR-IQA）数据集上进行监督微调（SFT），使模型学会将评估依据锚定于关键区域；2）通过强化学习（RL）进行动态策略探索，并采用KL-Coverage正则化器稳定训练以防止推理与评分多样性坍缩，同时结合渐进重采样策略缓解标注偏差。大量实验表明，Zoom-IQA在鲁棒性、可解释性与泛化能力方面均取得提升。在图像修复等下游任务中的应用进一步验证了Zoom-IQA的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of existing Image Quality Assessment (IQA) methods, where traditional approaches provide only numerical scores without explanation, and recent vision-language models (VLMs) exhibit unreliable reasoning due to poor visual-textual integration. The proposed Zoom-IQA model explicitly emulates key cognitive behaviors—uncertainty awareness, region reasoning, and iterative refinement—through a two-stage training pipeline: first, supervised fine-tuning on a Grounded-Rationale-IQA dataset to ground assessments in key image regions, followed by reinforcement learning stabilized by a KL-Coverage regularizer to prevent reasoning collapse, with a Progressive Re-sampling Strategy to mitigate annotation bias. Experimental results demonstrate that Zoom-IQA achieves improved robustness, explainability, and generalization, with effectiveness further validated in downstream applications like image restoration.</div>
<div class="mono" style="margin-top:8px">该研究针对现有图像质量评估（IQA）方法的局限性：要么提供没有解释的数值分数，要么给出缺乏精确分数的低级描述，以及近期视觉语言模型（VLM）因视觉和文本线索整合不佳而导致推理不可靠的问题。提出的Zoom-IQA模型通过模拟关键认知行为——不确定性感知、区域推理和迭代优化——采用两阶段训练流程：首先，在基于区域的理性IQA数据集上进行监督微调，将评估锚定在关键区域；其次，通过KL-Coverage正则化稳定的强化学习来防止推理和评分多样性崩溃，并结合渐进重采样策略减轻标注偏差。实验结果表明，Zoom-IQA在鲁棒性、可解释性和泛化性方面均有提升，其有效性在下游任务如图像修复中得到了进一步验证。</div>
</details>
</div>
<div class="card">
<div class="title">Global Context Compression with Interleaved Vision-Text Transformation</div>
<div class="meta-line">Authors: Dian Jiao, Jiaxin Duan, Shuai Zhao, Jiabing Leng, Yiran Zhang, Feng Huang</div>
<div class="meta-line">First: 2026-01-15T13:29:16+00:00 · Latest: 2026-01-15T13:29:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10378v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10378v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent achievements of vision-language models in end-to-end OCR point to a new avenue for low-loss compression of textual information. This motivates earlier works that render the Transformer&#x27;s input into images for prefilling, which effectively reduces the number of tokens through visual encoding, thereby alleviating the quadratically increased Attention computations. However, this partial compression fails to save computational or memory costs at token-by-token inference. In this paper, we investigate global context compression, which saves tokens at both prefilling and inference stages. Consequently, we propose VIST2, a novel Transformer that interleaves input text chunks alongside their visual encoding, while depending exclusively on visual tokens in the pre-context to predict the next text token distribution. Around this idea, we render text chunks into sketch images and train VIST2 in multiple stages, starting from curriculum-scheduled pretraining for optical language modeling, followed by modal-interleaved instruction tuning. We conduct extensive experiments using VIST2 families scaled from 0.6B to 8B to explore the training recipe and hyperparameters. With a 4$\times$ compression ratio, the resulting models demonstrate significant superiority over baselines on long writing tasks, achieving, on average, a 3$\times$ speedup in first-token generation, 77% reduction in memory usage, and 74% reduction in FLOPS. Our codes and datasets will be public to support further studies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于交错视觉-文本转换的全局上下文压缩</div>
<div class="mono" style="margin-top:8px">视觉语言模型在端到端OCR领域的最新成果为文本信息的低损压缩提供了新思路，这推动了早期将Transformer输入渲染为图像以进行预填充的研究，通过视觉编码有效减少令牌数量，从而缓解注意力计算量的二次增长。然而，这种局部压缩在逐令牌推理阶段无法节省计算或内存成本。本文研究全局上下文压缩，在预填充和推理阶段均减少令牌使用。为此，我们提出VIST2——一种新型Transformer，它将输入文本块与其视觉编码交错排列，并仅依赖前文中的视觉令牌来预测下一个文本令牌的分布。基于此，我们将文本块渲染为草图图像，并分多阶段训练VIST2：从课程调度的光学语言建模预训练开始，再到模态交错指令微调。通过规模从0.6B到8B的VIST2系列模型进行大量实验，探索训练方案与超参数。在4倍压缩比下，所得模型在长文本生成任务中显著优于基线，平均实现首令牌生成速度提升3倍、内存使用降低77%、浮点运算量减少74%。代码与数据集将公开以支持后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency of partial compression methods that only reduce tokens during prefilling but not during token-by-token inference, this research introduces VIST2, a Transformer model designed for global context compression. The method interleaves input text chunks with their visual encodings, using exclusively visual tokens from the pre-context to predict subsequent text token distributions; it involves rendering text into sketch images and employs a multi-stage training regimen including curriculum-scheduled pretraining and modal-interleaved instruction tuning. Experimental results from models scaled from 0.6B to 8B parameters show that with a 4× compression ratio, VIST2 significantly outperforms baselines on long writing tasks, delivering on average a 3× speedup in first-token generation, a 77% reduction in memory usage, and a 74% reduction in FLOPS.</div>
<div class="mono" style="margin-top:8px">该研究旨在降低Transformer模型在端到端OCR中的计算和内存成本，现有方法将输入压缩为图像仅能减轻预填充阶段的成本，而无法优化逐令牌推理阶段。为此，作者提出了VIST2，一种新颖的Transformer模型，它将文本块与其视觉编码交错排列，并仅依赖前文中的视觉令牌来预测下一个文本令牌分布，从而实现全局上下文压缩。通过从0.6B到8B参数规模的模型实验，结果表明在4倍压缩比下，VIST2在长文本生成任务中显著优于基线模型，平均实现了首次令牌生成速度提升3倍、内存使用减少77%以及浮点运算减少74%的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Hierarchical Refinement of Universal Multimodal Attacks on Vision-Language Models</div>
<div class="meta-line">Authors: Peng-Fei Zhang, Zi Huang</div>
<div class="meta-line">First: 2026-01-15T11:45:56+00:00 · Latest: 2026-01-15T11:45:56+00:00</div>
<div class="meta-line">Comments: 15 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10313v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10313v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing adversarial attacks for VLP models are mostly sample-specific, resulting in substantial computational overhead when scaled to large datasets or new scenarios. To overcome this limitation, we propose Hierarchical Refinement Attack (HRA), a multimodal universal attack framework for VLP models. HRA refines universal adversarial perturbations (UAPs) at both the sample level and the optimization level. For the image modality, we disentangle adversarial examples into clean images and perturbations, allowing each component to be handled independently for more effective disruption of cross-modal alignment. We further introduce a ScMix augmentation strategy that diversifies visual contexts and strengthens both global and local utility of UAPs, thereby reducing reliance on spurious features. In addition, we refine the optimization path by leveraging a temporal hierarchy of historical and estimated future gradients to avoid local minima and stabilize universal perturbation learning. For the text modality, HRA identifies globally influential words by combining intra-sentence and inter-sentence importance measures, and subsequently utilizes these words as universal text perturbations. Extensive experiments across various downstream tasks, VLP models, and datasets demonstrate the superiority of the proposed universal multimodal attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型通用多模态攻击的层次化精炼</div>
<div class="mono" style="margin-top:8px">现有视觉语言预训练模型的对抗攻击多为样本特异性方法，在大规模数据集或新场景中扩展时会产生巨大计算开销。为克服此局限，本文提出层次化精炼攻击——一种面向VLP模型的通用多模态攻击框架。该框架在样本层面与优化层面同步精炼通用对抗扰动：在图像模态中，通过解耦对抗样本为干净图像与扰动分量，使各组件独立处理以更有效破坏跨模态对齐；引入ScMix增强策略以多样化视觉语境，强化通用扰动的全局与局部效用，从而降低对伪特征的依赖。优化路径方面，利用历史梯度与预估未来梯度的时序层次结构避免局部极小值，稳定通用扰动学习。文本模态中，通过融合句内与句间重要性度量识别全局关键词语，并将其作为通用文本扰动。跨下游任务、VLP模型及数据集的广泛实验验证了所提通用多模态攻击的优越性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the computational inefficiency of sample-specific adversarial attacks on vision-language pretraining (VLP) models, this work proposes the Hierarchical Refinement Attack (HRA), a universal multimodal attack framework. The method refines universal adversarial perturbations (UAPs) hierarchically: for images, it disentangles clean images from perturbations and employs a ScMix augmentation strategy to diversify visual contexts and reduce reliance on spurious features, while also using a temporal hierarchy of gradients to stabilize optimization; for text, it identifies globally influential words as universal perturbations. Experimental results across various VLP models, datasets, and downstream tasks demonstrate the framework&#x27;s superior attack performance.</div>
<div class="mono" style="margin-top:8px">针对视觉语言预训练模型样本特异性对抗攻击计算开销大的问题，本研究提出了分层精炼攻击（HRA），一种通用的多模态攻击框架。该方法分层优化通用对抗扰动：对于图像模态，将对抗样本解耦为干净图像和扰动，并采用ScMix增强策略以多样化视觉上下文、减少对虚假特征的依赖，同时利用历史与未来梯度的时序层次来稳定优化；对于文本模态，则通过结合句内和句间重要性度量来识别全局有影响力的词作为通用文本扰动。在多种VLP模型、数据集和下游任务上的广泛实验验证了该框架优越的攻击性能。</div>
</details>
</div>
<div class="card">
<div class="title">A Study of Commonsense Reasoning over Visual Object Properties</div>
<div class="meta-line">Authors: Abhishek Kolari, Mohammadhossein Khojasteh, Yifan Jiang, Floris den Hengst, Filip Ilievski</div>
<div class="meta-line">First: 2025-08-14T11:28:40+00:00 · Latest: 2026-01-15T11:10:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10956v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10956v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by human categorization, object property reasoning involves identifying and recognizing low-level details and higher-level abstractions. While current visual question answering (VQA) studies consider multiple object properties, such as size, they typically blend perception and reasoning and lack representativeness in terms of reasoning and image categories, making it unclear whether and how vision-language models (VLMs) abstract and reason over depicted objects. To this end, we introduce a systematic evaluation framework comprising images of three representative types, three reasoning levels of increasing complexity, and four object property dimensions, informed by prior work on common sense. We develop a procedure to instantiate this framework in two VQA object reasoning benchmarks: OPTICS-CNT, comprising 360 images paired with 1,080 multi-level, count-based questions, and OPTICS-CMP, with 2.1k comparison questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings reveal significant limitations relative to humans, with the best-performing model achieving below 40% counting and 70% comparison accuracy. VLMs struggle particularly with photographic images, counterfactual reasoning, physical and functional properties, and higher counts. We make the OPTICS benchmark data and code available to support future work on scalable benchmarking methods, generalized annotation guidelines, and advanced reasoning VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉物体属性常识推理研究</div>
<div class="mono" style="margin-top:8px">受人类分类能力启发，物体属性推理涉及对底层细节与高层抽象特征的识别与认知。现有视觉问答研究虽考虑尺寸等多重物体属性，但常混淆感知与推理过程，且在推理维度与图像类别上缺乏代表性，导致视觉语言模型对描绘物体的抽象与推理机制尚不明确。为此，我们基于常识研究基础，构建了包含三类代表性图像、三级递进复杂度推理层次及四维物体属性的系统评估框架。通过在两大量化基准（OPTICS-CNT含360张图像与1080道多层级计数问题，OPTICS-CMP含2100道对比问题）中实例化该框架，对12个前沿视觉语言模型进行零样本实验。结果显示：最优模型计数准确率不足40%，对比准确率低于70%，与人类表现存在显著差距；模型在摄影图像、反事实推理、物理功能属性及高数量计数任务中表现尤为薄弱。我们公开OPTICS基准数据与代码，以支持可扩展评测方法、通用标注准则及高阶推理视觉语言模型的后续研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the need to systematically evaluate vision-language models&#x27; (VLMs) ability to perform commonsense reasoning about object properties, as existing visual question answering (VQA) benchmarks often conflate perception with reasoning and lack representativeness. The authors introduce a structured evaluation framework based on three image types, three reasoning levels, and four property dimensions, which they instantiate into two benchmarks: OPTICS-CNT for counting questions and OPTICS-CMP for comparison questions. In zero-shot experiments with 12 state-of-the-art VLMs, results show significant performance gaps compared to humans, with models achieving below 40% accuracy on counting and 70% on comparison tasks, and revealing particular difficulties with photographic images, counterfactual reasoning, and higher-level property abstractions.</div>
<div class="mono" style="margin-top:8px">本研究针对当前视觉问答基准中感知与推理混杂、缺乏代表性的问题，旨在探究视觉语言模型对物体属性的抽象与推理能力。作者提出了一个系统评估框架，包含三种图像类型、三个推理层级和四个属性维度，并实例化为两个基准：OPTICS-CNT用于计数问题，OPTICS-CMP用于比较问题。对12个先进视觉语言模型的零样本实验揭示了显著局限性，最佳模型在计数任务上准确率低于40%，在比较任务上低于70%，尤其在真实照片、反事实推理和高层级属性方面表现困难。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
