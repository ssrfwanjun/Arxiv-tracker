<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-25 06:24</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260125_0624</div>
    <div class="row"><div class="card">
<div class="title">Point Bridge: 3D Representations for Cross Domain Policy Learning</div>
<div class="meta-line">Authors: Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, Yashraj Narang, Ajay Mandlekar</div>
<div class="meta-line">First: 2026-01-22T18:59:24+00:00 · Latest: 2026-01-22T18:59:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pointbridge3d.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Point Bridge：面向跨域策略学习的3D表征方法</div>
<div class="mono" style="margin-top:8px">机器人基础模型正逐步实现通用智能体的愿景，但大规模真实世界操作数据集的稀缺仍制约着发展。仿真与合成数据生成提供了可扩展的替代方案，但其有效性受限于仿真与现实的视觉域差异。本研究提出Point Bridge框架，通过统一的、领域无关的点云表征，实现零样本仿真到现实策略迁移，无需显式的视觉或物体级对齐。该框架融合了基于视觉语言模型的自动化点云表征提取、基于Transformer的策略学习，以及高效的推理流程，仅使用合成数据即可训练出高效的现实操作智能体。结合少量真实示范数据进行协同训练后，Point Bridge性能进一步提升，显著优于现有基于视觉的仿真-现实协同训练方法。在单任务与多任务场景中，零样本仿真到现实迁移性能最高提升44%，结合有限真实数据后最高提升66%。机器人演示视频详见：https://pointbridge3d.github.io/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity of large-scale real-world robot manipulation datasets and the visual domain gap that limits the use of synthetic data, this work introduces Point Bridge, a framework for cross-domain policy learning. The method employs domain-agnostic point-based representations extracted automatically by Vision-Language Models (VLMs), trains policies with transformers on synthetic data, and uses efficient inference pipelines for zero-shot sim-to-real transfer without explicit visual alignment. Experimental results show that Point Bridge achieves up to a 44% improvement in zero-shot sim-to-real transfer performance and, when co-trained with small sets of real demonstrations, attains up to a 66% gain, substantially outperforming prior vision-based co-training methods in both single-task and multitask settings.</div>
<div class="mono" style="margin-top:8px">为解决大规模真实世界机器人操作数据稀缺以及仿真数据因视觉域差异而效用受限的问题，本研究提出了用于跨域策略学习的Point Bridge框架。该方法利用视觉语言模型自动提取统一的、与领域无关的3D点云表示，并在合成数据上训练基于Transformer的策略，从而实现无需显式视觉对齐的零样本仿真到现实迁移。实验结果表明，该框架在零样本迁移性能上最高提升44%，在结合少量真实演示进行协同训练后最高提升66%，在单任务和多任务设置中均显著优于先前的基于视觉的方法。</div>
</details>
</div>
<div class="card">
<div class="title">IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance</div>
<div class="meta-line">Authors: Jongwoo Park, Kanchana Ranasinghe, Jinhyeok Jang, Cristina Mata, Yoo Sung Jang, Michael S Ryoo</div>
<div class="meta-line">First: 2026-01-22T18:57:13+00:00 · Latest: 2026-01-22T18:57:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model&#x27;s built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IVRA：基于无训练提示引导的视觉-令牌关系优化以提升机器人动作策略</div>
<div class="mono" style="margin-top:8px">多数视觉-语言-动作模型将图像块展平为一维令牌序列，削弱了精确操作所需的二维空间线索。本文提出IVRA，一种轻量级、无需训练的方法，通过利用模型内置视觉编码器中已有的亲和性提示来增强空间理解，无需外部编码器或重新训练。IVRA选择性地将这些亲和性信号注入到包含实例级特征的语言模型层中。这种推理时干预能重新校准视觉-令牌的交互，在保持所有模型参数固定的同时更好地保留几何结构。我们通过在多种VLA架构（LLaRA、OpenVLA和FLOWER）上应用IVRA，跨越涵盖2D和3D操作的模拟基准（VIMA和LIBERO）及多种真实机器人任务，证明了其通用性。在2D VIMA任务中，IVRA在低数据场景下较基线LLaRA平均成功率提升4.2%；在3D LIBERO任务中，对OpenVLA和FLOWER基线均带来稳定增益，包括在基线准确率接近饱和时（96.3%至97.1%）的进一步提升。所有代码与模型将公开发布，可视化结果详见：jongwoopark7978.github.io/IVRA</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often lose crucial 2D spatial information when flattening image patches, which hampers precise robot manipulation. To address this, the authors propose IVRA, a training-free method that enhances spatial understanding by injecting affinity hints derived from the model&#x27;s own vision encoder into a specific language-model layer, thereby realigning visual-token interactions without modifying any parameters. Experiments on diverse VLA architectures across 2D and 3D simulated benchmarks and real-robot tasks show consistent improvements, such as a +4.2% average success increase on 2D VIMA with LLaRA and gains from 96.3% to 97.1% on 3D LIBERO with OpenVLA.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型通常将图像块展平为一维序列，这削弱了精确机器人操作所需的二维空间信息。为解决该问题，本文提出了IVRA，这是一种轻量级且无需训练的方法，它通过将模型自身视觉编码器中的亲和性提示注入到特定的语言模型层来增强空间理解，从而在不修改任何参数的情况下重新对齐视觉-标记的交互。在模拟基准（VIMA和LIBERO）和真实机器人任务上的实验结果表明了一致的性能提升；例如，在低数据设置下，IVRA将LLaRA在2D VIMA上的平均成功率提高了4.2%，并将OpenVLA在3D LIBERO上的性能从96.3%提升至97.1%。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning Compensated Model Predictive Control for Off-road Driving on Unknown Deformable Terrain</div>
<div class="meta-line">Authors: Prakhar Gupta, Jonathon M. Smereka, Yunyi Jia</div>
<div class="meta-line">First: 2024-08-17T16:53:51+00:00 · Latest: 2026-01-22T18:38:26+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE Transactions on Intelligent Vehicles as a Regular Paper; was withdrawn in March 2025. A revised version of this manuscript was submitted to ACC 2025 review as a regular paper in Sep 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.09253v2">Abs</a> · <a href="https://arxiv.org/pdf/2408.09253v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents an Actor-Critic reinforcement learning Compensated Model Predictive Controller (AC2MPC) designed for high-speed, off-road autonomous driving on deformable terrains. Addressing the difficulty of modeling unknown tire-terrain interaction and ensuring real-time control feasibility and performance, this framework integrates deep reinforcement learning with a model predictive controller to manage unmodeled nonlinear dynamics. We evaluate the controller framework over constant and varying velocity profiles using high-fidelity simulator Project Chrono. Our findings demonstrate that our controller statistically outperforms standalone model-based and learning-based controllers over three unknown terrains that represent sandy deformable track, sandy and rocky track and cohesive clay-like deformable soil track. Despite varied and previously unseen terrain characteristics, this framework generalized well enough to track longitudinal reference speeds with the least error. Furthermore, this framework required significantly less training data compared to purely learning based controller, converging in fewer steps while delivering better performance. Even when under-trained, this controller outperformed the standalone controllers, highlighting its potential for safer and more efficient real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向未知可变形地形的越野驾驶：强化学习补偿模型预测控制</div>
<div class="mono" style="margin-top:8px">本研究提出一种基于执行器-评判器强化学习的补偿模型预测控制器（AC2MPC），专为可变形地形上的高速越野自动驾驶设计。针对未知轮胎-地形交互建模困难及实时控制可行性问题，该框架将深度强化学习与模型预测控制器结合以处理未建模的非线性动力学。通过高保真仿真器Project Chrono，我们在恒定与变化速度工况下评估该控制器框架。实验表明，在代表沙质可变形路面、沙石混合路面及黏性类黏土可变形土壤路面的三种未知地形上，该控制器在统计意义上优于纯模型控制器与纯学习控制器。面对多样且未见过的地形特征，该框架仍能良好泛化，以最小误差跟踪纵向参考速度。此外，相较于纯学习控制器，本框架所需训练数据显著减少，收敛步数更少且性能更优。即使在训练不足的情况下，该控制器仍优于独立控制器，突显其在实际部署中具备更高安全性与效率的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of controlling autonomous vehicles at high speeds on unknown deformable off-road terrains, where modeling tire-terrain interactions is difficult and real-time performance is critical. The proposed method, an Actor-Critic reinforcement learning Compensated Model Predictive Controller (AC2MPC), integrates deep reinforcement learning with model predictive control to compensate for unmodeled nonlinear dynamics. Experimental evaluation in the high-fidelity Project Chrono simulator across three unknown terrain types (sandy, sandy-rocky, and clay-like) showed that AC2MPC statistically outperformed standalone model-based and learning-based controllers in tracking longitudinal reference speeds with the least error. The framework also demonstrated strong generalization, required significantly less training data than purely learning-based controllers, and maintained superior performance even when under-trained, indicating potential for safer and more efficient real-world deployment.</div>
<div class="mono" style="margin-top:8px">本研究针对未知可变形地形上的自主越野驾驶挑战，该场景中轮胎与地形的复杂相互作用难以精确建模，且需要实时控制。所提出的方法称为演员-评论家强化学习补偿模型预测控制器（AC2MPC），它将深度强化学习与模型预测控制器相结合，以补偿未建模的非线性动力学。在高保真仿真器Project Chrono中对三种未知地形（沙地、沙石混合地、类粘土）进行的实验评估表明，AC2MPC在纵向参考速度跟踪上的误差最小，统计性能显著优于独立的基于模型和基于学习的控制器。该框架对未见过的地形特征表现出良好的泛化能力，所需训练数据远少于纯学习型控制器，即使在训练不足时仍保持优越性能，显示出更安全、高效的实际部署潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</div>
<div class="meta-line">Authors: Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</div>
<div class="meta-line">First: 2026-01-22T18:09:30+00:00 · Latest: 2026-01-22T18:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16163v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16163v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#x27;s latent diffusion process, harnessing the model&#x27;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cosmos策略：面向视觉运动控制与规划的视觉模型微调</div>
<div class="mono" style="margin-top:8px">近期视频生成模型展现出捕捉复杂物理交互与时序场景演变的卓越能力。为利用其时空先验知识，机器人研究领域已采用视频模型进行策略学习，但需通过多阶段后训练及新增动作生成架构组件，引入了复杂性。本研究提出Cosmos策略，这是一种通过单阶段后训练将大型预训练视频模型（Cosmos-Predict2）适配为高效机器人策略的简洁方法：仅使用目标平台采集的机器人演示数据进行训练，无需修改模型架构。该策略通过视频模型的潜在扩散过程直接生成编码为潜在帧的机器人动作，利用模型的预训练先验与核心学习算法捕捉复杂动作分布。此外，Cosmos策略能同步生成编码为潜在帧的未来状态图像与价值函数（预期累积奖励），从而在测试阶段规划更高成功率的动作轨迹。实验评估显示，Cosmos策略在LIBERO与RoboCasa仿真基准测试中分别达到98.5%与67.1%的平均成功率，在现实世界复杂双手操作任务中获得最高平均分，其表现优于从头训练的扩散策略、基于视频模型的策略及相同演示数据微调的先进视觉-语言-动作模型。进一步地，基于策略推演数据，Cosmos策略可通过经验学习优化其世界模型与价值函数，并借助基于模型的规划在复杂任务中实现更高成功率。相关代码、模型与训练数据已发布于https://research.nvidia.com/labs/dir/cosmos-policy/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To leverage the strong spatiotemporal priors of video generation models for robotics without introducing complex multi-stage training or architectural modifications, this work introduces Cosmos Policy, which fine-tunes a pretrained video model (Cosmos-Predict2) directly on robot demonstration data. The method encodes robot actions, future state images, and value predictions as latent frames within the model&#x27;s existing latent diffusion process, enabling it to capture complex action distributions and support test-time planning. Experiments show state-of-the-art performance on LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates) and superior results in real-world bimanual manipulation, outperforming diffusion policies trained from scratch and other fine-tuned models, with additional improvements possible through learning from rollout data to refine its world model and value function.</div>
<div class="mono" style="margin-top:8px">为利用视频生成模型强大的时空先验进行机器人控制，同时避免复杂的多阶段训练或架构修改，本研究提出了Cosmos Policy，该方法直接在机器人演示数据上微调预训练视频模型（Cosmos-Predict2）。该方法将机器人动作、未来状态图像和价值预测编码为模型潜在扩散过程中的潜在帧，从而能够捕捉复杂的动作分布并支持测试时规划。实验表明，该方法在LIBERO和RoboCasa仿真基准上取得了最先进的性能（平均成功率分别为98.5%和67.1%），并在真实世界双手操作任务中获得了最高平均得分，优于从头训练的扩散策略和其他微调模型，且通过从 rollout 数据中学习以改进其世界模型和价值函数，还能实现进一步的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">ProbeMDE: Uncertainty-Guided Active Proprioception for Monocular Depth Estimation in Surgical Robotics</div>
<div class="meta-line">Authors: Britton Jordan, Jordan Thompson, Jesse F. d&#x27;Almeida, Hao Li, Nithesh Kumar, Susheela Sharma Stern, James Ferguson, Ipek Oguz, Robert J. Webster, Daniel Brown, Alan Kuntz</div>
<div class="meta-line">First: 2025-12-12T18:36:53+00:00 · Latest: 2026-01-22T17:44:32+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures. Project page: https://brittonjordan.github.io/probe_mde/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11773v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.11773v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://brittonjordan.github.io/probe_mde/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular depth estimation (MDE) provides a useful tool for robotic perception, but its predictions are often uncertain and inaccurate in challenging environments such as surgical scenes where textureless surfaces, specular reflections, and occlusions are common. To address this, we propose ProbeMDE, a cost-aware active sensing framework that combines RGB images with sparse proprioceptive measurements for MDE. Our approach utilizes an ensemble of MDE models to predict dense depth maps conditioned on both RGB images and on a sparse set of known depth measurements obtained via proprioception, where the robot has touched the environment in a known configuration. We quantify predictive uncertainty via the ensemble&#x27;s variance and measure the gradient of the uncertainty with respect to candidate measurement locations. To prevent mode collapse while selecting maximally informative locations to propriocept (touch), we leverage Stein Variational Gradient Descent (SVGD) over this gradient map. We validate our method in both simulated and physical experiments on central airway obstruction surgical phantoms. Our results demonstrate that our approach outperforms baseline methods across standard depth estimation metrics, achieving higher accuracy while minimizing the number of required proprioceptive measurements.
  Project page: https://brittonjordan.github.io/probe_mde/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProbeMDE：面向手术机器人单目深度估计的不确定性引导主动本体感知</div>
<div class="mono" style="margin-top:8px">单目深度估计（MDE）为机器人感知提供了有效工具，但其预测在手术场景等挑战性环境中常因纹理缺失表面、镜面反射和遮挡而存在不确定性与误差。为此，我们提出ProbeMDE——一种成本感知的主动感知框架，将RGB图像与稀疏本体感知测量相结合用于MDE。该方法通过集成MDE模型，基于RGB图像及通过本体感知获取的稀疏已知深度测量值（即机器人在已知构型下接触环境所得）预测稠密深度图。我们通过集成模型的方差量化预测不确定性，并计算不确定性相对于候选测量位置的梯度。为避免选择最具信息量的本体感知（接触）位置时出现模式崩溃，我们在梯度图上采用斯坦因变分梯度下降法。通过在中央气道阻塞手术体模的仿真与实体实验验证，结果表明该方法在标准深度估计指标上均优于基线方法，能以更少的本体感知测量次数实现更高精度。项目页面：https://brittonjordan.github.io/probe_mde/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Monocular depth estimation (MDE) in surgical robotics is challenged by textureless surfaces and specular reflections, leading to uncertain and inaccurate predictions. To address this, ProbeMDE introduces an active sensing framework that fuses RGB images with sparse, actively chosen proprioceptive depth measurements. The method uses an ensemble of MDE models to predict dense depth, quantifies uncertainty via ensemble variance, and selects optimal touch locations by applying Stein Variational Gradient Descent (SVGD) to the gradient of uncertainty. Experiments on surgical airway phantoms show the approach outperforms baselines in depth accuracy metrics while minimizing the number of required proprioceptive probes.</div>
<div class="mono" style="margin-top:8px">在手术机器人中，单目深度估计（MDE）因无纹理表面和镜面反射而面临预测不确定和不准确的挑战。为此，ProbeMDE提出了一种主动感知框架，将RGB图像与稀疏的本体感知深度测量相结合，利用MDE模型集成来预测基于这两种数据源的密集深度图。该方法通过集成方差量化预测不确定性，并通过对候选点的不确定性梯度应用Stein变分梯度下降（SVGD）来选择最佳触摸位置。在模拟和物理气道阻塞模型上的实验表明，ProbeMDE在标准深度指标上优于基线方法，以更少的本体感知测量实现了更高的精度。</div>
</details>
</div>
<div class="card">
<div class="title">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</div>
<div class="meta-line">Authors: Shijie Lian, Bin Yu, Xiaopeng Lin, Laurence T. Yang, Zhaolong Shen, Changti Wu, Yuzhuo Miao, Cong Huang, Kai Chen</div>
<div class="meta-line">First: 2026-01-21T17:15:22+00:00 · Latest: 2026-01-22T17:01:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15197v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15197v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BayesianVLA：基于潜在动作查询的视觉语言动作模型贝叶斯分解</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型在机器人操作中展现出潜力，但常难以泛化至新指令或复杂多任务场景。我们指出当前训练范式中存在一个关键缺陷：目标驱动的数据收集导致数据集偏差。在此类数据集中，仅凭视觉观测即可高度预测语言指令，致使指令与动作间的条件互信息趋近于零，这一现象我们称为“信息坍缩”。因此，模型退化为仅依赖视觉的策略，忽略语言约束并在分布外（OOD）场景中失效。为解决此问题，我们提出BayesianVLA，一种通过贝叶斯分解强制遵循指令的新框架。通过引入可学习的潜在动作查询，我们构建双分支架构以同时估计仅视觉先验$p(a \mid v)$和语言条件后验$π(a \mid v, \ell)$，进而优化策略以最大化动作与指令间的条件点互信息（PMI）。该目标有效惩罚视觉捷径，并奖励能显式解释语言命令的动作。无需新增数据，BayesianVLA显著提升了泛化能力。在SimplerEnv和RoboCasa上的大量实验证明了其显著优势，如在挑战性OOD基准SimplerEnv上实现11.3%的性能提升，验证了本方法在动作中稳健关联语言的能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action models often fail to generalize due to a dataset bias where language instructions become predictable from visual observations, leading to an Information Collapse that degrades models into vision-only policies. To counteract this, BayesianVLA introduces a Bayesian decomposition framework using learnable Latent Action Queries to separately model a vision-only prior and a language-conditioned posterior, optimizing the policy to maximize the conditional Pointwise Mutual Information between actions and instructions. Experiments on SimplerEnv and RoboCasa show the method substantially improves generalization without new data, achieving an 11.3% performance gain on a challenging out-of-distribution benchmark.</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型常因数据集偏差导致泛化能力不足，其中语言指令可从视觉观察中预测，引发信息坍缩，使模型退化为仅依赖视觉的策略。为解决该问题，本研究提出BayesianVLA框架，通过引入可学习的潜在动作查询构建双分支架构，分别估计仅视觉先验和语言条件后验，并优化策略以最大化动作与指令间的条件点互信息，从而惩罚视觉捷径。在SimplerEnv和RoboCasa基准上的大量实验表明，该方法显著提升了泛化性能，如在具有挑战性的分布外任务上获得了11.3%的性能提升，验证了其能有效将语言约束落实到动作中。</div>
</details>
</div>
<div class="card">
<div class="title">Efficiently Learning Robust Torque-based Locomotion Through Reinforcement with Model-Based Supervision</div>
<div class="meta-line">Authors: Yashuai Yan, Tobias Egle, Christian Ott, Dongheui Lee</div>
<div class="meta-line">First: 2026-01-22T16:56:52+00:00 · Latest: 2026-01-22T16:56:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16109v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16109v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a control framework that integrates model-based bipedal locomotion with residual reinforcement learning (RL) to achieve robust and adaptive walking in the presence of real-world uncertainties. Our approach leverages a model-based controller, comprising a Divergent Component of Motion (DCM) trajectory planner and a whole-body controller, as a reliable base policy. To address the uncertainties of inaccurate dynamics modeling and sensor noise, we introduce a residual policy trained through RL with domain randomization. Crucially, we employ a model-based oracle policy, which has privileged access to ground-truth dynamics during training, to supervise the residual policy via a novel supervised loss. This supervision enables the policy to efficiently learn corrective behaviors that compensate for unmodeled effects without extensive reward shaping. Our method demonstrates improved robustness and generalization across a range of randomized conditions, offering a scalable solution for sim-to-real transfer in bipedal locomotion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过基于模型监督的强化学习高效掌握鲁棒的基于扭矩的步态控制</div>
<div class="mono" style="margin-top:8px">我们提出一种控制框架，将基于模型的双足步态控制与残差强化学习相结合，以在现实世界不确定性条件下实现鲁棒且自适应的行走。该方法以包含发散运动分量轨迹规划器和全身控制器的模型控制器作为可靠基础策略。为应对动力学建模误差和传感器噪声等不确定性，我们引入通过领域随机化训练的残差策略。关键创新在于：训练时采用具备真实动力学特权的模型先知策略，通过新型监督损失指导残差策略。这种监督机制使策略能高效学习补偿未建模效应的校正行为，无需复杂奖励函数设计。实验表明，该方法在多种随机条件下均展现出更强的鲁棒性与泛化能力，为双足步态的仿真到现实迁移提供了可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To achieve robust bipedal walking under real-world uncertainties, this work integrates model-based control with residual reinforcement learning. The method uses a model-based controller as a base policy and trains a residual RL policy with domain randomization; a key innovation is supervising this residual policy via a model-based oracle with privileged ground-truth dynamics to efficiently learn corrective behaviors. Experiments show the approach improves robustness and generalization across randomized conditions, facilitating sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">为实现双足机器人在真实世界不确定性（如不精确的动力学模型和传感器噪声）下的鲁棒行走，本研究将基于模型的控制器与残差强化学习相结合。该方法利用一个具有真实动力学信息的模型预言机，通过一种新颖的监督损失来指导残差强化学习策略，从而无需复杂的奖励设计即可高效学习补偿未建模效应的校正行为。实验结果表明，该方法在一系列随机化条件下提升了鲁棒性和泛化能力，为扭矩控制双足行走的仿真到现实迁移提供了一个可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Application</div>
<div class="meta-line">Authors: Jiarui Cui, Maosong Wang, Wenqi Wu, Peiqi Li, Xianfei Pan</div>
<div class="meta-line">First: 2026-01-22T16:21:18+00:00 · Latest: 2026-01-22T16:21:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16078v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the core advantages of SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. In the previous paper, the theoretical analysis of autonomy property of navigation model in inertial, earth and world frames was given. A construction method for SE2(3) group navigation model is proposed to improve the non-inertial navigation model toward full autonomy. This paper serves as a counterpart to previous paper and conducts the real-world strapdown inertial navigation system (SINS)/odometer(ODO) experiments as well as Monte-Carlo simulations to demonstrate the performance of improved SE2(3) group based high-precision navigation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升基于SE2(3)群的扩展卡尔曼滤波组合导航自主性：应用研究</div>
<div class="mono" style="margin-top:8px">SE2(3)李群导航建模框架的核心优势之一在于误差传播的自主性。前期论文已从理论层面分析了惯性系、地球系与世界系中导航模型的自主特性，并提出一种SE2(3)群导航模型构建方法，以提升非惯性导航模型至完全自主。本文作为前期研究的实践对应，通过实际车载捷联惯性导航系统(SINS)/里程计(ODO)实验与蒙特卡洛仿真，验证改进型SE2(3)群高精度导航模型的性能表现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work aims to enhance the autonomy of navigation models within the SE2(3) Lie group framework, building on prior theoretical analysis of error propagation autonomy in inertial, earth, and world frames. The method involves constructing an improved SE2(3) group navigation model to achieve full autonomy, specifically refining non-inertial navigation models. Experimental validation through real-world SINS/odometer tests and Monte-Carlo simulations demonstrates the performance of these enhanced high-precision navigation models.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升SE2(3)李群框架中导航模型的自主性，基于先前对惯性系、地球系和世界系中误差传播自主性的理论分析。所提出的方法通过构建改进的SE2(3)群导航模型，以完善非惯性导航模型，从而实现完全自主。通过实际的车载惯性导航系统/里程计实验以及蒙特卡洛仿真验证，结果表明改进后的模型能够实现高性能的精密导航。</div>
</details>
</div>
<div class="card">
<div class="title">DTP: A Simple yet Effective Distracting Token Pruning Framework for Vision-Language Action Models</div>
<div class="meta-line">Authors: Chenyang Li, Jieyuan Liu, Bin Li, Bo Gao, Yilin Yuan, Yangfan He, Yuchen Li, Jingqun Tang</div>
<div class="meta-line">First: 2026-01-22T16:02:56+00:00 · Latest: 2026-01-22T16:02:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16065v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16065v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Action (VLA) models have shown remarkable progress in robotic manipulation by leveraging the powerful perception abilities of Vision-Language Models (VLMs) to understand environments and directly output actions. However, by default, VLA models may overly attend to image tokens in the task-irrelevant region, which we describe as &#x27;distracting tokens&#x27;. This behavior can disturb the model from the generation of the desired action tokens in each step, affecting the success rate of tasks. In this paper, we introduce a simple yet effective plug-and-play Distracting Token Pruning (DTP) framework, which dynamically detects and prunes these distracting image tokens. By correcting the model&#x27;s visual attention patterns, we aim to improve the task success rate, as well as exploring the performance upper boundaries of the model without altering its original architecture or adding additional inputs. Experiments on the SIMPLER Benchmark (Li et al., 2024) show that our method consistently achieving relative improvements in task success rates across different types of novel VLA models, demonstrating generalizability to transformer-based VLAs. Further analysis reveals a negative correlation between the task success rate and the amount of attentions in the task-irrelevant region for all models tested, highlighting a common phenomenon of VLA models that could guide future research. We also publish our code at: https://anonymous.4open.science/r/CBD3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DTP：一种面向视觉语言动作模型的简洁高效干扰令牌剪枝框架</div>
<div class="mono" style="margin-top:8px">视觉语言动作（VLA）模型通过利用视觉语言模型（VLM）强大的感知能力理解环境并直接输出动作，在机器人操作领域取得了显著进展。然而，默认情况下，VLA模型可能过度关注任务无关区域的图像令牌（即“干扰令牌”），这会干扰模型在每一步生成预期动作令牌的过程，从而影响任务成功率。本文提出了一种简洁高效的即插即用干扰令牌剪枝（DTP）框架，能够动态检测并剪除这些干扰图像令牌。通过修正模型的视觉注意力模式，我们的目标是在不改变原始架构或增加额外输入的前提下，提升任务成功率，并探索模型的性能上限。在SIMPLER基准测试（Li等人，2024）上的实验表明，该方法在不同类型的新型VLA模型中均能持续提升任务成功率，显示出对基于Transformer的VLA模型的普适性。进一步分析揭示，所有测试模型的任务成功率与任务无关区域的注意力强度均呈负相关，这凸显了VLA模型的共性现象，可为未来研究提供指引。代码已发布于：https://anonymous.4open.science/r/CBD3。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language Action (VLA) models for robotic manipulation can be disrupted by over-attending to irrelevant image regions, termed &#x27;distracting tokens&#x27;, which lowers task success. To address this, the authors propose a plug-and-play Distracting Token Pruning (DTP) framework that dynamically identifies and removes these tokens to correct visual attention without modifying the model architecture. Experiments on the SIMPLER benchmark show consistent relative improvements in success rates across various transformer-based VLA models, with analysis confirming a negative correlation between attention to irrelevant regions and task performance.</div>
<div class="mono" style="margin-top:8px">用于机器人操作的视觉-语言-动作（VLA）模型可能会过度关注图像中不相关的区域（即“干扰令牌”），从而降低任务成功率。为此，研究者提出了一种即插即用的干扰令牌剪枝（DTP）框架，该框架能动态识别并移除这些令牌，从而在不改变模型架构的情况下修正视觉注意力模式。在SIMPLER基准测试上的实验表明，该方法在不同类型的基于Transformer的VLA模型中均能持续提升任务成功率，进一步分析揭示了成功率与模型在任务无关区域上的注意力呈负相关关系。</div>
</details>
</div>
<div class="card">
<div class="title">Improve the autonomy of the SE2(3) group based Extended Kalman Filter for Integrated Navigation: Theoretical Analysis</div>
<div class="meta-line">Authors: Jiarui Cui, Maosong Wang, Wenqi Wu, Peiqi Li, Xianfei Pan</div>
<div class="meta-line">First: 2026-01-22T15:58:56+00:00 · Latest: 2026-01-22T15:58:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16062v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of core advantages of the SE2(3) Lie group framework for navigation modeling lies in the autonomy of error propagation. Current research on Lie group based extended Kalman filters has demonstrated that error propagation autonomy holds in low-precision applications, such as in micro electromechanical system (MEMS) based integrated navigation without considering earth rotation and inertial device biases. However, in high-precision navigation state estimation, maintaining autonomy is extremely difficult when considering with earth rotation and inertial device biases. This paper presents the theoretical analysis on the autonomy of SE2(3) group based high-precision navigation models under inertial, earth and world frame respectively. Through theoretical analysis, we find that the limitation of the traditional, trivial SE2(3) group navigation modeling method is that the presence of Coriolis force terms introduced by velocity in non-inertial frame. Therefore, a construction method for SE2(3) group navigation models is proposed, which brings the navigation models closer to full autonomy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>提升基于SE2(3)群的组合导航扩展卡尔曼滤波器自主性：理论分析</div>
<div class="mono" style="margin-top:8px">SE2(3)李群框架在导航建模中的核心优势之一在于误差传播的自主性。当前基于李群的扩展卡尔曼滤波器研究表明，误差传播自主性在低精度应用中（如不考虑地球自转和惯性器件偏差的微机电系统组合导航）得以保持。然而，在高精度导航状态估计中，当考虑地球自转和惯性器件偏差时，维持自主性极为困难。本文分别针对惯性系、地球系与世界系下的SE2(3)群高精度导航模型自主性进行了理论分析。通过理论分析发现，传统简易SE2(3)群导航建模方法的局限在于非惯性系中速度引入的科里奥利力项的存在。为此，提出了一种SE2(3)群导航模型的构建方法，使导航模型更趋近于完全自主。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to address the challenge of maintaining error propagation autonomy in high-precision integrated navigation when using the SE2(3) Lie group framework, which is crucial for robust state estimation but difficult to achieve when accounting for earth rotation and inertial device biases. The method involves a theoretical analysis of SE2(3)-based models under inertial, earth, and world frames, identifying that the Coriolis force terms introduced by velocity in non-inertial frames limit autonomy in traditional modeling. The key finding is that a newly proposed construction method for SE2(3) group navigation models overcomes this limitation by bringing the models closer to full autonomy.</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于SE2(3)李群的扩展卡尔曼滤波器在高精度组合导航中误差传播自主性的问题，传统方法在考虑地球旋转和惯性器件偏差时难以维持自主性。通过对惯性系、地球系和世界系下导航模型的理论分析，研究发现传统SE2(3)群建模方法的局限性在于非惯性系中速度引入的科里奥利力项。为此，提出了一种SE2(3)群导航模型的构建方法，该方法使导航模型更接近完全自主，理论分析结果支持了这一改进在高精度应用中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">DextER: Language-driven Dexterous Grasp Generation with Embodied Reasoning</div>
<div class="meta-line">Authors: Junha Lee, Eunha Park, Minsu Cho</div>
<div class="meta-line">First: 2026-01-22T15:23:35+00:00 · Latest: 2026-01-22T15:23:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16046v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16046v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language-driven dexterous grasp generation requires the models to understand task semantics, 3D geometry, and complex hand-object interactions. While vision-language models have been applied to this problem, existing approaches directly map observations to grasp parameters without intermediate reasoning about physical interactions. We present DextER, Dexterous Grasp Generation with Embodied Reasoning, which introduces contact-based embodied reasoning for multi-finger manipulation. Our key insight is that predicting which hand links contact where on the object surface provides an embodiment-aware intermediate representation bridging task semantics with physical constraints. DextER autoregressively generates embodied contact tokens specifying which finger links contact where on the object surface, followed by grasp tokens encoding the hand configuration. On DexGYS, DextER achieves 67.14% success rate, outperforming state-of-the-art by 3.83%p with 96.4% improvement in intention alignment. We also demonstrate steerable generation through partial contact specification, providing fine-grained control over grasp synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DextER：基于具身推理的语言驱动灵巧抓取生成</div>
<div class="mono" style="margin-top:8px">语言驱动的灵巧抓取生成要求模型理解任务语义、三维几何及复杂的手-物交互。尽管视觉-语言模型已应用于此问题，现有方法直接将观测映射至抓取参数，缺乏对物理交互的中间推理。本文提出DextER（基于具身推理的灵巧抓取生成），通过引入基于接触的具身推理实现多指操控。核心洞见在于：预测手部哪些连杆接触物体表面何处，可形成具身感知的中间表征，从而连接任务语义与物理约束。DextER通过自回归生成具身接触令牌（指定手指连杆与物体表面的接触位点），再生成编码手部构型的抓取令牌。在DexGYS数据集上，DextER达成67.14%成功率，较最优方法提升3.83个百分点，意图对齐度提升96.4%。实验还展示了通过部分接触规约实现可引导生成，为抓取合成提供细粒度控制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of generating dexterous grasps from language instructions, which requires understanding task semantics, 3D geometry, and complex hand-object interactions. The proposed method, DextER, introduces an embodied reasoning approach that autoregressively generates intermediate contact tokens specifying which finger links contact specific object surfaces, followed by tokens encoding the full hand configuration. Experimental results on the DexGYS benchmark show the model achieves a 67.14% success rate, outperforming the previous state-of-the-art by 3.83 percentage points and improving intention alignment by 96.4%, while also enabling steerable generation through partial contact specification.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决根据语言指令生成灵巧抓取姿态的挑战，这需要理解任务语义、三维几何和复杂的手-物体交互。提出的DextER方法引入了一种具身推理方法，自回归地生成中间接触标记，指定哪些手指连杆接触物体表面的特定位置，然后编码完整手部配置的标记。在DexGYS基准测试上的实验结果表明，该模型实现了67.14%的成功率，比之前的最先进方法高出3.83个百分点，并将意图对齐提高了96.4%，同时还能通过部分接触指定实现可引导的生成。</div>
</details>
</div>
<div class="card">
<div class="title">Collision-Free Humanoid Traversal in Cluttered Indoor Scenes</div>
<div class="meta-line">Authors: Han Xue, Sikai Liang, Zhikai Zhang, Zicheng Zeng, Yun Liu, Yunrui Lian, Jilong Wang, Qingtao Liu, Xuesong Shi, Li Yi</div>
<div class="meta-line">First: 2026-01-22T15:08:53+00:00 · Latest: 2026-01-22T15:08:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16035v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://axian12138.github.io/CAT/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of collision-free humanoid traversal in cluttered indoor scenes, such as hurdling over objects scattered on the floor, crouching under low-hanging obstacles, or squeezing through narrow passages. To achieve this goal, the humanoid needs to map its perception of surrounding obstacles with diverse spatial layouts and geometries to the corresponding traversal skills. However, the lack of an effective representation that captures humanoid-obstacle relationships during collision avoidance makes directly learning such mappings difficult. We therefore propose Humanoid Potential Field (HumanoidPF), which encodes these relationships as collision-free motion directions, significantly facilitating RL-based traversal skill learning. We also find that HumanoidPF exhibits a surprisingly negligible sim-to-real gap as a perceptual representation. To further enable generalizable traversal skills through diverse and challenging cluttered indoor scenes, we further propose a hybrid scene generation method, incorporating crops of realistic 3D indoor scenes and procedurally synthesized obstacles. We successfully transfer our policy to the real world and develop a teleoperation system where users could command the humanoid to traverse in cluttered indoor scenes with just a single click. Extensive experiments are conducted in both simulation and the real world to validate the effectiveness of our method. Demos and code can be found in our website: https://axian12138.github.io/CAT/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>杂乱室内场景中的无碰撞人形机器人穿越</div>
<div class="mono" style="margin-top:8px">本研究探讨了人形机器人在杂乱室内场景（如跨越地面散落物体、蹲伏通过低矮障碍或挤过狭窄通道）中实现无碰撞穿越的问题。为实现这一目标，机器人需将感知到的具有多样空间布局与几何形态的周围障碍物映射至相应的穿越技能。然而，由于缺乏能有效捕捉避碰过程中人形-障碍物关系的表征，直接学习此类映射较为困难。为此，我们提出人形势场（HumanoidPF），将此类关系编码为无碰撞运动方向，显著促进了基于强化学习的穿越技能学习。我们还发现，HumanoidPF作为一种感知表征展现出可忽略的仿真-现实差距。为进一步获得能泛化至多样复杂杂乱室内场景的穿越技能，我们提出混合场景生成方法，融合真实三维室内场景片段与程序化生成的障碍物。我们成功将策略迁移至现实世界，并开发了遥操作系统，用户仅需点击即可指挥人形机器人在杂乱室内场景中穿越。通过大量仿真与现实实验验证了方法的有效性。演示与代码详见项目网站：https://axian12138.github.io/CAT/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling humanoid robots to navigate cluttered indoor environments by hurdling over, crouching under, or squeezing through obstacles. The core method is the Humanoid Potential Field (HumanoidPF), a representation that encodes humanoid-obstacle relationships as collision-free motion directions to facilitate reinforcement learning of traversal skills, coupled with a hybrid scene generation technique using realistic 3D scene crops and procedurally synthesized obstacles for training. Experimental results demonstrate that the learned policy successfully transfers to a real-world humanoid with a minimal sim-to-real gap, and a teleoperation system allows users to command traversal with a single click.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决人形机器人在杂乱室内场景（如跨越地面物体、蹲伏通过低矮障碍或挤过狭窄通道）中的无碰撞穿越问题。其核心方法是提出人形势场（HumanoidPF），该表示将人形与障碍物的关系编码为无碰撞运动方向，从而显著促进了基于强化学习的穿越技能学习。实验结果表明，基于HumanoidPF学习的策略能够以极小的仿真到现实差距成功迁移到真实人形机器人上，并且所提出的混合场景生成方法结合了真实场景片段与程序化障碍物，使技能能泛化至多样化的挑战性场景，实现了单点击遥操作穿越。</div>
</details>
</div>
<div class="card">
<div class="title">Keyframe-Based Feed-Forward Visual Odometry</div>
<div class="meta-line">Authors: Weichen Dai, Wenhan Su, Da Kong, Yuhang Ming, Wanzeng Kong</div>
<div class="meta-line">First: 2026-01-22T14:45:42+00:00 · Latest: 2026-01-22T14:45:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16020v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The emergence of visual foundation models has revolutionized visual odometry~(VO) and SLAM, enabling pose estimation and dense reconstruction within a single feed-forward network. However, unlike traditional pipelines that leverage keyframe methods to enhance efficiency and accuracy, current foundation model based methods, such as VGGT-Long, typically process raw image sequences indiscriminately. This leads to computational redundancy and degraded performance caused by low inter-frame parallax, which provides limited contextual stereo information. Integrating traditional geometric heuristics into these methods is non-trivial, as their performance depends on high-dimensional latent representations rather than explicit geometric metrics. To bridge this gap, we propose a novel keyframe-based feed-forward VO. Instead of relying on hand-crafted rules, our approach employs reinforcement learning to derive an adaptive keyframe policy in a data-driven manner, aligning selection with the intrinsic characteristics of the underlying foundation model. We train our agent on TartanAir dataset and conduct extensive evaluations across several real-world datasets. Experimental results demonstrate that the proposed method achieves consistent and substantial improvements over state-of-the-art feed-forward VO methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于关键帧的前馈视觉里程计</div>
<div class="mono" style="margin-top:8px">视觉基础模型的出现革新了视觉里程计（VO）与SLAM，使得姿态估计与密集重建能在单一前馈网络中实现。然而，与利用关键帧方法提升效率与精度的传统流程不同，当前基于基础模型的方法（如VGGT-Long）通常不加区分地处理原始图像序列，导致计算冗余及因帧间视差较低（仅提供有限立体上下文信息）而引发的性能下降。将传统几何启发式方法融入这些模型并非易事，因其性能依赖于高维潜在表征而非显式几何度量。为弥合此差距，我们提出一种新颖的基于关键帧的前馈视觉里程计方法。该方法摒弃手工规则，采用强化学习以数据驱动方式推导自适应关键帧策略，使选择与底层基础模型的内在特性对齐。我们在TartanAir数据集上训练智能体，并在多个真实世界数据集上进行广泛评估。实验结果表明，所提方法相较于最先进的前馈视觉里程计方法实现了持续且显著的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the computational redundancy and performance degradation in visual foundation model-based visual odometry (VO) methods, which process all video frames indiscriminately unlike traditional keyframe-based pipelines. The proposed method introduces a keyframe selection mechanism for feed-forward VO, using reinforcement learning to learn an adaptive keyframe policy that aligns with the model&#x27;s latent representations instead of relying on hand-crafted geometric rules. Experiments on real-world datasets show that this approach yields consistent and substantial performance improvements over state-of-the-art feed-forward VO methods.</div>
<div class="mono" style="margin-top:8px">本研究针对当前基于视觉基础模型的前馈视觉里程计方法计算冗余且性能下降的问题，这些方法不加区分地处理所有帧，不同于传统的关键帧流程。所提出的方法引入了一种基于关键帧的前馈方法，利用强化学习来学习自适应的关键帧选择策略，使其与模型的潜在表征对齐，而非依赖手工设计的几何规则。在真实世界数据集上的实验表明，该方法相比最先进的前馈视觉里程计技术取得了持续且显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">PUMA: Perception-driven Unified Foothold Prior for Mobility Augmented Quadruped Parkour</div>
<div class="meta-line">Authors: Liang Wang, Kanzhong Yao, Yang Liu, Weikai Qin, Jun Wu, Zhe Sun, Qiuguo Zhu</div>
<div class="meta-line">First: 2026-01-22T14:16:12+00:00 · Latest: 2026-01-22T14:16:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15995v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15995v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parkour tasks for quadrupeds have emerged as a promising benchmark for agile locomotion. While human athletes can effectively perceive environmental characteristics to select appropriate footholds for obstacle traversal, endowing legged robots with similar perceptual reasoning remains a significant challenge. Existing methods often rely on hierarchical controllers that follow pre-computed footholds, thereby constraining the robot&#x27;s real-time adaptability and the exploratory potential of reinforcement learning. To overcome these challenges, we present PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process. This approach leverages terrain features to estimate egocentric polar foothold priors, composed of relative distance and heading, guiding the robot in active posture adaptation for parkour tasks. Extensive experiments conducted in simulation and real-world environments across various discrete complex terrains, demonstrate PUMA&#x27;s exceptional agility and robustness in challenging scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PUMA：面向移动增强型四足跑酷的感知驱动统一落脚点先验</div>
<div class="mono" style="margin-top:8px">四足机器人的跑酷任务已成为敏捷运动能力的重要测试基准。人类运动员能通过感知环境特征选择合适落脚点以跨越障碍，但赋予腿式机器人同等感知推理能力仍面临巨大挑战。现有方法多依赖遵循预计算落脚点的分层控制器，限制了机器人的实时适应性与强化学习的探索潜力。为突破这些局限，我们提出PUMA——一种将视觉感知与落脚点先验整合至单阶段训练过程的端到端学习框架。该方法利用地形特征估计以自我为中心的极坐标落脚点先验（包含相对距离与航向角），引导机器人在跑酷任务中主动调整姿态。通过在仿真与真实环境中对多种离散复杂地形进行大量实验，验证了PUMA在挑战性场景中卓越的敏捷性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of enabling legged robots to perform agile parkour with real-time perceptual reasoning akin to human athletes, as existing hierarchical controllers that rely on pre-computed footholds limit adaptability and reinforcement learning exploration. The method introduces PUMA, an end-to-end learning framework that integrates visual perception and foothold priors into a single-stage training process, using terrain features to estimate egocentric polar foothold priors for guiding active posture adaptation. Experimental results from simulation and real-world tests across various discrete complex terrains demonstrate PUMA&#x27;s exceptional agility and robustness in challenging parkour scenarios.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决四足机器人自主感知环境以选择落脚点进行敏捷跑酷的挑战，现有基于预计算落脚点的分层控制器限制了机器人的实时适应能力。提出的PUMA方法是一个端到端学习框架，将视觉感知与落脚点先验集成到单阶段训练中，利用地形特征估计以机器人为中心的极坐标落脚点先验（距离和朝向），从而引导机器人进行主动姿态调整。在多种离散复杂地形的仿真和真实环境实验中，结果表明PUMA在具有挑战性的场景中表现出卓越的敏捷性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Accurate Calibration and Robust LiDAR-Inertial Odometry for Spinning Actuated LiDAR Systems</div>
<div class="meta-line">Authors: Zijie Chen, Xiaowei Liu, Yong Xu, Shenghai Yuan, Jianping Li, Lihua Xie</div>
<div class="meta-line">First: 2026-01-22T13:28:09+00:00 · Latest: 2026-01-22T13:28:09+00:00</div>
<div class="meta-line">Comments: This article has been accepted for publication in IEEE Robotics and Automation Letters (RA-L). Personal use is permitted. All other uses require IEEE permission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15946v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15946v1">PDF</a> · <a href="https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr">Code1</a> · <a href="https://github.com/zijiechenrobotics/lm_calibr">Code2</a> · <a href="http://github.com/zijiechenrobotics/lm">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate calibration and robust localization are fundamental for downstream tasks in spinning actuated LiDAR applications. Existing methods, however, require parameterizing extrinsic parameters based on different mounting configurations, limiting their generalizability. Additionally, spinning actuated LiDAR inevitably scans featureless regions, which complicates the balance between scanning coverage and localization robustness. To address these challenges, this letter presents a targetless LiDAR-motor calibration (LM-Calibr) on the basis of the Denavit-Hartenberg convention and an environmental adaptive LiDAR-inertial odometry (EVA-LIO). LM-Calibr supports calibration of LiDAR-motor systems with various mounting configurations. Extensive experiments demonstrate its accuracy and convergence across different scenarios, mounting angles, and initial values. Additionally, EVA-LIO adaptively selects downsample rates and map resolutions according to spatial scale. This adaptivity enables the actuator to operate at maximum speed, thereby enhancing scanning completeness while ensuring robust localization, even when LiDAR briefly scans featureless areas. The source code and hardware design are available on GitHub: \textcolor{blue}{\href{https://github.com/zijiechenrobotics/lm_calibr}{github.com/zijiechenrobotics/lm\_calibr}}. The video is available at \textcolor{blue}{\href{https://youtu.be/cZyyrkmeoSk}{youtu.be/cZyyrkmeoSk}}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>旋转驱动激光雷达系统的精确标定与鲁棒激光雷达惯性里程计</div>
<div class="mono" style="margin-top:8px">精确标定与鲁棒定位是旋转驱动激光雷达应用下游任务的基础。然而，现有方法需根据不同安装构型对外参进行参数化，限制了其泛化能力。此外，旋转驱动激光雷达不可避免地会扫描特征缺失区域，这使扫描覆盖范围与定位鲁棒性之间的平衡复杂化。为应对这些挑战，本文基于Denavit-Hartenberg约定提出无靶标激光雷达-电机标定方法（LM-Calibr）以及环境自适应激光雷达惯性里程计（EVA-LIO）。LM-Calibr支持多种安装构型的激光雷达-电机系统标定。大量实验验证了其在不同场景、安装角度和初始值下的精度与收敛性。此外，EVA-LIO根据空间尺度自适应选择下采样率与地图分辨率。这种自适应性使驱动器能以最高速度运行，从而在激光雷达短暂扫描特征缺失区域时，既能提升扫描完整性，又能确保鲁棒定位。源代码与硬件设计已发布于GitHub：https://github.com/zijiechenrobotics/lm_calibr。演示视频可见：https://youtu.be/cZyyrkmeoSk</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for accurate calibration and robust localization in spinning actuated LiDAR systems, where existing methods lack generalizability across mounting configurations and struggle with featureless regions. The proposed method introduces a targetless LiDAR-motor calibration (LM-Calibr) based on the Denavit-Hartenberg convention for flexible extrinsic parameter estimation, and an environmental adaptive LiDAR-inertial odometry (EVA-LIO) that dynamically adjusts downsample rates and map resolutions. Experimental results show that LM-Calibr achieves accurate and convergent calibration across various scenarios and initial values, while EVA-LIO enables maximum actuator speed for improved scanning coverage and maintains robust localization even in feature-deficient areas.</div>
<div class="mono" style="margin-top:8px">本研究针对旋转驱动激光雷达系统，旨在解决其精确标定与鲁棒定位的挑战，现有方法因对不同安装配置的泛化能力不足，且在特征缺失区域易影响鲁棒性。提出的解决方案包括：基于Denavit-Hartenberg约定的无目标激光雷达-电机标定方法（LM-Calibr），以处理多种安装配置；以及环境自适应激光雷达-惯性里程计（EVA-LIO），可根据空间尺度动态调整下采样率和地图分辨率。实验结果表明，LM-Calibr在不同场景、安装角度和初始值下均实现了精确且收敛的标定，而EVA-LIO使驱动器能以最大速度运行，提升了扫描完整性，并在特征缺失区域仍能保持鲁棒的定位性能。</div>
</details>
</div>
<div class="card">
<div class="title">TeNet: Text-to-Network for Compact Policy Synthesis</div>
<div class="meta-line">Authors: Ariyan Bighashdel, Kevin Sebastian Luck</div>
<div class="meta-line">First: 2026-01-22T12:42:30+00:00 · Latest: 2026-01-22T12:42:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15912v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15912v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robots that follow natural-language instructions often either plan at a high level using hand-designed interfaces or rely on large end-to-end models that are difficult to deploy for real-time control. We propose TeNet (Text-to-Network), a framework for instantiating compact, task-specific robot policies directly from natural language descriptions. TeNet conditions a hypernetwork on text embeddings produced by a pretrained large language model (LLM) to generate a fully executable policy, which then operates solely on low-dimensional state inputs at high control frequencies. By using the language only once at the policy instantiation time, TeNet inherits the general knowledge and paraphrasing robustness of pretrained LLMs while remaining lightweight and efficient at execution time. To improve generalization, we optionally ground language in behavior during training by aligning text embeddings with demonstrated actions, while requiring no demonstrations at inference time. Experiments on MuJoCo and Meta-World benchmarks show that TeNet produces policies that are orders of magnitude smaller than sequence-based baselines, while achieving strong performance in both multi-task and meta-learning settings and supporting high-frequency control. These results show that text-conditioned hypernetworks offer a practical way to build compact, language-driven controllers for ressource-constrained robot control tasks with real-time requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TeNet：面向紧凑策略合成的文本到网络框架</div>
<div class="mono" style="margin-top:8px">遵循自然语言指令的机器人通常采用两种方式：通过人工设计的接口进行高层规划，或依赖难以实时部署的大型端到端模型。本文提出TeNet（文本到网络）框架，可直接从自然语言描述实例化紧凑的任务专用机器人策略。TeNet基于预训练大语言模型生成的文本嵌入条件化超网络，生成完全可执行的策略，该策略仅需在控制高频下处理低维状态输入。通过在策略实例化时仅使用一次语言描述，TeNet继承了预训练大语言模型的通用知识与语义泛化鲁棒性，同时保持轻量化高效执行。为提升泛化能力，我们在训练中通过将文本嵌入与演示动作对齐实现语言的行为锚定，而无需推理阶段的演示数据。在MuJoCo和Meta-World基准测试中，TeNet生成的策略比基于序列的基线模型小数个数量级，同时在多任务与元学习场景中均表现优异，并支持高频控制。结果表明：文本条件化超网络为资源受限且需实时响应的机器人控制任务，提供了一种构建紧凑语言驱动控制器的实用方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of deploying robots that follow natural-language instructions, where existing methods rely on hand-designed interfaces or large end-to-end models that are inefficient for real-time control. The proposed TeNet framework uses a hypernetwork conditioned on text embeddings from a pretrained large language model to generate compact, executable policies that operate on low-dimensional state inputs at high frequencies, decoupling language processing from execution. Experiments on MuJoCo and Meta-World benchmarks demonstrate that TeNet produces policies orders of magnitude smaller than sequence-based baselines while achieving strong performance in multi-task and meta-learning settings and supporting high-frequency control.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决机器人执行自然语言指令的部署难题，现有方法要么依赖手工设计的高层规划器，要么使用不适合实时控制的大型端到端模型。提出的TeNet框架利用基于预训练大语言模型文本嵌入调节的超网络，生成紧凑的任务特定策略，该策略在低维状态输入上以高频率运行，从而将语言处理与执行解耦。在MuJoCo和Meta-World基准测试中的实验表明，TeNet生成的策略比基于序列的基线模型小几个数量级，同时在多任务和元学习设置中表现出色，支持高效的实时控制。</div>
</details>
</div>
<div class="card">
<div class="title">Data-driven tool wear prediction in milling, based on a process-integrated single-sensor approach</div>
<div class="meta-line">Authors: Eric Hirsch, Christian Friedrich</div>
<div class="meta-line">First: 2024-12-27T23:10:32+00:00 · Latest: 2026-01-22T11:22:37+00:00</div>
<div class="meta-line">Comments: This work is a preprint and has been submitted for possible publication,14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.19950v5">Abs</a> · <a href="https://arxiv.org/pdf/2412.19950v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate tool wear prediction is essential for maintaining productivity and minimizing costs in machining. However, the complex nature of the tool wear process poses significant challenges to achieving reliable predictions. This study explores data-driven methods, in particular deep learning, for tool wear prediction. Traditional data-driven approaches often focus on a single process, relying on multi-sensor setups and extensive data generation, which limits generalization to new settings. Moreover, multi-sensor integration is often impractical in industrial environments. To address these limitations, this research investigates the transferability of predictive models using minimal training data, validated across two processes. Furthermore, it uses a simple setup with a single acceleration sensor to establish a low-cost data generation approach that facilitates the generalization of models to other processes via transfer learning. The study evaluates several machine learning models, including transformer-inspired convolutional neural networks (CNN), long short-term memory networks (LSTM), support vector machines (SVM), and decision trees, trained on different input formats such as feature vectors and short-time Fourier transform (STFT). The performance of the models is evaluated on two machines and on different amounts of training data, including scenarios with significantly reduced datasets, providing insight into their effectiveness under constrained data conditions. The results demonstrate the potential of specific models and configurations for effective tool wear prediction, contributing to the development of more adaptable and efficient predictive maintenance strategies in machining. Notably, the ConvNeXt model has an exceptional performance, achieving 99.1\% accuracy in identifying tool wear using data from only four milling tools operated until they are worn.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于工艺集成单传感器方法的铣削数据驱动刀具磨损预测</div>
<div class="mono" style="margin-top:8px">精确的刀具磨损预测对于维持加工生产率和控制成本至关重要，但磨损过程的复杂性给可靠预测带来重大挑战。本研究探索数据驱动方法（特别是深度学习）用于刀具磨损预测。传统数据驱动方法常局限于单一工艺，依赖多传感器设置和大量数据生成，限制了其在新场景中的泛化能力，且多传感器集成在工业环境中往往不切实际。为突破这些局限，本研究通过两个工艺验证，探究了使用最少训练数据的预测模型可迁移性；并采用单加速度传感器的简易配置，建立低成本数据生成方法，通过迁移学习促进模型向其他工艺泛化。研究评估了多种机器学习模型（包括Transformer启发的卷积神经网络、长短期记忆网络、支持向量机和决策树），这些模型基于特征向量和短时傅里叶变换等不同输入格式进行训练。通过在两种机床上使用不同规模训练数据（包括大幅缩减的数据集）评估模型性能，揭示了其在受限数据条件下的有效性。结果表明特定模型与配置能实现有效的刀具磨损预测，有助于开发适应性更强、更高效的加工预测性维护策略。值得注意的是，ConvNeXt模型表现卓越，仅使用四把磨损铣刀的数据就实现了99.1%的刀具磨损识别准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of accurate tool wear prediction in milling, which is crucial for productivity and cost control, by exploring data-driven methods that overcome the limitations of traditional multi-sensor setups and extensive data requirements. The research investigates the transferability of predictive models using minimal training data across two processes, employing a low-cost, process-integrated approach with a single acceleration sensor and evaluating various machine learning models, including transformer-inspired CNNs, LSTMs, SVMs, and decision trees, on different input formats like feature vectors and STFT. Experimental results demonstrate the models&#x27; effectiveness under constrained data conditions, with the ConvNeXt model achieving 99.1% accuracy in tool wear identification using data from only four worn milling tools, highlighting its potential for adaptable predictive maintenance strategies.</div>
<div class="mono" style="margin-top:8px">精确的刀具磨损预测对于保证加工生产率和控制成本至关重要，但传统的多传感器数据驱动方法通常复杂且泛化能力不足。本研究提出了一种低成本、过程集成的方法，使用单个加速度传感器，并探索迁移学习以使模型能够在不同铣削过程中以最少训练数据实现泛化。在两种机床上对包括ConvNeXt、LSTM、SVM和决策树在内的模型进行实验评估，结果表明ConvNeXt模型仅使用四个磨损铣刀的数据就实现了99.1%的刀具磨损识别准确率，证明了其在数据受限条件下的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</div>
<div class="meta-line">Authors: Libo Wang</div>
<div class="meta-line">First: 2025-11-30T08:37:01+00:00 · Latest: 2026-01-22T10:28:40+00:00</div>
<div class="meta-line">Comments: The Sigma model has been open-sourced on Hugging Face. Weights, dataset, some scripts, and logs are all available. The link is: https://huggingface.co/Veltraxor/Sigma</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00783v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.00783v3">PDF</a> · <a href="https://huggingface.co/Veltraxor/Sigma">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To address a fundamental limitation in cognitive systems, namely the absence of a time-updatable mediating thought space between semantics and continuous control, this work constructs and trains a vision-language-action model termed Sigma, deployed on a single RTX 4090. The model is built upon the open-source pi0.5_base backbone, with the svla_so101_pickplace dataset preprocessed into a structured training corpus. An independently designed VLA architecture is introduced to integrate deep semantic understanding with associative reasoning, enabling telepathic-style alignment between perception and action. Training proceeds through iterative optimization of data preprocessing, LoRA-based fine-tuning, and inference-stage adapter design. Evaluation is conducted using offline closed-loop replay, comparing Sigma against the untuned pi0.5_base under identical data conditions. Experimental results indicate a consistent reduction in control MSE across vector-, fragment-, and trajectory-level scales, while preserving the stability of the telepathy norm and semantic-text alignment quality. These findings demonstrate that mind-responsive alignment control can be quantitatively achieved through semantic and associative architectural integration without retraining the base model, providing a reproducible pathway for semantic alignment and intention-driven behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sigma：实现视觉-语言-动作模型心灵感应对齐的关键</div>
<div class="mono" style="margin-top:8px">为应对认知系统中语义与连续控制间缺乏时间可更新中介思维空间的核心局限，本研究构建并训练了名为Sigma的视觉-语言-动作模型，部署于单张RTX 4090显卡。该模型基于开源pi0.5_base主干网络，将svla_so101_pickplace数据集预处理为结构化训练语料。通过独立设计的VLA架构融合深度语义理解与关联推理，实现感知与行动间的心灵感应式对齐。训练过程通过数据预处理迭代优化、基于LoRA的微调及推理阶段适配器设计逐步推进。采用离线闭环回放评估，在相同数据条件下将Sigma与未调优的pi0.5_base进行对比。实验结果表明：模型在向量级、片段级和轨迹级尺度上均持续降低控制均方误差，同时保持心灵感应范数稳定性与语义-文本对齐质量。这些发现证明，通过语义与关联架构整合可定量实现思维响应式对齐控制，无需重训基础模型，为语义对齐与意图驱动行为提供了可复现的技术路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of a time-updatable mediating thought space between high-level semantics and low-level continuous control in cognitive systems. The method constructs a vision-language-action model called Sigma, based on the pi0.5_base backbone, using a preprocessed dataset and a novel architecture designed to integrate deep semantic understanding with associative reasoning for telepathic-style alignment. Experimental evaluation via offline closed-loop replay shows that Sigma, compared to the untuned base model, consistently reduces control mean squared error at vector-, fragment-, and trajectory-level scales while maintaining alignment stability and semantic-text quality, demonstrating that mind-responsive control can be achieved through semantic and associative integration without full model retraining.</div>
<div class="mono" style="margin-top:8px">本研究针对认知系统中缺乏一个可在时间上更新的、介于高层语义与低层连续控制之间的中介思维空间这一根本局限。作者基于开源pi0.5_base骨干网络，利用预处理后的svla_so101_pickplace数据集，构建并训练了一个名为Sigma的视觉-语言-动作模型，其独立设计的架构融合了深度语义理解与关联推理，以实现感知与行动之间的“心灵感应”式对齐。通过数据预处理、基于LoRA的微调和推理阶段适配器设计的迭代优化，与未经调优的基础模型相比，该模型在向量、片段和轨迹级别上均显示出控制均方误差的持续降低，同时保持了“心灵感应”范数稳定性和语义-文本对齐质量，证明了无需重新训练基础模型即可实现响应思维的量化对齐控制。</div>
</details>
</div>
<div class="card">
<div class="title">A Beacon Based Solution for Autonomous UUVs GNSS-Denied Stealthy Navigation</div>
<div class="meta-line">Authors: Alexandre Albore, Humbert Fiorino, Damien Pellier</div>
<div class="meta-line">First: 2026-01-22T09:34:56+00:00 · Latest: 2026-01-22T09:34:56+00:00</div>
<div class="meta-line">Comments: 8 pages. IEEE TechDefense 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15802v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous Unmanned Underwater Vehicles (UUVs) enable military and civilian covert operations in coastal areas without relying on support vessels or Global Navigation Satellite Systems (GNSS). Such operations are critical when surface access is not possible and stealthy navigation is required in restricted environments such as protected zones or dangerous areas under access ban. GNSS denied navigation is then essential to maintaining concealment as surfacing could expose UUVs to detection. To ensure a precise fleet positioning a constellation of beacons deployed by aerial or surface drones establish a synthetic landmark network that will guide the fleet of UUVs along an optimized path from the continental shelf to the goal on the shore. These beacons either submerged or floating emit acoustic signals for UUV localisation and navigation. A hierarchical planner generates an adaptive route for the drones executing primitive actions while continuously monitoring and replanning as needed to maintain trajectory accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信标的水下无人潜航器GNSS拒止隐蔽导航解决方案</div>
<div class="mono" style="margin-top:8px">自主式水下无人潜航器（UUV）可在不依赖支援舰船或全球导航卫星系统（GNSS）的情况下，于沿海区域执行军民隐蔽任务。当无法通过水面进入且需在保护区或禁入危险区等受限环境中实施隐蔽航行时，此类任务尤为关键。GNSS拒止导航对保持隐蔽性至关重要，因为上浮可能导致UUV暴露。为确保编队精确定位，通过空中或水面无人机部署的信标星座构建合成地标网络，引导UUV编队沿大陆架至岸上目标的最优路径航行。这些沉底或漂浮式信标发射声学信号用于UUV定位导航。分层规划器为执行基础动作的无人机生成自适应航线，同时持续监测并在必要时重新规划以保持轨迹精度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for stealthy and precise navigation of autonomous Unmanned Underwater Vehicles (UUVs) in GNSS-denied coastal environments, such as protected or dangerous zones, where surfacing for a GPS fix would compromise concealment. The proposed method establishes a synthetic landmark network using a constellation of acoustic beacons deployed by aerial or surface drones; a hierarchical planner then generates and continuously adapts optimized routes for the UUV fleet based on these signals. Experimental results indicate that this beacon-based solution enables accurate underwater positioning and guided navigation from the continental shelf to a shore-based goal while maintaining operational stealth.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自主无人水下航行器在GNSS拒止的沿海环境（如保护区或危险区域）中，对隐蔽且精确导航的需求，因为上浮获取卫星信号会暴露行踪。所提出的方法利用由空中或水面无人机部署的声学信标星座，构建一个合成地标网络来引导UUV编队；随后，一个分层规划器基于持续监测，生成并自适应地重新规划无人机的优化路径。实验结果表明，这种基于信标的解决方案能够实现从大陆架到岸边目标的水下精确定位与导航，同时保持所需的隐蔽性。</div>
</details>
</div>
<div class="card">
<div class="title">Glove2UAV: A Wearable IMU-Based Glove for Intuitive Control of UAV</div>
<div class="meta-line">Authors: Amir Habel, Ivan Snegirev, Elizaveta Semenyakina, Miguel Altamirano Cabrera, Jeffrin Sam, Fawad Mehboob, Roohan Ahmed Khan, Muhammad Ahsan Mustafa, Dzmitry Tsetserukou</div>
<div class="meta-line">First: 2026-01-22T09:03:57+00:00 · Latest: 2026-01-22T09:03:57+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at LBR of HRI 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15775v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15775v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents Glove2UAV, a wearable IMU-glove interface for intuitive UAV control through hand and finger gestures, augmented with vibrotactile warnings for exceeding predefined speed thresholds. To promote safer and more predictable interaction in dynamic flight, Glove2UAV is designed as a lightweight and easily deployable wearable interface intended for real-time operation. Glove2UAV streams inertial measurements in real time and estimates palm and finger orientations using a compact processing pipeline that combines median-based outlier suppression with Madgwick-based orientation estimation. The resulting motion estimations are mapped to a small set of control primitives for directional flight (forward/backward and lateral motion) and, when supported by the platform, to object-interaction commands. Vibrotactile feedback is triggered when flight speed exceeds predefined threshold values, providing an additional alert channel during operation. We validate real-time feasibility by synchronizing glove signals with UAV telemetry in both simulation and real-world flights. The results show fast gesture-based command execution, stable coupling between gesture dynamics and platform motion, correct operation of the core command set in our trials, and timely delivery of vibratile warning cues.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Glove2UAV：基于可穿戴IMU手套的无人机直观控制系统</div>
<div class="mono" style="margin-top:8px">本文提出Glove2UAV——一种基于可穿戴IMU手套的交互界面，通过手部与手指姿态实现无人机的直观控制，并集成振动触觉预警功能以提示超速状态。为提升动态飞行中的安全性与可预测性，该系统设计为轻量化、易部署的可穿戴实时操作界面。Glove2UAV实时传输惯性测量数据，并通过融合中值离群抑制与Madgwick方向估计算法的紧凑处理流程，实时估计手掌与手指朝向。最终运动估计被映射为少量飞行控制指令（前后/横向移动）及平台支持的物体交互指令。当飞行速度超过预设阈值时，系统触发振动触觉反馈以提供额外警示通道。我们通过仿真与真实飞行中手套信号与无人机遥测数据的同步验证了实时可行性。实验结果表明：该系统具备快速手势指令执行能力、手势动态与平台运动的稳定耦合性、核心指令集的准确运行效能以及振动预警信号的及时触发特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enable more intuitive and safer control of unmanned aerial vehicles (UAVs) by developing a wearable glove interface. The method involves a glove equipped with inertial measurement units (IMUs) that streams data in real time; a processing pipeline using median filtering and Madgwick-based estimation computes palm and finger orientations, which are mapped to directional flight and object-interaction commands, with vibrotactile warnings for excessive speed. Experimental validation in simulation and real flights demonstrated fast gesture execution, stable coupling between gestures and UAV motion, reliable operation of the core command set, and effective delivery of vibrotactile alerts.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过开发一种可穿戴手套接口，实现更直观、更安全的无人机控制。所提出的Glove2UAV系统使用惯性测量单元捕捉手部和手指手势，通过结合中值滤波和Madgwick方向估计的数据处理流程，将估计的运动映射为无人机飞行指令，并在速度过高时提供振动触觉警告。在模拟和真实飞行中的实验验证表明，该系统能实现快速的指令执行、稳定的手势-运动耦合、核心指令集的可靠操作以及触觉警告的有效传递。</div>
</details>
</div>
<div class="card">
<div class="title">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</div>
<div class="meta-line">Authors: Li Kang, Xiufeng Song, Heng Zhou, Yiran Qin, Jie Yang, Xiaohong Liu, Philip Torr, Lei Bai, Zhenfei Yin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-10T17:59:44+00:00 · Latest: 2026-01-22T08:52:35+00:00</div>
<div class="meta-line">Comments: Accepted by NeurIPS 2025 Track on Datasets and Benchmarks. Project page: https://faceong.github.io/VIKI-R/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09049v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09049v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://faceong.github.io/VIKI-R/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinating multiple embodied agents in dynamic environments remains a core challenge in artificial intelligence, requiring both perception-driven reasoning and scalable cooperation strategies. While recent works have leveraged large language models (LLMs) for multi-agent planning, a few have begun to explore vision-language models (VLMs) for visual reasoning. However, these VLM-based approaches remain limited in their support for diverse embodiment types. In this work, we introduce VIKI-Bench, the first hierarchical benchmark tailored for embodied multi-agent cooperation, featuring three structured levels: agent activation, task planning, and trajectory perception. VIKI-Bench includes diverse robot embodiments, multi-view visual observations, and structured supervision signals to evaluate reasoning grounded in visual inputs. To demonstrate the utility of VIKI-Bench, we propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model (VLM) using Chain-of-Thought annotated demonstrations, followed by reinforcement learning under multi-level reward signals. Our extensive experiments show that VIKI-R significantly outperforms baselines method across all task levels. Furthermore, we show that reinforcement learning enables the emergence of compositional cooperation patterns among heterogeneous agents. Together, VIKI-Bench and VIKI-R offer a unified testbed and method for advancing multi-agent, visual-driven cooperation in embodied AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIKI-R：基于强化学习的具身多智能体协同协调框架</div>
<div class="mono" style="margin-top:8px">在动态环境中协调多个具身智能体仍是人工智能的核心挑战，需要感知驱动的推理与可扩展的协作策略。现有研究多采用大语言模型进行多智能体规划，少数开始探索视觉语言模型在视觉推理中的应用，但这些方法对异构具身形态的支持仍有限。本研究提出首个面向具身多智能体协作的层次化基准VIKI-Bench，包含智能体激活、任务规划与轨迹感知三层结构，涵盖多类机器人形态、多视角视觉观测及结构化监督信号，以评估基于视觉输入的推理能力。基于此，我们提出两阶段框架VIKI-R：先通过思维链标注数据微调预训练视觉语言模型，再基于多层次奖励信号进行强化学习。实验表明VIKI-R在所有任务层级均显著优于基线方法，且强化学习能促使异构智能体涌现组合式协作模式。VIKI-Bench与VIKI-R共同为具身AI系统的多智能体视觉驱动协作提供了统一测试平台与方法体系。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of coordinating multiple embodied agents in dynamic environments, where existing vision-language model approaches lack support for diverse robot embodiments. The authors introduce VIKI-Bench, a hierarchical benchmark with three structured levels for evaluating embodied multi-agent cooperation, and propose VIKI-R, a two-stage framework that fine-tunes a pretrained vision-language model using Chain-of-Thought demonstrations followed by reinforcement learning with multi-level rewards. Experimental results demonstrate that VIKI-R significantly outperforms baseline methods across all task levels and enables the emergence of compositional cooperation patterns among heterogeneous agents.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决动态环境中多个具身智能体协调的挑战，这需要视觉推理和可扩展的合作策略。作者提出了VIKI-Bench，这是一个针对具身多智能体协作的分层基准测试，包含三个结构化层级，并提出了VIKI-R框架，该框架通过思维链标注演示对预训练的视觉语言模型进行微调，然后利用多级奖励进行强化学习。实验结果表明，VIKI-R在所有任务层级上均显著优于基线方法，并促进了异构智能体之间组合式合作模式的出现。</div>
</details>
</div>
<div class="card">
<div class="title">DualShield: Safe Model Predictive Diffusion via Reachability Analysis for Interactive Autonomous Driving</div>
<div class="meta-line">Authors: Rui Yang, Lei Zheng, Ruoyu Yao, Jun Ma</div>
<div class="meta-line">First: 2026-01-22T07:56:36+00:00 · Latest: 2026-01-22T07:56:36+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15729v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15729v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have emerged as a powerful approach for multimodal motion planning in autonomous driving. However, their practical deployment is typically hindered by the inherent difficulty in enforcing vehicle dynamics and a critical reliance on accurate predictions of other agents, making them prone to safety issues under uncertain interactions. To address these limitations, we introduce DualShield, a planning and control framework that leverages Hamilton-Jacobi (HJ) reachability value functions in a dual capacity. First, the value functions act as proactive guidance, steering the diffusion denoising process towards safe and dynamically feasible regions. Second, they form a reactive safety shield using control barrier-value functions (CBVFs) to modify the executed actions and ensure safety. This dual mechanism preserves the rich exploration capabilities of diffusion models while providing principled safety assurance under uncertain and even adversarial interactions. Simulations in challenging unprotected U-turn scenarios demonstrate that DualShield significantly improves both safety and task efficiency compared to leading methods from different planning paradigms under uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DualShield：基于可达性分析的交互式自动驾驶安全模型预测扩散方法</div>
<div class="mono" style="margin-top:8px">扩散模型已成为自动驾驶多模态运动规划的有效方法，但其实际部署常受限于难以严格遵循车辆动力学特性，且高度依赖对其他智能体的精确预测，导致在不确定交互场景下易出现安全问题。为应对这些局限，本文提出DualShield规划控制框架，该框架通过双重机制利用哈密顿-雅可比可达性值函数：首先，值函数作为主动引导，将扩散去噪过程导向安全且动力学可行的区域；其次，基于控制屏障值函数构建反应式安全屏障，修正执行动作以确保安全性。这种双重机制在保留扩散模型丰富探索能力的同时，为不确定甚至对抗性交互场景提供理论安全保障。在无保护U形转弯等挑战性场景的仿真实验中，DualShield相比现有主流规划方法，在不确定环境下显著提升了安全性与任务执行效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses safety concerns in deploying diffusion models for autonomous driving motion planning, which struggle with enforcing vehicle dynamics and rely heavily on accurate predictions of other agents. The proposed DualShield framework employs Hamilton-Jacobi reachability value functions in a dual role: proactively guiding the diffusion denoising process toward safe, dynamically feasible regions and reactively forming a safety shield using control barrier-value functions to modify executed actions. Experimental simulations in unprotected U-turn scenarios show that DualShield substantially enhances both safety and task efficiency compared to leading methods under uncertain conditions.</div>
<div class="mono" style="margin-top:8px">该研究针对扩散模型在自动驾驶运动规划中部署的安全性问题，这类模型难以强制执行车辆动力学，且严重依赖对其他智能体的准确预测，在不确定交互下存在风险。提出的DualShield框架以双重方式利用哈密顿-雅可比可达性值函数：主动引导扩散去噪过程朝向安全且动态可行的区域，以及被动地利用控制障碍值函数形成安全屏障以修改执行动作。在无保护掉头场景的仿真实验中，DualShield相比现有领先方法，在不确定性条件下显著提高了安全性和任务效率。</div>
</details>
</div>
<div class="card">
<div class="title">D-Optimality-Guided Reinforcement Learning for Efficient Open-Loop Calibration of a 3-DOF Ankle Rehabilitation Robot</div>
<div class="meta-line">Authors: Qifan Hu, Branko Celler, Weidong Mu, Steven W. Su</div>
<div class="meta-line">First: 2026-01-22T07:20:55+00:00 · Latest: 2026-01-22T07:20:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate alignment of multi-degree-of-freedom rehabilitation robots is essential for safe and effective patient training. This paper proposes a two-stage calibration framework for a self-designed three-degree-of-freedom (3-DOF) ankle rehabilitation robot. First, a Kronecker-product-based open-loop calibration method is developed to cast the input-output alignment into a linear parameter identification problem, which in turn defines the associated experimental design objective through the resulting information matrix. Building on this formulation, calibration posture selection is posed as a combinatorial design-of-experiments problem guided by a D-optimality criterion, i.e., selecting a small subset of postures that maximises the determinant of the information matrix. To enable practical selection under constraints, a Proximal Policy Optimization (PPO) agent is trained in simulation to choose 4 informative postures from a candidate set of 50. Across simulation and real-robot evaluations, the learned policy consistently yields substantially more informative posture combinations than random selection: the mean determinant of the information matrix achieved by PPO is reported to be more than two orders of magnitude higher with reduced variance. In addition, real-world results indicate that a parameter vector identified from only four D-optimality-guided postures provides stronger cross-episode prediction consistency than estimates obtained from a larger but unstructured set of 50 postures. The proposed framework therefore improves calibration efficiency while maintaining robust parameter estimation, offering practical guidance for high-precision alignment of multi-DOF rehabilitation robots.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于D最优性引导的强化学习用于三自由度踝关节康复机器人高效开环标定</div>
<div class="mono" style="margin-top:8px">多自由度康复机器人的精确对准对安全有效的患者训练至关重要。本文针对一款自主设计的三自由度踝关节康复机器人，提出了一种两阶段标定框架。首先，开发了一种基于克罗内克积的开环标定方法，将输入-输出对准问题转化为线性参数辨识问题，进而通过生成的信息矩阵定义相关实验设计目标。基于此表述，标定姿态选择被构建为以D最优性准则指导的组合实验设计问题，即从候选姿态中选取能最大化信息矩阵行列式的小规模子集。为在约束下实现实际选择，在仿真环境中训练近端策略优化智能体，使其从50个候选姿态中选取4个信息量最大的姿态。仿真与真实机器人实验表明，学习策略所选姿态组合的信息量始终显著优于随机选择：PPO所得信息矩阵行列式均值提高两个数量级以上且方差降低。此外，真实实验结果显示，仅基于四个D最优性引导姿态辨识的参数向量，其跨周期预测一致性优于从50个无结构姿态集中获得的估计结果。该框架在保持参数估计鲁棒性的同时提升了标定效率，为多自由度康复机器人的高精度对准提供了实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate alignment of multi-degree-of-freedom rehabilitation robots is crucial for safe and effective therapy, motivating the development of an efficient calibration method. The proposed two-stage framework first formulates the calibration of a 3-DOF ankle robot as a linear parameter identification problem using a Kronecker-product model, then frames posture selection as a combinatorial design-of-experiments problem optimized for D-optimality. A Proximal Policy Optimization (PPO) agent is trained to select a small, informative subset of four postures from 50 candidates. Experimental results show the learned policy selects postures yielding an information matrix determinant over two orders of magnitude higher than random selection, with lower variance, and real-robot tests demonstrate that parameters identified from just four D-optimal postures provide more consistent predictions than those from the full, unstructured set of 50 postures.</div>
<div class="mono" style="margin-top:8px">多自由度康复机器人的精确对准对于安全有效的训练至关重要，这促使了高效校准方法的开发。所提出的两阶段框架首先使用基于Kronecker积的模型，将3-DOF踝关节机器人的校准表述为线性参数辨识问题，进而将姿态选择构建为一个以D最优性准则优化的组合实验设计问题。为解决此选择问题，在仿真中训练了一个近端策略优化（PPO）智能体，从50个候选姿态中选择4个最优姿态。实验结果表明，学习到的策略所选姿态产生的信息矩阵行列式比随机选择高出两个数量级以上，且方差更小；真实机器人测试表明，仅从四个D最优姿态辨识出的参数比从50个非结构化姿态获得的参数具有更好的预测一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Layered Reasoning from a Single Viewpoint for Learning See-Through Grasping</div>
<div class="meta-line">Authors: Fang Wan, Chaoyang Song</div>
<div class="meta-line">First: 2023-12-15T14:21:14+00:00 · Latest: 2026-01-22T07:14:46+00:00</div>
<div class="meta-line">Comments: 39 pages, 13 figures, 2 tables, for supplementary videos, see https://bionicdl.ancorasir.com/?p=1658, for opensourced codes, see https://github.com/ancorasir/SeeThruFinger</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.09822v5">Abs</a> · <a href="https://arxiv.org/pdf/2312.09822v5">PDF</a> · <a href="https://github.com/ancorasir/SeeThruFinger">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sensory substitution enables biological systems to perceive stimuli that are typically perceived by another organ, which is inspirational for physical agents. Multimodal perception of intrinsic and extrinsic interactions is critical in building an intelligent robot that learns. This study presents a Vision-based See-Through Perception (VBSeeThruP) architecture that simultaneously perceives multiple intrinsic and extrinsic modalities from a single visual input, in a markerless manner, all packed into a soft robotic finger using the Soft Polyhedral Network design. It is generally applicable to miniature vision systems placed beneath deformable networks with a see-through design, capturing real-time images of the network&#x27;s physical interactions induced by contact-based events, overlaid on the visual scene of the external environment, as demonstrated in the ablation study. We present the VBSeeThruP&#x27;s capability for learning reactive grasping without using external cameras or dedicated force and torque sensors on the fingertips. Using the inpainted scene and the deformation mask, we further demonstrate the multimodal performance of the VBSeeThruP architecture to simultaneously achieve various perceptions, including but not limited to scene inpainting, object detection, depth sensing, scene segmentation, masked deformation tracking, 6D force/torque sensing, and contact event detection, all within a single sensory input from the in-finger vision markerlessly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单视角多层推理的透视抓取学习</div>
<div class="mono" style="margin-top:8px">感官替代使生物系统能感知通常由其他器官接收的刺激，这对物理智能体具有启发意义。内在与外在交互的多模态感知对构建学习型智能机器人至关重要。本研究提出一种基于视觉的透视感知架构，能从单一视觉输入中以无标记方式同步感知多种内在与外在模态，并通过软体多面体网络设计集成于软体机器人手指中。该架构普遍适用于部署在可变形网络下方的微型视觉系统，采用透视设计实时捕获由接触事件引发的网络物理交互图像，并叠加于外部环境视觉场景上（如消融实验所示）。我们展示了该架构无需外部摄像头或指尖专用力/扭矩传感器即可实现反应式抓取学习的能力。利用修复场景与形变掩膜，进一步验证了该架构同步实现多模态感知的性能，包括但不限于：场景修复、物体检测、深度感知、场景分割、掩膜形变追踪、六维力/扭矩感知及接触事件检测——所有功能均基于单一无标记指内视觉传感器输入。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Inspired by sensory substitution in biological systems, this research aims to enable robots to learn multimodal perception from a single visual source, eliminating the need for multiple external sensors. The method introduces a Vision-based See-Through Perception (VBSeeThruP) architecture, which integrates a soft robotic finger with an embedded miniature vision system to capture real-time, markerless images of both internal deformations and the external environment through a see-through design. Key experimental results demonstrate that this system can learn reactive grasping without external cameras or dedicated force/torque sensors, while simultaneously achieving multiple perceptual tasks such as scene inpainting, object detection, depth sensing, and 6D force/torque sensing from a single visual input.</div>
<div class="mono" style="margin-top:8px">本研究受生物感官替代启发，旨在让智能机器人从单一感官输入中感知多模态的内在与外在交互。方法上提出了视觉透视感知架构，在软体机器人手指内使用单个摄像头，以无标记方式实时捕获内部变形与外部环境的叠加图像。主要实验结果表明，该系统无需外部摄像头或专用力传感器即可学习反应式抓取，并能从单一视觉输入中同时实现场景修复、物体检测、深度感知、场景分割、变形跟踪、六维力/力矩传感和接触事件检测。</div>
</details>
</div>
<div class="card">
<div class="title">AION: Aerial Indoor Object-Goal Navigation Using Dual-Policy Reinforcement Learning</div>
<div class="meta-line">Authors: Zichen Yan, Yuchen Hou, Shenao Wang, Yichao Gao, Rui Huang, Lin Zhao</div>
<div class="meta-line">First: 2026-01-22T03:35:34+00:00 · Latest: 2026-01-22T03:35:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15614v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15614v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object-Goal Navigation (ObjectNav) requires an agent to autonomously explore an unknown environment and navigate toward target objects specified by a semantic label. While prior work has primarily studied zero-shot ObjectNav under 2D locomotion, extending it to aerial platforms with 3D locomotion capability remains underexplored. Aerial robots offer superior maneuverability and search efficiency, but they also introduce new challenges in spatial perception, dynamic control, and safety assurance. In this paper, we propose AION for vision-based aerial ObjectNav without relying on external localization or global maps. AION is an end-to-end dual-policy reinforcement learning (RL) framework that decouples exploration and goal-reaching behaviors into two specialized policies. We evaluate AION on the AI2-THOR benchmark and further assess its real-time performance in IsaacSim using high-fidelity drone models. Experimental results show that AION achieves superior performance across comprehensive evaluation metrics in exploration, navigation efficiency, and safety. The video can be found at https://youtu.be/TgsUm6bb7zg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AION：基于双策略强化学习的空中室内目标导航系统</div>
<div class="mono" style="margin-top:8px">目标导航任务要求智能体在未知环境中自主探索，并根据语义标签导航至指定目标物体。现有研究主要集中于二维平面移动的零样本目标导航，而将其扩展至具备三维移动能力的空中平台仍待深入探索。空中机器人虽具有卓越的机动性与搜索效率，但也带来了空间感知、动态控制及安全保障等新挑战。本文提出AION系统，实现不依赖外部定位或全局地图的视觉空中目标导航。该端到端双策略强化学习框架将探索行为与目标抵达行为解耦为两个专用策略。我们在AI2-THOR基准测试中评估AION，并基于高精度无人机模型在IsaacSim中进一步验证其实时性能。实验结果表明，AION在探索能力、导航效率及安全性等综合评估指标上均表现优异。演示视频详见：https://youtu.be/TgsUm6bb7zg。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the underexplored challenge of extending Object-Goal Navigation (ObjectNav) from 2D ground agents to aerial platforms with 3D locomotion, motivated by the superior maneuverability and search efficiency of drones, which also introduce new difficulties in perception, control, and safety. The proposed method, AION, is an end-to-end dual-policy reinforcement learning framework that decouples the task into two specialized policies for exploration and goal-reaching, operating without external localization or global maps using only vision. Experimental evaluation on the AI2-THOR benchmark and in IsaacSim with high-fidelity drone models demonstrates that AION achieves superior performance in exploration efficiency, navigation success, and safety compared to prior approaches.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决将目标物体导航任务从二维地面智能体扩展到具有三维运动能力的空中平台这一尚未充分探索的挑战，其动机在于无人机具有更优的机动性和搜索效率，但同时也带来了感知、控制和安全性方面的新难题。所提出的方法AION是一种端到端的双策略强化学习框架，它将任务解耦为专门用于探索和抵达目标的两个策略，仅依赖视觉输入，无需外部定位或全局地图。在AI2-THOR基准测试以及使用高保真无人机模型的IsaacSim仿真环境中的实验评估表明，AION在探索能力、导航效率和安全性等综合指标上均取得了优越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Airflow Source Seeking on Small Quadrotors Using a Single Flow Sensor</div>
<div class="meta-line">Authors: Lenworth Thomas, Tjaden Bridges, Sarah Bergbreiter</div>
<div class="meta-line">First: 2026-01-22T03:13:24+00:00 · Latest: 2026-01-22T03:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15607v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As environmental disasters happen more frequently and severely, seeking the source of pollutants or harmful particulates using plume tracking becomes even more important. Plume tracking on small quadrotors would allow these systems to operate around humans and fly in more confined spaces, but can be challenging due to poor sensitivity and long response times from gas sensors that fit on small quadrotors. In this work, we present an approach to complement chemical plume tracking with airflow source-seeking behavior using a custom flow sensor that can sense both airflow magnitude and direction on small quadrotors &lt; 100 g. We use this sensor to implement a modified version of the `Cast and Surge&#x27; algorithm that takes advantage of flow direction sensing to find and navigate towards flow sources. A series of characterization experiments verified that the system can detect airflow while in flight and reorient the quadrotor toward the airflow. Several trials with random starting locations and orientations were used to show that our source-seeking algorithm can reliably find a flow source. This work aims to provide a foundation for future platforms that can use flow sensors in concert with other sensors to enable richer plume tracking data collection and source-seeking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单气流传感器的小型四旋翼气流源追踪</div>
<div class="mono" style="margin-top:8px">随着环境灾害日益频发且严重，利用羽流追踪技术定位污染物或有害颗粒源的重要性愈发凸显。在小型四旋翼飞行器上实现羽流追踪，可使系统更贴近人类活动区域并适应狭窄空间作业，但受限于小型四旋翼搭载的气体传感器灵敏度低、响应时间长等挑战。本研究提出一种结合定制气流传感器的气流源追踪方法，该传感器可在重量小于100克的小型四旋翼上同步感知气流强度与方向。基于此传感器，我们改进了&#x27;抛射-突进&#x27;算法，利用气流方向感知能力实现流向源的定位与导航。通过系列特性实验验证，该系统能在飞行中检测气流并调整四旋翼朝向气流方向。多组随机起始位置与方向的测试表明，该溯源算法能可靠定位气流源。本研究旨在为未来平台奠定基础，通过融合气流传感器与其他传感器，实现更丰富的羽流追踪数据采集与溯源能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the increasing frequency of environmental disasters and the need for small quadrotors to track pollutant plumes in confined spaces, this work addresses the limitations of gas sensors by introducing an airflow source-seeking method using a custom flow sensor capable of sensing both magnitude and direction on sub-100g quadrotors. The method implements a modified &#x27;Cast and Surge&#x27; algorithm that leverages directional airflow sensing to locate and navigate toward flow sources. Experimental characterization confirmed in-flight airflow detection and quadrotor reorientation toward the airflow, while trials from random starting points demonstrated the algorithm&#x27;s reliable ability to find a flow source, establishing a foundation for integrating flow sensors with other sensors for enhanced plume tracking.</div>
<div class="mono" style="margin-top:8px">本研究针对环境灾害频发背景下小型四旋翼飞行器在受限空间内追踪污染物羽流的应用需求，旨在克服气体传感器灵敏度低、响应慢的局限，提出了一种利用定制流量传感器进行气流源搜寻的方法，该传感器可在重量小于100克的飞行器上同时感知气流大小和方向。该方法采用改进的“抛投与突进”算法，利用气流方向信息来定位并导航至气流源。实验结果表明，该系统能够在飞行中检测气流并调整飞行器朝向，且能从随机起始位置和方向可靠地找到气流源，为未来结合多种传感器实现更丰富的羽流追踪与源搜寻平台奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Natural Language Environment: Understanding Seamless Natural-Language-Based Human-Multi-Robot Interactions</div>
<div class="meta-line">Authors: Ziyi Liu, Xinyi Wang, Shao-Kang Hsia, Chenfei Zhu, Zhengzhe Zhu, Xiyun Hu, Anastasia Kouvaras Ostrowski, Karthik Ramani</div>
<div class="meta-line">First: 2026-01-19T19:22:00+00:00 · Latest: 2026-01-22T02:50:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13338v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13338v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As multiple robots are expected to coexist in future households, natural language is increasingly envisioned as a primary medium for human-robot and robot-robot communication. This paper introduces the concept of a Natural Language Environment (NLE), defined as an interaction space in which humans and multiple heterogeneous robots coordinate primarily through natural language.
  Rather than proposing a deployable system, this work aims to explore the design space of such environments. We first synthesize prior work on language-based human-robot interaction to derive a preliminary design space for NLEs. We then conduct a role-playing study in virtual reality to investigate how people conceptualize, negotiate, and coordinate human-multi-robot interactions within this imagined environment.
  Based on qualitative and quantitative analysis, we refine the preliminary design space and derive design implications that highlight key tensions and opportunities around task coordination dominance, robot autonomy, and robot personality in Natural Language Environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向自然语言环境：理解基于自然语言的无缝人-多机器人交互</div>
<div class="mono" style="margin-top:8px">随着未来家庭中多机器人共存的预期，自然语言日益被视为人机及机器人间通信的主要媒介。本文提出自然语言环境的概念，其定义为一个交互空间，其中人类与多个异构机器人主要通过自然语言进行协调。本研究并非旨在构建可部署系统，而是探索此类环境的设计空间。我们首先综合基于语言的人机交互先前研究，推导出自然语言环境的初步设计空间；随后通过虚拟现实中的角色扮演研究，探究人们在此设想环境中如何概念化、协商并协调人-多机器人交互。基于定性与定量分析，我们完善了初步设计空间，并得出设计启示，揭示了自然语言环境中任务协调主导权、机器人自主性与机器人个性之间的核心矛盾与机遇。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the anticipated proliferation of multi-robot systems in domestic settings, this paper explores the design space for Natural Language Environments (NLEs), where humans and heterogeneous robots coordinate primarily through natural language. The method involves synthesizing prior work to derive a preliminary design space and then conducting a role-playing study in virtual reality to investigate how people conceptualize and negotiate interactions within such an environment. Key experimental findings from qualitative and quantitative analysis refine the design space, highlighting critical tensions and opportunities concerning task coordination dominance, robot autonomy, and robot personality.</div>
<div class="mono" style="margin-top:8px">本研究基于未来家庭中多机器人系统共存的预期，旨在探索一种以自然语言为主要协调媒介的人与异构多机器人交互环境，即自然语言环境（NLE）的设计空间。方法上，首先综合了先前基于语言的人机交互研究以构建初步设计空间，随后通过虚拟现实角色扮演实验，探究人们在此类设想环境中如何概念化、协商和协调人与多机器人的交互。基于定性与定量分析的主要实验发现，研究细化了初步设计空间，并得出了设计启示，凸显了在NLE中关于任务协调主导权、机器人自主性和机器人个性方面的关键张力与机遇。</div>
</details>
</div>
<div class="card">
<div class="title">MapViT: A Two-Stage ViT-Based Framework for Real-Time Radio Quality Map Prediction in Dynamic Environments</div>
<div class="meta-line">Authors: Cyril Shih-Huan Hsu, Xi Li, Lanfranco Zanzi, Zhiheng Yang, Chrysa Papagianni, Xavier Costa Pérez</div>
<div class="meta-line">First: 2026-01-22T01:57:48+00:00 · Latest: 2026-01-22T01:57:48+00:00</div>
<div class="meta-line">Comments: This paper has been accepted for publication at IEEE International Conference on Communications (ICC) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15578v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15578v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in mobile and wireless networks are unlocking the full potential of robotic autonomy, enabling robots to take advantage of ultra-low latency, high data throughput, and ubiquitous connectivity. However, for robots to navigate and operate seamlessly, efficiently and reliably, they must have an accurate understanding of both their surrounding environment and the quality of radio signals. Achieving this in highly dynamic and ever-changing environments remains a challenging and largely unsolved problem. In this paper, we introduce MapViT, a two-stage Vision Transformer (ViT)-based framework inspired by the success of pre-train and fine-tune paradigm for Large Language Models (LLMs). MapViT is designed to predict both environmental changes and expected radio signal quality. We evaluate the framework using a set of representative Machine Learning (ML) models, analyzing their respective strengths and limitations across different scenarios. Experimental results demonstrate that the proposed two-stage pipeline enables real-time prediction, with the ViT-based implementation achieving a strong balance between accuracy and computational efficiency. This makes MapViT a promising solution for energy- and resource-constrained platforms such as mobile robots. Moreover, the geometry foundation model derived from the self-supervised pre-training stage improves data efficiency and transferability, enabling effective downstream predictions even with limited labeled data. Overall, this work lays the foundation for next-generation digital twin ecosystems, and it paves the way for a new class of ML foundation models driving multi-modal intelligence in future 6G-enabled systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapViT：面向动态环境中实时无线质量地图预测的双阶段视觉Transformer框架</div>
<div class="mono" style="margin-top:8px">移动与无线网络的最新进展正释放机器人自主性的全部潜力，使其能够利用超低延迟、高数据吞吐量和泛在连接。然而，为实现无缝、高效、可靠的导航与操作，机器人必须准确感知周边环境与无线信号质量。在高度动态且持续变化的环境中实现这一目标，仍是亟待解决的挑战性难题。本文提出MapViT——一种受大语言模型预训练-微调范式启发的双阶段视觉Transformer框架，旨在同时预测环境变化与预期无线信号质量。我们通过一组代表性机器学习模型评估该框架，分析其在不同场景下的优势与局限。实验结果表明，所提出的双阶段流程可实现实时预测，基于ViT的实现方案在精度与计算效率间取得了良好平衡，使MapViT成为移动机器人等资源受限平台的有前景解决方案。此外，通过自监督预训练阶段衍生的几何基础模型提升了数据效率与可迁移性，即使在标注数据有限时也能实现有效的下游预测。总体而言，本研究为下一代数字孪生生态系统奠定基础，并为推动未来6G系统中多模态智能的新型机器学习基础模型开辟道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable seamless robot navigation in dynamic environments, accurate prediction of both environmental changes and radio signal quality is required, but remains challenging. This paper proposes MapViT, a two-stage Vision Transformer framework inspired by the pre-train and fine-tune paradigm of LLMs, designed for real-time radio quality map prediction. Experimental evaluation shows the ViT-based implementation achieves a strong balance between accuracy and computational efficiency, enabling real-time prediction suitable for resource-constrained platforms, while the self-supervised pre-training stage improves data efficiency and transferability for downstream tasks with limited labeled data.</div>
<div class="mono" style="margin-top:8px">为使机器人在动态环境中实现无缝导航，需要准确预测环境变化与无线信号质量，这仍是一个未解决的挑战。本文提出MapViT，一个受大语言模型预训练与微调范式启发的两阶段视觉Transformer框架，旨在预测这些因素。实验评估表明，基于ViT的实现能够进行实时预测，在准确性与计算效率之间取得了良好平衡，同时自监督预训练阶段提升了数据效率与可迁移性，使得在有限标注数据下也能进行有效的下游预测。</div>
</details>
</div>
<div class="card">
<div class="title">A Mobile Magnetic Manipulation Platform for Gastrointestinal Navigation with Deep Reinforcement Learning Control</div>
<div class="meta-line">Authors: Zhifan Yan, Chang Liu, Yiyang Jiang, Wenxuan Zheng, Xinhao Chen, Axel Krieger</div>
<div class="meta-line">First: 2026-01-22T00:30:11+00:00 · Latest: 2026-01-22T00:30:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15545v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15545v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Targeted drug delivery in the gastrointestinal (GI) tract using magnetic robots offers a promising alternative to systemic treatments. However, controlling these robots is a major challenge. Stationary magnetic systems have a limited workspace, while mobile systems (e.g., coils on a robotic arm) suffer from a &quot;model-calibration bottleneck&quot;, requiring complex, pre-calibrated physical models that are time-consuming to create and computationally expensive. This paper presents a compact, low-cost mobile magnetic manipulation platform that overcomes this limitation using Deep Reinforcement Learning (DRL). Our system features a compact four-electromagnet array mounted on a UR5 collaborative robot. A Soft Actor-Critic (SAC)-based control strategy is trained through a sim-to-real pipeline, enabling effective policy deployment within 15 minutes and significantly reducing setup time. We validated the platform by controlling a 7-mm magnetic capsule along 2D trajectories. Our DRL-based controller achieved a root-mean-square error (RMSE) of 1.18~mm for a square path and 1.50~mm for a circular path. We also demonstrated successful tracking over a clinically relevant, 30 cm * 20 cm workspace. This work demonstrates a rapidly deployable, model-free control framework capable of precise magnetic manipulation in a large workspace,validated using a 2D GI phantom.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习控制的胃肠道导航移动磁操控平台</div>
<div class="mono" style="margin-top:8px">利用磁控机器人在胃肠道内进行靶向给药为全身性治疗提供了有前景的替代方案，但其精确控制仍面临重大挑战。传统固定式磁系统工作空间有限，而移动式系统（如搭载线圈的机械臂）存在“模型校准瓶颈”，需依赖耗时构建且计算成本高昂的预校准物理模型。本文提出一种紧凑型低成本移动磁操控平台，通过深度强化学习克服了这一局限。该系统将紧凑四电磁铁阵列搭载于UR5协作机器人，采用基于柔性执行器-评价器算法的控制策略，通过仿真到实物的训练流程，可在15分钟内实现有效策略部署，大幅缩短配置时间。平台通过控制7毫米磁胶囊沿二维轨迹运动进行验证：基于深度强化学习的控制器在方形路径上实现1.18毫米均方根误差，圆形路径上为1.50毫米，并在临床相关的30厘米×20厘米工作空间内成功完成轨迹跟踪。本研究展示了一种可快速部署的无模型控制框架，能在大型工作空间实现精确磁操控，并通过二维胃肠道模型验证了其有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of controlling magnetic robots for gastrointestinal drug delivery, where stationary systems have limited workspace and mobile systems face a model-calibration bottleneck requiring complex pre-calibrated physical models. The authors developed a compact mobile platform with a four-electromagnet array mounted on a UR5 robot and implemented a Deep Reinforcement Learning control strategy using Soft Actor-Critic trained through sim-to-real transfer, enabling policy deployment within 15 minutes. Experimental results demonstrated precise control of a 7-mm magnetic capsule along 2D trajectories, achieving root-mean-square errors of 1.18 mm for square paths and 1.50 mm for circular paths, with successful tracking across a clinically relevant 30 cm × 20 cm workspace using a gastrointestinal phantom.</div>
<div class="mono" style="margin-top:8px">利用磁力机器人进行胃肠道靶向给药前景广阔，但控制是一大挑战；固定系统工作空间有限，而移动系统则面临需要复杂预校准模型的“模型校准瓶颈”。为此，研究者开发了一个紧凑、低成本的移动平台，将四电磁铁阵列安装在UR5机械臂上，并采用基于深度强化学习（特别是柔性演员-评论家算法）的控制策略，通过仿真到现实的流程进行训练以实现快速部署。使用7毫米磁胶囊的实验验证表明，该控制器在方形和圆形二维轨迹上的均方根误差分别为1.18毫米和1.50毫米，并在胃肠道模型中成功实现了30厘米×20厘米临床相关工作空间内的路径跟踪。</div>
</details>
</div>
<div class="card">
<div class="title">CompliantVLA-adaptor: VLM-Guided Variable Impedance Action for Safe Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Heng Zhang, Wei-Hsing Huang, Qiyi Tong, Gokhan Solak, Puze Liu, Sheng Liu, Jan Peters, Arash Ajoudani</div>
<div class="meta-line">First: 2026-01-21T23:52:40+00:00 · Latest: 2026-01-21T23:52:40+00:00</div>
<div class="meta-line">Comments: under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15541v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15541v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/compliantvla">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a CompliantVLA-adaptor that augments the state-of-the-art Vision-Language-Action (VLA) models with vision-language model (VLM)-informed context-aware variable impedance control (VIC) to improve the safety and effectiveness of contact-rich robotic manipulation tasks. Existing VLA systems (e.g., RDT, Pi0, OpenVLA-oft) typically output position, but lack force-aware adaptation, leading to unsafe or failed interactions in physical tasks involving contact, compliance, or uncertainty. In the proposed CompliantVLA-adaptor, a VLM interprets task context from images and natural language to adapt the stiffness and damping parameters of a VIC controller. These parameters are further regulated using real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms the VLA baselines on a suite of complex contact-rich tasks, both in simulation and on real hardware, with improved success rates and reduced force violations. The overall success rate across all tasks increases from 9.86\% to 17.29\%, presenting a promising path towards safe contact-rich manipulation using VLAs. We release our code, prompts, and force-torque-impedance-scenario context datasets at https://sites.google.com/view/compliantvla.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CompliantVLA-adaptor：基于视觉语言模型引导的可变阻抗动作实现安全接触密集型操作</div>
<div class="mono" style="margin-top:8px">本文提出一种CompliantVLA-adaptor，通过视觉语言模型（VLM）提供的上下文感知可变阻抗控制（VIC）增强现有先进视觉-语言-动作（VLA）模型，以提升接触密集型机器人操作任务的安全性与有效性。现有VLA系统（如RDT、Pi0、OpenVLA-oft）通常仅输出位置指令，缺乏力感知适应能力，导致涉及接触、柔顺或不确定性的物理任务中出现不安全或失败交互。所提出的CompliantVLA-adaptor利用VLM解析图像与自然语言中的任务上下文，动态调整VIC控制器的刚度与阻尼参数，并借助实时力/力矩反馈调节参数以确保交互力维持在安全阈值内。实验表明，该方法在仿真与真实硬件的一系列复杂接触密集型任务中均优于基线VLA系统，成功率显著提升且违规力显著降低。所有任务整体成功率从9.86%提升至17.29%，为基于VLA的安全接触密集型操作提供了可行路径。代码、提示词及力-力矩-阻抗-场景上下文数据集已发布于https://sites.google.com/view/compliantvla。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the safety limitations of existing Vision-Language-Action (VLA) models, which output only position commands and lack force adaptation, leading to failures in contact-rich robotic manipulation. The proposed CompliantVLA-adaptor augments VLA models by using a Vision-Language Model (VLM) to interpret task context from images and language, generating variable impedance control parameters (stiffness and damping) that are further regulated in real-time using force/torque feedback to maintain safe interaction forces. Experimental results on complex contact-rich tasks in simulation and on real hardware show the method outperforms VLA baselines, increasing the overall success rate from 9.86% to 17.29% while reducing force violations.</div>
<div class="mono" style="margin-top:8px">本研究针对现有视觉-语言-动作模型仅输出位置指令、缺乏力适应能力，导致在接触密集的机器人操作中存在安全风险或任务失败的问题。所提出的CompliantVLA-adaptor方法，通过视觉-语言模型解析图像和语言中的任务上下文，生成可变阻抗控制参数，并利用实时力/力矩反馈调节这些参数以确保交互力处于安全阈值。在仿真和真实硬件上的复杂接触密集任务实验中，该方法优于基线视觉-语言-动作模型，整体成功率从9.86%提升至17.29%，同时减少了力违规情况。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
