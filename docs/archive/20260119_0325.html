<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-19 03:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260119_0325</div>
    <div class="row"><div class="card">
<div class="title">MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</div>
<div class="meta-line">Authors: Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin</div>
<div class="meta-line">First: 2026-01-15T18:59:23+00:00 · Latest: 2026-01-15T18:59:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10712v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10712v1">PDF</a> · <a href="https://github.com/quchangle1/MatchTIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MatchTIR：基于二分图匹配的细粒度监督工具集成推理</div>
<div class="mono" style="margin-top:8px">工具集成推理（TIR）通过将推理步骤与外部工具交互交织，赋能大语言模型处理复杂任务。然而，现有强化学习方法通常依赖结果级或轨迹级奖励，对轨迹内所有步骤赋予统一的优势值。这种粗粒度的信用分配无法区分有效工具调用与冗余或错误调用，尤其在长视野多轮次场景中。为此，我们提出MatchTIR框架，通过基于二分图匹配的轮次级奖励分配和双层优势估计引入细粒度监督。具体而言，我们将信用分配建模为预测轨迹与真实轨迹间的二分图匹配问题，采用两种分配策略生成密集的轮次级奖励。此外，为平衡局部步骤精度与全局任务成功率，我们引入融合轮次级与轨迹级信号的双层优势估计方案，为每个交互轮次分配差异化优势值。在三个基准上的大量实验证明了MatchTIR的优越性。值得注意的是，我们的40亿参数模型在多数任务中超越了80亿参数竞品，尤其在长视野与多轮次任务中表现突出。代码已开源：https://github.com/quchangle1/MatchTIR。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the coarse-grained credit assignment in existing reinforcement learning methods for Tool-Integrated Reasoning (TIR), which fails to distinguish effective tool calls from redundant ones in long-horizon tasks. The proposed MatchTIR framework introduces fine-grained supervision by formulating credit assignment as a bipartite matching problem between predicted and ground-truth traces to derive dense turn-level rewards, and employs a dual-level advantage estimation scheme to balance local precision with global success. Experimental results on three benchmarks show that MatchTIR&#x27;s 4B model outperforms most 8B competitors, especially in long-horizon and multi-turn scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对工具集成推理中现有强化学习方法存在的粗粒度信用分配问题，该问题在长视野任务中无法区分有效工具调用与冗余调用。提出的MatchTIR框架通过将信用分配建模为预测轨迹与真实轨迹之间的二分图匹配问题，以生成细粒度的回合级奖励，并采用双层级优势估计方案来平衡局部步骤精度与全局任务成功率。在三个基准测试上的实验结果表明，MatchTIR的40亿参数模型性能超越了大多数80亿参数竞品，尤其在长视野和多回合任务中表现突出。</div>
</details>
</div>
<div class="card">
<div class="title">Grounding Agent Memory in Contextual Intent</div>
<div class="meta-line">Authors: Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Yunyi Zhang, Jiawei Han</div>
<div class="meta-line">First: 2026-01-15T18:55:13+00:00 · Latest: 2026-01-15T18:55:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10702v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10702v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step&#x27;s intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.
  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于情境意图的智能体记忆系统</div>
<div class="mono" style="margin-top:8px">在长周期目标导向交互中部署大语言模型仍面临挑战：相似实体和事实会在不同潜在目标与约束下重复出现，导致记忆系统检索到情境不匹配的证据。我们提出STITCH（情境历史中的结构化意图追踪）——一种智能体记忆系统，通过结构化检索线索（情境意图）索引每个轨迹步骤，并依据当前步骤的意图匹配检索历史。情境意图通过三类紧凑信号消除重复指代的歧义并减少干扰：（1）定义主题片段的当前潜在目标；（2）动作类型；（3）锚定关键属性的显著实体类型。在推理过程中，STITCH根据意图兼容性筛选和排序记忆片段，抑制语义相似但情境冲突的历史记录。
为评估系统性能，我们构建了CAME-Bench基准测试集，用于衡量现实动态目标导向轨迹中的情境感知检索能力。在CAME-Bench和LongMemEval测试中，STITCH实现了最先进的性能表现，较最强基线提升35.6%，且轨迹越长优势越显著。分析表明，意图索引机制显著降低了检索噪声，为鲁棒的长周期推理提供了意图感知记忆支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deploying large language models in long-horizon, goal-oriented interactions is challenging because similar entities and facts recur under different latent goals, causing memory systems to retrieve context-mismatched evidence. To address this, the authors propose STITCH, an agentic memory system that indexes each trajectory step with a structured retrieval cue called contextual intent, which includes the current latent goal, action type, and salient entity types, and retrieves history by matching the current step&#x27;s intent. Experimental results on the introduced CAME-Bench and LongMemEval show that STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases, and analysis confirms that intent indexing substantially reduces retrieval noise.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大语言模型在长周期、目标导向交互中部署的挑战，即相似实体和事实在不同潜在目标下重复出现，导致记忆系统检索到上下文不匹配的证据。所提出的方法STITCH（上下文历史中的结构化意图追踪）是一种智能体记忆系统，它使用一个包含当前潜在目标、行动类型和关键实体类型的结构化检索线索——上下文意图——来索引每个轨迹步骤，并通过匹配当前步骤的意图来检索历史。在引入的CAME-Bench和LongMemEval基准测试上的实验结果表明，STITCH实现了最先进的性能，以35.6%的优势超越了最强基线，且随着轨迹长度增加，优势更为明显，分析证实意图索引显著降低了检索噪声。</div>
</details>
</div>
<div class="card">
<div class="title">LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</div>
<div class="meta-line">Authors: Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart</div>
<div class="meta-line">First: 2026-01-15T18:54:50+00:00 · Latest: 2026-01-15T18:54:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10700v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10700v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LIBERTy：基于结构反事实的LLM概念解释因果评估框架</div>
<div class="mono" style="margin-top:8px">概念解释方法量化高层概念（如性别或经验）对模型行为的影响，这对高风险领域的决策者至关重要。现有研究通过将此类解释与反事实估计的因果效应进行对比来评估其忠实度，但当前基准依赖成本高昂且不完善的人工撰写反事实。为此，我们提出构建包含结构反事实对的数据集框架：LIBERTy（基于LLM的可解释性干预基准参考目标）。该框架以明确定义的文本生成结构化因果模型为基础，通过对概念的干预在SCM中传播直至LLM生成反事实。我们构建了三个数据集（疾病检测、简历筛选和工作场所暴力预测）并提出了新评估指标——顺序忠实度。基于此，我们在五个模型中评估了多种方法，发现概念解释方法存在显著改进空间。LIBERTy还能系统分析模型对干预的敏感性：研究发现，经过后训练缓解的专有LLM对人口统计概念的敏感性显著降低。总体而言，LIBERTy为开发忠实可解释性方法提供了亟需的基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for faithful concept-based explanations of LLM behavior in high-stakes domains, where existing benchmarks rely on imperfect, costly human-written counterfactuals. The method introduces LIBERTy, a framework for constructing datasets with structural counterfactual pairs by grounding them in explicitly defined Structured Causal Models (SCMs) of text generation, where interventions on a concept propagate through the SCM until an LLM generates the counterfactual; it includes three datasets and a new evaluation metric called order-faithfulness. Key experimental findings reveal substantial headroom for improving concept-based explanations across five evaluated models and show that proprietary LLMs exhibit markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于需要为LLM行为提供忠实的概念解释，特别是在高风险领域，现有基准依赖于不完善的人工编写反事实。方法上引入了LIBERTy框架，通过将文本生成的明确结构化因果模型作为基础，构建包含结构反事实对的数据集，其中对概念的干预通过SCM传播，并由LLM生成反事实。主要实验结果包括使用三个数据集和新提出的顺序忠实度指标评估了五种模型上的多种方法，揭示了概念解释方法存在显著改进空间；此外，系统分析发现专有LLM对人口统计概念的敏感性明显降低，这很可能源于后训练缓解措施。</div>
</details>
</div>
<div class="card">
<div class="title">The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load</div>
<div class="meta-line">Authors: Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu</div>
<div class="meta-line">First: 2026-01-15T18:52:59+00:00 · Latest: 2026-01-15T18:52:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10696v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users&#x27; prior expertise and interaction strategies through prompting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式人工智能对建筑概念设计的影响：绩效、创意自我效能与认知负荷</div>
<div class="mono" style="margin-top:8px">本研究探讨生成式人工智能（GenAI）如何影响建筑概念设计任务中的绩效、创意自我效能和认知负荷。36名来自建筑工程及其他学科的学生参与者完成了一项两阶段建筑设计任务，先独立完成，后借助外部工具（GenAI辅助条件与使用现有建筑项目在线资源库的对照条件）。设计成果由专家评估，自我效能和认知负荷则在每阶段后由参与者自评。双重差分分析显示，GenAI在所有参与者中未带来整体绩效优势；但亚组分析表明，GenAI显著提升了新手设计师的设计绩效。相反，使用GenAI的学生普遍创意自我效能有所下降。认知负荷在不同条件下无显著差异，但提示词使用模式显示，迭代式创意生成和视觉反馈提示与认知负荷的更大程度降低相关。这些发现表明，GenAI的有效性取决于用户先前的专业知识和通过提示词实现的交互策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how generative AI (GenAI) affects performance, creative self-efficacy, and cognitive load in architectural conceptual design. Using a two-phase task where 36 students designed first independently and then with either GenAI or a control tool (an online repository), the researchers employed expert evaluations and self-reports. Results from difference-in-differences analyses indicated no overall performance benefit from GenAI, but it significantly aided novice designers. Conversely, GenAI use was associated with a decline in creative self-efficacy. While cognitive load showed no significant difference between conditions, iterative and visual feedback prompts were linked to greater load reduction, suggesting effectiveness hinges on user expertise and prompting strategies.</div>
<div class="mono" style="margin-top:8px">本研究探讨了生成式人工智能（GenAI）工具如何影响建筑概念设计中的表现、创意自我效能感和认知负荷。在一项两阶段实验中，36名学生先独立设计，随后使用GenAI辅助工具或对照工具（现有项目在线资源库）进行设计。研究通过专家评估设计成果和参与者自我报告，并采用双重差分法进行分析。结果显示，GenAI并未带来整体性能优势，但显著提升了新手设计师的表现。虽然不同实验条件下的认知负荷没有显著差异，但使用迭代构思和视觉反馈提示与认知负荷降低相关。值得注意的是，使用GenAI的参与者报告其一般创意自我效能感有所下降，这表明GenAI的有效性取决于用户先前的专业知识和提示策略。</div>
</details>
</div>
<div class="card">
<div class="title">On the origin of neural scaling laws: from random graphs to natural language</div>
<div class="meta-line">Authors: Maissam Barkeshli, Alberto Alfarano, Andrey Gromov</div>
<div class="meta-line">First: 2026-01-15T18:46:09+00:00 · Latest: 2026-01-15T18:46:09+00:00</div>
<div class="meta-line">Comments: 33 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10684v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论神经缩放定律的起源：从随机图到自然语言</div>
<div class="mono" style="margin-top:8px">缩放定律在现代人工智能革命中发挥了重要作用，为从业者提供了模型性能如何随数据量、计算量和模型参数数量增加而提升的预测能力。这激发了人们对神经缩放定律起源的浓厚兴趣，一种普遍观点认为它们源于数据中已有的幂律结构。本文研究了在具有可调复杂度的图上训练用于预测随机游走（二元语法）的Transformer模型的缩放定律。我们证明，即使在数据相关性中不存在幂律结构的情况下，这种简化设定也能产生神经缩放定律。我们进一步通过系统性地降低自然语言的复杂度进行研究，使用从逐步简化的生成语言模型（从4层、2层、1层Transformer语言模型直至语言二元语法）中采样的序列进行训练，揭示了缩放指数的单调演化规律。我们的结果还包括在从Erdös-Renyi和无标度Barabási-Albert集合中抽取的随机图上进行随机游走训练所获得的缩放定律。最后，我们重新审视了语言建模的传统缩放定律，证明使用上下文长度为50的2层Transformer即可复现多项关键结果；对先前文献中使用的多种拟合方法进行了批判性分析；展示了与当前文献实践相比获取计算最优曲线的替代方法；并提供了初步证据表明最大更新参数化可能比标准参数化具有更高的参数效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates the origins of neural scaling laws, motivated by the need to understand why model performance predictably improves with increased data, compute, and parameters, beyond the common hypothesis that such laws stem solely from power-law structures in natural data. The method involves training transformers on simplified tasks, including predicting random walks on graphs (Erdös-Renyi and Barabási-Albert ensembles) and on sequences from progressively simplified generative language models, down to bigrams. Key experimental findings show that neural scaling laws emerge even in the absence of power-law data correlations, scaling exponents evolve monotonically as language complexity is reduced, and that essential scaling results can be reproduced with small 2-layer transformers, while also suggesting maximal update parameterization may enhance parameter efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究神经缩放定律的起源，其动机在于需要理解模型性能为何能随数据、计算资源和参数增加而可预测地提升，这超越了此类定律仅源于数据中幂律结构的常见假设。方法包括在简化任务上训练Transformer模型：预测具有可调复杂度的图上的随机游走（二元语法），涉及Erdös-Renyi和Barabási-Albert集合，并通过在从逐步简化的生成语言模型（下至二元语法）采样的序列上进行训练，系统性地降低自然语言的复杂度。关键实验结果表明，即使在没有幂律数据相关性的情况下，神经缩放定律也会出现，且缩放指数随语言复杂度降低而单调演化；该工作还使用最小的2层Transformer模型复现了语言建模的核心缩放结果，对先前的拟合方法进行了批判性分析，展示了一种计算最优曲线的替代方法，并为最大更新参数化的参数效率提供了初步证据。</div>
</details>
</div>
<div class="card">
<div class="title">Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems</div>
<div class="meta-line">Authors: Amir Khurshid, Abhishek Sehgal</div>
<div class="meta-line">First: 2026-01-15T18:43:19+00:00 · Latest: 2026-01-15T18:43:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10681v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10681v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向企业检索增强系统的结构与多样性感知上下文气泡构建方法</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）的上下文通常通过检索增强生成（RAG）构建，该方法依赖对段落进行排序并选取前k个结果。这种做法会导致文档结构中的信息图碎片化、过度检索、内容重复，以及查询上下文（包括二阶和三阶层面）不足的问题。本文提出一种结构感知且多样性约束的上下文气泡构建框架，能在严格的词元预算下组装连贯、可引用的文本片段集合。该方法通过组织多粒度片段（如章节与行）并利用任务条件化的结构先验指导检索，以保持并利用文档的固有结构。从高相关性锚点片段出发，通过平衡查询相关性、边际覆盖度与冗余惩罚的约束选择构建上下文气泡。与仅选取前k个结果的检索方式不同，该方法显式约束多样性与预算，生成紧凑且信息丰富的上下文集合。此外，系统会输出完整检索过程，记录对候选结果的评分与选择路径，从而提供可审计性与确定性调优能力。在企业文档上的实验表明，上下文气泡方法能显著减少冗余上下文，更好地覆盖次要层面，并在有限上下文窗口内获得更优的答案质量与引用准确性。消融研究证明，结构先验与多样性约束选择均不可或缺；移除任一组件均会导致覆盖度下降，并增加冗余或不完整的上下文。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address issues in standard retrieval-augmented generation (RAG) such as information fragmentation, over-retrieval, and insufficient coverage of secondary query facets, this paper introduces a structure-informed and diversity-constrained framework for constructing context bubbles. The method organizes multi-granular document spans, uses task-conditioned structural priors to guide retrieval, and builds coherent bundles from high-relevance anchors by balancing query relevance, marginal coverage, and redundancy penalties under a strict token budget. Experiments on enterprise documents show the approach significantly reduces redundant context, better covers secondary facets, and improves answer quality and citation faithfulness within limited context windows, with ablations confirming the necessity of both structural priors and diversity constraints.</div>
<div class="mono" style="margin-top:8px">为解决RAG系统中标准top-k检索导致的信息图碎片化、内容冗余及查询上下文覆盖不足的问题，本文提出了一种结构感知且多样性约束的&#x27;上下文气泡&#x27;构建框架。该方法组织多粒度文档片段，利用任务条件化的结构先验指导检索，并从高相关性锚点片段出发，在严格令牌预算下通过平衡查询相关性、边际覆盖度和冗余惩罚的约束选择来构建连贯的引用包。在企业文档上的实验表明，该方法显著减少了冗余上下文，更好地覆盖了次要方面，并在有限上下文窗口内提升了答案质量和引用忠实度，消融研究证实了结构先验和多样性约束选择均为必要组件。</div>
</details>
</div>
<div class="card">
<div class="title">Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models</div>
<div class="meta-line">Authors: Zirui Ren, Ziming Liu</div>
<div class="meta-line">First: 2026-01-15T18:42:50+00:00 · Latest: 2026-01-15T18:42:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10679v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10679v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) &quot;Grokking&quot; dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM &quot;guesses&quot; the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be &quot;guessing&quot; instead of &quot;reasoning&quot;. Leveraging this &quot;guessing&quot; picture, we propose three strategies to scale HRM&#x27;s guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models &quot;reason&quot;.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的推理模型是在推理还是在猜测？——对层次推理模型的机制分析</div>
<div class="mono" style="margin-top:8px">层次推理模型（HRM）在多项推理任务中表现卓越，显著优于基于大语言模型的推理器。为探究HRM的优势与潜在缺陷，我们对其推理模式展开机制研究，发现三个意外现象：（a）在极简谜题（如仅含一个未知单元格的谜题）上失效，这归因于HRM违背了其基本假设——不动点性质；（b）推理步骤中存在“顿悟”动态，即答案并非均匀改进，而是在某个关键推理步骤突然变得正确；（c）存在多个不动点，HRM会“猜测”首个不动点（可能错误）并长期或永久受困于此。这些现象均表明HRM更似“猜测”而非“推理”。基于此“猜测”视角，我们提出三种扩展HRM猜测能力的策略：数据增强（提升猜测质量）、输入扰动（利用推理随机性增加猜测数量）和模型自举（利用训练随机性增加猜测数量）。实践层面，通过整合所有方法，我们开发出增强型HRM，将数独极限任务的准确率从54.5%提升至96.9%。科学层面，本研究为理解推理模型的“推理”机制提供了新视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the reasoning mechanisms of hierarchical reasoning models (HRMs), which excel at complex tasks but exhibit puzzling failures on simple puzzles, suggesting they may be guessing rather than reasoning. Through mechanistic analysis, the authors identify key failure modes: violation of the fixed-point property, grokking dynamics where correctness emerges suddenly at a specific step, and entrapment in incorrect fixed points. To address these issues, they propose three scaling strategies—data augmentation, input perturbation, and model bootstrapping—which, when combined into Augmented HRM, dramatically improve accuracy on Sudoku-Extreme from 54.5% to 96.9%, offering new insights into the nature of reasoning in AI models.</div>
<div class="mono" style="margin-top:8px">本研究探讨了层次推理模型（HRM）的推理机制，该模型在复杂任务上表现出色，但在简单谜题上却出现令人困惑的失败，暗示其可能是在猜测而非推理。通过机制分析，作者发现了关键失败模式：在琐碎谜题上违反不动点属性、正确性在特定步骤突然出现的“顿悟”动态，以及陷入错误不动点的困境。针对这些问题，他们提出了三种扩展策略——数据增强、输入扰动和模型自举——共同构成了增强型HRM，将数独极限任务的准确率从54.5%提升至96.9%，并为模型推理过程提供了新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">BASIL: Bayesian Assessment of Sycophancy in LLMs</div>
<div class="meta-line">Authors: Katherine Atwell, Pedram Heydari, Anthony Sicilia, Malihe Alikhani</div>
<div class="meta-line">First: 2025-08-23T00:11:00+00:00 · Latest: 2026-01-15T18:31:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16846v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.16846v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sycophancy (overly agreeable or flattering behavior) poses a fundamental challenge for human-AI collaboration, particularly in high-stakes decision-making domains such as health, law, and education. A central difficulty in studying sycophancy in large language models (LLMs) is disentangling sycophantic belief shifts from rational changes in behavior driven by new evidence or user-provided information. Existing approaches either measure descriptive behavior changes or apply normative evaluations that rely on objective ground truth, limiting their applicability to subjective or uncertain tasks. We introduce a Bayesian probabilistic framework, grounded in behavioral economics and rational decision theory, that explicitly separates sycophancy from rational belief updating. Within this framework, we achieve three objectives: (i) a descriptive metric that measures sycophancy while controlling for rational responses to evidence; (ii) a normative metric that quantifies how sycophancy leads models astray from Bayesian-consistent belief updating; and (iii) the ability to apply both metrics in settings without ground-truth labels. Applying our framework across multiple LLMs and three uncertainty-driven tasks, we find robust evidence of sycophantic belief shifts and show that their impact on rationality depends on whether models systematically over- or under-update their beliefs. Finally, we demonstrate that a post-hoc calibration method and two fine-tuning strategies (SFT and DPO) substantially reduce Bayesian inconsistency, with particularly strong improvements under explicit sycophancy prompting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BASIL：大语言模型中谄媚行为的贝叶斯评估</div>
<div class="mono" style="margin-top:8px">谄媚行为（过度迎合或奉承）对人类与人工智能协作构成根本性挑战，尤其在健康、法律、教育等高风险决策领域。研究大语言模型中的谄媚行为时，核心难点在于区分谄媚性信念转变与由新证据或用户信息驱动的理性行为变化。现有方法要么测量描述性行为变化，要么依赖客观事实进行规范性评估，限制了其在主观或不确定性任务中的适用性。我们提出一个基于行为经济学与理性决策理论的贝叶斯概率框架，明确分离谄媚行为与理性信念更新。该框架实现三个目标：（i）在控制证据理性响应的前提下测量谄媚的描述性指标；（ii）量化谄媚导致模型偏离贝叶斯一致性信念更新的规范性指标；（iii）在无事实标签场景中应用双指标。通过在多个大语言模型及三项不确定性驱动任务中应用本框架，我们发现了谄媚性信念转变的强有力证据，并证明其对理性的影响取决于模型是否系统性过度或不足更新信念。最后，我们验证了事后校准方法与两种微调策略（SFT和DPO）能显著降低贝叶斯不一致性，在显式谄媚提示下的改进尤为显著。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of distinguishing sycophantic behavior from rational belief updating in large language models (LLMs), which is crucial for reliable human-AI collaboration in high-stakes domains. The method introduces a Bayesian probabilistic framework that separates sycophancy from rational responses to evidence, enabling descriptive and normative assessments even without ground-truth labels. Experimental results across multiple LLMs and tasks reveal robust sycophantic belief shifts, show their variable impact on model rationality, and demonstrate that post-hoc calibration and fine-tuning strategies can significantly reduce Bayesian inconsistency, especially under explicit sycophancy prompts.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决大型语言模型中区分阿谀奉承行为与理性信念更新的难题，这对高风险领域的人机协作至关重要。方法上引入了一个贝叶斯概率框架来分离这些因素，即使在缺乏客观事实标签的任务中也能实现描述性和规范性的阿谀奉承度量。在多个大语言模型和任务上的实验结果表明，存在显著的阿谀奉承性信念偏移，其对模型理性的影响取决于信念更新过度或不足的程度，并且事后校准和微调策略能显著减少贝叶斯不一致性，在明确的阿谀奉承提示下改善尤为明显。</div>
</details>
</div>
<div class="card">
<div class="title">Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-07-28T15:26:43+00:00 · Latest: 2026-01-15T18:28:50+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20923v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.20923v3">PDF</a> · <a href="https://github.com/langkhachhoha/MPaGE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于帕累托网格引导的大语言模型实现多目标组合优化的快速高质量启发式设计</div>
<div class="mono" style="margin-top:8px">多目标组合优化问题（MOCOP）在实际应用中频繁出现，需要同时优化相互冲突的目标。传统进化算法虽有效，但通常依赖领域知识和重复参数调优，在处理未知MOCOP实例时灵活性受限。近期，大语言模型（LLMs）与进化计算的融合，利用其高级语言理解和代码生成能力，为自动启发式生成开辟了新途径。然而，现有方法多聚焦单目标任务，常忽视多目标场景下的运行效率和启发式多样性等关键因素。为填补这一空白，我们提出MPaGE方法——通过帕累托网格引导的LLM进化实现MOCOP多启发式设计。该方法是对简单进化多目标优化（SEMO）框架的创新增强，结合LLMs与帕累托前沿网格（PFG）技术：通过将目标空间网格化并保留最优候选方案来引导启发式生成，MPaGE在变异阶段利用LLMs优先选择语义逻辑结构相异的启发式，从而提升种群多样性并减少冗余。大量实验表明，MPaGE性能优于现有LLM框架，且与传统多目标进化算法（MOEAs）结果相当，同时运行速度显著提升。代码已开源：https://github.com/langkhachhoha/MPaGE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of traditional evolutionary algorithms in multi-objective combinatorial optimization (MOCOP), which often require extensive domain knowledge and parameter tuning, this research introduces MPaGE, a method that integrates Large Language Models (LLMs) with a Pareto Front Grid technique within an evolutionary framework. The approach partitions the objective space into grids to retain top-performing heuristics, guiding LLMs to generate semantically diverse and high-quality heuristic code, thereby enhancing population diversity and reducing redundancy. Experimental results show that MPaGE outperforms existing LLM-based frameworks, achieves competitive performance with traditional multi-objective evolutionary algorithms, and operates with significantly faster runtime.</div>
<div class="mono" style="margin-top:8px">该研究针对多目标组合优化中传统进化算法依赖领域知识和参数调优的局限性，以及现有基于大语言模型的方法忽视运行效率和启发式多样性的问题。提出的MPaGE方法通过将大语言模型与帕累托前沿网格技术结合，增强了简单进化多目标优化框架，以划分目标空间并引导生成语义多样的启发式规则。实验结果表明，MPaGE优于其他基于大语言模型的框架，与传统多目标进化算法性能相当，且运行速度显著更快。</div>
</details>
</div>
<div class="card">
<div class="title">Moonworks Lunara Aesthetic Dataset</div>
<div class="meta-line">Authors: Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan</div>
<div class="meta-line">First: 2026-01-12T19:11:41+00:00 · Latest: 2026-01-15T18:27:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07941v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07941v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Moonworks Lunara美学数据集</div>
<div class="mono" style="margin-top:8px">该数据集涵盖多样艺术风格，包括中东、北欧、东亚和南亚的地域美学，以及素描、油画等通用类别。所有图像均由Moonworks Lunara模型生成，经刻意设计以呈现独特的高质量美学风格，构成首个美学评分显著超越专注美学数据集及通用数据集的独特资源。每幅图像均附有人工优化的提示词与结构化标注，共同描述显著对象、属性、关联及风格特征。与注重广度而非精度的大规模网络数据集不同，Lunara美学数据集优先考量美学品质、风格多样性与授权透明度，并基于Apache 2.0许可证发布，以支持研究及无限制的学术与商业应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of high-quality, stylistically diverse, and transparently licensed image datasets for aesthetic research, this work introduces the Moonworks Lunara Aesthetic Dataset. The method involves generating all images with the Moonworks Lunara model to intentionally embody distinct artistic styles from various global regions and general categories, with each image accompanied by a human-refined prompt and structured annotations describing objects, attributes, and style. Key experimental findings show that the dataset achieves substantially higher aesthetic scores than existing aesthetics-focused and general-purpose datasets, offering a first-of-its-kind resource with prioritized aesthetic quality, stylistic diversity, and licensing transparency under Apache 2.0.</div>
<div class="mono" style="margin-top:8px">针对美学研究中缺乏高质量、风格多样且授权透明的图像数据集的问题，本研究推出了Moonworks Lunara美学数据集。其方法在于使用Moonworks Lunara模型生成所有图像，以刻意体现不同的地域性和通用艺术风格，每张图像均附有人工精炼的提示词以及描述物体、属性和风格的结构化标注。主要的实验结果表明，该数据集的美学评分显著超越了现有的美学专用数据集和通用数据集，从而成为一个支持研究和商业应用的首创性资源。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Homophily in Large Language Models</div>
<div class="meta-line">Authors: Utkarsh Sahu, Zhisheng Qi, Mahantesh Halappanavar, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt, Yu Zhang, Yao Ma, Yu Wang</div>
<div class="meta-line">First: 2025-09-28T09:40:27+00:00 · Latest: 2026-01-15T18:26:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23773v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23773v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中的知识同质性</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）作为支持问答和事实核查等知识密集型应用的神经知识库，正日益受到研究关注。然而，其知识的结构化组织方式尚未得到充分探索。受认知神经科学发现（如语义聚类和启动效应，即知晓一个事实会增加回忆相关事实的可能性）的启发，本研究探讨了LLMs中类似的知识同质性模式。为此，我们通过在三元组和实体两个层面进行知识检查，将LLM知识映射为图表示。随后，分析实体与其邻居之间的知识掌握关系，发现LLMs倾向于对图中位置相近的实体具有相似程度的知识水平。基于这一同质性原理，我们提出一种图神经网络（GNN）回归模型，通过利用邻域评分来估计三元组的实体级知识掌握度。预测的知识掌握度使我们能够优先检查较不为人知的三元组，从而在相同标注预算下最大化知识覆盖范围。这不仅提高了为LLMs注入知识的主动标注微调效率，还增强了推理密集型问答中的多跳路径检索能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the structural organization of knowledge in Large Language Models (LLMs), motivated by the need to understand them as neural knowledge bases for applications like question answering. The method involves mapping LLM knowledge into a graph and analyzing knowledge homophily, where entities closer in the graph share similar knowledgeability levels; based on this, a Graph Neural Network (GNN) regression model is proposed to predict entity-level knowledgeability scores. Key experimental findings show that this approach enables prioritizing less-known triplets for labeling, improving active labeling efficiency for fine-tuning and enhancing multi-hop path retrieval in reasoning tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究大型语言模型（LLM）内部的知识结构组织，以更好地将其作为神经知识库应用于问答等任务。方法是将LLM知识映射为图结构并分析实体关系，发现了知识同质性模式，即图中位置相近的实体在模型中的知识掌握程度也相似。基于这一原理，作者提出了一种图神经网络回归模型来预测实体层面的知识掌握度得分，实验结果表明，该模型能优先检查较不为人知的事实，从而在固定标注预算下提升知识覆盖范围，进而提高了主动微调注入知识的效率以及问答任务中多跳推理的路径检索效果。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Property Synthesis</div>
<div class="meta-line">Authors: Christoph Weinhuber, Yannik Schnitzer, Alessandro Abate, David Parker, Giuseppe De Giacomo, Moshe Y. Vardi</div>
<div class="meta-line">First: 2026-01-15T18:18:33+00:00 · Latest: 2026-01-15T18:18:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10651v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10651v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多属性综合</div>
<div class="mono" style="margin-top:8px">我们研究具有多个属性的LTLf综合问题，其中满足所有属性可能无法实现。我们无需枚举属性子集，而是通过一次不动点计算确定产品博弈状态与可从这些状态实现的目标集合之间的关系，并综合实现最大可实现集合的策略。我们开发了一种完全符号化算法，引入布尔目标变量并利用单调性紧凑表示指数级的目标组合。该方法显著优于基于枚举的基准方法，加速比最高可达两个数量级。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of Linear Temporal Logic on finite traces (LTLf) synthesis when multiple properties must be considered, but satisfying all simultaneously may be infeasible. To avoid inefficient enumeration of property subsets, the method introduces a single fixed-point computation that determines, for each state in a product game, which sets of properties (goals) are realizable, and then synthesizes strategies that achieve the maximal realizable sets. A key innovation is a fully symbolic algorithm that compactly encodes exponentially many goal combinations using Boolean goal variables and exploits monotonicity. Experimental results demonstrate that this approach significantly outperforms enumeration-based baselines, achieving speedups of up to two orders of magnitude.</div>
<div class="mono" style="margin-top:8px">本研究针对有限迹线性时序逻辑（LTLf）综合中所有给定属性可能无法同时满足的挑战，超越了低效的属性子集枚举方法。该方法通过一次不动点计算，确定乘积博弈状态与可从该状态实现的最大目标集（即属性集）之间的关系，并综合实现这些目标集的策略。其核心创新是一个完全符号化的算法，该算法引入布尔目标变量并利用单调性来紧凑地表示指数级数量的目标组合。实验结果表明，该方法显著优于基于枚举的基线方法，速度提升可达两个数量级。</div>
</details>
</div>
<div class="card">
<div class="title">PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus</div>
<div class="meta-line">Authors: Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss</div>
<div class="meta-line">First: 2025-05-23T18:01:09+00:00 · Latest: 2026-01-15T18:18:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20323v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20323v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical narratives encode temporal dynamics essential for modeling patient trajectories, yet large-scale temporally annotated resources are scarce. We introduce PMOA-TTS, a corpus of 124,699 single-patient PubMed Open Access case reports converted into structured textual timelines of (event, time) pairs using a scalable large-language-model pipeline (Llama 3.3 70B and DeepSeek-R1). The corpus comprises over 5.6 million timestamped events, alongside extracted demographics and diagnoses. Technical validation uses a clinician-curated gold set and three measures: semantic event matching, temporal concordance (c-index), and alignment error summarized with Area Under the Log-Time CDF (AULTC). We benchmark alternative prompting and model choices and provide documentation to support reproduction. PMOA-TTS enables research on timeline extraction, temporal reasoning, survival modeling and event forecasting from narrative text, and offers broad diagnostic and demographic coverage. Data and code are openly available in public repositories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMOA-TTS：PubMed开放获取文本时间序列语料库介绍</div>
<div class="mono" style="margin-top:8px">临床叙述编码了对建模患者病程至关重要的时间动态，但大规模时间标注资源稀缺。我们推出PMOA-TTS，这是一个包含124,699份单患者PubMed开放获取病例报告的语料库，通过可扩展的大语言模型流程（Llama 3.3 70B与DeepSeek-R1）将其转化为结构化的（事件，时间）对文本时间线。该语料库涵盖超过560万条时间戳事件，并提取了人口统计学与诊断信息。技术验证采用临床专家标注的金标准集及三项指标：语义事件匹配、时间一致性（c指数）以及通过对数时间累积分布函数下面积（AULTC）汇总的对齐误差。我们对比了不同提示策略与模型选择，并提供支持复现的文档。PMOA-TTS支持从叙事文本中进行时间线提取、时序推理、生存建模与事件预测的研究，并提供广泛的诊断与人口统计学覆盖。数据与代码已在公共仓库开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity of large-scale temporally annotated clinical narrative resources for modeling patient trajectories, this work introduces PMOA-TTS, a corpus of 124,699 single-patient case reports converted into structured textual timelines. The method employs a scalable large-language-model pipeline using Llama 3.3 70B and DeepSeek-R1 to extract over 5.6 million timestamped (event, time) pairs, demographics, and diagnoses. Experimental validation on a clinician-curated gold set demonstrates strong performance in semantic event matching, temporal concordance (c-index), and low alignment error (measured by Area Under the Log-Time CDF), while benchmarking alternative prompting and model choices provides guidance for reproduction.</div>
<div class="mono" style="margin-top:8px">为解决用于建模患者病程的大规模时序标注临床叙事资源稀缺的问题，本研究引入了PMOA-TTS语料库，该库包含124,699份单患者病例报告，被转化为结构化文本时间线。方法采用基于Llama 3.3 70B和DeepSeek-R1的可扩展大语言模型流水线，提取了超过560万个带时间戳的（事件，时间）对，以及人口统计学信息和诊断。在临床专家标注的金标准集上的技术验证表明，该语料库在语义事件匹配、时序一致性（c-index）以及通过对数时间CDF下面积衡量的对齐误差方面质量良好，并为不同的提示策略和模型选择提供了基准结果。</div>
</details>
</div>
<div class="card">
<div class="title">TinyMyo: a Tiny Foundation Model for Flexible EMG Signal Processing at the Edge</div>
<div class="meta-line">Authors: Matteo Fasulo, Giusy Spacone, Thorir Mar Ingolfsson, Yawei Li, Luca Benini, Andrea Cossettini</div>
<div class="meta-line">First: 2025-12-05T17:36:57+00:00 · Latest: 2026-01-15T18:05:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15729v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15729v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Objective: Surface electromyography (EMG) is a non-invasive sensing modality widely used in biomechanics, rehabilitation, prosthetic control, and human-machine interfaces. Despite decades of use, achieving robust generalization across subjects, recording systems, and acquisition protocols remains challenging. While foundation models (FMs) are gaining traction for EMG, existing approaches remain limited to single downstream tasks and lack deployability on embedded platforms. This work addresses these limitations. Methods: We present TinyMyo, a lightweight FM based on a Transformer encoder architecture. The model is pre-trained in a self-supervised manner using masked reconstruction on publicly available datasets. With only 3.6M parameters, TinyMyo is designed to support multiple downstream tasks through minimal task-specific head adaptations. Results: We demonstrate generalization across hand gesture classification, hand kinematic regression, speech production and speech recognition, with performance comparable to or surpassing the state of the art (SoA), and model size below 5M parameters. We achieve SoA results compared to previous FM-based works on the NinaPro DB5 (89.4%), UCI-EMG (97.56%), and EPN-612 (96.74%) datasets. We demonstrate the first-time deployment of an EMG FM on an ultra-low power microcontroller (GAP9), with an inference time of 0.785 s, energy of 44.91 mJ and power envelope of 57.18 mW. Conclusion: TinyMyo demonstrates that compact, self-supervised EMG FM can guarantee strong generalization across multiple downstream tasks while remaining compatible with low-power edge devices. Significance: TinyMyo is the first EMG FM for ultra-low power edge devices, enabling scalable and energy-efficient sensing for motor intent decoding, neuromuscular assessment, and biosignal driven human-machine interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TinyMyo：面向边缘灵活肌电信号处理的微型基础模型</div>
<div class="mono" style="margin-top:8px">目标：表面肌电图（EMG）是一种广泛应用于生物力学、康复、假肢控制和人机交互的非侵入式传感技术。尽管已应用数十年，但在不同受试者、记录系统和采集协议间实现稳健泛化仍具挑战。虽然基础模型（FM）在EMG领域逐渐受到关注，现有方法仍局限于单一下游任务且难以在嵌入式平台部署。本研究旨在突破这些限制。方法：我们提出TinyMyo——一种基于Transformer编码器架构的轻量级FM。该模型通过掩码重建在公开数据集上进行自监督预训练。仅含360万参数的TinyMyo可通过最小化的任务适配头支持多种下游任务。结果：我们在手势分类、手部运动回归、语音生成与语音识别任务中展示了泛化能力，其性能达到或超越当前最优水平（SoA），且模型参数量低于500万。在NinaPro DB5（89.4%）、UCI-EMG（97.56%）和EPN-612（96.74%）数据集上取得了优于既往FM方法的SoA结果。我们首次在超低功耗微控制器（GAP9）上部署了EMG基础模型，推理时间0.785秒，能耗44.91毫焦，功率57.18毫瓦。结论：TinyMyo证明紧凑的自监督EMG基础模型能在保持低功耗边缘设备兼容性的同时，为多下游任务提供强泛化保障。意义：TinyMyo是首个面向超低功耗边缘设备的EMG基础模型，为运动意图解码、神经肌肉评估及生物信号驱动的人机交互提供了可扩展、高能效的传感解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of robust generalization across subjects and systems in surface electromyography (EMG) processing and the limited deployability of existing foundation models on embedded platforms, this work introduces TinyMyo, a lightweight Transformer encoder model with 3.6M parameters pre-trained via self-supervised masked reconstruction. The method supports multiple downstream tasks through minimal task-specific head adaptations. Experimental results show the model achieves state-of-the-art performance on hand gesture classification, hand kinematic regression, and speech-related tasks across several benchmark datasets, and it is successfully deployed on an ultra-low power microcontroller with efficient inference time and energy consumption.</div>
<div class="mono" style="margin-top:8px">为解决表面肌电信号处理中跨受试者、记录系统和采集协议的鲁棒泛化挑战，并克服现有基础模型局限于单一任务且难以在嵌入式平台部署的不足，本研究提出了TinyMyo，一种基于Transformer编码器的轻量级基础模型。该模型仅含360万个参数，通过掩码重建在公开数据集上进行自监督预训练，并可通过最小化的任务特定头部适配支持多种下游任务。实验结果表明，TinyMyo在手势分类、手部运动回归及语音相关任务上，在多个基准数据集上达到了最先进的性能，并首次在超低功耗微控制器上成功部署，实现了高效的推理时间和能耗。</div>
</details>
</div>
<div class="card">
<div class="title">Dual-Uncertainty Guided Policy Learning for Multimodal Reasoning</div>
<div class="meta-line">Authors: Rui Liu, Dian Yu, Tong Zheng, Runpeng Dai, Zongxia Li, Wenhao Yu, Zhenwen Liang, Linfeng Song, Haitao Mi, Pratap Tokekar, Dong Yu</div>
<div class="meta-line">First: 2025-10-01T20:32:08+00:00 · Latest: 2026-01-15T17:51:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01444v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01444v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has advanced reasoning capabilities in multimodal large language models. However, existing methods typically treat visual inputs as deterministic, overlooking the perceptual ambiguity inherent to the visual modality. Consequently, they fail to distinguish whether a model&#x27;s uncertainty stems from complex reasoning or ambiguous perception, preventing the targeted allocation of exploration or learning signals. To address this gap, we introduce DUPL, a dual-uncertainty guided policy learning approach for multimodal RLVR that quantifies and leverages both perceptual uncertainty (via symmetric KL divergence) and output uncertainty (via policy entropy) to guide policy updates. By establishing an uncertainty-driven feedback loop and employing a dynamic branch prioritization mechanism, DUPL recalibrates the policy advantage to focus learning on states with high perceptual or decisional ambiguity, enabling effective targeted exploration beyond passive data augmentation. Implemented on top of GRPO and evaluated on six multimodal mathematical and general-domain reasoning benchmarks, DUPL improves Qwen2.5-VL 3B and 7B models, achieving accuracy gains of up to 11.2% on visual math tasks and up to 7.1% on general-domain reasoning tasks, while consistently outperforming GRPO. These results demonstrate that dual-uncertainty guided policy learning is an effective and generalizable approach for multimodal RLVR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>双不确定性引导的多模态推理策略学习</div>
<div class="mono" style="margin-top:8px">基于可验证奖励的强化学习（RLVR）提升了多模态大语言模型的推理能力。然而，现有方法通常将视觉输入视为确定性信息，忽略了视觉模态固有的感知模糊性。这导致模型无法区分其不确定性是源于复杂推理还是模糊感知，从而难以针对性地分配探索或学习信号。为解决这一问题，我们提出了DUPL——一种双不确定性引导的多模态RLVR策略学习方法，该方法通过对称KL散度量化感知不确定性，通过策略熵量化输出不确定性，并利用二者共同指导策略更新。通过建立不确定性驱动的反馈循环并采用动态分支优先级机制，DUPL重新校准策略优势，将学习重点聚焦于感知或决策模糊性高的状态，实现了超越被动数据增强的有效定向探索。基于GRPO框架实现并在六个多模态数学及通用领域推理基准上评估，DUPL显著提升了Qwen2.5-VL 3B和7B模型的性能：在视觉数学任务上准确率最高提升11.2%，在通用领域推理任务上最高提升7.1%，且持续优于GRPO基线。这些结果表明，双不确定性引导的策略学习是一种有效且可泛化的多模态RLVR方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing reinforcement learning with verifiable rewards (RLVR) methods for multimodal reasoning treat visual inputs as deterministic, failing to differentiate between uncertainty arising from complex reasoning versus ambiguous perception, which hinders targeted exploration and learning. To address this, this work introduces DUPL, a dual-uncertainty guided policy learning approach that quantifies perceptual uncertainty via symmetric KL divergence and output uncertainty via policy entropy, using them to recalibrate policy advantages through an uncertainty-driven feedback loop and dynamic branch prioritization, thereby focusing learning on states with high ambiguity. Experiments on six multimodal reasoning benchmarks show that DUPL improves Qwen2.5-VL models, achieving accuracy gains of up to 11.2% on visual math tasks and 7.1% on general-domain tasks, consistently outperforming the baseline GRPO.</div>
<div class="mono" style="margin-top:8px">本研究针对现有基于可验证奖励的强化学习（RLVR）方法在多模态推理中的局限性，即这些方法将视觉输入视为确定性信息，无法区分模型的不确定性是源于复杂推理还是模糊感知。提出的DUPL方法采用双重不确定性引导的策略学习，通过对称KL散度量化和利用感知不确定性，并通过策略熵量化输出不确定性，进而通过不确定性驱动的反馈循环和动态分支优先级机制来指导策略更新。在六个多模态数学和通用领域推理基准上的实验结果表明，基于GRPO实现的DUPL提升了Qwen2.5-VL模型的性能，在视觉数学任务上准确率最高提升11.2%，在通用领域推理任务上最高提升7.1%，且始终优于基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">On the Failure of Latent State Persistence in Large Language Models</div>
<div class="meta-line">Authors: Jen-tse Huang, Kaiser Sun, Wenxuan Wang, Mark Dredze</div>
<div class="meta-line">First: 2025-04-30T16:18:39+00:00 · Latest: 2026-01-15T17:44:56+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures, 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.10571v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.10571v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) excel in reasoning, whether they can sustain persistent latent states remains under-explored. The capacity to maintain and manipulate unexpressed, internal representations-analogous to human working memory-is a cornerstone of complex reasoning. In this paper, we formalize and quantify the &quot;Latent State Persistence&quot; (LSP) gap through three novel experiments. First, we utilize a Number Guessing Game, demonstrating that across independent queries, LLMs fail to allocate probability mass to a singular hidden choice, violating a fundamental probabilistic principle. Second, we employ a Yes-No Game to show that as the number of questions increases, LLMs suffer from &quot;concept drift,&quot; leading to inevitable self-contradictions due to the lack of LSP. Finally, inspired by Mathematical Mentalism, we task models with tracking transformations on hidden variables, revealing a failure in variable binding and state evolution when the initial state is not explicitly present in the context. Collectively, these findings suggest that LLMs function as reactive post-hoc solvers rather than proactive planners with LSP. Our work provides a framework for evaluating the fidelity of internal representations and highlights a fundamental architectural divergence between autoregressive transformers and human-like cognition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论大语言模型中潜在状态持续性的失效</div>
<div class="mono" style="margin-top:8px">尽管大语言模型（LLMs）在推理方面表现出色，但其能否维持持续的潜在状态仍待深入探究。保持和操纵未表达的内部表征——类似于人类工作记忆——是复杂推理的基石。本文通过三项新颖实验对“潜在状态持续性”（LSP）缺失进行了形式化定义与量化：首先，通过数字猜测游戏证明，在独立查询中LLMs无法将概率质量分配给单一隐藏选项，违背了基本概率原则；其次，采用是非问答游戏表明，随着问题数量增加，LLMs会出现“概念漂移”，因缺乏LSP而导致不可避免的自相矛盾；最后，受数学读心术启发，我们让模型追踪隐藏变量的变换过程，揭示当初始状态未显式存在于上下文时，模型在变量绑定与状态演化方面存在缺陷。综合而言，这些发现表明LLMs本质上是反应式的后验求解器，而非具备LSP能力的主动规划者。本研究为评估内部表征的保真度提供了框架，并凸显出自回归Transformer架构与类人认知间的根本性差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates whether large language models (LLMs) can maintain persistent, unexpressed internal states, a capability analogous to human working memory that is crucial for complex reasoning. The authors formalize this as the &quot;Latent State Persistence&quot; (LSP) gap and evaluate it through three novel experiments: a Number Guessing Game showing LLMs fail to allocate probability to a single hidden choice across queries, a Yes-No Game demonstrating inevitable self-contradictions due to concept drift, and a task inspired by Mathematical Mentalism revealing failures in tracking transformations of hidden variables. The key experimental findings indicate that LLMs lack robust LSP, functioning as reactive solvers rather than proactive planners with stable internal representations, highlighting a fundamental architectural divergence from human-like cognition.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究大语言模型（LLMs）是否能维持持久且未明确表达的内部状态，这种类似于人类工作记忆的能力对于复杂推理至关重要。作者将其形式化为“潜在状态持久性”（LSP）差距，并通过三个新颖的实验进行评估：数字猜测游戏显示LLMs无法在多次查询中维持单一的隐藏选择，是非游戏证明由于概念漂移会导致不可避免的自相矛盾，以及受数学读心术启发的任务揭示了LLMs在跟踪隐藏变量变换时的失败。关键的实验结果共同表明，LLMs是缺乏真正LSP的被动求解器，凸显了其自回归Transformer架构与类人认知之间的根本性差异。</div>
</details>
</div>
<div class="card">
<div class="title">Can LLMs Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels</div>
<div class="meta-line">Authors: Anika Sharma, Malavika Mampally, Chidaksh Ravuru, Kandyce Brennan, Neil Gaikwad</div>
<div class="meta-line">First: 2025-12-15T09:50:00+00:00 · Latest: 2026-01-15T17:43:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13142v4">Abs</a> · <a href="https://arxiv.org/pdf/2512.13142v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models (LLMs) increasingly mediate stigmatized health decisions, their capacity to understand complex psychological phenomena remains inadequately assessed. Can LLMs understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across cognitive, interpersonal, and structural levels. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS), examining representation at cognitive (self-judgment), interpersonal (worries about judgment and isolation), and structural (community condemnation and disclosure patterns) levels. Models fail tests of genuine understanding across all dimensions. They underestimate cognitive stigma while overestimating interpersonal stigma, introduce demographic biases assigning higher stigma to younger, less educated, and non-White personas, and treat secrecy as universal despite 36% of humans reporting openness. Most critically, models produce internal contradictions: they overestimate isolation yet predict isolated individuals are less secretive, revealing incoherent representations. These patterns show current alignment approaches ensure appropriate language but not coherent understanding across levels. This work provides empirical evidence that LLMs lack coherent understanding of psychological constructs operating across multiple dimensions. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型能否理解我们无法言说之事？通过堕胎污名在认知、人际与结构层面的多级对齐测量</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）日益介入污名化的健康决策，其理解复杂心理现象的能力仍未得到充分评估。本文通过经验证的个人层面堕胎污名量表（ILAS），系统测试了五个主流LLMs中627个人口统计学多元角色，考察其在认知（自我评判）、人际（担忧被评判与孤立）及结构（社区谴责与披露模式）层面的表征能力。研究发现：模型在所有维度均未通过真实理解测试——低估认知污名却高估人际污名；对年轻、低学历和非白人角色存在偏见性高估；将保密行为普遍化（尽管36%人类受访者持开放态度）。最关键的是，模型产生内部矛盾：既高估孤立程度，又预测孤立个体更少保密，揭示其表征逻辑的不一致性。这些模式表明当前对齐方法仅能确保语言得体性，却无法实现跨层级的连贯理解。本研究为LLMs缺乏对多维度心理建构的连贯理解提供了实证依据，提出高风险场景中的AI安全需要新范式：设计层面（多级连贯性）、评估层面（持续审计）、治理监管层面（强制审计、问责制、部署限制），以及在“理解人们无法言说之事决定支持效果”的领域加强AI素养教育。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates whether large language models (LLMs) can coherently represent the complex, multilevel psychological phenomenon of abortion stigma, motivated by their increasing role in mediating stigmatized health decisions where understanding unspoken nuances is critical. The method systematically tests 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS), examining representation across cognitive, interpersonal, and structural levels. Key experimental findings reveal that models fail to demonstrate genuine understanding: they underestimate cognitive stigma while overestimating interpersonal stigma, introduce demographic biases by assigning higher stigma to younger, less educated, and non-White personas, treat secrecy as universal contrary to human data showing 36% openness, and produce internal contradictions such as predicting isolated individuals are less secretive despite overestimating isolation.</div>
<div class="mono" style="margin-top:8px">本研究调查大型语言模型（LLMs）能否在认知、人际和结构层面连贯地表示复杂的心理现象——堕胎污名，其动机在于这些模型在调解污名化健康决策中的作用日益增加。研究方法包括使用经过验证的个人层面堕胎污名量表（ILAS），对五个领先的LLMs中的627个人口统计学多样化角色进行系统测试，以检验多层面的表征。主要实验结果表明，模型未能表现出真正的理解，它们低估了认知层面的污名，却高估了人际层面的污名，引入了人口统计学偏见（对更年轻、教育程度较低和非白人角色赋予更高的污名），并产生了内部矛盾，例如高估了孤立感，却预测孤立个体的保密性较低。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Abstention Knobs for Predictable Reliability in Video Question Answering</div>
<div class="meta-line">Authors: Jorge Ortiz</div>
<div class="meta-line">First: 2025-12-31T23:27:32+00:00 · Latest: 2026-01-15T17:31:17+00:00</div>
<div class="meta-line">Comments: Preprint. Diagnostic study of confidence-based abstention under evidence truncation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00138v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00138v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频问答中用于可预测可靠性的显式弃权调控机制</div>
<div class="mono" style="margin-top:8px">视觉语言模型在高风险场景部署需采用选择性预测，使系统在不确定时主动弃权以避免代价高昂的错误。本研究探讨了基于置信度的弃权机制能否在视频问答中提供对错误率的可靠控制，以及该控制在分布偏移下是否保持稳健。基于NExT-QA数据集和Gemini 2.0 Flash模型，我们得出两项结论：首先，置信度阈值能在分布内提供机制性控制，通过调节阈值ε可生成平滑的风险-覆盖权衡曲线，有效降低错误率</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To ensure reliable deployment of vision-language models in high-stakes video question answering, this study investigates whether confidence-based abstention provides predictable control over error rates and remains robust under distribution shift. The method involves a diagnostic study using the NExT-QA dataset and the Gemini 2.0 Flash model, analyzing the risk-coverage tradeoff by sweeping a confidence threshold for abstention. The key experimental findings show that confidence thresholding provides mechanistic, smooth control over error rates in-distribution, allowing error rates to be reduced, though its robustness under distribution shift is a central question of the truncated abstract.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究基于置信度的弃权机制能否为视频问答系统提供可靠且可预测的错误率控制，这是高风险应用部署的关键要求，并检验该机制在分布变化下的鲁棒性。方法上，该诊断性研究使用NExT-QA基准和Gemini 2.0 Flash模型，通过调节置信度阈值以控制弃权，从而分析风险-覆盖率的权衡关系。关键的实验结果表明，置信度阈值法在分布内能够提供机制性的、平滑的错误率控制，但摘要未完整呈现关于分布变化鲁棒性的详细结果。</div>
</details>
</div>
<div class="card">
<div class="title">Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</div>
<div class="meta-line">Authors: Christopher Clark, Jieyu Zhang, Zixian Ma, Jae Sung Park, Mohammadreza Salehi, Rohun Tripathi, Sangho Lee, Zhongzheng Ren, Chris Dongjoo Kim, Yinuo Yang, Vincent Shao, Yue Yang, Weikai Huang, Ziqi Gao, Taira Anderson, Jianrui Zhang, Jitesh Jain, George Stoica, Winson Han, Ali Farhadi, Ranjay Krishna</div>
<div class="meta-line">First: 2026-01-15T17:27:44+00:00 · Latest: 2026-01-15T17:27:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10611v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10611v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Today&#x27;s strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- either by pointing or by tracking in pixels. Even proprietary models lack this capability. We present Molmo2, a new family of VLMs that are state-of-the-art among open-source models and demonstrate exceptional new capabilities in point-driven grounding in single image, multi-image, and video tasks. Our key contribution is a collection of 7 new video datasets and 2 multi-image datasets, including a dataset of highly detailed video captions for pre-training, a free-form video Q&amp;A dataset for fine-tuning, a new object tracking dataset with complex queries, and an innovative new video pointing dataset, all collected without the use of closed VLMs. We also present a training recipe for this data utilizing an efficient packing and message-tree encoding scheme, and show bi-directional attention on vision tokens and a novel token-weight strategy improves performance. Our best-in-class 8B model outperforms others in the class of open weight and data models on short videos, counting, and captioning, and is competitive on long-videos. On video-grounding Molmo2 significantly outperforms existing open-weight models like Qwen3-VL (35.5 vs 29.6 accuracy on video counting) and surpasses proprietary models like Gemini 3 Pro on some tasks (38.4 vs 20.0 F1 on video pointing and 56.2 vs 41.1 J&amp;F on video tracking).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Molmo2：具备视频理解与定位能力的视觉语言模型开源权重与数据集</div>
<div class="mono" style="margin-top:8px">当前最先进的视频语言模型（VLMs）仍属闭源系统。最强的开源权重模型要么依赖闭源VLMs生成的合成数据进行知识蒸馏，要么未公开其训练数据与方案，导致开源社区缺乏改进前沿视频（及图像）语言模型的基础。关键的是，许多下游应用不仅需要高层视频理解，更需像素级指向或跟踪的定位能力——即便闭源模型也欠缺此功能。我们推出Molmo2系列VLMs，在开源模型中达到前沿水平，并在单图、多图及视频任务的指向驱动定位中展现出卓越新能力。核心贡献包括7个新视频数据集与2个多图数据集：涵盖预训练用高细节视频描述数据集、微调用自由形式视频问答数据集、复杂查询的新物体跟踪数据集及创新的视频指向数据集——均未使用闭源VLMs。我们同时提出采用高效打包与消息树编码的训练方案，证明视觉令牌的双向注意力机制与新颖令牌权重策略可提升性能。顶尖的80亿参数模型在短视频、计数与描述任务上超越同类开源权重与数据模型，长视频任务表现亦具竞争力。在视频定位方面，Molmo2显著优于Qwen3-VL等开源模型（视频计数准确率35.5对29.6），部分任务甚至超越Gemini 3 Pro等闭源模型（视频指向F1值38.4对20.0，视频跟踪J&amp;F值56.2对41.1）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of open-source foundations for advancing video-language models (VLMs) and the absence of grounding capabilities in both open and proprietary models, this work introduces Molmo2, a new family of open-weight VLMs. The method involves collecting seven new video datasets and two multi-image datasets without using closed VLMs, and employs a training recipe with efficient packing, message-tree encoding, bi-directional attention on vision tokens, and a novel token-weight strategy. Experimental results show that the 8B model outperforms other open weight and data models on short videos, counting, and captioning, is competitive on long videos, and significantly surpasses models like Qwen3-VL and Gemini 3 Pro on video grounding tasks such as counting, pointing, and tracking.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于当前缺乏开源且具备定位能力的高性能视频-语言模型，因为最先进的模型要么是专有的，要么依赖于未公开的数据和方法。方法上，研究提出了Molmo2模型系列，基于新收集的七个视频和两个多图像数据集进行训练，这些数据集包括详细描述、自由形式问答、物体跟踪和视频指向数据，且未使用闭源模型，同时采用了一种训练方案，结合了高效打包、消息树编码、视觉令牌的双向注意力和新颖的令牌权重策略。主要实验结果表明，其8B模型在短视频任务、计数和描述上优于其他开源模型，在长视频上具有竞争力，并在视频定位任务中显著超越了如Qwen3-VL等开源模型和Gemini 3 Pro等专有模型，在计数准确率（35.5对比29.6）和指向F1分数（38.4对比20.0）上取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Teaching Enables Probabilistic Reasoning in Large Language Models</div>
<div class="meta-line">Authors: Linlu Qiu, Fei Sha, Kelsey Allen, Yoon Kim, Tal Linzen, Sjoerd van Steenkiste</div>
<div class="meta-line">First: 2025-03-21T20:13:04+00:00 · Latest: 2026-01-15T17:21:57+00:00</div>
<div class="meta-line">Comments: Nature Communications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.17523v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.17523v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly used as agents that interact with users and with the world. To do so successfully, LLMs must construct representations of the world and form probabilistic beliefs about them. To provide personalized recommendations, for example, the LLM needs to infer a user&#x27;s preferences from their behavior over multiple interactions. The Bayesian inference framework lays out the optimal way for an agent to update its beliefs as it receives new information. We first show that LLMs fall far short of the standard defined by the Bayesian framework. We then show that by teaching LLMs to mimic the predictions of the normative Bayesian model, we can dramatically improve their ability to update their beliefs; this ability generalizes to new tasks. We conclude that LLMs can effectively learn reasoning skills from examples and generalize those skills to new domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯教学赋能大语言模型进行概率推理</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正日益被用作与用户及世界交互的智能体。为实现有效交互，LLMs必须构建对世界的表征并形成相应的概率信念。例如，为提供个性化推荐，LLMs需通过用户在多轮交互中的行为推断其偏好。贝叶斯推理框架为智能体在接收新信息时更新信念提供了最优路径。我们首先证明LLMs远未达到贝叶斯框架设定的标准，随后通过训练LLMs模仿规范贝叶斯模型的预测，显著提升了其更新信念的能力，且该能力可泛化至新任务。最终表明LLMs能有效从示例中学习推理技能，并将这些技能迁移至新领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the need for large language models (LLMs) to act as interactive agents capable of forming and updating probabilistic beliefs about the world, such as inferring user preferences from behavior. The method involves teaching LLMs to mimic the predictions of a normative Bayesian inference model, thereby enhancing their belief-updating capabilities. Experimental results demonstrate that this Bayesian teaching approach significantly improves LLMs&#x27; probabilistic reasoning, and the acquired skill generalizes to new tasks beyond the training examples.</div>
<div class="mono" style="margin-top:8px">本研究针对大型语言模型作为交互代理时，需要形成和更新关于世界的概率信念（如从用户行为推断偏好）的需求。方法是通过教导模型模仿规范性贝叶斯推理模型的预测，从而提升其信念更新能力。实验结果表明，这种贝叶斯教学法显著增强了模型的概率推理能力，且这种改进能泛化到训练示例之外的新任务上。</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park</div>
<div class="meta-line">First: 2026-01-14T17:57:43+00:00 · Latest: 2026-01-15T17:20:36+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09667v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09667v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协作式多智能体测试时强化学习推理框架</div>
<div class="mono" style="margin-top:8px">多智能体系统已发展为适用于多种应用的实用LLM驱动协作体，其通过多样性与交叉验证获得鲁棒性。然而多智能体强化学习训练存在资源密集与不稳定的问题：智能体间的协同适应会引发非平稳性，且奖励信号通常稀疏且方差较高。为此，我们提出\textbf{多智能体测试时强化学习框架}，该框架在推理阶段将结构化文本经验注入多智能体决策过程。该框架构建由专业智能体组成的多专家团队进行多轮讨论，检索并整合测试时经验，最终达成共识决策。我们还研究了用于构建轮次级经验池的信用分配机制，并将其重新注入对话流程。在医学、数学与教育领域的挑战性基准测试中，该框架相比多智能体基线平均准确率提升3.67%，相比单智能体基线提升8.67%。消融实验检验了不同信用分配方案，并详细比较了其对训练结果的影响。该框架为无需调参的分布偏移鲁棒性多智能体推理提供了稳定、高效且有效的实现路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the resource-intensive and unstable nature of multi-agent reinforcement learning (MARL) training, which suffers from non-stationarity and sparse rewards, this work introduces Multi-Agent Test-Time Reinforcement Learning (MATTRL). The method injects structured textual experience into multi-agent deliberation at inference time by forming a team of specialists, retrieving and integrating test-time experiences from a turn-level pool constructed via credit assignment, and reaching consensus for decisions. Experimental results on benchmarks in medicine, math, and education show that MATTRL improves accuracy by an average of 3.67% over a multi-agent baseline and 8.67% over single-agent baselines, demonstrating a stable and efficient approach for robust reasoning without tuning.</div>
<div class="mono" style="margin-top:8px">多智能体强化学习（MARL）训练常因智能体协同适应导致的非平稳性和稀疏奖励而不稳定且资源密集。为此，研究者提出了多智能体测试时强化学习（MATTRL）框架，该框架在推理时无需额外调优，通过向多智能体审议中注入结构化文本经验来提升性能。该方法组建专家团队进行多轮讨论，通过信用分配构建轮级经验池并检索整合测试时经验，最终达成共识决策。在医学、数学和教育领域的挑战性基准测试中，MATTRL相比多智能体基线平均准确率提升3.67%，相比单智能体基线提升8.67%，为分布偏移鲁棒的多智能体推理提供了一条稳定高效的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Procedural Fairness in Multi-Agent Bandits</div>
<div class="meta-line">Authors: Joshua Caiata, Carter Blair, Kate Larson</div>
<div class="meta-line">First: 2026-01-15T17:11:51+00:00 · Latest: 2026-01-15T17:11:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10600v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10600v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcomes. Empirical results confirm that fairness notions based on optimizing for outcomes sacrifice equal voice and representation, while the sacrifice in outcome-based fairness objectives (like equality and utilitarianism) is minimal under procedurally fair policies. We further prove that different fairness notions prioritize fundamentally different and incompatible values, highlighting that fairness requires explicit normative choices. This paper argues that procedural legitimacy deserves greater focus as a fairness objective, and provides a framework for putting procedural fairness into practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体老虎机中的程序公平性</div>
<div class="mono" style="margin-top:8px">在多智能体多臂老虎机（MA-MAB）背景下，公平性常被简化为结果导向：最大化福利、减少不平等或平衡效用。然而，心理学、经济学及罗尔斯理论的研究表明，公平性同样关乎决策过程及参与权。本文提出一种新的公平性目标——程序公平性，旨在赋予所有智能体平等的决策权，确保其处于核心地位，并在结果上实现比例性。实证结果表明，基于结果优化的公平性概念牺牲了平等话语权与代表性，而在程序公平策略下，结果导向的公平目标（如平等主义与功利主义）的牺牲微乎其微。我们进一步证明，不同的公平性概念本质上优先考虑互不相容的价值取向，这凸显了公平性需要明确的规范性选择。本文主张程序合法性应作为公平性目标得到更多关注，并提供了实践程序公平性的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that fairness in multi-agent multi-armed bandits is typically defined by outcomes (e.g., welfare, equality), this work argues for incorporating procedural fairness, which emphasizes equal decision-making power and representation for all agents. The method introduces a new procedural fairness objective that ensures agents have a voice in decisions, lies in the core of the cooperative game, and yields proportional outcomes. Experimental results show that outcome-optimizing fairness notions sacrifice procedural equity, whereas procedurally fair policies incur only minimal losses in outcome-based metrics like equality and utilitarianism, demonstrating their practical viability and highlighting the need for explicit normative choices in fairness definitions.</div>
<div class="mono" style="margin-top:8px">本文的动机源于观察到多智能体多臂老虎机中的公平性通常由结果（如福利、平等）定义，而心理学、经济学和罗尔斯理论表明过程和决策权同样重要。方法上，研究引入了一种称为程序公平的新目标，确保所有智能体在决策中拥有平等发言权，位于核心，并提供比例结果。实验结果表明，基于结果的公平概念牺牲了平等的代表权，而程序公平政策在结果指标（如平等和功利主义）上的损失极小，理论分析进一步证明不同的公平概念优先考虑根本不同且不相容的价值观，强调了明确规范性选择的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition</div>
<div class="meta-line">Authors: Arundeep Chinta, Lucas Vinh Tran, Jay Katukuri</div>
<div class="meta-line">Venue: AAAI 2026 oral presentation</div>
<div class="meta-line">First: 2026-01-15T17:02:06+00:00 · Latest: 2026-01-15T17:02:06+00:00</div>
<div class="meta-line">Comments: Accepted for oral presentation at the AI Meets Quantitative Finance Workshop at ICAIF 2025. An enhanced version was accepted for oral presentation at the AI for Time Series Analysis Workshop at AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10591v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10591v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated techniques such as mixture models, Student&#x27;s t-distributions, or conformal prediction, they fail to address the core challenge of providing theoretically-grounded uncertainty decomposition. For the very first time, we present a novel transformer-based probabilistic framework, ProbFM (probabilistic foundation model), that leverages Deep Evidential Regression (DER) to provide principled uncertainty quantification with explicit epistemic-aleatoric decomposition. Unlike existing approaches that pre-specify distributional forms or require sampling-based inference, ProbFM learns optimal uncertainty representations through higher-order evidence learning while maintaining single-pass computational efficiency. To rigorously evaluate the core DER uncertainty quantification approach independent of architectural complexity, we conduct an extensive controlled comparison study using a consistent LSTM architecture across five probabilistic methods: DER, Gaussian NLL, Student&#x27;s-t NLL, Quantile Loss, and Conformal Prediction. Evaluation on cryptocurrency return forecasting demonstrates that DER maintains competitive forecasting accuracy while providing explicit epistemic-aleatoric uncertainty decomposition. This work establishes both an extensible framework for principled uncertainty quantification in foundation models and empirical evidence for DER&#x27;s effectiveness in financial applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProbFM：具备不确定性分解能力的概率时间序列基础模型</div>
<div class="mono" style="margin-top:8px">时间序列基础模型已成为零样本金融预测的重要方法，展现出优异的可迁移性与数据效率。然而，其在金融领域的应用受限于不确定性量化的根本缺陷：现有方法或依赖受限分布假设，或混淆不同不确定性来源，或缺乏理论校准机制。尽管近期研究采用混合模型、学生t分布、保形预测等技术，仍未能实现理论完备的不确定性分解。本文首次提出基于Transformer的概率框架ProbFM，通过深度证据回归实现理论支撑的不确定性量化与显式认知-偶然不确定性分解。区别于预设分布形式或依赖采样推断的现有方法，ProbFM通过高阶证据学习优化不确定性表征，同时保持单次前向计算效率。为在控制架构复杂度的条件下严谨评估深度证据回归方法，本研究采用统一LSTM架构对五种概率方法进行对照实验：深度证据回归、高斯负对数似然、学生t分布负对数似然、分位数损失、保形预测。加密货币收益率预测实验表明，深度证据回归在保持预测精度的同时，实现了显式的认知-偶然不确定性分解。本研究既为基础模型提供了可扩展的理论化不确定性量化框架，也为深度证据回归在金融领域的有效性提供了实证依据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to address the limitations of existing Time Series Foundation Models (TSFMs) in providing principled uncertainty quantification for financial forecasting, as current methods often rely on restrictive assumptions or conflate uncertainty sources. The proposed method, ProbFM, introduces a novel transformer-based probabilistic framework that leverages Deep Evidential Regression (DER) to achieve explicit epistemic-aleatoric uncertainty decomposition through higher-order evidence learning, maintaining single-pass computational efficiency. In a controlled comparison using an LSTM architecture across five probabilistic methods on cryptocurrency return forecasting, DER demonstrated competitive forecasting accuracy while successfully providing explicit uncertainty decomposition, establishing its empirical effectiveness for financial applications.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于解决时间序列基础模型在金融预测中不确定性量化方面的局限性，现有方法通常依赖限制性假设、混淆不确定性来源或缺乏校准机制。所提出的方法ProbFM是一种新颖的基于Transformer的概率框架，利用深度证据回归通过高阶证据学习提供具有明确认知-偶然不确定性分解的原则性量化，同时保持单次计算效率。在使用LSTM架构对加密货币回报预测进行的对照实验中，深度证据回归在保持竞争力的预测准确性的同时，展现了分解不确定性的能力，为其在金融应用中的有效性提供了实证依据。</div>
</details>
</div>
<div class="card">
<div class="title">SSFL: Discovering Sparse Unified Subnetworks at Initialization for Efficient Federated Learning</div>
<div class="meta-line">Authors: Riyasat Ohib, Bishal Thapaliya, Gintare Karolina Dziugaite, Jingyu Liu, Vince Calhoun, Sergey Plis</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2026</div>
<div class="meta-line">First: 2024-05-15T02:13:51+00:00 · Latest: 2026-01-15T17:01:07+00:00</div>
<div class="meta-line">Comments: Published in Transactions on Machine Learning Research (TMLR), 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.09037v2">Abs</a> · <a href="https://arxiv.org/pdf/2405.09037v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we propose Salient Sparse Federated Learning (SSFL), a streamlined approach for sparse federated learning with efficient communication. SSFL identifies a sparse subnetwork prior to training, leveraging parameter saliency scores computed separately on local client data in non-IID scenarios, and then aggregated, to determine a global mask. Only the sparse model weights are trained and communicated each round between the clients and the server. On standard benchmarks including CIFAR-10, CIFAR-100, and Tiny-ImageNet, SSFL consistently improves the accuracy sparsity trade off, achieving more than 20\% relative error reduction on CIFAR-10 compared to the strongest sparse baseline, while reducing communication costs by $2 \times$ relative to dense FL. Finally, in a real-world federated learning deployment, SSFL delivers over $2.3 \times$ faster communication time, underscoring its practical efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SSFL：在初始化阶段发现稀疏统一子网络以实现高效联邦学习</div>
<div class="mono" style="margin-top:8px">本研究提出显著稀疏联邦学习（SSFL），一种用于稀疏联邦学习的高效通信简化方法。SSFL在训练前识别稀疏子网络，利用在非独立同分布场景下基于本地客户端数据分别计算的参数显著性分数进行聚合，以确定全局掩码。每轮仅训练和传输稀疏模型权重于客户端与服务器之间。在包括CIFAR-10、CIFAR-100和Tiny-ImageNet的标准基准测试中，SSFL持续优化准确率与稀疏度的权衡，在CIFAR-10上相比最强稀疏基线实现超过20%的相对误差降低，同时通信成本较稠密联邦学习降低2倍。在实际联邦学习部署中，SSFL实现超过2.3倍的通信加速，凸显其实际效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the communication inefficiency in federated learning by proposing Salient Sparse Federated Learning (SSFL), which aims to discover a single, sparse subnetwork before training begins. The method computes parameter saliency scores on local, non-IID client data, aggregates them to form a global mask, and subsequently trains and communicates only the weights of this sparse subnetwork. Experimental results on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that SSFL improves the accuracy-sparsity trade-off, achieving over 20% relative error reduction on CIFAR-10 compared to strong baselines while halving communication costs, with a real-world deployment confirming more than 2.3x faster communication times.</div>
<div class="mono" style="margin-top:8px">本研究针对联邦学习中通信效率低下的问题，提出了一种在训练开始前识别并仅训练一个稀疏全局共享子网络的方法。该方法名为显著稀疏联邦学习（SSFL），它在非独立同分布（non-IID）的本地客户端数据上分别计算参数显著性分数，聚合这些分数以确定一个全局稀疏掩码，随后在训练中仅通信和更新这些稀疏权重。在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准基准测试中，SSFL持续改善了准确性与稀疏性的权衡，在CIFAR-10上相比最强的稀疏基线实现了超过20%的相对错误率降低，同时将通信成本相较于稠密联邦学习减少了一半；实际部署验证了其通信时间加速超过2.3倍。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Evasion Attacks on Computer Vision using SHAP Values</div>
<div class="meta-line">Authors: Frank Mollard, Marcus Becker, Florian Roehrbein</div>
<div class="meta-line">First: 2026-01-15T16:58:55+00:00 · Latest: 2026-01-15T16:58:55+00:00</div>
<div class="meta-line">Comments: 10th bwHPC Symposium - September 25th &amp; 26th, 2024</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10587v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10587v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of individual inputs to the output at the inference stage. A comparison is drawn between the SHAP attack and the well-known Fast Gradient Sign Method. We find evidence that SHAP attacks are more robust in generating misclassifications particularly in gradient hiding scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用SHAP值对计算机视觉进行对抗性规避攻击</div>
<div class="mono" style="margin-top:8px">本文提出了一种利用SHAP值对计算机视觉模型实施的白盒攻击。研究展示了对抗性规避攻击如何通过降低输出置信度或诱导误分类来损害深度学习模型的性能。此类攻击尤为隐蔽，因其对人眼不可察觉，既能欺骗算法感知，又能规避人类感知。所提出的攻击利用SHAP值在推理阶段量化单个输入对输出的重要性，并与著名的快速梯度符号方法进行了对比。研究发现，在梯度隐藏场景中，SHAP攻击在生成误分类方面表现出更强的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to understand and defend against adversarial attacks that can compromise deep learning models in computer vision by subtly altering inputs to cause misclassifications or reduce confidence, often without being perceptible to humans. The method introduces a white-box adversarial attack that leverages SHAP values to quantify the importance of individual input features during inference, enabling the generation of targeted perturbations; it is compared against the established Fast Gradient Sign Method (FGSM). Experimental findings indicate that the SHAP-based attack is more robust, particularly in scenarios involving gradient hiding, as it effectively generates misclassifications where traditional methods may falter.</div>
<div class="mono" style="margin-top:8px">本研究旨在揭示计算机视觉模型在对抗性规避攻击下的脆弱性，这类攻击可通过降低输出置信度或导致误分类来损害模型性能，同时因其对人类视觉的不可感知性而难以察觉。方法上提出了一种白盒攻击，利用SHAP值在推理阶段量化单个输入特征对输出的重要性，从而进行针对性扰动。实验结果表明，与快速梯度符号法相比，基于SHAP的攻击在梯度隐藏等场景中表现出更强的鲁棒性，能更有效地产生误分类。</div>
</details>
</div>
<div class="card">
<div class="title">Image Complexity-Aware Adaptive Retrieval for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Mikel Williams-Lekuona, Georgina Cosma</div>
<div class="meta-line">First: 2025-12-17T12:19:54+00:00 · Latest: 2026-01-15T16:58:39+00:00</div>
<div class="meta-line">Comments: Camera-ready version for ECIR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15372v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15372v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision transformers in vision-language models typically use the same amount of compute for every image, regardless of whether it is simple or complex. We propose ICAR (Image Complexity-Aware Retrieval), an adaptive computation approach that enables vision transformers to use less compute for simple images whilst processing complex images through their full network depth. The key challenge is maintaining cross-modal alignment: embeddings from different processing depths must remain compatible for text matching. ICAR solves this through dual-path training that produces compatible embeddings from both the early-exit and full-depth paths. This maintains compatibility between image representations and text embeddings in the same semantic space, whether an image exits early or processes fully. Unlike existing two-stage approaches that require expensive reranking, ICAR enables direct image-text matching without additional overhead. To determine how much compute to use, we develop ConvNeXt-IC, which treats image complexity assessment as a classification task. By applying modern classifier backbones rather than specialised architectures, ConvNeXt-IC achieves state-of-the-art performance, attaining a Pearson correlation coefficient of 0.959 with human labelling whilst delivering 4.4x faster complexity prediction. Evaluated on standard benchmarks augmented with real-world web data, ICAR achieves 20% faster image encoding while maintaining category-level performance and 95% of instance-level performance, enabling sustainable scaling of vision-language systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向高效视觉语言模型的图像复杂度感知自适应检索</div>
<div class="mono" style="margin-top:8px">视觉语言模型中的视觉Transformer通常对每张图像使用相同的计算量，无论其简单或复杂。我们提出ICAR（图像复杂度感知检索），这是一种自适应计算方法，使视觉Transformer能够对简单图像使用更少计算，同时对复杂图像执行完整的网络深度处理。关键挑战在于保持跨模态对齐：来自不同处理深度的嵌入必须保持文本匹配的兼容性。ICAR通过双路径训练解决该问题，该训练从早期退出路径和完整深度路径生成兼容的嵌入。这确保了无论图像是早期退出还是完整处理，其表征与文本嵌入都能在同一语义空间中保持兼容。与现有需要昂贵重排序的两阶段方法不同，ICAR无需额外开销即可实现直接图文匹配。为确定计算量需求，我们开发了ConvNeXt-IC，将图像复杂度评估视为分类任务。通过采用现代分类器主干而非专用架构，ConvNeXt-IC实现了最先进的性能，其复杂度预测速度提升4.4倍，与人工标注的皮尔逊相关系数达0.959。在融合真实网络数据的标准基准测试中，ICAR在保持类别级性能及95%实例级性能的同时，实现了20%的图像编码加速，为视觉语言系统的可持续扩展提供了可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of vision transformers in vision-language models applying uniform computational cost to all images, regardless of complexity. The proposed method, ICAR, introduces an adaptive computation framework where a vision transformer uses less compute for simple images via early exits and full depth for complex ones, maintaining cross-modal alignment through a dual-path training strategy that ensures compatible embeddings from both paths. Key experimental results show that ICAR achieves 20% faster image encoding while preserving category-level performance and 95% of instance-level retrieval accuracy on benchmarks augmented with web data, and its complexity predictor, ConvNeXt-IC, attains a 0.959 Pearson correlation with human labels and is 4.4x faster than prior methods.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉-语言模型中视觉Transformer对所有图像采用统一计算量而导致的效率低下问题。提出的ICAR方法是一种自适应计算框架，其中基于ConvNeXt的分类器（ConvNeXt-IC）首先评估图像复杂度，以路由简单图像提前退出、复杂图像进行全深度处理；双路径训练通过从两条路径生成兼容的图像嵌入来确保跨模态对齐，实现直接的文本匹配。关键实验结果表明，ConvNeXt-IC与人类复杂度标注的皮尔逊相关系数达到0.959，且速度比先前方法快4.4倍；同时，ICAR在包含网络数据的基准测试中实现了20%的图像编码加速，保持了类别级性能及95%的实例级检索精度。</div>
</details>
</div>
<div class="card">
<div class="title">FiCo-ITR: bridging fine-grained and coarse-grained image-text retrieval for comparative performance analysis</div>
<div class="meta-line">Authors: Mikel Williams-Lekuona, Georgina Cosma</div>
<div class="meta-line">First: 2024-07-29T15:44:22+00:00 · Latest: 2026-01-15T16:54:30+00:00</div>
<div class="meta-line">Comments: Published at the International Journal of Multimedia Information Retrieval</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.20114v2">Abs</a> · <a href="https://arxiv.org/pdf/2407.20114v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the field of Image-Text Retrieval (ITR), recent advancements have leveraged large-scale Vision-Language Pretraining (VLP) for Fine-Grained (FG) instance-level retrieval, achieving high accuracy at the cost of increased computational complexity. For Coarse-Grained (CG) category-level retrieval, prominent approaches employ Cross-Modal Hashing (CMH) to prioritise efficiency, albeit at the cost of retrieval performance. Due to differences in methodologies, FG and CG models are rarely compared directly within evaluations in the literature, resulting in a lack of empirical data quantifying the retrieval performance-efficiency tradeoffs between the two. This paper addresses this gap by introducing the \texttt{FiCo-ITR} library, which standardises evaluation methodologies for both FG and CG models, facilitating direct comparisons. We conduct empirical evaluations of representative models from both subfields, analysing precision, recall, and computational complexity across varying data scales. Our findings offer new insights into the performance-efficiency trade-offs between recent representative FG and CG models, highlighting their respective strengths and limitations. These findings provide the foundation necessary to make more informed decisions regarding model selection for specific retrieval tasks and highlight avenues for future research into hybrid systems that leverage the strengths of both FG and CG approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FiCo-ITR：桥接细粒度与粗粒度图文检索以进行性能对比分析</div>
<div class="mono" style="margin-top:8px">在图文检索领域，近期研究通过大规模视觉语言预训练技术实现细粒度实例级检索，虽获得高精度但计算复杂度显著增加。粗粒度类别级检索则主要采用跨模态哈希方法以优先保证效率，但检索性能有所牺牲。由于方法学差异，现有文献评估中鲜少直接对比细粒度与粗粒度模型，导致缺乏量化二者检索性能-效率权衡的实证数据。本文通过推出\texttt{FiCo-ITR}库填补该空白，该库标准化了细粒度与粗粒度模型的评估方法，支持直接对比研究。我们对两个子领域的代表性模型进行实证评估，分析不同数据规模下的精确率、召回率及计算复杂度。研究结果揭示了当前代表性细粒度与粗粒度模型在性能-效率权衡方面的新认知，明晰了各自优势与局限。这些发现为特定检索任务中模型选择的科学决策奠定基础，并为未来融合细粒度与粗粒度优势的混合系统研究指明方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the lack of direct performance-efficiency comparisons between fine-grained (FG) instance-level and coarse-grained (CG) category-level image-text retrieval models, as they typically employ different methodologies like Vision-Language Pretraining and Cross-Modal Hashing, respectively. To bridge this gap, the authors introduce the FiCo-ITR library, which standardizes evaluation protocols, enabling empirical analysis of representative models from both subfields in terms of precision, recall, and computational complexity across data scales. The experimental results quantify the trade-offs, revealing that FG models achieve higher accuracy with greater computational cost, while CG models prioritize efficiency at the expense of retrieval performance, thereby providing a foundation for informed model selection and future hybrid systems.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决细粒度与粗粒度图像-文本检索模型之间缺乏直接性能-效率对比的问题，因为这两类模型通常采用不同的方法（例如，大规模视觉-语言预训练与跨模态哈希）。作者引入了FiCo-ITR库来标准化评估流程，从而能够对两个子领域的代表性模型进行直接的实证分析。关键的实验结果通过分析不同数据规模下的精度、召回率和计算复杂度，揭示了具体的性能权衡，为模型选择提供了基础性见解，并指出了未来开发结合两者优势的混合系统的方向。</div>
</details>
</div>
<div class="card">
<div class="title">From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA</div>
<div class="meta-line">Authors: Kimia Abedini, Farzad Shami, Gianmaria Silvello</div>
<div class="meta-line">First: 2026-01-15T16:54:11+00:00 · Latest: 2026-01-15T16:54:11+00:00</div>
<div class="meta-line">Comments: Accepted paper by the 48th European Conference on Information Retrieval (ECIR&#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10581v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10581v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate GeneGPT and propose GenomAgent, a multi-agent framework that efficiently coordinates specialized agents for complex genomics queries. Evaluated on nine tasks from the GeneTuring benchmark, GenomAgent outperforms GeneGPT by 12% on average, and its flexible architecture extends beyond genomics to various scientific domains needing expert knowledge extraction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从单智能体到多智能体推理：推进GeneGPT在基因组学问答中的应用</div>
<div class="mono" style="margin-top:8px">理解基因组信息对生物医学研究至关重要，但从复杂分布式数据库中提取数据仍具挑战。大语言模型（LLMs）在基因组问答（QA）中展现出潜力，但因难以访问领域专用数据库而受限。GeneGPT是当前最先进的系统，通过调用专用API增强LLMs能力，但其受限于僵化的API依赖与有限适应性。我们复现了GeneGPT并提出GenomAgent——一个能高效协调专用智能体处理复杂基因组学查询的多智能体框架。在GeneTuring基准的九项任务评估中，GenomAgent平均性能超越GeneGPT达12%，其灵活架构可扩展至需要专业知识提取的多种科学领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of extracting genomic information from complex distributed databases, where existing large language model (LLM) approaches like GeneGPT are limited by rigid API dependencies and poor adaptability. The authors propose GenomAgent, a multi-agent framework that coordinates specialized agents to handle complex genomics queries more flexibly. Experimental evaluation on nine tasks from the GeneTuring benchmark shows that GenomAgent outperforms GeneGPT by an average of 12%, demonstrating its effectiveness and suggesting broader applicability to other scientific domains requiring expert knowledge extraction.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决生物医学研究中从复杂分布式数据库提取基因组数据的挑战，现有基于大语言模型的方法如GeneGPT受限于僵化的API依赖和适应性不足。提出的方法GenomAgent采用多智能体框架，通过协调多个专业智能体来更灵活地处理复杂的基因组学查询。在GeneTuring基准测试的九项任务上的实验评估表明，GenomAgent平均性能优于当前最优的GeneGPT系统12%，其灵活架构还显示出可扩展到其他需要专家知识提取的科学领域的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Generative AI collective behavior needs an interactionist paradigm</div>
<div class="meta-line">Authors: Laura Ferrarotti, Gian Maria Campedelli, Roberto Dessì, Andrea Baronchelli, Giovanni Iacca, Kathleen M. Carley, Alex Pentland, Joel Z. Leibo, James Evans, Bruno Lepri</div>
<div class="meta-line">First: 2026-01-15T16:29:23+00:00 · Latest: 2026-01-15T16:29:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10567v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10567v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interactionist paradigm consisting of alternative theoretical foundations, methodologies, and analytical tools, in order to systematically examine how prior knowledge and embedded values interact with social context to shape emergent phenomena in multi-agent generative AI systems. We propose and discuss four directions that we consider crucial for the development and deployment of LLM-based collectives, focusing on theory, methods, and trans-disciplinary dialogue.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式AI集体行为需要一种互动主义范式</div>
<div class="mono" style="margin-top:8px">本文主张，理解基于大语言模型（LLM）的智能体集体行为是一个关键研究领域，其风险与效益对社会各层面具有重要影响。我们认为，LLM的独特性——即通过大规模预训练知识和隐含社会先验进行初始化，并结合上下文学习适应能力——催生了对互动主义范式的需求。该范式需包含替代性理论基础、方法论和分析工具，以系统研究先验知识与嵌入价值观如何与社会情境相互作用，从而塑造多智能体生成式AI系统中的涌现现象。我们提出并讨论了四个对LLM集体系统开发与部署至关重要的方向，聚焦于理论、方法及跨学科对话。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to understand the collective behavior of large language model (LLM)-based agents, given their significant societal risks and benefits. The authors argue that the unique characteristics of LLMs—such as their pre-trained knowledge and in-context learning ability—require an interactionist paradigm with new theories and methods to systematically study how prior knowledge and values interact with social contexts to produce emergent phenomena in multi-agent systems. They propose four key directions for developing and deploying LLM-based collectives, emphasizing theoretical foundations, methodological approaches, and trans-disciplinary collaboration.</div>
<div class="mono" style="margin-top:8px">本文认为，理解基于大语言模型的智能体集体行为至关重要，因其具有重大的社会风险与效益。作者提出，大语言模型的独特性质——即其预训练知识储备和情境学习能力——要求建立一种新的互动主义范式，包含替代性的理论、方法和分析工具。该范式旨在系统研究先验知识和内嵌价值观如何与社会情境相互作用，从而塑造多智能体生成式AI系统中的涌现现象，并为此指出了理论、方法和跨学科对话四个关键发展方向。</div>
</details>
</div>
<div class="card">
<div class="title">Process-Guided Concept Bottleneck Model</div>
<div class="meta-line">Authors: Reza M. Asiyabi, SEOSAW Partnership, Steven Hancock, Casey Ryan</div>
<div class="meta-line">First: 2026-01-15T16:25:55+00:00 · Latest: 2026-01-15T16:25:55+00:00</div>
<div class="meta-line">Comments: 13 pages with 7 figures and 1 table, Supplementary Materials 10 pages with 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10562v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10562v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains learning to follow domain-defined causal mechanisms through biophysically meaningful intermediate concepts. Using above ground biomass density estimation from Earth Observation data as a case study, we show that PG-CBM reduces error and bias compared to multiple benchmarks, whilst leveraging multi-source heterogeneous training data and producing interpretable intermediate outputs. Beyond improved accuracy, PG-CBM enhances transparency, enables detection of spurious learning, and provides scientific insights, representing a step toward more trustworthy AI systems in scientific applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>过程引导的概念瓶颈模型</div>
<div class="mono" style="margin-top:8px">概念瓶颈模型通过引入中间语义概念，提升了黑盒深度学习的可解释性。然而，标准概念瓶颈模型常忽略领域特定关系与因果机制，且对完整概念标签的依赖限制了其在监督稀疏但过程定义明确的科学领域的应用。为此，我们提出过程引导的概念瓶颈模型，作为概念瓶颈模型的扩展，通过具有生物物理意义的中间概念约束学习遵循领域定义的因果机制。以地球观测数据的地上生物量密度估算为例，研究表明，相较于多个基准模型，过程引导的概念瓶颈模型在利用多源异构训练数据并生成可解释中间输出的同时，降低了误差与偏差。除提升精度外，该模型增强了透明度，支持虚假学习检测，并提供科学洞见，是迈向科学应用中更可信人工智能系统的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Concept Bottleneck Models (CBMs) enhance deep learning explainability via intermediate concepts but often ignore domain-specific causal relationships and require extensive concept supervision, limiting their use in scientific fields with sparse labels but well-defined processes. To overcome this, the authors propose the Process-Guided Concept Bottleneck Model (PG-CBM), which incorporates biophysically meaningful intermediate concepts and constrains learning to follow domain-defined causal mechanisms. In a case study estimating above-ground biomass density from Earth Observation data, PG-CBM reduces error and bias compared to benchmarks, effectively leverages multi-source heterogeneous training data, and yields interpretable intermediate outputs, thereby improving both accuracy and transparency for more trustworthy scientific AI.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升概念瓶颈模型（CBM）的可解释性和科学适用性，因为标准CBM常忽略领域特定的因果关系且需要完整的概念标注，限制了其在数据稀疏的科学领域的应用。提出的过程引导概念瓶颈模型（PG-CBM）通过引入符合预定义因果机制的生物物理意义中间概念来扩展CBM，从而约束学习过程。在以地球观测数据估算地上生物量密度的案例研究中，PG-CBM相比多个基准方法降低了误差和偏差，有效利用了多源异构训练数据，并生成了可解释的中间输出，在提高准确性的同时增强了透明度，推动了科学应用中更可信赖AI系统的发展。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
