<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 04:50</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0450</div>
    <div class="row"><div class="card">
<div class="title">When LLaVA Meets Objects: Token Composition for Vision-Language-Models</div>
<div class="meta-line">Authors: Soumya Jahagirdar, Walid Bousselham, Anna Kukleva, Hilde Kuehne</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当LLaVA遇见物体：视觉语言模型的令牌组合方法</div>
<div class="mono" style="margin-top:8px">当前自回归视觉语言模型通常依赖大量视觉令牌表示图像，导致推理时计算需求较高。为解决此问题，我们提出Mask-LLaVA框架，利用多层级视觉特征构建紧凑而信息丰富的视觉表示。具体而言，我们将基于掩码的物体表征与全局令牌及局部图像块令牌相结合。训练阶段使用全部令牌，而测试阶段模型可灵活减少基于掩码的物体令牌数量，实现无需重新训练即可动态调整推理时的令牌数量，且性能无明显下降。在标准基准测试中，该方法仅使用少量视觉令牌即取得与当前令牌高效方法相当的结果，性能可比肩原始LLaVA基线。分析表明，多层级特征组合能以较少令牌实现高效学习，并在测试时通过动态令牌选择保持优异性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To reduce the high computational cost of autoregressive vision-language models, which typically require many visual tokens, this work introduces Mask-LLaVA, a framework that creates compact visual representations by combining multi-level features: mask-based object tokens, global tokens, and local patch tokens. During training, all token types are used, but the model can flexibly drop a significant number of the mask-based object tokens at inference time, enabling dynamic token count adaptation without retraining. Experimental results on standard benchmarks show performance competitive with other token-efficient methods and comparable to the original LLaVA baseline while using only a fraction of the visual tokens, demonstrating that multi-level feature combination supports efficient learning and dynamic token selection.</div>
<div class="mono" style="margin-top:8px">为解决自回归视觉语言模型因依赖大量视觉令牌而导致计算成本高的问题，本研究提出了Mask-LLaVA框架，该框架通过整合基于掩码的对象令牌、全局令牌和局部图像块令牌等多层次特征，构建紧凑而信息丰富的视觉表示。训练时使用所有令牌类型，但模型在推理时可以灵活丢弃大部分基于掩码的对象令牌，从而无需重新训练即可动态调整令牌数量。在标准基准测试上的实验结果表明，该方法仅使用一小部分视觉令牌，其性能即可与其他令牌高效方法竞争，并与原始LLaVA基线相当，证明了其高效学习和动态令牌选择的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?</div>
<div class="meta-line">Authors: Qing&#x27;an Liu, Juntong Feng, Yuhao Wang, Xinzhe Han, Yujie Cheng, Yue Zhu, Haiwen Diao, Yunzhi Zhuge, Huchuan Lu</div>
<div class="meta-line">First: 2026-02-04T17:48:55+00:00 · Latest: 2026-02-04T17:48:55+00:00</div>
<div class="meta-line">Comments: 27 pages, 19 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04802v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04802v1">PDF</a> · <a href="https://github.com/QingAnLiu/VISTA-Bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VISTA-Bench：视觉语言模型真的能像理解纯文本一样理解可视化文本吗？</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在跨文本与视觉模态的理解任务中取得了显著性能，但现有基准主要关注纯文本查询。在实际场景中，语言也常以图像中嵌入的可视化文本形式出现，这引发了对当前VLMs能否同等处理此类输入请求的疑问。我们提出了VISTA-Bench——一个涵盖多模态感知、推理到单模态理解领域的系统性基准。该基准通过在受控渲染条件下对比纯文本与可视化文本问题，评估模型对可视化文本的理解能力。对20余个代表性VLMs的广泛评估揭示出显著的模态差距：在纯文本查询中表现良好的模型，当相同语义内容以可视化文本呈现时，性能往往大幅下降。这种差距会随着感知难度的增加而进一步扩大，表明模型对渲染变化具有敏感性，尽管语义内容未变。总体而言，VISTA-Bench提供了一个原则性评估框架，可用于诊断这一局限性，并指导模型在符号化文本与像素层面实现更统一的语言表征。源数据集发布于https://github.com/QingAnLiu/VISTA-Bench。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the gap in evaluating Vision-Language Models (VLMs) on their ability to understand visualized text embedded in images, as existing benchmarks focus primarily on pure-text queries. The authors introduce VISTA-Bench, a systematic benchmark that contrasts pure-text and visualized-text questions across multimodal perception, reasoning, and unimodal understanding domains under controlled rendering conditions. Experiments with over 20 VLMs reveal a significant modality gap, where models perform substantially worse on visualized text, with the performance degradation amplified by increased perceptual difficulty, highlighting a sensitivity to rendering variations despite unchanged semantic content.</div>
<div class="mono" style="margin-top:8px">当前的视觉语言模型在纯文本查询上表现出色，但对于图像中嵌入的可视化文本（一种常见的现实场景）的处理能力尚不明确。为此，研究者提出了VISTA-Bench这一系统性基准，通过在受控渲染条件下对比模型对纯文本与可视化文本问题的响应，评估其在多模态感知、推理和单模态理解领域的性能。对超过20个代表性模型的广泛测试揭示了一个显著的模态差距：模型在可视化文本上的表现大幅下降，且随着感知难度的增加，这一差距进一步扩大，表明尽管语义内容相同，模型对渲染变化仍非常敏感。</div>
</details>
</div>
<div class="card">
<div class="title">Annotation Free Spacecraft Detection and Segmentation using Vision Language Models</div>
<div class="meta-line">Authors: Samet Hicsonmez, Jose Sosa, Dan Pineau, Inder Pal Singh, Arunkumar Rathinam, Abd El Rahman Shabayek, Djamila Aouada</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-04T16:07:29+00:00 · Latest: 2026-02-04T16:07:29+00:00</div>
<div class="meta-line">Comments: ICRA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04699v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04699v1">PDF</a> · <a href="https://github.com/giddyyupp/annotation-free-spacecraft-segmentation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的免标注航天器检测与分割方法</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在开放世界零样本视觉识别任务中展现出卓越性能，但其在航天领域的应用潜力尚未得到充分探索。在航天场景中，由于低可见度、光照变化及目标与行星背景融合等因素，精确的人工标注尤为困难。因此，开发无需大量人工标注即可检测与分割航天器及轨道目标的方法至关重要。本研究提出一种基于VLM的免标注空间目标检测与分割流程：首先利用预训练VLM自动为少量未标注真实数据生成伪标签，随后通过师生标签蒸馏框架训练轻量化模型。尽管伪标签存在固有噪声，但蒸馏过程相比直接零样本VLM推理仍带来显著性能提升。在SPARK-2024、SPEED+和TANGO数据集上的分割实验表明，平均精度（AP）最高可提升10个百分点。代码与模型已开源：https://github.com/giddyyupp/annotation-free-spacecraft-segmentation。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of detecting and segmenting spacecraft in space imagery where manual annotation is difficult due to low visibility and background blending. The method employs a pre-trained Vision Language Model (VLM) to automatically generate pseudo-labels for a small unlabeled dataset, which are then used in a teacher-student distillation framework to train lightweight models. Experiments on SPARK-2024, SPEED+, and TANGO datasets show that this approach improves average precision by up to 10 points over direct zero-shot VLM inference, demonstrating effective annotation-free performance.</div>
<div class="mono" style="margin-top:8px">本研究针对太空图像中航天器检测与分割的标注难题，该难题源于低可见度、光照变化和背景融合等因素。方法提出了一种无需标注的流程：首先使用预训练的视觉语言模型为少量未标注数据生成伪标签，然后通过师生蒸馏框架训练轻量级模型。在SPARK-2024、SPEED+和TANGO数据集上的实验结果表明，该方法显著优于直接的零样本视觉语言模型推理，在分割任务的平均精度上实现了最高10个百分点的稳定提升。</div>
</details>
</div>
<div class="card">
<div class="title">AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation</div>
<div class="meta-line">Authors: Jin-Chuan Shi, Binhong Ye, Tao Liu, Junzhe He, Yangjinhui Xu, Xiaoyang Liu, Zeju Li, Hao Chen, Chunhua Shen</div>
<div class="meta-line">First: 2026-02-04T15:42:58+00:00 · Latest: 2026-02-04T15:42:58+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04672v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AGILE：基于智能体生成机制的视频手物交互重建</div>
<div class="mono" style="margin-top:8px">从单目视频重建动态手物交互对于灵巧操作数据采集以及为机器人与VR创建真实数字孪生体至关重要。然而现有方法面临两大瓶颈：(1)依赖神经渲染常导致严重遮挡下产生破碎、无法直接用于仿真的几何体；(2)依赖脆弱的运动恢复结构初始化机制，在真实场景视频中频繁失效。为突破这些限制，我们提出AGILE框架，将范式从重建转向面向交互学习的智能体生成。首先，采用智能体流程：通过视觉语言模型引导生成模型合成完整、密闭的高保真纹理物体网格，完全规避视频遮挡问题。其次，彻底绕过脆弱的SfM方案，提出鲁棒的锚点追踪策略：基于基础模型在单帧交互起始帧初始化物体位姿，并通过生成资产与视频观测间强视觉相似性实现时序传播。最后，融合语义、几何与交互稳定性约束的接触感知优化确保物理合理性。在HO3D、DexYCB及真实场景视频上的大量实验表明，AGILE在全局几何精度上超越基线方法，并在现有方法常失效的挑战性序列中展现卓越鲁棒性。通过优先保障物理有效性，本方法产出可直接用于仿真的资产，并已通过机器人应用的实-仿重定向验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitations of current methods for reconstructing dynamic hand-object interactions from monocular videos, which often produce fragmented geometries and fail on in-the-wild footage due to reliance on neural rendering and brittle SfM initialization. The proposed AGILE framework shifts the paradigm to agentic generation, employing a Vision-Language Model to guide the synthesis of a complete object mesh and a robust anchor-and-track strategy for pose estimation, followed by a contact-aware optimization for physical plausibility. Experimental results on HO3D, DexYCB, and in-the-wild videos show that AGILE outperforms baselines in geometric accuracy and demonstrates superior robustness on challenging sequences, producing simulation-ready assets validated for robotic applications.</div>
<div class="mono" style="margin-top:8px">本研究针对现有从单目视频重建动态手物交互方法存在的局限性，这些方法常因依赖神经渲染和脆弱的运动恢复结构初始化，产生破碎的几何体并在真实场景视频中频繁失败。提出的AGILE框架将范式转向智能体生成，采用视觉语言模型引导合成完整、密封的物体网格，并通过基于基础模型的鲁棒锚定跟踪策略绕过运动恢复结构，利用单帧初始化姿态并进行时序传播，最后通过接触感知优化确保物理合理性。在HO3D、DexYCB和真实场景视频上的大量实验表明，AGILE在几何精度上优于基线方法，并在挑战性序列上展现出卓越的鲁棒性，生成了经过机器人应用验证的、可用于仿真的资产。</div>
</details>
</div>
<div class="card">
<div class="title">PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective</div>
<div class="meta-line">Authors: Haokui Zhang, Congyang Ou, Dawei Yan, Peng Wang, Qingsen Yan, Ying Li, Rong Xiao, Chunhua Shen</div>
<div class="meta-line">First: 2026-02-04T15:33:10+00:00 · Latest: 2026-02-04T15:33:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04657v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04657v1">PDF</a> · <a href="https://github.com/ocy1/PIO-FVLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, reducing redundant visual tokens in vision-language models (VLMs) to accelerate VLM inference has emerged as a hot topic. However, most existing methods rely on heuristics constructed based on inter-visual-token similarity or cross-modal visual-text similarity, which gives rise to certain limitations in compression performance and practical deployment. In contrast, we propose PIO-FVLM from the perspective of inference objectives, which transforms visual token compression into preserving output result invariance and selects tokens primarily by their importance to this goal. Specially, vision tokens are reordered with the guidance of token-level gradient saliency generated by our designed layer-local proxy loss, a coarse constraint from the current layer to the final result. Then the most valuable vision tokens are selected following the non-maximum suppression (NMS) principle. The proposed PIO-FVLM is training-free and compatible with FlashAttention, friendly to practical application and deployment. It can be deployed independently as an encoder-free method, or combined with encoder compression approaches like VisionZip for use as an encoder-involved method. On LLaVA-Next-7B, PIO-FVLM retains just 11.1% of visual tokens but maintains 97.2% of the original performance, with a 2.67$\times$ prefill speedup, 2.11$\times$ inference speedup, 6.22$\times$ lower FLOPs, and 6.05$\times$ reduced KV Cache overhead. Our code is available at https://github.com/ocy1/PIO-FVLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PIO-FVLM：从推理目标视角重新审视用于VLM加速的无训练视觉令牌缩减</div>
<div class="mono" style="margin-top:8px">近期，通过减少视觉语言模型中的冗余视觉令牌以加速推理成为研究热点。然而，现有方法多基于视觉令牌间相似性或跨模态视觉-文本相似性构建启发式规则，在压缩性能与实际部署上存在局限。本文从推理目标出发，提出PIO-FVLM方法，将视觉令牌压缩转化为保持输出结果不变性问题，并依据令牌对该目标的重要性进行筛选。具体而言，通过设计的层局部代理损失（一种从当前层到最终结果的粗粒度约束）生成令牌级梯度显著性，以此指导视觉令牌重排序，再遵循非极大值抑制原则选取最具价值的视觉令牌。PIO-FVLM无需训练、兼容FlashAttention，便于实际应用部署：既可独立作为无编码器方法使用，也可与VisionZip等编码器压缩方法结合。在LLaVA-Next-7B上，仅保留11.1%视觉令牌即可维持97.2%原始性能，实现2.67倍预填充加速、2.11倍推理加速、6.22倍FLOPs降低及6.05倍KV缓存开销减少。代码已开源：https://github.com/ocy1/PIO-FVLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To accelerate vision-language model inference by reducing redundant visual tokens, this work identifies limitations in existing heuristic methods based on token similarity. It proposes PIO-FVLM, a training-free method that reorients token compression toward preserving final output invariance. The approach reorders tokens using gradient saliency from a layer-local proxy loss and selects the most valuable ones via non-maximum suppression, achieving compatibility with FlashAttention. Experimental results on LLaVA-Next-7B show that retaining only 11.1% of visual tokens maintains 97.2% of original performance, while delivering significant speedups (2.67× prefill, 2.11× inference), reduced FLOPs (6.22×), and lower KV Cache overhead (6.05×).</div>
<div class="mono" style="margin-top:8px">为通过减少冗余视觉标记来加速视觉语言模型推理，本研究指出了现有基于标记相似性启发式方法的局限性。提出了PIO-FVLM，这是一种无需训练的方法，将标记压缩重新定向为保持最终输出不变性。该方法通过层局部代理损失生成的梯度显著性对标记重新排序，并利用非极大值抑制选择最有价值的标记，实现了与FlashAttention的兼容。在LLaVA-Next-7B上的实验结果表明，仅保留11.1%的视觉标记即可维持97.2%的原始性能，同时实现了显著的加速和计算开销降低。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Degradation with Vision Language Model</div>
<div class="meta-line">Authors: Guanzhou Lan, Chenyi Liao, Yuqi Yang, Qianli Ma, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li</div>
<div class="meta-line">First: 2026-02-04T13:51:15+00:00 · Latest: 2026-02-04T13:51:15+00:00</div>
<div class="meta-line">Comments: 17 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04565v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04565v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于视觉语言模型的退化理解研究</div>
<div class="mono" style="margin-top:8px">理解视觉退化是计算机视觉中关键而具挑战性的问题。尽管当前视觉语言模型在定性描述方面表现优异，却常难以理解图像退化背后的参数化物理机制。本研究将退化理解重新定义为层次化结构化预测任务，要求同时估计退化类型、参数键及其连续物理值。虽然这些子任务处于不同空间，我们证明其可通过自回归下一词元预测范式统一实现，其误差受值空间量化网格约束。基于此，我们提出DU-VLM——一种采用监督微调与结构化奖励强化学习训练的多模态思维链模型。进一步研究表明，DU-VLM可作为预训练扩散模型的零样本控制器，无需微调生成主干即可实现高保真图像复原。同时，我们构建了包含11万组洁净-退化配对数据及物理标注的大规模数据集\textbf{DU-110k}。大量实验表明，该方法在精度与鲁棒性上显著超越通用基线模型，并对未见分布具有泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of enabling vision-language models to understand the parametric physics of image degradations, moving beyond qualitative descriptions. The authors propose a hierarchical structured prediction task that unifies degradation type classification, parameter key identification, and continuous value estimation under a single autoregressive next-token prediction framework, with bounded error. The method, DU-VLM, is trained using supervised fine-tuning and reinforcement learning with structured rewards, and is shown to function as a zero-shot controller for pre-trained diffusion models to perform image restoration. Experimental results on the newly introduced DU-110k dataset demonstrate that the approach significantly outperforms generalist baselines in accuracy and robustness, with strong generalization to unseen data distributions.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉语言模型理解图像退化参数物理特性的挑战，超越定性描述。作者提出了一种分层结构化预测任务，将退化类型、参数键和连续物理值的估计统一在自回归的下一个令牌预测范式下，并具有有界误差。他们引入了DU-VLM模型，该模型通过监督微调和基于结构化奖励的强化学习进行训练，并证明其可作为预训练扩散模型的零样本控制器，实现高保真图像恢复。在新引入的DU-110k数据集上的大量实验表明，该方法在准确性和鲁棒性上显著优于通用基线，并对未见分布展现出良好的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">EgoActor: Grounding Task Planning into Spatial-aware Egocentric Actions for Humanoid Robots via Visual-Language Models</div>
<div class="meta-line">Authors: Yu Bai, MingMing Yu, Chaojie Li, Ziyi Bai, Xinlong Wang, Börje F. Karlsson</div>
<div class="meta-line">First: 2026-02-04T13:04:56+00:00 · Latest: 2026-02-04T13:04:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04515v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04515v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying humanoid robots in real-world settings is fundamentally challenging, as it demands tight integration of perception, locomotion, and manipulation under partial-information observations and dynamically changing environments. As well as transitioning robustly between sub-tasks of different types. Towards addressing these challenges, we propose a novel task - EgoActing, which requires directly grounding high-level instructions into various, precise, spatially aware humanoid actions. We further instantiate this task by introducing EgoActor, a unified and scalable vision-language model (VLM) that can predict locomotion primitives (e.g., walk, turn, move sideways, change height), head movements, manipulation commands, and human-robot interactions to coordinate perception and execution in real-time. We leverage broad supervision over egocentric RGB-only data from real-world demonstrations, spatial reasoning question-answering, and simulated environment demonstrations, enabling EgoActor to make robust, context-aware decisions and perform fluent action inference (under 1s) with both 8B and 4B parameter models. Extensive evaluations in both simulated and real-world environments demonstrate that EgoActor effectively bridges abstract task planning and concrete motor execution, while generalizing across diverse tasks and unseen environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoActor：基于视觉语言模型将人形机器人任务规划落地为空间感知的自我中心动作</div>
<div class="mono" style="margin-top:8px">在现实场景中部署人形机器人面临根本性挑战，因其需要在部分信息观测和动态变化环境下，紧密整合感知、移动与操作能力，并实现不同类型子任务间的稳健切换。为应对这些挑战，我们提出一项新任务——EgoActing，其要求将高层指令直接映射为多样化、精确且具有空间感知的人形机器人动作。我们进一步通过引入EgoActor实例化该任务：这是一个统一且可扩展的视觉语言模型，能够预测移动基元（如行走、转向、侧移、高度调整）、头部运动、操作指令及人机交互行为，以实时协调感知与执行。我们利用来自真实世界演示的纯RGB自我中心数据、空间推理问答及仿真环境演示进行广泛监督，使EgoActor能以8B和4B参数模型做出稳健的上下文感知决策，并在1秒内完成流畅动作推理。在仿真与真实环境中的大量实验表明，EgoActor能有效桥接抽象任务规划与具体运动执行，并泛化至多样化任务及未见环境。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deploying humanoid robots in real world requires robust integration of perception, locomotion, and manipulation under partial observations and dynamic conditions. To address this, the authors propose the EgoActing task and introduce EgoActor, a unified vision-language model that grounds high-level instructions into spatially-aware actions like locomotion primitives, head movements, and manipulation commands. The model is trained on egocentric RGB data, spatial reasoning QA, and simulated demonstrations, enabling real-time inference under 1 second. Experiments in simulation and real-world settings show that EgoActor effectively bridges task planning with motor execution and generalizes to diverse, unseen tasks and environments.</div>
<div class="mono" style="margin-top:8px">在现实世界中部署人形机器人面临根本性挑战，因其需要在部分信息观测和动态变化环境下紧密整合感知、移动和操作能力，并稳健地在不同类型子任务间切换。为此，研究提出了EgoActing新任务，旨在将高层指令直接转化为多样、精确且具有空间意识的人形机器人动作，并实例化了EgoActor模型——一个统一且可扩展的视觉语言模型，能够预测移动基元、头部运动、操作命令和人机交互，以实时协调感知与执行。该模型利用来自真实世界演示的以自我为中心RGB数据、空间推理问答以及模拟环境演示的广泛监督进行训练，使得EgoActor能够做出鲁棒的上下文感知决策，并在8B和4B参数模型上实现流畅的动作推断。在模拟和真实环境中的广泛评估表明，EgoActor有效衔接了抽象任务规划与具体运动执行，并能泛化至多样任务和未见环境。</div>
</details>
</div>
<div class="card">
<div class="title">OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models</div>
<div class="meta-line">Authors: Yufeng Zhong, Lei Chen, Xuanle Zhao, Wenkang Han, Liming Zheng, Jing Huang, Deyang Jiang, Yilin Cao, Lin Ma, Zhixiong Zeng</div>
<div class="meta-line">First: 2026-01-29T12:43:02+00:00 · Latest: 2026-02-04T12:53:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21639v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21639v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of large vision language models drives the demand for managing, and applying massive amounts of multimodal data, making OCR technology, which extracts information from visual images, increasingly popular. However, existing OCR methods primarily focus on recognizing text elements from images or scanned documents (Text-centric OCR), neglecting the identification of visual elements from visually information-dense image sources (Vision-centric OCR), such as charts, web pages and science plots. In reality, these visually information-dense images are widespread on the internet and have significant real-world application value, such as data visualization and web page analysis. In this technical report, we propose OCRVerse, the first holistic OCR method in end-to-end manner that enables unified text-centric OCR and vision-centric OCR. To this end, we constructe comprehensive data engineering to cover a wide range of text-centric documents, such as newspapers, magazines and books, as well as vision-centric rendered composites, including charts, web pages and scientific plots. Moreover, we propose a two-stage SFT-RL multi-domain training method for OCRVerse. SFT directly mixes cross-domain data to train and establish initial domain knowledge, while RL focuses on designing personalized reward strategies for the characteristics of each domain. Specifically, since different domains require various output formats and expected outputs, we provide sufficient flexibility in the RL stage to customize flexible reward signals for each domain, thereby improving cross-domain fusion and avoiding data conflicts. Experimental results demonstrate the effectiveness of OCRVerse, achieving competitive results across text-centric and vision-centric data types, even comparable to large-scale open-source and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OCRVerse：迈向端到端视觉语言模型中的全息OCR</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型的发展推动了对海量多模态数据管理及应用的需求，使得从视觉图像中提取信息的OCR技术日益普及。然而，现有OCR方法主要聚焦于从图像或扫描文档中识别文本元素（以文本为中心的OCR），而忽视了从视觉信息密集的图像源（如图表、网页和科学图表）中识别视觉元素（以视觉为中心的OCR）。实际上，这些视觉信息密集的图像在互联网上广泛存在，具有重要的现实应用价值，如数据可视化和网页分析。在本技术报告中，我们提出了OCRVerse，这是首个以端到端方式实现统一文本中心OCR与视觉中心OCR的全息OCR方法。为此，我们构建了全面的数据工程，涵盖广泛的文本中心文档（如报纸、杂志和书籍）以及视觉中心渲染复合体（包括图表、网页和科学图表）。此外，我们为OCRVerse提出了一种两阶段的SFT-RL多领域训练方法：SFT直接混合跨领域数据进行训练以建立初始领域知识，而RL则针对各领域特性设计个性化奖励策略。具体而言，由于不同领域需要多样化的输出格式和预期结果，我们在RL阶段提供了充分的灵活性，为每个领域定制灵活的奖励信号，从而提升跨领域融合能力并避免数据冲突。实验结果表明，OCRVerse在文本中心和视觉中心数据类型上均取得了具有竞争力的效果，甚至可与大规模开源及闭源模型相媲美。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for OCR technology that goes beyond traditional text recognition to also interpret visually information-dense images like charts and web pages, which are prevalent and valuable for applications such as data visualization. The method introduces OCRVerse, an end-to-end holistic OCR model that unifies text-centric and vision-centric OCR through comprehensive data engineering covering diverse document types and a two-stage SFT-RL multi-domain training approach, where supervised fine-tuning establishes initial knowledge and reinforcement learning employs personalized reward strategies per domain to handle varied output formats and avoid data conflicts. Experimental results show that OCRVerse achieves competitive performance across both text-centric and vision-centric data types, performing comparably to large-scale open-source and closed-source models.</div>
<div class="mono" style="margin-top:8px">本研究的动机在于，现有OCR方法主要关注从图像或扫描文档中识别文本元素，而忽略了从图表、网页等视觉信息密集的图像源中识别视觉元素，这些图像在互联网上广泛存在且具有重要的实际应用价值。为此，我们提出了OCRVerse，这是一种端到端的整体OCR方法，通过构建涵盖报纸、杂志、书籍等文本中心文档以及图表、网页、科学图表等视觉中心渲染复合材料的全面数据工程，并采用两阶段SFT-RL多领域训练方法：SFT直接混合跨领域数据以建立初始领域知识，而RL则针对每个领域的特点设计个性化奖励策略，以定制灵活的奖励信号，从而改善跨领域融合并避免数据冲突。实验结果表明，OCRVerse在文本中心和视觉中心数据类型上均取得了有竞争力的结果，甚至可与大规模开源和闭源模型相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">Less Precise Can Be More Reliable: A Systematic Evaluation of Quantization&#x27;s Impact on CLIP Beyond Accuracy</div>
<div class="meta-line">Authors: Aymen Bouguerra, Daniel Montoya, Alexandra Gomez-Villa, Chokri Mraidha, Fabio Arnez</div>
<div class="meta-line">First: 2025-09-25T13:54:34+00:00 · Latest: 2026-02-04T09:44:34+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21173v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.21173v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) such as CLIP have revolutionized zero-shot classification and safety-critical tasks, including Out-of-Distribution (OOD) detection. However, their high computational cost hinders efficient real-world deployment. While quantization is a standard solution for efficiency, its broader impact on reliability metrics beyond simple Top-1 accuracy remains critically under-explored. In this study, we conduct a large-scale evaluation of VLM quantization across a comprehensive experimental suite of over 700k evaluation runs with varying configurations. We find that, contrary to the assumption that quantization&#x27;s noise degrades performance, it can simultaneously improve accuracy, calibration, OOD detection, and robustness to noise, though not to covariate shift or spurious correlations. We leverage these counterintuitive findings to characterize the mechanics of quantization beyond simple regularization: we show that quantization dampens high-rank spectral components, compelling the model to rely more heavily on robust, low-rank features. Ultimately, this spectral filtering effect drives the observed improvements in generalization and noise tolerance, establishing a pathway to deploy faster, more reliable VLMs by utilizing quantization beyond its conventional role.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>降低精度反而提升可靠性：量化对CLIP模型影响的系统性评估——超越准确率的视角</div>
<div class="mono" style="margin-top:8px">以CLIP为代表的视觉-语言模型（VLM）彻底改变了零样本分类和安全关键任务（包括分布外检测）的实现方式。然而，其高昂的计算成本阻碍了实际场景的高效部署。量化虽是提升效率的常规方案，但其对可靠性指标（超越简单的Top-1准确率）的广泛影响仍缺乏深入研究。本研究通过超过70万次不同配置的实验，对VLM量化进行了大规模评估。我们发现：与量化噪声会降低性能的假设相反，量化能同时提升准确率、校准能力、分布外检测和噪声鲁棒性（但对协变量偏移和伪相关性无效）。基于这些反直觉的发现，我们揭示了超越简单正则化的量化机制：量化会抑制高秩谱成分，迫使模型更依赖鲁棒的低秩特征。这种谱滤波效应最终驱动了泛化能力和噪声容忍度的提升，为通过突破量化的传统角色来部署更快速、更可靠的VLM开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study systematically evaluates the impact of quantization on CLIP&#x27;s reliability beyond just accuracy, motivated by the need to reduce computational costs for real-world deployment while understanding its broader effects. Through a large-scale experimental suite involving over 700,000 runs, the authors find that quantization can paradoxically improve accuracy, calibration, out-of-distribution detection, and robustness to noise, though not to covariate shift or spurious correlations. They attribute these gains to a spectral filtering mechanism, where quantization dampens high-rank components, forcing the model to rely more on robust, low-rank features, thereby enhancing generalization and noise tolerance.</div>
<div class="mono" style="margin-top:8px">本研究系统评估了量化对CLIP模型可靠性的影响，其动机在于降低实际部署的计算成本，并理解量化在精度之外的广泛效应。通过超过70万次实验的大规模评估，研究发现量化可以同时提升准确性、校准性、分布外检测和噪声鲁棒性，但对协变量偏移和虚假相关性无效。研究表明，量化通过抑制高秩谱分量，迫使模型更多地依赖鲁棒的低秩特征，从而起到谱滤波作用，这解释了在泛化性和噪声容忍度方面观察到的改进。</div>
</details>
</div>
<div class="card">
<div class="title">When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models</div>
<div class="meta-line">Authors: Jaehyun Kwak, Nam Cao, Boryeong Cho, Segyu Lee, Sumyeong Ahn, Se-Young Yun</div>
<div class="meta-line">First: 2026-02-04T09:29:10+00:00 · Latest: 2026-02-04T09:29:10+00:00</div>
<div class="meta-line">Comments: Pre-print</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04356v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04356v1">PDF</a> · <a href="https://github.com/jackwaky/SAGA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时何地发起攻击？面向大型视觉语言模型的阶段式注意力引导对抗攻击</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型（LVLMs）的对抗攻击对于揭示现代多模态系统的安全漏洞至关重要。基于输入变换（如随机裁剪）的最新研究表明，空间局部化扰动可能比全局图像操作更有效。然而，对整个图像进行随机裁剪具有内在随机性，且无法高效利用有限的像素级扰动预算。我们提出两个关键发现：（1）区域注意力分数与对抗损失敏感度呈正相关；（2）攻击高注意力区域会引发注意力向后续显著区域的结构化重分配。基于这些发现，我们提出阶段式注意力引导攻击（SAGA），这是一种逐步将扰动集中于高注意力区域的注意力引导框架。SAGA能更高效地利用受限扰动预算，在十种LVLM上持续实现最先进的攻击成功率的同时，生成高度不易察觉的对抗样本。源代码发布于 https://github.com/jackwaky/SAGA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for efficient adversarial attacks on Large Vision-Language Models (LVLMs) to expose safety vulnerabilities. The proposed Stage-wise Attention-Guided Attack (SAGA) method progressively concentrates perturbations on high-attention image regions, guided by the observations that these regions correlate with adversarial loss sensitivity and that attacking them redistributes attention to other salient areas. Experimental results demonstrate that SAGA achieves state-of-the-art attack success rates across ten LVLMs while producing highly imperceptible adversarial examples through more efficient use of constrained perturbation budgets.</div>
<div class="mono" style="margin-top:8px">本研究针对现有大型视觉语言模型对抗攻击方法的低效性问题，指出随机裁剪等随机变换会浪费有限的扰动预算。基于区域注意力分数与对抗损失敏感性正相关、以及攻击高注意力区域会引导注意力重新分配的关键观察，提出了分阶段注意力引导攻击框架。该方法通过逐步将对抗扰动集中在高注意力区域，在十个不同的大型视觉语言模型上实现了最先进的攻击成功率，同时生成了高度不易察觉的对抗样本，显著提升了有限扰动预算的使用效率。</div>
</details>
</div>
<div class="card">
<div class="title">Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning</div>
<div class="meta-line">Authors: Qian-Wei Wang, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2026-02-04T09:01:55+00:00 · Latest: 2026-02-04T09:01:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04340v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04340v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于双提示调优的主动CLIP自适应显式不确定性建模</div>
<div class="mono" style="margin-top:8px">预训练的视觉-语言模型（如CLIP）展现出强大的可迁移性，但在有限标注预算下将其适配至下游图像分类任务仍具挑战。在主动学习场景中，模型需从大量未标注数据中选择信息量最大的样本进行标注。现有方法通常通过基于熵的准则或表征聚类来估计不确定性，而未从模型视角显式建模不确定性。本研究提出一种基于双提示调优的鲁棒不确定性建模框架，用于主动CLIP自适应。我们在CLIP的文本分支中引入两个可学习提示：正提示通过增强与轻量调优视觉嵌入对应的任务特定文本嵌入的判别性，提升分类可靠性；负提示则以反向训练方式显式建模预测标签正确的概率，为主动样本选择提供理论依据的不确定性信号。跨不同微调范式的广泛实验表明，在相同标注预算下，本方法持续优于现有主动学习方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of adapting pre-trained vision-language models like CLIP to downstream image classification tasks with limited annotation budgets in active learning, this work proposes an explicit uncertainty modeling framework based on dual-prompt tuning. The method introduces two learnable prompts in CLIP&#x27;s textual branch: a positive prompt enhances discriminability for reliable classification, while a negative prompt is trained in a reversed manner to explicitly model the correctness probability of predictions, providing a principled uncertainty signal for active sample selection. Extensive experiments across different fine-tuning paradigms show that this approach consistently outperforms existing active learning methods under the same annotation budget.</div>
<div class="mono" style="margin-top:8px">本研究针对在主动学习场景下，如何以有限标注预算将预训练的CLIP模型适配到下游图像分类任务中的挑战。该方法提出了一个双提示调优框架，在CLIP的文本分支中引入两个可学习的提示：正提示通过增强任务特定文本嵌入的判别性来提高分类可靠性，而负提示则以反向方式训练，显式建模预测正确的概率，从而为主动样本选择提供原则性的不确定性信号。在不同微调范式下的广泛实验表明，该方法在相同标注预算下持续优于现有的主动学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner</div>
<div class="meta-line">Authors: Qian-Wei Wang, Guanghao Meng, Ren Cai, Yaguang Song, Shu-Tao Xia</div>
<div class="meta-line">First: 2026-02-04T09:00:12+00:00 · Latest: 2026-02-04T09:00:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04337v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04337v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需人工标注的预训练视觉语言模型微调方法</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型（如CLIP）展现出强大的零样本泛化能力，但适应下游任务通常需要昂贵的标注数据。现有无监督自训练方法依赖伪标签，但常面临置信度过滤不可靠、确认偏误及低置信度样本利用不足的问题。我们提出协作微调（CoFT），一种通过双模型跨模态协作机制利用未标注数据的无监督适应框架。CoFT采用包含正负文本提示的双提示学习策略，以样本依赖方式显式建模伪标签纯净度，无需人工设定阈值或噪声假设。负提示同时正则化轻量视觉适应模块，提升噪声监督下的鲁棒性。CoFT采用两阶段训练方案：从高置信度样本的参数高效微调过渡至协作过滤伪标签指导的完整微调。基于CoFT的CoFT+框架通过迭代微调、动量对比学习与LLM生成提示进一步强化适应能力。大量实验表明，该方法在无监督方法乃至少样本监督基线上均取得持续提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of adapting large-scale vision-language models to downstream tasks without relying on costly human annotations, as existing unsupervised methods often suffer from unreliable pseudo-labeling and underutilization of low-confidence samples. The proposed method, Collaborative Fine-Tuning (CoFT), introduces a dual-model, cross-modal collaboration framework that employs positive and negative textual prompts to model pseudo-label cleanliness in a sample-dependent manner, eliminating the need for hand-crafted thresholds, and incorporates a two-phase training scheme from parameter-efficient to full fine-tuning. Experimental results show that CoFT, and its enhanced version CoFT+, consistently outperform existing unsupervised methods and even few-shot supervised baselines across various tasks.</div>
<div class="mono" style="margin-top:8px">为避免使用昂贵的人工标注数据来适配视觉-语言模型，本研究提出了协作微调（CoFT），这是一个利用双模型跨模态协作机制的无监督框架。该方法采用包含正负文本提示的双提示学习策略，以样本依赖的方式建模伪标签的洁净度，避免了固定阈值，并采用从参数高效到完全微调的两阶段训练方案。实验表明，CoFT及其增强版本CoFT+的性能优于现有的无监督方法，甚至超过了少量样本的监督基线。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement</div>
<div class="meta-line">Authors: Zipeng Zhu, Zhanghao Hu, Qinglin Zhu, Yuxi Hong, Yijun Liu, Jingyong Su, Yulan He, Lin Gui</div>
<div class="meta-line">First: 2026-02-04T08:13:01+00:00 · Latest: 2026-02-04T08:13:01+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04304v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04304v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static &quot;magic layer&quot; empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越静态裁剪：层级自适应视觉定位与解码增强</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）通过将视觉图块与文本嵌入空间对齐而快速发展，但固定的视觉标记预算迫使图像必须调整至统一的预训练分辨率，这通常会抹除细粒度细节，并因过度依赖语言先验而导致幻觉。近期基于注意力引导的增强方法（如裁剪或区域聚焦注意力分配）缓解了这一问题，但这类方法通常依赖于在简单识别基准上凭经验选择的静态“魔法层”，因而可能无法迁移至复杂推理任务。与此静态假设相反，我们提出了视觉定位的动态视角。通过层级敏感性分析，我们证明视觉定位是一个动态过程：简单物体识别任务依赖中间层，而复杂的视觉搜索与推理任务则需要视觉信息在更深层被重新激活。基于这一发现，我们提出了基于查询的视觉激活（VAQ）指标，该指标通过测量注意力对输入查询的敏感性，识别出与查询特定视觉定位最相关的注意力映射层。在此基础上，我们进一步提出了LASER（面向推理的层级自适应注意力引导选择性视觉与解码增强），这是一种无需训练的自适应推理流程，能够为视觉定位和问答任务动态选择适合的层级。在多样化视觉问答基准上的实验表明，LASER能显著提升不同复杂度任务的视觉问答准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of large vision-language models (LVLMs) that use a fixed visual-token budget, which forces image resizing and erases fine details, leading to hallucinations. The authors propose a dynamic perspective on visual grounding, introducing Visual Activation by Query (VAQ) to identify the most relevant attention layer for query-specific grounding based on a layer-wise sensitivity analysis. They then develop LASER, a training-free inference method that adaptively selects layers for visual localization and decoding. Experiments on various VQA benchmarks demonstrate that LASER significantly improves accuracy across tasks of varying complexity.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型中固定视觉标记预算会抹去细节并导致幻觉的问题，本研究挑战了使用单一静态层进行注意力引导增强的方法。作者提出动态视角，通过分层敏感性分析证明视觉定位是任务依赖的：简单识别依赖中间层，而复杂推理需要在更深层重新激活视觉信息。他们引入了基于查询的视觉激活（VAQ）来识别与查询特定定位最相关的层，并开发了LASER，一种无需训练的自适应推理方法，能根据任务选择层进行视觉定位和问答。在多样化的视觉问答基准测试中，LASER显著提高了不同复杂度任务的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">MultiPriv: Benchmarking Individual-Level Privacy Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Xiongtao Sun, Hui Li, Jiaming Zhang, Yujie Yang, Kaili Liu, Ruxin Feng, Wen Jun Tan, Wei Yang Bryan Lim</div>
<div class="meta-line">First: 2025-11-21T04:33:11+00:00 · Latest: 2026-02-04T07:29:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16940v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16940v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) pose significant individual-level privacy risks by linking fragmented multimodal data to identifiable individuals through hierarchical chain-of-thought reasoning. However, existing privacy benchmarks remain structurally insufficient for this threat, as they primarily evaluate privacy perception while failing to address the more critical risk of privacy reasoning: a VLM&#x27;s ability to infer and link distributed information to construct individual profiles. To address this gap, we propose MultiPriv, the first benchmark designed to systematically evaluate individual-level privacy reasoning in VLMs. We introduce the Privacy Perception and Reasoning (PPR) framework and construct a bilingual multimodal dataset with synthetic individual profiles, where identifiers (e.g., faces, names) are linked to sensitive attributes. This design enables nine challenging tasks spanning attribute detection, cross-image re-identification, and chained inference. We conduct a large-scale evaluation of over 50 open-source and commercial VLMs. Our analysis shows that 60 percent of widely used VLMs can perform individual-level privacy reasoning with up to 80 percent accuracy, posing a significant threat to personal privacy. MultiPriv provides a foundation for developing and assessing privacy-preserving VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MultiPriv：视觉语言模型中个体层面隐私推理的基准测试</div>
<div class="mono" style="margin-top:8px">现代视觉语言模型通过分层思维链推理将碎片化的多模态数据与可识别个体关联，构成显著的个体层面隐私风险。然而，现有隐私基准在结构上不足以应对此威胁，因其主要评估隐私感知，却未能解决更关键的隐私推理风险：即VLM推断并关联分散信息以构建个体画像的能力。为填补这一空白，我们提出首个系统性评估VLM个体层面隐私推理的基准MultiPriv。我们引入隐私感知与推理框架，构建了包含合成个体画像的双语多模态数据集，其中标识符（如人脸、姓名）与敏感属性相关联。该设计涵盖属性检测、跨图像重识别及链式推理等九项挑战性任务。我们对超过50个开源及商业VLM进行大规模评估，分析表明60%的常用VLM能以高达80%的准确率执行个体层面隐私推理，对个人隐私构成严重威胁。MultiPriv为开发与评估隐私保护型VLM奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern Vision-Language Models (VLMs) can infer private information about individuals by linking fragmented multimodal data, but existing benchmarks only assess basic privacy perception, not the more critical hierarchical reasoning that constructs individual profiles. To address this gap, the authors introduce MultiPriv, the first benchmark for evaluating individual-level privacy reasoning, built on a Privacy Perception and Reasoning (PPR) framework and a bilingual multimodal dataset with synthetic profiles linking identifiers to sensitive attributes. This enables nine challenging tasks including attribute detection, cross-image re-identification, and chained inference. Evaluating over 50 VLMs revealed that 60% of widely used models can perform such reasoning with up to 80% accuracy, demonstrating a significant privacy threat and establishing a foundation for developing privacy-preserving VLMs.</div>
<div class="mono" style="margin-top:8px">现代视觉语言模型（VLMs）能够通过关联碎片化数据推断敏感个人信息，但现有基准仅评估基本的隐私感知，而非更危险的推理能力。为此，研究者提出了MultiPriv基准，它基于隐私感知与推理框架和一个包含合成个人档案的双语数据集构建，用于评估模型在属性检测、链式推理等九项任务上的表现。对超过50个模型的评估表明，60%的常用VLM能以高达80%的准确率进行此类隐私推理，这凸显了重大的隐私风险。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey on Vision-Language-Action Models for Embodied AI</div>
<div class="meta-line">Authors: Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King</div>
<div class="meta-line">First: 2024-05-23T01:43:54+00:00 · Latest: 2026-02-04T06:41:11+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/yueen-ma/Awesome-VLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.14093v7">Abs</a> · <a href="https://arxiv.org/pdf/2405.14093v7">PDF</a> · <a href="https://github.com/yueen-ma/Awesome-VLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied AI is widely recognized as a cornerstone of artificial general intelligence because it involves controlling embodied agents to perform tasks in the physical world. Building on the success of large language models and vision-language models, a new category of multimodal models -- referred to as vision-language-action models (VLAs) -- has emerged to address language-conditioned robotic tasks in embodied AI by leveraging their distinct ability to generate actions. The recent proliferation of VLAs necessitates a comprehensive survey to capture the rapidly evolving landscape. To this end, we present the first survey on VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized into three major lines of research. The first line focuses on individual components of VLAs. The second line is dedicated to developing VLA-based control policies adept at predicting low-level actions. The third line comprises high-level task planners capable of decomposing long-horizon tasks into a sequence of subtasks, thereby guiding VLAs to follow more general user instructions. Furthermore, we provide an extensive summary of relevant resources, including datasets, simulators, and benchmarks. Finally, we discuss the challenges facing VLAs and outline promising future directions in embodied AI. A curated repository associated with this survey is available at: https://github.com/yueen-ma/Awesome-VLA.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具身人工智能中视觉-语言-动作模型综述</div>
<div class="mono" style="margin-top:8px">具身人工智能被广泛认为是通用人工智能的基石，其核心在于控制具身代理在物理世界中执行任务。依托大语言模型和视觉-语言模型的成功，一类新型多模态模型——视觉-语言-动作模型应运而生，凭借其生成动作的独特能力，致力于解决具身人工智能中语言条件化的机器人任务。随着VLA模型的快速发展，亟需系统性综述以梳理该领域动态。为此，我们首次提出面向具身人工智能的VLA模型综述。本文建立了细致的VLA分类体系，涵盖三大研究方向：首类聚焦VLA各独立组件；次类致力于开发基于VLA的低层动作预测控制策略；第三类为高层任务规划器，能够将长周期任务分解为子任务序列，从而引导VLA执行更通用的用户指令。此外，我们系统梳理了相关资源，包括数据集、仿真平台与评测基准。最后，探讨了VLA面临的挑战，并展望了具身人工智能的未来发展方向。本综述的精选资源库详见：https://github.com/yueen-ma/Awesome-VLA。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this survey arises from the emergence of vision-language-action models (VLAs) as a new multimodal approach for embodied AI, building upon large language and vision-language models to address language-conditioned robotic tasks through action generation. The method involves presenting the first comprehensive survey on VLAs, organizing them into a detailed taxonomy with three research lines: individual VLA components, VLA-based low-level action prediction policies, and high-level task planners that decompose long-horizon instructions. The key findings include an extensive resource summary covering datasets, simulators, and benchmarks, along with identified challenges and future directions for VLAs in embodied AI.</div>
<div class="mono" style="margin-top:8px">本综述的动机源于视觉-语言-动作模型（VLAs）作为具身人工智能中语言条件机器人控制的关键方法而兴起，其建立在大型语言模型和视觉-语言模型进展之上。方法上，本文进行了全面的分类综述，将VLA研究组织为三个方向：个体模型组件、低级动作预测策略，以及能够分解长周期任务指令的高级任务规划器。主要成果包括对VLA领域进行了系统梳理，广泛总结了相关数据集、模拟器和基准测试，并指出了当前挑战和具身人工智能的未来发展方向。</div>
</details>
</div>
<div class="card">
<div class="title">STELAR-VISION: Self-Topology-Aware Efficient Learning for Aligned Reasoning in Vision</div>
<div class="meta-line">Authors: Chen Li, Han Zhang, Zhantao Yang, Fangyi Chen, Zihan Wang, Anudeepsekhar Bolimera, Marios Savvides</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-12T07:27:50+00:00 · Latest: 2026-02-04T06:14:03+00:00</div>
<div class="meta-line">Comments: This paper has been accepted at AAAI 2026. This is the author&#x27;s extended version. The final version will appear in the official proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.08688v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.08688v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have made significant strides in reasoning, yet they often struggle with complex multimodal tasks and tend to generate overly verbose outputs. A key limitation is their reliance on chain-of-thought (CoT) reasoning, despite many tasks benefiting from alternative topologies like trees or graphs. To address this, we introduce STELAR-Vision, a training framework for topology-aware reasoning. At its core is TopoAug, a synthetic data pipeline that enriches training with diverse topological structures. Using supervised fine-tuning and reinforcement learning, we post-train Qwen2VL models with both accuracy and efficiency in mind. Additionally, we propose Frugal Learning, which reduces output length with minimal accuracy loss. On MATH-V and VLM-S2H, STELAR-Vision improves accuracy by 9.7% over its base model and surpasses the larger Qwen2VL-72B-Instruct by 7.3%. On five out-of-distribution benchmarks, it outperforms Phi-4-Multimodal-Instruct by up to 28.4% and LLaMA-3.2-11B-Vision-Instruct by up to 13.2%, demonstrating strong generalization. Compared to Chain-Only training, our approach achieves 4.3% higher overall accuracy on in-distribution datasets and consistently outperforms across all OOD benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STELAR-VISION：面向视觉对齐推理的自拓扑感知高效学习框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型在推理方面取得显著进展，但在处理复杂多模态任务时仍面临困难，且常生成冗长输出。其关键局限在于依赖链式思维推理，而许多任务其实更适合树状或图状等拓扑结构。为此，我们提出STELAR-Vision——一种拓扑感知推理训练框架。其核心是TopoAug合成数据管道，通过多样化拓扑结构增强训练数据。采用监督微调与强化学习相结合的方法，我们在保持准确率的同时注重效率地对Qwen2VL模型进行后训练。此外，我们提出节俭学习法，能以极小的准确率损失大幅缩减输出长度。在MATH-V和VLM-S2H数据集上，STELAR-Vision相比基础模型准确率提升9.7%，并超越更大的Qwen2VL-72B-Instruct模型7.3%。在五个分布外基准测试中，其性能最高超越Phi-4-Multimodal-Instruct达28.4%，超越LLaMA-3.2-11B-Vision-Instruct达13.2%，展现出强大泛化能力。与纯链式训练相比，本方法在分布内数据集上整体准确率提升4.3%，且在全部分布外基准测试中均保持优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models often struggle with complex multimodal reasoning and generate verbose outputs due to their reliance on chain-of-thought reasoning, which is suboptimal for tasks better suited to alternative topological structures like trees or graphs. To address this, the authors introduce STELAR-Vision, a training framework that incorporates topology-aware reasoning through a synthetic data pipeline called TopoAug, which enriches training with diverse topological structures, and employs supervised fine-tuning and reinforcement learning to post-train models with a focus on both accuracy and efficiency, alongside a Frugal Learning method to reduce output length. Experimental results show that STELAR-Vision improves accuracy by 9.7% over its base model on MATH-V and VLM-S2H, surpasses the larger Qwen2VL-72B-Instruct by 7.3%, and outperforms models like Phi-4-Multimodal-Instruct and LLaMA-3.2-11B-Vision-Instruct by up to 28.4% and 13.2% respectively on out-of-distribution benchmarks, while also achieving 4.3% higher overall accuracy than chain-only training on in-distribution datasets.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在处理复杂多模态推理任务时，常因依赖链式思维推理而表现不佳并产生冗长输出，而许多任务更适合树状或图状等替代拓扑结构。为此，研究者提出了STELAR-Vision训练框架，其核心是TopoAug合成数据管道，用于生成多样化的拓扑推理结构，并结合监督微调和强化学习来提升准确性和效率，同时引入Frugal Learning方法以减少输出长度。实验结果表明，在MATH-V和VLM-S2H数据集上，STELAR-Vision相比其基础模型准确率提升了9.7%，并超越了更大的Qwen2VL-72B-Instruct模型7.3%；在五个分布外基准测试中，其性能优于Phi-4-Multimodal-Instruct模型高达28.4%，优于LLaMA-3.2-11B-Vision-Instruct模型高达13.2%，展现出强大的泛化能力，且相比仅使用链式思维的训练方法，在分布内数据集上整体准确率高出4.3%。</div>
</details>
</div>
<div class="card">
<div class="title">Same or Not? Enhancing Visual Perception in Vision-Language Models</div>
<div class="meta-line">Authors: Damiano Marsili, Aditya Mehta, Ryan Y. Lin, Georgia Gkioxari</div>
<div class="meta-line">First: 2025-12-29T16:43:47+00:00 · Latest: 2026-02-04T04:03:31+00:00</div>
<div class="meta-line">Comments: Project webpage: https://glab-caltech.github.io/twin/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23592v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23592v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://glab-caltech.github.io/twin/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition (&quot;Is it a cat or a dog?&quot;) over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>相同与否？增强视觉-语言模型的视觉感知能力</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在宏观视觉理解方面表现出色，但仍存在粒度粗糙、视觉偏见和忽略细微视觉细节的问题。现有训练语料通过强调通用识别（“这是猫还是狗？”）而非细粒度感知，加剧了这一局限。为此，我们提出了旨在增强VLM感知能力的新训练语料和任务。TWIN是一个包含56.1万对图像查询的大规模数据集，要求模型判断两张视觉相似的图像是否描绘同一物体，从而促使模型关注细微视觉线索。该数据集涵盖不同情境、视角和外观的多样化日常物体。基于TWIN对VLMs进行微调，可在细粒度识别方面取得显著提升，甚至在艺术、动物、植物和地标等未见领域也有效果。为量化这些提升，我们引入了FGVQA基准测试套件，包含1.2万个查询，整合了多领域的细粒度识别与检索数据集。现有VLMs在FGVQA上表现欠佳，但经TWIN微调后性能提升高达19.3%，且不影响通用VQA基准的表现。最后，TWIN数据集能有效扩展物体标注规模，分析表明规模是性能提升的关键。我们期望TWIN能作为开源VLM训练语料的即插即用补充，推动未来模型的感知精度提升。项目网页：https://glab-caltech.github.io/twin/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-language models often lack fine-grained perceptual abilities due to training on corpora focused on general recognition, leading to visual biases and missed details. To address this, the authors introduce TWIN, a large-scale dataset of 561,000 image-pair queries that tasks models with determining if two visually similar images depict the same object, thereby encouraging attention to nuanced visual cues. Fine-tuning VLMs on TWIN yields significant improvements in fine-grained recognition, achieving up to a 19.3% gain on the introduced FGVQA benchmark across diverse unseen domains like art and landmarks, without compromising general VQA performance.</div>
<div class="mono" style="margin-top:8px">视觉语言模型在细粒度视觉感知方面存在不足，往往关注粗粒度的识别而忽略细微的视觉细节。为解决这一问题，研究者提出了TWIN数据集，包含56.1万个图像对查询，要求模型判断两个视觉上相似的图像是否描绘同一物体，从而促使模型关注细微的视觉线索。在TWIN上微调的视觉语言模型在细粒度识别上取得显著提升，在新提出的FGVQA基准测试中，在艺术、动物等多个未见领域上性能最高提升19.3%，同时不影响其在通用视觉问答任务上的表现。</div>
</details>
</div>
<div class="card">
<div class="title">WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models</div>
<div class="meta-line">Authors: Zijin Yang, Yu Sun, Kejiang Chen, Jiawei Zhao, Jun Jiang, Weiming Zhang, Nenghai Yu</div>
<div class="meta-line">First: 2026-01-29T12:14:32+00:00 · Latest: 2026-02-04T03:23:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.21610v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.21610v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Digital watermarking is essential for securing generated images from diffusion models. Accurate watermark evaluation is critical for algorithm development, yet existing methods have significant limitations: they lack a unified framework for both residual and semantic watermarks, provide results without interpretability, neglect comprehensive security considerations, and often use inappropriate metrics for semantic watermarks. To address these gaps, we propose WMVLM, the first unified and interpretable evaluation framework for diffusion model image watermarking via vision-language models (VLMs). We redefine quality and security metrics for each watermark type: residual watermarks are evaluated by artifact strength and erasure resistance, while semantic watermarks are assessed through latent distribution shifts. Moreover, we introduce a three-stage training strategy to progressively enable the model to achieve classification, scoring, and interpretable text generation. Experiments show WMVLM outperforms state-of-the-art VLMs with strong generalization across datasets, diffusion models, and watermarking methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WMVLM：基于视觉语言模型的扩散模型图像水印评估方法</div>
<div class="mono" style="margin-top:8px">数字水印对保护扩散模型生成的图像安全至关重要。准确的水印评估是算法开发的关键，但现有方法存在明显局限：缺乏针对残留水印与语义水印的统一框架、结果可解释性不足、忽视全面的安全考量，且常对语义水印使用不恰当的评估指标。为填补这些空白，我们提出WMVLM——首个基于视觉语言模型（VLM）的统一可解释扩散模型图像水印评估框架。我们重新定义了各类水印的质量与安全指标：残留水印通过伪影强度与抗擦除能力评估，语义水印则通过潜在分布偏移进行度量。此外，我们引入三阶段训练策略，使模型逐步实现分类、评分及可解释文本生成功能。实验表明，WMVLM在跨数据集、扩散模型与水印方法的泛化能力上均优于当前最先进的视觉语言模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for a more robust and interpretable evaluation framework for digital watermarks in diffusion model-generated images, as existing methods are fragmented, lack interpretability, and use inadequate metrics. The proposed method, WMVLM, introduces a unified evaluation framework using vision-language models, redefining quality and security metrics for residual and semantic watermarks separately and employing a three-stage training strategy for classification, scoring, and interpretable text generation. Experimental results demonstrate that WMVLM outperforms state-of-the-art vision-language models and exhibits strong generalization across various datasets, diffusion models, and watermarking techniques.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于，现有方法缺乏针对残差水印和语义水印的统一评估框架、可解释性差且使用不恰当的指标，因此需要一种更有效的评估扩散模型生成图像中数字水印的方法。所提出的WMVLM方法利用视觉语言模型，引入了一个统一且可解释的评估框架，重新定义了每种水印类型的质量和安全指标——通过伪影强度和擦除抵抗力评估残差水印，通过潜在分布偏移评估语义水印——并采用三阶段训练策略实现分类、评分和可解释的文本生成。实验结果表明，WMVLM优于最先进的视觉语言模型，并在不同数据集、扩散模型和水印技术上表现出强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">UniVRSE: Unified Vision-conditioned Response Semantic Entropy for Hallucination Detection in Medical Vision-Language Models</div>
<div class="meta-line">Authors: Zehui Liao, Shishuai Hu, Ke Zou, Mengyuan Jin, Yanning Zhang, Huazhu Fu, Liangli Zhen, Yong Xia</div>
<div class="meta-line">First: 2025-03-26T12:45:34+00:00 · Latest: 2026-02-04T03:14:16+00:00</div>
<div class="meta-line">Comments: Under Review. 12 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.20504v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.20504v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) have great potential for medical image understanding, particularly in Visual Report Generation (VRG) and Visual Question Answering (VQA), but they may generate hallucinated responses that contradict visual evidence, limiting clinical deployment. Although uncertainty-based hallucination detection methods are intuitive and effective, they are limited in medical VLMs. Specifically, Semantic Entropy (SE), effective in text-only LLMs, becomes less reliable in medical VLMs due to their overconfidence from strong language priors. To address this challenge, we propose UniVRSE, a Unified Vision-conditioned Response Semantic Entropy framework for hallucination detection in medical VLMs. UniVRSE strengthens visual guidance during uncertainty estimation by contrasting the semantic predictive distributions derived from an original image-text pair and a visually distorted counterpart, with higher entropy indicating hallucination risk. For VQA, UniVRSE works on the image-question pair, while for VRG, it decomposes the report into claims, generates verification questions, and applies vision-conditioned entropy estimation at the claim level. To evaluate hallucination detection, we propose a unified pipeline that generates responses on medical datasets and derives hallucination labels via factual consistency assessment. However, current evaluation methods rely on subjective criteria or modality-specific rules. To improve reliability, we introduce Alignment Ratio of Atomic Facts (ALFA), a novel method that quantifies fine-grained factual consistency. ALFA-derived labels provide ground truth for robust benchmarking. Experiments on six medical VQA/VRG datasets and three VLMs show UniVRSE significantly outperforms existing methods with strong cross-modal generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniVRSE：面向医疗视觉语言模型幻觉检测的统一视觉条件响应语义熵框架</div>
<div class="mono" style="margin-top:8px">视觉语言模型在医学图像理解领域（尤其是视觉报告生成和视觉问答任务）具有巨大潜力，但可能产生与视觉证据矛盾的幻觉响应，限制了临床部署。尽管基于不确定性的幻觉检测方法直观有效，但在医疗视觉语言模型中存在局限：仅适用于纯文本大语言模型的语义熵方法，因医疗视觉语言模型受强语言先验导致的过度自信而可靠性下降。为此，我们提出UniVRSE——一种用于医疗视觉语言模型幻觉检测的统一视觉条件响应语义熵框架。该框架通过对比原始图文对与视觉失真对应体衍生的语义预测分布来增强不确定性估计中的视觉引导，更高熵值表示幻觉风险。在视觉问答任务中，UniVRSE作用于图像-问题对；在视觉报告生成任务中，则将报告分解为声明、生成验证问题，并在声明层级应用视觉条件熵估计。为评估幻觉检测性能，我们构建了在医学数据集生成响应并通过事实一致性评估推导幻觉标签的统一流程。针对现有评估方法依赖主观标准或模态特定规则的问题，我们提出原子事实对齐比——一种量化细粒度事实一致性的新方法，其衍生的标签为稳健基准测试提供真实依据。在六个医疗视觉问答/视觉报告生成数据集和三个视觉语言模型上的实验表明，UniVRSE凭借强大的跨模态泛化能力显著优于现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study addresses the problem of hallucinated responses in medical vision-language models (VLMs), which can generate outputs contradicting visual evidence and hinder clinical use. The proposed UniVRSE framework enhances uncertainty-based detection by contrasting semantic predictive distributions from original and visually distorted image-text pairs, with higher entropy signaling hallucination risk; it adapts this approach for both VQA tasks and VRG tasks by decomposing reports into claims and generating verification questions. Experimental evaluation on six medical datasets using a novel factual consistency metric called ALFA shows that UniVRSE outperforms existing methods and demonstrates strong cross-modal generalization.</div>
<div class="mono" style="margin-top:8px">医学视觉语言模型（VLMs）容易产生与视觉证据相矛盾的幻觉响应，这阻碍了其临床部署；现有的基于不确定性的检测方法（如语义熵）在该领域可靠性较低，因为模型受强语言先验影响而过度自信。为解决此问题，本文提出了UniVRSE，一个统一的框架，通过对比原始图像-文本对与视觉失真对应物的语义预测分布来增强视觉引导，其中更高的熵表示幻觉风险；对于视觉问答（VQA），它基于图像-问题对操作，而对于视觉报告生成（VRG），它将报告分解为声明，生成验证问题，并在声明级别应用视觉条件熵估计。在六个医学VQA/VRG数据集和三个VLM上的实验评估，采用新颖的细粒度事实一致性度量方法ALFA进行鲁棒基准测试，结果表明UniVRSE显著优于现有方法，并展现出强大的跨模态泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">VEAttack: Downstream-agnostic Vision Encoder Attack against Large Vision Language Models</div>
<div class="meta-line">Authors: Hefei Mei, Zirui Wang, Shen You, Minjing Dong, Chang Xu</div>
<div class="meta-line">First: 2025-05-23T03:46:04+00:00 · Latest: 2026-02-04T02:10:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17440v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17440v2">PDF</a> · <a href="https://github.com/hefeimei06/VEAttack-LVLM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities in multimodal understanding and generation, yet their vulnerability to adversarial attacks raises significant robustness concerns. While existing effective attacks always focus on task-specific white-box settings, these approaches are limited in the context of LVLMs, which are designed for diverse downstream tasks and require expensive full-model gradient computations. Motivated by the pivotal role and wide adoption of the vision encoder in LVLMs, we propose a simple yet effective Vision Encoder Attack (VEAttack), which targets the vision encoder of LVLMs only. Specifically, we propose to generate adversarial examples by minimizing the cosine similarity between the clean and perturbed visual features, without accessing the following large language models, task information, and labels. It significantly reduces the computational overhead while eliminating the task and label dependence of traditional white-box attacks in LVLMs. To make this simple attack effective, we propose to perturb images by optimizing image tokens instead of the classification token. We provide both empirical and theoretical evidence that VEAttack can easily generalize to various tasks. VEAttack has achieved a performance degradation of 94.5% on image caption task and 75.7% on visual question answering task. We also reveal some key observations to provide insights into LVLM attack/defense: 1) hidden layer variations of LLM, 2) token attention differential, 3) Möbius band in transfer attack, 4) low sensitivity to attack steps. The code is available at https://github.com/hefeimei06/VEAttack-LVLM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VEAttack：针对大型视觉语言模型的下游无关视觉编码器攻击</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）在多模态理解与生成方面展现出卓越能力，但其对抗攻击的脆弱性引发了严重的鲁棒性担忧。现有有效攻击通常聚焦于任务特定的白盒设置，但这些方法在LVLMs场景中存在局限——LVLMs专为多样化下游任务设计，且需要昂贵的全模型梯度计算。基于视觉编码器在LVLMs中的核心作用与广泛采用，我们提出一种简洁高效的视觉编码器攻击方法（VEAttack），该方法仅针对LVLMs的视觉编码器。具体而言，我们通过最小化干净视觉特征与扰动后特征的余弦相似度来生成对抗样本，无需访问后续大型语言模型、任务信息及标签。这显著降低了计算开销，同时消除了传统白盒攻击在LVLMs中对任务和标签的依赖。为使该简洁攻击生效，我们提出通过优化图像令牌（而非分类令牌）来扰动图像。我们通过实证与理论证据表明，VEAttack可轻松泛化至多种任务：在图像描述任务中实现94.5%的性能下降，在视觉问答任务中实现75.7%的性能下降。同时，我们揭示了若干关键发现以深化对LVLM攻防的理解：1）LLM隐藏层变异，2）令牌注意力差异，3）迁移攻击中的莫比乌斯带现象，4）对攻击步数的低敏感性。代码已开源：https://github.com/hefeimei06/VEAttack-LVLM。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for efficient and generalizable adversarial attacks against large vision-language models (LVLMs) that are designed for diverse downstream tasks, this research introduces VEAttack, a method that targets only the vision encoder to circumvent the computational expense and task-specific limitations of full-model white-box attacks. The method generates adversarial examples by minimizing the cosine similarity between clean and perturbed visual features, optimizing image tokens rather than the classification token, without requiring access to the language model, task information, or labels. Experimental results demonstrate its effectiveness, achieving a 94.5% performance degradation on image captioning and 75.7% on visual question answering, while also providing insights into LVLM vulnerabilities through observations on hidden layer variations, token attention differentials, and transfer attack phenomena.</div>
<div class="mono" style="margin-top:8px">针对大型视觉语言模型（LVLMs）的对抗攻击脆弱性，以及现有任务特定白盒方法需要全模型梯度计算且依赖下游任务的局限性，本文提出了VEAttack，一种仅针对视觉编码器的下游无关攻击方法。该方法通过最小化干净与扰动视觉特征之间的余弦相似度来生成对抗样本，优化图像令牌而非分类令牌，无需访问后续大语言模型、任务信息或标签。实验结果表明，VEAttack能有效泛化至不同任务，在图像描述任务上导致94.5%的性能下降，在视觉问答任务上导致75.7%的性能下降，同时通过对隐藏层变化、令牌注意力和迁移攻击模式的分析，揭示了LVLM鲁棒性的关键见解。</div>
</details>
</div>
<div class="card">
<div class="title">Image Corruption-Inspired Membership Inference Attacks against Large Vision-Language Models</div>
<div class="meta-line">Authors: Zongyu Wu, Minhua Lin, Zhiwei Zhang, Fali Wang, Xianren Zhang, Xiang Zhang, Suhang Wang</div>
<div class="meta-line">First: 2025-06-14T04:22:36+00:00 · Latest: 2026-02-04T00:24:48+00:00</div>
<div class="meta-line">Comments: Accepted by EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12340v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.12340v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (LVLMs) have demonstrated outstanding performance in many downstream tasks. However, LVLMs are trained on large-scale datasets, which can pose privacy risks if training images contain sensitive information. Therefore, it is important to detect whether an image is used to train the LVLM. Recent studies have investigated membership inference attacks (MIAs) against LVLMs, including detecting image-text pairs and single-modality content. In this work, we focus on detecting whether a target image is used to train the target LVLM. We design simple yet effective Image Corruption-Inspired Membership Inference Attacks (ICIMIA) against LVLMs, which are inspired by LVLM&#x27;s different sensitivity to image corruption for member and non-member images. We first perform an MIA method under the white-box setting, where we can obtain the embeddings of the image through the vision part of the target LVLM. The attacks are based on the embedding similarity between the image and its corrupted version. We further explore a more practical scenario where we have no knowledge about target LVLMs and we can only query the target LVLMs with an image and a textual instruction. We then conduct the attack by utilizing the output text embeddings&#x27; similarity. Experiments on existing datasets validate the effectiveness of our proposed methods under those two different settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于图像损坏启发的视觉语言大模型成员推断攻击</div>
<div class="mono" style="margin-top:8px">视觉语言大模型（LVLMs）在下游任务中展现出卓越性能，但其训练依赖大规模数据集，若训练图像包含敏感信息则可能引发隐私风险。因此，检测图像是否用于训练LVLM至关重要。现有研究已探索针对LVLMs的成员推断攻击（MIAs），涵盖图文对及单模态内容检测。本研究聚焦于检测目标图像是否用于训练目标LVLM，提出一种简洁高效的基于图像损坏启发的成员推断攻击方法（ICIMIA），其灵感来源于LVLM对成员与非成员图像在图像损坏敏感性上的差异。首先在白盒设置下实施MIA方法，通过目标LVLM的视觉模块获取图像嵌入表征，基于原始图像与其损坏版本间的嵌入相似度进行攻击。进一步探索更实际的场景：在未知目标LVLM内部信息的情况下，仅能通过图像和文本指令查询模型，此时利用输出文本嵌入的相似度实施攻击。在现有数据集上的实验验证了所提方法在两种不同设置下的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses privacy risks in large vision-language models (LVLMs) by developing membership inference attacks to determine if specific images were used in training, motivated by concerns that training data may contain sensitive information. The method proposes Image Corruption-Inspired Membership Inference Attacks (ICIMIA), which exploit differences in how LVLMs process corrupted versions of member versus non-member images through two approaches: a white-box attack using vision encoder embeddings and a more practical black-box attack using text output embeddings from model queries. Experimental results demonstrate that both attack methods effectively identify training data membership across standard datasets, validating their effectiveness under different access scenarios.</div>
<div class="mono" style="margin-top:8px">本研究针对大型视觉语言模型（LVLMs）的隐私风险，开发了成员推理攻击（MIAs）以检测特定图像是否用于训练。该方法名为图像损坏启发的成员推理攻击（ICIMIA），利用LVLMs对成员与非成员图像损坏的不同敏感性。在白盒设置下，通过视觉编码器计算原始图像与损坏图像的嵌入相似性；在更实用的黑盒设置下，则利用模型输出的文本嵌入相似性。在现有数据集上的实验结果验证了两种攻击场景的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">VideoBrain: Learning Adaptive Frame Sampling for Long Video Understanding</div>
<div class="meta-line">Authors: Junbo Zou, Ziheng Huang, Shengjie Zhang, Liwen Zhang, Weining Shen</div>
<div class="meta-line">First: 2026-02-04T00:08:35+00:00 · Latest: 2026-02-04T00:08:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04094v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04094v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-form video understanding remains challenging for Vision-Language Models (VLMs) due to the inherent tension between computational constraints and the need to capture information distributed across thousands of frames. Existing approaches either sample frames uniformly (risking information loss) or select keyframes in a single pass (with no recovery from poor choices). We propose VideoBrain, an end-to-end framework that enables VLMs to adaptively acquire visual information through learned sampling policies. Our approach features dual complementary agents: a CLIP-based agent for semantic retrieval across the video and a Uniform agent for dense temporal sampling within intervals. Unlike prior agent-based methods that rely on text-only LLMs orchestrating visual tools, our VLM directly perceives frames and reasons about information sufficiency. To prevent models from invoking agents indiscriminately to maximize rewards, we introduce a behavior-aware reward function coupled with a data classification pipeline that teaches the model when agent invocation is genuinely beneficial. Experiments on four long video benchmarks demonstrate that VideoBrain achieves +3.5% to +9.0% improvement over the baseline while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoBrain：面向长视频理解的自适应帧采样学习框架</div>
<div class="mono" style="margin-top:8px">长视频理解对视觉语言模型（VLM）仍具挑战性，因其需在计算约束与跨数千帧的信息捕获需求间取得平衡。现有方法采用均匀帧采样（易丢失信息）或单次关键帧选择（无法修正错误选择）。本文提出VideoBrain端到端框架，使VLM能通过习得的采样策略自适应获取视觉信息。该框架采用双互补智能体：基于CLIP的语义检索智能体（跨视频检索）与均匀采样智能体（区间内密集时序采样）。不同于依赖纯文本大语言模型调度视觉工具的既有方案，我们的VLM可直接感知视频帧并推理信息充分性。为防止模型为最大化奖励而滥用智能体，我们设计了行为感知奖励函数与数据分类流程，指导模型识别智能体调用的真实效益。在四个长视频基准测试中，VideoBrain以减少30-40%采样帧数为代价，性能较基线提升3.5%-9.0%，并在短视频基准上展现出强大的跨数据集泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of long-form video understanding for Vision-Language Models (VLMs), where computational constraints conflict with the need to capture information across thousands of frames. The proposed VideoBrain framework enables VLMs to adaptively sample frames using dual complementary agents: a CLIP-based agent for semantic retrieval and a Uniform agent for dense temporal sampling within intervals, allowing the VLM to directly perceive frames and reason about information sufficiency. To prevent indiscriminate agent invocation, a behavior-aware reward function and data classification pipeline are introduced. Experiments on four long video benchmarks show VideoBrain achieves a +3.5% to +9.0% improvement over baselines while using 30-40% fewer frames, with strong cross-dataset generalization to short video benchmarks.</div>
<div class="mono" style="margin-top:8px">该研究针对视觉语言模型（VLMs）在长视频理解中的挑战，即计算限制与需要从数千帧中捕获信息之间存在固有矛盾，而现有的均匀采样或单遍关键帧选择方法存在信息丢失或选择不佳后无法恢复的风险。提出的VideoBrain框架通过端到端方法使VLMs能够学习自适应帧采样策略，其特点是采用双互补智能体：一个基于CLIP的智能体用于跨视频的语义检索，另一个均匀采样智能体用于在区间内进行密集时间采样，这使得VLM能直接感知帧并推理信息充分性，而非依赖纯文本大语言模型来协调视觉工具。为防止模型为最大化奖励而随意调用智能体，引入了行为感知奖励函数与数据分类流程，教导模型何时调用智能体真正有益。在四个长视频基准测试上的实验结果表明，VideoBrain在比基线少用30-40%帧数的同时，性能提升了+3.5%至+9.0%，并在短视频基准上表现出强大的跨数据集泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">RF-DETR: Neural Architecture Search for Real-Time Detection Transformers</div>
<div class="meta-line">Authors: Isaac Robinson, Peter Robicheaux, Matvei Popov, Deva Ramanan, Neehar Peri</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-11-12T18:58:39+00:00 · Latest: 2026-02-03T23:53:20+00:00</div>
<div class="meta-line">Comments: This work has been accepted to the International Conference on Learning Representations (ICLR) 2026. Project Page: https://rfdetr.roboflow.com/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09554v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09554v2">PDF</a> · <a href="https://github.com/roboflow/rf-detr">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary detectors achieve impressive performance on COCO, but often fail to generalize to real-world datasets with out-of-distribution classes not typically found in their pre-training. Rather than simply fine-tuning a heavy-weight vision-language model (VLM) for new domains, we introduce RF-DETR, a light-weight specialist detection transformer that discovers accuracy-latency Pareto curves for any target dataset with weight-sharing neural architecture search (NAS). Our approach fine-tunes a pre-trained base network on a target dataset and evaluates thousands of network configurations with different accuracy-latency tradeoffs without re-training. Further, we revisit the &quot;tunable knobs&quot; for NAS to improve the transferability of DETRs to diverse target domains. Notably, RF-DETR significantly improves over prior state-of-the-art real-time methods on COCO and Roboflow100-VL. RF-DETR (nano) achieves 48.0 AP on COCO, beating D-FINE (nano) by 5.3 AP at similar latency, and RF-DETR (2x-large) outperforms GroundingDINO (tiny) by 1.2 AP on Roboflow100-VL while running 20x as fast. To the best of our knowledge, RF-DETR (2x-large) is the first real-time detector to surpass 60 AP on COCO. Our code is available at https://github.com/roboflow/rf-detr</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RF-DETR：面向实时检测Transformer的神经架构搜索</div>
<div class="mono" style="margin-top:8px">开放词汇检测器在COCO数据集上表现优异，但通常难以泛化到包含预训练未见分布外类别的真实数据集。本文提出RF-DETR，一种轻量级专用检测Transformer，通过权重共享神经架构搜索为任意目标数据集发现精度-延迟帕累托曲线。该方法在目标数据集上微调预训练基础网络，无需重新训练即可评估数千种不同精度-延迟权衡的网络配置。我们重新审视NAS的“可调参数”以提升DETR向多样化目标域的迁移能力。RF-DETR在COCO和Roboflow100-VL数据集上显著超越现有实时方法：RF-DETR（nano）在COCO上达到48.0 AP，在同等延迟下比D-FINE（nano）高5.3 AP；RF-DETR（2x-large）在Roboflow100-VL上比GroundingDINO（tiny）高1.2 AP，且运行速度快20倍。据我们所知，RF-DETR（2x-large）是首个在COCO上突破60 AP的实时检测器。代码已开源：https://github.com/roboflow/rf-detr</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limited generalization of open-vocabulary detectors to out-of-distribution classes without relying on heavy fine-tuning of vision-language models, this paper introduces RF-DETR, a lightweight specialist detection transformer. The method employs weight-sharing neural architecture search to efficiently discover accuracy-latency Pareto curves for a target dataset by fine-tuning a pre-trained base network and evaluating numerous configurations without retraining, while revisiting tunable NAS parameters to enhance DETR transferability. Key experimental results show RF-DETR significantly outperforms prior real-time methods, with its nano variant achieving 48.0 AP on COCO (5.3 AP higher than D-FINE nano at similar latency) and its 2x-large variant surpassing GroundingDINO tiny by 1.2 AP on Roboflow100-VL while being 20x faster, and it is the first real-time detector to exceed 60 AP on COCO.</div>
<div class="mono" style="margin-top:8px">为解决开放词汇检测器在包含分布外类别的真实数据集上泛化能力有限的问题，本研究提出了RF-DETR，一种轻量级专用检测Transformer。该方法采用权重共享的神经架构搜索（NAS），通过微调预训练基础网络并无需重新训练即可评估数千种配置，从而高效地为任何目标数据集发现精度-延迟帕累托曲线，同时重新审视NAS的“可调旋钮”以提升DETR的迁移性。实验结果表明，RF-DETR显著优于先前的实时检测方法；例如，其nano版本在COCO上达到48.0 AP，在相似延迟下比D-FINE（nano）高出5.3 AP，而2x-large版本在Roboflow100-VL上以20倍的速度超越GroundingDINO（tiny）1.2 AP，并成为首个在COCO上突破60 AP的实时检测器。</div>
</details>
</div>
<div class="card">
<div class="title">Recov-Vision: Linking Street View Imagery and Vision-Language Models for Post-Disaster Recovery</div>
<div class="meta-line">Authors: Yiming Xiao, Archit Gupta, Miguel Esparza, Yu-Hsuan Ho, Antonia Sebastian, Hannah Weas, Rose Houck, Ali Mostafavi</div>
<div class="meta-line">First: 2025-09-25T00:01:38+00:00 · Latest: 2026-02-03T20:31:48+00:00</div>
<div class="meta-line">Comments: 20 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20628v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.20628v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Building-level occupancy after disasters is vital for triage, inspections, utility re-energization, and equitable resource allocation. Overhead imagery provides rapid coverage but often misses facade and access cues that determine habitability, while street-view imagery captures those details but is sparse and difficult to align with parcels. We present FacadeTrack, a street-level, language-guided framework that links panoramic video to parcels, rectifies views to facades, and elicits interpretable attributes (for example, entry blockage, temporary coverings, localized debris) that drive two decision strategies: a transparent one-stage rule and a two-stage design that separates perception from conservative reasoning. Evaluated across two post-Hurricane Helene surveys, the two-stage approach achieves a precision of 0.927, a recall of 0.781, and an F-1 score of 0.848, compared with the one-stage baseline at a precision of 0.943, a recall of 0.728, and an F-1 score of 0.822. Beyond accuracy, intermediate attributes and spatial diagnostics reveal where and why residual errors occur, enabling targeted quality control. The pipeline provides auditable, scalable occupancy assessments suitable for integration into geospatial and emergency-management workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Recov-Vision：融合街景影像与视觉语言模型实现灾后恢复评估</div>
<div class="mono" style="margin-top:8px">灾后建筑居住状态评估对分级处置、现场勘查、公用设施恢复及资源公平分配至关重要。航空影像虽能快速覆盖区域，却常遗漏决定宜居性的立面特征与通行线索；街景影像虽能捕捉这些细节，但存在数据稀疏且难以与地块对齐的问题。本文提出FacadeTrack框架，通过语言引导将全景视频与地块关联，校正立面视角，并提取可解释属性（如入口堵塞、临时覆盖物、局部废墟），驱动两种决策策略：透明的单阶段规则，以及将感知与保守推理分离的双阶段设计。在飓风海伦灾后两次勘测中，双阶段方法取得精确率0.927、召回率0.781、F-1分数0.848，优于单阶段基线的0.943精确率、0.728召回率及0.822 F-1分数。除精度外，中间属性与空间诊断能揭示残留误差的成因与位置，实现针对性质量控制。该流程提供可审计、可扩展的居住状态评估方案，适用于地理空间与应急管理工作流集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of assessing building habitability after disasters, where overhead imagery lacks facade details and street-view imagery is difficult to align with property parcels. The authors introduce FacadeTrack, a framework that links panoramic street-view video to parcels, rectifies views to building facades, and uses vision-language models to extract interpretable attributes like entry blockage and debris. The method offers two decision strategies: a transparent one-stage rule and a two-stage design separating perception from reasoning. Evaluated on post-Hurricane Helene surveys, the two-stage approach achieved a precision of 0.927, recall of 0.781, and F-1 score of 0.848, outperforming the one-stage baseline in recall and F-1 score while providing interpretable attributes for error analysis and quality control.</div>
<div class="mono" style="margin-top:8px">为改进灾后建筑占用情况评估（这对应急响应和资源分配至关重要），本研究提出了FacadeTrack框架，该框架将街景图像与视觉-语言模型结合，以捕捉常被高空图像忽略的立面细节。该方法处理全景视频，将其与地块对齐，校正立面视图，并利用语言引导模型提取可解释属性（如入口堵塞、临时覆盖物和局部碎片），采用单阶段规则和两阶段感知-推理决策策略。在飓风海伦后的调查评估中，两阶段方法实现了0.927的精确率、0.781的召回率和0.848的F-1分数，在召回率和F-1分数上优于单阶段基线，同时中间属性提供了空间诊断以解释错误并支持质量控制。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Long-Horizon Vision-Language-Action Models via Static-Dynamic Disentanglement</div>
<div class="meta-line">Authors: Weikang Qiu, Tinglin Huang, Aosong Feng, Rex Ying</div>
<div class="meta-line">First: 2026-02-03T20:17:47+00:00 · Latest: 2026-02-03T20:17:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03983v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently emerged as a promising paradigm for generalist robotic control. Built upon vision-language model (VLM) architectures, VLAs predict actions conditioned on visual observations and language instructions, achieving strong performance and generalization across tasks. However, VLAs face two major challenges: limited long-horizon context and inefficient inference due to the quadratic attention complexity and large parameter counts. Our work is motivated by the observation that much of the visual information in a trajectory remains static across timesteps (e.g., the background). Leveraging this property, we propose SD-VLA, a framework that disentangles visual inputs into multi-level static and dynamic tokens, which enables (1) retaining a single copy of static tokens across frames to significantly reduce context length, and (2) reusing the key-value (KV) cache of static tokens through a lightweight recache gate that updates only when necessary. This design enables efficient multi-frame integration and efficient inference. In addition, we introduce a new benchmark that more effectively evaluates the long-horizon temporal dependency modeling ability of VLAs. Experimental results show that our approach outperforms baselines on this benchmark by 39.8% absolute improvement in success rate, and achieves a 3.9% gain on the SimplerEnv benchmark. Moreover, SD-VLA delivers a 2.26x inference speedup over the base VLA model on the same benchmark, enabling faster and more practical real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于静态-动态解耦的高效长时程视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型作为通用机器人控制的新兴范式，基于视觉-语言模型（VLM）架构构建，能够根据视觉观测和语言指令预测动作，在多任务中展现出优异的性能与泛化能力。然而，VLA面临两大挑战：长时程上下文建模受限，以及因二次注意力复杂度与庞大参数量导致的推理低效。本研究基于轨迹中大量视觉信息（如背景）在时间步间保持静态的观察，提出SD-VLA框架，将视觉输入解耦为多层次的静态与动态令牌。该设计实现：（1）跨帧保留静态令牌的单一副本以显著压缩上下文长度；（2）通过轻量级重缓存门仅在必要时更新静态令牌的键值（KV）缓存，实现跨帧复用。这一架构支持高效的多帧信息整合与推理加速。此外，我们构建了能更有效评估VLA长时程时序依赖建模能力的新基准。实验表明，该方法在该基准上的成功率绝对提升39.8%，在SimplerEnv基准上提升3.9%。同时，SD-VLA在同等基准上相比基础VLA模型实现2.26倍推理加速，为实际部署提供更高效的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the inefficiency of Vision-Language-Action (VLA) models in long-horizon robotic control, where quadratic attention complexity and large parameter counts limit context and slow inference. The proposed SD-VLA method disentangles visual inputs into multi-level static and dynamic tokens, retaining a single copy of static tokens across frames and reusing their key-value cache via a lightweight recache gate to update only when necessary, thereby reducing context length and accelerating inference. Experiments on a new long-horizon benchmark show a 39.8% absolute improvement in success rate, a 3.9% gain on SimplerEnv, and a 2.26x inference speedup over the base VLA model.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视觉-语言-动作（VLA）模型在长时程机器人控制中的效率问题，标准架构因二次注意力复杂性和参数量大，限制了上下文长度并降低了推理速度。为此，提出的SD-VLA框架将视觉输入解耦为多层次的静态和动态令牌，跨帧保留静态令牌的单一副本，并通过轻量级重缓存门仅在必要时更新其键值缓存，从而实现高效的多帧集成。在新长时程基准测试中，实验结果显示成功率绝对提升39.8%，在SimplerEnv基准上增益3.9%，且推理速度比基础VLA模型快2.26倍，证明了其改进的时序建模能力和更快的实际部署速度。</div>
</details>
</div>
<div class="card">
<div class="title">VLS: Steering Pretrained Robot Policies via Vision-Language Models</div>
<div class="meta-line">Authors: Shuo Liu, Ishneet Sukhvinder Singh, Yiqing Xu, Jiafei Duan, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-03T19:50:16+00:00 · Latest: 2026-02-03T19:50:16+00:00</div>
<div class="meta-line">Comments: 11 Pages, Project page: https://vision-language-steering.github.io/webpage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03973v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vision-language-steering.github.io/webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Why do pretrained diffusion or flow-matching policies fail when the same task is performed near an obstacle, on a shifted support surface, or amid mild clutter? Such failures rarely reflect missing motor skills; instead, they expose a limitation of imitation learning under train-test shifts, where action generation is tightly coupled to training-specific spatial configurations and task specifications. Retraining or fine-tuning to address these failures is costly and conceptually misaligned, as the required behaviors already exist but cannot be selectively adapted at test time. We propose Vision-Language Steering (VLS), a training-free framework for inference-time adaptation of frozen generative robot policies. VLS treats adaptation as an inference-time control problem, steering the sampling process of a pretrained diffusion or flow-matching policy in response to out-of-distribution observation-language inputs without modifying policy parameters. By leveraging vision-language models to synthesize trajectory-differentiable reward functions, VLS guides denoising toward action trajectories that satisfy test-time spatial and task requirements. Across simulation and real-world evaluations, VLS consistently outperforms prior steering methods, achieving a 31% improvement on CALVIN and a 13% gain on LIBERO-PRO. Real-world deployment on a Franka robot further demonstrates robust inference-time adaptation under test-time spatial and semantic shifts. Project page: https://vision-language-steering.github.io/webpage/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLS：通过视觉语言模型引导预训练机器人策略</div>
<div class="mono" style="margin-top:8px">为何预训练的扩散或流匹配策略在障碍物附近、偏移支撑面上或轻度杂乱环境中执行相同任务时会失效？此类失效很少源于运动技能缺失，而是揭示了模仿学习在训练-测试偏移下的局限性：动作生成与训练特定的空间配置和任务规范紧密耦合。针对这些失效进行重新训练或微调不仅成本高昂，且存在概念偏差——所需行为本已存在，却无法在测试时选择性适配。我们提出视觉语言引导（VLS），一种无需训练的冻结生成式机器人策略推理时适配框架。VLS将适配视为推理时控制问题，通过视觉语言模型合成轨迹可微奖励函数，引导去噪过程生成满足测试时空间与任务要求的动作轨迹，在保持策略参数不变的前提下，根据分布外观测-语言输入引导预训练扩散/流匹配策略的采样过程。在仿真与真实环境评估中，VLS始终优于现有引导方法，在CALVIN和LIBERO-PRO上分别实现31%和13%的性能提升。Franka机器人的真实部署进一步验证了其在测试时空间与语义偏移下的鲁棒推理时适配能力。项目页面：https://vision-language-steering.github.io/webpage/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the failure of pretrained diffusion or flow-matching robot policies to generalize under test-time shifts, such as the presence of obstacles or changed spatial configurations, which stems from the tight coupling of action generation to training-specific conditions in imitation learning. The proposed method, Vision-Language Steering (VLS), is a training-free framework that adapts frozen generative policies at inference time by treating adaptation as a control problem; it steers the policy&#x27;s sampling process using trajectory-differentiable reward functions synthesized by vision-language models from observation-language inputs, without modifying the policy parameters. Key experimental results show that VLS outperforms prior steering methods, achieving a 31% improvement on the CALVIN benchmark and a 13% gain on LIBERO-PRO, with real-world deployment on a Franka robot demonstrating robust adaptation to spatial and semantic shifts.</div>
<div class="mono" style="margin-top:8px">本研究针对预训练的扩散或流匹配机器人策略在训练-测试分布偏移（如存在障碍物或空间配置改变）下的失败问题，这种失败并非由于技能缺失，而是源于动作生成与训练特定条件的紧密耦合。提出的视觉语言引导（VLS）框架实现了无需训练、在推理时自适应的能力，将其视为一个控制问题；它利用视觉语言模型根据分布外观察-语言输入合成的轨迹可微分奖励函数，引导冻结生成策略的采样过程，而不修改策略参数。实验结果表明，VLS优于先前的引导方法，在CALVIN基准上实现了31%的性能提升，在LIBERO-PRO上获得了13%的增益，在Franka机器人上的真实世界部署进一步验证了对空间和语义偏移的鲁棒自适应能力。</div>
</details>
</div>
<div class="card">
<div class="title">SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?</div>
<div class="meta-line">Authors: Azmine Toushik Wasi, Wahid Faisal, Abdur Rahman, Mahfuz Ahmed Anik, Munem Shahriar, Mohsin Mahmud Topu, Sadia Tasnim Meem, Rahatun Nesa Priti, Sabrina Afroz Mitu, Md. Iqramul Hoque, Shahriyar Zaman Ridoy, Mohammed Eunus Ali, Majd Hawasly, Mohammad Raza, Md Rizwan Parvez</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T17:52:02+00:00 · Latest: 2026-02-03T17:52:02+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. 92 Pages. 42 Figures and 29 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03916v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03916v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spatialab-reasoning.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs&#x27; spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth &amp; Occlusion, Orientation, Size &amp; Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs&#x27; spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatiaLab：视觉语言模型能否在真实场景中进行空间推理？</div>
<div class="mono" style="margin-top:8px">空间推理是人类认知的基本能力，但对当前视觉语言模型（VLMs）仍是重大挑战。先前研究多依赖合成或大语言模型生成的环境，其任务设计有限且呈谜题式结构，未能捕捉VLMs在真实场景中面临的复杂视觉噪声与多样空间关系。为此，我们推出SpatiaLab——一个在真实无约束场景中评估VLMs空间推理能力的综合基准。该基准涵盖六大类别（相对定位、深度与遮挡、方向、尺寸与比例、空间导航、三维几何），共包含1,400个视觉问答对，每类下设五个子类，形成30种任务类型。每个子类至少含25道题目，每个主类至少含200道题目，支持选择题与开放式评估。通过对开源/闭源模型、专注推理模型及专用空间推理模型等多类前沿VLM的实验，发现其空间推理能力与人类存在显著差距：选择题测试中InternVL3.5-72B准确率为54.93%，人类达87.57%；开放式测试中所有模型性能下降约10-25%，GPT-5-mini以40.93%居首，人类为64.93%。结果凸显了现有模型在处理复杂空间关系、深度感知、导航及三维几何方面的核心局限。SpatiaLab通过提供多样化的真实场景评估框架，揭示了推进VLM空间推理能力的关键挑战与机遇，为未来实现鲁棒且类人空间理解的研究提供基准。项目地址：https://spatialab-reasoning.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing benchmarks that rely on synthetic or LLM-generated environments and fail to capture real-world complexity, this paper introduces SpatiaLab, a comprehensive benchmark for evaluating vision-language models&#x27; spatial reasoning in realistic, unconstrained contexts. The method involves constructing a dataset of 1,400 visual question-answer pairs across six major categories and 30 distinct task types, supporting both multiple-choice and open-ended evaluation. Experimental results across diverse state-of-the-art models reveal a substantial performance gap compared to humans, with the top-performing model achieving 54.93% accuracy in multiple-choice and 40.93% in open-ended settings, versus 87.57% and 64.93% for humans, respectively, highlighting key limitations in handling complex spatial relationships.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉语言模型在真实、视觉复杂环境中进行空间推理的能力有限的问题，因为先前的基准测试依赖于合成或简化的谜题式设置。作者引入了SpatiaLab，这是一个包含1400个视觉问答对的综合基准，涵盖六个主要类别和30种不同的任务类型，旨在无约束的真实世界环境中评估视觉语言模型。在多种最先进的视觉语言模型上的实验结果表明，与人类相比存在显著的性能差距，在多项选择设置中表现最佳的模型准确率为54.93%，而人类为87.57%，并且在开放式评估中所有模型都表现出显著的性能下降，突显了模型在处理复杂空间关系、深度感知和三维几何方面的关键局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot large vision-language model prompting for automated bone identification in paleoradiology x-ray archives</div>
<div class="meta-line">Authors: Owen Dong, Lily Gao, Manish Kota, Bennett A. Landmana, Jelena Bekvalac, Gaynor Western, Katherine D. Van Schaik</div>
<div class="meta-line">First: 2026-02-03T17:14:23+00:00 · Latest: 2026-02-03T17:14:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03750v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03750v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Paleoradiology, the use of modern imaging technologies to study archaeological and anthropological remains, offers new windows on millennial scale patterns of human health. Unfortunately, the radiographs collected during field campaigns are heterogeneous: bones are disarticulated, positioning is ad hoc, and laterality markers are often absent. Additionally, factors such as age at death, age of bone, sex, and imaging equipment introduce high variability. Thus, content navigation, such as identifying a subset of images with a specific projection view, can be time consuming and difficult, making efficient triaging a bottleneck for expert analysis. We report a zero shot prompting strategy that leverages a state of the art Large Vision Language Model (LVLM) to automatically identify the main bone, projection view, and laterality in such images. Our pipeline converts raw DICOM files to bone windowed PNGs, submits them to the LVLM with a carefully engineered prompt, and receives structured JSON outputs, which are extracted and formatted onto a spreadsheet in preparation for validation. On a random sample of 100 images reviewed by an expert board certified paleoradiologist, the system achieved 92% main bone accuracy, 80% projection view accuracy, and 100% laterality accuracy, with low or medium confidence flags for ambiguous cases. These results suggest that LVLMs can substantially accelerate code word development for large paleoradiology datasets, allowing for efficient content navigation in future anthropology workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>零样本大型视觉语言模型提示在古放射学X射线档案中自动骨骼识别</div>
<div class="mono" style="margin-top:8px">古放射学利用现代成像技术研究考古和人类学遗骸，为千年尺度的人类健康模式提供了新视角。然而，野外考察中收集的X光片存在异质性：骨骼脱位、摆放随意、侧位标记常缺失，且死亡年龄、骨骼年龄、性别及成像设备等因素引入高变异性，使得内容导航（如识别特定投影视角的图像子集）耗时费力，成为专家分析的高效分诊瓶颈。本文提出一种零样本提示策略，利用先进的大型视觉语言模型（LVLM）自动识别图像中的主要骨骼、投影视角和侧位。流程将原始DICOM文件转换为骨骼窗PNG图像，通过精心设计的提示提交至LVLM，接收结构化JSON输出并提取整理至电子表格以备验证。经专家委员会认证的古放射学家对100张随机样本的评审显示，系统在主要骨骼识别准确率达92%、投影视角80%、侧位100%，对模糊案例标注低或中置信度标志。结果表明，LVLM能显著加速大型古放射学数据集的编码词开发，为未来人类学工作流程实现高效内容导航。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Paleoradiology X-ray archives are heterogeneous and lack standardized annotations, making manual content navigation and triage for expert analysis time-consuming and difficult. To address this, the authors propose a zero-shot prompting strategy that leverages a state-of-the-art Large Vision Language Model (LVLM); their pipeline converts raw DICOM files to bone-windowed PNGs, submits them to the LVLM with an engineered prompt, and extracts structured JSON outputs for validation. In an expert-reviewed test on 100 random images, the system achieved 92% accuracy for main bone identification, 80% for projection view, and 100% for laterality, demonstrating its potential to substantially accelerate codeword development and enable efficient content navigation in anthropological workflows.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决古放射学X射线档案中因骨骼脱位、临时定位和侧位标记缺失导致的图像异质性问题，以提升内容导航和专家分析效率。方法采用零样本提示策略，利用先进的大型视觉语言模型（LVLM），将原始DICOM文件转换为骨窗PNG图像，并通过精心设计的提示提取结构化JSON输出，以识别主要骨骼、投影视图和侧位。在专家评审的100张图像上的实验结果显示，主要骨骼准确率达92%，投影视图准确率为80%，侧位准确率为100%，并对模糊病例设置了低或中置信度标志，表明该方法能显著加速大型古放射学数据集的编码词开发，优化人类学工作流程。</div>
</details>
</div>
<div class="card">
<div class="title">Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment</div>
<div class="meta-line">Authors: Johny J. Lopez, Md Meftahul Ferdaus, Mahdi Abdelguerfi</div>
<div class="meta-line">First: 2026-02-03T17:03:46+00:00 · Latest: 2026-02-03T17:03:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03742v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03742v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous inspection of underground infrastructure, such as sewer and culvert systems, is critical to public safety and urban sustainability. Although robotic platforms equipped with visual sensors can efficiently detect structural deficiencies, the automated generation of human-readable summaries from these detections remains a significant challenge, especially on resource-constrained edge devices. This paper presents a novel two-stage pipeline for end-to-end summarization of underground deficiencies, combining our lightweight RAPID-SCAN segmentation model with a fine-tuned Vision-Language Model (VLM) deployed on an edge computing platform. The first stage employs RAPID-SCAN (Resource-Aware Pipeline Inspection and Defect Segmentation using Compact Adaptive Network), achieving 0.834 F1-score with only 0.64M parameters for efficient defect segmentation. The second stage utilizes a fine-tuned Phi-3.5 VLM that generates concise, domain-specific summaries in natural language from the segmentation outputs. We introduce a curated dataset of inspection images with manually verified descriptions for VLM fine-tuning and evaluation. To enable real-time performance, we employ post-training quantization with hardware-specific optimization, achieving significant reductions in model size and inference latency without compromising summarization quality. We deploy and evaluate our complete pipeline on a mobile robotic platform, demonstrating its effectiveness in real-world inspection scenarios. Our results show the potential of edge-deployable integrated AI systems to bridge the gap between automated defect detection and actionable insights for infrastructure maintenance, paving the way for more scalable and autonomous inspection solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向地下基础设施评估的边缘优化视觉语言模型</div>
<div class="mono" style="margin-top:8px">地下基础设施（如污水管和涵洞系统）的自主巡检对公共安全和城市可持续性至关重要。尽管搭载视觉传感器的机器人平台能高效检测结构缺陷，但如何从检测结果自动生成人类可读的总结报告仍是一大挑战，尤其在资源受限的边缘设备上。本文提出一种新颖的两阶段端到端地下缺陷总结流程：将轻量化RAPID-SCAN分割模型与部署在边缘计算平台的微调视觉语言模型（VLM）相结合。第一阶段采用RAPID-SCAN（基于紧凑自适应网络的资源感知管道巡检与缺陷分割模型），仅用0.64M参数即实现0.834的F1分数，高效完成缺陷分割。第二阶段使用微调的Phi-3.5 VLM，根据分割结果生成简洁的领域专用自然语言总结。我们构建了包含人工验证描述的巡检图像数据集用于VLM微调与评估。通过硬件专用优化的训练后量化技术，在保持总结质量的同时显著压缩模型规模并降低推理延迟，实现实时性能。我们在移动机器人平台上部署并评估完整流程，验证了其在真实巡检场景中的有效性。该成果表明，边缘可部署的集成人工智能系统能够弥合自动缺陷检测与基础设施维护可行洞察之间的鸿沟，为更具可扩展性和自主性的巡检解决方案奠定基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of generating human-readable summaries from visual defect detections in underground infrastructure inspection, which is critical for public safety but difficult to perform on resource-constrained edge devices. The authors propose a two-stage pipeline: first, a lightweight segmentation model called RAPID-SCAN performs efficient defect detection, achieving a 0.834 F1-score with only 0.64M parameters; second, a fine-tuned Phi-3.5 Vision-Language Model (VLM) generates concise, domain-specific natural language summaries from the segmentation outputs. Key experimental findings include the successful deployment of the quantized and hardware-optimized pipeline on a mobile robotic platform, demonstrating real-time performance with reduced model size and latency while maintaining summarization quality, thereby bridging automated detection and actionable maintenance insights.</div>
<div class="mono" style="margin-top:8px">本研究针对地下基础设施巡检中，如何在资源受限的边缘设备上从视觉缺陷检测自动生成可读摘要这一维护关键但具有挑战性的问题。作者提出一个两阶段流程：首先，使用轻量级分割模型（RAPID-SCAN）进行高效缺陷检测，该模型仅用0.64M参数即达到0.834的F1分数；其次，采用微调的Phi-3.5视觉语言模型（VLM）根据分割输出生成简洁的领域特定文本摘要。在移动机器人平台上的实验部署表明，训练后量化技术显著降低了模型大小和推理延迟，实现了实时性能且未损害摘要质量，有效连接了自动检测与可操作的维护洞察。</div>
</details>
</div>
<div class="card">
<div class="title">RegionReasoner: Region-Grounded Multi-Round Visual Reasoning</div>
<div class="meta-line">Authors: Wenfang Sun, Hao Chen, Yingjun Du, Yefeng Zheng, Cees G. M. Snoek</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T16:52:16+00:00 · Latest: 2026-02-03T16:52:16+00:00</div>
<div class="meta-line">Comments: Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03733v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models have achieved remarkable progress in visual reasoning, yet most existing systems rely on single-step or text-only reasoning, limiting their ability to iteratively refine understanding across multiple visual contexts. To address this limitation, we introduce a new multi-round visual reasoning benchmark with training and test sets spanning both detection and segmentation tasks, enabling systematic evaluation under iterative reasoning scenarios. We further propose RegionReasoner, a reinforcement learning framework that enforces grounded reasoning by requiring each reasoning trace to explicitly cite the corresponding reference bounding boxes, while maintaining semantic coherence via a global-local consistency reward. This reward extracts key objects and nouns from both global scene captions and region-level captions, aligning them with the reasoning trace to ensure consistency across reasoning steps. RegionReasoner is optimized with structured rewards combining grounding fidelity and global-local semantic alignment. Experiments on detection and segmentation tasks show that RegionReasoner-7B, together with our newly introduced benchmark RegionDial-Bench, considerably improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency, establishing a strong baseline for this emerging research direction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RegionReasoner：基于区域的多轮视觉推理框架</div>
<div class="mono" style="margin-top:8px">大规模视觉语言模型在视觉推理领域取得显著进展，但现有系统多依赖单步或纯文本推理，难以通过多轮视觉上下文迭代优化理解。为此，我们构建了涵盖检测与分割任务的训练与测试集，形成系统性多轮视觉推理评测基准。进一步提出RegionReasoner强化学习框架，通过强制要求每个推理轨迹显式引用对应参考边界框实现具身推理，同时采用全局-局部一致性奖励机制保持语义连贯性。该机制从全局场景描述与区域级描述中提取关键对象及名词，并与推理轨迹对齐以确保跨步骤一致性。RegionReasoner通过融合定位保真度与全局-局部语义对齐的结构化奖励进行优化。在检测与分割任务上的实验表明，RegionReasoner-7B模型结合我们新构建的RegionDial-Bench基准，显著提升了多轮推理准确率、空间定位精度及全局-局部一致性，为该新兴研究方向建立了坚实基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitation of existing vision-language models that rely on single-step or text-only reasoning and lack iterative refinement across visual contexts, this work introduces a new multi-round visual reasoning benchmark and proposes RegionReasoner, a reinforcement learning framework. The method enforces grounded reasoning by requiring each reasoning trace to explicitly cite reference bounding boxes and maintains semantic coherence via a global-local consistency reward, which aligns key objects and nouns from global and region-level captions with the reasoning trace. Experiments on detection and segmentation tasks demonstrate that RegionReasoner-7B, evaluated with the new RegionDial-Bench, significantly improves multi-round reasoning accuracy, spatial grounding precision, and global-local consistency.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于现有大型视觉语言模型大多依赖单步或纯文本推理，缺乏在多轮视觉语境中迭代优化理解的能力。为此，作者提出了一个新的多轮视觉推理基准，并设计了RegionReasoner框架，该框架采用强化学习，要求每个推理步骤显式引用对应的参考边界框以实现接地推理，同时通过全局-局部一致性奖励来保持语义连贯性，该奖励从全局场景描述和区域描述中提取关键物体和名词，并与推理轨迹对齐以确保跨步骤的一致性。在检测和分割任务上的实验表明，7B参数的RegionReasoner模型结合新引入的RegionDial-Bench基准，在多轮推理准确性、空间接地精度和全局-局部一致性方面均取得了显著提升。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260206_0345.html">20260206_0345</a>
<a href="archive/20260205_0628.html">20260205_0628</a>
<a href="archive/20260205_0537.html">20260205_0537</a>
<a href="archive/20260205_0450.html">20260205_0450</a>
<a href="archive/20260205_0346.html">20260205_0346</a>
<a href="archive/20260204_0633.html">20260204_0633</a>
<a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
