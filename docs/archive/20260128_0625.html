<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 06:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0625</div>
    <div class="row"><div class="card">
<div class="title">DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception</div>
<div class="meta-line">Authors: Tim Broedermannn, Christos Sakaridis, Luigi Piccinelli, Wim Abbeloos, Luc Van Gool</div>
<div class="meta-line">First: 2025-09-11T20:03:00+00:00 · Latest: 2026-01-26T18:33:05+00:00</div>
<div class="meta-line">Comments: Code and models are available at https://github.com/timbroed/DGFusion</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09828v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.09828v3">PDF</a> · <a href="https://github.com/timbroed/DGFusion">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model&#x27;s inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models are available at https://github.com/timbroed/DGFusion</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DGFusion：基于深度引导的传感器融合实现鲁棒语义感知</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆的鲁棒语义感知依赖于有效结合具有互补优缺点的多传感器。现有先进的语义感知传感器融合方法通常在输入空间范围内统一处理传感器数据，这在面对挑战性条件时会限制性能。相比之下，我们提出了一种新颖的深度引导多模态融合方法，通过整合深度信息升级了条件感知融合。我们的网络DGFusion将多模态分割构建为多任务问题，利用通常存在于户外传感器套件中的激光雷达测量数据，既作为模型输入之一，也作为学习深度的真实标签。我们设计的辅助深度头有助于学习深度感知特征，这些特征被编码为空间变化的局部深度标记，用于调节注意力跨模态融合。结合全局条件标记，这些局部深度标记能根据场景中随空间变化的传感器可靠性（主要取决于深度）动态调整传感器融合。此外，我们为深度学习提出了鲁棒损失函数，这对于从恶劣条件下通常稀疏且含噪的激光雷达输入中学习至关重要。我们的方法在具有挑战性的MUSES和DeLiVER数据集上实现了最先进的全景与语义分割性能。代码与模型发布于https://github.com/timbroed/DGFusion</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enhance the robustness of semantic perception for autonomous vehicles under challenging conditions, this research introduces DGFusion, a depth-guided multimodal fusion network. The method treats multimodal segmentation as a multi-task problem, using LiDAR data both as an input and as ground truth for an auxiliary depth prediction head. This head learns depth-aware features, which are encoded into spatially varying local depth tokens and a global condition token to dynamically adapt cross-modal fusion based on the depth-dependent reliability of each sensor. A robust loss is also designed to handle sparse and noisy LiDAR data. The approach achieves state-of-the-art performance in panoptic and semantic segmentation on the MUSES and DeLiVER datasets.</div>
<div class="mono" style="margin-top:8px">为提升自动驾驶系统在复杂环境下的鲁棒语义感知能力，本研究提出了DGFusion，一种深度引导的多模态融合网络。该方法将多模态分割视为多任务学习问题，利用激光雷达数据既作为模型输入，也作为学习深度的监督信号。通过一个辅助的深度预测头学习深度感知特征，并将其编码为空间变化的局部深度令牌与全局条件令牌，从而根据依赖于深度的传感器可靠性动态调整跨模态融合。同时，提出了一种鲁棒的损失函数以处理稀疏且有噪声的激光雷达数据。该方法在MUSES和DeLiVER数据集上实现了最先进的全景与语义分割性能。</div>
</details>
</div>
<div class="card">
<div class="title">Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery</div>
<div class="meta-line">Authors: Shutong Chen, Adnan Aijaz, Yansha Deng</div>
<div class="meta-line">First: 2026-01-26T18:32:33+00:00 · Latest: 2026-01-26T18:32:33+00:00</div>
<div class="meta-line">Comments: Submit to IEEE for potential publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18765v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向目标的通信技术实现快速鲁棒的机器人故障检测与恢复</div>
<div class="mono" style="margin-top:8px">自主机器人系统广泛应用于智能工厂，其运行环境动态、不确定且需人机协作，要求具备低延迟、鲁棒的故障检测与恢复能力。现有框架存在通信计算延迟大、运动轨迹生成不可靠等局限，主因是通信-计算-控制回路设计未考虑下游故障检测与恢复目标。为此，我们提出面向目标的通信框架，通过联合设计适配快速鲁棒故障检测与恢复的3C回路，以最小化故障处理时间、最大化任务成功率。在故障检测方面，该框架创新性地通过表征提取器定义并提取三维场景图作为语义表征，通过监测三维场景图的空间关系变化实现故障检测。在故障恢复方面，我们通过低秩自适应微调小型语言模型，并借助知识蒸馏增强其推理泛化能力以生成恢复动作。同时设计了轻量化目标导向数字孪生重构模块，在需要精细控制时仅使用任务相关物体轮廓重构数字孪生，优化小型语言模型生成的恢复动作。大量仿真表明：相较于依赖视觉语言模型进行故障检测、大型语言模型进行故障恢复的先进框架，本框架将故障处理时间降低达82.6%，任务成功率提升达76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing fault detection and recovery (FDR) frameworks in autonomous robots, which suffer from communication delays and unreliable motion generation due to a disjointed communication-computation-control (3C) loop, this work proposes a Goal-oriented Communication (GoC) framework. The method jointly designs the 3C loop for FDR by using a 3D scene graph (3D-SG) for semantic fault detection via spatial relationship monitoring and employing a fine-tuned small language model (SLM) enhanced with knowledge distillation for recovery motion generation, supplemented by a lightweight digital twin for motion refinement. Experimental results from extensive simulations show the framework reduces FDR time by up to 82.6% and increases task success rate by up to 76% compared to state-of-the-art vision/language model-based approaches.</div>
<div class="mono" style="margin-top:8px">为使在动态环境中运行的自主机器人能够实现快速、鲁棒的故障检测与恢复（FDR），本研究针对现有框架存在的通信延迟和运动生成不可靠等局限性，提出了一个目标导向通信（GoC）框架，对通信-计算-控制环路进行协同设计。该方法创新性地使用3D场景图通过监测空间关系变化进行语义故障检测，并采用经知识蒸馏增强的微调小型语言模型来生成恢复动作，辅以轻量级数字孪生进行动作细化。大量仿真实验结果表明，与依赖视觉语言模型进行故障检测和大语言模型进行故障恢复的先进框架相比，该框架将FDR时间降低了高达82.6%，并将任务成功率提高了高达76%。</div>
</details>
</div>
<div class="card">
<div class="title">Goal-oriented Semantic Communication for Robot Arm Reconstruction in Digital Twin: Feature and Temporal Selections</div>
<div class="meta-line">Authors: Shutong Chen, Emmanouil Spyrakos-Papastavridis, Yichao Jin, Yansha Deng</div>
<div class="meta-line">First: 2024-11-13T18:14:23+00:00 · Latest: 2026-01-26T18:16:49+00:00</div>
<div class="meta-line">Comments: Accepted by IEEE Journal on Selected Areas in Communications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.08835v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.08835v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As one of the most promising technologies in industry, the Digital Twin (DT) facilitates real-time monitoring and predictive analysis for real-world systems by precisely reconstructing virtual replicas of physical entities. However, this reconstruction faces unprecedented challenges due to the everincreasing communication overhead, especially for digital robot arm reconstruction. To this end, we propose a novel goal-oriented semantic communication (GSC) framework to extract the GSC information for the robot arm reconstruction task in the DT, with the aim of minimising the communication load under the strict and relaxed reconstruction error constraints. Unlike the traditional reconstruction framework that periodically transmits a reconstruction message for real-time DT reconstruction, our framework implements a feature selection (FS) algorithm to extract the semantic information from the reconstruction message, and a deep reinforcement learning-based temporal selection algorithm to selectively transmit the semantic information over time. We validate our proposed GSC framework through both Pybullet simulations and lab experiments based on the Franka Research 3 robot arm. For a range of distinct robotic tasks, simulation results show that our framework can reduce the communication load by at least 59.5% under strict reconstruction error constraints and 80% under relaxed reconstruction error constraints, compared with traditional communication framework. Also, experimental results confirm the effectiveness of our framework, where the communication load is reduced by 53% in strict constraint case and 74% in relaxed constraint case. The demo is available at: https://youtu.be/2OdeHKxcgnk.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向数字孪生中机械臂重构的目标导向语义通信：特征与时序选择</div>
<div class="mono" style="margin-top:8px">作为工业领域最具前景的技术之一，数字孪生通过精确重构物理实体的虚拟副本，实现对现实系统的实时监测与预测分析。然而，由于通信开销的持续增长，尤其是针对数字机械臂重构任务，该过程面临前所未有的挑战。为此，本文提出一种新颖的目标导向语义通信框架，旨在为数字孪生中的机械臂重构任务提取关键语义信息，以在严格与宽松的重构误差约束下最小化通信负载。与传统周期性传输重构消息以实现实时数字孪生重构的框架不同，本框架采用特征选择算法从重构消息中提取语义信息，并基于深度强化学习的时序选择算法动态控制语义信息的传输时机。通过基于Franka Research 3机械臂的Pybullet仿真与实验室实验验证表明：在多种机器人任务场景下，相较于传统通信框架，本框架可在严格重构误差约束下降低至少59.5%的通信负载，在宽松约束下降低80%；实验数据进一步证实其有效性，在严格与宽松约束下分别实现53%和74%的通信负载降低。演示视频详见：https://youtu.be/2OdeHKxcgnk。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of high communication overhead in reconstructing digital twins of robot arms by proposing a goal-oriented semantic communication framework. The method combines a feature selection algorithm to extract semantic information from reconstruction messages and a deep reinforcement learning-based temporal selection algorithm to control the transmission timing. Experimental results from PyBullet simulations and lab tests with a Franka Research 3 robot arm demonstrate that the framework reduces communication load by at least 59.5% under strict reconstruction error constraints and up to 80% under relaxed constraints compared to traditional methods.</div>
<div class="mono" style="margin-top:8px">本研究针对数字孪生系统中机器人手臂实时重建面临的高通信开销挑战，提出了一种面向目标的语义通信框架。该框架通过特征选择算法从重建消息中提取语义信息，并采用基于深度强化学习的时间选择算法来决定信息的传输时机，从而最小化通信负载。基于Pybullet仿真和Franka Research 3机器人手臂的实验验证表明，与传统周期性传输方法相比，该框架在严格重建误差约束下可减少至少59.5%的通信负载，在宽松约束下可减少80%；实验室实验进一步证实了在相应约束下通信负载分别降低了53%和74%。</div>
</details>
</div>
<div class="card">
<div class="title">Actor-Critic Cooperative Compensation to Model Predictive Control for Off-Road Autonomous Vehicles Under Unknown Dynamics</div>
<div class="meta-line">Authors: Prakhar Gupta, Jonathon M Smereka, Yunyi Jia</div>
<div class="meta-line">Venue: ICRA 2025</div>
<div class="meta-line">First: 2025-03-01T17:59:55+00:00 · Latest: 2026-01-26T17:57:52+00:00</div>
<div class="meta-line">Comments: 7 pages, Accepted at 2025 IEEE ICRA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.00577v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.00577v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents an Actor-Critic Cooperative Compensated Model Predictive Controller (AC3MPC) designed to address unknown system dynamics. To avoid the difficulty of modeling highly complex dynamics and ensuring realtime control feasibility and performance, this work uses deep reinforcement learning with a model predictive controller in a cooperative framework to handle unknown dynamics. The model-based controller takes on the primary role as both controllers are provided with predictive information about the other. This improves tracking performance and retention of inherent robustness of the model predictive controller. We evaluate this framework for off-road autonomous driving on unknown deformable terrains that represent sandy deformable soil, sandy and rocky soil, and cohesive clay-like deformable soil. Our findings demonstrate that our controller statistically outperforms standalone model-based and learning-based controllers by upto 29.2% and 10.2%. This framework generalized well over varied and previously unseen terrain characteristics to track longitudinal reference speeds with lower errors. Furthermore, this required significantly less training data compared to purely learning-based controller, while delivering better performance even when under-trained.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向未知动力学下越野自动驾驶车辆的演员-评论家协同补偿模型预测控制</div>
<div class="mono" style="margin-top:8px">本研究提出一种演员-评论家协同补偿模型预测控制器（AC3MPC），用于处理未知系统动力学。为规避高度复杂动力学建模的困难并确保实时控制可行性及性能，该工作将深度强化学习与模型预测控制器置于协同框架中以处理未知动力学。基于模型的控制器承担主导角色，因两种控制器均能获取对方的预测信息。这提升了轨迹跟踪性能并保持了模型预测控制器固有的鲁棒性。我们在代表沙质可变形土壤、沙石混合土壤及黏性类黏土可变形土壤的未知可变形地形上评估该越野自动驾驶框架。实验表明，本控制器在统计意义上分别优于独立模型控制器与学习型控制器达29.2%和10.2%。该框架能良好泛化至多样且未经训练的 terrain 特征，以更低误差跟踪纵向参考速度。此外，相较于纯学习型控制器，本方法所需训练数据显著减少，即使在训练不足时仍能提供更优性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of controlling off-road autonomous vehicles on unknown deformable terrains where accurate system dynamics modeling is difficult. The proposed Actor-Critic Cooperative Compensated Model Predictive Controller (AC3MPC) integrates deep reinforcement learning with model predictive control in a cooperative framework, where both controllers exchange predictive information to handle unknown dynamics while preserving the robustness of the model-based approach. Experimental evaluation on various deformable terrains including sandy soil, rocky soil, and cohesive clay showed that AC3MPC statistically outperformed standalone model-based and learning-based controllers by up to 29.2% and 10.2% respectively, achieved better generalization to unseen terrain characteristics with lower tracking errors, and required significantly less training data than purely learning-based approaches while maintaining performance even when under-trained.</div>
<div class="mono" style="margin-top:8px">本研究针对越野自动驾驶车辆在未知可变形地形动力学下的控制难题，旨在解决精确建模困难与实时性能要求之间的冲突。提出的演员-评论家协同补偿模型预测控制器（AC3MPC）将深度强化学习与模型预测控制集成于协同框架中，通过控制器间的预测信息交换处理未知动力学，同时保持模型方法的鲁棒性。在多种可变形地形（沙土、沙石混合土、粘性类粘土）上的实验表明，AC3MPC在统计上显著优于独立的模型控制器和学习控制器，性能提升分别达29.2%和10.2%，在未见地形上实现了更低误差的纵向速度跟踪，且所需训练数据显著减少，即使在训练不足时仍能保持良好性能。</div>
</details>
</div>
<div class="card">
<div class="title">Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge</div>
<div class="meta-line">Authors: Li Kang, Heng Zhou, Xiufeng Song, Rui Li, Bruno N. Y. Chen, Ziye Wang, Ximeng Meng, Stone Tao, Yiran Qin, Xiaohong Liu, Ruimao Zhang, Lei Bai, Yilun Du, Hao Su, Philip Torr, Zhenfei Yin, Ruihao Gong, Yejun Zeng, Fengjun Zhong, Shenghao Jin, Jinyang Guo, Xianglong Liu, Xiaojun Jia, Tianqi Shan, Wenqi Ren, Simeng Qin, Jialing Yang, Xiaoyu Ma, Tianxing Chen, Zixuan Li, Zijian Cai, Yan Qin, Yusen Qin, Qiangyu Chen, Kaixuan Wang, Zhaoming Han, Yao Mu, Ping Luo, Yuanqi Yao, Haoming Song, Jan-Nico Zaech, Fabien Despinoy, Danda Pani Paudel, Luc Van Gool</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2026-01-26T17:56:19+00:00 · Latest: 2026-01-26T17:56:19+00:00</div>
<div class="meta-line">Comments: MARS Challenge @ NeurIPS 2025 Workshop on Space in Vision, Language, and Embodied AI. Challenge page: https://mars-eai.github.io/MARS-Challenge-Webpage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18733v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18733v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mars-eai.github.io/MARS-Challenge-Webpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enhancing system efficiency through task delegation, and enabling advanced human-agent interactions. To address the challenges posed by multi-agent collaboration, we propose the Multi-Agent Robotic System (MARS) Challenge, held at the NeurIPS 2025 Workshop on SpaVLE. The competition focuses on two critical areas: planning and control, where participants explore multi-agent embodied planning using vision-language models (VLMs) to coordinate tasks and policy execution to perform robotic manipulation in dynamic environments. By evaluating solutions submitted by participants, the challenge provides valuable insights into the design and coordination of embodied multi-agent systems, contributing to the future development of advanced collaborative AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体机器人系统（MARS）挑战赛的进展与创新</div>
<div class="mono" style="margin-top:8px">多模态大语言模型与视觉-语言-动作模型的最新进展显著推动了具身人工智能领域的发展。随着该领域向更复杂的任务场景过渡，多智能体系统框架正成为实现可扩展、高效协作解决方案的关键。这一转变主要受三大因素驱动：智能体能力持续增强、通过任务委派提升系统效率，以及实现更先进的人机交互。为应对多智能体协作带来的挑战，我们在NeurIPS 2025 SpaVLE研讨会上提出举办多智能体机器人系统（MARS）挑战赛。竞赛聚焦两大关键领域：规划与控制——参赛者将探索利用视觉语言模型进行多智能体具身规划以协调任务，并通过策略执行在动态环境中完成机器人操控。通过评估参赛方案，本挑战赛为具身多智能体系统的设计与协调提供宝贵洞见，助力未来高级协作式人工智能系统的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this work stems from the need to address complex task scenarios in Embodied AI, where multi-agent systems are essential for scalable and collaborative solutions. The method involves organizing the Multi-Agent Robotic System (MARS) Challenge, which focuses on multi-agent embodied planning using vision-language models and policy execution for robotic manipulation in dynamic environments. Key experimental findings from evaluating participant submissions provide insights into the design and coordination of embodied multi-agent systems, advancing the development of collaborative AI.</div>
<div class="mono" style="margin-top:8px">该研究的动机是应对具身人工智能中复杂任务场景的需求，其中多智能体系统对于可扩展和协作的解决方案至关重要。方法包括组织多智能体机器人系统（MARS）挑战赛，重点关注使用视觉语言模型进行动态环境中的任务协调和机器人操作的规划与控制。通过评估参赛者提交的方案，关键实验结果为具身多智能体系统的设计和协调提供了见解，推动了协作人工智能的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Trustworthy Evaluation of Robotic Manipulation: A New Benchmark and AutoEval Methods</div>
<div class="meta-line">Authors: Mengyuan Liu, Juyi Sheng, Peiming Li, Ziyi Wang, Tianming Xu, Tiantian Xu, Hong Liu</div>
<div class="meta-line">First: 2026-01-26T17:47:42+00:00 · Latest: 2026-01-26T17:47:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18723v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18723v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://term-bench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Driven by the rapid evolution of Vision-Action and Vision-Language-Action models, imitation learning has significantly advanced robotic manipulation capabilities. However, evaluation methodologies have lagged behind, hindering the establishment of Trustworthy Evaluation for these behaviors. Current paradigms rely on binary success rates, failing to address the critical dimensions of trust: Source Authenticity (i.e., distinguishing genuine policy behaviors from human teleoperation) and Execution Quality (e.g., smoothness and safety). To bridge these gaps, we propose a solution that combines the Eval-Actions benchmark and the AutoEval architecture. First, we construct the Eval-Actions benchmark to support trustworthiness analysis. Distinct from existing datasets restricted to successful human demonstrations, Eval-Actions integrates VA and VLA policy execution trajectories alongside human teleoperation data, explicitly including failure scenarios. This dataset is structured around three core supervision signals: Expert Grading (EG), Rank-Guided preferences (RG), and Chain-of-Thought (CoT). Building on this, we propose the AutoEval architecture: AutoEval leverages Spatio-Temporal Aggregation for semantic assessment, augmented by an auxiliary Kinematic Calibration Signal to refine motion smoothness; AutoEval Plus (AutoEval-P) incorporates the Group Relative Policy Optimization (GRPO) paradigm to enhance logical reasoning capabilities. Experiments show AutoEval achieves Spearman&#x27;s Rank Correlation Coefficients (SRCC) of 0.81 and 0.84 under the EG and RG protocols, respectively. Crucially, the framework possesses robust source discrimination capabilities, distinguishing between policy-generated and teleoperated videos with 99.6% accuracy, thereby establishing a rigorous standard for trustworthy robotic evaluation. Our project and code are available at https://term-bench.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人操作的可信评估：新基准与自动评估方法</div>
<div class="mono" style="margin-top:8px">在视觉-动作与视觉-语言-动作模型快速发展的推动下，模仿学习显著提升了机器人操作能力。然而，评估方法的发展相对滞后，阻碍了这些行为的可信评估体系的建立。当前范式依赖二元成功率，未能涵盖可信度的关键维度：来源真实性（即区分真实策略行为与人类遥操作）和执行质量（如流畅性与安全性）。为弥补这些不足，我们提出了结合Eval-Actions基准与AutoEval架构的解决方案。首先，我们构建了支持可信度分析的Eval-Actions基准。区别于现有仅包含成功人类演示的数据集，Eval-Actions整合了VA与VLA策略执行轨迹及人类遥操作数据，明确包含失败场景。该数据集围绕三个核心监督信号构建：专家评分（EG）、排序引导偏好（RG）和思维链（CoT）。在此基础上，我们提出AutoEval架构：AutoEval利用时空聚合进行语义评估，并通过辅助运动学校准信号优化动作流畅度；AutoEval Plus（AutoEval-P）引入群体相对策略优化（GRPO）范式以增强逻辑推理能力。实验表明，AutoEval在EG和RG协议下的斯皮尔曼等级相关系数（SRCC）分别达到0.81和0.84。关键的是，该框架具备强大的来源判别能力，能以99.6%的准确率区分策略生成视频与遥操作视频，从而为机器人可信评估建立了严格标准。项目与代码详见https://term-bench.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid advancement of imitation learning for robotic manipulation has outpaced evaluation methods, which currently rely on simplistic binary success rates and fail to assess critical trust dimensions like Source Authenticity and Execution Quality. To address this, the authors introduce the Eval-Actions benchmark, which integrates policy execution trajectories and human teleoperation data including failures, and the AutoEval architecture, which uses Spatio-Temporal Aggregation and a Kinematic Calibration Signal for assessment, with an enhanced version (AutoEval-P) employing Group Relative Policy Optimization for better reasoning. Experimental results demonstrate that AutoEval achieves Spearman&#x27;s Rank Correlation Coefficients of 0.81 and 0.84 under Expert Grading and Rank-Guided preference protocols, and can distinguish between policy-generated and teleoperated videos with 99.6% accuracy, establishing a rigorous standard for trustworthy evaluation.</div>
<div class="mono" style="margin-top:8px">机器人模仿学习的快速发展使得现有评估方法相对滞后，这些方法依赖简单的二元成功率，无法评估来源真实性和执行质量等关键信任维度。为此，研究者提出了Eval-Actions基准，该基准整合了策略执行轨迹和包含失败案例的人类遥操作数据，并以专家评分、排序引导偏好和思维链作为核心监督信号。同时，他们设计了AutoEval架构，利用时空聚合和运动学校准信号进行评估，并进一步提出了融入群体相对策略优化的AutoEval-P版本以增强逻辑推理能力。实验表明，AutoEval在专家评分和排序引导协议下的斯皮尔曼等级相关系数分别达到0.81和0.84，并能以99.6%的准确率区分策略生成与遥操作视频，从而为可信的机器人评估建立了严格标准。</div>
</details>
</div>
<div class="card">
<div class="title">A Computationally Efficient Maximum A Posteriori Sequence Estimation via Stein Variational Inference</div>
<div class="meta-line">Authors: Min-Won Seo, Solmaz S. Kia</div>
<div class="meta-line">First: 2023-12-14T06:46:35+00:00 · Latest: 2026-01-26T17:43:08+00:00</div>
<div class="meta-line">Comments: 20 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.08684v4">Abs</a> · <a href="https://arxiv.org/pdf/2312.08684v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State estimation in robotic systems presents significant challenges, particularly due to the prevalence of multimodal posterior distributions in real-world scenarios. One effective strategy for handling such complexity is to compute maximum a posteriori (MAP) sequences over a discretized or sampled state space, which enables a concise representation of the most likely state trajectory. However, this approach often incurs substantial computational costs, especially in high-dimensional settings. In this article, we propose a novel MAP sequence estimation method, Stein-MAP-Seq, which effectively addresses multimodality while substantially reducing computational and memory overhead. Our key contribution is a sequential variational inference framework that captures temporal dependencies in dynamical system models and integrates Stein variational gradient descent (SVGD) into a Viterbi-style dynamic programming algorithm, enabling computationally efficient MAP sequence estimation. This integration allows the method to focus computational effort on MAP-consistent modes rather than exhaustively exploring the entire state space. Stein-MAP-Seq inherits the parallelism and mode-seeking behavior of SVGD, allowing particle updates to be efficiently executed on parallel hardware and significantly reducing the number of trajectory candidates required for MAP-sequence recursion compared to conventional methods that rely on hundreds to thousands of particles. We validate the proposed approach on a range of highly multimodal scenarios, including nonlinear dynamics with ambiguous observations, unknown data association with outliers, range-only localization under temporary unobservability, and high-dimensional robotic manipulators. Experimental results demonstrate substantial improvements in estimation accuracy and robustness to multimodality over existing estimation methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于斯坦因变分推断的计算高效最大后验序列估计方法</div>
<div class="mono" style="margin-top:8px">机器人系统中的状态估计面临重大挑战，主要源于实际场景中普遍存在的多峰后验分布。处理此类复杂性的有效策略是在离散化或采样的状态空间上计算最大后验（MAP）序列，从而实现对最可能状态轨迹的简洁表示。然而，该方法通常会产生高昂的计算成本，尤其在高维场景中。本文提出一种新颖的MAP序列估计方法——Stein-MAP-Seq，在有效处理多峰性的同时显著降低计算与内存开销。我们的核心贡献是提出一种序列变分推断框架，该框架能够捕捉动态系统模型中的时间依赖性，并将斯坦因变分梯度下降（SVGD）集成至维特比式动态规划算法中，从而实现计算高效的MAP序列估计。这种集成使方法能够将计算资源集中于MAP一致性模态，而非穷举探索整个状态空间。Stein-MAP-Seq继承了SVGD的并行化特性与模态搜索能力，支持在并行硬件上高效执行粒子更新，相较于依赖数百至数千粒子的传统方法，显著减少了MAP序列递归所需的轨迹候选数量。我们在多种高度多峰场景中验证了所提方法，包括含模糊观测的非线性动力学、存在异常值的未知数据关联、暂时不可观测条件下的纯距离定位以及高维机器人操纵器。实验结果表明，该方法在估计精度和对多峰性的鲁棒性方面较现有估计方法均有显著提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the computational challenges of maximum a posteriori (MAP) sequence estimation in robotic state estimation, where multimodal posteriors are common. The proposed Stein-MAP-Seq method integrates Stein variational gradient descent (SVGD) into a Viterbi-style dynamic programming algorithm within a sequential variational inference framework, focusing computational effort on MAP-consistent modes rather than exhaustive state-space exploration. Experiments on nonlinear dynamics, data association with outliers, range-only localization, and high-dimensional manipulators show the method significantly improves estimation accuracy and robustness to multimodality while reducing computational and memory overhead compared to conventional particle-based approaches.</div>
<div class="mono" style="margin-top:8px">机器人系统中的状态估计常面临多模态后验分布的挑战，使得最大后验序列估计在计算上代价高昂，尤其在高维场景中。为此，本文提出Stein-MAP-Seq方法，将Stein变分梯度下降集成到维特比式动态规划算法中，构建序列变分推断框架，从而将计算资源集中于与最大后验一致的模态，而非遍历整个状态空间。在非线性动态、含异常值的数据关联、仅测距定位及高维机械臂等多模态场景上的实验表明，该方法相比传统基于大量粒子的方法，显著提升了估计精度和对多模态的鲁棒性，同时降低了计算与内存开销。</div>
</details>
</div>
<div class="card">
<div class="title">Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning</div>
<div class="meta-line">Authors: Judith Vilella-Cantos, Mauro Martini, Marcello Chiaberge, Mónica Ballesta, David Valiente</div>
<div class="meta-line">First: 2026-01-26T17:38:56+00:00 · Latest: 2026-01-26T17:38:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Localization in agricultural environments is challenging due to their unstructured nature and lack of distinctive landmarks. Although agricultural settings have been studied in the context of object classification and segmentation, the place recognition task for mobile robots is not trivial in the current state of the art. In this study, we propose MinkUNeXt-VINE, a lightweight, deep-learning-based method that surpasses state-of-the-art methods in vineyard environments thanks to its pre-processing and Matryoshka Representation Learning multi-loss approach. Our method prioritizes enhanced performance with low-cost, sparse LiDAR inputs and lower-dimensionality outputs to ensure high efficiency in real-time scenarios. Additionally, we present a comprehensive ablation study of the results on various evaluation cases and two extensive long-term vineyard datasets employing different LiDAR sensors. The results demonstrate the efficiency of the trade-off output produced by this approach, as well as its robust performance on low-cost and low-resolution input data. The code is publicly available for reproduction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>低成本、高效率：基于套娃表示学习的葡萄园激光雷达地点识别</div>
<div class="mono" style="margin-top:8px">农业环境因其非结构化特性及缺乏显著地标，使得定位任务极具挑战。尽管现有研究已在农业场景的对象分类与分割方面有所探索，但移动机器人的地点识别任务在当前技术前沿仍非易事。本研究提出MinkUNeXt-VINE——一种轻量级深度学习方法，通过其预处理流程与套娃表示学习的多损失策略，在葡萄园环境中超越了现有最优方法。该方法优先采用低成本稀疏激光雷达输入与低维输出，以保障实时场景下的高效性能，并通过对多种评估案例及两个采用不同激光雷达传感器的长期葡萄园数据集进行系统消融实验。结果表明，该方法在输出效率权衡方面表现优异，且在低成本、低分辨率输入数据上具有鲁棒性能。相关代码已公开以供复现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of mobile robot localization in unstructured agricultural environments like vineyards, which lack distinctive landmarks and make place recognition difficult. The authors propose MinkUNeXt-VINE, a lightweight deep learning method that employs specialized pre-processing and a Matryoshka Representation Learning multi-loss approach to achieve efficient place recognition from sparse, low-cost LiDAR data. Experimental results on two long-term vineyard datasets with different LiDAR sensors show that the method surpasses state-of-the-art performance, demonstrating robust localization with low-resolution inputs and an efficient trade-off in output dimensionality for real-time operation.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决移动机器人在葡萄园等缺乏显著地标、非结构化的农业环境中的定位难题。作者提出了MinkUNeXt-VINE，一种轻量级的深度学习方法，该方法采用特定的预处理和套娃表征学习多损失策略，以实现从低成本、稀疏激光雷达数据中进行高效地点识别。在两个使用不同激光雷达传感器的长期葡萄园数据集上的实验结果表明，该方法超越了现有最佳性能，在低分辨率输入下展现出鲁棒的定位能力，并通过输出维度的有效权衡确保了实时运行的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models</div>
<div class="meta-line">Authors: Christian Reichenbächer, Philipp Rank, Jochen Hipp, Oliver Bringmann</div>
<div class="meta-line">First: 2025-06-11T18:30:20+00:00 · Latest: 2026-01-26T17:24:02+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures; This work has been submitted to the IEEE for possible publication; Code available at: https://codeocean.com/capsule/1003615/tree</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10098v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.10098v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the first application of Gaussian Mixture Copula Models to the statistical modeling of driving scenarios for the safety validation of automated driving systems. Knowledge of the joint probability distribution of scenario parameters is essential for scenario-based safety assessment, where risk quantification depends on the likelihood of concrete parameter combinations. Gaussian Mixture Copula Models bring together the multimodal expressivity of Gaussian Mixture Models and the flexibility of copulas, enabling separate modeling of marginal distributions and dependence. We benchmark Gaussian Mixture Copula Models against previously proposed approaches - Gaussian Mixture Models and Gaussian Copula Models - using real-world driving data drawn from two scenarios defined in United Nations Regulation No. 157. Our evaluation on approximately 18 million instances of these two scenarios demonstrates that Gaussian Mixture Copula Models consistently surpass Gaussian Copula Models and perform competitively with Gaussian Mixture Models, as measured by both log-likelihood and Sinkhorn distance, with relative performance depending on the scenario. The results are promising for the adoption of Gaussian Mixture Copula Models as a statistical foundation for future scenario-based validation frameworks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于高斯混合Copula模型的场景参数联合概率估计</div>
<div class="mono" style="margin-top:8px">本文首次将高斯混合Copula模型应用于自动驾驶系统安全验证的驾驶场景统计建模。场景参数联合概率分布的知识对于基于场景的安全评估至关重要，其中风险量化取决于具体参数组合的可能性。高斯混合Copula模型融合了高斯混合模型的多模态表达能力与Copula的灵活性，实现了边缘分布与相关性的分离建模。我们使用联合国第157号法规定义的两个场景的真实驾驶数据，将高斯混合Copula模型与先前提出的高斯混合模型和高斯Copula模型进行基准比较。基于约1800万个场景实例的评估表明，在对数似然和Sinkhorn距离指标下，高斯混合Copula模型始终优于高斯Copula模型，并与高斯混合模型性能相当，相对表现因场景而异。该结果为采用高斯混合Copula模型作为未来场景化验证框架的统计基础提供了前景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for accurate joint probability distributions of driving scenario parameters to enable rigorous, scenario-based safety validation of automated driving systems. The proposed method employs Gaussian Mixture Copula Models, which combine the multimodal modeling capability of Gaussian Mixture Models with the flexibility of copulas to separately model marginal distributions and dependencies. Experimental evaluation on approximately 18 million real-world driving instances from two UN Regulation No. 157 scenarios shows that the proposed models consistently outperform Gaussian Copula Models and perform competitively with Gaussian Mixture Models, with performance measured by log-likelihood and Sinkhorn distance varying by scenario.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自动驾驶系统基于场景的安全验证中，对驾驶场景参数联合概率分布的精确建模需求。所提出的方法采用高斯混合Copula模型，该模型结合了高斯混合模型的多模态表示能力和Copula模型的依赖结构灵活性，能够分别对边缘分布及其相关性进行建模。在来自联合国第157号法规中两个场景的大约1800万个真实世界驾驶实例上的实验评估表明，高斯混合Copula模型在似然对数损失和Sinkhorn距离指标上持续优于高斯Copula模型，并与高斯混合模型表现相当，其相对性能因具体场景而异。</div>
</details>
</div>
<div class="card">
<div class="title">A Pragmatic VLA Foundation Model</div>
<div class="meta-line">Authors: Wei Wu, Fan Lu, Yunnan Wang, Shuai Yang, Shi Liu, Fangjing Wang, Qian Zhu, He Sun, Yong Wang, Shuailei Ma, Yiyu Ren, Kejia Zhang, Hui Yu, Jingmei Zhao, Shuai Zhou, Zhenqi Qiu, Houlong Xiong, Ziyu Wang, Zechen Wang, Ran Cheng, Yong-Lu Li, Yongtao Huang, Xing Zhu, Yujun Shen, Kecheng Zheng</div>
<div class="meta-line">First: 2026-01-26T17:08:04+00:00 · Latest: 2026-01-26T17:08:04+00:00</div>
<div class="meta-line">Comments: Project Webpage: https://technology.robbyant.com/lingbot-vla/, Code: https://github.com/Robbyant/lingbot-vla/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18692v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18692v1">PDF</a> · <a href="https://github.com/Robbyant/lingbot-vla/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offering great potential in robotic manipulation, a capable Vision-Language-Action (VLA) foundation model is expected to faithfully generalize across tasks and platforms while ensuring cost efficiency (e.g., data and GPU hours required for adaptation). To this end, we develop LingBot-VLA with around 20,000 hours of real-world data from 9 popular dual-arm robot configurations. Through a systematic assessment on 3 robotic platforms, each completing 100 tasks with 130 post-training episodes per task, our model achieves clear superiority over competitors, showcasing its strong performance and broad generalizability. We have also built an efficient codebase, which delivers a throughput of 261 samples per second per GPU with an 8-GPU training setup, representing a 1.5~2.8$\times$ (depending on the relied VLM base model) speedup over existing VLA-oriented codebases. The above features ensure that our model is well-suited for real-world deployment. To advance the field of robot learning, we provide open access to the code, base model, and benchmark data, with a focus on enabling more challenging tasks and promoting sound evaluation standards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>实用型VLA基础模型</div>
<div class="mono" style="margin-top:8px">具备机器人操作巨大潜能的视觉-语言-动作（VLA）基础模型，需在确保成本效益（如适应所需的数据与GPU时数）的同时，可靠地跨任务与平台泛化。为此，我们基于9种主流双机械臂配置采集约2万小时真实数据，开发了LingBot-VLA模型。通过在3个机器人平台上进行系统评估（每个平台完成100项任务，每项任务包含130次训练后测试），本模型显著优于同类方案，展现出卓越性能与广泛泛化能力。我们还构建了高效代码库，在8-GPU训练配置下实现每秒每GPU处理261个样本，较现有VLA专用代码库提速1.5~2.8倍（具体取决于所基于的VLM模型）。上述特性确保本模型适用于实际部署。为推进机器人学习领域发展，我们开源了代码、基础模型与基准数据，致力于支持更具挑战性的任务并推动健全的评估标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the need for a cost-efficient and generalizable Vision-Language-Action (VLA) foundation model for robotic manipulation, this work introduces LingBot-VLA, trained on approximately 20,000 hours of real-world data from nine dual-arm robot configurations. The method is evaluated through a systematic assessment on three robotic platforms, each performing 100 tasks with 130 post-training episodes per task, where the model demonstrates clear superiority over competitors, highlighting strong performance and broad generalizability. Key experimental findings also include an efficient codebase achieving a throughput of 261 samples per second per GPU on an 8-GPU setup, representing a 1.5 to 2.8 times speedup over existing VLA-oriented codebases, which supports real-world deployment.</div>
<div class="mono" style="margin-top:8px">为满足机器人操作中对成本高效且泛化性强的视觉-语言-动作基础模型的需求，本研究利用来自九种双臂机器人配置的大约20,000小时真实世界数据开发了LingBot-VLA。该方法通过在三个机器人平台上进行系统评估，每个平台执行100个任务，每个任务包含130次训练后测试，模型在性能和泛化能力上均明显优于竞争对手。关键实验结果包括在8-GPU设置下实现了每秒每GPU 261个样本的训练吞吐量，相比现有VLA代码库提速1.5至2.8倍，这证明了其适用于实际部署。</div>
</details>
</div>
<div class="card">
<div class="title">Masked Generative Policy for Robotic Control</div>
<div class="meta-line">Authors: Lipeng Zhuang, Shiyu Fan, Florent P. Audonnet, Yingdong Ru, Edmond S. L. Ho, Gerardo Aragon Camarasa, Paul Henderson</div>
<div class="meta-line">First: 2025-12-09T20:37:40+00:00 · Latest: 2026-01-26T17:04:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09101v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09101v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>掩码生成策略在机器人控制中的应用</div>
<div class="mono" style="margin-top:8px">本文提出掩码生成策略（MGP），一种用于视觉运动模仿学习的新型框架。我们将动作表示为离散标记，并训练一个条件掩码变换器，该模型能并行生成标记并快速优化低置信度标记。我们进一步提出两种新采样范式：MGP-Short采用基于评分的并行掩码生成优化方案处理马尔可夫任务；MGP-Long通过单次预测完整轨迹，并根据新观测动态优化低置信度动作标记。凭借全局一致性预测和鲁棒自适应执行能力，MGP-Long能在传统方法难以处理的复杂非马尔可夫任务中实现可靠控制。在涵盖Meta-World和LIBERO基准的150项机器人操作任务评估中，MGP相比当前最先进的扩散策略和自回归策略，在实现快速推理的同时获得更高的成功率。具体而言，MGP在150项任务中将平均成功率提升9%，单序列推理时间最高缩短35倍；在动态与缺失观测环境中将平均成功率提升60%，并成功解决了两项其他先进方法无法处理的非马尔可夫场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of existing visuomotor imitation learning methods in handling complex, non-Markovian tasks, this paper introduces the Masked Generative Policy (MGP) framework. The method represents actions as discrete tokens and employs a conditional masked transformer that first generates tokens in parallel and then iteratively refines only those with low confidence; it further proposes two sampling paradigms, MGP-Short for Markovian tasks and MGP-Long for non-Markovian tasks, with the latter predicting full trajectories and dynamically refining actions based on new observations. Experimental evaluations on 150 robotic manipulation tasks from Meta-World and LIBERO benchmarks demonstrate that MGP achieves faster inference (up to 35x speedup) and higher success rates (a 9% average improvement), with particularly significant gains in dynamic and missing-observation environments (60% average improvement) and the ability to solve two non-Markovian scenarios where prior state-of-the-art methods fail.</div>
<div class="mono" style="margin-top:8px">为解决现有视觉运动模仿学习方法在处理复杂、非马尔可夫机器人任务时的局限性，本文提出了掩码生成策略（MGP）框架。该方法将动作表示为离散令牌，并采用条件掩码变换器，首先生成并行令牌，然后仅迭代优化低置信度令牌；进一步提出了两种采样范式：MGP-Short用于马尔可夫任务，结合并行掩码生成和基于分数的优化，MGP-Long用于非马尔可夫任务，通过单次预测完整轨迹并根据新观察动态优化令牌。在Meta-World和LIBERO基准的150个操作任务上的实验评估表明，MGP实现了更快的推理速度（最高加速35倍）和更高的成功率（平均提升9%），在动态和缺失观察环境中提升尤为显著（平均提升60%），并在两个先前最先进方法失败的非马尔可夫场景中取得成功。</div>
</details>
</div>
<div class="card">
<div class="title">Gaussian Variational Inference with Non-Gaussian Factors for State Estimation: A UWB Localization Case Study</div>
<div class="meta-line">Authors: Andrew Stirling, Mykola Lukashchuk, Dmitry Bagaev, Wouter Kouw, James R. Forbes</div>
<div class="meta-line">First: 2025-12-22T20:17:04+00:00 · Latest: 2026-01-26T16:40:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19855v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19855v2">PDF</a> · <a href="https://github.com/decargroup/gvi_ws">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This letter extends the exactly sparse Gaussian variational inference (ESGVI) algorithm for state estimation in two complementary directions. First, ESGVI is generalized to operate on matrix Lie groups, enabling the estimation of states with orientation components while respecting the underlying group structure. Second, factors are introduced to accommodate heavy-tailed and skewed noise distributions, as commonly encountered in ultra-wideband (UWB) localization due to non-line-of-sight (NLOS) and multipath effects. Both extensions are shown to integrate naturally within the ESGVI framework while preserving its sparse and derivative-free structure. The proposed approach is validated in a UWB localization experiment with NLOS-rich measurements, demonstrating improved accuracy and comparable consistency. Finally, a Python implementation within a factor-graph-based estimation framework is made open-source (https://github.com/decargroup/gvi_ws) to support broader research use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于非高斯因子的高斯变分推断在状态估计中的应用：以超宽带定位为例</div>
<div class="mono" style="margin-top:8px">本文从两个互补方向扩展了精确稀疏高斯变分推断（ESGVI）算法在状态估计中的应用。首先，将ESGVI推广至矩阵李群上运行，使其能够估计包含方向分量的状态，同时保持底层群结构。其次，引入因子以处理重尾和偏态噪声分布，这在超宽带（UWB）定位中因非视距（NLOS）和多径效应而常见。两项扩展均能自然地融入ESGVI框架，同时保持其稀疏和无导数的结构。所提方法在包含大量NLOS测量的UWB定位实验中验证，显示出更高的精度和相当的稳定性。最后，基于因子图的估计框架提供了开源Python实现（https://github.com/decargroup/gvi_ws），以支持更广泛的研究应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research aims to enhance state estimation robustness for challenging sensor environments like ultra-wideband (UWB) localization, where non-line-of-sight and multipath effects cause heavy-tailed and skewed noise. The method extends the exactly sparse Gaussian variational inference (ESGVI) algorithm in two ways: generalizing it to operate on matrix Lie groups to properly handle orientation states, and incorporating non-Gaussian factors to model complex noise distributions. Experimental validation in a UWB localization scenario with rich NLOS measurements shows that the proposed approach achieves improved estimation accuracy while maintaining computational consistency comparable to standard methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在提升状态估计对非高斯噪声（如超宽带定位中由非视距和多径效应引起的噪声）的鲁棒性。该方法将精确稀疏高斯变分推断算法扩展到矩阵李群上以处理包含方向的估计，并引入非高斯因子来建模重尾和偏斜的噪声分布。在富含非视距测量的超宽带定位实验中验证表明，所提方法在保持与基线方法相当的一致性的同时，提高了估计精度。</div>
</details>
</div>
<div class="card">
<div class="title">Constraint-Aware Discrete-Time PID Gain Optimization for Robotic Joint Control Under Actuator Saturation</div>
<div class="meta-line">Authors: Ojasva Mishra, Xiaolong Wu, Min Xu</div>
<div class="meta-line">First: 2026-01-26T16:11:05+00:00 · Latest: 2026-01-26T16:11:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18639v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The precise regulation of rotary actuation is fundamental in autonomous robotics, yet practical PID loops deviate from continuous-time theory due to discrete-time execution, actuator saturation, and small delays and measurement imperfections. We present an implementation-aware analysis and tuning workflow for saturated discrete-time joint control. We (i) derive PI stability regions under Euler and exact zero-order-hold (ZOH) discretizations using the Jury criterion, (ii) evaluate a discrete back-calculation anti-windup realization under saturation-dominant regimes, and (iii) propose a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust IAE objective with soft penalties on overshoot and saturation duty. Baseline sweeps ($τ=1.0$~s, $Δt=0.01$~s, $u\in[-10,10]$) quantify rise/settle trends for P/PI/PID. Under a randomized model family emulating uncertainty, delay, noise, quantization, and tighter saturation, robustness-oriented tuning improves median IAE from $0.843$ to $0.430$ while keeping median overshoot below $2\%$. In simulation-only tuning, the certification screen rejects $11.6\%$ of randomly sampled gains within bounds before full robust evaluation, improving sample efficiency without hardware experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>执行器饱和下机器人关节控制的约束感知离散时间PID增益优化</div>
<div class="mono" style="margin-top:8px">旋转驱动的精确调节是自主机器人技术的基础，然而实际PID回路因离散时间执行、执行器饱和以及微小延迟和测量缺陷而偏离连续时间理论。本文提出一种面向饱和离散时间关节控制的实现感知分析与整定流程：首先利用朱里判据推导欧拉和精确零阶保持离散化下的PI稳定域；其次评估饱和主导工况下的离散反向计算抗饱和实现；最后提出混合认证贝叶斯优化流程，在优化具有超调与饱和占空比软惩罚的鲁棒IAE指标时，同步筛选解析不稳定候选参数和行为不安全瞬态过程。基准扫描（τ=1.0秒，Δt=0.01秒，u∈[-10,10]）量化了P/PI/PID的上升/稳定趋势。在模拟不确定性、延迟、噪声、量化及更严格饱和的随机模型族中，鲁棒导向整定将IAE中位数从0.843降至0.430，同时保持超调中位数低于2%。纯仿真整定中，认证筛选在完整鲁棒评估前可剔除边界内11.6%的随机采样增益，在无需硬件实验的情况下提升了采样效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the practical challenges in robotic joint control where discrete-time execution, actuator saturation, and small delays cause PID loops to deviate from ideal continuous-time theory. The method involves deriving PI stability regions using Jury criterion for Euler and exact zero-order-hold discretizations, evaluating a discrete back-calculation anti-windup scheme, and proposing a hybrid-certified Bayesian optimization workflow that screens analytically unstable candidates and behaviorally unsafe transients while optimizing a robust integral absolute error objective. Experimental results show that robustness-oriented tuning on a randomized model family improves median IAE from 0.843 to 0.430 while keeping median overshoot below 2%, and the certification screen rejects 11.6% of randomly sampled gains before full evaluation, enhancing sample efficiency.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人关节控制中因离散时间执行、执行器饱和及微小延迟导致PID控制器偏离理想连续时间理论的实践挑战。方法包括使用Jury判据推导不同离散化下的PI稳定性区域，评估离散抗饱和方案，并提出一种混合认证的贝叶斯优化流程，该流程在优化鲁棒积分绝对误差目标的同时，筛选分析不稳定和行为不安全的瞬态。实验结果表明，在随机模型不确定性下，面向鲁棒性的调参将中位积分绝对误差从0.843改善至0.430，同时保持中位超调低于2%，且认证筛选在完整鲁棒评估前预先拒绝了11.6%的采样增益，从而提高了样本效率。</div>
</details>
</div>
<div class="card">
<div class="title">ExoGS: A 4D Real-to-Sim-to-Real Framework for Scalable Manipulation Data Collection</div>
<div class="meta-line">Authors: Yiming Wang, Ruogu Zhang, Minyang Li, Hao Shi, Junbo Wang, Deyi Li, Jieji Ren, Wenhai Liu, Weiming Wang, Hao-Shu Fang</div>
<div class="meta-line">First: 2026-01-26T16:04:12+00:00 · Latest: 2026-01-26T16:04:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18629v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18629v1">PDF</a> · <a href="https://github.com/zaixiabalala/ExoGS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-to-Sim-to-Real technique is gaining increasing interest for robotic manipulation, as it can generate scalable data in simulation while having narrower sim-to-real gap. However, previous methods mainly focused on environment-level visual real-to-sim transfer, ignoring the transfer of interactions, which could be challenging and inefficient to obtain purely in simulation especially for contact-rich tasks. We propose ExoGS, a robot-free 4D Real-to-Sim-to-Real framework that captures both static environments and dynamic interactions in the real world and transfers them seamlessly to a simulated environment. It provides a new solution for scalable manipulation data collection and policy learning. ExoGS employs a self-designed robot-isomorphic passive exoskeleton AirExo-3 to capture kinematically consistent trajectories with millimeter-level accuracy and synchronized RGB observations during direct human demonstrations. The robot, objects, and environment are reconstructed as editable 3D Gaussian Splatting assets, enabling geometry-consistent replay and large-scale data augmentation. Additionally, a lightweight Mask Adapter injects instance-level semantics into the policy to enhance robustness under visual domain shifts. Real-world experiments demonstrate that ExoGS significantly improves data efficiency and policy generalization compared to teleoperation-based baselines. Code and hardware files have been released on https://github.com/zaixiabalala/ExoGS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExoGS：一种用于可扩展操作数据采集的四维实-仿-实框架</div>
<div class="mono" style="margin-top:8px">实-仿-实技术在机器人操作领域日益受到关注，因其能在仿真中生成可扩展数据，同时缩小仿真与现实的差距。然而，现有方法主要聚焦于环境层面的视觉实-仿迁移，忽略了交互过程的迁移——对于接触密集型任务，纯仿真获取此类交互既具挑战性又效率低下。本文提出ExoGS，一种无需机器人的四维实-仿-实框架，能够同步捕获真实世界中的静态环境与动态交互，并将其无缝迁移至仿真环境。该框架为可扩展操作数据采集与策略学习提供了新方案。ExoGS采用自主设计的机器人同构被动外骨骼AirExo-3，在人类直接示教过程中以毫米级精度捕获运动学一致轨迹及同步RGB观测。机器人、物体与环境被重建为可编辑的3D高斯泼溅资产，支持几何一致的重放与大规模数据增强。此外，轻量级掩码适配器向策略注入实例级语义，提升了视觉域偏移下的鲁棒性。真实世界实验表明，相较于基于遥操作的基线方法，ExoGS显著提升了数据效率与策略泛化能力。代码与硬件文件已发布于https://github.com/zaixiabalala/ExoGS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of efficiently collecting scalable manipulation data with a narrow sim-to-real gap, noting that prior real-to-sim-to-real methods focused on visual environment transfer while neglecting dynamic interactions, which are difficult to simulate for contact-rich tasks. The proposed ExoGS framework captures real-world static environments and dynamic interactions using a passive exoskeleton to record accurate human demonstrations, reconstructs these elements as editable 3D Gaussian Splatting assets for simulation replay and augmentation, and employs a Mask Adapter to inject instance semantics into policies for robustness against visual shifts. Experimental results show that ExoGS enhances data efficiency and policy generalization over teleoperation baselines in real-world tests.</div>
<div class="mono" style="margin-top:8px">该研究针对机器人技术中高效收集可扩展操作数据的挑战，重点关注了真实-仿真-真实范式，该范式常忽略动态交互的迁移，尤其在接触丰富的任务中。提出的ExoGS框架使用机器人同构被动外骨骼AirExo-3捕获真实世界的静态环境和动态交互，记录精确轨迹和同步RGB数据，然后将这些元素重建为可编辑的3D高斯溅射资产用于仿真回放和增强，同时通过掩码适配器注入实例语义以提高策略鲁棒性。实验结果表明，ExoGS相比基于遥操作的方法，显著提升了数据效率和策略泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Cross-Platform Scaling of Vision-Language-Action Models from Edge to Cloud GPUs</div>
<div class="meta-line">Authors: Amir Taherin, Juyi Lin, Arash Akbari, Arman Akbari, Pu Zhao, Weiwei Chen, David Kaeli, Yanzhi Wang</div>
<div class="meta-line">First: 2025-09-15T00:00:37+00:00 · Latest: 2026-01-26T15:57:20+00:00</div>
<div class="meta-line">Comments: To appear in the Asilomar Conference on Signals, Systems, and Computers 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.11480v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.11480v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have emerged as powerful generalist policies for robotic control, yet their performance scaling across model architectures and hardware platforms, as well as their associated power budgets, remain poorly understood. This work presents an evaluation of five representative VLA models -- spanning state-of-the-art baselines and two newly proposed architectures -- targeting edge and datacenter GPU platforms. Using the LIBERO benchmark, we measure accuracy alongside system-level metrics, including latency, throughput, and peak memory usage, under varying edge power constraints and high-performance datacenter GPU configurations. Our results identify distinct scaling trends: (1) architectural choices, such as action tokenization and model backbone size, strongly influence throughput and memory footprint; (2) power-constrained edge devices exhibit non-linear performance degradation, with some configurations matching or exceeding older datacenter GPUs; and (3) high-throughput variants can be achieved without significant accuracy loss. These findings provide actionable insights when selecting and optimizing VLAs across a range of deployment constraints. Our work challenges current assumptions about the superiority of datacenter hardware for robotic inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉-语言-动作模型从边缘到云端GPU的跨平台扩展性研究</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型已成为机器人控制领域强大的通用策略，但其在不同模型架构与硬件平台间的性能扩展特性及相应功耗预算仍缺乏深入理解。本研究评估了五种代表性VLA模型——涵盖前沿基线模型与两种新提出架构——针对边缘与数据中心GPU平台。基于LIBERO基准测试，我们在不同边缘功耗约束与高性能数据中心GPU配置下，测量了模型精度及系统级指标（包括延迟、吞吐量与峰值内存使用量）。研究发现以下扩展趋势：（1）动作标记化与模型主干规模等架构选择显著影响吞吐量与内存占用；（2）功耗受限的边缘设备呈现非线性性能衰减，部分配置可匹配或超越早期数据中心GPU；（3）可在保持精度的前提下实现高吞吐量变体。这些发现为在不同部署约束下选择与优化VLA模型提供了实践指导。本研究对当前‘数据中心硬件在机器人推理中具有绝对优势’的假设提出了挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the performance scaling of Vision-Language-Action (VLA) models for robotic control across different model architectures and hardware platforms, motivated by a lack of understanding of their efficiency under various deployment constraints. The method involves evaluating five representative VLA models, including established baselines and two novel architectures, on both edge and datacenter GPUs using the LIBERO benchmark to measure accuracy, latency, throughput, and memory usage under different power budgets. Key experimental findings reveal that architectural choices like action tokenization significantly impact throughput and memory, that power-constrained edge devices can match the performance of older datacenter GPUs despite non-linear degradation, and that high-throughput variants can be achieved without major accuracy loss, challenging assumptions about datacenter hardware superiority.</div>
<div class="mono" style="margin-top:8px">本研究旨在探究用于机器人控制的视觉-语言-动作（VLA）模型在不同模型架构和硬件平台上的扩展性，因为其性能、功耗预算与部署目标之间的关系尚不明确。方法上，该工作评估了五个代表性VLA模型（包括现有基准和两种新提出的架构），在边缘和云端GPU上使用LIBERO基准测试，在多种功耗限制下测量任务准确率以及延迟、吞吐量和内存使用等系统指标。主要实验结果表明：架构选择（如动作标记化和主干网络大小）显著影响吞吐量和内存占用；受功耗限制的边缘设备表现出非线性性能下降，但某些配置的性能可与较旧的云端GPU相媲美；并且可以在不显著损失准确性的情况下实现高吞吐量模型变体，这对云端硬件在机器人推理中具有优越性的现有假设提出了挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation</div>
<div class="meta-line">Authors: Seokju Lee, Kyung-Soo Kim</div>
<div class="meta-line">First: 2026-01-26T15:13:36+00:00 · Latest: 2026-01-26T15:13:36+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures, Accepted to IEEE Robotics and Automation Letters (RA-L)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18569v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18569v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this letter, we propose an Attention-Based Neural-Augmented Kalman Filter (AttenNKF) for state estimation in legged robots. Foot slip is a major source of estimation error: when slip occurs, kinematic measurements violate the no-slip assumption and inject bias during the update step. Our objective is to estimate this slip-induced error and compensate for it. To this end, we augment an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer error conditioned on foot-slip severity and then applies this estimate as a post-update compensation to the InEKF state (i.e., after the filter update). The compensator is trained in a latent space, which aims to reduce sensitivity to raw input scales and encourages structured slip-conditioned compensations, while preserving the InEKF recursion. Experiments demonstrate improved performance compared to existing legged-robot state estimators, particularly under slip-prone conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力机制的神经增强卡尔曼滤波器在腿式机器人状态估计中的应用</div>
<div class="mono" style="margin-top:8px">本文提出一种基于注意力机制的神经增强卡尔曼滤波器（AttenNKF），用于腿式机器人的状态估计。足部滑移是估计误差的主要来源：当滑移发生时，运动学测量违反无滑移假设，并在更新步骤中引入偏差。本研究的目标是估计这种滑移引起的误差并进行补偿。为此，我们在不变扩展卡尔曼滤波器（InEKF）基础上，引入一个神经补偿器，该补偿器利用注意力机制根据足部滑移严重程度推断误差，并将估计值作为后更新补偿应用于InEKF状态（即在滤波器更新后执行）。补偿器在潜在空间中训练，旨在降低对原始输入尺度的敏感性，促进结构化滑移条件补偿，同时保持InEKF的递归特性。实验表明，与现有腿式机器人状态估计方法相比，该方法性能显著提升，尤其在易滑移条件下表现优异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the problem of state estimation errors in legged robots caused by foot slip, which violates the no-slip assumption in kinematic models and introduces bias. The proposed method, an Attention-Based Neural-Augmented Kalman Filter (AttenNKF), augments an Invariant Extended Kalman Filter (InEKF) with a neural compensator that uses an attention mechanism to infer slip-induced error based on slip severity and applies it as a post-update correction. Experimental results show that this approach outperforms existing state estimators for legged robots, especially in slip-prone conditions.</div>
<div class="mono" style="margin-top:8px">本研究针对足式机器人因足部打滑导致的状态估计误差问题，该问题违背了运动学模型的无滑移假设并引入偏差。所提出的方法是一种基于注意力的神经增强卡尔曼滤波器（AttenNKF），它通过一个使用注意力机制的神经补偿器来增强不变扩展卡尔曼滤波器，该补偿器根据打滑严重程度推断误差，并将其作为更新后校正。实验结果表明，该方法优于现有的足式机器人状态估计器，尤其是在易打滑的条件下。</div>
</details>
</div>
<div class="card">
<div class="title">Fast and Safe Trajectory Optimization for Mobile Manipulators With Neural Configuration Space Distance Field</div>
<div class="meta-line">Authors: Yulin Li, Zhiyuan Song, Yiming Li, Zhicheng Song, Kai Chen, Chunxin Zheng, Zhihai Bi, Jiahang Cao, Sylvain Calinon, Fan Shi, Jun Ma</div>
<div class="meta-line">First: 2026-01-26T14:55:26+00:00 · Latest: 2026-01-26T14:55:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18548v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18548v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile manipulators promise agile, long-horizon behavior by coordinating base and arm motion, yet whole-body trajectory optimization in cluttered, confined spaces remains difficult due to high-dimensional nonconvexity and the need for fast, accurate collision reasoning. Configuration Space Distance Fields (CDF) enable fixed-base manipulators to model collisions directly in configuration space via smooth, implicit distances. This representation holds strong potential to bypass the nonlinear configuration-to-workspace mapping while preserving accurate whole-body geometry and providing optimization-friendly collision costs. Yet, extending this capability to mobile manipulators is hindered by unbounded workspaces and tighter base-arm coupling. We lift this promise to mobile manipulation with Generalized Configuration Space Distance Fields (GCDF), extending CDF to robots with both translational and rotational joints in unbounded workspaces with tighter base-arm coupling. We prove that GCDF preserves Euclidean-like local distance structure and accurately encodes whole-body geometry in configuration space, and develop a data generation and training pipeline that yields continuous neural GCDFs with accurate values and gradients, supporting efficient GPU-batched queries. Building on this representation, we develop a high-performance sequential convex optimization framework centered on GCDF-based collision reasoning. The solver scales to large numbers of implicit constraints through (i) online specification of neural constraints, (ii) sparsity-aware active-set detection with parallel batched evaluation across thousands of constraints, and (iii) incremental constraint management for rapid replanning under scene changes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经构型空间距离场的移动机械臂快速安全轨迹优化</div>
<div class="mono" style="margin-top:8px">移动机械臂通过协调基座与机械臂运动，有望实现灵活的长时域行为，但在杂乱受限空间中进行全身轨迹优化仍面临高维非凸性与快速精确碰撞推理的双重挑战。构型空间距离场（CDF）使固定基座机械臂能够通过平滑隐式距离直接在构型空间建模碰撞。该表示法具备绕过非线性构型-工作空间映射的潜力，同时保持精确的全身几何结构并提供优化友好的碰撞代价。然而，将这一能力扩展至移动机械臂受到无界工作空间和更紧密的基座-臂耦合的制约。本研究通过广义构型空间距离场（GCDF）将这一前景延伸至移动操作领域，将CDF扩展至具有平移与旋转关节、在无界工作空间中具有紧密基座-臂耦合的机器人。我们证明GCDF保持了类欧几里得局部距离结构，并在构型空间中精确编码全身几何，开发了能够生成具有精确值和梯度的连续神经GCDF的数据生成与训练流程，支持高效的GPU批量查询。基于此表示法，我们构建了以GCDF碰撞推理为核心的高性能序列凸优化框架。该求解器通过以下方式实现大规模隐式约束的扩展：（1）神经约束的在线设定，（2）具备稀疏感知的主动集检测与数千约束的并行批量评估，（3）支持场景变化下快速重规划的增量约束管理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of whole-body trajectory optimization for mobile manipulators in cluttered environments, where high-dimensional nonconvexity and the need for fast collision checking hinder performance. The method introduces Generalized Configuration Space Distance Fields (GCDF), which extend Configuration Space Distance Fields to robots with translational and rotational joints in unbounded workspaces, and develops a neural GCDF model with accurate gradients. Experimental results demonstrate that the GCDF-based sequential convex optimization framework efficiently handles thousands of implicit collision constraints through batched GPU queries and active-set detection, enabling rapid replanning.</div>
<div class="mono" style="margin-top:8px">为使移动机械臂在杂乱受限空间中实现敏捷的长程运动规划，本研究针对高维非凸轨迹优化与快速精确碰撞检测的难题展开。方法上提出了广义构型空间距离场（GCDF），将构型空间距离场扩展至具有平移和旋转关节、在无界工作空间中紧密耦合的机器人，通过专门数据管道训练的神经网络保留了精确全身几何信息并提供利于优化的梯度。实验结果表明，基于GCDF碰撞推理的序列凸优化框架，结合批量GPU查询与主动集约束管理，实现了高效、可扩展的轨迹规划，适用于场景变化下的快速重规划。</div>
</details>
</div>
<div class="card">
<div class="title">DecompGAIL: Learning Realistic Traffic Behaviors with Decomposed Multi-Agent Generative Adversarial Imitation Learning</div>
<div class="meta-line">Authors: Ke Guo, Haochen Liu, Xiaojun Wu, Chen Lv</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-10-08T11:46:39+00:00 · Latest: 2026-01-26T14:53:15+00:00</div>
<div class="meta-line">Comments: accepted by ICLR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.06913v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.06913v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realistic traffic simulation is critical for the development of autonomous driving systems and urban mobility planning, yet existing imitation learning approaches often fail to model realistic traffic behaviors. Behavior cloning suffers from covariate shift, while Generative Adversarial Imitation Learning (GAIL) is notoriously unstable in multi-agent settings. We identify a key source of this instability: irrelevant interaction misguidance, where a discriminator penalizes an ego vehicle&#x27;s realistic behavior due to unrealistic interactions among its neighbors. To address this, we propose Decomposed Multi-agent GAIL (DecompGAIL), which explicitly decomposes realism into ego-map and ego-neighbor components, filtering out misleading neighbor: neighbor and neighbor: map interactions. We further introduce a social PPO objective that augments ego rewards with distance-weighted neighborhood rewards, encouraging overall realism across agents. Integrated into a lightweight SMART-based backbone, DecompGAIL achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DecompGAIL：基于分解式多智能体生成对抗模仿学习的真实交通行为建模</div>
<div class="mono" style="margin-top:8px">真实交通仿真对自动驾驶系统开发和城市交通规划至关重要，但现有模仿学习方法常难以建模真实交通行为。行为克隆存在协变量偏移问题，而生成对抗模仿学习（GAIL）在多智能体场景中稳定性欠佳。我们发现其不稳定的关键根源在于无关交互误导：判别器因邻域车辆间不真实的交互而惩罚主车（ego vehicle）的真实行为。为此，我们提出分解式多智能体GAIL（DecompGAIL），将真实性显式分解为主车-地图和主车-邻域两个组件，过滤误导性的邻域-邻域及邻域-地图交互。进一步引入社会PPO目标，通过距离加权的邻域奖励增强主车奖励，促进跨智能体的整体真实性。基于轻量级SMART框架构建的DecompGAIL在WOMD Sim Agents 2025基准测试中达到最先进性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for realistic traffic simulation in autonomous driving and urban planning, where existing imitation learning methods like behavior cloning suffer from covariate shift and Generative Adversarial Imitation Learning (GAIL) is unstable in multi-agent settings due to irrelevant interaction misguidance. The proposed method, Decomposed Multi-agent GAIL (DecompGAIL), addresses this by explicitly decomposing realism into ego-map and ego-neighbor components to filter out misleading neighbor-neighbor and neighbor-map interactions, and introduces a social PPO objective that augments ego rewards with distance-weighted neighborhood rewards. Key experimental results show that DecompGAIL, integrated into a SMART-based backbone, achieves state-of-the-art performance on the WOMD Sim Agents 2025 benchmark.</div>
<div class="mono" style="margin-top:8px">该研究旨在改进用于自动驾驶和城市规划的真实交通仿真，以解决现有模仿学习方法中行为克隆存在协变量偏移以及生成对抗模仿学习在多智能体设置中不稳定的问题。所提出的方法——分解多智能体生成对抗模仿学习（DecompGAIL）——将真实性明确分解为自车-地图和自车-邻居两个组成部分，以过滤掉来自邻居-邻居和邻居-地图交互的无关误导，并引入了一个社会近端策略优化目标，通过距离加权的邻居奖励来增强自车奖励。实验结果表明，基于轻量级SMART骨干网络集成的DecompGAIL在WOMD Sim Agents 2025基准测试中取得了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction</div>
<div class="meta-line">Authors: Linyong Gan, Zimo Li, Wenxin Xu, Xingjian Li, Jianhua Z. Huang, Enmei Tu, Shuhang Chen</div>
<div class="meta-line">First: 2026-01-26T14:42:31+00:00 · Latest: 2026-01-26T14:42:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18537v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18537v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate long-horizon vessel trajectory prediction remains challenging due to compounded uncertainty from complex navigation behaviors and environmental factors. Existing methods often struggle to maintain global directional consistency, leading to drifting or implausible trajectories when extrapolated over long time horizons. To address this issue, we propose a semantic-key-point-conditioned trajectory modeling framework, in which future trajectories are predicted by conditioning on a high-level Next Key Point (NKP) that captures navigational intent. This formulation decomposes long-horizon prediction into global semantic decision-making and local motion modeling, effectively restricting the support of future trajectories to semantically feasible subsets. To efficiently estimate the NKP prior from historical observations, we adopt a pretrain-finetune strategy. Extensive experiments on real-world AIS data demonstrate that the proposed method consistently outperforms state-of-the-art approaches, particularly for long travel durations, directional accuracy, and fine-grained trajectory prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SKETCH：基于语义关键点条件化的长时程船舶轨迹预测</div>
<div class="mono" style="margin-top:8px">由于复杂航行行为与环境因素叠加的不确定性，准确的长时程船舶轨迹预测仍具挑战。现有方法常难以保持全局方向一致性，导致长时外推时轨迹漂移或失真。为此，我们提出一种语义关键点条件化的轨迹建模框架，通过以捕捉航行意图的高层下一关键点（NKP）为条件预测未来轨迹。该框架将长时程预测分解为全局语义决策与局部运动建模，有效将未来轨迹约束在语义可行的子集中。为从历史观测中高效估计NKP先验，我们采用预训练-微调策略。基于真实AIS数据的大量实验表明，所提方法在长航时、方向准确性和细粒度轨迹预测方面均持续优于现有先进方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to improve long-horizon vessel trajectory prediction, which is difficult due to complex navigation behaviors and environmental uncertainties that cause existing methods to produce globally inconsistent or implausible trajectories. The proposed method, SKETCH, introduces a semantic key-point conditioning framework that predicts future trajectories by conditioning on a high-level Next Key Point (NKP) representing navigational intent, thereby decomposing the problem into global semantic decision-making and local motion modeling to restrict predictions to semantically feasible subsets. Experimental results on real-world AIS data show that this approach consistently outperforms state-of-the-art methods, especially in long-duration prediction, directional accuracy, and fine-grained trajectory details.</div>
<div class="mono" style="margin-top:8px">由于复杂的航行行为和环境不确定性，准确的长时域船舶轨迹预测具有挑战性，现有方法在长时外推时常出现方向不一致和轨迹不合理的问题。为此，本文提出SKETCH框架，通过以表征航行意图的高层下一关键点（NKP）为条件进行轨迹预测，从而将长时域预测分解为全局语义决策和局部运动建模，将未来轨迹限制在语义可行的子集内；采用预训练-微调策略来高效估计NKP先验。在真实AIS数据上的大量实验表明，该方法在长时航行、方向准确性和细粒度轨迹预测方面持续优于现有先进方法。</div>
</details>
</div>
<div class="card">
<div class="title">GPA-VGGT:Adapting VGGT to Large Scale Localization by Self-Supervised Learning with Geometry and Physics Aware Loss</div>
<div class="meta-line">Authors: Yangfan Xu, Lilian Zhang, Xiaofeng He, Pengdong Wu, Wenqi Wu, Jun Mao</div>
<div class="meta-line">First: 2026-01-23T16:46:59+00:00 · Latest: 2026-01-26T14:14:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16885v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16885v2">PDF</a> · <a href="https://github.com/X-yangfan/GPA-VGGT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based general visual geometry frameworks have shown promising performance in camera pose estimation and 3D scene understanding. Recent advancements in Visual Geometry Grounded Transformer (VGGT) models have shown great promise in camera pose estimation and 3D reconstruction. However, these models typically rely on ground truth labels for training, posing challenges when adapting to unlabeled and unseen scenes. In this paper, we propose a self-supervised framework to train VGGT with unlabeled data, thereby enhancing its localization capability in large-scale environments. To achieve this, we extend conventional pair-wise relations to sequence-wise geometric constraints for self-supervised learning. Specifically, in each sequence, we sample multiple source frames and geometrically project them onto different target frames, which improves temporal feature consistency. We formulate physical photometric consistency and geometric constraints as a joint optimization loss to circumvent the requirement for hard labels. By training the model with this proposed method, not only the local and global cross-view attention layers but also the camera and depth heads can effectively capture the underlying multi-view geometry. Experiments demonstrate that the model converges within hundreds of iterations and achieves significant improvements in large-scale localization. Our code will be released at https://github.com/X-yangfan/GPA-VGGT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GPA-VGGT：通过几何与物理感知损失的自监督学习将VGGT适配于大规模定位任务</div>
<div class="mono" style="margin-top:8px">基于Transformer的通用视觉几何框架在相机姿态估计与三维场景理解中展现出优异性能。近期视觉几何基础Transformer（VGGT）模型在相机姿态估计与三维重建领域取得显著进展，但这类模型通常依赖标注真值进行训练，在适应未标注及未见场景时面临挑战。本文提出一种自监督框架，利用未标注数据训练VGGT模型，从而增强其在大规模环境中的定位能力。为实现这一目标，我们将传统成对关系扩展为序列级几何约束用于自监督学习：在每个序列中采样多帧源图像，通过几何投影将其映射至不同目标帧，以提升时序特征一致性。我们将物理光度一致性约束与几何约束构建为联合优化损失函数，从而避免对硬标签的依赖。通过该方法训练模型，不仅局部与全局跨视角注意力层能有效捕捉潜在的多视角几何关系，相机参数头与深度估计头也获得同步优化。实验表明，模型在数百次迭代内即可收敛，并在大规模定位任务中取得显著性能提升。代码发布于https://github.com/X-yangfan/GPA-VGGT。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitation of transformer-based visual geometry models like VGGT that require ground truth labels for training, this work introduces a self-supervised learning framework for large-scale camera localization. The method extends pairwise geometric constraints to sequence-wise ones, sampling multiple source frames and projecting them onto different target frames to enforce temporal feature consistency, and formulates a joint optimization loss based on photometric and geometric constraints without needing labels. Experimental results show the model converges in hundreds of iterations and achieves significant performance improvements in large-scale localization tasks.</div>
<div class="mono" style="margin-top:8px">本研究针对视觉几何基础Transformer（VGGT）模型通常依赖真实标签进行训练、难以适应无标注大规模场景的局限性，提出了一种自监督学习框架GPA-VGGT。该方法将成对几何关系扩展为序列级约束，通过采样多个源帧并将其几何投影到不同目标帧来增强时序特征一致性，并基于物理光度一致性和几何约束构建联合优化损失以摆脱对标签的依赖。实验结果表明，该模型在数百次迭代内即可收敛，并在大规模定位任务中取得显著性能提升，其注意力层与相机/深度头均能有效捕捉底层多视图几何关系。</div>
</details>
</div>
<div class="card">
<div class="title">DV-VLN: Dual Verification for Reliable LLM-Based Vision-and-Language Navigation</div>
<div class="meta-line">Authors: Zijun Li, Shijie Li, Zhenxi Zhang, Bin Li, Shoujun Zhou</div>
<div class="meta-line">First: 2026-01-26T13:47:55+00:00 · Latest: 2026-01-26T13:47:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18492v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18492v1">PDF</a> · <a href="https://github.com/PlumJun/DV-VLN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-and-Language Navigation (VLN) requires an embodied agent to navigate in a complex 3D environment according to natural language instructions. Recent progress in large language models (LLMs) has enabled language-driven navigation with improved interpretability. However, most LLM-based agents still rely on single-shot action decisions, where the model must choose one option from noisy, textualized multi-perspective observations. Due to local mismatches and imperfect intermediate reasoning, such decisions can easily deviate from the correct path, leading to error accumulation and reduced reliability in unseen environments. In this paper, we propose DV-VLN, a new VLN framework that follows a generate-then-verify paradigm. DV-VLN first performs parameter-efficient in-domain adaptation of an open-source LLaMA-2 backbone to produce a structured navigational chain-of-thought, and then verifies candidate actions with two complementary channels: True-False Verification (TFV) and Masked-Entity Verification (MEV). DV-VLN selects actions by aggregating verification successes across multiple samples, yielding interpretable scores for reranking. Experiments on R2R, RxR (English subset), and REVERIE show that DV-VLN consistently improves over direct prediction and sampling-only baselines, achieving competitive performance among language-only VLN agents and promising results compared with several cross-modal systems.Code is available at https://github.com/PlumJun/DV-VLN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DV-VLN：基于大语言模型的视觉语言导航双验证可靠框架</div>
<div class="mono" style="margin-top:8px">视觉语言导航要求具身智能体依据自然语言指令在复杂三维环境中行进。大语言模型的最新进展提升了语言驱动导航的可解释性，但多数基于大语言模型的智能体仍依赖单次动作决策——模型需从含噪声的多视角文本化观测中选取单一选项。由于局部错配与中间推理缺陷，此类决策易偏离正确路径，导致误差累积及未知环境下的可靠性下降。本文提出DV-VLN，该新型视觉语言导航框架遵循“生成-验证”范式：首先对开源LLaMA-2主干网络进行参数高效的领域自适应以生成结构化导航思维链，随后通过真伪验证与掩码实体验证双通道验证候选动作。DV-VLN通过聚合多样本的验证成功次数选择动作，生成可解释的重新排序分数。在R2R、RxR（英文子集）和REVERIE数据集上的实验表明，DV-VLN持续优于直接预测与纯采样基线，在纯语言视觉语言导航智能体中取得竞争优势，与若干跨模态系统相比亦展现出潜力。代码已开源：https://github.com/PlumJun/DV-VLN。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the reliability issues in LLM-based vision-and-language navigation, where single-shot action decisions from noisy observations often lead to error accumulation, this paper introduces DV-VLN, a framework that employs a generate-then-verify paradigm. The method first adapts a LLaMA-2 backbone for structured navigational reasoning and then verifies candidate actions through dual channels—True-False Verification and Masked-Entity Verification—aggregating verification successes across samples to rerank actions. Experimental results on R2R, RxR, and REVERIE benchmarks demonstrate that DV-VLN consistently outperforms direct prediction and sampling-only baselines, achieving competitive performance among language-only agents and promising results compared to cross-modal systems.</div>
<div class="mono" style="margin-top:8px">针对基于大语言模型的视觉语言导航中，从嘈杂的文本化观察中进行单次动作决策容易导致错误累积的可靠性问题，本文提出了DV-VLN，一种先生成后验证的框架。该方法首先对LLaMA-2主干进行参数高效的领域自适应以生成结构化的导航思维链，然后通过两个互补的通道——真假验证和掩码实体验证——对候选动作进行验证，并通过聚合多个样本的验证成功次数来选择动作。在R2R、RxR和REVERIE基准上的实验结果表明，DV-VLN持续优于直接预测和仅采样的基线方法，在纯语言智能体中取得了有竞争力的性能，并与多个跨模态系统相比展现出有前景的结果。</div>
</details>
</div>
<div class="card">
<div class="title">SG-CADVLM: A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation</div>
<div class="meta-line">Authors: Hongyi Zhao, Shuo Wang, Qijie He, Ziyuan Pu</div>
<div class="meta-line">First: 2026-01-26T12:53:12+00:00 · Latest: 2026-01-26T12:53:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18442v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18442v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous vehicle safety validation requires testing on safety-critical scenarios, but these events are rare in real-world driving and costly to test due to collision risks. Crash reports provide authentic specifications of safety-critical events, offering a vital alternative to scarce real-world collision trajectory data. This makes them valuable sources for generating realistic high-risk scenarios through simulation. Existing approaches face significant limitations because data-driven methods lack diversity due to their reliance on existing latent distributions, whereas adversarial methods often produce unrealistic scenarios lacking physical fidelity. Large Language Model (LLM) and Vision Language Model (VLM)-based methods show significant promise. However, they suffer from context suppression issues where internal parametric knowledge overrides crash specifications, producing scenarios that deviate from actual accident characteristics. This paper presents SG-CADVLM (A Context-Aware Decoding Powered Vision Language Model for Safety-Critical Scenario Generation), a framework that integrates Context-Aware Decoding with multi-modal input processing to generate safety-critical scenarios from crash reports and road network diagrams. The framework mitigates VLM hallucination issues while enabling the simultaneous generation of road geometry and vehicle trajectories. The experimental results demonstrate that SG-CADVLM generates critical risk scenarios at a rate of 84.4% compared to 12.5% for the baseline methods, representing an improvement of 469%, while producing executable simulations for autonomous vehicle testing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SG-CADVLM：一种基于上下文感知解码的视觉语言模型用于安全关键场景生成</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆安全验证需在安全关键场景下进行测试，但此类事件在真实驾驶中罕见且因碰撞风险导致测试成本高昂。事故报告提供了安全关键事件的真实规格，为稀缺的真实碰撞轨迹数据提供了重要替代来源，使其成为通过仿真生成真实高风险场景的宝贵资源。现有方法存在显著局限：数据驱动方法因依赖现有潜在分布而缺乏多样性，对抗性方法则常生成缺乏物理真实性的非现实场景。基于大语言模型（LLM）和视觉语言模型（VLM）的方法展现出巨大潜力，但受限于上下文抑制问题——内部参数知识会覆盖事故规格，导致生成的场景偏离实际事故特征。本文提出SG-CADVLM（一种基于上下文感知解码的视觉语言模型用于安全关键场景生成），该框架将上下文感知解码与多模态输入处理相结合，能够根据事故报告和路网图生成安全关键场景。该框架缓解了VLM的幻觉问题，同时实现了道路几何与车辆轨迹的同步生成。实验结果表明，SG-CADVLM生成关键风险场景的比例达84.4%，而基线方法仅为12.5%，提升了469%，且能生成可用于自动驾驶测试的可执行仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity and high cost of real-world safety-critical scenarios for autonomous vehicle validation, this research leverages crash reports as specifications to generate realistic high-risk simulations. The proposed SG-CADVLM framework integrates Context-Aware Decoding with multi-modal input processing of crash reports and road network diagrams to mitigate vision-language model hallucination and context suppression, enabling the simultaneous generation of physically plausible road geometry and vehicle trajectories. Experiments show the method generates critical risk scenarios at a rate of 84.4%, a 469% improvement over baseline methods that achieve only 12.5%, while producing executable simulations for testing.</div>
<div class="mono" style="margin-top:8px">为解决自动驾驶安全验证中真实安全关键场景稀缺且测试成本高的问题，本研究利用事故报告作为真实规范来生成高风险仿真场景。所提出的SG-CADVLM框架集成了上下文感知解码与多模态输入处理（事故报告和路网图），以缓解视觉语言模型的幻觉和上下文抑制问题，从而能同时生成真实的路网几何和车辆轨迹。实验结果表明，该方法生成关键风险场景的比例达到84.4%，相比基线方法的12.5%提升了469%，同时能产生可用于测试的可执行仿真。</div>
</details>
</div>
<div class="card">
<div class="title">Close-Fitting Dressing Assistance Based on State Estimation of Feet and Garments with Semantic-based Visual Attention</div>
<div class="meta-line">Authors: Takuma Tsukakoshi, Tamon Miyake, Tetsuya Ogata, Yushi Wang, Takumi Akaishi, Shigeki Sugano</div>
<div class="meta-line">First: 2025-05-06T10:28:39+00:00 · Latest: 2026-01-26T11:10:17+00:00</div>
<div class="meta-line">Comments: Accepted at RA-L, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.03400v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.03400v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the population continues to age, a shortage of caregivers is expected in the future. Dressing assistance, in particular, is crucial for opportunities for social participation. Especially dressing close-fitting garments, such as socks, remains challenging due to the need for fine force adjustments to handle the friction or snagging against the skin, while considering the shape and position of the garment. This study introduces a method uses multi-modal information including not only robot&#x27;s camera images, joint angles, joint torques, but also tactile forces for proper force interaction that can adapt to individual differences in humans. Furthermore, by introducing semantic information based on object concepts, rather than relying solely on RGB data, it can be generalized to unseen feet and background. In addition, incorporating depth data helps infer relative spatial relationship between the sock and the foot. To validate its capability for semantic object conceptualization and to ensure safety, training data were collected using a mannequin, and subsequent experiments were conducted with human subjects. In experiments, the robot successfully adapted to previously unseen human feet and was able to put socks on 10 participants, achieving a higher success rate than Action Chunking with Transformer and Diffusion Policy. These results demonstrate that the proposed model can estimate the state of both the garment and the foot, enabling precise dressing assistance for close-fitting garments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语义视觉注意力的足部与衣物状态估计的紧身衣物穿着辅助</div>
<div class="mono" style="margin-top:8px">随着人口持续老龄化，未来预计将出现护理人员短缺。穿着辅助对于社会参与机会尤为关键，尤其是紧身衣物（如袜子）的穿着，由于需要精细的力调节以处理与皮肤的摩擦或勾挂，同时考虑衣物的形状和位置，仍具挑战性。本研究提出一种方法，利用多模态信息（包括机器人摄像头图像、关节角度、关节扭矩及触觉力）实现适应个体差异的适当力交互。此外，通过引入基于物体概念的语义信息（而非仅依赖RGB数据），该方法可泛化至未见过的足部与背景。结合深度数据有助于推断袜子与足部间的相对空间关系。为验证其语义物体概念化能力并确保安全性，训练数据使用人体模型收集，后续实验在人类受试者中进行。实验中，机器人成功适应了先前未见的人类足部，并为10名参与者穿上了袜子，其成功率高于基于Transformer的动作分块与扩散策略方法。结果表明，所提模型能估计衣物与足部状态，实现对紧身衣物的精准穿着辅助。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the caregiver shortage and support social participation for the aging population, this research tackles the challenging task of robotic close-fitting garment dressing, such as putting on socks, which requires fine force adjustments to manage friction. The method employs multi-modal state estimation, integrating robot camera images, joint angles, joint torques, and tactile forces, and enhances generalization by incorporating semantic object concepts and depth data to understand garment-foot spatial relationships without relying solely on RGB data. Experimental validation involved training with a mannequin for safety and testing with human subjects, where the robot successfully adapted to unseen feet and achieved a higher sock-dressing success rate across 10 participants compared to Action Chunking with Transformer and Diffusion Policy baselines, demonstrating effective state estimation for precise assistance.</div>
<div class="mono" style="margin-top:8px">针对人口老龄化和护理人员短缺的问题，本研究旨在解决机器人辅助穿着紧身衣物（如袜子）的挑战，这需要精细的力调节以处理摩擦并适应个体差异。方法整合了包括相机图像、关节角度、关节扭矩和触觉力在内的多模态感官数据，并通过引入语义物体概念和深度数据来增强泛化能力，以推断衣物与脚部之间的空间关系。实验验证使用人体模型进行训练并在人类受试者上测试，机器人成功适应了未见过的脚部，并在穿袜任务中取得了比Action Chunking with Transformer和Diffusion Policy更高的成功率，证明了其能有效估计衣物和脚部状态，实现精确的穿衣辅助。</div>
</details>
</div>
<div class="card">
<div class="title">TC-IDM: Grounding Video Generation for Executable Zero-shot Robot Motion</div>
<div class="meta-line">Authors: Weishi Mi, Yong Bao, Xiaowei Chi, Xiaozhu Ju, Zhiyuan Qin, Kuangzhi Ge, Kai Tang, Peidong Jia, Shanghang Zhang, Jian Tang</div>
<div class="meta-line">First: 2026-01-26T10:06:56+00:00 · Latest: 2026-01-26T10:06:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18323v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18323v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The vision-language-action (VLA) paradigm has enabled powerful robotic control by leveraging vision-language models, but its reliance on large-scale, high-quality robot data limits its generalization. Generative world models offer a promising alternative for general-purpose embodied AI, yet a critical gap remains between their pixel-level plans and physically executable actions.
  To this end, we propose the Tool-Centric Inverse Dynamics Model (TC-IDM). By focusing on the tool&#x27;s imagined trajectory as synthesized by the world model, TC-IDM establishes a robust intermediate representation that bridges the gap between visual planning and physical control.
  TC-IDM extracts the tool&#x27;s point cloud trajectories via segmentation and 3D motion estimation from generated videos. Considering diverse tool attributes, our architecture employs decoupled action heads to project these planned trajectories into 6-DoF end-effector motions and corresponding control signals.
  This plan-and-translate paradigm not only supports a wide range of end-effectors but also significantly improves viewpoint invariance. Furthermore, it exhibits strong generalization capabilities across long-horizon and out-of-distribution tasks, including interacting with deformable objects.
  In real-world evaluations, the world model with TC-IDM achieves an average success rate of 61.11 percent, with 77.7 percent on simple tasks and 38.46 percent on zero-shot deformable object tasks. It substantially outperforms end-to-end VLA-style baselines and other inverse dynamics models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TC-IDM：面向可执行零样本机器人运动的视频生成基础模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）范式通过利用视觉语言模型实现了强大的机器人控制，但其对大规模高质量机器人数据的依赖限制了泛化能力。生成式世界模型为通用具身AI提供了有前景的替代方案，但其像素级规划与物理可执行动作之间仍存在关键鸿沟。为此，我们提出了工具中心化逆动力学模型（TC-IDM）。该模型聚焦于世界模型合成的工具想象轨迹，建立了一个稳健的中间表示层，从而弥合视觉规划与物理控制之间的差距。TC-IDM通过从生成视频中进行分割和三维运动估计来提取工具点云轨迹。针对多样化的工具属性，我们的架构采用解耦动作头，将这些规划轨迹映射为六自由度末端执行器运动及相应控制信号。这种“规划-转换”范式不仅支持广泛的末端执行器类型，还显著提升了视角不变性。此外，该模型在长时程任务和分布外任务（包括与可变形物体交互）中展现出强大的泛化能力。在真实世界评估中，搭载TC-IDM的世界模型实现了61.11%的平均成功率，其中简单任务达77.7%，零样本可变形物体任务达38.46%，显著优于端到端VLA基线及其他逆动力学模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To bridge the gap between pixel-level plans from generative world models and physically executable robot actions, this research introduces the Tool-Centric Inverse Dynamics Model (TC-IDM). The method extracts the imagined 3D trajectory of a tool from generated videos via segmentation and motion estimation, then uses decoupled action heads to translate this into executable 6-DoF end-effector motions. Experimental results show that this plan-and-translate approach achieves strong generalization, with the system reaching an average 61.11% success rate in real-world tasks, significantly outperforming end-to-end vision-language-action baselines and other inverse dynamics models.</div>
<div class="mono" style="margin-top:8px">为弥合生成世界模型的像素级规划与物理可执行机器人动作之间的差距，本研究提出了以工具为中心的反向动力学模型（TC-IDM）。该方法通过分割和运动估计从生成视频中提取工具的想象三维轨迹，然后使用解耦的动作头将该轨迹转换为可执行的六自由度末端执行器运动。实验结果表明，这种先规划后转换的范式显著提升了视角不变性和泛化能力，在真实世界任务中取得了61.11%的平均成功率，大幅优于端到端的视觉-语言-动作基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis</div>
<div class="meta-line">Authors: Yuan Gao, Mattia Piccinini, Yuchen Zhang, Dingrui Wang, Korbinian Moller, Roberto Brusnicki, Baha Zarrouki, Alessio Gambi, Jan Frederik Totz, Kai Storms, Steven Peters, Andrea Stocco, Bassam Alrifaee, Marco Pavone, Johannes Betz</div>
<div class="meta-line">First: 2025-06-13T07:25:59+00:00 · Latest: 2026-01-26T09:55:34+00:00</div>
<div class="meta-line">Comments: Final version (Accepted by the IEEE Open Journal of Intelligent Transportation Systems)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.11526v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.11526v3">PDF</a> · <a href="https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自动驾驶中的基础模型：场景生成与场景分析综述</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆在复杂环境中的安全导航依赖于处理广泛多样且罕见的驾驶场景。基于仿真和场景的测试已成为自动驾驶系统开发和验证的关键方法。传统场景生成依赖基于规则的系统、知识驱动模型和数据驱动合成，往往产生有限多样性且不切实际的安全关键案例。随着基础模型——新一代预训练通用人工智能模型——的出现，开发者能够处理异构输入（如自然语言、传感器数据、高精地图和控制动作），实现复杂驾驶场景的合成与解析。本文综述了截至2025年5月基础模型在自动驾驶场景生成与分析中的应用。我们提出了一个统一分类体系，涵盖用于自动驾驶场景生成与分析的大语言模型、视觉语言模型、多模态大语言模型、扩散模型和世界模型。此外，我们回顾了相关方法论、开源数据集、仿真平台和基准挑战，并考察了专门针对场景生成与分析设计的评估指标。最后，本文总结了当前面临的开放挑战与研究问题，并展望了未来有前景的研究方向。所有被综述的论文均列于持续维护的存储库中，该库包含补充材料，可通过 https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis 访问。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey addresses the need for diverse and realistic driving scenarios to ensure autonomous vehicle safety, noting that traditional rule-based or data-driven methods often lack diversity and fail to produce critical edge cases. It reviews how foundation models—including large language models, vision-language models, multimodal models, diffusion models, and world models—can process heterogeneous inputs like natural language, sensor data, and maps to generate and analyze complex driving scenarios. The study systematically examines methodologies, datasets, simulation platforms, benchmarks, and evaluation metrics, concluding with identified open challenges and future research directions.</div>
<div class="mono" style="margin-top:8px">本综述针对自动驾驶车辆在复杂环境中安全导航的需求，指出传统基于规则或数据驱动的方法在生成多样且真实的驾驶场景方面存在局限，特别是难以产生关键的安全案例。文章探讨了基础模型，包括大语言模型、视觉语言模型、多模态大语言模型、扩散模型和世界模型，如何利用自然语言、传感器数据等异构输入来生成和分析复杂驾驶场景。该调查系统地对这些模型进行了分类，审查了相关方法、开源数据集、仿真平台、基准挑战和专用评估指标，并指出了该领域面临的开放挑战和未来的研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Quest2ROS2: A ROS 2 Framework for Bi-manual VR Teleoperation</div>
<div class="meta-line">Authors: Jialong Li, Zhenguo Wang, Tianci Wang, Maj Stenmark, Volker Krueger</div>
<div class="meta-line">First: 2026-01-26T09:19:16+00:00 · Latest: 2026-01-26T09:19:16+00:00</div>
<div class="meta-line">Comments: HRI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18289v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18289v1">PDF</a> · <a href="https://github.com/Taokt/Quest2ROS2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quest2ROS2 is an open-source ROS2 framework for bi-manual teleoperation designed to scale robot data collection. Extending Quest2ROS, it overcomes workspace limitations via relative motion-based control, calculating robot movement from VR controller pose changes to enable intuitive, pose-independent operation. The framework integrates essential usability and safety features, including real-time RViz visualization, streamlined gripper control, and a pause-and-reset function for smooth transitions. We detail a modular architecture that supports &quot;Side-by-Side&quot; and &quot;Mirror&quot; control modes to optimize operator experience across diverse platforms. Code is available at: https://github.com/Taokt/Quest2ROS2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Quest2ROS2：面向双手VR遥操作的ROS 2框架</div>
<div class="mono" style="margin-top:8px">Quest2ROS2是一个用于双手遥操作的开源ROS2框架，旨在扩展机器人数据采集规模。作为Quest2ROS的延伸，该框架通过基于相对运动的控制方式突破工作空间限制，依据VR控制器姿态变化计算机器人运动轨迹，实现直观且与姿态无关的操作。框架集成了实时RViz可视化、简化的夹爪控制、暂停重置功能等核心可用性与安全特性，确保流畅的任务切换。我们详细阐述了支持&quot;并列式&quot;与&quot;镜像式&quot;控制模式的模块化架构，以优化跨平台操作体验。代码已开源：https://github.com/Taokt/Quest2ROS2。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To scale robot data collection through intuitive teleoperation, this work introduces Quest2ROS2, an open-source ROS2 framework for bi-manual control that overcomes workspace limitations by using relative motion-based control, calculating robot movements from VR controller pose changes. The method features a modular architecture supporting &#x27;Side-by-Side&#x27; and &#x27;Mirror&#x27; control modes, integrated with real-time visualization, streamlined gripper control, and a pause-and-reset function for usability and safety. Experimental validation demonstrates that the framework enables pose-independent, intuitive operation across diverse robotic platforms.</div>
<div class="mono" style="margin-top:8px">为通过直观遥操作扩展机器人数据收集规模，本研究提出了Quest2ROS2，一个用于双手控制的开源ROS 2框架。该方法通过基于相对运动的控制来克服工作空间限制，根据VR控制器姿态变化计算机器人运动，并采用支持“并排”和“镜像”控制模式的模块化架构。实验集成展示了关键的可用性与安全功能，包括实时RViz可视化、简化的夹爪控制以及用于平滑操作的暂停与重置功能。</div>
</details>
</div>
<div class="card">
<div class="title">ORION: Option-Regularized Deep Reinforcement Learning for Cooperative Multi-Agent Online Navigation</div>
<div class="meta-line">Authors: Shizhe Zhang, Jingsong Liang, Zhitao Zhou, Shuhan Ye, Yizhuo Wang, Ming Siang Derek Tan, Jimmy Chiun, Yuhong Cao, Guillaume Sartoretti</div>
<div class="meta-line">First: 2026-01-03T10:55:10+00:00 · Latest: 2026-01-26T08:48:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01155v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01155v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing methods for multi-agent navigation typically assume fully known environments, offering limited support for partially known scenarios such as warehouses or factory floors. There, agents may need to plan trajectories that balance their own path optimality with their ability to collect and share information about the environment that can help their teammates reach their own goals. To these ends, we propose ORION, a novel deep reinforcement learning framework for cooperative multi-agent online navigation in partially known environments. Starting from an imperfect prior map, ORION trains agents to make decentralized decisions, coordinate to reach their individual targets, and actively reduce map uncertainty by sharing online observations in a closed perception-action loop. We first design a shared graph encoder that fuses prior map with online perception into a unified representation, providing robust state embeddings under dynamic map discrepancies. At the core of ORION is an option-critic framework that learns to reason about a set of high-level cooperative modes that translate into sequences of low-level actions, allowing agents to switch between individual navigation and team-level exploration adaptively. We further introduce a dual-stage cooperation strategy that enables agents to assist teammates under map uncertainty, thereby reducing the overall makespan. Across extensive maze-like maps and large-scale warehouse environments, our simulation results show that ORION achieves high-quality, real-time decentralized cooperation over varying team sizes, outperforming state-of-the-art classical and learning-based baselines. Finally, we validate ORION on physical robot teams, demonstrating its robustness and practicality for real-world cooperative navigation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORION：面向协作多智能体在线导航的选项正则化深度强化学习</div>
<div class="mono" style="margin-top:8px">现有多智能体导航方法通常假设环境完全已知，对仓库或工厂车间等部分已知场景的支持有限。在这些场景中，智能体需规划能平衡自身路径最优性与环境信息收集共享能力的轨迹，以协助队友达成各自目标。为此，我们提出ORION——一种面向部分已知环境中协作多智能体在线导航的新型深度强化学习框架。基于不完善的先验地图，ORION训练智能体进行分散式决策，协调抵达各自目标，并通过在闭环感知-行动循环中共享在线观测主动降低地图不确定性。我们首先设计共享图编码器，将先验地图与在线感知融合为统一表征，在动态地图差异下提供鲁棒的状态嵌入。ORION的核心是选项-评判框架，通过学习推理一组可转化为底层动作序列的高层协作模式，使智能体能自适应切换个体导航与团队级探索。我们进一步提出双阶段协作策略，使智能体能在地图不确定条件下协助队友，从而缩短整体任务完成时间。在多样化迷宫式地图和大规模仓库环境中的仿真结果表明，ORION在不同团队规模下均能实现高质量实时分散协作，性能优于当前最先进的经典方法与学习基线。最后，我们在实体机器人团队上验证了ORION，证明了其在现实世界协作导航中的鲁棒性与实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the challenge of cooperative multi-agent navigation in partially known environments like warehouses, where agents must balance individual path optimality with collective information gathering, this paper proposes ORION, a deep reinforcement learning framework. The method employs a shared graph encoder to fuse prior maps with online perceptions and an option-critic framework that learns high-level cooperative modes, enabling adaptive switching between navigation and exploration, alongside a dual-stage cooperation strategy for assisting teammates under uncertainty. Experimental results in simulated maze and warehouse settings show that ORION achieves high-quality, real-time decentralized cooperation across varying team sizes, outperforming existing baselines, and its robustness is further validated on physical robot teams.</div>
<div class="mono" style="margin-top:8px">针对部分已知环境中多智能体导航需平衡个体路径最优与协作信息收集的问题，本文提出了ORION深度强化学习框架。该方法采用共享图编码器融合先验地图与在线感知，并利用选项-评论家框架学习可自适应切换导航与探索的高层协作模式，同时引入双阶段协作策略以在不确定环境下协助队友。在模拟迷宫和仓库环境中的实验结果表明，ORION在不同团队规模下实现了高质量、实时的去中心化协作，性能优于现有基线方法，并在物理机器人团队上验证了其鲁棒性和实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Safe Learning for Contact-Rich Robot Tasks: A Survey from Classical Learning-Based Methods to Safe Foundation Models</div>
<div class="meta-line">Authors: Heng Zhang, Rui Dai, Gokhan Solak, Pokuang Zhou, Yu She, Arash Ajoudani</div>
<div class="meta-line">First: 2025-12-10T21:01:02+00:00 · Latest: 2026-01-26T07:32:41+00:00</div>
<div class="meta-line">Comments: version 2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11908v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11908v2">PDF</a> · <a href="https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project">Code1</a> · <a href="https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact-rich tasks pose significant challenges for robotic systems due to inherent uncertainty, complex dynamics, and the high risk of damage during interaction. Recent advances in learning-based control have shown great potential in enabling robots to acquire and generalize complex manipulation skills in such environments, but ensuring safety, both during exploration and execution, remains a critical bottleneck for reliable real-world deployment. This survey provides a comprehensive overview of safe learning-based methods for robot contact-rich tasks. We categorize existing approaches into two main domains: safe exploration and safe execution. We review key techniques, including constrained reinforcement learning, risk-sensitive optimization, uncertainty-aware modeling, control barrier functions, and model predictive safety shields, and highlight how these methods incorporate prior knowledge, task structure, and online adaptation to balance safety and efficiency. A particular emphasis of this survey is on how these safe learning principles extend to and interact with emerging robotic foundation models, especially vision-language models (VLMs) and vision-language-action models (VLAs), which unify perception, language, and control for contact-rich manipulation. We discuss both the new safety opportunities enabled by VLM/VLA-based methods, such as language-level specification of constraints and multimodal grounding of safety signals, and the amplified risks and evaluation challenges they introduce. Finally, we outline current limitations and promising future directions toward deploying reliable, safety-aligned, and foundation-model-enabled robots in complex contact-rich environments. More details and materials are available at our \href{ https://github.com/jack-sherman01/Awesome-Learning4Safe-Contact-rich-tasks}{Project GitHub Repository}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>接触密集型机器人任务的安全学习：从经典学习方法到安全基础模型的综述</div>
<div class="mono" style="margin-top:8px">接触密集型任务因固有的不确定性、复杂动力学特性及交互过程中的高损伤风险，对机器人系统构成重大挑战。基于学习的控制方法近期取得显著进展，展现出使机器人在此类环境中获取并泛化复杂操作技能的潜力，但如何在探索与执行过程中确保安全性，仍是实现可靠实际部署的关键瓶颈。本综述系统梳理了面向机器人接触密集型任务的安全学习方法，将现有研究归纳为安全探索与安全执行两大领域，重点评述了约束强化学习、风险敏感优化、不确定性感知建模、控制屏障函数及模型预测安全屏障等关键技术，并阐释这些方法如何融合先验知识、任务结构与在线适应机制以平衡安全性与效率。综述特别关注这些安全学习原则如何延伸至新兴的机器人基础模型（尤其是视觉语言模型与视觉语言动作模型），并探讨其交互机制——这类模型通过统一感知、语言与控制能力来处理接触密集型操作任务。我们既分析了基于视觉语言模型/视觉语言动作模型方法带来的新安全机遇（如语言级约束规范与安全信号的多模态锚定），也剖析了其引发的风险放大与评估挑战。最后，本文总结了当前局限性与未来发展方向，以推动在复杂接触密集型环境中部署可靠、安全对齐且具备基础模型能力的机器人系统。更多资料详见项目GitHub仓库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey addresses the critical need for safety in learning-based robotic control for contact-rich tasks, where uncertainty and complex dynamics pose high risks of damage. It systematically reviews methods categorized into safe exploration and safe execution, covering techniques like constrained reinforcement learning, control barrier functions, and model predictive safety shields that integrate prior knowledge and online adaptation to balance safety with performance. The analysis extends to emerging foundation models such as vision-language-action models, highlighting their potential for language-specified safety constraints while noting amplified risks and evaluation challenges, ultimately outlining future directions for reliable deployment.</div>
<div class="mono" style="margin-top:8px">本综述针对接触式机器人任务中基于学习的控制方法的安全需求，其中不确定性和复杂动力学存在损坏风险。它系统性地回顾了分为安全探索和安全执行的方法，涵盖了约束强化学习、控制屏障函数和模型预测安全防护等技术，这些技术整合先验知识和在线适应以平衡安全与性能。分析进一步扩展到新兴的基础模型如视觉-语言-动作模型，强调了其通过语言指定安全约束的潜力，同时指出了放大的风险与评估挑战，最终为可靠部署指明了未来方向。</div>
</details>
</div>
<div class="card">
<div class="title">Contact SLAM: An Active Tactile Exploration Policy Based on Physical Reasoning Utilized in Robotic Fine Blind Manipulation Tasks</div>
<div class="meta-line">Authors: Gaozhao Wang, Xing Liu, Zhenduo Ye, Zhengxiong Liu, Panfeng Huang</div>
<div class="meta-line">First: 2025-12-11T09:59:08+00:00 · Latest: 2026-01-26T07:20:46+00:00</div>
<div class="meta-line">Comments: 9 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10481v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10481v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact-rich manipulation is difficult for robots to execute and requires accurate perception of the environment. In some scenarios, vision is occluded. The robot can then no longer obtain real-time scene state information through visual feedback. This is called ``blind manipulation&quot;. In this manuscript, a novel physically-driven contact cognition method, called ``Contact SLAM&quot;, is proposed. It estimates the state of the environment and achieves manipulation using only tactile sensing and prior knowledge of the scene. To maximize exploration efficiency, this manuscript also designs an active exploration policy. The policy gradually reduces uncertainties in the manipulation scene. The experimental results demonstrated the effectiveness and accuracy of the proposed method in several contact-rich tasks, including the difficult and delicate socket assembly task and block-pushing task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>接触SLAM：一种基于物理推理的主动触觉探索策略及其在机器人精细盲操作任务中的应用</div>
<div class="mono" style="margin-top:8px">接触密集型操作对机器人执行具有挑战性，需要精确的环境感知。在某些场景中，视觉信息会被遮挡，机器人无法通过视觉反馈获取实时场景状态，这种情况称为“盲操作”。本文提出一种名为“接触SLAM”的新型物理驱动接触感知方法，仅通过触觉传感和场景先验知识即可估计环境状态并完成操作。为最大化探索效率，本文还设计了主动探索策略，逐步降低操作场景中的不确定性。实验结果表明，该方法在包括高难度精细插接装配任务和方块推动任务在内的多种接触密集型任务中均表现出有效性和精确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of robotic manipulation in vision-occluded scenarios, known as blind manipulation, where robots lack real-time visual feedback. The authors propose Contact SLAM, a physically-driven contact cognition method that estimates environmental state using only tactile sensing and prior scene knowledge, complemented by an active exploration policy to efficiently reduce scene uncertainty. Experimental validation on contact-rich tasks, including delicate socket assembly and block-pushing, demonstrates the method&#x27;s effectiveness and accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人视觉被遮挡的“盲操作”场景，其中机器人无法通过视觉反馈获取实时环境信息。作者提出了Contact SLAM，一种基于物理推理的方法，仅利用触觉感知和先验场景知识来估计环境状态，并设计了一种主动探索策略以高效降低操作场景的不确定性。在包括精细插座装配和推块任务在内的多个接触密集型任务上的实验结果验证了该方法的有效性和准确性。</div>
</details>
</div>
<div class="card">
<div class="title">EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks</div>
<div class="meta-line">Authors: Joohwan Seo, Arvind Kruthiventy, Soomi Lee, Megan Teng, Seoyeon Choi, Xiang Zhang, Jongeun Choi, Roberto Horowitz</div>
<div class="meta-line">Venue: RSS</div>
<div class="meta-line">First: 2025-07-15T03:45:26+00:00 · Latest: 2026-01-26T06:44:47+00:00</div>
<div class="meta-line">Comments: Submitted to RSS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10961v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.10961v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a framework for learning vision-based robotic policies for contact-rich manipulation tasks that generalize spatially across task configurations. We focus on achieving robust spatial generalization of the policy for the peg-in-hole (PiH) task trained from a small number of demonstrations. We propose EquiContact, a hierarchical policy composed of a high-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF) and a novel low-level compliant visuomotor policy (Geometric Compliant ACT, G-CompACT). G-CompACT operates using only localized observations (geometrically consistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB images) and produces actions defined in the end-effector frame. Through these design choices, we show that the entire EquiContact pipeline is SE(3)-equivariant, from perception to force control. We also outline three key components for spatially generalizable contact-rich policies: compliance, localized policies, and induced equivariance. Real-world experiments on PiH, screwing, and surface wiping tasks demonstrate a near-perfect success rate and robust generalization to unseen spatial configurations, validating the proposed framework and principles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EquiContact：一种用于空间可泛化接触丰富任务的分层SE(3)视觉到力等变策略</div>
<div class="mono" style="margin-top:8px">本文提出了一种学习基于视觉的机器人策略的框架，用于接触丰富的操作任务，该策略能在不同任务配置间实现空间泛化。我们专注于通过少量演示训练，实现针对插孔任务策略的鲁棒空间泛化。我们提出了EquiContact，一种由高层视觉规划器（扩散等变描述符场，Diff-EDF）和新型低层顺应性视觉运动策略（几何顺应ACT，G-CompACT）组成的分层策略。G-CompACT仅使用局部化观测（几何一致误差向量、力-力矩读数及腕部RGB图像）进行操作，并在末端执行器坐标系中生成动作。通过这些设计选择，我们展示了整个EquiContact流程从感知到力控制均具有SE(3)等变性。我们还概述了空间可泛化接触丰富策略的三个关键组成部分：顺应性、局部化策略及诱导等变性。在插孔、拧螺丝和表面擦拭任务上的真实世界实验展示了接近完美的成功率及对未见空间配置的鲁棒泛化能力，验证了所提出的框架与原则。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of achieving robust spatial generalization for vision-based robotic policies in contact-rich manipulation tasks, such as peg-in-hole, with limited demonstrations. The proposed EquiContact framework employs a hierarchical SE(3)-equivariant policy, consisting of a high-level vision planner (Diff-EDF) and a novel low-level compliant visuomotor policy (G-CompACT) that uses localized observations like geometrically consistent error vectors, force-torque readings, and wrist-mounted RGB images to generate end-effector frame actions. Experimental results on peg-in-hole, screwing, and surface wiping tasks demonstrate near-perfect success rates and robust generalization to unseen spatial configurations, validating the framework&#x27;s design principles of compliance, localized policies, and induced equivariance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决接触密集型操作任务（如轴孔装配）中，基于视觉的机器人策略在有限演示下实现鲁棒空间泛化的挑战。所提出的方法EquiContact是一种分层SE(3)等变策略，包含一个高层视觉规划器（Diff-EDF）和一个新颖的低层顺应性视觉运动控制器（G-CompACT），该控制器利用局部几何、力和视觉观测来生成末端执行器动作。在轴孔装配、拧螺丝和表面擦拭任务上的实验结果表明，该方法取得了接近完美的成功率，并对未见过的空间配置表现出强大的泛化能力，从而验证了其顺应性、局部化和等变性的设计原则。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
