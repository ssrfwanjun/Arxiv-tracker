<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-04 06:33</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260204_0633</div>
    <div class="row"><div class="card">
<div class="title">Flow Policy Gradients for Robot Control</div>
<div class="meta-line">Authors: Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong, Carmelo Sferrazza, Yi Ma, Rocky Duan, Pieter Abbeel, Guanya Shi, Karen Liu, Angjoo Kanazawa</div>
<div class="meta-line">First: 2026-02-02T18:56:49+00:00 · Latest: 2026-02-02T18:56:49+00:00</div>
<div class="meta-line">Comments: Project webpage: https://hongsukchoi.github.io/fpo-control</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02481v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02481v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hongsukchoi.github.io/fpo-control">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人控制的流策略梯度方法</div>
<div class="mono" style="margin-top:8px">基于似然的策略梯度方法是当前从奖励信号训练机器人控制策略的主流方法。这类方法依赖可微分的动作似然函数，导致策略输出被限制在高斯分布等简单分布形式。本研究展示了流匹配策略梯度——一种绕过似然计算的新框架——如何在具有挑战性的机器人控制场景中有效训练和微调更具表达能力的策略。我们提出了改进的目标函数，成功应用于足式机器人运动、人形机器人动作跟踪和操作任务，并在两台人形机器人上实现了稳健的仿真到现实迁移。通过消融实验和训练动态分析，结果表明：策略可利用流表示进行探索性训练，同时在微调鲁棒性方面优于基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of likelihood-based policy gradient methods in robot control, which rely on differentiable action likelihoods and thus restrict policies to simple distributions like Gaussians. The method introduces flow matching policy gradients, a framework that bypasses likelihood computation, and proposes an improved objective to train more expressive policies. Experimental results demonstrate effectiveness in legged locomotion, humanoid motion tracking, and manipulation tasks, with successful sim-to-real transfer on two humanoid robots, showing enhanced exploration and fine-tuning robustness compared to baselines.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于基于似然的策略梯度方法在机器人控制中的局限性，这些方法依赖于可微的动作似然，从而将策略输出限制为高斯分布等简单分布。方法采用流匹配策略梯度框架，绕过似然计算，并提出了改进的目标函数来训练更具表达能力的策略。实验结果表明，该方法在腿部运动、人形机器人运动跟踪和操作任务中表现有效，并在两个仿人机器人上实现了成功的仿真到现实迁移，显示出相比基线方法更强的探索能力和微调鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos</div>
<div class="meta-line">Authors: Yinhuai Wang, Qihan Zhao, Yuen Fui Lau, Runyi Yu, Hok Wai Tsui, Qifeng Chen, Jingbo Wang, Jiangmiao Pang, Ping Tan</div>
<div class="meta-line">First: 2026-02-02T18:53:01+00:00 · Latest: 2026-02-02T18:53:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanX：基于人类视频实现类人机器人敏捷且泛化的交互技能</div>
<div class="mono" style="margin-top:8px">使类人机器人执行敏捷自适应的交互任务一直是机器人学的核心挑战。现有方法受限于真实交互数据的稀缺或需要精细的任务特定奖励设计，制约了其扩展性。为缩小这一差距，我们提出HumanX——一个全栈框架，可将人类视频转化为无需任务特定奖励、适用于类人机器人的可泛化现实交互技能。HumanX包含两个协同设计的组件：XGen（数据生成管道，可从视频合成多样且物理合理的机器人交互数据，支持可扩展的数据增强）和XMimic（统一模仿学习框架，用于学习可泛化的交互技能）。在篮球、足球、羽毛球、货物拾取及反应性对抗五个不同领域的评估中，HumanX成功习得10种技能，并零样本迁移至实体Unitree G1类人机器人。习得能力包括复杂动作（如无外部感知的假动作转身后仰跳投）和交互任务（如从单段视频演示中学习连续超过10个周期的人机传接序列）。实验表明，HumanX的泛化成功率较现有方法提升8倍以上，为学习多样化的现实机器人交互技能提供了可扩展且任务无关的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the limitations of scarce interaction data and task-specific reward engineering in enabling agile humanoid interactions, this paper introduces HumanX, a framework that compiles human videos into generalizable robot skills. The method integrates XGen, a pipeline for synthesizing diverse, physically plausible robot data from videos with scalable augmentation, and XMimic, a unified imitation learning framework for skill acquisition. Experimental results across five domains, including basketball and football, show that HumanX acquires 10 skills and transfers them zero-shot to a physical humanoid, achieving complex maneuvers like pump-fake jumpshots and sustained human-robot passing, with over 8 times higher generalization success than prior methods.</div>
<div class="mono" style="margin-top:8px">为解决人形机器人交互中数据稀缺和任务特定奖励设计带来的可扩展性限制，本文提出了HumanX框架，将人类视频编译为可泛化的机器人技能。该方法整合了XGen（从视频合成多样且物理合理的机器人交互数据并支持可扩展增强的流水线）和XMimic（用于技能学习的统一模仿学习框架）。在篮球、足球、羽毛球、货物拾取和反应性格斗五个领域的实验结果表明，HumanX成功习得了10种技能，并零样本迁移到物理Unitree G1人形机器人上，实现了如假动作转身后仰跳投等复杂操作以及持续超过10个周期的人机传球序列，其泛化成功率比先前方法提高了8倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">TIC-VLA: A Think-in-Control Vision-Language-Action Model for Robot Navigation in Dynamic Environments</div>
<div class="meta-line">Authors: Zhiyu Huang, Yun Zhang, Johnson Liu, Rui Song, Chen Tang, Jiaqi Ma</div>
<div class="meta-line">First: 2026-02-02T18:47:49+00:00 · Latest: 2026-02-02T18:47:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02459v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02459v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ucla-mobility.github.io/TIC-VLA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robots in dynamic, human-centric environments must follow language instructions while maintaining real-time reactive control. Vision-language-action (VLA) models offer a promising framework, but they assume temporally aligned reasoning and control, despite semantic inference being inherently delayed relative to real-time action. We introduce Think-in-Control (TIC)-VLA, a latency-aware framework that explicitly models delayed semantic reasoning during action generation. TIC-VLA defines a delayed semantic-control interface that conditions action generation on delayed vision-language semantic states and explicit latency metadata, in addition to current observations, enabling policies to compensate for asynchronous reasoning. We further propose a latency-consistent training pipeline that injects reasoning inference delays during imitation learning and online reinforcement learning, aligning training with asynchronous deployment. To support realistic evaluation, we present DynaNav, a physics-accurate, photo-realistic simulation suite for language-guided navigation in dynamic environments. Extensive experiments in simulation and on a real robot show that TIC-VLA consistently outperforms prior VLA models while maintaining robust real-time control under multi-second reasoning latency. Project website: https://ucla-mobility.github.io/TIC-VLA/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TIC-VLA：面向动态环境机器人导航的思控一体视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">在动态、以人为中心的环境中，机器人需遵循语言指令并保持实时反应控制。视觉-语言-动作（VLA）模型提供了有前景的框架，但其假设推理与控制在时间上完全同步，而语义推理本质上存在相对于实时动作的延迟。本文提出思控一体（TIC）-VLA框架，该延迟感知框架在动作生成过程中显式建模延迟的语义推理。TIC-VLA定义了一个延迟语义-控制接口，除当前观测外，还将延迟的视觉-语言语义状态与显式延迟元数据作为动作生成的条件，使策略能够补偿异步推理。我们进一步提出延迟一致性训练流程，在模仿学习与在线强化学习中注入推理延迟，使训练与异步部署场景对齐。为支持真实评估，我们开发了DynaNav——一个物理精确、照片级真实的动态环境语言导航仿真套件。大量仿真与真实机器人实验表明，TIC-VLA在保持多秒级推理延迟下鲁棒实时控制的同时，性能持续优于现有VLA模型。项目网站：https://ucla-mobility.github.io/TIC-VLA/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the inherent delay between semantic reasoning and real-time reactive control in vision-language-action (VLA) models for robot navigation, this research introduces the Think-in-Control (TIC)-VLA framework. The method explicitly models delayed semantic reasoning by conditioning action generation on both current observations and delayed vision-language semantic states, alongside explicit latency metadata, and employs a latency-consistent training pipeline that injects reasoning delays during imitation and reinforcement learning. Experimental results, validated in a new photo-realistic simulation suite (DynaNav) and on a real robot, demonstrate that TIC-VLA consistently outperforms prior VLA models and maintains robust control under multi-second reasoning latencies.</div>
<div class="mono" style="margin-top:8px">为解决机器人在动态环境中执行语言指令时语义推理固有延迟的挑战，本研究提出了TIC-VLA，一种延迟感知的视觉-语言-动作框架。该方法通过将动作生成同时基于当前观测和带有明确延迟元数据的延迟语义状态，显式建模延迟的语义推理，并采用延迟一致的训练流程，在模仿学习和在线强化学习中注入推理延迟。在新构建的高物理精度、照片级真实感仿真环境（DynaNav）和真实机器人上的大量实验表明，TIC-VLA性能持续优于先前的VLA模型，并能在数秒推理延迟下保持鲁棒的实时控制。</div>
</details>
</div>
<div class="card">
<div class="title">Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning</div>
<div class="meta-line">Authors: Albert Gassol Puigjaner, Angelos Zacharia, Kostas Alexis</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-02T18:47:02+00:00 · Latest: 2026-02-02T18:47:02+00:00</div>
<div class="meta-line">Comments: ICRA 2026, 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02456v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02456v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph&#x27;s semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向任务推理的关系感知分层三维场景图</div>
<div class="mono" style="margin-top:8px">以结构化方式表示和理解三维环境对自主智能体的导航与场景推理至关重要。传统同步定位与建图方法虽能生成度量重建并扩展至度量-语义建图，但缺乏更高层次的抽象与关系推理能力。三维场景图作为一种能捕捉层次结构与物体关系的强大表示方法应运而生。本研究提出一种增强型分层三维场景图，其整合了多抽象层级的开放词汇特征并支持物体关系推理。该方法利用视觉语言模型推断语义关系，并创新性地引入结合大语言模型与视觉语言模型的任务推理模块，以解析场景图的语义与关系信息，使智能体能够进行任务推理并更智能地与环境交互。通过在四足机器人上部署于多环境与任务中验证了本方法的推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of high-level abstraction and relational reasoning in traditional SLally Localization and Mapping (SLAM) and metric-semantic maps, this work proposes an enhanced hierarchical 3D scene graph representation. The method integrates open-vocabulary features across multiple abstraction levels and leverages a Vision Language Model (VLM) to infer semantic relationships between objects. A key innovation is a task reasoning module that combines Large Language Models (LLM) and the VLM to interpret the graph&#x27;s semantic and relational information. Experimental validation on a quadruped robot across multiple environments and tasks demonstrates the system&#x27;s ability to effectively reason about tasks and intelligently interact with the environment.</div>
<div class="mono" style="margin-top:8px">本研究针对传统SLAM与度量语义地图缺乏高层抽象与关系推理能力的问题，提出了一种增强的层次化三维场景图表示方法。该方法在多个抽象层次上整合开放词汇特征，并利用视觉语言模型推断物体间的语义关系；其核心创新是一个结合大语言模型与视觉语言模型的任务推理模块，用于解析场景图的语义与关系信息。在四足机器人上的多环境、多任务实验验证表明，该系统能够有效进行任务推理并实现与环境的智能交互。</div>
</details>
</div>
<div class="card">
<div class="title">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</div>
<div class="meta-line">Authors: Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, Sherry Yang</div>
<div class="meta-line">First: 2026-02-02T18:44:45+00:00 · Latest: 2026-02-02T18:44:45+00:00</div>
<div class="meta-line">Comments: https://world-gymnast.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02454v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02454v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://world-gymnast.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone&#x27;s household.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>World-Gymnast：在世界模型中通过强化学习训练机器人</div>
<div class="mono" style="margin-top:8px">机器人通过与物理世界交互进行学习，从根本上受限于物理交互的成本。两种替代方案——基于专家演示的监督微调（SFT）和基于软件模拟器的强化学习（RL），分别受限于可用专家数据的规模以及操作任务中的仿真到现实差距。随着近期从真实世界视频-动作数据中学习的世界模型的出现，我们提出一个问题：在世界模型中训练策略，是否比监督学习或软件仿真更能有效提升真实机器人的性能？我们提出World-Gymnast方法，该方法通过在动作条件视频世界模型中展开策略，并利用视觉语言模型（VLM）对展开结果进行奖励，从而对视觉-语言-动作（VLA）策略进行强化学习微调。在Bridge机器人实验设置中，World-Gymnast的性能超越SFT高达18倍，超越软件仿真高达2倍。更重要的是，World-Gymnast展现了世界模型强化学习的独特能力，包括：基于世界模型对多样化语言指令和新场景进行训练、在新场景中进行测试时训练，以及在线迭代优化世界模型与策略。我们的结果表明，学习世界模型并在云端训练机器人策略，可能是弥合仅能在演示环境中工作的机器人与适用于任意家庭环境的机器人之间差距的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the high cost and limitations of physical robot interaction, which is bottlenecked by expensive real-world trials, scarce expert demonstration data for supervised fine-tuning, and the sim-to-real gap in software simulators. The method, World-Gymnast, addresses this by performing reinforcement learning fine-tuning of a vision-language-action policy within an action-conditioned video world model learned from real-world data, using a vision-language model to reward policy rollouts. Key experimental results on the Bridge robot setup show that World-Gymnast outperforms supervised fine-tuning by up to 18x and software simulation by up to 2x, while also demonstrating capabilities such as training on diverse language instructions and novel scenes, test-time training, and online iterative improvement of both the world model and policy.</div>
<div class="mono" style="margin-top:8px">该研究的动机在于物理机器人交互的高成本和局限性，以及受限于专家数据的监督微调和因仿真到现实差距而受限的模拟器强化学习。所提出的方法World-Gymnast，通过在动作条件视频世界模型中执行策略，并使用视觉语言模型作为奖励函数评估轨迹，对视觉-语言-动作策略进行强化学习微调。关键实验结果表明，在Bridge机器人设置中，World-Gymnast的性能比监督微调高出多达18倍，比软件模拟高出多达2倍，同时还展示了在多样化语言指令和新场景中训练、测试时训练以及迭代模型改进等能力。</div>
</details>
</div>
<div class="card">
<div class="title">3D Foundation Model-Based Loop Closing for Decentralized Collaborative SLAM</div>
<div class="meta-line">Authors: Pierre-Yves Lajoie, Benjamin Ramtoula, Daniele De Martini, Giovanni Beltrame</div>
<div class="meta-line">First: 2026-02-02T18:30:32+00:00 · Latest: 2026-02-02T18:30:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02430v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02430v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decentralized Collaborative Simultaneous Localization And Mapping (C-SLAM) techniques often struggle to identify map overlaps due to significant viewpoint variations among robots. Motivated by recent advancements in 3D foundation models, which can register images despite large viewpoint differences, we propose a robust loop closing approach that leverages these models to establish inter-robot measurements. In contrast to resource-intensive methods requiring full 3D reconstruction within a centralized map, our approach integrates foundation models into existing SLAM pipelines, yielding scalable and robust multi-robot mapping. Our contributions include: (1) integrating 3D foundation models to reliably estimate relative poses from monocular image pairs within decentralized C-SLAM; (2) introducing robust outlier mitigation techniques critical to the use of these relative poses; and (3) developing specialized pose graph optimization formulations that efficiently resolve scale ambiguities. We evaluate our method against state-of-the-art approaches, demonstrating improvements in localization and mapping accuracy, alongside significant gains in computational and memory efficiency. These results highlight the potential of our approach for deployment in large-scale multi-robot scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于三维基础模型的去中心化协同SLAM闭环检测</div>
<div class="mono" style="margin-top:8px">去中心化协同即时定位与地图构建（C-SLAM）技术常因机器人间视角差异显著而难以识别地图重叠区域。受三维基础模型最新进展的启发——该模型能在视角差异较大时仍实现图像配准，我们提出一种鲁棒的闭环检测方法，利用此类模型建立机器人间的测量关联。与需在集中式地图中进行全三维重建的资源密集型方法不同，本方法将基础模型集成至现有SLAM流程，实现可扩展且鲁棒的多机器人建图。主要贡献包括：（1）在去中心化C-SLAM中集成三维基础模型，通过单目图像对可靠估计相对位姿；（2）引入对使用此类相对位姿至关重要的鲁棒异常值抑制技术；（3）开发专有位姿图优化模型，有效解决尺度歧义问题。实验表明，相较于前沿方法，本方法在定位与建图精度上均有提升，同时显著提高计算与内存效率，凸显了其在大规模多机器人场景中的应用潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Decentralized Collaborative SLAM (C-SLAM) systems often fail to detect map overlaps between robots due to large viewpoint differences. To address this, we propose a robust loop closing method that leverages 3D foundation models, which excel at registering images across significant viewpoint changes, to establish inter-robot measurements. Our approach integrates these models into existing SLAM pipelines, avoiding the need for full 3D reconstruction, and introduces robust outlier mitigation and specialized pose graph optimization to resolve scale ambiguities. Experimental evaluation shows our method improves localization and mapping accuracy while achieving significant gains in computational and memory efficiency compared to state-of-the-art approaches.</div>
<div class="mono" style="margin-top:8px">针对去中心化协同SLAM中因视角差异大而难以识别地图重叠的挑战，本研究提出一种利用3D基础模型的鲁棒闭环方法。该方法将基础模型集成到现有SLAM流程中，从单目图像对可靠估计相对位姿，结合了鲁棒的异常值抑制技术，并采用专门的位姿图优化以解决尺度模糊性。实验评估表明，与先进方法相比，该方法提高了定位与建图精度，同时在计算和内存效率上取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Evolution with Graph-of-Thoughts: A Bi-Level Language Model Framework for Reinforcement Learning</div>
<div class="meta-line">Authors: Changwei Yao, Xinzi Liu, Chen Li, Marios Savvides</div>
<div class="meta-line">First: 2025-09-19T16:35:27+00:00 · Latest: 2026-02-02T18:18:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.16136v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.16136v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing effective reward functions remains a major challenge in reinforcement learning (RL), often requiring considerable human expertise and iterative refinement. Recent advances leverage Large Language Models (LLMs) for automated reward design, but these approaches are limited by hallucinations, reliance on human feedback, and challenges with handling complex, multi-step tasks. In this work, we introduce Reward Evolution with Graph-of-Thoughts (RE-GoT), a novel bi-level framework that enhances LLMs with structured graph-based reasoning and integrates Visual Language Models (VLMs) for automated rollout evaluation. RE-GoT first decomposes tasks into text-attributed graphs, enabling comprehensive analysis and reward function generation, and then iteratively refines rewards using visual feedback from VLMs without human intervention. Extensive experiments on 10 RoboGen and 4 ManiSkill2 tasks demonstrate that RE-GoT consistently outperforms existing LLM-based baselines. On RoboGen, our method improves average task success rates by 32.25%, with notable gains on complex multi-step tasks. On ManiSkill2, RE-GoT achieves an average success rate of 93.73% across four diverse manipulation tasks, significantly surpassing prior LLM-based approaches and even exceeding expert-designed rewards. Our results indicate that combining LLMs and VLMs with graph-of-thoughts reasoning provides a scalable and effective solution for autonomous reward evolution in RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于思维图演化的奖励函数设计：一种用于强化学习的双层语言模型框架</div>
<div class="mono" style="margin-top:8px">设计有效的奖励函数仍是强化学习（RL）中的主要挑战，通常需要大量专业知识和迭代优化。近期研究利用大语言模型（LLMs）实现自动化奖励设计，但这些方法存在幻觉问题、依赖人工反馈，且难以处理复杂多步任务。本文提出基于思维图演化的奖励函数设计方法（RE-GoT），该双层框架通过结构化图推理增强LLMs，并集成视觉语言模型（VLMs）实现自动轨迹评估。RE-GoT首先将任务分解为文本属性图以进行全面分析和奖励函数生成，随后利用VLMs的视觉反馈进行无人干预的迭代优化。在10个RoboGen任务和4个ManiSkill2任务上的实验表明，RE-GoT持续优于现有LLM基线方法：在RoboGen上平均任务成功率提升32.25%，在复杂多步任务中表现尤为突出；在ManiSkill2的四个操作任务中平均成功率高达93.73%，显著超越现有LLM方法甚至部分专家设计的奖励函数。结果表明，将LLMs和VLMs与思维图推理相结合，可为强化学习的自主奖励演化提供可扩展的有效解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of designing effective reward functions in reinforcement learning, which typically demands substantial human expertise and iterative tuning. The proposed method, Reward Evolution with Graph-of-Thoughts (RE-GoT), introduces a bi-level framework that enhances Large Language Models with structured graph-based reasoning to decompose tasks into text-attributed graphs for reward generation, and integrates Visual Language Models to provide automated visual feedback for iterative reward refinement without human intervention. Experimental results on 10 RoboGen and 4 ManiSkill2 tasks show that RE-GoT outperforms existing LLM-based baselines, improving average task success rates by 32.25% on RoboGen and achieving a 93.73% average success rate on ManiSkill2, even surpassing expert-designed rewards in some cases.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决强化学习中设计有效奖励函数的挑战，该过程通常需要大量人类专业知识和迭代调整。提出的方法——基于思维图演化的奖励函数（RE-GoT），采用双层框架，通过结构化图推理增强大语言模型以分解任务并生成奖励函数，并集成视觉语言模型，利用视觉反馈进行自动评估，从而无需人工干预即可迭代优化奖励。在10个RoboGen和4个ManiSkill2任务上的实验结果表明，RE-GoT优于现有基于大语言模型的基线方法，在RoboGen上将平均任务成功率提高了32.25%，在ManiSkill2上实现了93.73%的平均成功率，在某些情况下甚至超过了专家设计的奖励函数。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Monte Carlo Tree Search for Makespan-Efficient Object Rearrangement in Cluttered Spaces</div>
<div class="meta-line">Authors: Hanwen Ren, Junyong Kim, Aathman Tharmasanthiran, Ahmed H. Qureshi</div>
<div class="meta-line">First: 2026-02-02T18:10:45+00:00 · Latest: 2026-02-02T18:10:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02411v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02411v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Object rearrangement planning in complex, cluttered environments is a common challenge in warehouses, households, and rescue sites. Prior studies largely address monotone instances, whereas real-world tasks are often non-monotone-objects block one another and must be temporarily relocated to intermediate positions before reaching their final goals. In such settings, effective multi-agent collaboration can substantially reduce the time required to complete tasks. This paper introduces Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a novel framework for general-purpose makespan-efficient object rearrangement planning in challenging environments. CAM-MCTS combines centralized task assignment-where agents remain aware of each other&#x27;s intended actions to facilitate globally optimized planning-with an asynchronous task execution strategy that enables agents to take on new tasks at appropriate time steps, rather than waiting for others, guided by a one-step look-ahead cost estimate. This design minimizes idle time, prevents unnecessary synchronization delays, and enhances overall system efficiency. We evaluate CAM-MCTS across a diverse set of monotone and non-monotone tasks in cluttered environments, demonstrating consistent reductions in makespan compared to strong baselines. Finally, we validate our approach on a real-world multi-agent system under different configurations, further confirming its effectiveness and robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体蒙特卡洛树搜索在杂乱空间中实现完工时间高效的对象重排</div>
<div class="mono" style="margin-top:8px">在复杂杂乱环境中的对象重排规划是仓库、家庭和救援场所面临的常见挑战。现有研究主要针对单调实例，而现实任务往往是非单调的——对象相互阻挡，必须临时移至中间位置才能到达最终目标。在此类场景中，有效的多智能体协作能显著缩短任务完成时间。本文提出集中式异步多智能体蒙特卡洛树搜索（CAM-MCTS），这是一种用于挑战性环境中通用型完工时间高效对象重排规划的新框架。CAM-MCTS融合了集中式任务分配（智能体可感知彼此的行动意图以实现全局优化规划）与异步任务执行策略（通过单步前瞻成本估计引导智能体在适当时机承接新任务，无需等待其他智能体）。该设计最小化空闲时间，避免不必要的同步延迟，提升整体系统效率。我们在杂乱环境中对单调与非单调任务集进行评估，证明相比强基线方法能持续缩短完工时间。最后，我们在不同配置的真实多智能体系统上验证了该方法的有效性与鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Object rearrangement in cluttered spaces, such as warehouses, often involves non-monotone scenarios where objects block each other, requiring temporary relocations; effective multi-agent collaboration is crucial to reduce task completion time. This paper proposes Centralized, Asynchronous, Multi-agent Monte Carlo Tree Search (CAM-MCTS), a framework that integrates centralized task assignment for globally optimized planning with asynchronous execution, allowing agents to take on new tasks without waiting, guided by a one-step look-ahead cost estimate to minimize idle time and synchronization delays. Experimental results on diverse monotone and non-monotone tasks show consistent reductions in makespan compared to strong baselines, with real-world multi-agent system validation confirming its effectiveness and robustness.</div>
<div class="mono" style="margin-top:8px">杂乱空间中的物体重排常涉及非单调场景，即物体相互阻挡需临时移开，多智能体协作可缩短任务完成时间。本文提出集中式异步多智能体蒙特卡洛树搜索（CAM-MCTS），该方法结合集中式任务分配以实现全局优化规划，并采用异步执行策略，通过一步前瞻成本估计指导智能体适时承接新任务，从而减少空闲时间和同步延迟。实验结果表明，在单调和非单调任务中，相比基线方法，该框架均能持续降低完工时间，真实多智能体系统的验证进一步证实了其有效性和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Terrain Costmap Generation via Scaled Preference Conditioning</div>
<div class="meta-line">Authors: Luisa Mao, Garrett Warnell, Peter Stone, Joydeep Biswas</div>
<div class="meta-line">First: 2025-11-14T18:04:20+00:00 · Latest: 2026-02-02T18:07:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11529v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11529v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Successful autonomous robot navigation in off-road domains requires the ability to generate high-quality terrain costmaps that are able to both generalize well over a wide variety of terrains and rapidly adapt relative costs at test time to meet mission-specific needs. Existing approaches for costmap generation allow for either rapid test-time adaptation of relative costs (e.g., semantic segmentation methods) or generalization to new terrain types (e.g., representation learning methods), but not both. In this work, we present scaled preference conditioned all-terrain costmap generation (SPACER), a novel approach for generating terrain costmaps that leverages synthetic data during training in order to generalize well to new terrains, and allows for rapid test-time adaptation of relative costs by conditioning on a user-specified scaled preference context. Using large-scale aerial maps, we provide empirical evidence that SPACER outperforms other approaches at generating costmaps for terrain navigation, with the lowest measured regret across varied preferences in five of seven environments for global path planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于尺度偏好条件化的地形代价图生成</div>
<div class="mono" style="margin-top:8px">在越野环境中实现成功的自主机器人导航，需要能够生成高质量的地形代价图，这些代价图既能广泛适应多种地形，又能在测试时快速调整相对成本以满足特定任务需求。现有的代价图生成方法要么允许快速测试时调整相对成本（如语义分割方法），要么能泛化到新地形类型（如表示学习方法），但无法同时兼顾两者。本研究提出了尺度偏好条件化全地形代价图生成（SPACER），这是一种新颖的地形代价图生成方法，通过在训练中利用合成数据以实现对新地形的良好泛化，并通过用户指定的尺度偏好上下文进行条件化，实现测试时相对成本的快速调整。基于大规模航拍地图，我们提供了实证证据，表明SPACER在生成地形导航代价图方面优于其他方法，在七个全局路径规划环境中的五个中，对不同偏好的测量遗憾值最低。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for autonomous robot navigation systems that can both generalize across diverse off-road terrains and rapidly adapt cost assessments to specific mission requirements at test time. The proposed method, called Scaled Preference Conditioned All-Terrain Costmap Generation (SPACER), uses synthetic data during training to achieve generalization and enables rapid test-time adaptation by conditioning costmap generation on a user-specified scaled preference context. Experimental evaluation on large-scale aerial maps demonstrates that SPACER outperforms existing approaches, achieving the lowest measured regret across varied preferences in five out of seven environments for global path planning.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决自主机器人生成高质量地形代价地图的需求，该地图需能泛化至多种越野地形，并允许根据特定任务要求快速调整相对代价。所提出的方法称为基于缩放偏好条件的全地形代价地图生成（SPACER），其在训练阶段利用合成数据以实现泛化能力，并通过基于用户指定的缩放偏好上下文来调节代价地图生成，从而实现快速适应。基于大规模航空地图的实验评估表明，SPACER的性能优于现有方法，在七个测试环境中的五个里，针对不同用户偏好的全局路径规划实现了最低的遗憾值。</div>
</details>
</div>
<div class="card">
<div class="title">SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation</div>
<div class="meta-line">Authors: Mu Huang, Hui Wang, Kerui Ren, Linning Xu, Yunsong Zhou, Mulin Yu, Bo Dai, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-02-02T17:59:31+00:00 · Latest: 2026-02-02T17:59:31+00:00</div>
<div class="meta-line">Comments: Project page: https://city-super.github.io/SoMA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02402v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02402v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://city-super.github.io/SoMA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulating deformable objects under rich interactions remains a fundamental challenge for real-to-sim robot manipulation, with dynamics jointly driven by environmental effects and robot actions. Existing simulators rely on predefined physics or data-driven dynamics without robot-conditioned control, limiting accuracy, stability, and generalization. This paper presents SoMA, a 3D Gaussian Splat simulator for soft-body manipulation. SoMA couples deformable dynamics, environmental forces, and robot joint actions in a unified latent neural space for end-to-end real-to-sim simulation. Modeling interactions over learned Gaussian splats enables controllable, stable long-horizon manipulation and generalization beyond observed trajectories without predefined physical models. SoMA improves resimulation accuracy and generalization on real-world robot manipulation by 20%, enabling stable simulation of complex tasks such as long-horizon cloth folding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SoMA：面向机器人软体操作的真实到仿真神经模拟器</div>
<div class="mono" style="margin-top:8px">在丰富交互条件下模拟可变形物体仍是真实到仿真机器人操作的核心挑战，其动力学同时受环境效应与机器人动作驱动。现有模拟器依赖预定义物理模型或无机器人条件控制的数据驱动动力学，限制了精度、稳定性和泛化能力。本文提出SoMA——基于3D高斯溅射的软体操作模拟器，将可变形动力学、环境作用力与机器人关节动作耦合于统一潜在神经空间，实现端到端真实到仿真模拟。通过学习的高斯溅射建模交互，无需预定义物理模型即可实现可控、稳定的长时程操作，并泛化至未观测轨迹。SoMA在真实机器人操作任务中将重模拟精度与泛化能力提升20%，支持长时程布料折叠等复杂任务的稳定仿真。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of accurately simulating deformable objects in robotic manipulation, where dynamics are influenced by both environmental forces and robot actions. The authors propose SoMA, a neural simulator that uses 3D Gaussian Splatting to model soft-body dynamics in a unified latent space, integrating robot joint actions for end-to-end real-to-simulation. Experimental results show that SoMA improves resimulation accuracy and generalization by 20% over existing methods, enabling stable, long-horizon simulation of complex tasks like cloth folding without predefined physical models.</div>
<div class="mono" style="margin-top:8px">本研究针对机器人操作中可变形物体模拟的挑战，其动力学同时受环境力和机器人动作影响。作者提出了SoMA，一种利用3D高斯泼溅在统一潜在空间中建模软体动力学的神经模拟器，它整合机器人关节动作以实现端到端的真实到模拟，无需预定义物理模型。实验结果表明，SoMA在真实世界任务中将重模拟准确性和泛化能力提高了20%，能够稳定模拟如布料折叠等超越观测轨迹的长时程操作。</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning</div>
<div class="meta-line">Authors: Amisha Bhaskar, Pratap Tokekar, Stefano Di Cairano, Alexander Schperberg</div>
<div class="meta-line">First: 2026-02-02T17:57:37+00:00 · Latest: 2026-02-02T17:57:37+00:00</div>
<div class="meta-line">Comments: 10 pages main text and 4 figures, and 11 pages appendix and 10 figures, total 21 pages and 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02396v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02396v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic imitation learning typically requires models that capture multimodal action distributions while operating at real-time control rates and accommodating multiple sensing modalities. Although recent generative approaches such as diffusion models, flow matching, and Implicit Maximum Likelihood Estimation (IMLE) have achieved promising results, they often satisfy only a subset of these requirements. To address this, we introduce PRISM, a single-pass policy based on a batch-global rejection-sampling variant of IMLE. PRISM couples a temporal multisensory encoder (integrating RGB, depth, tactile, audio, and proprioception) with a linear-attention generator using a Performer architecture. We demonstrate the efficacy of PRISM on a diverse real-world hardware suite, including loco-manipulation using a Unitree Go2 with a 7-DoF arm D1 and tabletop manipulation with a UR5 manipulator. Across challenging physical tasks such as pre-manipulation parking, high-precision insertion, and multi-object pick-and-place, PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency (30-50 Hz) closed-loop control. We further validate our approach on large-scale simulation benchmarks, including CALVIN, MetaWorld, and Robomimic. In CALVIN (10% data split), PRISM improves success rates by approximately 25% over diffusion and approximately 20% over flow matching, while simultaneously reducing trajectory jerk by 20x-50x. These results position PRISM as a fast, accurate, and multisensory imitation policy that retains multimodal action coverage without the latency of iterative sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRISM：用于单次多感官模仿学习的Performer RS-IMLE方法</div>
<div class="mono" style="margin-top:8px">机器人模仿学习通常需要模型能够捕捉多模态动作分布，同时以实时控制速率运行并兼容多种传感模态。尽管扩散模型、流匹配和隐式最大似然估计（IMLE）等近期生成方法已取得显著成果，但它们往往仅满足部分要求。为此，我们提出了PRISM，一种基于IMLE批量全局拒绝采样变体的单次策略。PRISM通过Performer架构，将时序多感官编码器（整合RGB、深度、触觉、音频和本体感知）与线性注意力生成器耦合。我们在多样化真实硬件平台上验证了PRISM的有效性，包括使用Unitree Go2搭载7自由度D1机械臂的运动操控，以及UR5机械臂的桌面操作。在预操作泊车、高精度插入和多物体抓放等挑战性物理任务中，PRISM的成功率比最先进的扩散策略提高10-25%，同时保持高频（30-50 Hz）闭环控制。我们进一步在CALVIN、MetaWorld和Robomimic等大规模仿真基准测试中验证了该方法。在CALVIN（10%数据划分）中，PRISM的成功率较扩散模型提升约25%，较流匹配提升约20%，同时将轨迹急动度降低20-50倍。这些结果表明PRISM是一种快速、准确且保留多模态动作覆盖的多感官模仿策略，无需迭代采样带来的延迟。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Robotic imitation learning requires models that handle multimodal action distributions, real-time control, and diverse sensory inputs, but existing generative approaches often fall short on some requirements. To address this, the authors propose PRISM, a single-pass policy based on a batch-global rejection-sampling variant of Implicit Maximum Likelihood Estimation (IMLE), which integrates a temporal multisensory encoder for modalities like RGB, depth, tactile, audio, and proprioception with a linear-attention generator using a Performer architecture. Experimental results on real-world hardware tasks, such as loco-manipulation and tabletop manipulation, show that PRISM outperforms state-of-the-art diffusion policies by 10-25% in success rate while maintaining high-frequency control at 30-50 Hz; in large-scale simulation benchmarks like CALVIN, it improves success rates by about 25% over diffusion and 20% over flow matching while reducing trajectory jerk by 20x-50x.</div>
<div class="mono" style="margin-top:8px">机器人模仿学习需要模型能够处理多模态动作分布、实时控制及多种传感模式，但现有生成方法往往无法同时满足这些要求。为此，研究者提出了PRISM，这是一种基于批量全局拒绝采样变体隐式最大似然估计（IMLE）的单次通过策略，它通过Performer架构将整合RGB、深度、触觉、音频和本体感觉的时序多感官编码器与线性注意力生成器相结合。在真实硬件任务（如移动操作和桌面操作）上的实验结果表明，PRISM在保持30-50 Hz控制频率的同时，成功率比最先进的扩散策略高出10-25%；在CALVIN等仿真基准测试中，其成功率比扩散方法提高约25%，比流匹配提高约20%，同时将轨迹急动度降低了20-50倍。</div>
</details>
</div>
<div class="card">
<div class="title">Mapping-Guided Task Discovery and Allocation for Robotic Inspection of Underwater Structures</div>
<div class="meta-line">Authors: Marina Ruediger, Ashis G. Banerjee</div>
<div class="meta-line">Venue: ICRA</div>
<div class="meta-line">First: 2026-02-02T17:51:32+00:00 · Latest: 2026-02-02T17:51:32+00:00</div>
<div class="meta-line">Comments: This paper will appear in the proceedings of the 2026 IEEE International Conference on Robotics and Automation (ICRA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02389v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02389v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Task generation for underwater multi-robot inspections without prior knowledge of existing geometry can be achieved and optimized through examination of simultaneous localization and mapping (SLAM) data. By considering hardware parameters and environmental conditions, a set of tasks is generated from SLAM meshes and optimized through expected keypoint scores and distance-based pruning. In-water tests are used to demonstrate the effectiveness of the algorithm and determine the appropriate parameters. These results are compared to simulated Voronoi partitions and boustrophedon patterns for inspection coverage on a model of the test environment. The key benefits of the presented task discovery method include adaptability to unexpected geometry and distributions that maintain coverage while focusing on areas more likely to present defects or damage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>水下结构机器人检测的建图引导任务发现与分配</div>
<div class="mono" style="margin-top:8px">通过分析同步定位与建图（SLAM）数据，可在无需先验几何知识的情况下实现并优化水下多机器人检测任务生成。该方法结合硬件参数与环境条件，从SLAM网格生成任务集，并通过预期关键点评分与基于距离的剪枝进行优化。水下实验验证了算法有效性并确定了最佳参数。研究结果与测试环境模型中模拟的Voronoi分区及往复扫描模式的检测覆盖率进行了对比。所提任务发现方法的核心优势包括：对意外几何结构的适应性，以及在保持覆盖范围的同时聚焦于更可能出现缺陷或损伤区域的分布特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the challenge of autonomously generating inspection tasks for underwater multi-robot systems when prior geometric knowledge of structures is unavailable. The method leverages data from simultaneous localization and mapping (SLAM) to create meshes, from which a set of inspection tasks is generated and optimized by evaluating expected keypoint scores and pruning based on distance, while accounting for hardware and environmental constraints. Experimental validation through in-water tests and comparisons to simulated Voronoi partitions and boustrophedon patterns demonstrated the method&#x27;s effectiveness, showing its adaptability to unexpected geometries and its ability to maintain coverage while focusing on areas with a higher likelihood of defects.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决水下多机器人系统在缺乏结构先验几何知识时自主生成检测任务的挑战。该方法利用同步定位与建图（SLAM）数据生成网格，并在此基础上通过评估预期关键点得分和基于距离的剪枝来生成和优化任务集，同时考虑硬件和环境约束。通过水下实验验证，并在模型环境中与模拟的Voronoi分区和往复式扫描模式进行比较，结果表明所提算法能有效保持覆盖范围、适应意外几何形状，并将检测重点集中在更可能出现缺陷的区域。</div>
</details>
</div>
<div class="card">
<div class="title">HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction</div>
<div class="meta-line">Authors: Wei Zhang, Qing Cheng, David Skuddis, Niclas Zeller, Daniel Cremers, Norbert Haala</div>
<div class="meta-line">Venue: IEEE Transactions on Robotics, Volume 41, Pages 6478-6493, Year 2025</div>
<div class="meta-line">First: 2024-11-27T01:39:21+00:00 · Latest: 2026-02-02T17:08:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.17982v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.17982v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hi-slam2.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast and accurate monocular scene reconstruction using only RGB input. Existing Neural SLAM or 3DGS-based SLAM methods often trade off between rendering quality and geometry accuracy, our research demonstrates that both can be achieved simultaneously with RGB input alone. The key idea of our approach is to enhance the ability for geometry estimation by combining easy-to-obtain monocular priors with learning-based dense SLAM, and then using 3D Gaussian splatting as our core map representation to efficiently model the scene. Upon loop closure, our method ensures on-the-fly global consistency through efficient pose graph bundle adjustment and instant map updates by explicitly deforming the 3D Gaussian units based on anchored keyframe updates. Furthermore, we introduce a grid-based scale alignment strategy to maintain improved scale consistency in prior depths for finer depth details. Through extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate significant improvements over existing Neural SLAM methods and even surpass RGB-D-based methods in both reconstruction and rendering quality. The project page and source code will be made available at https://hi-slam2.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HI-SLAM2：面向快速单目场景重建的几何感知高斯SLAM系统</div>
<div class="mono" style="margin-top:8px">我们提出HI-SLAM2，一种几何感知的高斯SLAM系统，仅使用RGB输入即可实现快速精确的单目场景重建。现有基于神经SLAM或3DGS的SLAM方法常在渲染质量与几何精度间权衡，本研究证明仅凭RGB输入即可同时实现两者。该方法的核心思想是通过将易获取的单目先验与基于学习的稠密SLAM相结合来增强几何估计能力，并以3D高斯泼溅作为核心地图表示来高效建模场景。在闭环检测时，我们通过高效的位姿图光束法平差和基于锚定关键帧更新的3D高斯单元显式形变，实现实时全局一致性及瞬时地图更新。此外，我们引入基于网格的尺度对齐策略，在先验深度中保持更优的尺度一致性以获取精细深度细节。通过在Replica、ScanNet和ScanNet++数据集上的大量实验，本方法在重建与渲染质量上均显著优于现有神经SLAM方法，甚至超越基于RGB-D的方法。项目页面与源代码发布于https://hi-slam2.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the trade-off between rendering quality and geometric accuracy in monocular SLAM systems by introducing HI-SLAM2, a geometry-aware Gaussian SLAM framework. The method integrates monocular depth priors with learning-based dense SLAM for robust geometry estimation and employs 3D Gaussian splatting as its core map representation. It ensures global consistency through efficient pose graph optimization and explicit deformation of Gaussian units during loop closure, while a grid-based scale alignment strategy refines depth details. Experimental evaluations on Replica, ScanNet, and ScanNet++ datasets demonstrate that HI-SLAM2 outperforms existing neural SLAM methods and even surpasses some RGB-D-based approaches in both reconstruction accuracy and rendering quality.</div>
<div class="mono" style="margin-top:8px">本研究针对现有神经SLAM和基于3D高斯泼溅（3DGS）的SLAM系统在单目场景重建中渲染质量与几何精度难以兼得的问题，提出了HI-SLAM2方法。该方法通过将易于获取的单目深度先验与基于学习的稠密SLAM相结合以提升几何估计能力，并以3D高斯泼溅为核心地图表示；通过高效的位姿图优化实现闭环时的全局一致性，并基于关键帧更新显式变形3D高斯单元，同时采用网格化的尺度对齐策略以优化深度细节。在Replica、ScanNet和ScanNet++数据集上的实验表明，该方法在重建和渲染质量上均显著优于现有神经SLAM方法，甚至超越了部分基于RGB-D的方法。</div>
</details>
</div>
<div class="card">
<div class="title">TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour</div>
<div class="meta-line">Authors: Shaoting Zhu, Baijun Ye, Jiaxuan Wang, Jiakang Chen, Ziwen Zhuang, Linzhan Mou, Runhan Huang, Hang Zhao</div>
<div class="meta-line">First: 2026-02-02T16:55:10+00:00 · Latest: 2026-02-02T16:55:10+00:00</div>
<div class="meta-line">Comments: Project Page: https://ttt-parkour.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02331v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02331v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ttt-parkour.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving highly dynamic humanoid parkour on unseen, complex terrains remains a challenge in robotics. Although general locomotion policies demonstrate capabilities across broad terrain distributions, they often struggle with arbitrary and highly challenging environments. To overcome this limitation, we propose a real-to-sim-to-real framework that leverages rapid test-time training (TTT) on novel terrains, significantly enhancing the robot&#x27;s capability to traverse extremely difficult geometries. We adopt a two-stage end-to-end learning paradigm: a policy is first pre-trained on diverse procedurally generated terrains, followed by rapid fine-tuning on high-fidelity meshes reconstructed from real-world captures. Specifically, we develop a feed-forward, efficient, and high-fidelity geometry reconstruction pipeline using RGB-D inputs, ensuring both speed and quality during test-time training. We demonstrate that TTT-Parkour empowers humanoid robots to master complex obstacles, including wedges, stakes, boxes, trapezoids, and narrow beams. The whole pipeline of capturing, reconstructing, and test-time training requires less than 10 minutes on most tested terrains. Extensive experiments show that the policy after test-time training exhibits robust zero-shot sim-to-real transfer capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TTT-Parkour：用于感知机器人跑酷的快速测试时训练</div>
<div class="mono" style="margin-top:8px">在未见过的复杂地形上实现高度动态的人形机器人跑酷仍是机器人学中的一项挑战。尽管通用运动策略在广泛地形分布中展现出能力，但它们常难以应对任意且极具挑战性的环境。为克服此限制，我们提出一种实-仿-实框架，利用在新地形上的快速测试时训练，显著增强机器人穿越极端困难几何结构的能力。我们采用两阶段端到端学习范式：策略首先在多样程序生成地形上进行预训练，随后基于真实世界捕获重建的高保真网格进行快速微调。具体而言，我们开发了一种使用RGB-D输入的前馈式高效高保真几何重建流程，确保测试时训练的速度与质量。实验表明，TTT-Parkour使人形机器人能够掌握包括楔形块、桩柱、箱体、梯形台及窄梁在内的复杂障碍。完整的捕获、重建与测试时训练流程在多数测试地形上耗时不足10分钟。大量实验证明，经过测试时训练的策略展现出稳健的零样本仿真到现实迁移能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of enabling humanoid robots to perform dynamic parkour on unseen, complex terrains, where general locomotion policies often fail. The method introduces a real-to-sim-to-real framework using rapid test-time training (TTT), which involves pre-training a policy on diverse procedurally generated terrains and then fine-tuning it on high-fidelity meshes reconstructed from real-world RGB-D captures via an efficient pipeline. Experimental results demonstrate that this approach allows robots to master complex obstacles like wedges, stakes, and narrow beams, with the entire capture-to-training process taking under 10 minutes per terrain and enabling robust zero-shot sim-to-real transfer.</div>
<div class="mono" style="margin-top:8px">为使仿人机器人能够在极具挑战性的未知地形上运动，本研究提出了一个虚实结合框架，利用快速测试时训练实现快速适应。该方法采用两阶段学习范式：首先在程序生成的各种地形上预训练运动策略，然后利用基于RGB-D输入的高效前馈式高保真几何重建流程，在真实地形网格上对策略进行快速微调。实验结果表明，经过测试时训练的策略能使机器人成功穿越楔形物、桩、箱体、梯形物和窄梁等复杂障碍，整个从捕捉、重建到训练的过程在大多数地形上耗时不到10分钟，并展现出鲁棒的零样本虚实迁移能力。</div>
</details>
</div>
<div class="card">
<div class="title">Before Autonomy Takes Control: Software Testing in Robotics</div>
<div class="meta-line">Authors: Nils Chur, Thiago Santos de Moura, Argentina Ortega, Sven Peldszus, Thorsten Berger, Nico Hochgeschwender, Yannic Noller</div>
<div class="meta-line">First: 2026-02-02T16:30:23+00:00 · Latest: 2026-02-02T16:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02293v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02293v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic systems are complex and safety-critical software systems. As such, they need to be tested thoroughly. Unfortunately, robot software is intrinsically hard to test compared to traditional software, mainly since the software needs to closely interact with hardware, account for uncertainty in its operational environment, handle disturbances, and act highly autonomously. However, given the large space in which robots operate, anticipating possible failures when designing tests is challenging. This paper presents a mapping study by considering robotics testing papers and relating them to the software testing theory. We consider 247 robotics testing papers and map them to software testing, discussing the state-of-the-art software testing in robotics with an illustrated example, and discuss current challenges. Forming the basis to introduce both the robotics and software engineering communities to software testing challenges. Finally, we identify open questions and lessons learned.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自主控制之前：机器人学中的软件测试</div>
<div class="mono" style="margin-top:8px">机器人系统是复杂且安全关键的软件系统，因此需要全面测试。然而，与传统软件相比，机器人软件本质上更难测试，主要因为软件需与硬件紧密交互、考虑操作环境的不确定性、处理干扰并高度自主运行。鉴于机器人运行环境的广阔性，在设计测试时预测潜在故障极具挑战。本文通过分析机器人测试文献并将其与软件测试理论关联，开展了一项映射研究。我们考察了247篇机器人测试论文，将其映射至软件测试领域，结合示例探讨机器人学中软件测试的最新进展及当前挑战，为机器人学和软件工程社区引入软件测试挑战奠定基础。最后，我们提出了开放性问题并总结了经验教训。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of thoroughly testing safety-critical robotic software, which is inherently difficult due to its complex interactions with hardware, environmental uncertainty, and autonomous operation. The authors conduct a mapping study by analyzing 247 robotics testing papers and relating them to established software testing theory, thereby discussing the state-of-the-art and illustrating it with an example. The main findings include a comprehensive mapping of current practices, the identification of key challenges in anticipating failures, and the formulation of open questions and lessons learned to inform both robotics and software engineering communities.</div>
<div class="mono" style="margin-top:8px">本研究旨在应对复杂、安全关键的机器人软件需要彻底测试的需求，这类测试因硬件交互、环境不确定性和高度自主性而本质困难，使得预测故障具有挑战性。研究方法是通过映射研究，分析247篇机器人测试论文，将其与软件测试理论相关联，以讨论最新技术并用示例说明。主要发现包括对当前实践的全面映射、识别该领域的主要挑战，并提出了开放性问题与经验教训，为机器人和软件工程社区提供参考。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems</div>
<div class="meta-line">Authors: Jon Škerlj, Seongjin Bien, Abdeldjallil Naceri, Sami Haddadin</div>
<div class="meta-line">First: 2026-02-02T16:11:12+00:00 · Latest: 2026-02-02T16:11:12+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02269v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02269v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present $multipanda\_ros2$, a novel open-source ROS2 architecture for multi-robot control of Franka Robotics robots. Leveraging ros2 control, this framework provides native ROS2 interfaces for controlling any number of robots from a single process. Our core contributions address key challenges in real-time torque control, including interaction control and robot-environment modeling. A central focus of this work is sustaining a 1kHz control frequency, a necessity for real-time control and a minimum frequency required by safety standards. Moreover, we introduce a controllet-feature design pattern that enables controller-switching delays of $\le 2$ ms, facilitating reproducible benchmarking and complex multi-robot interaction scenarios. To bridge the simulation-to-reality (sim2real) gap, we integrate a high-fidelity MuJoCo simulation with quantitative metrics for both kinematic accuracy and dynamic consistency (torques, forces, and control errors). Furthermore, we demonstrate that real-world inertial parameter identification can significantly improve force and torque accuracy, providing a methodology for iterative physics refinement. Our work extends approaches from soft robotics to rigid dual-arm, contact-rich tasks, showcasing a promising method to reduce the sim2real gap and providing a robust, reproducible platform for advanced robotics research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于multipanda_ros2弥合仿真与现实差距：面向多机械臂系统的实时ROS2框架</div>
<div class="mono" style="margin-top:8px">本文提出$multipanda\_ros2$——一种用于Franka Robotics机器人多机控制的新型开源ROS2架构。该框架基于ros2_control，提供原生ROS2接口以支持单进程控制任意数量机器人。核心贡献在于解决实时力矩控制中的关键挑战，包括交互控制与机器人-环境建模。研究重点在于维持1kHz控制频率，这是实时控制的必要条件及安全标准的最低频率要求。此外，我们引入控制单元特征设计模式，实现≤2ms的控制器切换延迟，为可复现基准测试和复杂多机器人交互场景提供支持。为弥合仿真与现实差距，我们集成高保真MuJoCo仿真，并建立运动精度与动态一致性（力矩、力及控制误差）的量化指标。实验证明，真实世界惯性参数辨识能显著提升力与力矩精度，为迭代式物理模型优化提供方法论。本工作将软机器人技术延伸至刚性双机械臂密集接触任务，展示了缩小仿真现实差距的有效途径，为前沿机器人研究提供稳健可复现的平台。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for a real-time, high-frequency control framework for multi-robot systems to bridge the simulation-to-reality gap. The authors introduce multipanda_ros2, an open-source ROS2 architecture that leverages ros2_control to manage multiple Franka robots from a single process, sustaining a 1kHz control frequency and enabling controller-switching delays of ≤2 ms via a controllet-feature design pattern. Key experimental findings demonstrate that integrating a high-fidelity MuJoCo simulation with quantitative metrics for kinematic and dynamic consistency, along with real-world inertial parameter identification, significantly improves force and torque accuracy, providing a robust platform for contact-rich, dual-arm tasks.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决多机器人系统对鲁棒实时控制框架的需求，以弥合仿真与现实的差距。它提出了multipanda_ros2，一个基于ros2_control的开源ROS2架构，用于从单一进程控制多台Franka机器人，核心是维持1kHz控制频率并通过控制体-特征设计模式实现快速控制器切换。实验验证表明，该框架实现了≤2毫秒的控制器切换延迟，集成了高保真MuJoCo仿真与运动学和动力学精度的定量指标，并证明现实世界的惯性参数辨识能提高力和扭矩精度，将其能力扩展到刚性双臂、接触丰富的任务中。</div>
</details>
</div>
<div class="card">
<div class="title">What does really matter in image goal navigation?</div>
<div class="meta-line">Authors: Gianluca Monaci, Philippe Weinzaepfel, Christian Wolf</div>
<div class="meta-line">First: 2025-07-02T12:50:26+00:00 · Latest: 2026-02-02T16:05:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01667v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.01667v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image goal navigation requires two different skills: firstly, core navigation skills, including the detection of free space and obstacles, and taking decisions based on an internal representation; and secondly, computing directional information by comparing visual observations to the goal image. Current state-of-the-art methods either rely on dedicated image-matching, or pre-training of computer vision modules on relative pose estimation. In this paper, we study whether this task can be efficiently solved with end-to-end training of full agents with RL, as has been claimed by recent work. A positive answer would have impact beyond Embodied AI and allow training of relative pose estimation from reward for navigation alone. In this large experimental study we investigate the effect of architectural choices like late fusion, channel stacking, space-to-depth projections and cross-attention, and their role in the emergence of relative pose estimators from navigation training. We show that the success of recent methods is influenced up to a certain extent by simulator settings, leading to shortcuts in simulation. However, we also show that these capabilities can be transferred to more realistic setting, up to some extent. We also find evidence for correlations between navigation performance and probed (emerging) relative pose estimation performance, an important sub skill.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像目标导航中真正重要的是什么？</div>
<div class="mono" style="margin-top:8px">图像目标导航需要两种不同技能：一是核心导航技能，包括自由空间与障碍物检测，以及基于内部表征进行决策；二是通过将视觉观测与目标图像对比来计算方向信息。当前最先进的方法要么依赖专用图像匹配，要么基于相对位姿估计预训练计算机视觉模块。本文研究了该任务能否如近期研究所述，通过强化学习对完整智能体进行端到端训练高效解决。肯定答案将对具身人工智能领域产生更广泛影响，并允许仅从导航奖励中训练相对位姿估计。在这项大规模实验研究中，我们探讨了延迟融合、通道堆叠、空间到深度投影与交叉注意力等架构选择的影响，及其在导航训练中催生相对位姿估计器的作用。研究表明，现有方法的成功在一定程度上受仿真器设置影响，导致模拟捷径；但这些能力可在一定程度上迁移至更现实的场景。同时发现导航性能与探测到的（涌现）相对位姿估计性能存在相关性，后者是该任务的重要子技能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates whether image goal navigation can be effectively solved through end-to-end reinforcement learning (RL) training of full agents, challenging the reliance of state-of-the-art methods on dedicated image-matching or pre-trained vision modules for relative pose estimation. The method involves a large-scale experimental study examining architectural choices such as late fusion, channel stacking, space-to-depth projections, and cross-attention to understand their role in enabling agents to develop relative pose estimation capabilities from navigation rewards alone. Key findings reveal that simulator settings can create shortcuts that influence the success of recent methods, but these learned capabilities can be partially transferred to more realistic environments; furthermore, the study identifies correlations between navigation performance and the emergent skill of relative pose estimation.</div>
<div class="mono" style="margin-top:8px">本文研究图像目标导航任务是否可以通过端到端强化学习训练完整智能体来有效解决，而非依赖当前主流方法中的专用图像匹配或预训练视觉模块。方法上，通过大规模实验研究了多种架构选择（如延迟融合、通道堆叠、空间到深度投影和交叉注意力）对智能体从导航奖励中隐式学习相对位姿估计能力的影响。主要实验结果表明，模拟器设置会在一定程度上影响性能表现，形成捷径，但所学能力可以部分迁移到更真实的场景中；此外，研究发现导航性能与涌现的相对位姿估计子技能之间存在相关性。</div>
</details>
</div>
<div class="card">
<div class="title">Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL</div>
<div class="meta-line">Authors: Julian Lemmel, Felix Resch, Mónika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</div>
<div class="meta-line">First: 2026-02-02T15:41:53+00:00 · Latest: 2026-02-02T15:41:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02236v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents&#x27; performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于实时循环强化学习的自动驾驶预训练控制器在线微调</div>
<div class="mono" style="margin-top:8px">在现实应用中部署预训练策略面临重大挑战，从根本上限制了基于学习的控制系统的实际适用性。当自主系统遭遇系统动力学变化、传感器漂移或任务目标改变时，固定策略的性能会迅速下降。研究表明，采用实时循环强化学习（RTRRL）——一种具有生物合理性的在线适应算法——能够有效微调预训练策略，提升自主智能体在驾驶任务中的表现。进一步研究发现，RTRRL可与近期受生物启发的循环网络模型（液阻-液容循环神经网络）产生协同效应。我们在模拟CarRacing环境及配备事件相机的RoboRacer实车循线任务中，验证了该闭环方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the performance degradation of fixed pretrained policies in autonomous driving when encountering environmental changes, such as shifts in system dynamics or sensor drift. The method employs Real-Time Recurrent Reinforcement Learning (RTRRL) for online fine-tuning of pretrained controllers, synergizing it with a biologically inspired Liquid-Resistance Liquid-Capacitance recurrent network. Experimental results demonstrate improved performance in a simulated CarRacing environment and a real-world line-following task using a RoboRacer with an event camera.</div>
<div class="mono" style="margin-top:8px">该研究针对自动驾驶中预训练策略在遇到环境变化（如系统动态或传感器漂移）时性能下降的问题。方法采用实时循环强化学习（RTRRL）对预训练控制器进行在线微调，并结合了受生物启发的液阻液容循环神经网络。实验结果表明，该方法在模拟CarRacing环境和真实世界使用事件相机的RoboRacer小车循线任务中均提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">LangMap: A Hierarchical Benchmark for Open-Vocabulary Goal Navigation</div>
<div class="meta-line">Authors: Bo Miao, Weijia Liu, Jun Luo, Lachlan Shinnick, Jian Liu, Thomas Hamilton-Smith, Yuhe Yang, Zijie Wu, Vanja Videnovic, Feras Dayoub, Anton van den Hengel</div>
<div class="meta-line">First: 2026-02-02T15:26:19+00:00 · Latest: 2026-02-02T15:26:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02220v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02220v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://bo-miao.github.io/LangMap">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The relationships between objects and language are fundamental to meaningful communication between humans and AI, and to practically useful embodied intelligence. We introduce HieraNav, a multi-granularity, open-vocabulary goal navigation task where agents interpret natural language instructions to reach targets at four semantic levels: scene, room, region, and instance. To this end, we present Language as a Map (LangMap), a large-scale benchmark built on real-world 3D indoor scans with comprehensive human-verified annotations and tasks spanning these levels. LangMap provides region labels, discriminative region descriptions, discriminative instance descriptions covering 414 object categories, and over 18K navigation tasks. Each target features both concise and detailed descriptions, enabling evaluation across different instruction styles. LangMap achieves superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy using four times fewer words. Comprehensive evaluations of zero-shot and supervised models on LangMap reveal that richer context and memory improve success, while long-tailed, small, context-dependent, and distant goals, as well as multi-goal completion, remain challenging. HieraNav and LangMap establish a rigorous testbed for advancing language-driven embodied navigation. Project: https://bo-miao.github.io/LangMap</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LangMap：面向开放词汇目标导航的分层基准</div>
<div class="mono" style="margin-top:8px">物体与语言之间的关系对于人类与人工智能之间的有效沟通及实际可用的具身智能至关重要。本文提出HieraNav，一种多粒度、开放词汇的目标导航任务，要求智能体解析自然语言指令，在场景、房间、区域和实例四个语义层级上抵达目标。为此，我们构建了Language as a Map（LangMap）——一个基于真实世界三维室内扫描的大规模基准，包含全面的人工验证标注及跨层级任务。LangMap提供区域标签、区分性区域描述、覆盖414个物体类别的区分性实例描述，以及超过1.8万个导航任务。每个目标均配备简洁与详细两种描述，支持不同指令风格的评估。LangMap实现了卓越的标注质量，其区分性准确率较GOAT-Bench提升23.8%，且用词量仅为后者的四分之一。在LangMap上对零样本与监督模型的综合评估表明：更丰富的上下文与记忆能提升成功率，但长尾分布、小型目标、上下文依赖目标、远距离目标以及多目标完成仍是挑战。HieraNav与LangMap为推进语言驱动的具身导航建立了严谨的测试平台。项目地址：https://bo-miao.github.io/LangMap</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To advance embodied AI that interprets natural language for navigation, this work introduces HieraNav, a hierarchical open-vocabulary goal navigation task requiring agents to reach targets at four semantic levels (scene, room, region, instance). The authors develop LangMap, a large-scale benchmark built on real 3D indoor scans, featuring human-verified annotations including region labels, discriminative region and instance descriptions covering 414 object categories, and over 18,000 navigation tasks with both concise and detailed target descriptions. LangMap demonstrates superior annotation quality, outperforming GOAT-Bench by 23.8% in discriminative accuracy while using four times fewer words. Experimental evaluations show that models benefit from richer context and memory, but still struggle with long-tailed categories, small objects, context-dependent goals, distant targets, and multi-goal completion.</div>
<div class="mono" style="margin-top:8px">为推进语言驱动的具身智能，本研究提出了HieraNav，一种分层开放词汇目标导航任务，要求智能体在四个语义层级（场景、房间、区域、实例）上解析自然语言指令。方法涉及构建LangMap基准，这是一个基于真实世界3D室内扫描的大规模数据集，包含人工验证的标注，如区域标签、覆盖414个对象类别的区分性区域和实例描述，以及超过1.8万个导航任务。关键实验结果表明，LangMap在减少四分之三词汇量的情况下，比GOAT-Bench的区分准确率高出23.8%；评估还发现，尽管更丰富的上下文和记忆能提升成功率，但在长尾类别、小物体、上下文依赖目标、远距离目标以及多目标完成方面仍存在挑战。</div>
</details>
</div>
<div class="card">
<div class="title">MineInsight: A Multi-sensor Dataset for Humanitarian Demining Robotics in Off-Road Environments</div>
<div class="meta-line">Authors: Mario Malizia, Charles Hamesse, Ken Hasselmann, Geert De Cubber, Nikolaos Tsiogkas, Eric Demeester, Rob Haelterman</div>
<div class="meta-line">Venue: IEEE Robotics and Automation Letters, vol. 11, no. 2, pp. 1650-1657, Feb. 2026</div>
<div class="meta-line">First: 2025-06-05T10:08:24+00:00 · Latest: 2026-02-02T14:51:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.04842v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.04842v2">PDF</a> · <a href="https://github.com/mariomlz99/MineInsight">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The use of robotics in humanitarian demining increasingly involves computer vision techniques to improve landmine detection capabilities. However, in the absence of diverse and realistic datasets, the reliable validation of algorithms remains a challenge for the research community. In this paper, we introduce MineInsight, a publicly available multi-sensor, multi-spectral dataset designed for off-road landmine detection. The dataset features 35 different targets (15 landmines and 20 commonly found objects) distributed along three distinct tracks, providing a diverse and realistic testing environment. MineInsight is, to the best of our knowledge, the first dataset to integrate dual-view sensor scans from both an Unmanned Ground Vehicle and its robotic arm, offering multiple viewpoints to mitigate occlusions and improve spatial awareness. It features two LiDARs, as well as images captured at diverse spectral ranges, including visible (RGB, monochrome), visible short-wave infrared (VIS-SWIR), and long-wave infrared (LWIR). Additionally, the dataset provides bounding boxes generated by an automated pipeline and refined with human supervision. We recorded approximately one hour of data in both daylight and nighttime conditions, resulting in around 38,000 RGB frames, 53,000 VIS-SWIR frames, and 108,000 LWIR frames. MineInsight serves as a benchmark for developing and evaluating landmine detection algorithms. Our dataset is available at https://github.com/mariomlz99/MineInsight.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MineInsight：面向野外环境人道主义排雷机器人的多传感器数据集</div>
<div class="mono" style="margin-top:8px">人道主义排雷领域对机器人技术的应用日益依赖计算机视觉技术以提升地雷探测能力。然而，由于缺乏多样且真实的数据集，算法的可靠验证仍是研究界面临的挑战。本文介绍了MineInsight——一个专为野外环境地雷检测设计的公开多传感器、多光谱数据集。该数据集包含分布在三条独立路径上的35种不同目标（15种地雷与20种常见物体），提供了多样化的真实测试环境。据我们所知，MineInsight是首个集成无人地面车辆及其机械臂双视角传感器扫描的数据集，通过多视角数据可有效减少遮挡并提升空间感知能力。数据集配备两台激光雷达，并涵盖可见光（RGB、单色）、可见光-短波红外（VIS-SWIR）及长波红外（LWIR）等多光谱范围的图像。此外，数据集提供通过自动化流程生成并经人工校准优化的边界标注框。我们在日间与夜间条件下采集了约一小时数据，共获得约3.8万帧RGB图像、5.3万帧VIS-SWIR图像及10.8万帧LWIR图像。MineInsight可作为地雷检测算法开发与评估的基准数据集，相关资源已发布于https://github.com/mariomlz99/MineInsight。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the lack of diverse and realistic datasets for validating computer vision algorithms in humanitarian demining robotics, this paper introduces MineInsight, a publicly available multi-sensor, multi-spectral dataset. The method involves collecting data from an Unmanned Ground Vehicle and its robotic arm, integrating dual-view scans from two LiDARs and images across visible, visible short-wave infrared, and long-wave infrared spectral ranges, with bounding boxes generated via an automated pipeline and human refinement. Key experimental findings include the dataset&#x27;s composition of approximately one hour of data in daylight and nighttime conditions, featuring 35 different targets across three tracks and resulting in around 38,000 RGB, 53,000 VIS-SWIR, and 108,000 LWIR frames, establishing a benchmark for developing and evaluating landmine detection algorithms.</div>
<div class="mono" style="margin-top:8px">针对人道主义排雷机器人领域缺乏多样化和真实数据集来验证计算机视觉算法的问题，本文提出了MineInsight，一个公开可用的多传感器数据集。该方法使用无人地面车辆及其机械臂在越野环境中采集数据，整合了两个激光雷达的双视角扫描以及多个光谱范围（RGB、单色、可见光-短波红外、长波红外）的图像，以减少遮挡。主要实验结果表明，该数据集包含分布在三条路径上的35个不同目标，在白天和夜间条件下记录了约一小时的数据，产生了约38,000帧RGB图像、53,000帧可见光-短波红外图像和108,000帧长波红外图像，为地雷检测算法的开发提供了基准。</div>
</details>
</div>
<div class="card">
<div class="title">Extending the Law of Intersegmental Coordination: Implications for Powered Prosthetic Controls</div>
<div class="meta-line">Authors: Elad Siman Tov, Nili E. Krausz</div>
<div class="meta-line">First: 2026-02-02T14:49:35+00:00 · Latest: 2026-02-02T14:49:35+00:00</div>
<div class="meta-line">Comments: Submitted to 2026 IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Powered prostheses are capable of providing net positive work to amputees and have advanced in the past two decades. However, reducing amputee metabolic cost of walking remains an open problem. The Law of Intersegmental Coordination (ISC) has been observed across gaits and has been previously implicated in energy expenditure of walking, yet it has rarely been analyzed or applied within the context of lower-limb amputee gait. This law states that the elevation angles of the thigh, shank and foot over the gait cycle are not independent. In this work, we developed a method to analyze intersegmental coordination for lower-limb 3D kinematic data, to simplify ISC analysis. Moreover, inspired by motor control, biomechanics and robotics literature, we used our method to broaden ISC toward a new law of coordination of moments. We find these Elevation Space Moments (ESM), and present results showing a moment-based coordination for able bodied gait. We also analyzed ISC for amputee gait walking with powered and passive prosthesis, and found that while elevation angles remained planar, the ESM showed less coordination. We use ISC as a constraint to predict the shank angles/moments that would compensate for alterations due to a passive foot so as to mimic a healthy thigh angle/moment profile. This may have implications for improving powered prosthetic control. We developed the ISC3d toolbox that is freely available online, which may be used to compute kinematic and kinetic ISC in 3D. This provides a means to further study the role of coordination in gait and may help address fundamental questions of the neural control of human movement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展节段间协调定律：对动力假肢控制的启示</div>
<div class="mono" style="margin-top:8px">动力假肢能为截肢者提供净正功，近二十年来已取得显著进展。然而，降低截肢者行走代谢成本仍是一个未解难题。节段间协调定律已在多种步态中被观察到，且先前研究认为其与行走能量消耗相关，但该定律在下肢截肢者步态中鲜有被分析或应用。该定律指出，步态周期中大腿、小腿和足部的抬升角度并非独立。本研究开发了一种分析下肢三维运动学数据节段间协调的方法，以简化ISC分析。此外，受运动控制、生物力学和机器人学文献启发，我们运用该方法将ISC扩展为一种新的力矩协调定律。我们发现了这些抬升空间力矩，并展示了健全步态中基于力矩的协调结果。我们还分析了截肢者使用动力与被动假肢行走时的ISC，发现尽管抬升角度保持平面性，但ESM显示出较低的协调性。我们以ISC为约束，预测了可补偿被动足部所致变化的小腿角度/力矩，以模拟健康大腿的角度/力矩曲线。这可能对改进动力假肢控制具有启示意义。我们开发了可在线免费获取的ISC3d工具箱，可用于计算三维运动学与动力学ISC。这为深入研究协调在步态中的作用提供了工具，并可能有助于解决人类运动神经控制的基础性问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study addresses the challenge of reducing metabolic cost in amputee walking with powered prostheses by investigating the Law of Intersegmental Coordination (ISC), which describes the planar covariation of thigh, shank, and foot elevation angles during gait. The authors developed a method to analyze ISC from 3D kinematic data and extended it to propose a new law of coordination for joint moments, termed Elevation Space Moments (ESM). Experimental analysis revealed that while amputee gait with powered and passive prostheses maintained planar elevation angles, the ESM showed reduced coordination; furthermore, using ISC as a constraint allowed prediction of compensatory shank angles and moments to mimic healthy thigh profiles, offering potential for improved prosthetic control algorithms.</div>
<div class="mono" style="margin-top:8px">本研究针对动力假肢使用者行走代谢成本高的难题，通过探究描述步态中大腿、小腿和足部抬升角共面协变关系的节段间协调定律（ISC）来寻求解决方案。作者开发了一种从三维运动学数据中分析ISC的方法，并将其扩展至关节力矩的协调，提出了抬升空间力矩（ESM）新定律。实验分析发现，截肢者使用动力和被动假肢行走时，抬升角虽保持共面，但ESM的协调性减弱；此外，利用ISC作为约束可预测补偿性小腿角度和力矩以模拟健康大腿轮廓，这为改进假肢控制算法提供了潜在途径。</div>
</details>
</div>
<div class="card">
<div class="title">Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding</div>
<div class="meta-line">Authors: Soheil Behnam Roudsari, Alexandre S. Brandão, Felipe N. Martins</div>
<div class="meta-line">First: 2026-02-02T14:44:27+00:00 · Latest: 2026-02-02T14:44:27+00:00</div>
<div class="meta-line">Comments: 6 pages, 6 figures, submitted to IEEE SAS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02167v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02167v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Indoor service robots need perception that is robust, more privacy-friendly than RGB video, and feasible on embedded hardware. We present a camera-free 2D LiDAR object detection pipeline that encodes short-term temporal context by stacking three consecutive scans as RGB channels, yielding a compact YOLOv8n input without occupancy-grid construction while preserving angular structure and motion cues. Evaluated in Webots across 160 randomized indoor scenarios with strict scenario-level holdout, the method achieves 98.4% mAP@0.5 (0.778 mAP@0.5:0.95) with 94.9% precision and 94.7% recall on four object classes. On a Raspberry Pi 5, it runs in real time with a mean post-warm-up end-to-end latency of 47.8ms per frame, including scan encoding and postprocessing. Relative to a closely related occupancy-grid LiDAR-YOLO pipeline reported on the same platform, the proposed representation is associated with substantially lower reported end-to-end latency. Although results are simulation-based, they suggest that lightweight temporal encoding can enable accurate and real-time LiDAR-only detection for embedded indoor robotics without capturing RGB appearance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于三帧RGB扫描编码的实时二维激光雷达目标检测</div>
<div class="mono" style="margin-top:8px">室内服务机器人需要具备鲁棒、比RGB视频更保护隐私且能在嵌入式硬件上实现的感知能力。本文提出一种无需摄像头的二维激光雷达目标检测流程，通过将连续三帧扫描数据堆叠为RGB通道来编码短期时序上下文，生成紧凑的YOLOv8n输入，无需构建占据栅格地图，同时保留角度结构和运动线索。在Webots中通过160个随机室内场景（采用严格场景级保留验证）进行评估，该方法在四类目标上实现了98.4%的mAP@0.5（0.778 mAP@0.5:0.95），精确率94.9%，召回率94.7%。在树莓派5平台上，系统可实现实时运行，预热后平均端到端单帧延迟为47.8毫秒（含扫描编码与后处理）。相较于同一平台已报道的占据栅格LiDAR-YOLO流程，所提表征方法对应的端到端延迟显著降低。虽然结果基于仿真，但表明轻量级时序编码能够为嵌入式室内机器人实现无需RGB外观捕捉的精准实时纯激光雷达检测。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the need for robust, privacy-friendly, and computationally efficient perception for indoor service robots. The method proposes a camera-free 2D LiDAR object detection pipeline that encodes temporal context by stacking three consecutive LiDAR scans as the red, green, and blue channels of an image, creating a compact input for a YOLOv8n network without constructing occupancy grids. Evaluated in 160 randomized indoor scenarios in Webots with strict scenario-level holdout, the approach achieved 98.4% mAP@0.5, 94.9% precision, and 94.7% recall on four object classes, and ran in real-time on a Raspberry Pi 5 with a mean end-to-end latency of 47.8ms per frame, showing substantially lower latency compared to a related occupancy-grid-based pipeline.</div>
<div class="mono" style="margin-top:8px">为满足室内服务机器人对鲁棒、保护隐私且计算高效的感知需求，本研究提出了一种无相机的二维激光雷达物体检测方法。其核心方法是将连续三帧激光雷达扫描数据堆叠为RGB通道，以编码短期时序上下文，从而为YOLOv8n网络构建紧凑的输入，避免了占用栅格图的构建，同时保留了角度结构和运动信息。在160个随机室内场景的仿真评估中，该方法取得了98.4%的mAP@0.5指标，并在树莓派5上实现了实时性能，平均端到端延迟为每帧47.8毫秒，相较于相关的占用栅格基线方法，延迟显著降低。</div>
</details>
</div>
<div class="card">
<div class="title">FD-VLA: Force-Distilled Vision-Language-Action Model for Contact-Rich Manipulation</div>
<div class="meta-line">Authors: Ruiteng Zhao, Wenshuo Wang, Yicheng Ma, Xiaocong Li, Francis E. H. Tay, Marcelo H. Ang, Haiyue Zhu</div>
<div class="meta-line">First: 2026-02-02T14:19:46+00:00 · Latest: 2026-02-02T14:19:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02142v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Force sensing is a crucial modality for Vision-Language-Action (VLA) frameworks, as it enables fine-grained perception and dexterous manipulation in contact-rich tasks. We present Force-Distilled VLA (FD-VLA), a novel framework that integrates force awareness into contact-rich manipulation without relying on physical force sensors. The core of our approach is a Force Distillation Module (FDM), which distills force by mapping a learnable query token, conditioned on visual observations and robot states, into a predicted force token aligned with the latent representation of actual force signals. During inference, this distilled force token is injected into the pretrained VLM, enabling force-aware reasoning while preserving the integrity of its vision-language semantics. This design provides two key benefits: first, it allows practical deployment across a wide range of robots that lack expensive or fragile force-torque sensors, thereby reducing hardware cost and complexity; second, the FDM introduces an additional force-vision-state fusion prior to the VLM, which improves cross-modal alignment and enhances perception-action robustness in contact-rich scenarios. Surprisingly, our physical experiments show that the distilled force token outperforms direct sensor force measurements as well as other baselines, which highlights the effectiveness of this force-distilled VLA approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FD-VLA：面向接触式密集操作的力蒸馏视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">力感知是视觉-语言-动作框架的关键模态，它能在接触式密集任务中实现精细感知与灵巧操作。本文提出力蒸馏VLA，这是一种无需依赖物理力传感器即可将力感知融入接触式密集操作的新型框架。其核心是力蒸馏模块，该模块通过将基于视觉观测与机器人状态的可学习查询令牌映射至与真实力信号潜在表征对齐的预测力令牌，从而实现力蒸馏。在推理过程中，该蒸馏力令牌被注入预训练的视觉语言模型中，使其在保持视觉-语言语义完整性的同时具备力感知推理能力。此设计带来两大优势：其一，可在缺乏昂贵或易损力-扭矩传感器的各类机器人上实际部署，从而降低硬件成本与复杂度；其二，力蒸馏模块在视觉语言模型前引入了额外的力-视觉-状态融合先验，提升了跨模态对齐能力，并增强了接触式密集场景下的感知-动作鲁棒性。令人惊讶的是，物理实验表明蒸馏力令牌的性能优于直接传感器力测量及其他基线方法，这凸显了该力蒸馏VLA方法的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To enable fine-grained perception and dexterous manipulation in contact-rich tasks without relying on physical force sensors, this work introduces FD-VLA, a Vision-Language-Action framework that integrates force awareness through distillation. The method employs a Force Distillation Module (FDM) that maps a learnable query token, conditioned on visual and robot state inputs, into a predicted force token aligned with latent force signal representations; this token is then injected into a pretrained VLM during inference for force-aware reasoning. Physical experiments demonstrate that this distilled force token outperforms direct sensor force measurements and other baselines, enhancing cross-modal alignment and robustness in contact-rich manipulation while reducing hardware dependency.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决视觉-语言-动作（VLA）模型在接触丰富的精细操作任务中需要力感知的问题，特别是针对缺乏物理力传感器的机器人。方法提出了力蒸馏VLA（FD-VLA）框架，其核心是力蒸馏模块（FDM），该模块将视觉观测和机器人状态映射为预测的力令牌，在推理时注入预训练的VLM中，从而实现力感知推理而不改变其核心语义。实验结果表明，这种蒸馏出的力令牌不仅便于在无力传感器的机器人上部署，而且在接触丰富的操作任务中超越了直接力传感器测量和其他基线方法，提升了跨模态对齐和任务鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Contrastive Decoding for Robotic Foundation Models</div>
<div class="meta-line">Authors: Shihan Wu, Xu Luo, Ji Zhang, Junlin Xie, Jingkuan Song, Heng Tao Shen, Lianli Gao</div>
<div class="meta-line">First: 2025-05-19T15:39:08+00:00 · Latest: 2026-02-02T14:02:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.13255v5">Abs</a> · <a href="https://arxiv.org/pdf/2505.13255v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://koorye.github.io/PCD">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robotic foundation models, or generalist robot policies, hold immense potential to enable flexible, general-purpose and dexterous robotic systems. Despite their advancements, our empirical experiments reveal that existing robot policies are prone to learning spurious correlations from pre-training trajectories, adversely affecting their generalization capabilities beyond the training data. To tackle this, we propose a novel Policy Contrastive Decoding (PCD) approach, which redirects the robot policy&#x27;s focus toward object-relevant visual clues by contrasting action probability distributions derived from original and object-masked visual inputs. As a training-free method, our PCD can be used as a plugin to improve different types of robot policies without needing to finetune or access model weights. We conduct extensive experiments on top of three open-source robot policies, including the autoregressive policy OpenVLA and the diffusion-based policies Octo and $π_0$. The obtained results in both simulation and real-world environments prove PCD&#x27;s flexibility and effectiveness, e.g., PCD enhances the state-of-the-art policy $π_0$ by 8.9% in the simulation environment and by 108% in the real-world environment. Code and demos are publicly available at: https://koorye.github.io/PCD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器人基础模型中的策略对比解码方法</div>
<div class="mono" style="margin-top:8px">机器人基础模型（即通用机器人策略）具有实现灵活、通用且灵巧的机器人系统的巨大潜力。尽管相关研究已取得进展，但我们的实证实验表明，现有机器人策略容易从预训练轨迹中学习到虚假相关性，从而对其在训练数据之外的泛化能力产生负面影响。为解决这一问题，我们提出了一种新颖的策略对比解码方法，该方法通过对比源自原始视觉输入与物体掩码视觉输入的动作概率分布，将机器人策略的注意力重新导向与物体相关的视觉线索。作为一种免训练方法，PCD可作为插件用于改进各类机器人策略，无需微调或访问模型权重。我们在三种开源机器人策略（包括自回归策略OpenVLA以及基于扩散的策略Octo和$π_0$）上进行了大量实验。在仿真和真实环境中获得的结果均证明了PCD的灵活性与有效性：例如，PCD将最先进策略$π_0$在仿真环境中的性能提升8.9%，在真实环境中提升108%。代码与演示已公开于：https://koorye.github.io/PCD。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Robotic foundation models often learn spurious correlations from pre-training data, which limits their generalization. To address this, the authors propose Policy Contrastive Decoding (PCD), a training-free method that improves robot policies by contrasting action probability distributions from original and object-masked visual inputs, thereby focusing the policy on object-relevant visual clues. Experiments on policies like OpenVLA, Octo, and π₀ show that PCD enhances performance, notably improving π₀ by 8.9% in simulation and 108% in real-world settings, demonstrating its flexibility and effectiveness as a plug-in solution.</div>
<div class="mono" style="margin-top:8px">机器人基础模型常从预训练轨迹中学习虚假关联，影响其泛化能力。为此，研究者提出策略对比解码（PCD），这是一种无需训练的方法，通过对比原始和物体掩码视觉输入产生的动作概率分布，使策略聚焦于物体相关视觉线索。在OpenVLA、Octo和π₀等开源策略上的实验表明，PCD能有效提升性能，例如将π₀在模拟环境中的成功率提高8.9%，在真实环境中提高108%。</div>
</details>
</div>
<div class="card">
<div class="title">SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robots</div>
<div class="meta-line">Authors: Yue Wang</div>
<div class="meta-line">First: 2025-10-02T12:57:59+00:00 · Latest: 2026-02-02T13:16:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01984v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.01984v3">PDF</a> · <a href="http://github.com/YueWang996/sparc">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quadruped mammals coordinate spinal bending and axial compression to enhance locomotion agility and efficiency. However, existing robotic spines typically lack the active compliance required to support such dynamic behaviours. We present SPARC, a compact 3-DoF sagittal-plane spine module that enables simultaneous revolute and prismatic motions within a 1.26 kg package. Using a floating-base impedance controller, we facilitate independent, task-space tuning of spinal stiffness and damping to mimic biological load-bearing strategies. Benchtop experiments confirm high-fidelity rendering of commanded impedance, with linear force-displacement error within 1.5%. Systematic locomotion simulations reveal a critical speed-dependency: while low-speed efficiency is insensitive to spinal properties, precise impedance tuning becomes indispensable for high-speed performance. Our results demonstrate that an optimally compliant spine reduces power consumption by 21% at 0.9 m/s compared to a rigid-spine baseline. This efficiency gain is mechanistically attributed to the spine&#x27;s role in augmenting stride length and acting as a mechanical low-pass filter to attenuate high-frequency torque fluctuations. SPARC provides an open-source platform for systematic studies of spine compliance in legged locomotion. Available at: github.com/YueWang996/sparc</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARC：兼具棱柱与旋转柔顺性的四足机器人脊柱模块</div>
<div class="mono" style="margin-top:8px">四足哺乳动物通过协调脊柱弯曲与轴向压缩来提升运动灵活性与效率，但现有机器人脊柱通常缺乏支持此类动态行为的主动柔顺性。本文提出SPARC——一种紧凑的三自由度矢状面脊柱模块，可在1.26公斤的封装内实现旋转与棱柱运动的同步执行。通过浮基阻抗控制器，我们实现了脊柱刚度与阻尼在任务空间中的独立调节，以模拟生物承重策略。台架实验验证了指令阻抗的高保真复现，线性力-位移误差低于1.5%。系统性运动仿真揭示了关键的速度依赖性：低速效率对脊柱特性不敏感，而高速性能则高度依赖精确的阻抗调节。实验表明，在0.9米/秒速度下，优化柔顺脊柱较刚性基准可降低21%的能耗。此效益机制在于脊柱能增加步幅长度，并作为机械低通滤波器衰减高频扭矩波动。SPARC为系统研究腿式运动中脊柱柔顺性提供了开源平台。项目地址：github.com/YueWang996/sparc</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Quadruped mammals leverage spinal flexibility for agile and efficient locomotion, but robotic spines often lack the necessary active compliance. To address this, the authors developed SPARC, a lightweight 3-DoF spine module that provides both revolute and prismatic motion, controlled via a floating-base impedance controller for independent tuning of stiffness and damping. Experimental results show the system accurately renders commanded impedance with minimal force-displacement error, and simulations indicate that while spinal properties have little effect at low speeds, optimal compliance tuning at high speeds (0.9 m/s) reduces power consumption by 21% compared to a rigid spine, primarily by increasing stride length and filtering high-frequency torque fluctuations.</div>
<div class="mono" style="margin-top:8px">四足哺乳动物利用脊柱弯曲来提升运动敏捷性和效率，但现有机器人脊柱通常缺乏实现此类动态行为所需的主动柔顺性。为此，研究者提出了SPARC，一个紧凑的3自由度脊柱模块，能实现转动和平移运动，并通过浮动基阻抗控制器独立调节脊柱刚度和阻尼以模拟生物负载策略。台架实验证实了指令阻抗的高保真度，线性力-位移误差在1.5%以内；系统化的运动仿真揭示了关键的速度依赖性：低速时效率对脊柱特性不敏感，而高速性能则高度依赖精确的阻抗调谐。结果表明，与刚性脊柱基线相比，在0.9米/秒速度下，优化后的柔顺脊柱可降低21%的功耗，这归因于脊柱增加了步长并作为机械低通滤波器衰减了高频扭矩波动。</div>
</details>
</div>
<div class="card">
<div class="title">Frictional Contact Solving for Material Point Method</div>
<div class="meta-line">Authors: Etienne Ménager, Justin Carpentier</div>
<div class="meta-line">First: 2026-02-02T12:34:32+00:00 · Latest: 2026-02-02T12:34:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately handling contact with friction remains a core bottleneck for Material Point Method (MPM), from reliable contact point detection to enforcing frictional contact laws (non-penetration, Coulomb friction, and maximum dissipation principle). In this paper, we introduce a frictional-contact pipeline for implicit MPM that is both precise and robust. During the collision detection phase, contact points are localized with particle-centric geometric primitives; during the contact resolution phase, we cast frictional contact as a Nonlinear Complementarity Problem (NCP) over contact impulses and solve it with an Alternating Direction Method of Multipliers (ADMM) scheme. Crucially, the formulation reuses the same implicit MPM linearization, yielding efficiency and numerical stability. The method integrates seamlessly into the implicit MPM loop and is agnostic to modeling choices, including material laws, interpolation functions, and transfer schemes. We evaluate it across seven representative scenes that span elastic and elasto-plastic responses, simple and complex deformable geometries, and a wide range of contact conditions. Overall, the proposed method enables accurate contact localization, reliable frictional handling, and broad generality, making it a practical solution for MPM-based simulations in robotics and related domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>物质点法中摩擦接触问题的求解方法</div>
<div class="mono" style="margin-top:8px">精确处理含摩擦的接触问题始终是物质点法（MPM）的核心瓶颈，涉及从可靠的接触点检测到摩擦接触定律（非穿透条件、库仑摩擦定律及最大耗散原理）的强制执行。本文提出了一种面向隐式MPM的精确鲁棒摩擦接触处理流程：在碰撞检测阶段，采用以粒子为中心的几何基元实现接触点定位；在接触求解阶段，将摩擦接触问题构建为关于接触冲量的非线性互补问题（NCP），并采用交替方向乘子法（ADMM）进行求解。该方案的关键优势在于复用隐式MPM的线性化框架，兼顾计算效率与数值稳定性。该方法可无缝集成至隐式MPM计算循环，且独立于材料本构、插值函数及传输格式等建模选择。通过在七类典型场景（涵盖弹性/弹塑性响应、简单/复杂可变形几何、多样化接触条件）的测试表明：所提方法能实现精确的接触定位、可靠的摩擦处理及广泛的适用性，为机器人学及相关领域的MPM仿真提供了实用解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurately handling frictional contact in the Material Point Method (MPM) is challenging due to difficulties in reliable contact detection and enforcement of contact laws. To address this, the authors propose a pipeline for implicit MPM that localizes contact points using particle-centric geometric primitives and formulates frictional contact resolution as a Nonlinear Complementarity Problem (NCP), solved via an Alternating Direction Method of Multipliers (ADMM) scheme that reuses the implicit MPM linearization for efficiency and stability. Experimental evaluation across seven scenes with varying materials, geometries, and contact conditions demonstrates that the method achieves accurate contact localization, reliable frictional handling, and broad generality for simulations in robotics and related fields.</div>
<div class="mono" style="margin-top:8px">在物质点法中，精确处理带摩擦的接触是一个核心瓶颈，涉及可靠的接触点检测和摩擦接触定律的强制执行。为此，研究者为隐式物质点法提出了一种摩擦接触处理流程：在碰撞检测阶段使用以粒子为中心的几何基元定位接触点；在接触求解阶段，将摩擦接触构建为关于接触冲量的非线性互补问题，并通过交替方向乘子法求解，该公式复用了隐式物质点法的线性化以提高效率和数值稳定性。在涵盖弹性和弹塑性响应、简单与复杂几何以及多种接触条件的七个代表性场景中的实验表明，该方法能实现精确的接触定位、可靠的摩擦处理，并具有广泛的通用性。</div>
</details>
</div>
<div class="card">
<div class="title">Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization</div>
<div class="meta-line">Authors: Ahmad Farooq, Kamran Iqbal</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-02T12:32:28+00:00 · Latest: 2026-02-02T12:32:28+00:00</div>
<div class="meta-line">Comments: Accepted at the 2026 IEEE International Conference on Robotics and Automation (ICRA 2026), Vienna, Austria. 9 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02035v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent reinforcement learning systems deployed in real-world robotics applications face severe communication constraints that significantly impact coordination effectiveness. We present a framework that combines information bottleneck theory with vector quantization to enable selective, bandwidth-efficient communication in multi-agent environments. Our approach learns to compress and discretize communication messages while preserving task-critical information through principled information-theoretic optimization. We introduce a gated communication mechanism that dynamically determines when communication is necessary based on environmental context and agent states. Experimental evaluation on challenging coordination tasks demonstrates that our method achieves 181.8% performance improvement over no-communication baselines while reducing bandwidth usage by 41.4%. Comprehensive Pareto frontier analysis shows dominance across the entire success-bandwidth spectrum with area-under-curve of 0.198 vs 0.142 for next-best methods. Our approach significantly outperforms existing communication strategies and establishes a theoretically grounded framework for deploying multi-agent systems in bandwidth-constrained environments such as robotic swarms, autonomous vehicle fleets, and distributed sensor networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于信息瓶颈与矢量量化的带宽高效多智能体通信</div>
<div class="mono" style="margin-top:8px">现实机器人应用中的多智能体强化学习系统面临严峻通信约束，显著影响协同效能。本文提出融合信息瓶颈理论与矢量量化的框架，实现多智能体环境中的选择性带宽高效通信。该方法通过基于信息论的优化学习压缩与离散化通信消息，同时保留任务关键信息。我们引入门控通信机制，依据环境上下文与智能体状态动态判定通信时机。在复杂协同任务上的实验表明：本方法较无通信基线实现181.8%的性能提升，同时降低41.4%带宽占用。完整的帕累托前沿分析显示，本方法在成功率-带宽全域占据主导地位（曲线下面积0.198，次优方法为0.142），显著优于现有通信策略，为机器人集群、自动驾驶车队、分布式传感器网络等带宽受限环境中的多智能体系统部署建立了理论框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the communication constraints in multi-agent reinforcement learning for real-world robotics, where limited bandwidth hampers coordination. The method integrates information bottleneck theory with vector quantization to selectively compress and discretize messages, preserving task-critical information through information-theoretic optimization, and employs a gated mechanism to dynamically decide when to communicate. Experiments on coordination tasks show a 181.8% performance gain over no-communication baselines while reducing bandwidth usage by 41.4%, with Pareto analysis indicating dominance across success-bandwidth trade-offs (area-under-curve of 0.198 versus 0.142 for next-best methods).</div>
<div class="mono" style="margin-top:8px">为解决现实多智能体机器人应用中严重的通信约束问题，本文提出一个结合信息瓶颈理论与矢量量化的框架，以实现选择性、带宽高效的通信。该方法通过信息论优化学习压缩和离散化消息，同时保留任务关键信息，并采用门控机制动态决定通信的必要性。在协调任务上的实验结果表明，该方法相比无通信基线实现了181.8%的性能提升，同时将带宽使用降低了41.4%，帕累托前沿分析显示其在整个成功率-带宽谱上占优，曲线下面积为0.198，而次优方法为0.142。</div>
</details>
</div>
<div class="card">
<div class="title">Synchronized Online Friction Estimation and Adaptive Grasp Control for Robust Gentle Grasp</div>
<div class="meta-line">Authors: Zhenwei Niu, Xiaoyi Chen, Jiayu Hu, Zhaoyang Liu, Xiaozu Ju</div>
<div class="meta-line">First: 2026-02-02T12:21:27+00:00 · Latest: 2026-02-02T12:21:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02026v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02026v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a unified framework for gentle robotic grasping that synergistically couples real-time friction estimation with adaptive grasp control. We propose a new particle filter-based method for real-time estimation of the friction coefficient using vision-based tactile sensors. This estimate is seamlessly integrated into a reactive controller that dynamically modulates grasp force to maintain a stable grip. The two processes operate synchronously in a closed-loop: the controller uses the current best estimate to adjust the force, while new tactile feedback from this action continuously refines the estimation. This creates a highly responsive and robust sensorimotor cycle. The reliability and efficiency of the complete framework are validated through extensive robotic experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同步在线摩擦估计与自适应抓取控制实现鲁棒轻柔抓取</div>
<div class="mono" style="margin-top:8px">我们提出了一种轻柔机器人抓取统一框架，将实时摩擦估计与自适应抓取控制协同耦合。通过基于视觉的触觉传感器，我们提出一种新的粒子滤波方法用于实时估计摩擦系数。该估计值被无缝集成至反应式控制器中，动态调节抓取力以保持稳定抓持。两个过程在闭环中同步运行：控制器利用当前最优估计调整力度，而该动作产生的新触觉反馈持续优化估计，从而形成高度响应且鲁棒的感知运动循环。通过大量机器人实验验证了完整框架的可靠性与效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To achieve robust gentle grasping that adapts to unknown object friction, this work develops a unified framework that synchronizes real-time friction estimation with adaptive grasp control. The method employs a particle filter to estimate the friction coefficient from vision-based tactile sensor data and integrates this estimate into a reactive controller that dynamically adjusts grasp force. Experimental validation demonstrates that the closed-loop system reliably maintains stable grips by continuously refining friction estimates and modulating forces based on tactile feedback.</div>
<div class="mono" style="margin-top:8px">为实现对未知摩擦物体的鲁棒轻柔抓取，本研究提出了一个将实时摩擦估计与自适应抓取控制同步的统一框架。该方法采用粒子滤波器基于视觉触觉传感器数据估计摩擦系数，并将估计值集成到动态调节抓取力的反应式控制器中。实验验证表明，该闭环系统通过持续优化摩擦估计并根据触觉反馈调节抓力，能够可靠地保持稳定抓取。</div>
</details>
</div>
<div class="card">
<div class="title">A Survey on Efficient Vision-Language-Action Models</div>
<div class="meta-line">Authors: Zhaoshu Yu, Bo Wang, Pengpeng Zeng, Haonan Zhang, Ji Zhang, Zheng Wang, Lianli Gao, Jingkuan Song, Nicu Sebe, Heng Tao Shen</div>
<div class="meta-line">First: 2025-10-27T17:57:33+00:00 · Latest: 2026-02-02T12:16:44+00:00</div>
<div class="meta-line">Comments: 28 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.24795v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.24795v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://evla-survey.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. Despite their remarkable performance, foundational VLAs are hindered by the prohibitive computational and data demands inherent to their large-scale architectures. While a surge of recent research has focused on enhancing VLA efficiency, the field lacks a unified framework to consolidate these disparate advancements. To bridge this gap, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire model-training-data pipeline. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效视觉-语言-动作模型综述</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型（VLA）是具身智能的重要前沿领域，旨在连接数字知识与物理世界交互。尽管性能显著，基础VLA模型受限于其大规模架构固有的巨大计算与数据需求。近期研究虽聚焦提升VLA效率，但领域缺乏统一框架整合这些分散进展。为填补空白，本综述首次对高效视觉-语言-动作模型（Efficient VLA）在全流程（模型-训练-数据）进行系统性梳理：提出统一分类体系，将现有技术归纳为三大支柱：（1）高效模型设计——聚焦高效架构与模型压缩；（2）高效训练——降低模型学习阶段计算负担；（3）高效数据收集——解决机器人数据获取与利用瓶颈。通过在此框架下批判性评述前沿方法，本综述不仅为学界建立基础参考，还总结了典型应用、阐明关键挑战并规划未来研究方向。项目页面持续更新：https://evla-survey.github.io/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This survey addresses the computational and data inefficiencies of large-scale Vision-Language-Action models (VLAs) that hinder their deployment in embodied intelligence. It proposes a unified taxonomy to review efficient VLA techniques, categorizing them into three pillars: efficient model design, efficient training, and efficient data collection. The work consolidates state-of-the-art methods, summarizes applications, identifies challenges, and outlines a future research roadmap to advance the field.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决基础视觉-语言-动作模型在具身智能应用中面临的高计算与数据需求瓶颈，这些瓶颈阻碍了其实际部署。为此，该综述提出了一个统一的分类法，系统性地回顾了高效VLA技术，将其归纳为三大支柱：高效模型设计（如架构与压缩）、降低学习成本的高效训练，以及面向机器人数据的高效收集。通过对前沿方法的批判性分析，主要成果整合了该领域的分散进展，建立了基础性参考框架，并基于此规划了未来的研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Reformulating AI-based Multi-Object Relative State Estimation for Aleatoric Uncertainty-based Outlier Rejection of Partial Measurements</div>
<div class="meta-line">Authors: Thomas Jantos, Giulio Delama, Stephan Weiss, Jan Steinbrener</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-02T12:04:34+00:00 · Latest: 2026-02-02T12:04:34+00:00</div>
<div class="meta-line">Comments: Accepted for publication at ICRA 2026, Vienna, Austria</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02006v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02006v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Precise localization with respect to a set of objects of interest enables mobile robots to perform various tasks. With the rise of edge devices capable of deploying deep neural networks (DNNs) for real-time inference, it stands to reason to use artificial intelligence (AI) for the extraction of object-specific, semantic information from raw image data, such as the object class and the relative six degrees of freedom (6-DoF) pose. However, fusing such AI-based measurements in an Extended Kalman Filter (EKF) requires quantifying the DNNs&#x27; uncertainty and outlier rejection capabilities.
  This paper presents the benefits of reformulating the measurement equation in AI-based, object-relative state estimation. By deriving an EKF using the direct object-relative pose measurement, we can decouple the position and rotation measurements, thus limiting the influence of erroneous rotation measurements and allowing partial measurement rejection. Furthermore, we investigate the performance and consistency improvements for state estimators provided by replacing the fixed measurement covariance matrix of the 6-DoF object-relative pose measurements with the predicted aleatoric uncertainty of the DNN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于人工智能的多目标相对状态估计重构：面向部分测量的随机不确定性异常值剔除</div>
<div class="mono" style="margin-top:8px">相对于一组目标物体的精确定位使移动机器人能够执行多种任务。随着能够部署深度神经网络进行实时推理的边缘设备兴起，利用人工智能从原始图像数据中提取物体特定的语义信息（如物体类别和相对六自由度位姿）成为合理选择。然而，在扩展卡尔曼滤波中融合此类基于AI的测量需要量化深度神经网络的不确定性和异常值剔除能力。本文阐述了在基于AI的物体相对状态估计中重构测量方程的优势：通过推导使用直接物体相对位姿测量的EKF，可实现位置与旋转测量的解耦，从而限制错误旋转测量的影响并支持部分测量剔除。此外，我们研究了用深度神经网络预测的随机不确定性替代固定测量协方差矩阵后，对状态估计器性能与一致性的提升效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the need to improve the reliability of AI-based object-relative state estimation for mobile robots, as deep neural network (DNN) measurements for 6-DoF pose can be uncertain or contain outliers. The method reformulates the measurement equation for an Extended Kalman Filter (EKF) to directly use object-relative pose measurements, decoupling position and rotation to limit error propagation and enable partial measurement rejection; it further replaces a fixed measurement covariance with the DNN&#x27;s predicted aleatoric uncertainty. Key experimental findings show that this approach enhances estimator performance and consistency by effectively rejecting outliers and better quantifying measurement uncertainty.</div>
<div class="mono" style="margin-top:8px">本研究针对移动机器人定位中融合基于AI的目标位姿测量所面临的挑战，即深度神经网络（DNN）能提供语义信息，但需要可靠的 uncertainty 量化和异常值剔除以实现稳健的状态估计。该方法重新构建了扩展卡尔曼滤波器（EKF）的测量方程，以直接使用目标相对位姿测量，解耦位置和旋转以限制误差传播，并支持对部分错误测量的剔除；同时，用DNN预测的偶然不确定性替代了固定的测量协方差矩阵。实验结果表明，该重构通过有效减轻异常旋转测量的影响，提升了状态估计器的性能和一致性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260204_0541.html">20260204_0541</a>
<a href="archive/20260204_0456.html">20260204_0456</a>
<a href="archive/20260204_0354.html">20260204_0354</a>
<a href="archive/20260202_0623.html">20260202_0623</a>
<a href="archive/20260202_0525.html">20260202_0525</a>
<a href="archive/20260202_0441.html">20260202_0441</a>
<a href="archive/20260202_0331.html">20260202_0331</a>
<a href="archive/20260201_0625.html">20260201_0625</a>
<a href="archive/20260201_0527.html">20260201_0527</a>
<a href="archive/20260201_0443.html">20260201_0443</a>
<a href="archive/20260201_0331.html">20260201_0331</a>
<a href="archive/20260131_0628.html">20260131_0628</a>
<a href="archive/20260131_0535.html">20260131_0535</a>
<a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
