<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-31 05:35</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260131_0535</div>
    <div class="row"><div class="card">
<div class="title">Discovering Hidden Gems in Model Repositories</div>
<div class="meta-line">Authors: Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-29T18:59:55+00:00 · Latest: 2026-01-29T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22157v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of &quot;hidden gems&quot;, unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模型仓库中隐藏瑰宝的发现</div>
<div class="mono" style="margin-top:8px">公共模型仓库托管着数百万个微调模型，但社区使用仍过度集中在少数基础检查点上。本研究探讨这种集中现象反映的是有效的市场选择，还是优质模型被系统性忽视。通过对2000多个模型的大规模评估，我们揭示了“隐藏瑰宝”的普遍存在——这些冷门微调模型显著优于热门同类。值得注意的是，在Llama-3.1-8B模型家族中，我们发现下载量极少的检查点能将数学性能从83.2%提升至96.0%，且不增加推理成本。然而，通过对每个上传模型进行穷举评估来发现这些模型在计算上是不可行的。为此，我们将模型发现问题建模为多臂老虎机问题，通过共享查询集和激进淘汰策略加速序列减半搜索算法。我们的方法仅需每个候选模型约50次查询即可检索出最优模型，将发现速度提升50倍以上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The study investigates the concentration of community usage on a few foundation models in public repositories, questioning whether this reflects optimal selection or if superior fine-tuned models are being overlooked. By evaluating over 2,000 models, the authors identify &quot;hidden gems&quot;—unpopular fine-tunes that outperform popular ones, such as rarely downloaded Llama-3.1-8B checkpoints improving math performance from 83.2% to 96.0% without added inference costs. To address the computational infeasibility of exhaustive evaluation, they formulate model discovery as a Multi-Armed Bandit problem, accelerating the Sequential Halving algorithm with shared query sets and aggressive elimination schedules, achieving over 50x faster discovery with as few as 50 queries per candidate.</div>
<div class="mono" style="margin-top:8px">本研究探讨了公共模型库中社区使用高度集中于少数基础模型的现象，是反映了有效的市场选择，还是存在性能更优的微调模型被系统性忽视的问题。通过对超过2000个模型进行广泛评估，作者揭示了&#x27;隐藏瑰宝&#x27;的普遍存在，即不受欢迎但显著优于流行模型的微调版本，例如一些下载量极少的Llama-3.1-8B检查点，能在不增加推理成本的情况下将数学性能从83.2%提升至96.0%。为解决穷举评估的计算不可行性，他们将模型发现问题形式化为多臂老虎机问题，并通过使用共享查询集和激进淘汰策略来加速序列减半搜索算法，从而仅需每个候选模型约50次查询即可检索到顶级模型，将发现速度提升了超过50倍。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</div>
<div class="meta-line">Authors: Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu</div>
<div class="meta-line">First: 2026-01-29T18:59:53+00:00 · Latest: 2026-01-29T18:59:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22156v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合线性注意力机制的正确实现：面向超长上下文的高效蒸馏与有效架构</div>
<div class="mono" style="margin-top:8px">混合Transformer架构通过结合softmax注意力模块与循环神经网络（RNN），在长上下文建模中展现出理想的性能-吞吐量权衡，但其大规模从头预训练的高昂成本阻碍了实际应用与研究。近期研究表明，预训练的softmax注意力模块可通过参数迁移与知识蒸馏转化为RNN模块，但现有迁移方法需消耗大量训练数据（超过100亿词元），且所得混合模型在长上下文场景下表现欠佳——而这正是混合模型相比纯Transformer模型具备显著推理加速优势的场景。本文提出HALO（基于层优化的混合注意力），一种将Transformer模型蒸馏为RNN-注意力混合模型的流程，并进一步提出HypeNet混合架构。该架构通过新型位置编码方案（命名为HyPE）及多项结构改进，实现了卓越的长度泛化能力。我们使用HALO将Qwen3系列模型转换为HypeNet，在保持与原Transformer模型相当性能的同时，获得了更优的长上下文处理性能与效率，整个转换过程仅需23亿词元，不足其预训练数据的0.01%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high computational cost of pre-training hybrid Transformer-RNN models for long-context tasks, this work introduces HALO, an efficient distillation pipeline that converts pre-trained softmax attention blocks into RNN blocks, requiring only 2.3B tokens of training data. The method is paired with a novel hybrid architecture called HypeNet, which incorporates a new position encoding scheme (HyPE) and architectural modifications to enhance length generalization. Experimental results show that converting the Qwen3 model series into HypeNet via HALO yields performance comparable to the original Transformer while achieving superior long-context performance and inference efficiency.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决为长上下文建模从头预训练混合Transformer-RNN模型的高计算成本问题，并改进现有依赖大量数据的转换方法在长上下文性能上的不足。方法上提出了HALO，一个将预训练Transformer模型蒸馏为混合模型的流程，以及HypeNet，一种新颖的混合架构，采用新的位置编码方案（HyPE）和架构修改以增强长度泛化能力。关键实验结果表明，使用仅2.3B token（少于原始预训练数据的0.01%）通过HALO转换Qwen3模型系列，实现了与原始Transformer相当的性能，同时获得了更优的长上下文性能和推理效率。</div>
</details>
</div>
<div class="card">
<div class="title">Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing</div>
<div class="meta-line">Authors: Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang</div>
<div class="meta-line">First: 2026-01-29T18:59:50+00:00 · Latest: 2026-01-29T18:59:50+00:00</div>
<div class="meta-line">Comments: accepted by DATE2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22151v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22151v1">PDF</a> · <a href="https://github.com/TUDa-HWAI/NN2Logic">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.
  The code is open source at https://github.com/TUDa-HWAI/NN2Logic</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最新成果：面向边缘计算的神经网络逻辑流转换方法</div>
<div class="mono" style="margin-top:8px">神经网络已成功应用于各类资源受限的边缘设备。由于功耗限制，这些设备通常配备中央处理器而非图形处理器。当前前沿研究仍聚焦于高效执行海量乘累加运算，但CPU本身并不擅长大规模执行此类数学运算，因其更适于执行控制流逻辑（即计算机算法）。为提升神经网络在CPU上的计算效率，本文提出将其转换为逻辑流执行。具体而言，首先将神经网络转换为等效决策树，从中筛选具有恒定叶节点的决策路径，并压缩为逻辑流。此类逻辑流由if-else结构及精简的乘累加运算构成。实验结果表明，在模拟RISC-V CPU上延迟最高可降低14.9%，且精度无损。代码已开源：https://github.com/TUDa-HWAI/NN2Logic</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To improve neural network execution efficiency on CPU-based edge devices, which are better suited for control flow logic than extensive multiply-accumulate (MAC) operations, this work proposes converting neural networks into logic flows. The method first transforms neural networks into equivalent decision trees, then selects and compresses constant-leaf decision paths into logic flows composed of if-else structures with fewer MAC operations. Experiments on a simulated RISC-V CPU show the approach reduces latency by up to 14.9% without accuracy loss.</div>
<div class="mono" style="margin-top:8px">为提高神经网络在基于CPU的边缘设备上的执行效率，这些设备更擅长控制流逻辑而非大规模乘累加运算，本研究提出将神经网络转换为逻辑流。该方法首先将神经网络转换为等效的决策树，然后选择并压缩具有恒定叶节点的决策路径，形成由if-else结构和少量乘累加操作组成的逻辑流。在模拟RISC-V CPU上的实验表明，该方法可在不损失精度的情况下将延迟降低高达14.9%。</div>
</details>
</div>
<div class="card">
<div class="title">FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</div>
<div class="meta-line">Authors: Ajay Patel, Colin Raffel, Chris Callison-Burch</div>
<div class="meta-line">First: 2026-01-29T18:58:47+00:00 · Latest: 2026-01-29T18:58:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22146v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22146v1">PDF</a> · <a href="https://huggingface.co/fineinstructions">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised &quot;predict the next word&quot; objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of &quot;instruction-tuning&quot; data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With &quot;supervised&quot; synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FineInstructions：将合成指令扩展至预训练规模</div>
<div class="mono" style="margin-top:8px">由于监督训练数据有限，大语言模型（LLMs）通常通过自监督的“预测下一个词”目标在海量非结构化文本数据上进行预训练。为使模型对用户有用，还需在规模小得多的“指令调优”数据上进行进一步训练，这些数据由指令与响应的监督训练样本组成。为克服监督数据不足的问题，我们提出一种方法，可将互联网规模预训练文档中的知识转化为数十亿条合成指令与答案训练对。所得数据集名为FineInstructions，使用了约1800万个基于真实用户查询和提示创建的指令模板。这些模板与来自非结构化预训练语料库的人工撰写源文档匹配并实例化。通过在此规模生成的“监督式”合成训练数据，LLM可完全基于指令调优目标从头开始预训练，这更贴近LLMs预期下游用途（响应用户提示）的数据分布。我们进行了严格的逐词训练实验，发现在衡量自由形式响应质量的标准基准测试中，基于FineInstructions的预训练表现优于标准预训练及其他已提出的合成预训练技术。相关资源可在 https://huggingface.co/fineinstructions 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the scarcity of supervised instruction-tuning data for large language models (LLMs), this research introduces FineInstructions, a method to generate billions of synthetic instruction-answer pairs by transforming knowledge from unstructured pre-training corpora. The approach uses approximately 18 million instruction templates derived from real user queries, which are then matched and instantiated with human-written source documents from internet-scale pre-training data. Experimental results from controlled token-for-token training show that pre-training an LLM from scratch solely on this synthetic instruction data outperforms standard self-supervised pre-training and other synthetic pre-training techniques on benchmarks evaluating free-form response quality.</div>
<div class="mono" style="margin-top:8px">为解决大语言模型（LLM）监督指令微调数据稀缺的问题，本研究提出了FineInstructions方法，通过将非结构化预训练语料库中的知识转化为数十亿个合成指令-答案对来生成数据。该方法使用约1800万个源自真实用户查询的指令模板，并将其与互联网规模预训练数据中的人工撰写源文档进行匹配和实例化。在受控的逐词训练实验中，结果表明，完全基于此合成指令数据从头预训练的LLM，在评估自由形式响应质量的基准测试上，优于标准的自监督预训练和其他提出的合成预训练技术。</div>
</details>
</div>
<div class="card">
<div class="title">MORPH: PDE Foundation Models with Arbitrary Data Modality</div>
<div class="meta-line">Authors: Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Alexander Scheinker, Diane Oyen, Nathan Debardeleben, Earl Lawrence, Ayan Biswas</div>
<div class="meta-line">First: 2025-09-25T22:38:36+00:00 · Latest: 2026-01-29T18:57:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21670v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.21670v4">PDF</a> · <a href="https://github.com/lanl/MORPH">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters, MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MORPH：支持任意数据模态的偏微分方程基础模型</div>
<div class="mono" style="margin-top:8px">本文提出MORPH——一种与模态无关、自回归的偏微分方程基础模型。该模型基于卷积视觉Transformer架构，能无缝处理不同分辨率（1D-3D）、多物理场（混合标量与矢量分量）的异构时空数据集。其架构融合三大核心组件：（1）分量卷积模块，联合处理标量与矢量通道以捕捉局部相互作用；（2）场间交叉注意力机制，建模并选择性传递不同物理场间的信息；（3）轴向注意力机制，将完整时空自注意力分解至独立的空间/时间轴，在保持表达力的同时降低计算负担。我们在异构PDE数据集上预训练了多个模型变体，并评估其在下游预测任务中的迁移性能。通过全模型微调与参数高效的低秩适配器技术，MORPH显著优于从头训练的模型。大量实验表明，该模型在多项任务中达到或超越现有基线及前沿方法。这些能力共同构成了灵活而强大的学习框架，能够有效处理科学观测数据的异构性与多模态特性，为可扩展、数据高效的科学机器学习开辟了新路径。项目代码、数据集与模型已开源：https://github.com/lanl/MORPH。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of learning from heterogeneous spatiotemporal datasets in scientific machine learning by introducing MORPH, a modality-agnostic autoregressive foundation model for partial differential equations. The method employs a convolutional vision transformer backbone with component-wise convolution for local interactions, inter-field cross-attention for information propagation between physical fields, and axial attentions to reduce computational cost. Experimental results show that pretrained MORPH variants, fine-tuned or adapted with low-rank adapters, outperform models trained from scratch and match or exceed strong baselines and state-of-the-art models on various downstream prediction tasks.</div>
<div class="mono" style="margin-top:8px">该研究针对科学机器学习中从异构时空数据集中学习的挑战，提出了MORPH，一种与模态无关、自回归的偏微分方程基础模型。该方法采用卷积视觉Transformer主干网络，结合分量卷积处理局部交互，场间交叉注意力在不同物理场间传播信息，以及轴向注意力以降低计算成本。在多样化PDE数据集上预训练并在下游任务上微调的实验结果表明，MORPH优于从头训练的模型，并达到或超越了强基线及最新先进模型的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data</div>
<div class="meta-line">Authors: Grzegorz Stefanski, Alberto Presta, Michal Byra</div>
<div class="meta-line">First: 2026-01-29T18:56:41+00:00 · Latest: 2026-01-29T18:56:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22141v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>路由彩票：面向异构数据的自适应子网络</div>
<div class="mono" style="margin-top:8px">在剪枝中，彩票假说认为大型网络包含稀疏子网络（即“中奖彩票”），这些子网络可独立训练以达到与稠密网络相当的性能。然而，现有方法大多假设存在一个适用于所有输入的通用中奖彩票，忽略了现实数据的固有异构性。本研究提出“路由彩票”（RTL），一种自适应剪枝框架，能够发现多个专用子网络（称为自适应彩票），每个子网络针对特定类别、语义簇或环境条件进行定制。在多种数据集和任务中，RTL在平衡准确率和召回率上持续优于单模型及多模型基线，同时参数使用量比独立模型减少高达10倍，且表现出语义对齐特性。此外，我们识别了在激进剪枝下出现的“子网络坍缩”现象，并提出一种子网络相似度评分，可实现无需标签的过度稀疏化诊断。总体而言，本研究将剪枝重新定义为一种使模型结构与数据异构性对齐的机制，为构建更模块化、情境感知的深度学习模型开辟了新路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the observation that the Lottery Ticket Hypothesis typically assumes a single universal winning subnetwork, which overlooks the heterogeneity of real-world data. To address this, the authors propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, termed adaptive tickets, each tailored to specific data classes, clusters, or conditions. Experimental results across diverse datasets and tasks show that RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, using up to 10 times fewer parameters than independent models while demonstrating semantic alignment; the work also identifies subnetwork collapse under aggressive pruning and introduces a similarity score for diagnosing oversparsification.</div>
<div class="mono" style="margin-top:8px">本研究动机源于观察到彩票假说通常假设存在单一通用获胜子网络，这忽略了现实世界数据的异质性。方法提出了“路由彩票”（RTL），这是一种自适应剪枝框架，能发现多个称为自适应彩票的专用子网络，每个子网络针对特定数据类别、语义簇或环境条件进行定制。关键实验结果表明，在多种数据集和任务中，RTL在平衡准确率和召回率上持续优于单模型和多模型基线，同时使用的参数比独立模型少多达10倍，且结构呈现语义对齐；该框架还通过一种新的相似度分数识别并诊断了在激进剪枝下的子网络崩溃现象。</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</div>
<div class="meta-line">Authors: Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22137v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRISM：用于加速神经网络训练的无分布自适应矩阵函数计算</div>
<div class="mono" style="margin-top:8px">矩阵函数（如平方根、逆根和正交化）在神经网络训练的预处理梯度方法中起着核心作用。这推动了迭代算法的发展，这些算法避免显式特征分解，主要依赖矩阵乘法，从而非常适合现代GPU加速器。我们提出PRISM（用于矩阵函数计算的多项式拟合与随机迭代草图），这是一个加速计算矩阵函数的迭代算法的通用框架。PRISM将自适应多项式逼近与随机草图技术相结合：在每次迭代中，它通过草图最小二乘问题拟合当前谱的多项式代理，以最小开销自适应于当前实例。我们将PRISM应用于加速类牛顿-舒尔茨迭代的矩阵平方根和正交化计算，这些是机器学习中的核心原语。与现有方法不同，PRISM无需显式谱界或奇异值估计，并能自动适应演化中的谱。实证表明，将PRISM集成到Shampoo和Muon优化器中可加速训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Matrix functions like square roots and orthogonalization are crucial for preconditioned gradient methods in neural network training, but existing iterative algorithms often require explicit spectral bounds or singular value estimates, limiting their adaptability. To address this, the authors propose PRISM, a framework that accelerates iterative matrix function computation by combining adaptive polynomial approximation with randomized sketching; at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, automatically adapting to the evolving spectrum without needing prior spectral bounds. Experimental results show that PRISM effectively accelerates training when integrated into optimizers such as Shampoo and Muon, demonstrating improved performance without manual tuning.</div>
<div class="mono" style="margin-top:8px">矩阵函数如平方根和正交化在神经网络训练的预条件梯度方法中至关重要，但现有迭代算法通常依赖显式的谱边界或奇异值估计，限制了其适应性和效率。为此，作者提出了PRISM框架，通过结合自适应多项式逼近和随机化草图技术来加速迭代矩阵函数计算；在每次迭代中，它通过草图最小二乘问题拟合当前谱的多项式代理，自动适应演化谱而无需先验谱边界。实验结果表明，将PRISM集成到Shampoo和Muon等优化器中能有效加速训练，展示了无需手动调优的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">StepShield: When, Not Whether to Intervene on Rogue Agents</div>
<div class="meta-line">Authors: Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 2 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StepShield：何时而非是否干预恶意智能体</div>
<div class="mono" style="margin-top:8px">现有智能体安全基准仅报告二元准确率，将早期干预与事后分析混为一谈。在第8步标记违规的检测器可实现干预，而在第48步报告的检测器仅具取证价值。这一区分至关重要，但现有基准无法衡量。我们推出首个评估违规检测时机的基准StepShield，其包含9,213条代码智能体轨迹，含1,278个精细标注的训练对和7,935条轨迹的测试集（真实恶意率8.1%）。恶意行为基于六类现实安全事件。我们提出三个新颖的时序指标：早期干预率（EIR）、干预间隔和节省令牌数。评估显示，基于LLM的评判器达到59% EIR，而静态分析器仅26%，这2.3倍的性能差距在标准准确率指标中完全不可见。早期检测具有直接经济效益：级联混合检测器HybridGuard将监控成本降低75%，预计企业级应用五年累计节省1.08亿美元。通过将评估重点从‘是否’转向‘何时’，StepShield为构建更安全、更经济可行的AI智能体奠定新基础。代码与数据基于Apache 2.0协议开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current agent safety benchmarks only measure binary detection accuracy, failing to distinguish between early intervention and post-mortem analysis, which is critical for practical safety. To address this, the authors introduce StepShield, a benchmark with 9,213 code agent trajectories grounded in real-world security incidents, and propose three temporal metrics: Early Intervention Rate, Intervention Gap, and Tokens Saved. Experimental results reveal that an LLM-based judge achieves a 59% Early Intervention Rate, outperforming a static analyzer&#x27;s 26%—a gap invisible to standard accuracy metrics—and demonstrate that their cascaded HybridGuard detector can reduce monitoring costs by 75%, projecting significant economic savings.</div>
<div class="mono" style="margin-top:8px">现有智能体安全基准仅报告二元检测准确率，无法区分早期干预与事后分析，而这对实际安全至关重要。为此，研究者提出了StepShield基准，通过分析9,213条代码智能体轨迹（其中包含基于真实安全事件的恶意行为）来评估违规行为被检测到的时机。提出的时序指标显示，基于大语言模型的判断器实现了59%的早期干预率，显著优于静态分析器的26%——这一性能差距在标准准确率指标中完全无法体现；实验进一步表明，通过级联混合检测器进行早期干预可将监控成本降低75%，并预计在企业规模下实现可观的经济效益。</div>
</details>
</div>
<div class="card">
<div class="title">Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</div>
<div class="meta-line">Authors: Ziming Dong, Hardik Sharma, Evan O&#x27;Toole, Jaya Prakash Champati, Kui Wu</div>
<div class="meta-line">First: 2026-01-29T18:52:54+00:00 · Latest: 2026-01-29T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22132v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>支付提示而非答案：面向成本效益推理的LLM引导框架</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在复杂推理任务上展现出顶尖性能，但其推理成本限制了规模化部署。小语言模型（SLMs）虽能大幅节约成本，但准确性显著落后。现有路由与级联方法将LLM视为全有或全无的资源：查询要么完全绕过LLM，要么以全额成本由LLM生成完整响应。本文提出LLM引导框架，仅向LLM请求简短前缀（提示）并提供给SLM。这一简单机制在数学与编程任务中效果显著：即使仅包含完整LLM响应10-30%的提示也能大幅提升SLM准确率。引导框架泛化了路由与级联方法，在理想决策下实现更低成本。我们开发了联合判断是否需要提示及请求词元数量的两阶段预测器。在广泛使用的数学推理（GSM8K、CNK12）和代码生成（HumanEval、MBPP）基准测试中，引导框架相比纯LLM推理可降低42-94%成本；与前沿路由和级联基线相比，在保持准确性的同时实现最高2.8倍的成本缩减。据我们所知，这是首个利用词元级预算控制实现SLM-LLM协同的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To address the high inference costs of Large Language Models (LLMs) while mitigating the accuracy gap of Small Language Models (SLMs), this work introduces LLM Shepherding, a framework that strategically requests only a short initial prefix, or &#x27;hint&#x27;, from the LLM to guide the SLM&#x27;s response generation. The method employs a two-stage predictor to decide if a hint is needed and its optimal length, generalizing prior routing and cascading approaches. Experiments on mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks show that hints comprising 10-30% of a full LLM response significantly boost SLM accuracy, achieving 42-94% cost savings compared to LLM-only inference and up to a 2.8x cost reduction over existing baselines while maintaining accuracy.</div>
<div class="mono" style="margin-top:8px">为解决大语言模型推理成本高昂而小语言模型精度不足的问题，本研究提出了LLM Shepherding框架，其核心方法是仅向大模型请求一个简短的前缀提示来引导小模型生成完整回答。该方法通过一个两阶段预测器联合决定是否需要提示以及请求的令牌数量，从而泛化了现有的路由和级联方法。在数学推理和代码生成基准测试上的实验表明，该方法相比仅使用大模型将推理成本降低了42-94%，并在保持精度的同时，相比先进的基线方法实现了高达2.8倍的成本降低。</div>
</details>
</div>
<div class="card">
<div class="title">SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization</div>
<div class="meta-line">Authors: Leonard Papenmeier, Petru Tighineanu</div>
<div class="meta-line">First: 2026-01-29T18:51:58+00:00 · Latest: 2026-01-29T18:51:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMOG：面向多目标贝叶斯优化的可扩展元学习方法</div>
<div class="mono" style="margin-top:8px">多目标优化旨在解决具有竞争性目标的问题，通常仅能通过黑盒方式访问问题且测量预算有限。许多应用场景中存在来自相关优化任务的历史数据，这为利用元学习加速优化提供了可能。贝叶斯优化作为黑盒优化的有效技术，已分别扩展至元学习和多目标优化领域，但能同时处理两种场景的方法——即面向多目标贝叶斯优化的元学习先验——仍鲜有研究。本文提出SMOG，一种基于多输出高斯过程的可扩展模块化元学习模型，能显式学习目标间的相关性。SMOG构建跨元任务与目标任务的结构化联合高斯过程先验，在基于元数据条件化后，通过灵活残差多输出核增强得到闭式目标任务先验。该构建以理论完备的方式将元数据不确定性传递至目标代理模型。SMOG支持分层并行训练：元任务高斯过程经单次拟合后缓存，实现与元任务数量的线性扩展。所得代理模型可与标准多目标贝叶斯优化采集函数无缝集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the need for efficient multi-objective optimization in black-box settings where historical data from related tasks is available, aiming to leverage meta-learning to accelerate the process. The method introduces SMOG, a scalable meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives, constructing a structured joint prior across tasks and yielding a closed-form target-task prior with a flexible residual kernel to propagate metadata uncertainty. Experimental results demonstrate that SMOG achieves linear scaling with the number of meta-tasks through hierarchical, parallel training and integrates seamlessly with standard acquisition functions for multi-objective Bayesian optimization.</div>
<div class="mono" style="margin-top:8px">该研究针对测量次数有限的多目标优化问题，利用相关任务的历史数据，通过元学习来加速优化过程。方法提出了SMOG，这是一种基于多输出高斯过程的可扩展元学习模型，它显式学习目标间的相关性，构建跨任务的结构化联合先验，并产生一个带有灵活残差核的闭式目标任务先验，以传播元数据的不确定性。实验结果表明，SMOG通过分层并行训练实现了与元任务数量的线性扩展，并能与标准采集函数无缝集成，实现高效优化。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</div>
<div class="meta-line">Authors: Yifeng Ding, Lingming Zhang</div>
<div class="meta-line">First: 2026-01-29T18:50:29+00:00 · Latest: 2026-01-29T18:50:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22129v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22129v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Replay：面向软件工程代理的高效测试时扩展方法</div>
<div class="mono" style="margin-top:8px">测试时扩展已被广泛用于增强大语言模型代理在软件工程任务中的能力，但传统从头重复采样轨迹的方法计算成本高昂。现有方法虽尝试通过专用价值代理降低成本，但存在模型校准偏差且难以适配合成自定义bash脚本工具的现代代理。本文提出SWE-Replay，首个无需依赖噪声价值评估的高效通用测试时扩展技术。该方法通过复用历史试验轨迹，在关键中间步骤动态选择从头探索或利用存档经验，其步骤选择机制基于代码库探索潜力与推理意义，而非外部大语言模型的质量评估。在SWE-Bench Verified上的实验表明，SWE-Replay在保持性能（最高提升3.8%）的同时，持续优于原始扩展方法并降低最高17.4%的成本。在SWE-Bench Pro与多语言场景的进一步验证证实了其泛化能力，为软件工程代理的高效测试时扩展奠定了坚实基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the high computational cost of standard test-time scaling for LLM-based software engineering agents, which involves repeatedly sampling new trajectories from scratch, and the limitations of recent value-based methods that suffer from miscalibration and poor generalization to agents using custom bash scripts. The proposed method, SWE-Replay, introduces an efficient and generalizable scaling technique that recycles trajectories from prior trials, dynamically choosing between exploring from scratch or exploiting archived experience by branching at critical intermediate steps selected based on the potential and reasoning significance of repository exploration, without relying on external LLM quality estimates. Experimental results on SWE-Bench Verified demonstrate that SWE-Replay reduces costs by up to 17.4% while maintaining or improving performance by up to 3.8% compared to naive scaling, with further evaluations on SWE-Bench Pro and Multilingual confirming its generalizability.</div>
<div class="mono" style="margin-top:8px">本研究针对基于大语言模型的软件工程代理在测试时扩展中因重复从头采样轨迹而产生的高计算成本，以及近期基于价值评估的方法存在校准偏差、难以泛化至使用自定义bash脚本的现代代理的局限性。提出的方法SWE-Replay是一种高效且可泛化的扩展技术，它通过回收先前试验的轨迹，根据仓库探索的潜力和推理重要性，在关键中间步骤动态选择从头探索或利用存档经验进行分支，而不依赖外部大语言模型的质量评估。在SWE-Bench Verified上的实验结果表明，与朴素扩展相比，SWE-Replay在保持或提升性能达3.8%的同时，将成本降低了高达17.4%；在SWE-Bench Pro和Multilingual上的进一步评估验证了其泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</div>
<div class="meta-line">Authors: John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</div>
<div class="meta-line">First: 2026-01-29T18:49:27+00:00 · Latest: 2026-01-29T18:49:27+00:00</div>
<div class="meta-line">Comments: Project page: https://edit-yourself.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://edit-yourself.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EditYourself：基于扩散变换器的音频驱动说话人视频生成与编辑框架</div>
<div class="mono" style="margin-top:8px">当前生成式视频模型擅长根据文本和图像提示生成新内容，但在编辑现有预录制视频方面存在关键空白——对口语脚本的细微修改需保持动作连贯性、时序一致性、说话人身份特征及精准唇形同步。本文提出EditYourself，一种基于扩散变换器的音频驱动视频到视频编辑框架，支持通过文字脚本修改说话人视频，包括视觉语音内容的无缝增删与时间轴调整。该框架在通用视频扩散模型基础上，通过音频条件约束与区域感知的编辑专项训练扩展其视频到视频能力，实现精准唇形同步和基于时空修复的时序连贯重构，能在新增片段中合成逼真人体动作，同时长期保持视觉保真度与身份一致性。本工作标志着生成式视频模型向专业视频后期制作实用工具迈出基础性一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of current generative video models in editing existing talking head videos, where script modifications must preserve motion coherence, speaker identity, and lip synchronization. The method introduces EditYourself, a diffusion transformer-based framework that augments a general-purpose video diffusion model with audio conditioning and region-aware training for video-to-video editing, enabling transcript-based addition, removal, and retiming of spoken content via spatiotemporal inpainting. Experimental results demonstrate that the approach achieves precise lip synchronization, temporally coherent restructuring, realistic motion synthesis in new segments, and maintains visual fidelity and identity consistency over long durations, advancing generative models for practical video post-production.</div>
<div class="mono" style="margin-top:8px">本研究针对当前生成式视频模型在编辑预录制说话人视频方面的不足，即修改脚本时需要保持动作连贯性、说话人身份和口型同步。方法上提出了EditYourself，这是一个基于扩散变换器（DiT）的框架，通过音频条件化和区域感知训练增强通用视频扩散模型，实现音频驱动的视频到视频编辑，支持基于文本脚本的内容添加、删除和重定时，并利用时空修复技术。实验结果表明，该系统能够实现精确的口型同步，在新片段中合成逼真的人体动作，并在长时间内保持视觉保真度和身份一致性，标志着向实用视频后期制作生成工具迈出了基础性一步。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics</div>
<div class="meta-line">Authors: Winfried Ripken, Michael Plainer, Gregor Lied, Thorben Frank, Oliver T. Unke, Stefan Chmiela, Frank Noé, Klaus Robert Müller</div>
<div class="meta-line">First: 2026-01-29T18:47:46+00:00 · Latest: 2026-01-29T18:47:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22123v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习哈密顿流映射：大时间步分子动力学的平均流一致性</div>
<div class="mono" style="margin-top:8px">模拟哈密顿系统的长期演化受限于稳定数值积分所需的小时间步长。为突破此限制，我们提出一种学习哈密顿流映射的框架，通过预测选定时间跨度$Δt$内的平均相空间演化，实现远超经典积分器稳定性极限的大时间步稳定更新。为此，我们对时间平均哈密顿动力学施加平均流一致性条件。与先前方法不同，该框架仅需独立相空间样本进行训练，无需获取未来状态，从而避免昂贵的轨迹生成。在多种哈密顿系统中的验证表明，本方法尤其能改进基于机器学习力场（MLFF）的分子动力学模拟。所提模型在保持相近训练与推理成本的同时，支持显著增大的积分时间步长，且可直接利用广泛存在的无轨迹MLFF数据集进行训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Simulating long-time Hamiltonian dynamics is constrained by the small timesteps needed for numerical stability in classical integrators. To overcome this, the authors propose a framework for learning Hamiltonian Flow Maps by predicting the mean phase-space evolution over a large timestep Δt, enforcing a Mean Flow consistency condition for time-averaged dynamics. This approach enables training on independent phase-space samples without requiring future states or expensive trajectory generation. Experimental validation across diverse Hamiltonian systems shows the method particularly enhances molecular dynamics simulations with machine-learned force fields, maintaining comparable computational cost while supporting significantly larger stable integration timesteps when trained on trajectory-free datasets.</div>
<div class="mono" style="margin-top:8px">模拟哈密顿系统的长时间演化受限于传统数值积分器所需的小时间步长。为突破此限制，本文提出一个学习哈密顿流映射的框架，通过预测选定大时间步长Δt内的平均相空间演化，并施加源自时间平均动力学的平均流一致性条件。该方法可直接在独立的相空间样本上训练，无需未来状态信息或昂贵的轨迹生成。在多种系统上的实验验证表明，特别是在使用机器学习力场的分子动力学模拟中，该方法在保持相近计算成本的同时，能够支持比传统方法大得多的稳定积分时间步长。</div>
</details>
</div>
<div class="card">
<div class="title">Alpha Discovery via Grammar-Guided Learning and Search</div>
<div class="meta-line">Authors: Han Yang, Dong Hao, Zhuohan Wang, Qi Shi, Xingtong Li</div>
<div class="meta-line">First: 2026-01-29T18:46:15+00:00 · Latest: 2026-01-29T18:46:15+00:00</div>
<div class="meta-line">Comments: 24 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22119v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于语法引导学习与搜索的阿尔法因子发现</div>
<div class="mono" style="margin-top:8px">自动发现公式化阿尔法因子是量化金融的核心问题。现有方法常忽略句法和语义约束，依赖对非结构化、无边界空间的穷举搜索。本文提出AlphaCFG，一种基于语法的框架，用于定义和发现句法有效、金融可解释且计算高效的阿尔法因子。AlphaCFG使用面向阿尔法的上下文无关语法定义树形结构、规模可控的搜索空间，并将阿尔法发现建模为树形结构语言马尔可夫决策过程，进而通过语法感知的蒙特卡洛树搜索（由句法敏感的价值与策略网络引导）求解。在中国和美国股市数据集上的实验表明，AlphaCFG在搜索效率和交易盈利能力上均优于当前最先进的基线方法。除交易策略外，AlphaCFG还可作为量化金融中符号化因子发现与优化的通用框架，适用于资产定价和投资组合构建等领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of automatically discovering formulaic alpha factors in quantitative finance, where existing methods often overlook syntactic and semantic constraints, leading to inefficient searches over unstructured spaces. The proposed method, AlphaCFG, introduces a grammar-based framework that defines a tree-structured, size-controlled search space using an alpha-oriented context-free grammar, formulating alpha discovery as a tree-structured linguistic Markov decision process solved via grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experimental results on Chinese and U.S. stock market datasets demonstrate that AlphaCFG surpasses state-of-the-art baselines in both search efficiency and trading profitability, and it extends as a general framework for symbolic factor discovery in areas like asset pricing and portfolio construction.</div>
<div class="mono" style="margin-top:8px">该研究针对量化金融中自动发现公式化阿尔法因子的挑战，现有方法常忽略句法和语义约束，导致在非结构化空间中进行低效搜索。提出的方法AlphaCFG引入了一种基于语法的框架，使用面向阿尔法的上下文无关文法定义树状结构、大小可控的搜索空间，将阿尔法发现建模为树状结构语言马尔可夫决策过程，并通过语法感知的蒙特卡洛树搜索结合语法敏感的价值和策略网络求解。在中国和美国股市数据集上的实验结果表明，AlphaCFG在搜索效率和交易盈利能力上均优于现有先进基线，并可扩展为资产定价和投资组合构建等量化金融领域中符号因子发现与优化的通用框架。</div>
</details>
</div>
<div class="card">
<div class="title">Diverse Approaches to Optimal Execution Schedule Generation</div>
<div class="meta-line">Authors: Robert de Witt, Mikko S. Pakkanen</div>
<div class="meta-line">First: 2026-01-29T18:41:52+00:00 · Latest: 2026-01-29T18:41:52+00:00</div>
<div class="meta-line">Comments: 27 pages, 15 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22113v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22113v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present the first application of MAP-Elites, a quality-diversity algorithm, to trade execution. Rather than searching for a single optimal policy, MAP-Elites generates a diverse portfolio of regime-specialist strategies indexed by liquidity and volatility conditions. Individual specialists achieve 8-10% performance improvements within their behavioural niches, while other cells show degradation, suggesting opportunities for ensemble approaches that combine improved specialists with the baseline PPO policy. Results indicate that quality-diversity methods offer promise for regime-adaptive execution, though substantial computational resources per behavioural cell may be required for robust specialist development across all market conditions. To ensure experimental integrity, we develop a calibrated Gymnasium environment focused on order scheduling rather than tactical placement decisions. The simulator features a transient impact model with exponential decay and square-root volume scaling, fit to 400+ U.S. equities with R^2&gt;0.02 out-of-sample. Within this environment, two Proximal Policy Optimization architectures - both MLP and CNN feature extractors - demonstrate substantial improvements over industry baselines, with the CNN variant achieving 2.13 bps arrival slippage versus 5.23 bps for VWAP on 4,900 out-of-sample orders ($21B notional). These results validate both the simulation realism and provide strong single-policy baselines for quality-diversity methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最优执行计划生成的多样化方法</div>
<div class="mono" style="margin-top:8px">本研究首次将质量多样性算法MAP-Elites应用于交易执行领域。该方法不寻求单一最优策略，而是生成按流动性和波动率条件索引的多样化制度专家策略组合。个体专家在其行为生态位内实现8-10%的性能提升，而其他单元则出现性能退化，这为结合改进专家与基线PPO策略的集成方法提供了机遇。结果表明质量多样性方法在制度自适应执行方面具有潜力，但每个行为单元可能需要大量计算资源以在所有市场条件下实现稳健的专家策略开发。为确保实验完整性，我们开发了专注于订单调度而非战术性挂单决策的校准化Gymnasium环境。该模拟器采用具有指数衰减和平方根成交量缩放特性的瞬态冲击模型，基于400多只美国股票拟合，样本外R²&gt;0.02。在此环境中，两种近端策略优化架构（MLP和CNN特征提取器）均较行业基线实现显著改进，其中CNN变体在4,900笔样本外订单（名义价值210亿美元）上达到2.13个基点的到达滑点，而VWAP策略为5.23个基点。这些结果既验证了模拟环境的真实性，也为质量多样性方法提供了强有力的单策略基线。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to move beyond single optimal policies in trade execution and instead develop a diverse portfolio of strategies that can adapt to varying market regimes. The method applies MAP-Elites, a quality-diversity algorithm, to generate a collection of regime-specialist strategies indexed by liquidity and volatility conditions, using a calibrated Gymnasium simulator with a transient impact model. Key experimental findings show that individual specialist strategies achieve 8-10% performance improvements within their specific niches, while other cells exhibit degradation, indicating potential for ensemble methods; additionally, baseline Proximal Policy Optimization models, particularly a CNN variant, significantly outperform industry benchmarks, achieving 2.13 bps arrival slippage compared to 5.23 bps for VWAP on out-of-sample orders.</div>
<div class="mono" style="margin-top:8px">本研究首次将质量多样性算法MAP-Elites应用于交易执行，其动机是超越单一最优策略，生成一个针对不同流动性及波动性市场状态的多样化专业策略组合。该方法通过开发一个基于瞬态冲击模型的校准Gymnasium模拟器，并使用具有MLP和CNN架构的近端策略优化算法来训练状态专家策略。主要实验结果表明，个体专家策略在其特定行为生态位内实现了8-10%的性能提升，同时基于CNN的PPO策略显著优于行业VWAP基准，在样本外订单上将到达滑点从5.23个基点降低至2.13个基点，尽管在所有市场条件下开发稳健的专家策略可能需要大量的计算资源。</div>
</details>
</div>
<div class="card">
<div class="title">Physics Informed Reconstruction of Four-Dimensional Atmospheric Wind Fields Using Multi-UAS Swarm Observations in a Synthetic Turbulent Environment</div>
<div class="meta-line">Authors: Abdullah Tasim, Wei Sun</div>
<div class="meta-line">First: 2026-01-29T18:40:32+00:00 · Latest: 2026-01-29T18:40:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22111v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22111v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate reconstruction of atmospheric wind fields is essential for applications such as weather forecasting, hazard prediction, and wind energy assessment, yet conventional instruments leave spatio-temporal gaps within the lower atmospheric boundary layer. Unmanned aircraft systems (UAS) provide flexible in situ measurements, but individual platforms sample wind only along their flight trajectories, limiting full wind-field recovery. This study presents a framework for reconstructing four-dimensional atmospheric wind fields using measurements obtained from a coordinated UAS swarm. A synthetic turbulence environment and high-fidelity multirotor simulation are used to generate training and evaluation data. Local wind components are estimated from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM) and assimilated into a physics-informed neural network (PINN) to reconstruct a continuous wind field in space and time. For local wind estimation, the bidirectional LSTM achieves root-mean-square errors (RMSE) of 0.064 and 0.062 m/s for the north and east components in low-wind conditions, increasing to 0.122 to 0.129 m/s under moderate winds and 0.271 to 0.273 m/s in high-wind conditions, while the vertical component exhibits higher error, with RMSE values of 0.029 to 0.091 m/s. The physics-informed reconstruction recovers the dominant spatial and temporal structure of the wind field up to 1000 m altitude while preserving mean flow direction and vertical shear. Under moderate wind conditions, the reconstructed mean wind field achieves an overall RMSE between 0.118 and 0.154 m/s across evaluated UAS configurations, with the lowest error obtained using a five-UAS swarm. These results demonstrate that coordinated UAS measurements enable accurate and scalable four-dimensional wind-field reconstruction without dedicated wind sensors or fixed infrastructure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多无人机集群观测的合成湍流环境中四维大气风场物理信息重构</div>
<div class="mono" style="margin-top:8px">大气风场的精确重构对天气预报、灾害预警和风能评估等应用至关重要，但传统仪器在低层大气边界层内存在时空观测空白。无人机系统虽能提供灵活的原位测量，但单机仅能沿航线采样，限制了全风场重构。本研究提出一种利用协同无人机集群观测数据重构四维大气风场的框架：通过合成湍流环境与高保真多旋翼仿真生成训练与评估数据；采用双向长短期记忆网络从无人机动力学数据估算局地风分量，再通过物理信息神经网络同化数据，实现时空连续风场重构。在局地风估算中，双向LSTM对低风速条件下北向与东向分量的均方根误差分别为0.064与0.062 m/s，中风速时增至0.122-0.129 m/s，高风速时达0.271-0.273 m/s，垂直分量误差较高（0.029-0.091 m/s）。物理信息重构方法在1000米高度内恢复了风场主导时空结构，同时保持了平均流向与垂直切变。中风速条件下，重构平均风场在不同无人机配置中的整体均方根误差为0.118-0.154 m/s，其中五机集群误差最低。结果表明：协同无人机观测无需专用风传感器或固定基础设施，即可实现精确且可扩展的四维风场重构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate reconstruction of atmospheric wind fields is needed for weather and hazard prediction, but conventional instruments create spatio-temporal gaps in the lower atmosphere. To address this, the study develops a framework that uses measurements from a coordinated swarm of unmanned aircraft systems (UAS) to reconstruct four-dimensional wind fields. The method first estimates local wind components from UAS dynamics using a bidirectional long short-term memory network (Bi-LSTM), then assimilates these estimates into a physics-informed neural network (PINN) for continuous spatio-temporal reconstruction. Experimental results in a synthetic turbulent environment show the Bi-LSTM achieves root-mean-square errors (RMSE) as low as 0.062–0.064 m/s for horizontal components in low winds, while the PINN-based reconstruction recovers the dominant wind structure up to 1000 m altitude, with an overall RMSE of 0.118–0.154 m/s under moderate winds using a five-UAS swarm.</div>
<div class="mono" style="margin-top:8px">精确重建大气风场对于天气预报和灾害预测至关重要，但传统仪器在低层大气边界层中会留下时空观测空白。为解决此问题，本研究提出了一种利用协同无人机群观测数据重建风场的框架。该方法首先使用双向长短期记忆网络从无人机飞行动力学中估计局部风分量，然后将这些估计值同化到物理信息神经网络中，以重建连续的四维风场。在合成湍流环境中的实验结果表明，双向长短期记忆网络在不同风况下对水平风分量的均方根误差为0.064-0.273米/秒；而基于物理信息神经网络的重建能保持风场的主要时空结构和平均流向，在中等风况下使用五架无人机群配置时，整体均方根误差达到0.118-0.154米/秒。</div>
</details>
</div>
<div class="card">
<div class="title">Value-Based Pre-Training with Downstream Feedback</div>
<div class="meta-line">Authors: Shuqi Ke, Giulia Fanti</div>
<div class="meta-line">First: 2026-01-29T18:38:09+00:00 · Latest: 2026-01-29T18:38:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22108v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22108v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can a small amount of verified goal information steer the expensive self-supervised pretraining of foundation models? Standard pretraining optimizes a fixed proxy objective (e.g., next-token prediction), which can misallocate compute away from downstream capabilities of interest. We introduce V-Pretraining: a value-based, modality-agnostic method for controlled continued pretraining in which a lightweight task designer reshapes the pretraining task to maximize the value of each gradient step. For example, consider self-supervised learning (SSL) with sample augmentation. The V-Pretraining task designer selects pretraining tasks (e.g., augmentations) for which the pretraining loss gradient is aligned with a gradient computed over a downstream task (e.g., image segmentation). This helps steer pretraining towards relevant downstream capabilities. Notably, the pretrained model is never updated on downstream task labels; they are used only to shape the pretraining task. Under matched learner update budgets, V-Pretraining of 0.5B--7B language models improves reasoning (GSM8K test Pass@1) by up to 18% relative over standard next-token prediction using only 12% of GSM8K training examples as feedback. In vision SSL, we improve the state-of-the-art results on ADE20K by up to 1.07 mIoU and reduce NYUv2 RMSE while improving ImageNet linear accuracy, and we provide pilot evidence of improved token efficiency in continued pretraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于价值的下游反馈预训练方法</div>
<div class="mono" style="margin-top:8px">少量已验证的目标信息能否引导基础模型昂贵的自监督预训练？标准预训练优化固定代理目标（如下一标记预测），可能导致计算资源偏离关注的下游能力。我们提出V-Pretraining：一种基于价值、模态无关的受控持续预训练方法，通过轻量级任务设计器重塑预训练任务以最大化每个梯度步的价值。例如在样本增强的自监督学习中，任务设计器选择预训练损失梯度与下游任务梯度对齐的预训练任务（如增强策略），从而将预训练导向相关下游能力。关键的是，预训练模型从未使用下游任务标签更新，标签仅用于塑造预训练任务。在相同更新预算下，对0.5B-7B语言模型进行V-Pretraining，仅使用12%的GSM8K训练样本作为反馈，推理能力相对标准下一标记预测提升达18%。在视觉自监督学习中，ADE20K指标提升1.07 mIoU，NYUv2 RMSE降低且ImageNet线性准确率提升，并提供了持续预训练中标记效率提升的初步证据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the misalignment between standard self-supervised pretraining objectives, like next-token prediction, and downstream task performance, which can lead to inefficient compute allocation. The proposed method, V-Pretraining, is a value-based, modality-agnostic approach that uses a lightweight task designer to reshape the pretraining task selection, such as choosing specific data augmentations, so that the pretraining loss gradient aligns with gradients computed from a small set of downstream task feedback. Experimental results show that applying V-Pretraining to language models (0.5B–7B parameters) improves reasoning performance on GSM8K by up to 18% relative using only 12% of the training examples for feedback, while in vision SSL, it advances state-of-the-art on ADE20K segmentation by up to 1.07 mIoU, reduces depth estimation error on NYUv2, and maintains or improves ImageNet linear accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对标准自监督预训练目标（如下一个词预测）与下游任务性能之间的错位问题，这种错位可能导致计算资源分配低效。提出的方法V-Pretraining是一种基于价值、与模态无关的技术，它通过一个轻量级任务设计器来重塑预训练任务的选择（例如选择特定的数据增强），使得预训练损失梯度与基于少量下游任务反馈计算的梯度对齐，从而在不直接使用下游标签训练的情况下引导模型学习相关能力。实验结果表明，将V-Pretraining应用于语言模型（0.5B–7B参数）时，仅使用12%的GSM8K训练样本作为反馈，就能将推理性能相对提升高达18%；在视觉自监督学习中，该方法将ADE20K分割任务的性能提升了高达1.07 mIoU，降低了NYUv2的误差，同时保持或提升了ImageNet线性准确率，并显示出在持续预训练中更好的令牌效率。</div>
</details>
</div>
<div class="card">
<div class="title">Prior-Informed Flow Matching for Graph Reconstruction</div>
<div class="meta-line">Authors: Harvey Chen, Nicolas Zilberstein, Santiago Segarra</div>
<div class="meta-line">First: 2026-01-29T18:38:02+00:00 · Latest: 2026-01-29T18:38:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22107v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22107v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Prior-Informed Flow Matching (PIFM), a conditional flow model for graph reconstruction. Reconstructing graphs from partial observations remains a key challenge; classical embedding methods often lack global consistency, while modern generative models struggle to incorporate structural priors. PIFM bridges this gap by integrating embedding-based priors with continuous-time flow matching. Grounded in a permutation equivariant version of the distortion-perception theory, our method first uses a prior, such as graphons or GraphSAGE/node2vec, to form an informed initial estimate of the adjacency matrix based on local information. It then applies rectified flow matching to refine this estimate, transporting it toward the true distribution of clean graphs and learning a global coupling. Experiments on different datasets demonstrate that PIFM consistently enhances classical embeddings, outperforming them and state-of-the-art generative baselines in reconstruction accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于先验信息的流匹配图重构方法</div>
<div class="mono" style="margin-top:8px">本文提出先验信息流匹配（PIFM）——一种用于图重构的条件流模型。从局部观测重构完整图结构仍是关键挑战：经典嵌入方法常缺乏全局一致性，而现代生成模型难以融入结构先验。PIFM通过将基于嵌入的先验知识与连续时间流匹配相结合来弥合这一鸿沟。基于置换等变版本的失真-感知理论，本方法首先利用图函数或GraphSAGE/node2vec等先验，根据局部信息形成邻接矩阵的知情初始估计，随后应用修正流匹配对该估计进行精细化处理，使其向干净图的真实分布迁移并学习全局耦合关系。在不同数据集上的实验表明，PIFM能持续提升经典嵌入方法的性能，在重构精度上超越现有方法及最先进的生成基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of reconstructing graphs from partial observations, where classical embedding methods often lack global consistency and modern generative models struggle to incorporate structural priors. The proposed Prior-Informed Flow Matching (PIFM) method integrates embedding-based priors, such as graphons or GraphSAGE/node2vec, to form an initial adjacency estimate from local information, and then refines it using continuous-time rectified flow matching to learn a global coupling and transport the estimate toward the true clean graph distribution. Experimental results on various datasets show that PIFM consistently enhances classical embeddings and outperforms both them and state-of-the-art generative baselines in reconstruction accuracy.</div>
<div class="mono" style="margin-top:8px">本研究针对从部分观测中重建图这一挑战，其中经典嵌入方法常缺乏全局一致性，而现代生成模型难以融入结构先验。提出的先验信息流匹配方法通过整合基于嵌入的先验（如图子或GraphSAGE/node2vec），利用局部信息形成初始邻接矩阵估计，然后应用修正流匹配进行细化，将其传输至干净图的真实分布并学习全局耦合。在不同数据集上的实验表明，该方法持续提升了经典嵌入的性能，在重建精度上优于它们及先进的生成基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">ECO: Quantized Training without Full-Precision Master Weights</div>
<div class="meta-line">Authors: Mahdi Nikdan, Amir Zandieh, Dan Alistarh, Vahab Mirrokni</div>
<div class="meta-line">First: 2026-01-29T18:35:01+00:00 · Latest: 2026-01-29T18:35:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22101v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22101v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ECO：无需全精度主权重的量化训练方法</div>
<div class="mono" style="margin-top:8px">量化技术显著提升了大型语言模型（LLM）训练的计算与内存效率，但现有方法仍需依赖高精度累积更新：具体而言，梯度更新必须作用于高精度权重缓冲区（即$\textit{主权重}$）。该缓冲区带来显著内存开销，尤其对于稀疏专家混合（SMoE）模型，其模型参数与优化器状态占主导内存。为此，我们提出误差补偿优化器（ECO），通过直接将更新应用于量化参数来消除主权重。ECO在每步训练后量化权重，并将量化误差精确注入优化器动量，形成无需额外内存的误差反馈循环。我们证明，在标准假设与衰减学习率下，ECO能收敛至最优解的常数半径邻域，而简单移除主权重可能导致误差与学习率成反比。实验部分展示了小型Transformer（30-800M）、Gemma-3 1B模型及2.1B参数SMoE模型在FP8量化预训练中的结果，以及DeepSeek-MoE-16B模型在INT4精度下的微调效果。在所有实验中，ECO在保持近乎无损精度的前提下与含主权重的基线方法性能相当，显著优化了静态内存与验证损失的帕累托边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Quantization reduces memory and compute costs in LLM training but still requires high-precision master weights to accumulate updates, which is especially burdensome for memory-intensive models like Sparse Mixture of Experts (SMoE). To eliminate this overhead, the authors propose the Error-Compensating Optimizer (ECO), a method that applies gradient updates directly to quantized weights, quantizes them after each step, and injects the quantization error into the optimizer momentum via an error-feedback loop without extra memory. Theoretically, ECO converges to a neighborhood of the optimum under standard assumptions, unlike naive approaches that incur large errors. Experiments on Transformers (30M-800M), Gemma-3 1B, a 2.1B SMoE model with FP8, and fine-tuning DeepSeek-MoE-16B in INT4 show that ECO matches the accuracy of master-weight baselines nearly losslessly, improving the memory-accuracy Pareto frontier.</div>
<div class="mono" style="margin-top:8px">量化降低了大型语言模型训练的内存和计算成本，但现有方法仍需高精度主权重来累积更新，这带来了显著的内存开销，对稀疏专家混合模型尤为突出。为消除该缓冲区，本文提出误差补偿优化器，它直接将梯度更新应用于量化权重，每步后重新量化，并将量化误差注入优化器动量以形成无额外内存的误差反馈循环。理论分析表明，该方法能收敛至最优解邻域，而简单移除主权重会导致误差与学习率成反比。在30M-800M Transformer、Gemma-3 1B、2.1B稀疏专家混合模型上的FP8量化实验，以及INT4精度下微调DeepSeek-MoE-16B的结果显示，该方法在近乎无损的精度下匹配了使用主权重的基线，显著改善了内存与验证损失的帕累托前沿。</div>
</details>
</div>
<div class="card">
<div class="title">Do graph neural network states contain graph properties?</div>
<div class="meta-line">Authors: Tom Pelletreau-Duris, Ruud van Bakel, Michael Cochez</div>
<div class="meta-line">Venue: Proceedings of Machine Learning Research vol 284:1_37 2025, 19th Conference on Neurosymbolic Learning and Reasoning</div>
<div class="meta-line">First: 2024-11-04T15:26:07+00:00 · Latest: 2026-01-29T18:33:46+00:00</div>
<div class="meta-line">Comments: 10 pages, 22 figures, conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.02168v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.02168v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep neural networks (DNNs) achieve state-of-the-art performance on many tasks, but this often requires increasingly larger model sizes, which in turn leads to more complex internal representations. Explainability techniques (XAI) have made remarkable progress in the interpretability of ML models. However, the non-euclidean nature of Graph Neural Networks (GNNs) makes it difficult to reuse already existing XAI methods. While other works have focused on instance-based explanation methods for GNNs, very few have investigated model-based methods and, to our knowledge, none have tried to probe the embedding of the GNNs for structural graph properties. In this paper we present a model agnostic explainability pipeline for Graph Neural Networks (GNNs) employing diagnostic classifiers. We propose to consider graph-theoretic properties as the features of choice for studying the emergence of representations in GNNs. This pipeline aims to probe and interpret the learned representations in GNNs across various architectures and datasets, refining our understanding and trust in these models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图神经网络状态是否包含图属性？</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）在许多任务上实现了最先进的性能，但这通常需要越来越大的模型规模，进而导致更复杂的内部表示。可解释性技术（XAI）在机器学习模型的可解释性方面取得了显著进展。然而，图神经网络（GNNs）的非欧几里得特性使得难以复用现有的XAI方法。尽管其他研究专注于GNNs的基于实例的解释方法，但极少有研究探讨基于模型的方法，且据我们所知，尚未有尝试探测GNNs嵌入中的结构图属性。本文提出了一种模型无关的图神经网络（GNNs）可解释性流程，采用诊断分类器。我们建议将图论属性作为研究GNNs表示涌现的特征选择。该流程旨在探测和解释不同架构与数据集中GNNs学习到的表示，以深化对这些模型的理解与信任。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of interpreting the complex internal representations of Graph Neural Networks (GNNs), as their non-Euclidean nature hinders the application of existing explainability (XAI) methods, and few studies have probed these embeddings for structural graph properties. The method introduces a model-agnostic explainability pipeline that employs diagnostic classifiers to analyze GNN states, specifically using graph-theoretic properties as features to study the emergence of representations across various architectures and datasets. The key experimental findings demonstrate that this pipeline successfully probes and interprets the learned representations, thereby refining understanding and trust in GNN models by revealing how they encode structural graph information.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决图神经网络（GNN）内部复杂表示难以解释的问题，因为其非欧几里得特性阻碍了现有可解释性方法的直接应用。方法上，提出了一种与模型无关的可解释性流程，利用诊断分类器来探测GNN嵌入是否编码了结构性的图论属性。关键的实验结果表明，该流程能够成功分析不同GNN架构和数据集上的学习表示，从而增强了对这些模型的理解和信任。</div>
</details>
</div>
<div class="card">
<div class="title">Boosting CVaR Policy Optimization with Quantile Gradients</div>
<div class="meta-line">Authors: Yudong Luo, Erick Delage</div>
<div class="meta-line">First: 2026-01-29T18:33:46+00:00 · Latest: 2026-01-29T18:33:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22100v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing Conditional Value-at-risk (CVaR) using policy gradient (a.k.a CVaR-PG) faces significant challenges of sample inefficiency. This inefficiency stems from the fact that it focuses on tail-end performance and overlooks many sampled trajectories. We address this problem by augmenting CVaR with an expected quantile term. Quantile optimization admits a dynamic programming formulation that leverages all sampled data, thus improves sample efficiency. This does not alter the CVaR objective since CVaR corresponds to the expectation of quantile over the tail. Empirical results in domains with verifiable risk-averse behavior show that our algorithm within the Markovian policy class substantially improves upon CVaR-PG and consistently outperforms other existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于分位数梯度的CVaR策略优化增强方法</div>
<div class="mono" style="margin-top:8px">使用策略梯度优化条件风险价值（CVaR-PG）面临样本效率低下的显著挑战，这源于其仅关注尾部性能而忽略大量采样轨迹。我们通过为CVaR增加期望分位数项来解决此问题。分位数优化可采用动态规划形式，能充分利用所有采样数据从而提升样本效率。由于CVaR对应于尾部区域分位数的期望，该方法不会改变CVaR目标函数。在具有可验证风险规避行为的领域进行实证研究，结果表明我们提出的马尔可夫策略类算法显著优于CVaR-PG，并持续超越其他现有方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the sample inefficiency of Conditional Value-at-Risk policy gradient (CVaR-PG) methods, which focus on tail-end performance and discard many sampled trajectories. To improve efficiency, the method augments the CVaR objective with an expected quantile term, enabling a dynamic programming formulation that leverages all sampled data without altering the original CVaR objective. Experimental results in domains with verifiable risk-averse behavior demonstrate that the proposed algorithm within the Markovian policy class substantially outperforms CVaR-PG and consistently exceeds other existing methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决条件风险价值（CVaR）策略梯度方法因仅关注尾部性能而丢弃大量采样轨迹所导致的样本效率低下问题。所提出的方法通过向CVaR目标添加期望分位数项，实现了可利用所有采样数据的动态规划公式，从而在不改变原CVaR目标的前提下提高了优化效率。在可验证风险规避行为的领域中的实验结果表明，该算法在马尔可夫策略类中显著优于标准CVaR-PG方法，并持续超越其他现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">GeoNorm: Unify Pre-Norm and Post-Norm with Geodesic Optimization</div>
<div class="meta-line">Authors: Chuanyang Zheng, Jiankai Sun, Yihang Gao, Chi Wang, Yuehao Wang, Jing Xiong, Liliang Ren, Bo Peng, Qingmei Wang, Xiaoran Shang, Mac Schwager, Anderson Schneider, Yuriy Nevmyvaka, Xiaodong Liu</div>
<div class="meta-line">First: 2026-01-29T18:31:31+00:00 · Latest: 2026-01-29T18:31:31+00:00</div>
<div class="meta-line">Comments: Tech Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22095v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22095v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The placement of normalization layers, specifically Pre-Norm and Post-Norm, remains an open question in Transformer architecture design. In this work, we rethink these approaches through the lens of manifold optimization, interpreting the outputs of the Feed-Forward Network (FFN) and attention layers as update directions in optimization. Building on this perspective, we introduce GeoNorm, a novel method that replaces standard normalization with geodesic updates on the manifold. Furthermore, analogous to learning rate schedules, we propose a layer-wise update decay for the FFN and attention components. Comprehensive experiments demonstrate that GeoNorm consistently outperforms existing normalization methods in Transformer models. Crucially, GeoNorm can be seamlessly integrated into standard Transformer architectures, achieving performance improvements with negligible additional computational cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GeoNorm：基于测地线优化统一前归一化与后归一化</div>
<div class="mono" style="margin-top:8px">归一化层（尤其是前归一化与后归一化）的放置位置在Transformer架构设计中仍是一个开放性问题。本研究从流形优化的角度重新审视这些方法，将前馈网络与注意力层的输出解释为优化过程中的更新方向。基于这一视角，我们提出了GeoNorm——一种在流形上进行测地线更新以替代标准归一化的新方法。此外，受学习率调度机制启发，我们为前馈网络和注意力组件设计了分层更新衰减策略。综合实验表明，GeoNorm在Transformer模型中持续优于现有归一化方法。关键的是，GeoNorm能够无缝集成到标准Transformer架构中，以可忽略的额外计算成本实现性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the open question of whether to place normalization layers before (Pre-Norm) or after (Post-Norm) sub-layers in Transformers. It reinterprets the outputs of attention and feed-forward layers as optimization update directions and introduces GeoNorm, a method that replaces standard normalization with geodesic updates on a manifold, alongside a layer-wise update decay schedule. Experiments show that GeoNorm consistently outperforms existing normalization techniques in Transformer models and can be integrated with negligible extra computational cost.</div>
<div class="mono" style="margin-top:8px">本研究针对Transformer中归一化层放置（如前归一化和后归一化）这一未决问题展开。方法上提出了GeoNorm，它将前馈网络和注意力层的输出重新解释为优化更新方向，并用流形上的测地线更新替代标准归一化，同时引入了类似学习率调度的层间更新衰减。实验结果表明，GeoNorm在Transformer模型中持续优于现有归一化方法，且能无缝集成到标准架构中，以可忽略的额外计算成本实现性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Latent Adversarial Regularization for Offline Preference Optimization</div>
<div class="meta-line">Authors: Enyi Jiang, Yibo Jacky Zhang, Yinglun Xu, Andreas Haupt, Nancy Amato, Sanmi Koyejo</div>
<div class="meta-line">First: 2026-01-29T18:21:57+00:00 · Latest: 2026-01-29T18:21:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22083v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22083v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning from human feedback typically relies on preference optimization that constrains policy updates through token-level regularization. However, preference optimization for language models is particularly challenging because token-space similarity does not imply semantic or behavioral similarity. To address this challenge, we leverage latent-space regularization for language model preference optimization. We introduce GANPO, which achieves latent-space regularization by penalizing divergence between the internal representations of a policy model and a reference model. Given that latent representations are not associated with explicit probability densities, we adopt an adversarial approach inspired by GANs to minimize latent-space divergence. We integrate GANPO as a regularizer into existing offline preference optimization objectives. Experiments across multiple model architectures and tasks show consistent improvements from latent-space regularization. Further, by comparing GANPO-induced inferential biases with those from token-level regularization, we find that GANPO provides more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minor computational overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向离线偏好优化的隐式对抗正则化</div>
<div class="mono" style="margin-top:8px">基于人类反馈的学习通常依赖偏好优化，通过词元级正则化约束策略更新。然而，语言模型的偏好优化尤为困难，因为词元空间相似性并不意味语义或行为相似性。为解决这一挑战，我们采用隐空间正则化进行语言模型偏好优化。我们提出GANPO方法，通过惩罚策略模型与参考模型内部表征的差异来实现隐空间正则化。鉴于隐式表征没有显式概率密度对应，我们采用受生成对抗网络启发的对抗方法最小化隐空间差异。我们将GANPO作为正则化器集成到现有离线偏好优化目标中。跨多种模型架构和任务的实验表明，隐空间正则化能带来持续改进。进一步通过比较GANPO与词元级正则化引发的推理偏差，发现GANPO在分布偏移和噪声条件下能提供更稳健的结构反馈，同时以微小计算开销保持可比的下游性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitation of token-level regularization in language model preference optimization, where token-space similarity does not guarantee semantic or behavioral alignment. The method introduces GANPO, which applies latent adversarial regularization by minimizing the divergence between the internal representations of a policy model and a reference model using a GAN-inspired adversarial approach, integrated into existing offline preference optimization objectives. Experimental results across various models and tasks demonstrate that GANPO consistently improves performance, offering more robust structural feedback under distributional shift and noise while maintaining comparable downstream performance with minimal computational overhead.</div>
<div class="mono" style="margin-top:8px">语言模型偏好优化通常采用词元级正则化，但词元空间相似性并不能保证语义或行为相似性，这限制了其效果。为解决这一问题，研究者提出了GANPO方法，该方法通过惩罚策略模型与参考模型内部表示之间的差异，实现了潜在空间正则化；由于潜在表示缺乏明确的概率密度，该方法采用了受生成对抗网络（GAN）启发的对抗性方法。在多种模型架构和任务上的实验表明，GANPO能持续提升性能，与词元级正则化相比，它在分布偏移和噪声下能提供更稳健的结构反馈，同时以微小的计算开销保持了相当的下游性能。</div>
</details>
</div>
<div class="card">
<div class="title">Vecchia-Inducing-Points Full-Scale Approximations for Gaussian Processes</div>
<div class="meta-line">Authors: Tim Gyger, Reinhard Furrer, Fabio Sigrist</div>
<div class="meta-line">First: 2025-07-07T14:49:06+00:00 · Latest: 2026-01-29T18:19:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.05064v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.05064v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gaussian processes are flexible, probabilistic, non-parametric models widely used in machine learning and statistics. However, their scalability to large data sets is limited by computational constraints. To overcome these challenges, we propose Vecchia-inducing-points full-scale (VIF) approximations combining the strengths of global inducing points and local Vecchia approximations. Vecchia approximations excel in settings with low-dimensional inputs and moderately smooth covariance functions, while inducing point methods are better suited to high-dimensional inputs and smoother covariance functions. Our VIF approach bridges these two regimes by using an efficient correlation-based neighbor-finding strategy for the Vecchia approximation of the residual process, implemented via a modified cover tree algorithm. We further extend our framework to non-Gaussian likelihoods by introducing iterative methods that substantially reduce computational costs for training and prediction by several orders of magnitudes compared to Cholesky-based computations when using a Laplace approximation. In particular, we propose and compare novel preconditioners and provide theoretical convergence results. Extensive numerical experiments on simulated and real-world data sets show that VIF approximations are both computationally efficient as well as more accurate and numerically stable than state-of-the-art alternatives. All methods are implemented in the open source C++ library GPBoost with high-level Python and R interfaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高斯过程的Vecchia诱导点全尺度近似方法</div>
<div class="mono" style="margin-top:8px">高斯过程是机器学习与统计学中广泛使用的灵活、概率性、非参数模型，但其在大规模数据集上的可扩展性受计算限制。为克服这一挑战，本文提出Vecchia诱导点全尺度（VIF）近似方法，融合全局诱导点与局部Vecchia近似的优势。Vecchia近似在低维输入与中等平滑协方差函数场景表现优异，而诱导点方法更适用于高维输入及更平滑的协方差函数。VIF方法通过基于相关性的高效邻点搜索策略（采用改进的覆盖树算法实现）对残差过程进行Vecchia近似，从而衔接两种范式。我们进一步将框架扩展至非高斯似然场景，引入迭代方法结合拉普拉斯近似，相比基于Cholesky分解的计算，将训练与预测的计算成本降低数个数量级。特别地，我们提出并比较了新型预条件子，并提供了理论收敛性证明。在模拟与真实数据集上的大量数值实验表明，VIF近似不仅计算高效，其精度与数值稳定性也优于当前先进方法。所有方法均在开源C++库GPBoost中实现，并提供高级Python与R接口。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Gaussian processes face computational scalability issues with large datasets, motivating the development of Vecchia-inducing-points full-scale (VIF) approximations to combine the complementary strengths of global inducing-point methods and local Vecchia approximations. The method employs a correlation-based neighbor-finding strategy, implemented via a modified cover tree algorithm, for the Vecchia approximation of the residual process, and extends to non-Gaussian likelihoods using iterative methods with novel preconditioners within a Laplace approximation framework to drastically reduce training and prediction costs. Experimental results on simulated and real-world data demonstrate that VIF approximations achieve greater computational efficiency, accuracy, and numerical stability compared to existing state-of-the-art alternatives.</div>
<div class="mono" style="margin-top:8px">本研究针对高斯过程在大规模数据集上的计算可扩展性限制，提出了Vecchia诱导点全尺度（VIF）近似方法。该方法通过改进的覆盖树算法实现基于相关性的邻居查找策略，将全局诱导点与局部Vecchia近似相结合，并通过带有新型预处理器的迭代方法扩展到非高斯似然。在模拟和真实数据集上的实验结果表明，与基于Cholesky分解的方法相比，VIF近似实现了数量级的计算效率提升，同时比现有先进方法具有更高的精度和数值稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">Think Locally, Explain Globally: Graph-Guided LLM Investigations via Local Reasoning and Belief Propagation</div>
<div class="meta-line">Authors: Saurabh Jha, Rohan Arora, Bhavya, Noah Zheutlin, Paulina Toro Isaza, Laura Shwartz, Yu Deng, Daby Sow, Ruchi Mahindru, Ruchir Puri</div>
<div class="meta-line">First: 2026-01-25T17:27:19+00:00 · Latest: 2026-01-29T18:18:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17915v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17915v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents excel when environments are mostly static and the needed information fits in a model&#x27;s context window, but they often fail in open-ended investigations where explanations must be constructed by iteratively mining evidence from massive, heterogeneous operational data. These investigations exhibit hidden dependency structure: entities interact, signals co-vary, and the importance of a fact may only become clear after other evidence is discovered. Because the context window is bounded, agents must summarize intermediate findings before their significance is known, increasing the risk of discarding key evidence. ReAct-style agents are especially brittle in this regime. Their retrieve-summarize-reason loop makes conclusions sensitive to exploration order and introduces run-to-run non-determinism, producing a reliability gap where Pass-at-k may be high but Majority-at-k remains low. Simply sampling more rollouts or generating longer reasoning traces does not reliably stabilize results, since hypotheses cannot be autonomously checked as new evidence arrives and there is no explicit mechanism for belief bookkeeping and revision. In addition, ReAct entangles semantic reasoning with controller duties such as tool orchestration and state tracking, so execution errors and plan drift degrade reasoning while consuming scarce context.
  We address these issues by formulating investigation as abductive reasoning over a dependency graph and proposing EoG (Explanations over Graphs), a disaggregated framework in which an LLM performs bounded local evidence mining and labeling (cause vs symptom) while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. On a representative ITBench diagnostics task, EoG improves both accuracy and run-to-run consistency over ReAct baselines, including a 7x average gain in Majority-at-k entity F1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>局部思考，全局解释：基于局部推理与信念传播的图引导大语言模型调查框架</div>
<div class="mono" style="margin-top:8px">大语言模型智能体在环境相对静态且所需信息能适配模型上下文窗口时表现优异，但在需要从海量异构操作数据中迭代挖掘证据以构建解释的开放式调查任务中常显不足。此类调查存在隐含的依赖结构：实体相互关联、信号协同变化，且事实的重要性往往需在其他证据被发现后才能显现。由于上下文窗口受限，智能体必须在明确其意义前就总结中间发现，增加了丢弃关键证据的风险。ReAct类智能体在此场景下尤为脆弱——其“检索-总结-推理”循环使结论对探索顺序敏感，并引入运行间非确定性，导致可靠性缺口：即使k次尝试通过率可能较高，但k次多数一致性仍保持低位。单纯增加采样轮次或生成更长推理轨迹并不能稳定结果，因为新证据出现时假设无法自主验证，且缺乏明确的信念簿记与修正机制。此外，ReAct将语义推理与工具编排、状态跟踪等控制职责耦合，致使执行错误和计划漂移在消耗有限上下文的同时损害推理质量。
我们通过将调查建模为依赖图上的溯因推理提出解决方案：EoG（图解释框架）采用解耦架构，由大语言模型执行有界的局部证据挖掘与标注（原因vs症状），而确定性控制器管理图遍历、状态维护及信念传播以计算最小解释边界。在典型ITBench诊断任务中，EoG相比ReAct基线在准确性与运行间一致性上均实现提升，其中k次多数实体F1分数平均提升达7倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the limitations of LLM agents in open-ended investigations, where they struggle to construct explanations by iteratively mining evidence from massive, heterogeneous data due to bounded context windows and the hidden dependency structure among entities. The proposed method, Explanations over Graphs (EoG), formulates investigation as abductive reasoning over a dependency graph, disaggregating the process so an LLM performs local evidence mining and labeling while a deterministic controller manages traversal, state, and belief propagation to compute a minimal explanatory frontier. Experimental results on the ITBench diagnostics task show that EoG improves both accuracy and run-to-run consistency over ReAct baselines, achieving a 7x average gain in Majority-at-k entity F1.</div>
<div class="mono" style="margin-top:8px">本研究针对大语言模型代理在开放式调查中的局限性，即由于上下文窗口有限以及实体间存在隐藏的依赖关系，它们难以通过从海量异构数据中迭代挖掘证据来构建解释。提出的方法EoG（基于图的解释）将调查建模为在依赖图上的溯因推理，将过程解耦：大语言模型负责局部证据挖掘和标注，而一个确定性控制器则管理图遍历、状态和信念传播，以计算最小解释边界。在ITBench诊断任务上的实验结果表明，与ReAct基线相比，EoG提高了准确性和运行间一致性，在Majority-at-k实体F1指标上平均获得了7倍的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Where Do the Joules Go? Diagnosing Inference Energy Consumption</div>
<div class="meta-line">Authors: Jae-Won Chung, Ruofan Wu, Jeff J. Ma, Mosharaf Chowdhury</div>
<div class="meta-line">First: 2026-01-29T18:16:45+00:00 · Latest: 2026-01-29T18:16:45+00:00</div>
<div class="meta-line">Comments: The ML.ENERGY Leaderboard v3.0 is open https://ml.energy/leaderboard</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22076v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22076v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Energy is now a critical ML computing resource. While measuring energy consumption and observing trends is a valuable first step, accurately understanding and diagnosing why those differences occur is crucial for optimization. To that end, we begin by presenting a large-scale measurement study of inference time and energy across the generative AI landscape with 46 models, 7 tasks, and 1,858 different configurations on NVIDIA H100 and B200 GPUs. Our empirical findings span order-of-magnitude variations: LLM task type can lead to 25$\times$ energy differences, video generation sometimes consumes more than 100$\times$ the energy of images, and GPU utilization differences can result in 3--5$\times$ energy differences. Based on our observations, we present a framework for reasoning about the underlying mechanisms that govern time and energy consumption. The essence is that time and energy are determined by latent metrics like memory and utilization, which are in turn affected by various factors across the algorithm, software, and hardware layers. Our framework also extends directly to throughput per watt, a critical metric for power-constrained datacenters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>焦耳去向何方？诊断推理能耗</div>
<div class="mono" style="margin-top:8px">能源已成为机器学习计算的关键资源。测量能耗并观察趋势是重要的第一步，但准确理解并诊断差异产生的原因对优化至关重要。为此，我们首先在生成式AI领域开展了一项大规模测量研究，涵盖46个模型、7项任务及1,858种不同配置，基于英伟达H100和B200 GPU进行推理时间与能耗分析。实证发现存在数量级差异：大语言模型任务类型可导致25倍能耗差异，视频生成能耗有时超图像生成100倍以上，GPU利用率差异可造成3-5倍能耗波动。基于观测结果，我们提出一个用于解析时间与能耗底层机制的框架。其核心在于：时间与能耗由内存占用、利用率等潜在指标决定，而这些指标又受算法层、软件层和硬件层多重因素影响。该框架可直接延伸至每瓦吞吐量分析——这是功耗受限数据中心的关键指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the critical role of energy as a computing resource in machine learning, this study conducts a large-scale measurement analysis to diagnose the underlying causes of energy consumption during AI inference. The method involves benchmarking 46 generative AI models across 7 tasks and 1,858 configurations on NVIDIA H100 and B200 GPUs, leading to the development of a framework that explains energy and time costs through latent metrics like memory usage and GPU utilization, which are influenced by algorithmic, software, and hardware factors. Key experimental findings reveal substantial energy variations: LLM task types cause up to 25× differences, video generation can consume over 100× more energy than image generation, and GPU utilization disparities result in 3–5× energy differences, highlighting the importance of throughput per watt for power-constrained datacenters.</div>
<div class="mono" style="margin-top:8px">本研究基于能源作为机器学习关键计算资源的背景，旨在诊断推理能耗背后的影响因素。方法上，通过对46个生成式AI模型、7项任务和1,858种配置在NVIDIA H100和B200 GPU上进行大规模实测，并构建了一个通过内存使用和GPU利用率等潜在指标解释能耗与时间的分析框架。主要实验结果显示出数量级的能耗差异：大语言模型任务类型可导致高达25倍的能耗差异，视频生成有时比图像生成多消耗100倍以上的能量，而GPU利用率差异会造成3至5倍的能耗变化，这强调了在算法、软件和硬件层面进行优化的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">How Many Ratings per Item are Necessary for Reliable Significance Testing?</div>
<div class="meta-line">Authors: Christopher Homan, Flip Korn, Deepak Pandita, Chris Welty</div>
<div class="meta-line">First: 2024-12-04T02:31:28+00:00 · Latest: 2026-01-29T18:15:35+00:00</div>
<div class="meta-line">Comments: Accepted at EACL Findings 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.02968v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.02968v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A cornerstone of machine learning evaluation is the (often hidden) assumption that model and human responses are reliable enough to evaluate models against unitary, authoritative, ``gold standard&#x27;&#x27; data, via simple metrics such as accuracy, precision, and recall. The generative AI revolution would seem to explode this assumption, given the critical role stochastic inference plays. Yet, in spite of public demand for more transparency in AI -- along with strong evidence that humans are unreliable judges -- estimates of model reliability are conventionally based on, at most, a few output responses per input item. We adapt a method, previously used to evaluate the reliability of various metrics and estimators for machine learning evaluation, to determine whether an (existing or planned) dataset has enough responses per item to assure reliable null hypothesis statistical testing. We show that, for many common metrics, collecting even 5-10 responses per item (from each model and team of human evaluators) is not sufficient. We apply our methods to several of the very few extant gold standard test sets with multiple disaggregated responses per item and show that even these datasets lack enough responses per item. We show how our methods can help AI researchers make better decisions about how to collect data for AI evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>每个项目需要多少评分才能进行可靠的显著性检验？</div>
<div class="mono" style="margin-top:8px">机器学习评估的一个基石是（通常隐含的）假设：模型和人类响应的可靠性足以通过准确率、精确率和召回率等简单指标，将模型与单一、权威的“黄金标准”数据进行对比评估。鉴于随机推断的关键作用，生成式AI革命似乎打破了这一假设。然而，尽管公众要求AI更加透明，且有强有力证据表明人类判断并不可靠，但模型可靠性的估计通常最多基于每个输入项目的少数输出响应。我们采用一种先前用于评估机器学习评估中各种指标和估计器可靠性的方法，以确定（现有或计划中的）数据集是否在每个项目上拥有足够的响应来确保可靠的零假设统计检验。研究表明，对于许多常见指标，即使每个项目收集5-10个响应（来自每个模型和人类评估团队）也是不够的。我们将方法应用于少数现有的每个项目具有多个独立响应的黄金标准测试集，结果显示即使这些数据集在每个项目上的响应数量也不足。我们展示了该方法如何帮助AI研究人员更好地决策如何收集用于AI评估的数据。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for transparent and reliable AI evaluation, especially given the stochastic nature of generative models and known human unreliability, this study adapts a statistical method to assess whether existing or planned datasets contain enough responses per item for reliable null hypothesis significance testing. The method analyzes the required number of ratings per item for common evaluation metrics. Key experimental findings show that collecting only 5-10 responses per item is often insufficient for reliable testing, and an analysis of several existing multi-response datasets confirms they too lack adequate per-item ratings. The work provides a tool to guide better data collection practices for AI evaluation.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估人工智能评估数据集的可靠性，因为传统实践通常每个项目仅依赖少数人类或模型响应，这在生成式AI的随机性背景下可能不足以进行稳健的统计检验。该方法采用了一种现有技术来评估数据集是否包含足够的每项目响应，以确保对常见指标进行可靠的零假设显著性检验。实验结果表明，即使每个项目收集5-10个响应通常也不够充分，对几个现有多响应数据集的分析证实它们缺乏足够的每项目评分，这凸显了该方法在指导改进AI评估数据收集方面的实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Harmonizing Safety and Speed: A Human-Algorithm Approach to Enhance the FDA&#x27;s Medical Device Clearance Policy</div>
<div class="meta-line">Authors: Mohammad Zhalechian, Soroush Saghafian, Omar Robles</div>
<div class="meta-line">First: 2024-07-16T15:11:29+00:00 · Latest: 2026-01-29T18:09:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.11823v3">Abs</a> · <a href="https://arxiv.org/pdf/2407.11823v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The United States Food and Drug Administration&#x27;s (FDA&#x27;s) 510(k) pathway allows manufacturers to gain medical device approval by demonstrating substantial equivalence to a legally marketed device. However, the inherent ambiguity of this regulatory procedure has been associated with high recall among many devices cleared through this pathway, raising significant safety concerns. In this paper, we develop a combined human-algorithm approach to assist the FDA in improving its 510(k) medical device clearance process by reducing recall risk and regulatory workload. We first develop machine learning methods to estimate the risk of recall of 510(k) medical devices based on the information available at the time of submission. We then propose a data-driven clearance policy that recommends acceptance, rejection, or deferral to FDA&#x27;s committees for in-depth evaluation. We conduct an empirical study using a unique dataset of over 31,000 submissions that we assembled based on data sources from the FDA and Centers for Medicare and Medicaid Service (CMS). Compared to the FDA&#x27;s current practice, which has a recall rate of 10.3% and a normalized workload measure of 100%, a conservative evaluation of our policy shows a 32.9% improvement in the recall rate and a 40.5% reduction in the workload. Our analyses further suggest annual cost savings of approximately $1.7 billion for the healthcare system driven by avoided replacement costs, which is equivalent to 1.1% of the entire United States annual medical device expenditure. Our findings highlight the value of a holistic and data-driven approach to improve the FDA&#x27;s current 510(k) pathway.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全与效率的协调：人机协同优化FDA医疗器械审批政策</div>
<div class="mono" style="margin-top:8px">美国食品药品监督管理局（FDA）的510(k)审批路径允许制造商通过证明与已上市器械实质等同性获得批准，但该流程的模糊性导致大量经此途径获批的器械召回率居高，引发重大安全隐患。本研究提出一种人机协同方法，通过降低召回风险与监管负荷协助FDA改进510(k)审批流程：首先开发机器学习模型，基于申报时可用信息预测器械召回风险；继而构建数据驱动的审批策略，向FDA委员会推荐接受、拒绝或转交深入评估。基于FDA及医疗保险和医疗补助服务中心（CMS）数据构建的31,000余份申报数据集实证表明：相较于FDA现行10.3%召回率及100%标准化工作负荷，保守评估下本策略可使召回率改善32.9%，工作负荷降低40.5%。进一步分析显示，医疗系统因避免置换成本每年可节约约17亿美元，相当于美国全年医疗器械支出的1.1%。本研究凸显了采用整体性数据驱动方法优化FDA现有510(k)路径的重要价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by safety concerns and high recall rates associated with the FDA&#x27;s 510(k) medical device clearance pathway, this study develops a combined human-algorithm approach to improve the process. The method first employs machine learning to estimate device recall risk based on submission data and then proposes a data-driven policy that triages submissions into acceptance, rejection, or deferral for committee review. An empirical evaluation on over 31,000 submissions shows the proposed policy can reduce the recall rate by 32.9% and cut regulatory workload by 40.5% compared to current practice, with potential annual healthcare cost savings of approximately $1.7 billion.</div>
<div class="mono" style="margin-top:8px">针对美国食品药品监督管理局（FDA）510(k)医疗器械审批路径存在的安全风险和高召回率问题，本研究开发了一种人机协同的方法来改进该流程。该方法首先利用机器学习，基于提交时的信息预测器械的召回风险，然后提出一种数据驱动的审批策略，将申请分类为批准、拒绝或转交委员会深入评估。基于超过31,000份申请数据的实证研究表明，与现行做法相比，该策略可将召回率降低32.9%，并将监管工作量减少40.5%，预计每年可为医疗系统节省约17亿美元的成本。</div>
</details>
</div>
<div class="card">
<div class="title">Making Foundation Models Probabilistic via Singular Value Ensembles</div>
<div class="meta-line">Authors: Mehmet Ozgur Turkoglu, Dominik J. Mühlematter, Alexander Becker, Konrad Schindler, Helge Aasen</div>
<div class="meta-line">First: 2026-01-29T18:07:18+00:00 · Latest: 2026-01-29T18:07:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22068v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22068v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models have become a dominant paradigm in machine learning, achieving remarkable performance across diverse tasks through large-scale pretraining. However, these models often yield overconfident, uncalibrated predictions. The standard approach to quantifying epistemic uncertainty, training an ensemble of independent models, incurs prohibitive computational costs that scale linearly with ensemble size, making it impractical for large foundation models. We propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that builds on a simple, but powerful core assumption: namely, that the singular vectors of the weight matrices constitute meaningful subspaces of the model&#x27;s knowledge. Pretrained foundation models encode rich, transferable information in their weight matrices. If the singular vectors are indeed meaningful (orthogonal) &quot;knowledge directions&quot;. To obtain a model ensemble, we modulate only how strongly each direction contributes to the output. Rather than learning entirely new parameters, we freeze the singular vectors and only train per-member singular values that rescale the contribution of each direction in that shared knowledge basis. Ensemble diversity emerges naturally as stochastic initialization and random sampling of mini-batches during joint training cause different members to converge to different combinations of the same underlying knowledge. SVE achieves uncertainty quantification comparable to explicit deep ensembles while increasing the parameter count of the base model by less than 1%, making principled uncertainty estimation accessible in resource-constrained settings. We validate SVE on NLP and vision tasks with various different backbones and show that it improves calibration while maintaining predictive accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奇异值集成实现基础模型的概率化</div>
<div class="mono" style="margin-top:8px">基础模型已成为机器学习的主导范式，通过大规模预训练在多样化任务中取得显著性能。然而，这些模型常产生过度自信且未校准的预测。量化认知不确定性的标准方法——训练独立模型集成——会带来随集成规模线性增长的巨额计算成本，对大型基础模型不具可行性。本文提出奇异值集成（SVE），一种基于核心假设的参数高效隐式集成方法：权重矩阵的奇异向量构成模型知识的有意义子空间。预训练基础模型在其权重矩阵中编码了丰富可迁移的信息，若奇异向量确为有意义（正交）的“知识方向”，则可通过仅调控各方向对输出的贡献强度获得模型集成。该方法冻结奇异向量，仅训练用于在共享知识基中重新缩放各方向贡献的成员专属奇异值，无需学习全新参数。集成多样性通过联合训练中随机初始化与小批量随机采样自然形成，使不同成员收敛至同一底层知识的不同组合。SVE在仅增加基础模型不足1%参数量的前提下，实现了与显式深度集成相当的不确定性量化，使资源受限场景下的原则性不确定性估计成为可能。我们在多种骨干网络的NLP与视觉任务上验证了SVE，证明其在保持预测准确性的同时提升了校准能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Foundation models often produce overconfident and uncalibrated predictions, and quantifying their epistemic uncertainty via standard deep ensembles is computationally prohibitive due to the linear scaling of cost with ensemble size. To address this, the authors propose Singular Value Ensemble (SVE), a parameter-efficient implicit ensemble method that assumes the singular vectors of a pretrained model&#x27;s weight matrices form meaningful orthogonal knowledge directions; SVE freezes these singular vectors and only trains a small set of per-ensemble-member singular values to rescale each direction&#x27;s contribution, thereby generating ensemble diversity through stochastic training while sharing the vast majority of parameters. Experiments on NLP and vision tasks with various backbones demonstrate that SVE achieves uncertainty quantification comparable to explicit deep ensembles, improves calibration, and maintains predictive accuracy, all while increasing the parameter count by less than 1%.</div>
<div class="mono" style="margin-top:8px">基础模型常产生过度自信且校准不佳的预测，而现有不确定性量化方法（如深度集成）计算成本过高，难以应用于大型模型。为此，本文提出奇异值集成（SVE），这是一种参数高效的隐式集成方法，其核心假设是预训练模型权重矩阵的奇异向量代表了正交的知识方向；SVE冻结这些奇异向量，仅训练每个集成成员独有的奇异值来缩放各方向的贡献，通过随机训练自然产生集成多样性。在自然语言处理和视觉任务上的实验表明，SVE实现了与深度集成相当的不确定性量化效果，同时仅增加不到1%的参数，在保持预测准确性的同时改善了校准性能。</div>
</details>
</div>
<div class="card">
<div class="title">SiDGen: Structure-informed Diffusion for Generative modeling of Ligands for Proteins</div>
<div class="meta-line">Authors: Samyak Sanghvi, Nishant Ranjan, Tarak Karmakar</div>
<div class="meta-line">First: 2025-11-12T18:25:51+00:00 · Latest: 2026-01-29T17:59:16+00:00</div>
<div class="meta-line">Comments: 10 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09529v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09529v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing ligands that are both chemically valid and structurally compatible with protein binding pockets is a key bottleneck in computational drug discovery. Existing approaches either ignore structural context or rely on expensive, memory-intensive encoding that limits throughput and scalability. We present SiDGen (Structure-informed Diffusion Generator), a protein-conditioned diffusion framework that integrates masked SMILES generation with lightweight folding-derived features for pocket awareness. To balance expressivity with efficiency, SiDGen supports two conditioning pathways: a streamlined mode that pools coarse structural signals from protein embeddings and a full mode that injects localized pairwise biases for stronger coupling. A coarse-stride folding mechanism with nearest-neighbor upsampling alleviates the quadratic memory costs of pair tensors, enabling training on realistic sequence lengths. Learning stability is maintained through in-loop chemical validity checks and an invalidity penalty, while large-scale training efficiency is restored \textit{via} selective compilation, dataloader tuning, and gradient accumulation. In automated benchmarks, SiDGen generates ligands with high validity, uniqueness, and novelty, while achieving competitive performance in docking-based evaluations and maintaining reasonable molecular properties. These results demonstrate that SiDGen can deliver scalable, pocket-aware molecular design, providing a practical route to conditional generation for high-throughput drug discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SiDGen：基于结构信息的扩散模型用于蛋白质配体生成建模</div>
<div class="mono" style="margin-top:8px">设计既符合化学有效性又与蛋白质结合口袋结构兼容的配体，是计算药物发现的关键瓶颈。现有方法要么忽略结构上下文，要么依赖昂贵且内存密集的编码方式，限制了通量和可扩展性。我们提出SiDGen（结构信息扩散生成器），一种蛋白质条件化扩散框架，将掩码SMILES生成与轻量级折叠衍生特征相结合以实现口袋感知。为平衡表达能力与效率，SiDGen支持两种条件化路径：一种从蛋白质嵌入中汇集粗粒度结构信号的流线型模式，以及一种注入局部成对偏置以实现更强耦合的完整模式。采用最近邻上采样的粗步长折叠机制缓解了成对张量的二次内存开销，使得在真实序列长度上训练成为可能。通过循环内化学有效性检查和无效性惩罚保持学习稳定性，同时通过选择性编译、数据加载器调优和梯度累积恢复大规模训练效率。在自动化基准测试中，SiDGen生成的配体具有高有效性、独特性和新颖性，在基于对接的评估中取得竞争性性能，并保持合理的分子性质。这些结果表明SiDGen能够实现可扩展的口袋感知分子设计，为高通量药物发现的条件化生成提供了实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research addresses the challenge of designing chemically valid and structurally compatible ligands for protein binding pockets in drug discovery, where existing methods often neglect structural context or suffer from high computational costs. The proposed method, SiDGen, is a protein-conditioned diffusion framework that combines masked SMILES generation with lightweight folding-derived features for pocket awareness, offering two conditioning pathways: a streamlined mode using pooled protein embeddings and a full mode with localized pairwise biases for stronger coupling, alongside a coarse-stride folding mechanism to reduce memory overhead. Experimental results show that SiDGen generates ligands with high validity, uniqueness, and novelty, achieving competitive performance in docking-based evaluations while maintaining reasonable molecular properties, thus demonstrating its scalability and effectiveness for high-throughput molecular design.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决为蛋白质结合口袋设计化学有效且结构兼容的配体这一挑战，这是计算药物发现中的一个瓶颈，现有方法常忽略结构背景或计算成本过高。提出的方法SiDGen是一个蛋白质条件扩散框架，它将掩码SMILES生成与轻量级折叠衍生特征相结合以实现口袋感知，提供两种条件路径：一种使用池化蛋白质嵌入的简化模式，另一种是具有局部成对偏置以实现更强耦合的完整模式。关键实验结果表明，SiDGen生成的配体具有高有效性、独特性和新颖性，在基于对接的评估中表现优异，并保持了合理的分子性质，证明了其在高通量药物发现中实现可扩展、口袋感知分子设计的潜力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260131_0449.html">20260131_0449</a>
<a href="archive/20260131_0342.html">20260131_0342</a>
<a href="archive/20260130_0631.html">20260130_0631</a>
<a href="archive/20260130_0533.html">20260130_0533</a>
<a href="archive/20260130_0449.html">20260130_0449</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0630.html">20260129_0630</a>
<a href="archive/20260129_0536.html">20260129_0536</a>
<a href="archive/20260129_0450.html">20260129_0450</a>
<a href="archive/20260129_0336.html">20260129_0336</a>
<a href="archive/20260128_0625.html">20260128_0625</a>
<a href="archive/20260128_0439.html">20260128_0439</a>
<a href="archive/20260128_0334.html">20260128_0334</a>
<a href="archive/20260127_0627.html">20260127_0627</a>
<a href="archive/20260127_0529.html">20260127_0529</a>
<a href="archive/20260127_0439.html">20260127_0439</a>
<a href="archive/20260127_0333.html">20260127_0333</a>
<a href="archive/20260126_0626.html">20260126_0626</a>
<a href="archive/20260126_0526.html">20260126_0526</a>
<a href="archive/20260126_0327.html">20260126_0327</a>
<a href="archive/20260125_0624.html">20260125_0624</a>
<a href="archive/20260125_0524.html">20260125_0524</a>
<a href="archive/20260125_0440.html">20260125_0440</a>
<a href="archive/20260125_0325.html">20260125_0325</a>
<a href="archive/20260124_0627.html">20260124_0627</a>
<a href="archive/20260124_0526.html">20260124_0526</a>
<a href="archive/20260124_0444.html">20260124_0444</a>
<a href="archive/20260124_0334.html">20260124_0334</a>
<a href="archive/20260123_0627.html">20260123_0627</a>
<a href="archive/20260123_0529.html">20260123_0529</a>
<a href="archive/20260123_0446.html">20260123_0446</a>
<a href="archive/20260123_0334.html">20260123_0334</a>
<a href="archive/20260122_2019.html">20260122_2019</a>
<a href="archive/20260122_2013.html">20260122_2013</a>
<a href="archive/20260122_2008.html">20260122_2008</a>
<a href="archive/20260122_2001.html">20260122_2001</a>
<a href="archive/20260122_0629.html">20260122_0629</a>
<a href="archive/20260122_0535.html">20260122_0535</a>
<a href="archive/20260122_0449.html">20260122_0449</a>
<a href="archive/20260122_0336.html">20260122_0336</a>
<a href="archive/20260121_0625.html">20260121_0625</a>
<a href="archive/20260121_0529.html">20260121_0529</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260121_0423.html">20260121_0423</a>
<a href="archive/20260120_0626.html">20260120_0626</a>
<a href="archive/20260120_0528.html">20260120_0528</a>
<a href="archive/20260120_0443.html">20260120_0443</a>
<a href="archive/20260120_0330.html">20260120_0330</a>
<a href="archive/20260119_0624.html">20260119_0624</a>
<a href="archive/20260119_0525.html">20260119_0525</a>
<a href="archive/20260119_0440.html">20260119_0440</a>
<a href="archive/20260119_0325.html">20260119_0325</a>
<a href="archive/20260118_2225.html">20260118_2225</a>
<a href="archive/20260118_2159.html">20260118_2159</a>
<a href="archive/20260118_2135.html">20260118_2135</a>
<a href="archive/20260118_2040.html">20260118_2040</a>
<a href="archive/20260118_1953.html">20260118_1953</a>
<a href="archive/20260118_1907.html">20260118_1907</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
